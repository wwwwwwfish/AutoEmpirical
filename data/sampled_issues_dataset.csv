issue,title,comments,state,created_at,updated_at,body,comments_content,label
https://github.com/tensorflow/tfjs/issues/2818,module not found on MacOS,13,closed,2020-03-04T11:09:26Z,2020-03-30T22:50:26Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version""@tensorflow/tfjs-node"": ""^1.5.2""#### Browser versionnode.js v.12.x#### Describe the problem or feature requestWhen I enter '$ node index', I got an error like as below.``` consoleinternal/modules/cjs/loader.js:800    throw err,    ^Error: Cannot find module '/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node'Require stack:- /Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js- /Users/giwanlim/dev/tfjs_tutorial/index.js    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:797:15)    at Function.Module._load (internal/modules/cjs/loader.js:690:27)    at Module.require (internal/modules/cjs/loader.js:852:19)    at require (internal/modules/cjs/helpers.js:74:18)    at Object.<anonymous> (/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js:46:16)    at Module._compile (internal/modules/cjs/loader.js:959:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:995:10)    at Module.load (internal/modules/cjs/loader.js:815:32)    at Function.Module._load (internal/modules/cjs/loader.js:727:14)    at Module.require (internal/modules/cjs/loader.js:852:19) {  code: 'MODULE_NOT_FOUND',  requireStack: [    '/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js',    '/Users/giwanlim/dev/tfjs_tutorial/index.js'  ]}```#### Code to reproduce the bug / link to feature request``` javascript// index.jsconst tf = require('@tensorflow/tfjs'),require('@tensorflow/tfjs-node'),```","['@march23hare thanks for reporting ,can you please share complete code and steps to reproduce the error.=====', ""here is the exact steps1. `$ mkdir tfjs_tutorial`2. `$ cd tfjs_tutorial`3. `$ yarn init -y`4. `$ yarn add @tensorflow/tfjs-node`5. `$ yarn add @tensorflow/tfjs`6. `$ touch index.js` and edit it like as below. This is the complete code. I used to test importing modules before I write some codes.``` javascript// index.jsconst tf = require('@tensorflow/tfjs'),require('@tensorflow/tfjs-node'),```7. `$ node index`8. ERROR!``` consoleinternal/modules/cjs/loader.js:800    throw err,    ^Error: Cannot find module '/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node'Require stack:- /Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js- /Users/giwanlim/dev/tfjs_tutorial/index.js    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:797:15)    at Function.Module._load (internal/modules/cjs/loader.js:690:27)    at Module.require (internal/modules/cjs/loader.js:852:19)    at require (internal/modules/cjs/helpers.js:74:18)    at Object.<anonymous> (/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js:46:16)    at Module._compile (internal/modules/cjs/loader.js:959:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:995:10)    at Module.load (internal/modules/cjs/loader.js:815:32)    at Function.Module._load (internal/modules/cjs/loader.js:727:14)    at Module.require (internal/modules/cjs/loader.js:852:19) {  code: 'MODULE_NOT_FOUND',  requireStack: [    '/Users/giwanlim/dev/tfjs_tutorial/node_modules/@tensorflow/tfjs-node/dist/index.js',    '/Users/giwanlim/dev/tfjs_tutorial/index.js'  ]}```====="", '@march23hare thank you , which tutorial are you running ?=====', ""I had have a plan to run https://www.tensorflow.org/js/tutorials/setup?hl=ko#see-sample-code-for-node.js-usage, but I've never run.====="", '@march23hare did you get chance to look at the similar issue [here](https://github.com/tensorflow/tfjs/issues/2678) =====', 'It did not work.1.  there is nothing in ` ~/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5```` console$ ls -altotal 0drwxr-xr-x  2 giwanlim  staff  64  3  6 14:10 .drwxr-xr-x  3 giwanlim  staff  96  3  6 14:10 ..```2. run `npm run build-addon-from-source` at `~/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node` got error``` console> @tensorflow/tfjs-node@1.6.0 build-addon-from-source /Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node> node-pre-gyp install --build-from-sourcenode-pre-gyp WARN Using needle for node-pre-gyp https download Traceback (most recent call last):  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/gyp_main.py"", line 50, in <module>    sys.exit(gyp.script_main())  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 554, in script_main    return main(sys.argv[1:])  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 547, in main    return gyp_main(args)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 532, in gyp_main    generator.GenerateOutput(flat_list, targets, data, params)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 2215, in GenerateOutput    part_of_all=qualified_target in needed_targets)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 845, in Write    mac_bundle_deps, extra_outputs, part_of_all)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1539, in WriteTarget    self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1896, in GetSortedXcodePostbuildEnv    additional_settings={\'CHROMIUM_STRIP_SAVE_FILE\': strip_save_file})  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1885, in GetSortedXcodeEnv    additional_settings)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1616, in GetSortedXcodeEnv    additional_settings)  File ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1527, in _GetXcodeEnv    if XcodeVersion() >= \'0500\' and not env.get(\'SDKROOT\'):TypeError: \'>=\' not supported between instances of \'tuple\' and \'str\'gyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:210:5)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:272:12)gyp ERR! System Darwin 19.3.0gyp ERR! command ""/Users/giwanlim/.nvm/versions/node/v12.14.0/bin/node"" ""/Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""configure"" ""--build-from-source"" ""--module=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5"" ""--napi_version=5"" ""--node_abi_napi=napi"" ""--napi_build_version=5"" ""--node_napi_label=napi-v5""gyp ERR! cwd /Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v12.14.0gyp ERR! node-gyp -v v5.0.5gyp ERR! not ok node-pre-gyp ERR! build error node-pre-gyp ERR! stack Error: Failed to execute \'/Users/giwanlim/.nvm/versions/node/v12.14.0/bin/node /Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --build-from-source --module=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5 --napi_version=5 --node_abi_napi=napi --napi_build_version=5 --node_napi_label=napi-v5\' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/giwanlim/dev/tfjs_tut/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:210:5)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1021:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:283:5)node-pre-gyp ERR! System Darwin 19.3.0node-pre-gyp ERR! command ""/Users/giwanlim/.nvm/versions/node/v12.14.0/bin/node"" ""/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/node_modules/.bin/node-pre-gyp"" ""install"" ""--build-from-source""node-pre-gyp ERR! cwd /Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v12.14.0node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok Failed to execute \'/Users/giwanlim/.nvm/versions/node/v12.14.0/bin/node /Users/giwanlim/.nvm/versions/node/v12.14.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --build-from-source --module=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5 --napi_version=5 --node_abi_napi=napi --napi_build_version=5 --node_napi_label=napi-v5\' (1)npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@1.6.0 build-addon-from-source: `node-pre-gyp install --build-from-source`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the @tensorflow/tfjs-node@1.6.0 build-addon-from-source script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /Users/giwanlim/.npm/_logs/2020-03-06T05_16_11_179Z-debug.log```I\'ve re-installed tfjs-node module 1.6.0 but it still not works.=====', 'Hey. Same issue here.**What I tried so far:**- `npm install @tensorflow/tfjs-node@1.5.1 --save`- Change `python3 ` to `python2`- Went to `tfjs-node` and ran `npm run build-addon-from-source`, here\'s the output:```> @tensorflow/tfjs-node@1.5.1 build-addon-from-source /Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node> node-pre-gyp install --build-from-sourcenode-pre-gyp WARN Using request for node-pre-gyp https download   CXX(target) Release/obj.target/tfjs_binding/binding/tfjs_backend.oclang: error: no such file or directory: \'Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/deps/include\'make: *** [Release/obj.target/tfjs_binding/binding/tfjs_backend.o] Error 1gyp ERR! build error gyp ERR! stack Error: `make` failed with exit code: 2gyp ERR! stack     at ChildProcess.onExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:196:23)gyp ERR! stack     at ChildProcess.emit (events.js:198:13)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:248:12)gyp ERR! System Darwin 18.2.0gyp ERR! command ""/usr/local/lib/node_modules/node/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""build"" ""--build-from-source"" ""--module=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4"" ""--napi_version=4"" ""--node_abi_napi=napi"" ""--napi_build_version=4"" ""--node_napi_label=napi-v4""gyp ERR! cwd /Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v10.16.3gyp ERR! node-gyp -v v5.0.3gyp ERR! not ok node-pre-gyp ERR! build error node-pre-gyp ERR! stack Error: Failed to execute \'/usr/local/lib/node_modules/node/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js build --build-from-source --module=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4 --napi_version=4 --node_abi_napi=napi --napi_build_version=4 --node_napi_label=napi-v4\' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:198:13)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:259:5)node-pre-gyp ERR! System Darwin 18.2.0node-pre-gyp ERR! command ""/usr/local/lib/node_modules/node/bin/node"" ""/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/.bin/node-pre-gyp"" ""install"" ""--build-from-source""node-pre-gyp ERR! cwd /Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v10.16.3node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok Failed to execute \'/usr/local/lib/node_modules/node/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js build --build-from-source --module=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4 --napi_version=4 --node_abi_napi=napi --napi_build_version=4 --node_napi_label=napi-v4\' (1)npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@1.5.1 build-addon-from-source: `node-pre-gyp install --build-from-source`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the @tensorflow/tfjs-node@1.5.1 build-addon-from-source script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /Users/cyrus/.npm/_logs/2020-03-10T00_42_24_953Z-debug.log```**My settings now:**My `package.json`:```    ""@tensorflow-models/mobilenet"": ""^2.0.4"",    ""@tensorflow/tfjs"": ""^1.6.0"",    ""@tensorflow/tfjs-node"": ""^1.5.1"",    ""@tensorflow/tfjs-node-gpu"": ""^1.6.0"",```**My code:**```const readImage = path => {  const imageBuffer = fs.readFileSync(path),  const tfimage = tfnode.node.decodeImage(imageBuffer),  return tfimage,},const performTheImageClassification = async path  => {    const image = readImage(""img_to_predict/actual.jpeg""),    console.log(image),     const mobilenetModel = await mobilenet.load(),     const predictions = await mobilenetModel.classify(image),     console.log(\'Classification Results:\', predictions),},```**My error logs:**````node-pre-gyp info This Node instance does not support builds for N-API version 5node-pre-gyp info This Node instance does not support builds for N-API version 5internal/modules/cjs/loader.js:638    throw err,    ^Error: Cannot find module \'/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node\'    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:636:15)    at Function.Module._load (internal/modules/cjs/loader.js:562:25)    at Module.require (internal/modules/cjs/loader.js:692:17)    at require (internal/modules/cjs/helpers.js:25:18)    at Object.<anonymous> (/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/node_modules/@tensorflow/tfjs-node/dist/index.js:46:16)    at Module._compile (internal/modules/cjs/loader.js:778:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)    at Module.load (internal/modules/cjs/loader.js:653:32)    at tryModuleLoad (internal/modules/cjs/loader.js:593:12)    at Function.Module._load (internal/modules/cjs/loader.js:585:3)    at Module.require (internal/modules/cjs/loader.js:692:17)    at require (internal/modules/cjs/helpers.js:25:18)    at Object.<anonymous> (/Users/cyrus/Documents/Code/01. Code/debord_slow_sender_meme/server.js:27:16)    at Module._compile (internal/modules/cjs/loader.js:778:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)    at Module.load (internal/modules/cjs/loader.js:653:32)```=====', 'I am having the same problem and have been unsuccessful at getting `@tensorflow/tfjs-node` running on my Mac. I have tried node versions 10 and 12 and get the same `node-pre-gyp ERR` message when installing. I am on MacOS Catalina. =====', 'As mentioned [here](https://github.com/tensorflow/tfjs/tree/master/tfjs-node#mac-os-x-requires-xcode) , you should only include only one of the packages. Please check @march23hare @sfarthin =====', '@rthadur This error has been occurred when I required only one module.## package.json``` package.json{  ""name"": ""tfjs_tut"",  ""version"": ""1.0.0"",  ""main"": ""index.js"",  ""license"": ""MIT"",  ""scripts"": {    ""start"": ""node index""  },  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.6.0""  }}```## index.js```index.jsconst tf = require(\'@tensorflow/tfjs-node\'),// Train a simple model:const model = tf.sequential(),model.add(tf.layers.dense({units: 100, activation: \'relu\', inputShape: [10]})),model.add(tf.layers.dense({units: 1, activation: \'linear\'})),model.compile({optimizer: \'sgd\', loss: \'meanSquaredError\'}),const xs = tf.randomNormal([100, 10]),const ys = tf.randomNormal([100, 1]),model.fit(xs, ys, {  epochs: 100,  callbacks: {    onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`)  }}),```## CLI``` console$ node --versionv12.14.0$ yarn run startyarn run v1.21.1$ node indexinternal/modules/cjs/loader.js:800    throw err,    ^Error: Cannot find module \'/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node\'Require stack:- /Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/dist/index.js- /Users/giwanlim/dev/tfjs_tut/index.js    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:797:15)    at Function.Module._load (internal/modules/cjs/loader.js:690:27)    at Module.require (internal/modules/cjs/loader.js:852:19)    at require (internal/modules/cjs/helpers.js:74:18)    at Object.<anonymous> (/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/dist/index.js:46:16)    at Module._compile (internal/modules/cjs/loader.js:959:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:995:10)    at Module.load (internal/modules/cjs/loader.js:815:32)    at Function.Module._load (internal/modules/cjs/loader.js:727:14)    at Module.require (internal/modules/cjs/loader.js:852:19) {  code: \'MODULE_NOT_FOUND\',  requireStack: [    \'/Users/giwanlim/dev/tfjs_tut/node_modules/@tensorflow/tfjs-node/dist/index.js\',    \'/Users/giwanlim/dev/tfjs_tut/index.js\'  ]}```=====', ""hi @march23hare 1. are you still seeing the same issue with latest version (1.7.0)? 2. can you confirm what is inside `node_modules/@tensorflow/tfjs-node/deps` and `node_modules/@tensorflow/tfjs-node/lib'? ====="", ""@kangyizhang, I've tried with the version `1.7.0` and it's working! I'll let @march23hare  confirm but on my side everything is ok. thanks!====="", '@kangyizhang Hi.1. It works well with latest version! Thank you!2. Here is a list```console$ ls -aR ./node_modules/@tensorflow/tfjs-node.                          DEVELOPMENT.md             binding                    dist                       package.json               tsconfig.json..                         README.md                  binding.gyp                lib                        scripts.nycrc                     WINDOWS_TROUBLESHOOTING.md deps                       node_modules               src./node_modules/@tensorflow/tfjs-node/binding:.                napi_auto_ref.h  tf_auto_tensor.h tfjs_backend.cc  tfjs_binding.cc..               tf_auto_status.h tfe_auto_op.h    tfjs_backend.h   utils.h./node_modules/@tensorflow/tfjs-node/deps:.                         ..                        LICENSE                   THIRD_PARTY_TF_C_LICENSES include                   lib./node_modules/@tensorflow/tfjs-node/deps/include:.          ..         tensorflow./node_modules/@tensorflow/tfjs-node/deps/include/tensorflow:.  .. c./node_modules/@tensorflow/tfjs-node/deps/include/tensorflow/c:.                    c_api.h              eager                tf_datatype.h        tf_tensor.h..                   c_api_experimental.h tf_attrtype.h        tf_status.h./node_modules/@tensorflow/tfjs-node/deps/include/tensorflow/c/eager:.       ..      c_api.h./node_modules/@tensorflow/tfjs-node/deps/lib:.                                    libtensorflow.1.15.0.dylib           libtensorflow.dylib                  libtensorflow_framework.1.dylib..                                   libtensorflow.1.dylib                libtensorflow_framework.1.15.0.dylib libtensorflow_framework.dylib./node_modules/@tensorflow/tfjs-node/dist:.                                 index.d.ts                        nodejs_kernel_backend.d.ts        tensorboard.js..                                index.js                          nodejs_kernel_backend.js          tensorboard.js.mapcallbacks.d.ts                    index.js.map                      nodejs_kernel_backend.js.map      tensorboard_test.d.tscallbacks.js                      index_test.d.ts                   nodejs_kernel_backend_test.d.ts   tensorboard_test.jscallbacks.js.map                  index_test.js                     nodejs_kernel_backend_test.js     tensorboard_test.js.mapcallbacks_test.d.ts               index_test.js.map                 nodejs_kernel_backend_test.js.map tfjs_binding.d.tscallbacks_test.js                 int64_tensors.d.ts                proto                             tfjs_binding.jscallbacks_test.js.map             int64_tensors.js                  run_tests.d.ts                    tfjs_binding.js.mapcanvas_test.d.ts                  int64_tensors.js.map              run_tests.js                      tfjs_binding_test.d.tscanvas_test.js                    int64_tensors_test.d.ts           run_tests.js.map                  tfjs_binding_test.jscanvas_test.js.map                int64_tensors_test.js             saved_model.d.ts                  tfjs_binding_test.js.mapimage.d.ts                        int64_tensors_test.js.map         saved_model.js                    version.d.tsimage.js                          io                                saved_model.js.map                version.jsimage.js.map                      kernels                           saved_model_test.d.ts             version.js.mapimage_test.d.ts                   node.d.ts                         saved_model_test.jsimage_test.js                     node.js                           saved_model_test.js.mapimage_test.js.map                 node.js.map                       tensorboard.d.ts./node_modules/@tensorflow/tfjs-node/dist/io:.                       file_system.js.map      index.d.ts              io_utils.js             io_utils_test.js.map    node_http_test.d.ts..                      file_system_test.d.ts   index.js                io_utils.js.map         node_http.d.ts          node_http_test.jsfile_system.d.ts        file_system_test.js     index.js.map            io_utils_test.d.ts      node_http.js            node_http_test.js.mapfile_system.js          file_system_test.js.map io_utils.d.ts           io_utils_test.js        node_http.js.map./node_modules/@tensorflow/tfjs-node/dist/kernels:.                             Softmax.js                    SquaredDifference.js          all_kernels.js                non_max_suppression_v5.js..                            Softmax.js.map                SquaredDifference.js.map      all_kernels.js.map            non_max_suppression_v5.js.mapSoftmax.d.ts                  SquaredDifference.d.ts        all_kernels.d.ts              non_max_suppression_v5.d.ts./node_modules/@tensorflow/tfjs-node/dist/proto:.         ..        api_pb.js./node_modules/@tensorflow/tfjs-node/lib:.       ..      napi-v5./node_modules/@tensorflow/tfjs-node/lib/napi-v5:.                 ..                tfjs_binding.node./node_modules/@tensorflow/tfjs-node/node_modules:.    ..   .bin./node_modules/@tensorflow/tfjs-node/node_modules/.bin:.            ..           node-pre-gyp rimraf./node_modules/@tensorflow/tfjs-node/scripts:.                                      build-and-upload-windows-addon.bat     ensure-cpu-gpu-packages-align.js       publish-npm-gpu.sh..                                     custom-binary.sample.json              get-addon-name.js                      resources.jsbuild-and-upload-addon.sh              deps-constants.js                      install.js                             test-ci.shbuild-and-upload-windows-addon-gpu.bat deps-stage.js                          print-full-package-host.js             test-ts-integration.sh./node_modules/@tensorflow/tfjs-node/src:.                        image.ts                 io                       nodejs_kernel_backend.ts saved_model.ts           version.ts..                       index.ts                 kernels                  proto                    tensorboard.tscallbacks.ts             int64_tensors.ts         node.ts                  run_tests.ts             tfjs_binding.ts./node_modules/@tensorflow/tfjs-node/src/io:.              ..             file_system.ts index.ts       io_utils.ts    node_http.ts./node_modules/@tensorflow/tfjs-node/src/kernels:.                         ..                        Softmax.ts                SquaredDifference.ts      all_kernels.ts            non_max_suppression_v5.ts./node_modules/@tensorflow/tfjs-node/src/proto:.         ..        api.proto api_pb.js```=====']",0
https://github.com/tensorflow/tfjs/issues/4418,tfjs 2.8.0 is broken and introduces regressions in tf.image.cropAndResize,10,closed,2020-12-16T19:29:47Z,2020-12-18T02:02:27Z,"As subject line says, TFJS 2.8.0 unfortunately seems like a broken version.First, default parameter in `cropAndResize` was unintentionally removed (and apparently just re-added via #4407) which causes quite a lot of errors in existing appsSecond, even when specifying resize method as `bilinear`, `cropAndResize` FAILS on `WebGL` baclend:```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.ts:103)    at GPGPUContext.createProgram (gpgpu_context.ts:280)    at compileProgram (gpgpu_math.ts:93)    at backend_webgl.ts:858    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:902)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:857)    at Object.cropAndResize3 [as kernelFunc] (CropAndResize.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)```With WebGL code dump highlighting error in```204 setOutput(float(undefined)),```IMO, this is is blocking bug - once fixed, TFJS packages should be republished!(and I'm not sure how this passed any level of testing?)Btw, same code confirmed working after downgrade to tfjs 2.7.0.","['cc @annxingyuan @lina128 for visibility - sorry for spam, but this is bad=====', 'cc @tafsiri =====', ""Hi @vladmandic - thanks for reporting this issue - just wanted to let you know that you can avoid the fragment shader compile error by passing both `method` and `extrapolationValue` into `cropAndResize`, e.g.: `tf.image.cropAndResize(image, boxes, boxInd, [5,5], 'bilinear', 0 /* extrapolation value */) // note the last argument` ====="", ""@annxingyuan thanks for that - but I have `cropAndResize` all over my code, I'll just wait until this is fixed and tfjs is republished - in the meantime i'm sticking with tfjs 2.7.0.====="", 'Hi @vladmandic, we are going to release the fix in 2.8.1 very soon.=====', ""Since `2.8.0` is tagged as `latest`, the whole package looks deprecated now, see:https://www.npmjs.com/package/@tensorflow/tfjsI'd recommend to tag `2.7.0` as `latest` until you have a fixed version to remove the warning from the package npm page:```npm dist-tag add @tensorflow/tfjs@2.7.0 latest```====="", ""Thanks for the tip @mgol! We've tagged 2.7.0 as the latest npm version for all packages that were in the 2.8.0 release. ====="", '2.8.1 is released. It should fix the bug.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">No</a>=====', 'tfjs 2.8.1 fixes this issue, but exposes another issue in #4429=====']",1
https://github.com/tensorflow/tfjs/issues/2586,Firebase and Tensorflow-automl not working together,4,closed,2019-12-19T02:44:25Z,2020-01-23T22:23:16Z,"Hello everybody. I have a very strange issue.When I use tfjs-node without firebase, it works very well. I can get my predicts with online image links. But when I import firebase and after sign-in (only sign-in, not using) I get `TypeError: response.arrayBuffer is not a function` that error. I can not load my model.index.js```firebase.auth().signInWithEmailAndPassword('email', 'pass').then(() => {  console.log('signed')  notConfirmedFortunes.findNotConfirmedFortunes(firebase),})``````const automl = require('@tensorflow/tfjs-automl'),var firebase = require('firebase'),let model = null,require('@tensorflow/tfjs-node'),async function loadModel() {    model = await automl.loadImageClassification('https://xxxxxx.com/graph/model.json'),}async function findNotConfirmedFortunes() {    if(model == null) {        setTimeout(() => {            loadModel(),            findNotConfirmedFortunes()        }, 15000)    }    else {        // firebase.database().ref(`***/**path to database**/*****/`)        // .on('child_added', function(snapshot) {         //     predictImages.predict(snapshot.val(), model)        // })    }}module.exports={    findNotConfirmedFortunes}findNotConfirmedFortunes()```Here is my envoriment,```{  ""name"": ""confirmation"",  ""version"": ""1.0.0"",  ""description"": """",  ""main"": ""index.js"",  ""scripts"": {    ""test"": ""echo \""Error: no test specified\"" && exit 1""  },  ""author"": """",  ""license"": ""ISC"",  ""dependencies"": {    ""@tensorflow/tfjs-automl"": ""^1.0.0"",    ""@tensorflow/tfjs-converter"": ""^1.4.0"",    ""@tensorflow/tfjs-node"": ""^1.4.0"",    ""firebase"": ""^7.5.0"",    ""jpeg-js"": ""^0.3.4"",    ""xmlhttprequest"": ""^1.8.0""  }}```","['```(node:21613) UnhandledPromiseRejectionWarning: TypeError: response.arrayBuffer is not a function    at /Users/yagizozandikbas/Desktop/confirmation/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js:97:90    at Array.map (<anonymous>)    at Object.<anonymous> (/Users/yagizozandikbas/Desktop/confirmation/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js:97:48)    at step (/Users/yagizozandikbas/Desktop/confirmation/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js:48:23)    at Object.next (/Users/yagizozandikbas/Desktop/confirmation/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js:29:53)    at fulfilled (/Users/yagizozandikbas/Desktop/confirmation/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js:20:58)    at process._tickCallback (internal/process/next_tick.js:68:7)(node:21613) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 7)```=====', '@ozican looks like `node-fetch` is disabled in firebase environment. Can you try to call `tf.fetch(modelUrl)` (https://js.tensorflow.org/api/latest/#fetch) in firebase and check the response?=====', ""you could add this to your Firebase Functions index file:`global.fetch = require('node-fetch'),`====="", 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/5536,webgpu: fromPixels for HTMLVideoElement related cases fail to execute 'importExternalTexture',1,open,2021-08-26T08:17:18Z,2021-08-26T08:28:21Z,"Run `yarn test` under tfjs-backend-webgpu.Expect all cases pass. However, below cases fail:```FAILED fromPixels test-webgpu {""WEBGPU_CPU_FORWARD"":false} fromPixels for HTMLVideoElementdebug.js:21 Error: Failed to execute 'importExternalTexture' on 'GPUDevice': Failed to import texture from videoerror properties: Object({ code: 0, INDEX_SIZE_ERR: 1, DOMSTRING_SIZE_ERR: 2, HIERARCHY_REQUEST_ERR: 3, WRONG_DOCUMENT_ERR: 4, INVALID_CHARACTER_ERR: 5, NO_DATA_ALLOWED_ERR: 6, NO_MODIFICATION_ALLOWED_ERR: 7, NOT_FOUND_ERR: 8, NOT_SUPPORTED_ERR: 9, INUSE_ATTRIBUTE_ERR: 10, INVALID_STATE_ERR: 11, SYNTAX_ERR: 12, INVALID_MODIFICATION_ERR: 13, NAMESPACE_ERR: 14, INVALID_ACCESS_ERR: 15, VALIDATION_ERR: 16, TYPE_MISMATCH_ERR: 17, SECURITY_ERR: 18, NETWORK_ERR: 19, ABORT_ERR: 20, URL_MISMATCH_ERR: 21, QUOTA_EXCEEDED_ERR: 22, TIMEOUT_ERR: 23, INVALID_NODE_TYPE_ERR: 24, DATA_CLONE_ERR: 25 })    at Object.fromPixelsExternalImage (FromPixelsExternalImage.ts:84)    at Object.fromPixels [as kernelFunc] (FromPixels.ts:65)```However, if I change flag `WEBGPU_USE_IMPORT` to false, above case can pass. It seems that the import path has   problem.cc @shaoboyan ","[""@qjia7  I'll take this=====""]",1
https://github.com/tensorflow/tfjs/issues/5575,Codelab on Tensorflow js,1,closed,2021-09-02T14:56:39Z,2021-09-02T18:46:56Z,"https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#2Has references to link to tfjs-node is deprecated - this codelab needs to be updated_Install Node.js and npm. For supported platforms and dependencies, please see the tfjs-node installation guide._","['Submitted the change internally , will be updated once the change is merged. Thank you so much=====']",1
https://github.com/tensorflow/tfjs/issues/2805,Cannot read images from Tensorflow.js Tensorcamera on React Native,9,closed,2020-02-28T07:46:34Z,2020-03-04T14:49:48Z,"I am trying to use Tensorflow.js posenet on React Native but I can not get 'images'.Currently Expo-camera works ok, but images is undefined.Please advise how to iterate getting images from TensorCamera.Thanks.```const TensorCamera = cameraWithTensors(Camera),export default class MyComponent extends React.Component {  constructor(props) {    super(props),    this.handleCameraStream = this.handleCameraStream.bind(this),  }  async estimatePoseOnImage(imageElement) {    const net = await posenet.load(),    const pose = await net.estimateSinglePose(imageElement, {      flipHorizontal: false,    }),    this.setState({ pose: pose }),    return pose,  }  handleCameraStream( images ) {    const loop = async () => {      const nextImageTensor = await images.next().value,      await this.estimatePoseOnImage(nextImageTensor),      requestAnimationFrame(loop),    },    loop(),  }  render() {    return (      <View>        <TensorCamera          type={Camera.Constants.Type.front}          onReady={this.handleCameraStream}          autorender={true}        />      </View>    ),  }}```","[""I'm trying to follow this demo => https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsx, but none of the models' load. I'm using expo to make the test.  @rthadur @tafsiri ====="", '@fergalindez Thanks, Yes I am also following cameraWithTensors on  https://js.tensorflow.org/api_react_native/latest/#Media-Camera . And My code is JavaScript.=====', ""@tafsiri I just changed from `this.setState({ pose: pose }),` to `this.setState({ pose }),` and run it. This this.setState lead to render block and shows this.> ReferenceError: Can't find variable: React The 1st loop gets 'images' and const 'nextImageTensor' is defined, but const 'nextImageTensor' is undefined and cannot run 'estimatePoseOnImage' after the 2nd loop, and shows the above error. And do I need to use iterater and tf.tensor3d for the data from camera before posenet?  Thanks!====="", ""@fergalindez Please make a new issue describing the problems/errors you are running into. @sauceishere Could you post a reproduction somewhere on github where I could take a look and try and reproduce. The React reference not found seems like it could be a project setup issue, but it's hard to tell whats going on. If `images` is defined then you can check if the [iterator is done](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators) or not (it will be done/completed if the component is unmounted). ====="", 'Also could you post what version of Expo you are using.=====', '@tafsiri  Thanks very much for the feedback. My code is here https://github.com/sauceishere/tensorcamerajs. and expo version is  3.11.7. `images` is defined only for the 1st loop, and will be undefined later.=====', ""@sauceishere Could you try changing your [react import](https://github.com/sauceishere/tensorcamerajs/blob/master/App.js#L1) to `import * as React from 'react',` and having your main component extend React.Component. Newer versions of expo use a different import format that what is used in the integration_test app.If that doesn't work then from looking at the error stack it seemed to be coming from react-native-svg, so I would first remove all the things from the app other than the model loading and inference and make sure that works before introducing rendering.====="", '@tafsiri Thanks. I modified the code according to your advise and it works!! By the way, I saw your gif on this page https://blog.tensorflow.org/2020/02/tensorflowjs-for-react-native-is-here.html , do you run it on Android or iOS? I just want to know because my code does not render smoothly as yours.Thanks again for your great help.=====', 'Great! That GIF was produced on Android on a Pixel 3.=====']",0
https://github.com/tensorflow/tfjs/issues/1945,Frozen Model conversion issues,4,closed,2019-08-30T03:36:47Z,2021-02-13T16:26:34Z,"When converting Frozen Model, a pb file and a JSON file and a group1-shard1of1 file will be generated. Is this normal?The format in the JSON file is like this```json[  {    ""path"": [],    ""weights"": []  }]```tensorflowjs 0.8.6","['Yes that is expected given that version of TensorFlow.js you are using. Newer versions do not produce the pb file.=====', ""To converter a frozen model and be compatible with the latest tfjs version,use the following steps:7. I have a model formatted as a Session bundle or Frozen model. How do Iconvert it to TensorFlow.js?You can install a previous version of TensorFlow.js in a virtualenvironment to convert the model to the JSON format. Here is how you canachieve this.   - Set up the virtual environment:virtualenv --no-site-packages venv. venv/bin/activatepip install tensorflowjs==0.8.6venv is the name of the virtual environment.   - Convert a session bundle model:tensorflowjs_converter \\    --input_format=tf_session_bundle \\    --output_json=true \\    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\    /mobilenet/session_bundle \\    /mobilenet/web_model   - Convert a frozen model:tensorflowjs_converter \\    --input_format=tf_frozen_model \\    --output_json=true \\    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\    --saved_model_tags=serve \\    /mobilenet/frozen_model.pb \\    /mobilenet/web_modelOn Mon, Sep 2, 2019 at 11:23 AM 曲智超 <notifications@github.com> wrote:> @tafsiri <https://github.com/tafsiri> So how to use them?>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/1945?email_source=notifications&email_token=AAAA7MSNTDGYDHD7VR2IAS3QHSBJPA5CNFSM4ISI4CKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5UTFXQ#issuecomment-526987998>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AAAA7MV6YW4J44VOSSVKL3LQHSBJPANCNFSM4ISI4CKA>> .>====="", 'For Converting Frozen_inference_model.pb to tensroflowjs(model.json) : Please Refer this Link : https://github.com/tensorflow/tfjs-models/blob/master/deeplab/scripts/convert_deeplab.sh=====', ""> For Converting Frozen_inference_model.pb to tensroflowjs(model.json) : Please Refer this Link : https://github.com/tensorflow/tfjs-models/blob/master/deeplab/scripts/convert_deeplab.shtensorflowjs_converter \\    --input_format=tf_frozen_model \\    --output_json=true \\    --saved_model_tags=serve \\    --output_node_names='SemanticPredictions' \\    --quantization_bytes 2 \\    frozen_inference_graph.pb \\    $OUTPUT_DIR=====""]",0
https://github.com/tensorflow/tfjs/issues/5488,Replace deprecated `setWasmPath` with `setWasmPaths`,3,closed,2021-08-13T14:03:16Z,2021-08-13T15:10:18Z,Bad way (only uses `tfjs-backend-wasm.wasm`): https://github.com/tensorflow/tfjs-models/blob/8faa92d8933d8036deae63b034db399c09173a2e/facemesh/demo/index.js#L27Good way (uses `tfjs-backend-wasm-simd.wasm` or `tfjs-backend-wasm-threaded-simd.wasm` if possible):https://github.com/tensorflow/tfjs-models/blob/8faa92d8933d8036deae63b034db399c09173a2e/face-landmarks-detection/demo/index.js#L28,"['Hi @kungfooman `facemesh` is no longer supported , please refer to `face-landmarks-detection `. Thank you ![image](https://user-images.githubusercontent.com/43972606/129371594-f0bd395e-b5e7-45d6-ab4b-4d8279ed8536.png)=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5488"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5488"">No</a>=====', 'Well, too bad then, `facemesh` detects my face better than `face-landmarks-detection`.=====']",1
https://github.com/tensorflow/tfjs/issues/5145,Model using `WebGL` backend returns nonsense results while working fine with other backends,2,closed,2021-05-30T15:45:38Z,2021-05-31T11:04:18Z,"I have a custom MobileNet-v3/CenterNet model trained on COCO dataset at <https://github.com/vladmandic/mb3-centernet>  and it works perfectly using `tfjs-node` in NodeJS or using `tfjs-backend-cpu` in browser  However, when using `tfjs-backend-webgl` in browser, results are **all fixed-value nonsense**  Input is `[1, 512, 512, 3]`, using pixel range 0..255Output is array of `[x1, y1, x2, y2, score, class]` pre-sorted by score  Example results when using `tfjs-node` or `tfjs-backend-cpu`: There are different boxes marked around different classes```js[  [46.219512939453125, 111.64230346679688, 205.2839813232422, 507.42388916015625, 0.4410598874092102, 0],  [111.56916809082031, 245.57809448242188, 166.20912170410156, 411.6197814941406, 0.35047441720962524, 27],  [75.27127075195312, 411.9134521484375, 110.96424865722656, 507.50360107421875, 0.31512993574142456, 40],  ...]```Example result for the same input when using `tfjs-backend-webgl`:All of the boxes and classes are fixed values!```js[  [174.25, 2.09375, 228.125, 58.3125, 0.6572265625, 64],  [174.25, 2.09375, 228.125, 58.3125, 0.53369140625, 64],  [174.25, 2.09375, 228.125, 58.3125, 0.44091796875, 64],  ...]```Just as a test, I've tried with different input normalizations as well as different versions of weights quantizations - no changes.I cannot test with `tfjs-backend-wasm` due to missing `Mod` op, see #5110  But given it works with `tfjs-backend-cpu`, it's definitely an issue with `webgl` backend  Environment: TFJS 3.6.0 on Ubuntu 20.04 and Chrome 91","[""my fault, issue was due to model clipping - i forgot i had `tf.ENV.set('WEBGL_FORCE_F16_TEXTURES', true)` set globally.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5145"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5145"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5265,Problems with running script code in tflite,5,closed,2021-06-30T08:31:59Z,2021-07-29T06:16:20Z,"##### When I run the script code in the tflite README, an error like this appears in the chrome console:![ksnip_20210630-162655](https://user-images.githubusercontent.com/41098760/123928042-0e316b00-d9c0-11eb-9f7d-599df35f04ab.png)##### The code in my html:- https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite#via-a-script-tag```html    <!-- Adds the CPU backend -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>    <!-- Import @tensorflow/tfjs-core -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>    <!--      Import @tensorflow/tfjs-tflite      Note that we need to explicitly load dist/tf-tflite.min.js so that it can      locate WASM module files from their default location (dist/).    -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.min.js""></script>```","['@Zengyf-CVer are you using VPN ? is there a restriction for google links ?=====', '> @Zengyf-CVer are you using VPN ? is there a restriction for google links ?##### Google link is normal, I run the following script is no problem：```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.min.js""></script>```##### But when running the following two scripts, there is a problem that the link cannot be found：```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script> ```##### A question that confuses me, why is there` .map` behind the link?![ksnip_20210701-075135](https://user-images.githubusercontent.com/41098760/124044962-3ca35a80-da41-11eb-8c3a-59487e08a358.png)=====', 'In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!=====', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5265"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5265"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5851,[wasm] Node tests fail with out of memory error,3,closed,2021-11-15T18:10:06Z,2021-11-16T19:01:13Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 5.10.46- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): main branch (e34d7c3b748173f685c9ec3cf549709534a03a07)- TensorFlow.js version (use command below):- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**Running `yarn test-node` in tfjs-backend-wasm throws `RangeError: WebAssembly.instantiate(): Out of memory: wasm memory`.**Describe the expected behavior**Tests pass with no out-of-memory error.**Standalone code to reproduce the issue**In tfjs-backend-wasm, run `yarn && yarn build-deps && yarn build-npm && yarn test-node`. I've also observed that this bug only appears on some machines.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.This memory leak is likely in the setup / teardown of the backend that happens every time we use `describeWithFlags`. I created a fake test that just runs `describeWithFlags` 1000 times, and it consistently fails after running tests from 101 `describeWithFlags` calls.","['This appears to be caused by the wasm-generated code subscribing to the `uncaughtException` event, which prevents GC across backend re-instantiations.=====', 'https://github.com/emscripten-core/emscripten/issues/12740=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5851"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5851"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5948,fromPixels for HTMLVideoElement is slow in Firefox and Safari with WebGL backend,1,open,2021-12-15T18:59:38Z,2021-12-15T19:08:19Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur, running on Intel i7 Mac Mini (2018), Intel UHD Graphics 630- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.12.0- Browser version: Firefox 95, Safari 14.1.1- Tensorflow.js Converter Version: N/A**Describe the current behavior**Currently when using `tf.browser.fromPixels` on a `HTMLVideoElement` with an mp4 source, this method takes a long time on the WebGL backend (around 7 ms on my machine). According to profiling information from Firefox, most of the time is spent on copying the video data to a temporary canvas. When I disabled this temporary canvas in a custom fork tfjs and sent `HTMLVideoElement` directly to `backend.gpgpu.uploadPixelDataToTexture`, `fromPixels` does not even register in the profiler anymore and overall FPS goes up from 22 to 40 for my use case. I observe the same behavior in Safari, Chrome does not suffer from this issue.**Describe the expected behavior**fromPixels does not take 7ms to complete.**Standalone code to reproduce the issue**Our team is hitting this problem with mocap4face (https://github.com/facemoji/mocap4face/tree/main/js-example), you can also check a running demo at https://facemoji.co/mocap4face/ and run the Firefox profiling there to see the issue (make sure to check Show Gecko Platform Data in the Profiler settings to better see the canvas copy).**Other info / logs**The issue seems to be gone when I comment out [lines 58 to 69 in tfjs-backend-webgl/src/kernels/FromPixels.ts](https://github.com/tensorflow/tfjs/blob/20baee0ac6a39da914c659526bdc67709d700f9d/tfjs-backend-webgl/src/kernels/FromPixels.ts#L58-L69) (the `if (isImage || isVideo) { ... }` ).Note that the code seems to work fine without this extra condition in all the browsers I tested (even Windows and Mobile ones).",['related issue https://github.com/tensorflow/tfjs/issues/5947 ====='],1
https://github.com/tensorflow/tfjs/issues/5413,Support for `ReverseSequence` in `tensorflowjs_converter`,0,open,2021-08-01T16:03:14Z,2021-08-02T20:12:21Z,"Hi, I'm getting following error when trying to use `tensorflowjs_converter` to convert a Bidirectional LSTM-CRF model that uses the [CRF from TensorFlow addons](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/CRF):Command:```tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model <tf_model> <tfjs_model>```Output:```tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 154, in optimize_graph    ', '.join(unsupported))ValueError: Unsupported Ops in the model before optimizationReverseSequence```Can `tensorflowjs_converter` please support this operation? Thanks!",[],0
https://github.com/tensorflow/tfjs/issues/4524,tfjs-backend-wasm-threaded-simd.wasm is not found starting from 2.8.1,2,closed,2021-01-12T20:18:40Z,2021-01-14T19:38:35Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below): 2.8.1- Browser version:- Tensorflow.js Converter Version: 2.8.1**Describe the current behavior**Reproduce steps:1. Turn on simd flag by going to chrome://flag and enable simd option2. Run this simple codepen example: https://codepen.io/na-li/pen/XWXqgdM3. Error message in developer console: Initialization of backend wasm failed. RuntimeError: abort(both async and sync fetching of the wasm failed). Build with -s ASSERTIONS=1 for more info.**Describe the expected behavior**Expect simd wasm to load successfully. Change above codepen's dependency from 2.8.1 to 2.8.0 to see the normal behavior.For reference, diff between 2.8.0 and 2.8.1: https://github.com/tensorflow/tfjs/releases/tag/tfjs-v2.8.1","['https://github.com/tensorflow/tfjs/pull/4528 fixed it, it has been released in 2.8.4.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4524"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4524"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1565,how to consume complex/zipped dataset with model.fitDataset?,1,closed,2019-05-02T07:30:09Z,2019-05-02T15:17:04Z,"It seems the Dataset can be composed and have arbitrary data structure.Dataset can be zipped. how to consume a zipped Dataset?If we call model.fitDataset, inside the model, how can we consume the dataset and walk through the structure and override which method toconsume it?I find the model.fit take tensor arguments which looks like simpler.But I am just wondering if it can do lazy data input. I assume Dataset is designedto be able to do lazy data input.George Wu","[""Hello, this sort of question is better suited to StackOverflow.  This issues list is for bugs and feature requests.  Thanks and please use the tag 'tensorflow.js' on your StackOverflow question to makes sure it's discoverable.=====""]",0
https://github.com/tensorflow/tfjs/issues/721,tf.div presumably using too much memory space and leaking memory,1,closed,2018-09-21T09:29:42Z,2019-10-15T00:53:15Z,"#### TensorFlow.js version0.13.0#### Browser versionGoogle Chrome  69.0.3497.100  64 bits#### Describe the problem or feature requestI am trying to load MNIST training images dataset.The raw data is constituted of 47 040 000 (60 000 * 28 * 28) integers representing grey scale values between 0 and 255. My goal is to reshape this into a 4D tensor (60k * 28 * 28 * 1) and scale the values between 0 and 1.Whenever I do a simple For loop through all 47 040 000 integers to divide them individually in the raw data array, the memory usage increases by something like 100 to 400 Mo. Whenever I try to divide the 4D tensor by a scalar using the function tf.div, I get several Go of ram used.I hadn't the problem using the 0.10.0 version of tensorflow few months ago.Am I doing something wrong?#### Code to reproduce the bug / link to feature request```javascript        // data_raw is of size 47 040 000	return tf.tidy(function(){	        for(var i=0,i<data_raw.length,i++)		        data_raw[i] = data_raw[i]/256.0,	        return tf.tensor4d( data_raw, [60000, 28, 28, 1] ),         }``````javascript        // data_raw is of size 47 040 000	return tf.tidy(function(){		var a = tf.tensor4d( data_raw, [60000, 28, 28, 1] ),		return a.div( tf.scalar(256) ),	}),```","['Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/4129,[webgl] Investigate how to reduce memory usage from texture allocation with minimal performance impact.,9,open,2020-10-25T19:28:03Z,2020-12-08T17:15:20Z,"Hi,I've been using the Universal Sentence Encoders model and had a hard to track memory leak. I eventually found [this comment](https://github.com/tensorflow/tfjs/issues/4015#issuecomment-704335135)  which suggested setting ```jstf.env().set(""WEBGL_DELETE_TEXTURE_THRESHOLD"", 0),```Which solved the issue. I don't know what it did but it worked and I suggest documenting it more prominently. I'd volunteer to do so myself but don't understand what this does so leaving it as a suggestionThanks !","['```// set this flag so that textures are deleted when tensors are disposed.tf.env().set(""WEBGL_DELETE_TEXTURE_THRESHOLD"", 0),```=====', '> ```> // set this flag so that textures are deleted when tensors are disposed.> tf.env().set(""WEBGL_DELETE_TEXTURE_THRESHOLD"", 0),> ```I\'m not sure of that was for my sake, but if it was I (and similar users) don\'t understand it. I\'m just a simple NLP guy, I know some tensorflow and am fighting my way in the JS world to make things works. I (and people like me), want to use this lib, but have no idea what a texture is.More practically, it would be cool if it said somewhere: ""Hey, if you see your script using 10G of ram, you might have a memory leak. Try this because WebGL allocates textures but doesn\'t release them by default ... "" Hope that\'s helpful=====', 'I think a goo place to do this would be in the README for Universal Sentence Encoder. @talolard if you were interested in making a PR for that that would be great. Else we can do that. =====', 'cc @annxingyuan =====', ""> I think a goo place to do this would be in the README for Universal Sentence Encoder. @talolard if you were interested in making a PR for that that would be great. Else we can do that.I'd be happy to. To make the thing compelte, can you point me to why it's a universal encoders thing and a general tf.js issue ? e.g. If I was building my own model it wouldn't be an issue ? ====="", ""@talolard Changing the default behavior of `WEBGL_DELETE_TEXTURE_THRESHOLD` is hopefully more of a last resort for our users, so I think that's why we'd want to document this closer to the specific use cases. ====="", ""Thanks @annxingyuan Can you shed some light on what it does and why it's specifc to Universal Sentence Encoders. I'd love to submit a PR with the relevant information but don't feel like I know what I'm talking about yet. Tnx====="", 'not intending to hijack this thread, but `WEBGL_DELETE_TEXTURE_THRESHOLD` is pretty much required for execution of any complex model due to TFJS extremely high WebGL memory usage. On the other hand, it has a really bad performance impact - it slows down subsequent inferences by 2-3x.The goal should be to optimize TFJS not to use 10x memory with `backend-webgl` compared to other backends.See this one for more details: <https://github.com/tensorflow/tfjs/issues/4166>=====', ""@vladmandic Yes you're right - adjusting `WEBGL_DELETE_TEXTURE_THRESHOLD` does not address the underlying issue, which is why I'm hesitant to encourage people to rely on it, although I understand that it can improve performance in certain cases. We need to figure out how to reduce the memory footprint while minimizing the cost of aggressive texture deletion.I've changed the title of this issue to reflect.=====""]",0
https://github.com/tensorflow/tfjs/issues/4883,Align NaN propagation behavior with TF for Max/Min ops,1,closed,2021-03-29T20:48:12Z,2021-05-04T03:31:35Z,"Since TensorFlow 2.4.0, the NaN will always propagated for Max/Min ops, we need to align TFJS with TF.```>>> tf.__version__'2.4.1'>>> x = tf.constant([2, float(""nan""), 2], dtype=tf.float32)2021-03-29 10:24:35.228622: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set2021-03-29 10:24:35.229125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.>>> a = tf.raw_ops.Max(input=x, axis=0)>>> a<tf.Tensor: shape=(), dtype=float32, numpy=nan>```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4883"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4883"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1160,Tutorial: Importing a Keras Model,1,closed,2019-01-28T21:39:30Z,2019-07-09T19:57:05Z,- Importing Python Models,['Link to doc https://github.com/tensorflow/tfjs-website/blob/devsite/docs/tutorials/conversion/import-keras.md====='],0
https://github.com/tensorflow/tfjs/issues/67,Add support for additional random tensors,2,closed,2018-04-02T20:01:09Z,2020-03-06T21:26:46Z,"_From @jgartman on March 28, 2018 1:17_TensorFlow.js currently supports tensors of random samples drawn from normal, truncated normal or uniform distributions and it looks like multinomial sampling is being added back to the api soon.  Tensorflow’s random_ops supports all of these as well as the [poisson](https://www.tensorflow.org/api_docs/python/tf/random_gamma), and [gamma](https://www.tensorflow.org/api_docs/python/tf/random_poisson) distributions.  I was wondering if it would be worthwhile to support these in TensorFlow.js as well?Thanks_Copied from original issue: tensorflow/tfjs-core#903_","[""I'll give this a shot====="", 'This has been fixed [here](https://github.com/tensorflow/tfjs-core/pull/1365/files) , so closing this issue.Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/1578,Slowness in copying the input array buffer before uploading to GPU,3,closed,2019-05-10T18:22:56Z,2019-06-10T21:59:57Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.1.0#### Browser versionWeChat mini app#### Describe the problem or feature requestboth uploadMatrixToTexture and uploadMatrixToPackedTexture methods make a copy of the input arraybuffer in CPU, which is very slow in WeChat mini app.While fromPixels directly upload the arraybuffer and use shader program to manipulate the data.We should do the same for other uploadToGPU path.#### Code to reproduce the bug / link to feature request","['@annxingyuan do you have a sense for how hard this would be (e.g. uploading whatever the user gives raw and then running a shader for packing)?=====', ""@nsthorat it feels like that would be straightforward - I'll sync with Ping on this today. ====="", 'Cool, I think the only other complication will be with texture types.=====']",0
https://github.com/tensorflow/tfjs/issues/4531,Why can not convert tfjs-graph-model to keras?,4,closed,2021-01-14T08:18:55Z,2021-01-16T02:29:26Z,"I have a tfjs graph model, it can only be loaded by tfjs. and can not convert to h5 or even tfjs-layer-model. How can I use it in keras?","['@qiankunxienb unfortunately graph model cannot be converted to h5 or tfjs layers model, the reason is graph model is inference only model, which h5 and layers model support training.If you have the original TensorFlow TF saved model that graph model is converted from, you can try to convert that to layers or h5 models.=====', '@pyu10055 the problem is that, I want to use your model in https://nsfwjs.com/quant_mid/* , however, it can only be loaded by tfjs. How can I use it in keras?=====', '@qiankunxienb This model is not created by us, you can ask the author of the model to share the original TensorFlow model, it could be built from Keras. =====', 'Thanks for your reply.=====']",0
https://github.com/tensorflow/tfjs/issues/1655,Problems with loading model with quantization option,1,closed,2019-06-10T10:08:04Z,2020-06-05T05:41:09Z,"project: tfjs-converter#### Describe the problem or feature requestif input model of type ""js layers"" has quantization, weights loaded incorrect#### Steps to reproduce the bugconvert model with quantization, convert it back, compare weights in source file and double convertetd model file. They are differs.#### Code to fix issuein file ""quantization.py"" in function: dequantize_weights call to ""np.round"" must be removed","['Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/1782,tfjs-core npm contains experimental backends,0,closed,2019-07-29T17:27:53Z,2019-08-01T15:01:19Z,We need to add those directory to npm ignore,[],0
https://github.com/tensorflow/tfjs/issues/5398,[webgl] Bazel build includes tfjs-core,2,closed,2021-07-29T03:44:18Z,2021-08-03T18:27:37Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**bundle size of webgl backend increased 1MB due to including tfjs-core files, there maybe also has a risk that we use different version of tfjs-core in one APP.**Provide the exact sequence of commands / steps that you executed before running into the problem**`cd tfjs-backend-webgl && yarn && yarn build`**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.``` ls -lh ../dist/bin/tfjs-backend-webgl/dist/tf-backend-webgl.js-r--r--r-- 1 yf Domain Users 1.9M Jul 28 10:09 ../dist/bin/tfjs-backend-webgl/dist/tf-backend-webgl.js```","[""We likely need to mark tfjs-core as external in the tfjs_web_bundle rule. I'm OOO today, but I can take a look on Monday. Thanks for catching this!====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5398"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5398"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/307,Chaining is broken with round,3,closed,2018-05-18T01:47:59Z,2018-05-18T13:17:33Z,Issue shows up locally and with example code in the docshttps://js.tensorflow.org/api/0.10.0/#roundNOTE:`tf.round` works fine.,"['Confirming I can reproduce this bug on tfjs website at the URL provided by @nbardy =====', 'Cannot reproduce this error from HEAD. This was fixed in https://github.com/tensorflow/tfjs-core/pull/994=====', ""Thanks, @ManrajGrover ! I'll close this issue for now.=====""]",0
https://github.com/tensorflow/tfjs/issues/4171,Error when trying to use tfjs-node on Windows 10,16,closed,2020-11-03T00:17:40Z,2021-12-08T02:36:38Z,"In my `main.mjs` file I have `import * as tf from ""@tensorflow/tfjs-node"",`.I get this error when I try to run the file:    node:internal/modules/cjs/loader:1142      return process.dlopen(module, path.toNamespacedPath(filename)),                    ^    Error: The specified module could not be found.    \\?\D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\lib\napi-v6\tfjs_binding.node        at Object.Module._extensions..node (node:internal/modules/cjs/loader:1142:18)        at Module.load (node:internal/modules/cjs/loader:948:32)        at Function.Module._load (node:internal/modules/cjs/loader:789:14)        at Module.require (node:internal/modules/cjs/loader:972:19)        at require (node:internal/modules/cjs/helpers:88:18)        at Object.<anonymous> (D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\dist\index.js:58:16)        at Module._compile (node:internal/modules/cjs/loader:1083:30)        at Object.Module._extensions..js (node:internal/modules/cjs/loader:1112:10)        at Module.load (node:internal/modules/cjs/loader:948:32)        at Function.Module._load (node:internal/modules/cjs/loader:789:14) {      code: 'ERR_DLOPEN_FAILED'    }Here's the output of `node-gyp configure --verbose`:    gyp info it worked if it ends with ok    gyp verb cli [    gyp verb cli   'D:\\Program Files\\nodejs\\node.exe',    gyp verb cli   'D:\\Program Files\\nodejs\\node_modules\\node-gyp\\bin\\node-gyp.js',    gyp verb cli   'configure',    gyp verb cli   '--verbose'    gyp verb cli ]    gyp info using node-gyp@7.1.2    gyp info using node@15.0.1 | win32 | x64    gyp verb command configure []    gyp verb find Python Python is not set from command line or npm configuration    gyp verb find Python Python is not set from environment variable PYTHON          gyp verb find Python checking if ""python3"" can be used    gyp verb find Python - executing ""python3"" to get executable path    gyp verb find Python - ""python3"" is not in PATH or produced an error    gyp verb find Python checking if ""python"" can be used    gyp verb find Python - executing ""python"" to get executable path        gyp verb find Python - executable path is ""D:\Python27\python.exe""          gyp verb find Python - executing ""D:\Python27\python.exe"" to get version    gyp verb find Python - version is ""2.7.18""    gyp info find Python using Python version 2.7.18 found at ""D:\Python27\python.exe""    gyp verb get node dir no --target version specified, falling back to host node version: 15.0.1    gyp verb command install [ '15.0.1' ]    gyp verb install input version string ""15.0.1""    gyp verb install installing version: 15.0.1       gyp verb install --ensure was passed, so won't reinstall if already installed    gyp verb install version is already installed, need to check ""installVersion""    gyp verb got ""installVersion"" 9    gyp verb needs ""installVersion"" 9    gyp verb install version is good    gyp verb get node dir target node version installed: 15.0.1    gyp verb build dir attempting to create ""build"" dir: D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\build    gyp verb build dir ""build"" dir needed to be created? undefined    gyp verb find VS msvs_version not set from command line or npm config    gyp verb find VS VCINSTALLDIR not set, not running in VS Command Prompt    gyp verb find VS checking VS2019 (16.7.30621.155) found at:    gyp verb find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community""    gyp verb find VS - found ""Visual Studio C++ core features""    gyp verb find VS - found VC++ toolset: v142    gyp verb find VS - found Windows SDK: 10.0.18362.0    gyp info find VS using VS2019 (16.7.30621.155) found at:    gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community""    gyp info find VS run with --verbose for detailed information    gyp verb build/config.gypi creating config file    gyp verb build/config.gypi writing out config file: D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\build\config.gypi    gyp verb config.gypi checking for gypi file: D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\config.gypi    gyp verb common.gypi checking for gypi file: D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node\common.gypi    gyp verb gyp gyp format was not specified, forcing ""msvs""    gyp info spawn D:\Python27\python.exe    gyp info spawn args [    gyp info spawn args   'D:\\Users\\fried\\AppData\\Roaming\\nvm\\v15.0.1\\node_modules\\node-gyp\\gyp\\gyp_main.py',    gyp info spawn args   'binding.gyp',    gyp info spawn args   '-f',    gyp info spawn args   'msvs',    gyp info spawn args   '-I',    gyp info spawn args   'D:\\Users\\fried\\Documents\\VSCode Projects\\Ai+ML\\Mountain Car\\node_modules\\@tensorflow\\tfjs-node\\build\\config.gypi',    gyp info spawn args   '-I',    gyp info spawn args   'D:\\Users\\fried\\AppData\\Roaming\\nvm\\v15.0.1\\node_modules\\node-gyp\\addon.gypi',    gyp info spawn args   '-I',    gyp info spawn args   'C:\\Users\\fried\\AppData\\Local\\node-gyp\\Cache\\15.0.1\\include\\node\\common.gypi',    gyp info spawn args   '-Dlibrary=shared_library',    gyp info spawn args   '-Dvisibility=default',    gyp info spawn args   '-Dnode_root_dir=C:\\Users\\fried\\AppData\\Local\\node-gyp\\Cache\\15.0.1',    gyp info spawn args   '-Dnode_gyp_dir=D:\\Users\\fried\\AppData\\Roaming\\nvm\\v15.0.1\\node_modules\\node-gyp',    gyp info spawn args   '-Dnode_lib_file=C:\\\\Users\\\\fried\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\15.0.1\\\\<(target_arch)\\\\node.lib',     gyp info spawn args   '-Dmodule_root_dir=D:\\Users\\fried\\Documents\\VSCode Projects\\Ai+ML\\Mountain Car\\node_modules\\@tensorflow\\tfjs-node',    gyp info spawn args   '-Dnode_engine=v8',    gyp info spawn args   '--depth=.',    gyp info spawn args   '--no-parallel',    gyp info spawn args   '--generator-output',    gyp info spawn args   'D:\\Users\\fried\\Documents\\VSCode Projects\\Ai+ML\\Mountain Car\\node_modules\\@tensorflow\\tfjs-node\\build',            gyp info spawn args   '-Goutput_dir=.'    gyp info spawn args ]    gyp: Undefined variable module_name in binding.gyp while trying to load binding.gyp    gyp ERR! configure error    gyp ERR! stack Error: `gyp` failed with exit code: 1    gyp ERR! stack     at ChildProcess.onCpExit (D:\Users\fried\AppData\Roaming\nvm\v15.0.1\node_modules\node-gyp\lib\configure.js:351:16)    gyp ERR! stack     at ChildProcess.emit (node:events:327:20)    gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:277:12)    gyp ERR! System Windows_NT 10.0.19041    gyp ERR! command ""D:\\Program Files\\nodejs\\node.exe"" ""D:\\Program Files\\nodejs\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--verbose""    gyp ERR! cwd D:\Users\fried\Documents\VSCode Projects\Ai+ML\Mountain Car\node_modules\@tensorflow\tfjs-node    gyp ERR! node -v v15.0.1    gyp ERR! node-gyp -v v7.1.2    gyp ERR! not okAs you can see from the log I made sure that it was using Python 2 instead of Python 3, but it still isn't working. I've tried following the instructions [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md) as well, but to no avail.","['@Jwiggiff can you please check [here](https://github.com/tensorflow/tfjs/issues/2619) for a similar issue.=====', ""I read through this issue, however I did not find a solution. I don't know how similar these issues are though, since the OP in that issue received a completely different error it looks like. However, I did notice a few things. First of all, in `../node_modules/@tensorflow/tfjs-node/lib/napi-v6` it only contains one file, `tfjs_binding.node`. In that other issue, they mentioned something about a tensorflow.dll that is supposed to be in that folder. I don't know if that's changed since the version in the other issue though. I did run `npm install` in the `tfjs-node` directory but it still does not work.Also, here are my specs in case they are useful:Mobo: MSI Tomahawk B450 MAXCPU: AMD Ryzen 5 3600XGPU: GTX 1060 6gbOS: Windows 10 Pro v2004====="", '@Jwiggiff what version of node and tfjs-node you are using ? also can you run` npm rebuild @tensorflow/tfjs-node build-addon-from-source` in your repo and try again=====', 'Node: v15.0.1tfjs-node: 2.7.0Output of `npm rebuild @tensorflow/tfjs-node build-addon-from-source`:    > @tensorflow/tfjs-node@2.7.0 install D:\\Users\\fried\\Documents\\VSCode Projects\\Ai+ML\\Mountain     Car\\node_modules\\@tensorflow\\tfjs-node    > node scripts/install.js    CPU-windows-2.7.0.zip    * Building TensorFlow Node.js bindings    @tensorflow/tfjs-node@2.7.0 D:\\Users\\fried\\Documents\\VSCode Projects\\Ai+ML\\Mountain     Car\\node_modules\\@tensorflow\\tfjs-nodeI still get the same error when running `node main.mjs` after running this command=====', '1. module_name problem solves with other runner, usenode-pre-gyp rebuild2. You shoul manualy past files to node_modules/@tensorflow/tfjs-node/lib/napi-v6. Copy it from deps/lib=====', 'It kind of worked! So first, I installed `node-pre-gyp` globally, and then I ran `node-pre-gyp rebuild` in `node_modules/@tensorflow/tfjs-node`. I still got the same error when running `node main.mjs`. Then, I copied `tensorflow.dll` and `tensorflow.lib` from `deps/lib` to `lib/napi-v6`. Now I am getting this error:    tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not     compiled to use: AVX2=====', ""It's a warrning. Computation should work despite it====="", '@Jwiggiff is this resolved after you built from source manually?=====', ""I'm having the same issue after upgrading to Node 14. Everything was working fine with Node 12.As a workaround, copying the DLLs manually solves the issue.====="", ""yes @ASnow 's solution seemed to work. Thank you!====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4171"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4171"">No</a>=====', ""@Jwiggiff, @rthadur the issue shouldn't be closed on a workaround involving to manually move files from node_modules subdirectories.There's a problem with Node.js N-api that should be fixed on Windows because many other users will get it soon.The install script stops at napi v6 while recent Node.js versions use napi v7 (https://nodejs.org/api/n-api.html#n_api_n_api_version_matrix).dll file isn't linked properly because ``cp.exec('node scripts/deps-stage.js symlink ' + modulePath),`` in install.js generates ``running node scripts/deps-stage.js symlink ./lib/napi-v7`` while tfjs_binding.node is stored in lib/napi-v6.====="", 'cc @pyu10055 @tafsiri =====', 'I think this is not related to windows because I see this issue on- macOS 11.01- python 2.7.16- node 15.3.0- npm 7.0.14-   ""@tensorflow/tfjs"": ""^2.7.0"",-   ""@tensorflow/tfjs-node"": ""~2.7.0"",I tried `npm rebuild @tensorflow/tfjs-node build-addon-from-source` with no luck. **UPDATE:**Seems like the issue was the white space in the project path.  Something like `/MyFolder/My Name/` always failed.=====', 'Why does this get closed again? The issue is still persisting on multiple platforms and node versions.=====', ""I encounter the same problem and struggled for two days. After installing and compiling tfjs-node again and again, I still get the problem as below.![截屏2021-12-08 上午10 13 11](https://user-images.githubusercontent.com/30199027/145136229-9003c512-b88f-491f-a66b-bae889ad7c76.png) In the beginning, I think my windows compile tool isn't installed properly. Because there are many Error in the output of the command `node-gyp configure --verbose`. After another failed attempt, I notice that the error comes from the miss of the `tfjs_binging.node` file in the `\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v7\\` folder. Then, I check this location and find there are only a `\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\` folder. Suddenly, I realize that may be the true problem. So, I change the folder name from `napi-v8` to `napi-v7` and my program finally works after compiled again. Now, I think this is a compatibility issue. On my MAC, I have the node v14.16.0 and there is a `napi-v7` folder, so the tfjs-node always works. On my Windows computer, the node version is V14.17.3 and there is only a  `napi-v8` folder. =====""]",0
https://github.com/tensorflow/tfjs/issues/5216,Safari 15 / iOS 15 Vertex/Shader Errors,6,closed,2021-06-12T16:15:36Z,2021-07-14T18:11:11Z,"DESCRIPTIONI'm getting a consistent vertex/fragment error across the official TFJS demos using Safari 15 on the new developer release of iOS 15.0. It would seem changes have been introduced into Webkit that are causing this, so hopefully shouldn't require any changes by the tensorflow team, but I did want to raise it here.ERROR LOG:Internal error compiling shader with Metal backend.Please submit this shader, or website as a bug to https://bugs.webkit.orgError: Failed to link vertex and fragment shaders.STEPS TO REPRODUCE:The error can be seen running these demos on Safari 15:[https://emojiscavengerhunt.withgoogle.com/](https://emojiscavengerhunt.withgoogle.com/)[https://magenta.tensorflow.org/demos/performance_rnn/index.html](https://magenta.tensorflow.org/demos/performance_rnn/index.html)[https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html](https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html)HARDWARE:I am using an iPhone XS running iOS 15. I tested these on a iPhone X running 14.6 and had no errors.REPORTING:I filed a bug report with WebKit too, but this may be something a team member may be better positioned to follow up on as it seems to be breaking pretty consistently across my own code and the official TFJS demos:[https://bugs.webkit.org/show_bug.cgi?id=226953](https://bugs.webkit.org/show_bug.cgi?id=226953)","['@heystoney can you paste the details of this failure? We usually show the shader source code and where the compilation fails.You might need to use safari remote debug to get the error message. thanks.=====', ""For sure. So using [this one](https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html) as example (but the error is the same in each) in Safari 15:Before erroring it will log:**Internal error compiling shader with Metal backend. (webgl_util.ts:154)Please submit this shader, or website as a bug to https://bugs.webkit.org**The console error that follows is:**Unhandled Promise Rejection: Error: Failed to link vertex and fragment shaders.**The error always occurs when tfjs is running its training (or segmentation for bodypix). In this case, the line that triggers the error is:`await demo.train(trainIterations, batchSize, numTestExamples),`That's all I get in my logs, so I'm not sure if there is an additional debug mode these could be run in with more information, but hopefully that helps. I had also attempted to run this page as well:https://js.tensorflow.org/debug/It didn't output any tf.ENV information or navigator.userAgent or run any tf.scalar calculations. No logs either.====="", ""Just a small update -- I ran the tests again in the new iOS15 Beta 2 and this error still exists across the official TFJS examples.A [patch](https://bugs.webkit.org/show_bug.cgi?id=226953) had been created by the Webkit team, but I'm not sure if it was included in the new iOS release. This may need to be followed up on by someone closer to the Webkit team so TFJS can continue working when iOS15 is released.====="", ""One more quick update -- heard back from a member of the Webkit team that their patch for this issue did not make it into iOS15 Beta 2, but they are hopeful it will be in the next one. I'll test when Beta 3 is released and update/close the ticket if it's fixed.====="", 'This has been fixed in iOS 15 beta 3. Closing it out.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5216"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5216"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4936,[wasm]  Error downloading wasm-binaries.tbz2 when yarn build or build-npm,3,closed,2021-04-15T07:12:38Z,2021-05-19T05:57:54Z,"In windows, under tfjs-backend-wasm folder,  yarn build(or build-npm) failed due to Error downloading.In linux, everything goes well.When I tried yarn build-npm, it complains:```Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp12762910739068097029/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not Found```It seems  file https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2 can not be downloaded by chrome. And https://storage.googleapis.com/webassembly doesn't contains windows build.Then I tried (a cmd used in yarn build):```yarn bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasm ```It also complains the same file 404:```C:\workspace\wasm\tfjs\tfjs-backend-wasm>yarn bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasmyarn run v1.22.5$ C:\workspace\wasm\tfjs\tfjs-backend-wasm\node_modules\.bin\bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasmINFO: Repository emscripten_bin_win instantiated at:  C:/workspace/wasm/tfjs/WORKSPACE:27:22: in <toplevel>  C:/users/abc/_bazel_abc/js5lmfgj/external/emsdk/emscripten_deps.bzl:48:21: in emscripten_depsRepository rule http_archive defined at:  C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>WARNING: Download from https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2 failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not FoundERROR: An error occurred during the fetch of repository 'emscripten_bin_win':   Traceback (most recent call last):        File ""C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 111, column 45, in _http_archive_impl                download_info = ctx.download_and_extract(Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundERROR: Error fetching repository: Traceback (most recent call last):        File ""C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 111, column 45, in _http_archive_impl                download_info = ctx.download_and_extract(Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundINFO: Repository nodejs_windows_amd64 instantiated at:  C:/workspace/wasm/tfjs/WORKSPACE:9:13: in <toplevel>  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/index.bzl:77:23: in yarn_install  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:757:15: in node_repositories  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:777:18: in _maybeRepository rule node_repositories_rule defined at:  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:696:41: in <toplevel>ERROR: C:/users/abc/_bazel_abc/js5lmfgj/external/emsdk/BUILD:27:6: @emsdk//:binaries depends on @emscripten_bin_win//:all in repository @emscripten_bin_win which failed to fetch. no such package '@emscripten_bin_win//': java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundERROR: Analysis of target '//tfjs-backend-wasm/src/cc:tfjs-backend-wasm' failed, build aborted: Analysis failedINFO: Elapsed time: 1.663sINFO: 0 processes.FAILED: Build did NOT complete successfully (1 packages loaded, 2 targets configured)    Fetching @emscripten_npm_win, Restarting.    Fetching @xnnpack, Cloning 3bfbdaf00211b313b143af39279bb6bf1f7effc0 of https://github.com/google/XNNPACK.giterror Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```**System information**- TensorFlow.js version: commit 081b28468bf18505326b543f1ec450d725e1dbb4 (HEAD -> master, origin/master, origin/HEAD)Date:   Wed Apr 14 20:04:45 2021 -0700**Describe the problem****Provide the exact sequence of commands / steps that you executed before running into the problem****Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.","[""Thanks for reporting this.I'm setting up a VM to reproduce it, and I've talked to the author of the Emscripten rules. We have found out that it's caused by an incorrect extension on the download. It should end in `.zip` instead of `.tbz2` for Windows. For example, https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.zip, should be downloadable. I'll send a patch to Emscripten once I get it working. However there may be some other Windows comparability issues other than having an incorrect URL, so I can't guarantee this will be a quick fix.As a workaround, you can use [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install-win10).====="", 'Thanks @mattsoulanille! Then I will close this first.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4936"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4936"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4923,optical flow tracking of the facelandmarks in video/live webcam feed,5,closed,2021-04-12T09:29:24Z,2021-08-07T08:23:57Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using):- Are you willing to contribute it (Yes/No):**Describe the feature and the current behavior/state.**wanted to check if anyone tried to implement optical flow (LK) for tracking in between frames instead of predicting the landmarks on each and every frame when trying on live webcam or videos. **Will this change the current api? How?****Who will benefit with this feature?****Any Other info.**","['cc @pyu10055=====', ""@abhishek-peri i've played with it a bit, but it's too much math for javascript - execution time ends up slower than model inference.btw, current `blazeface` + `facemesh` combo already skips detection if boxes are known and just modifies boxes coordinates depending on mesh results, so it skips detection inference as much as possible and just runs mesh inference (although that algorithm is a bit buggy, took me a while to fix it)====="", ""I've had some success adding acceleration and damping on the key points, essentially doing extrapolation between face-mesh frames to reduce jitteriness, and make the response more fluid. Wouldn't be so difficult if you took the blazeface bounding box coords, which update more often, and then scaled the accelerations accordingly.If the end I some multi order low pass filters to the movement, as it seemed less jitter was more satisfying than faster response times.====="", 'Closing as stale. Please @mention us if this needs more attention.=====', 'Hello, Have you tried opencv.js ?  it provided Optical flow method  [here](https://docs.opencv.org/3.3.1/db/d7f/tutorial_js_lucas_kanade.html)=====']",0
https://github.com/tensorflow/tfjs/issues/1342,Toxicity classifier demo showing different result with figure in readme.,0,closed,2019-03-07T06:03:27Z,2019-03-09T14:02:39Z,"#### TensorFlow.js version1.0.0 #### Browser versionVersion 72.0.3626.121 (Official Build) (64-bit)#### Describe the problemToxicity classifier demo showing different result with figure in readme.Below the image is from the readme.<img width=""1011"" alt=""screen shot 2019-03-07 at 3 06 19 pm"" src=""https://github.com/tensorflow/tfjs-models/blob/master/toxicity/images/demo.jpg?raw=true"">In the image, input text ""now join the anti gay hitler rebellion now!"" results toxicity and identity flag positive.https://storage.googleapis.com/tfjs-models/demos/toxicity/index.htmlBut on the demo page, typing input text with ""now join the anti gay hitler rebellion now!"" makes every flag ""negative"".I think the mismatch between this picture and the demo will cause confusion.#### Code to reproduce the bug / link to feature request1. Visit https://storage.googleapis.com/tfjs-models/demos/toxicity/index.html2. Type ""now join the anti gay hitler rebellion now!"" on the input area.3. Click ""classify"" button.4. Will show result like following image. <img width=""1011"" alt=""screen shot 2019-03-07 at 3 06 19 pm"" src=""https://user-images.githubusercontent.com/3903575/53935688-a5d16800-40ea-11e9-9c8e-9d53d6c685f9.png"">",[],0
https://github.com/tensorflow/tfjs/issues/1719,fromPixels produces different results in WebGL / CPU if the image element has to be resized.,1,closed,2019-07-03T15:47:11Z,2019-07-08T18:56:02Z,"WebGL directly uploads the image element to a texture which creates artifacts, while CPU first renders to a canvas. We should investigate rendering to a canvas in the WebGL backend, or playing with image rendering CSS rules to remove the artifacts. Related: #1488",['Any idea what kind of overhead the extra render might create?====='],0
https://github.com/tensorflow/tfjs/issues/103,feature request: toPixels,3,closed,2018-04-05T17:51:22Z,2018-04-12T15:40:13Z,"_From @oveddan on March 11, 2018 16:19_Right now deeplearn.js has a method [fromPixels](https://deeplearnjs.org/docs/api/index.html#dl.fromPixels) which takes an `ImageData` and converts it into a `Tensor3D`.It would be nice to have something that does the inverse, like `toPixels` - which would take a 2d or 3d tensor and convert it to an `ImageData`_Copied from original issue: tensorflow/tfjs-core#845_","[""_From @carlthome on April 3, 2018 18:7_Yes, this would be handy! Particularly if `tf.toPixels` could handle RGB, RGBA and grayscale images like [`tf.summary.image`](https://www.tensorflow.org/api_docs/python/tf/summary/image).This mess is what I'm using, and I'd gladly replace it:```jsfunction toPixels(tensor) {  const pixels = tensor.dataSync(),  const imageData = new ImageData(tensor.shape[0], tensor.shape[1]),  if (tensor.shape.length == 2 || tensor.shape[2] == 1) {    // Grayscale    for (let i = 0, i < pixels.length, i++) {      imageData.data[i*4+0] = pixels[i],      imageData.data[i*4+1] = pixels[i],      imageData.data[i*4+2] = pixels[i],      imageData.data[i*4+3] = 255,    }  } else if (tensor.shape[2] == 3) {    // RGB    for (let i = 0, i < pixels.length / 3, i++) {      imageData.data[i*4+0] = pixels[i*3+0],      imageData.data[i*4+1] = pixels[i*3+1],      imageData.data[i*4+2] = pixels[i*3+2],      imageData.data[i*4+3] = 255,    }  } else if (tensor.shape[2] == 4) {    // RGBA    imageData.data = pixels,  }  return imageData,}```====="", ""Constructor for ImageData takes (width, height) as arguments and you are passing rows (shape[0]) as width and columns (shape[1]) as height..? That doesn't seem right.====="", '@mlajtos, thanks. Some sloppy refactoring on my part when posting the above. It should be `const imageData = new ImageData(tensor.shape[1], tensor.shape[0]),`=====']",0
https://github.com/tensorflow/tfjs/issues/5479,WEBGL_USE_SHAPES_UNIFORMS causes test failures on MacOS,3,closed,2021-08-12T23:06:30Z,2021-08-23T16:14:08Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: Chrome 92.0.4515.107- Tensorflow.js Converter Version:**Describe the current behavior**WebGL tests using `WEBGL_USE_SHAPES_UNIFORMS` fail on MacOS Chrome 92.**Describe the expected behavior**WebGL tests using `WEBGL_USE_SHAPES_UNIFORMS` pass on MacOS Chrome 92.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.`yarn run-browserstack --browsers=bs_chrome_mac --testEnv webgl2 --flags '{""WEBGL_USE_SHAPES_UNIFORMS"": true}'`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Chrome 92.0.4515.107 (Mac OS 10.13.6) where webgl2 {""WEBGL_USE_SHAPES_UNIFORMS"":true} Tensor3D FAILED	Error: Arrays differ: actual[2] = 3, expected[2] = 5.```[More logs in this nightly run](https://console.cloud.google.com/cloud-build/builds/b634d7e6-4b93-46b4-b484-1a5c5f99d578?project=834911136599)","['I will take a look why there are so many failures with WEBGL_USE_SHAPES_UNIFORMS = true.=====', ""Fixed by #5502  (not sure why the automation didn't close it).====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5479"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5479"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4932,tfjs 3.4.0 regression: wasm causes browser hang,9,closed,2021-04-14T19:32:42Z,2021-04-22T20:43:37Z,"tfjs 3.4.0 was just released and unfortunately wasm module is somewhat broken  to be precise, `tfjs-backend-wasm.wasm` and `tfjs-backend-wasm-simd.wasm` seem to work (although haven't tested in depth)  what's definitely broken is `tfjs-backend-wasm-threaded-simd.wasm`and symptoms are simple - complete browser hang,  up to a point where browser window is nonresponsive to close requests and requires a hard killto reproduce: - enable `WebAssembly SIMD support` in <chrome://flags>- enable `WebAssembly threads support` in <chrome://flags>- set backend to `wasm`- run any model inferencethis used to work in all recent versions of tfjsenvironment: tfjs 3.4.0 on windows 10 with chrome 89 or edge 89","['https://github.com/tensorflow/tfjs/issues/4796 this might be related , will try to test in my local as well and update , thank you cc @mattsoulanille =====', 'I test in my local, the problem is gone after turning off the SIMD flag in chrome. We are working on a fix. Thank you for reporting the bug!=====', '@mattsoulanille This maybe related to the recent changes to bazel wasm build, https://github.com/tensorflow/tfjs/pull/4769=====', ""any updates on this one? as much as i'd love to use tfjs 3.4.0 due to support of tf2, this is a blocker for me.====="", ""No updates yet, but I've been able to reproduce it. I agree with Ping that this is likely related to changes made in #4769, and I'm taking a look.====="", ""@mattsoulanille - thanks - *'taking a look'* is an update, just wanted to know what's going on as it's a blocker for me.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4932"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4932"">No</a>=====', ""@mattsoulanille thanks!given it's a regression that causes hangs, does this warrant spinning up a tfjs service build (e.g., 3.4.1)?(cc @pyu10055)====="", 'Hi @vladmandic , we are releasing 3.5.0 with this fix today.=====']",1
https://github.com/tensorflow/tfjs/issues/4002,urllib.error.HTTPError: HTTP Error 404: Not Found,8,closed,2020-10-01T11:08:57Z,2020-10-08T16:51:38Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):`NO`- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):```ProductName:	Mac OS XProductVersion:	10.14.5BuildVersion:	18F132```- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:`Not applicable`- TensorFlow.js installed from (npm or script link):`Not applicable`- TensorFlow.js version (use command below):`Not applicable`- Browser version:`Not applicable`- Tensorflow.js Converter Version:```$ tensorflowjs_converter --versiontensorflowjs 2.4.0Dependency versions:  keras 2.4.0  tensorflow 2.3.1```**Describe the current behavior**If I run the command```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/tensorflow/tfjs-model/toxicity/1/default/1' model/```**Describe the expected behavior**to download the model from  'https://tfhub.dev/tensorflow/tfjs-model/toxicity/1/default/1' **Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.To run the command```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/tensorflow/tfjs-model/toxicity/1/default/1' model/```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py"", line 757, in pip_main    main([' '.join(sys.argv[1:])])  File ""/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py"", line 761, in main    convert(argv[0].split(' '))  File ""/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py"", line 711, in convert    experiments=args.experiments)  File ""/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 701, in convert_tf_hub_module    module_path = hub.resolve(module_handle)  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/module_v2.py"", line 52, in resolve    return registry.resolver(handle)  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/registry.py"", line 42, in __call__    return impl(*args, **kwargs)  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/compressed_module_resolver.py"", line 88, in __call__    self._lock_file_timeout_sec())  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/resolver.py"", line 402, in atomic_download    download_fn(handle, tmp_dir)  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/compressed_module_resolver.py"", line 83, in download    response = self._call_urlopen(request)  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/compressed_module_resolver.py"", line 96, in _call_urlopen    return url.urlopen(request)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 222, in urlopen    return opener.open(url, data, timeout)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 531, in open    response = meth(req, response)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 641, in http_response    'http', request, response, code, msg, hdrs)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 563, in error    result = self._call_chain(*args)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 503, in _call_chain    result = func(*args)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 755, in http_error_302    return self.parent.open(new, timeout=req.timeout)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 531, in open    response = meth(req, response)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 641, in http_response    'http', request, response, code, msg, hdrs)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 569, in error    return self._call_chain(*args)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 503, in _call_chain    result = func(*args)  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 649, in http_error_default    raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 404: Not Found```","[""Can confirm that this is not working, both for this model and pretty much from any url I copy from tfhub.@pyu10055 the url listed in our docs 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1' does still work. I wonder if there is an issue with the newer urls that have 'tfjs_model' in them, or if there is a redirect that needs to be resolved that urllib is not handling. Could you take a look. ====="", '@loretoparisi The issue is that, toxicity model is an JS model, and the converter can only work with TFHub modules which this model does not have.are you trying to download the model artifacts to use in an offline use case?=====', '@pyu10055 yes I would like to use the TFHub model artifacts in the offline case, so I was trying the converter to load this model. In fact I realized this `model.json` is different than the regular ones (it is missing some keys see [here](https://stackoverflow.com/questions/64153455/tensorflow-js-load-tfhub-model-locally)).Any way to convert this model and/or use it offline?Thank you=====', 'The toxicty model seems to be a graph model, so even if you download it and try converting it with `--input_format=tfjs_layers_model` flag it will failMy question:Is there any way to get the sequential variant of these tfhub js models? So we can convert itUpdate: I read [this](https://stackoverflow.com/questions/64153455/tensorflow-js-load-tfhub-model-locally) , so my question is somewhat irrelavant to what OP is trying to achieve, but I still would like to ask that question=====', 'I would add that, as someone on SF, pointed out, it should be possible to locally host the models folder and serve the model artifacts (via model.json) using maybe a simple nodejs http server.My point here is: why is not possible to convert the `GraphModel` to a SavedModel o standard layered model and make it accessible locally via the `tfjs.loadLayersModel` api?Thanks=====', ""Thanks for the clarification @pyu10055 and @loretoparisi. So it seems in this case you shouldn't need to to do any conversion as it is already a tfjs graphmodel? You could download the model artifacts here https://tfhub.dev/tensorflow/tfjs-model/toxicity/1/default/1 and serve them from your own storage/locally. Your stack overflow posts suggests you are partway there.The missing piece that I can see would be updating the [model wrapper](https://github.com/tensorflow/tfjs-models/blob/master/toxicity/src/index.ts#L68) to support loading from a custom url (and doing the same for loading the tokenizer/vocabulary). This would be needed unless you plan on writing your own pre and post processing code for the model at which point you wouldn't need the [wrapper code](https://github.com/tensorflow/tfjs-models/blob/master/toxicity/src/index.ts) from the hosted toxicity model.Could you tell us what you would overall like to achieve so that we can capture what the appropriate feature requests might be?====="", '@tafsiri thank you. I think as you say the wrapper should support custom urls. More specifically, as the layered model loading api, it could support the `file://`protocol, so that it would be easy to load models from local serving (via http) or from the file system.Thanks a lot for your help!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4002"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4002"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/470,Plans for N-D Convolution?,14,closed,2018-06-26T02:53:03Z,2021-09-01T22:40:41Z,"TFJS already has `conv1d` and `conv2d`, which are great, but I'm really missing Python's N-D convolution ([tf.nn.convolution](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)).Are there plans for implementing this? And if not, would the team be open to PRs?","[""We don't have plans for implementing it, but happy to take PRs!====="", ""@nsthorat I've implemented convolution [here](https://github.com/julianoks/tfjs-core/commit/73c6b67b6c1a62110fa9c66658b0bcc9903b3b67). Essentially, my implementation determines which convolution to use (`conv1d`, `conv2d`) based on the rank of the filter.At this point, I'd like your guidance on how to proceed with the tests.To me, there are 2 viable approaches:- Testing the correct convolution was called with the correct arguments (I don't know how to do this), or- Reusing the tests from `conv1d` and `conv2d`. We can either copy and paste those tests, or we can factor those tests and call them from both `conv1d`/`conv2d` and `convolution`.What do you think?====="", 'Hi @julianoks Having an ND wrapper around conv1d and conv2d is not very useful at this point, but if conv3d support gets in, I can see the value. Are you interested in implementing conv3d?=====', ""@dsmilkov Unfortunately, I don't think I'll have time to implement `conv3d` anytime soon.====="", 'No worries. In that case, we can postpone the `convolution` wrapper.=====', ""@julianoks @dsmilkov I'm taking a stab at `conv3d` , will update with progress within a few days.====="", 'Just a quick update - this is a little more complex than I initially thought, but giving it some free time over the next few weeks and thinking I can have it finished in July.=====', ""@dsmilkov For conv3d input formats, it looks like the conv2d implementation in tfjs-core strays from the TensorFlow Python documentation. Guessing this is from the deeplearn.js days, but I'm wondering if I should match the paradigm that's currently used in tfjs-core, or whether I should stick to the Python documentation exactly?P.S. No rush in answering, changing this should be easy at any point.Python Conv2d documentation: https://www.tensorflow.org/api_docs/python/tf/nn/conv2dtfjs-core implementation: https://github.com/tensorflow/tfjs-core/blob/master/src/ops/conv.tsPython Conv3d documentation: https://www.tensorflow.org/api_docs/python/tf/nn/conv3dmy stubbed implementation of conv3d for tfjs-core: https://github.com/zboldyga/tfjs-core/commit/9b98586bffe3ef0ae2d30e703938cdcf62c76169Currently I'm following your existing paradigm. It seems like making a change to conv3d would warrant changing conv2d and possibly conv1d, which would be a breaking change.====="", 'Just curious -- what specifically diverges between tfjs-core and the python documentation, is it just the naming? The implementation should be identical since we test this against real TensorFlow.If naming is the only difference, I would stick with what we did for conv2d in tfjs-core.=====', ""@nsthorat The Python documentation for tf.nn.conv2d shows the following function description:`tf.nn.conv2d(    input,    filter,    strides,    padding,    use_cudnn_on_gpu=True,    data_format='NHWC',    dilations=[1, 1, 1, 1],    name=None)`Comparing this python documentation to the tfjs-core implementation of conv2d, I see the following differences:1. Input is strictly a 4D tensor in the python implementation [batch, in_height, in_width, in_channels], whereas the tfjs-core implementation allows a 4D tensor [batch, in_height, in_width, in_channels] OR a 3D tensor [in_height, in_width, in_channels] (in this case tfjs assumes batch size of 1).2. With the Python implementation, strides is a 1D tensor of length 4 [batch, height, width, channels], where it’s recommended that batch == channels == 1. For the tfjs-core implementation, strides is a 1D tensor of length 2 [height, width]. Batch and channels are automatically assumed to be 1 in the tfjs implementation.3. Python documentation says padding must be ‘SAME’ or ‘VALID’, and the tfjs-core implementation allows for ‘SAME’, ‘VALID’, or an integer input.4. Dilations faces the same issue as in #2. Python documentation calls for a 1D tensor of length 4, and tfjs-core implementation calls for a 1D tensor of length 2 (and automatically assumes first and last parameters are 1)... The Python documentation states that dilations in the batch and depth dimensions must be 1, but the input is still 4D in the Python implementation.5. dimRoundingMode is a parameter in the tfjs-core implementation, but this parameter does not appear in the Python documentation. 6. name is not a parameter for the tfjs-core implementation (I'm assuming this has some sort of debugging or logging use, it's an optional parameter in the Python implementation).I didn't check the Python documentation against the Python implementation, I'm assuming that the documentation is valid. ====="", 'Hi @zboldyga ,Nice comparison on the differences. When addding `conv3d`, I would follow what we did with `conv2d` in tfjs-core. In this case of conv3d, this means: take conv4d or conv5d tensor, strides is length 3, padding is same and valid (no need for supporting the integer input yet), dilations is length 3, skip dimRounding for now, and skip name.=====', ""@dsmilkov Rad, thanks!I believe the cpu kernel implementation is complete - adding some more unit tests this weekend and will open a PR soon.I also implemented the webGL kernel and ran into a few bumps as the language is a bit foreign to me. I'm going to try a few more things, but might add some comments in the PR to seek guidance from someone that has experience with webGL.Will follow up in a few days!====="", 'building on the great work of @zboldyga, i submitted a PR for Conv3D in tfjs-layers: https://github.com/tensorflow/tfjs-layers/pull/495=====', 'Related PR has been merged, closing this issue.=====']",0
https://github.com/tensorflow/tfjs/issues/4276,.predict & .execute optional arguments type error,7,closed,2020-11-19T20:51:42Z,2020-11-25T15:55:17Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX Big Sur- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): 2.7.0- TensorFlow.js version (use command below): 2.7.0- Browser version: n/a- Tensorflow.js Converter Version: n/a**Describe the current behavior**Using Typescript, `model.predict` & `model.execute` require 2 arguments, however in the docs these arguments are shown to be optional – https://js.tensorflow.org/api/latest/#tf.GraphModel.execute – This therefore throws an error. I'm hesitant to throw an empty object into `model.predict` because I can't see how the optional keys inside the object are set, and it's I can't find any documentation passing the default option for `model.execute` from my model so I can't bypass the typechecking.**Describe the expected behavior**For both `model.predict` & `model.execute` I should be able to pass one argument – the tensor inputs and it pass type check,.**Standalone code to reproduce the issue**https://codesandbox.io/s/admiring-river-u3jgf?file=/src/index.ts**Other info / logs** Include any logs or source code that would be helpful to<img width=""476"" alt=""image"" src=""https://user-images.githubusercontent.com/37798644/99721131-9fde0f00-2aa6-11eb-86a8-5d56d1d7c0db.png"">","[""I want to add, I'd do a PR but I want to be sure which is incorrect the type or the docs.====="", '@joshuaellis looks like there is some inconsistency on the predict/execute method between InferenceModel interface and GraphModel.Can you check if you type the model as tf.GraphModel instead, will the type check pass?=====', ""I've changed it to `tf.GraphModel` but `model.execute` still fails with `tf.LayersModel`.in `training.d.ts` line 296 execute is written as –```jsexecute(inputs: Tensor | Tensor[] | NamedTensorMap, outputs: string | string[]): Tensor | Tensor[],```so outputs is still required.====="", '@joshuaellis you can use predict() method with tf.layersModel, the output strings are not required there.=====', 'That would mean the docs are wrong then? as it says execute has an optional argument `outputs`=====', '@joshuaellis The doc for GraphModel and LayersModel are different, only execute on GraphModel says `outputs` is optional, LayersModel is consistent with InferenceModel interface, the `outputs` is required argument.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4276"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4276"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4284,Inconsistent ”Could not get context for WebGL version 2” error message,8,open,2020-11-20T08:53:20Z,2021-11-02T09:46:55Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution: macOS 10.15.7 and iOS 14.1- Mobile device: iPhone 11- TensorFlow.js installed from (npm or script link): `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0`- TensorFlow.js version: `2.7.0`- Browser version:   - Safari 14.0 on iOS:    `Mozilla/5.0 (iPhone, CPU iPhone OS 14_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1`  - Safari 14.0.1 on macOS:    `Mozilla/5.0 (Macintosh, Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15`- Tensorflow.js Converter Version: ?**Describe the current behavior**Steps to reproduce:1. Load https://js.tensorflow.org/api/latest/#add2. Click the first Run button3. Console log prints:```Could not get context for WebGL version 2Tensor    [11, 22, 33, 44]```4. Click Run again5. Console log prints:```Tensor    [11, 22, 33, 44]```**Describe the expected behavior**Expected the log messages in steps 3 and 5 to be identical, including the WebGL error message.**Standalone code to reproduce the issue**Reproducible test case: https://js.tensorflow.org/api/latest/#add**Other info / logs**The latest stable Safari 14 ships with WebGL 2.0 disabled by default. Make sure WebGL 2.0 is disabled via Safari > Experimental WebKit Features to be able to reproduce this issue.","['@annxingyuan Can you help to confirm? The WebGL 2 check happens on the first API call, once it is identified missing, we will be using WebGL 1 instead. The following API calls would not trigger the warning message. =====', 'i am having same issue on chrome @pyu10055 ![image](https://user-images.githubusercontent.com/40135431/105950950-4f31f600-6095-11eb-864e-62e67fda0fe4.png)=====', 'Better documentation on the warning message, people understand that this is the case for safari.=====', 'Thank you for dealing with this issue, but is there a temporary solution for this? For instance, would assigning the backend to WebGL version 1 helps? If yes, could you tell us how to do it, please?=====', 'any updates on this issue ?=====', 'Hi Guys, Not sure of the root cause for this bug, but one of our client reported such issue on some Windows Machine with Chrome. The workaround was to change a flag in Chrome to not use D3D11 as a backend for ANGLE but to force OpenGL.As this issue only occurred on few computers it might be related to a combination of Drivers + CPU/GPU platforms. This has only be reported on our side for integrated GPU.=====', 'I am getting this error when trying to use tensorflow js in safari, with an ionic app.=====', '> I am getting this error when trying to use tensorflow js in safari, with an ionic app.Change to wasm backend=====']",1
https://github.com/tensorflow/tfjs/issues/4937,Env flag affects the peak memory in e2e benchmark.,1,open,2021-04-15T07:55:51Z,2021-04-16T00:59:36Z,"Steps to reproduce:1. Access https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html2. Use the default backend `wasm` and click `Run benchmark`.3. See the left tablePeak memory | 19.63 MB-- | --4. Switch to `webgl` backend. Change any flags, for example `webgl pack`. And then switch back to `wasm`.5. Click `Run benchmark` again. The peak memory increases as below:Peak memory | 33.51 MB-- | --6. If we repeat 4 and 5, you will find the peak memory gets bigger and bigger. However, if you don't change the flag, the peak memory won't change. It happens for all backends that the flag will affect the peak memory. It seems like a bug.","[""fyi @lina128. This is the issue that I mentioned in yesterday's meeting.=====""]",1
https://github.com/tensorflow/tfjs/issues/1693,"ValueError: Unsupported Ops in the model before optimization Variable, Unique, Assign",2,closed,2019-06-25T11:31:41Z,2020-06-05T18:08:31Z,"Hello, While I run the below command, I get an error ""ValueError: Unsupported Ops in the model before optimization Variable, Unique, Assign"".tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model export/saved_model export/web_modelEven though I use '--skip_op_check' to skip the ops, these ops error come up again when I call the model for prediction in frontend. Can these ops (Variable, Unique, Assign) be added for supported operations by [Tensorflow.js]TensorFlow.js 1.2.2.1Tensorflow: 1.14.0Browser version - Google Chrome: 75.0.3770.100Thanks.","['@richasDen Theoretically the Variable op should have been replaced with a constant during the conversion, and Assign op is usually an op comes with the Variable op.  If you can share the model with us, we would like to investigate further. thanks.=====', 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/5946,qna model fails with very short passages,4,closed,2021-12-15T12:38:18Z,2021-12-16T15:44:09Z,"### recreate/demonstration of the bug```jsconst tfjs = require('@tensorflow/tfjs-node'),const qna = require('@tensorflow-models/qna'),const question = ""Who did John see?"",const passage = ""John saw Mary"",qna.load()    .then((model) => {        return model.findAnswers(question, passage),    })    .then((answers) => {        console.log(answers),    }),```This fails with an uncaught exception:```(node:2393) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'index' of undefined    at QuestionAndAnswerImpl.convertBack (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:323:34)    at QuestionAndAnswerImpl.getBestAnswers (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:282:27)    at QuestionAndAnswerImpl.<anonymous> (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:227:47)    at step (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:33:23)    at Object.next (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:14:53)    at fulfilled (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:5:58)    at processTicksAndRejections (internal/process/task_queues.js:93:5)```The above recreate is for Node.js because that was simpler to demonstrate, but I have seen the same errors happen when running in browsers as well. For example, if you paste the same question (""Who did John see?"") and passage (""John saw Mary"") into https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html you'll get the same uncaught exception in your browser console.```[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating 't[u].index')	(anonymous function) (demo.20d541c6.js:1658:774)	asyncFunctionResume	(anonymous function)	promiseReactionJobWithoutPromise	promiseReactionJob```### Expected behaviorThis should return an answer (or at least successfully return an empty array).### CauseI think the problem is in a check (to see if the indexes are in the known tokens map) failing to take the output offset into account.https://github.com/tensorflow/tfjs-models/blob/ccfcb2ab9b479b7e053f8eafaf68bd75a71c6551/qna/src/question_and_answer.ts#L277```js        if (tokenToOrigMap[start] && tokenToOrigMap[end] && end >= start) {```This means the later indexes into the tokens map are not guaranteed safe, and can return undefined. https://github.com/tensorflow/tfjs-models/blob/ccfcb2ab9b479b7e053f8eafaf68bd75a71c6551/qna/src/question_and_answer.ts#L336-L339```js    const shiftedStart = start + OUTPUT_OFFSET,    const shiftedEnd = end + OUTPUT_OFFSET,    const startIndex = tokenToOrigMap[shiftedStart],    const endIndex = tokenToOrigMap[shiftedEnd],```### Suggested fixI think updating the check to include the output offset should correct the problem. This protects entry into the `convertBack` function so that it is only called with values that will definitely be in `tokensToOrigMap`.","[""I've put my suggested fix into a pull request in case that helps explain it better!https://github.com/tensorflow/tfjs-models/pull/888 ====="", 'Thank you @dalelane , I have assigned the reviewer for PR. =====', 'Related PR has been merged , closing this issue. Please @mention to reopen if issue still persists.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5946"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5946"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1507,24 versus 17 parts (PoseNet vs BodyPix),1,closed,2019-04-09T19:40:08Z,2019-04-10T19:01:15Z,Is there a way to get more parts for PoseNet? It is missing hands and feet for example.,"[""No I don't believe the model is trained to predict positions for those parts. Details on the original model can be found here https://arxiv.org/pdf/1803.08225.pdf=====""]",0
https://github.com/tensorflow/tfjs/issues/4312,Use tfjs-react-native with posenet and bundleResourceIO,6,closed,2020-11-26T21:02:08Z,2021-12-11T17:02:21Z,"I want to build an react native app with the use of posenet. A requirement is, that it works offline.so the examples work fine. when i do:```import * as posenet from '@tensorflow-models/posenet'this.model = await posenet.load(),``` so what i did next is converting the model to one weigth file and load the model locally with:```const modelJson = require('./assets/model/original/model-stride32.json'),const modelWeights = require('./assets/model/original/weigth.bin'),    this.model = await tf.loadGraphModel(   bundleResourceIO(modelJson, modelWeights)),```so what i want to do next is to run something like this:```const predictions = await this.model.estimateSinglePose(imageTensor)```but of course this doesn't work because the model doesn't know the function. i know i can instanciate the model by using something like this:```const checkpointLoader = new posenet.CheckpointLoader(checkpoint.url),const variables = await checkpointLoader.getAllVariables(),const model =  new posenet.MobileNet(variables, checkpoint.architecture),```but the checkpoint loader needs an url. i want to use the local files.does anyone has a hint for me how to use posenet without rewriting the whole node-module?thx in advance","['Try using the [modelUrl param](https://github.com/tensorflow/tfjs-models/tree/c37b1193358f12cc6726262285ad3d6b0f1046a0/posenet#config-params-in-posenetload) of posenet but passing the IO handler rather than a string, it would look something like this```const net = await posenet.load({  modelUrl: bundleResourceIO(modelJson, modelWeights)}),```=====', 'Hey @tafsiri,thx alot, it is working now. You saved my day and my project.=====', '@dabass51 could you tell me where you got the modelJson and modelWeights files? Thanks :)=====', '> @dabass51 could you tell me where you got the modelJson and modelWeights files? Thanks :)You can check on Tensorflow Hub, this [link](https://tfhub.dev/s?deployment-format=tfjs&q=posenet) has all the available models which you can download. =====', '@dabass51 I want to achieve pose estimation in my React Native app can you provide me with link to some docs or instructions on how to add tfjs and poseet into a React Native app for android. I was able to set up tfjs by installing necessary dependencies inside my React Native project. Can you provide something to move forward? Thanks=====', 'Could anyone say me how can I use the model bodypix in react native? =====']",1
https://github.com/tensorflow/tfjs/issues/5631,Can't run electron application - Getting Uncaught TypeError error message,4,closed,2021-09-16T16:01:27Z,2021-09-27T17:43:48Z,"Hi Team,Kindly help me. I'm developing chatbot application using Angular10 and convert to desktop application using ElectronJS.For offline chatbot experience, I used tensorflow/tfjs. it working in browser but we loading electron application i'm getting the below attached error message. Note: Enabled NodeIntegration (true) in electron application.Please let me know, if need any other information.![tensorflow error](https://user-images.githubusercontent.com/67852635/133645276-03f2a5ef-50bd-4af7-ad82-20d2ee1e002f.PNG)![tensorflow platform](https://user-images.githubusercontent.com/67852635/133645279-fd5dde84-f0ab-4e65-914d-585b7c99d20f.PNG)I'm using the below mentioned dependencies (package.json)""dependencies"": {    ""@angular/animations"": ""~10.0.6"",    ""@angular/cdk"": ""^11.2.5"",    ""@angular/common"": ""~10.0.6"",    ""@angular/compiler"": ""~10.0.6"",    ""@angular/core"": ""~10.0.6"",    ""@angular/flex-layout"": ""^10.0.0-beta.32"",    ""@angular/forms"": ""~10.0.6"",    ""@angular/localize"": ""~10.0.6"",    ""@angular/material"": ""^8.2.3"",    ""@angular/platform-browser"": ""~10.0.6"",    ""@angular/platform-browser-dynamic"": ""~10.0.6"",    ""@angular/router"": ""~10.0.6"",    ""@angular/service-worker"": ""~10.0.6"",    ""@ng-bootstrap/ng-bootstrap"": ""^8.0.0"",    ""@ng-idle/core"": ""^10.0.0-beta.1"",    ""@ng-select/ng-select"": ""^6.1.0"",    ""@tensorflow/tfjs"": ""^3.9.0"",    ""@types/webgl2"": ""0.0.6"",    ""angularx-qrcode"": ""^10.0.12"",    ""bootstrap"": ""^4.6.0"",    ""compute-cosine-similarity"": ""^1.0.0"",    ""core-js"": ""^2.5.4"",    ""font-awesome"": ""^4.7.0"",    ""hammerjs"": ""^2.0.8"",    ""jquery"": ""^3.5.1"",    ""jszip"": ""^3.5.0"",    ""jszip-utils"": ""^0.1.0"",    ""lodash.merge"": ""^4.6.2"",    ""material-design-icons"": ""^3.0.1"",    ""moment"": ""^2.29.1"",    ""net"": ""^1.0.2"",    ""ng-connection-service"": ""^1.0.4"",    ""ng2-file-upload"": ""^1.3.0"",    ""ngx-color-picker"": ""^11.0.0"",    ""ngx-electron"": ""^2.2.0"",    ""ngx-toastr"": ""^10.0.4"",    ""node-jose"": ""^2.0.0"",    ""popper.js"": ""^1.14.3"",    ""rxjs"": ""~6.5.5"",    ""rxjs-compat"": ""^6.3.3"",    ""sockjs-client"": ""^1.5.1"",    ""stompjs"": ""^2.3.3"",    ""stopwords"": ""0.0.9"",    ""tslib"": ""^2.1.0"",    ""uuid"": ""^8.3.2"",    ""wink-lemmatizer"": ""^3.0.1"",    ""wink-tokenizer"": ""^5.2.3"",    ""zone.js"": ""~0.10.3""  },  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.1000.5"",    ""@angular-devkit/build-ng-packagr"": ""~0.1000.8"",    ""@angular/cli"": ""~10.0.5"",    ""@angular/compiler-cli"": ""~10.0.6"",    ""@angular/language-service"": ""~9.0.0"",    ""@types/jasmine"": ""~3.5.0"",    ""@types/jasminewd2"": ""~2.0.3"",    ""@types/lodash.merge"": ""^4.6.6"",    ""@types/node"": ""^12.19.15"",    ""@types/node-jose"": ""^1.1.5"",    ""@types/uuid"": ""^8.3.0"",    ""codelyzer"": ""^6.0.0"",    ""csscomb"": ""^4.3.0"",    ""electron"": ""^11.2.3"",    ""electron-packager"": ""^15.2.0"",    ""husky"": ""^4.3.8"",    ""jasmine-core"": ""~3.5.0"",    ""jasmine-spec-reporter"": ""~5.0.0"",    ""karma"": ""~5.0.0"",    ""karma-chrome-launcher"": ""~3.1.0"",    ""karma-coverage-istanbul-reporter"": ""~3.0.2"",    ""karma-jasmine"": ""~3.3.0"",    ""karma-jasmine-html-reporter"": ""^1.5.0"",    ""ng-packagr"": ""^10.0.0"",    ""protractor"": ""~7.0.0"",    ""ts-node"": ""~8.3.0"",    ""tslint"": ""~6.1.0"",    ""typescript"": ""~3.9.5""  } ","['Did you get chance to check this [link](https://stackoverflow.com/questions/65562209/this-util-textencoder-is-not-a-constructor-only-in-electron-app-works-in-chrome) ? if it does not help please provide a minimal reproduction code.Thanks Rajesh =====', ""The stackoverflow link is not helped me.While loading application, tensorflow try to set env().setPlatform('node', new PlatformNode()),Here require('util') return null and doesn't have TextEncoder function. It seems, require('util') not refer @type/node component.export class PlatformNode {    constructor() {        // tslint:disable-next-line:no-require-imports        this.util = require('util'),               // According to the spec, the built-in encoder can do only UTF-8 encoding.        // https://developer.mozilla.org/en-US/docs/Web/API/TextEncoder/TextEncoder        this.textEncoder = new this.util.TextEncoder(),    }====="", 'Hi @gsvin99, I am not super familiar with electron, but I think you probably need to turn off node integration in the renderer process so it can use browser js bundle instead of nodejs bundle. Also, it might help checking out this [tfjs electron example](https://github.com/tensorflow/tfjs-examples/tree/master/electron) (it has not been updated for the newer tfjs versions but its setup might help). It shows you how to use tfjs in renderer process and main process. Hope these can help you. Thanks!=====', 'Thanks @jinjingforever and @rthadur Electron application working, after upgrading newer version, disable node integration and enable context isolation. =====']",1
https://github.com/tensorflow/tfjs/issues/5516,movenet-multipose model fails on inference using tfjs-backend-wasm,6,closed,2021-08-20T12:32:10Z,2021-11-23T01:58:24Z,[MoveNet MultiPose model](https://storage.googleapis.com/movenet/MoveNet.MultiPose%20Model%20Card.pdf) was released and published on [TFHub](https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1)but fails on inference using `tfjs-backend-wasm`:```human.js:56658 Uncaught (in promise) Error: Broadcasting along outer dims is not yet supported for float32 Less.    at Object.kernelFunc3 [as kernelFunc] (human.js:56658)    at kernelFunc3 (human.js:8316)    at human.js:8357    at Engine.scopedRun (human.js:8238)    at Engine.runKernelFunc (human.js:8355)    at Engine.runKernel (human.js:8275)    at less_ (human.js:12427)    at less__op (human.js:8849)    at executeOp11 (human.js:34000)```no issues running model using `tfjs-backend-webgl`  i'm reporting this as an issue instead of a feature as the model is officially published on `tfhub` thus expectation is that it works  environment: tfjs 3.8.0 on ubuntu 21.10,"['I just tested with tfjs-backend-wasm@3.11.0, it works. Can you confirm? @vladmandic =====', '@lina128 ive retried with **tfjs 3.11.0** and tfjs fresh build from main (including building wasm binaries) and issue is the same:> Uncaught (in promise) Error: Broadcasting along outer dims is not yet supported for float32 Less.But to reproduce, input `[1, 256, 256, 3]` must be an actual image that contains body,  otherwise model short-circuits and it never reaches affected op(thats the general problem testing with synthetic inputs, model ops dont actually get excercised)=====', 'Can you try the demo here? https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenetChange the backend-runtime to tfjs-wasm. Does it work on your machine? Or can you share a code snippet that can reproduce the error?=====', 'Sure - the demo results in exactly the same error, screenshot below  (note: need to select `multipose` variation for reproduction)![image](https://user-images.githubusercontent.com/57876960/141872355-516e95bc-8340-49c3-aec3-a7f0ff767e65.png)=====', ""Ah, you said multipose, sorry I was testing singlepose. Ok, I'm able to reproduce the error now. Will get back to you soon.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5516"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5516"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5166,typo in documentation,1,closed,2021-06-03T17:20:05Z,2021-06-03T18:21:55Z,"[https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#4](https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#4)I believe ""sparseCategoricalCrossentroy"" should be ""sparseCategoricalCrossentropy""This template is for miscellaneous issues not covered by the other issue categories.For questions on how to work with TensorFlow.js, or support for problems that are not verified bugs in TensorFlow.js, please go to [StackOverflow](https://stackoverflow.com/tags/tensorflow.js).",['This will be fixed and published accordingly. Thank you ====='],1
https://github.com/tensorflow/tfjs/issues/4944,TypeError: t is not a function,2,closed,2021-04-16T13:11:27Z,2021-04-21T05:16:24Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):``` javascript  let img = new Image(),  img.width = 260,  img.height = 260,  img.src = captureEl.current.toDataURL('image/jpg'),  model.predict(img2x(img)),  // where error came// img2xexport const img2x = (imgEl: HTMLImageElement): tf.Tensor => {  return tf.tidy(() => {    const input = tf.browser.fromPixels(imgEl)      .toFloat()      .sub(255/2)      .div(255/2)      .reshape([1, 260, 260, 3]), // the model's inputShape    return input,  })}```- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MACOS 11.2.3- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):  3.3.0- Browser version: Chrome  89.0.4389.128- Tensorflow.js Converter Version:  no**Describe the current behavior**![截屏2021-04-16 下午9 06 32](https://user-images.githubusercontent.com/33031512/115028663-b5415300-9ef7-11eb-8665-df1afc992fc1.png)**Describe the expected behavior**The model will predict the image and return an index of classes.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""Already resolve this problem. It's caused by two version of `tensorflow.js`'s import conflict.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4944"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4944"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1172,Web Worker: Backend name 'cpu' not found in registry`,1,closed,2019-01-29T19:29:41Z,2019-10-08T20:23:06Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versionversion 0.14.1#### Browser versionchrome 71#### Describe the problem or feature requestUsing the web-worker does not work on the version 11.6 onward. Registering backend (cpu) fails because it is not found in the registry. `Uncaught (in promise) Error: Backend name 'cpu' not found in registry`#### Code to reproduce the bug / link to feature request    <head>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.14.2/dist/tf.min.js""></script>    <script>        const worker_function = () => {            onmessage =  () => {                console.log('from web worker')                    this.window = this                    importScripts('https://cdn.jsdelivr.net/npm/setimmediate@1.0.5/setImmediate.min.js')                    importScripts('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.14.2')                    tf.setBackend('cpu')                                        const res = tf.zeros([1, 2]).add(tf.ones([1, 2]))                    res.print()                                        postMessage({res: res.dataSync(), shape: res.shape})            },        }        if (window != self)            worker_function(),    </script>    <script>        const worker = new Worker(URL.createObjectURL(new Blob([""("" + worker_function.toString() + "")()""], { type: 'text/javascript' }))),        worker.postMessage({}),        worker.onmessage = (message) => {        	console.log('from main thread')        	const {data} = message        	tf.tensor(data.res, data.shape).print()        }    </script>    </head>","['Above error is not happening with latest version , please use latest version.![image](https://user-images.githubusercontent.com/43972606/66430330-b32b3980-e9ce-11e9-97f9-771d712f7e5d.png)=====']",0
https://github.com/tensorflow/tfjs/issues/5023,"Getting Error for export { ImageClassificationModel, loadImageClassification } from './img_classification when trying to do object detection with AutoML model in NodeJS",2,closed,2021-05-03T11:08:56Z,2021-05-04T20:49:07Z,"I am trying to make Object detection in Node (Not Html) with custom model that was trained. **System information**- OS Platform - Mac OS Catalina- TensorFlow.js installed using NPM- TensorFlow.js version          ""dependencies"": {            ""@tensorflow/tfjs-automl"": ""^1.1.0"",            ""@tensorflow/tfjs-node"": ""^3.6.1""          }**Describe the current behavior**Trying to make object detection predict working in Node js. Below is the code`const tf = require(""@tensorflow/tfjs-node""),const automl = require(""@tensorflow/tfjs-automl""),const fs = require(""fs""),const model_url = ""/model.json"",const image_path = process.argv.slice(2)[0],if (!image_path) {  throw new Error(""/MegaFortuneWheel.png""),}const image = fs.readFileSync(""/MegaFortuneWheel.png""),const decoded_image = tf.node.decodeJpeg(image),async function run() {  const model = await automl.loadObjectDetection(model_url),  const predictions = await model.detect(decoded_image),  console.log(predictions),}run().catch(console.error),`**Standalone code to reproduce the issue**Below is full code with the model fileshttps://github.com/ayandebbarman/TensorFlowObjectDetection**Other info / logs** Include any logs or source code that would be helpful to This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags./Users/ayan.barman/Documents/Node/TF_git/node_modules/@tensorflow/tfjs-automl/dist/index.js:18export { ImageClassificationModel, loadImageClassification } from './img_classification',^^^^^^SyntaxError: Unexpected token 'export'    at Object.compileFunction (node:vm:355:18)    at wrapSafe (node:internal/modules/cjs/loader:1038:15)    at Module._compile (node:internal/modules/cjs/loader:1072:27)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:988:32)    at Function.Module._load (node:internal/modules/cjs/loader:828:14)    at Module.require (node:internal/modules/cjs/loader:1012:19)    at require (node:internal/modules/cjs/helpers:93:18)    at Object.<anonymous> (/Users/ayan.barman/Documents/Node/TF_git/tf.js:2:16)    at Module._compile (node:internal/modules/cjs/loader:1108:14)","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5023"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5023"">No</a>=====', 'Duplicate of https://github.com/tensorflow/tfjs/issues/5017=====']",1
https://github.com/tensorflow/tfjs/issues/4483,tfjs-react-native: Any way to make models load faster? (outside of Metro),3,closed,2021-01-05T04:39:36Z,2021-08-12T13:29:37Z,"I load three graph models when my app boots. On my new Galaxy S20+, this takes ~3 seconds with Metro (served over HTTP right?) and ~9 seconds with a release bundle (loaded with react-native-fs I believe). These numbers are fine (although I'd be interested to know why it takes 3x as long when the file is already on-device _edit: I realize now this is because over HTTP we use fetch as binary, whereas with fs we cycle between bytes/base64 multiple times_ ), but now that my app is getting ready for release, I'm seeing load times of 30 seconds or more on some older devices. This is really pushing the limit of what I'm comfortable with.Is there any recourse here? Each of my models is a tfjs model with 6 or 7 weights shards, 4mb each. ~75mb total on file.","[""I think I know what's going on here.I found this in react-native-fs:```InputStream inputStream = getInputStream(filepath),byte[] inputData = getInputStreamBytes(inputStream),String base64Content = Base64.encodeToString(inputData, Base64.NO_WRAP),promise.resolve(base64Content),```And this in BundleResourceHandler:```base64Weights = await RNFS.readFileRes(fileName)...const weightData = util.encodeString(base64Weights, 'base64').buffer```So the flow is:RNFS reads bytes > RNFS converts to base64 > TFJS receives base64 > TFJS converts to bytesIt looks like this is a package out there that can fix this cycle:https://www.npmjs.com/package/rn-fetch-blobSpecifically the `readStream` method: https://github.com/joltup/rn-fetch-blob/wiki/File-System-Access-API#readstreampath-encoding-buffersize-interval-promisernfbreadstream====="", ""@tafsiri Check this out! I think I'm on to something here. I know this code is pretty raw (mixing Expo FS and RNFS...) but it actually works! And I can use this exact same code to load buffers in Metro or in release.I was able to reduce my shards load time from 9-12 seconds to 3-3.5 seconds!Flow:1. Get asset uris from expo-asset2. Copy files from cache into document directory3. use react-native-static-server to serve doc directory over HTTP4. use axios to grab the .bin files from the serverI also think it's definitely possible to skip the cache>docDir copies, but it's late and I will try tomorrow...```async function loadAllShards() {  const t1 = Date.now()  const [family, genus, species] = await Asset.loadAsync([    require('../../assets/family_shards.bin'),    require('../../assets/genus_shards.bin'),    require('../../assets/species_shards.bin')  ])  console.log('loaded assets in', Date.now() - t1, 'ms')  if (!family.localUri || !genus.localUri || !species.localUri) {    throw new Error('shards file is missing a localUri')  }  await FileSystem.copyAsync({    from: family.localUri,    to: FileSystem.documentDirectory + 'family_shards.bin'  })  await FileSystem.copyAsync({    from: genus.localUri,    to: FileSystem.documentDirectory + 'genus_shards.bin'  })  await FileSystem.copyAsync({    from: species.localUri,    to: FileSystem.documentDirectory + 'species_shards.bin'  })  console.log('starting server', RNFS.DocumentDirectoryPath, Date.now() - t1)  const server = new StaticServer(0, RNFS.DocumentDirectoryPath)  const baseURL = await server.start()  console.log('serving at', baseURL, Date.now() - t1)  const axios = Axios.create({    baseURL  })  const buffers = await Promise.all(    ['family_shards.bin', 'genus_shards.bin', 'species_shards.bin'].map(      async file => {        const response = await axios.get<ArrayBuffer>(file, {          responseType: 'arraybuffer'        })        return response.data      }    )  )  console.log('got all buffers in', Date.now() - t1, 'ms')}```====="", 'Ran into the same issue when loading a single model with four weight shards, totalling to ~13MB. The tf.loadGraphModel method resolves reasonably quickly on iOS (iPhone7: 3s when developing locally, 9s when installed from testflight) but the load times on Android devices running built release apk are **much slower** leaving the app UI completely unresponsive in the meantime 😕 loadGraphModel taking: 24s on Redmi 7, 60s on nexus 5x, 78s on samsung a10 😖 @zholmes1  Could you elaborate on how you ended up using the _buffers_ array? I don’t see them used in any wayin the posted code and wanted to attempt try your version of the code. Running it I’m also running into an issue where axios returns undefined for response.data if ""arraybuffer"" responseType is specified.=====']",1
https://github.com/tensorflow/tfjs/issues/5113,Dead link in tutorial,1,closed,2021-05-24T20:31:06Z,2021-05-25T18:56:01Z,"This page:https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0This link is dead:This codelab will not go over the theory behind audio recognition models. If you are curious about that, **check out this tutorial**.","['This is fixed now, please verify.=====']",1
https://github.com/tensorflow/tfjs/issues/4337,tf.stridedSlice is not a function,3,closed,2020-12-02T16:42:15Z,2020-12-02T17:48:01Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):NA- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.7.0- Browser version:Google Chrome is up to dateVersion 87.0.4280.66 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Uncaught (in promise) TypeError: points.stridedSlice is not a function**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Earlier I was working with 2.4.0 version and it was working with that. Update to 2.7.0 has caused the this. =====', '@hkhoont Can you please provide reproduction code and model if possible.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/264,Memory Leak in toPixels(),3,closed,2018-05-04T19:44:12Z,2020-04-13T17:04:48Z,"#### TensorFlow.js version0.10.0#### Browser versionFirefox 59.0.2Chrome 66.0.3359.139#### Describe the problem or feature requestWhen using the `toPixels()` function, two extra tensors are created and are not cleaned up#### Code to reproduce the bug / link to feature requestThe following code outputs '0 0 2' to the console of a browser when ran.```html<!DOCTYPE html><html><body>  <canvas id='test' width='400' height='400' />  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.10.0""></script>  <script>  let fn = (async () => {    let canvas = document.getElementById('test'),    let count1 = tf.memory().numTensors,    let tensor = tf.fromPixels(canvas),    tensor.dispose(),    let count2 = tf.memory().numTensors,    tensor = tf.fromPixels(canvas),    await tf.toPixels(tensor, canvas),    tensor.dispose(),    let count3 = tf.memory().numTensors,    console.log(count1, count2, count3),  })(),  </script></body></html>```","['Looks like memory leaks occur in tfjs-core/src/ops/array_ops.ts at lines 637-638: ```    const min = (await img.min().data())[0],    const max = (await img.max().data())[0], ```=====', 'I am still facing  this issue . One extra tensor is being assigned in ``` tf.browser.toPixels() ``` which I am not able to dispose . Is this issue solved ? @nsthorat @davidsoergel  @rthadur =====', 'This issue can be solved by ```tf.engine().startScope()tf.browser.toPixel()tf.engine().endScope()```tested on version 1.7.0=====']",0
https://github.com/tensorflow/tfjs/issues/4988,Is real time object detection possible using TensorCamera and Cocossd ?,4,closed,2021-04-26T13:46:42Z,2021-07-14T17:19:20Z,"When I use Tensor Camera 1. It is very much laggy 2. I guess it keeps capturing each frame and thus we just get one item in prediction using the mobilenet model 3. In case we use cocossd for multiple image detection it throw error """"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead""""So How is it possible to make a real-time multiple image detection, I can see an example in flutter so is that the same possible in React Native anyway? Flutter example that I need in React Native : - https://heartbeat.fritz.ai/turning-the-mobile-camera-into-a-real-time-object-detector-with-flutter-and-tensorflow-lite-f412962b1805","['You can refer tensorflow js face landmark detection from the tensorflow website itself it might help you ,for capturing the images u can use openCV =====', 'This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow.js) since it is not a bug or feature request. There is also a larger community that reads questions there.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4988"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4988"">No</a>=====', '> > > When I use Tensor Camera> >     1. It is very much laggy> >     2. I guess it keeps capturing each frame and thus we just get one item in prediction using the mobilenet model> >     3. In case we use cocossd for multiple image detection it throw error """"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead""""> > > So How is it possible to make a real-time multiple image detection, I can see an example in flutter so is that the same possible in React Native anyway?> > Flutter example that I need in React Native : - https://heartbeat.fritz.ai/turning-the-mobile-camera-into-a-real-time-object-detector-with-flutter-and-tensorflow-lite-f412962b1805Did you find a solution?=====']",1
https://github.com/tensorflow/tfjs/issues/5179,Error: Cannot evaluate flag 'DEBUG': no evaluation function found.,9,closed,2021-06-05T07:31:21Z,2021-06-19T11:41:23Z,"**System information**- OS Platform and Distribution: **Windows 10**- Mobile device: **Mi 8 Lite running on Android 10**- TensorFlow.js installed from (npm or script link): **npm**- TensorFlow.js version: **3.7.0**Every time I tried to run the apps and import @tensorflow/tfjs, I got an error message like this in Metro.![image](https://user-images.githubusercontent.com/19814559/120883919-3d93c880-c60a-11eb-9ee8-f7fa3b9873ad.png)These are the dependencies I used (this is literally a fresh new React Native project created by react-native cli.![image](https://user-images.githubusercontent.com/19814559/120883943-6fa52a80-c60a-11eb-9165-c25a3e9cbb33.png)I ran into this problem every time I tried to run the apps, I thought by creating a new project and install tfjs first without installing any other library/dependencies will solve this, but it still the same.Any solution?","['I am getting this error too on `3.7.0`. <img width=""1048"" alt=""Screen Shot 2021-06-06 at 6 26 36 PM"" src=""https://user-images.githubusercontent.com/11457183/120934125-f7fffa00-c6f4-11eb-8b95-4cfed2765eb3.png""><img width=""423"" alt=""Screen Shot 2021-06-06 at 6 30 06 PM"" src=""https://user-images.githubusercontent.com/11457183/120934201-457c6700-c6f5-11eb-9476-041fc2088bc3.png"">- =====', 'is this solved ? I have same issue=====', 'I just have solved this issue, before you import tensorflowjs on react native . you MUST follow this configuration first https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native=====', '> I just have solved this issue, before you import tensorflowjs on react native . you MUST follow this configuration first https://github.com/tensorflow/tfjs/tree/master/tfjs-react-nativeI can make sure that I have followed those steps, and what steps did you do or you missed before?=====', 'I think I missed on react-native-fs configuration, but I forgot precisely. For the ease, I suggest you try to create new project and then follow those step first before you code your program and import tensorflow.js. It really worked for me. By the way, I use React Native CLI=====', ""I've tried that, even before you tell me. I'm using React Native CLI too.May I see your metro.config.js?====="", '> I\'ve tried that, even before you tell me. I\'m using React Native CLI too.> May I see your metro.config.js?This is my **metro.config.js**: const { getDefaultConfig } = require(\'metro-config\'), module.exports = (async () => {   const defaultConfig = await getDefaultConfig(),   const { assetExts } = defaultConfig.resolver,   return {     resolver: {       // Add bin to assetExts       assetExts: [...assetExts, \'bin\'],     }   }, })(),And just in case, if you want to see my **package.json** :{  ""name"": ""MyApp"",  ""version"": ""0.0.1"",  ""private"": true,  ""scripts"": {    ""android"": ""react-native run-android"",    ""ios"": ""react-native run-ios"",    ""start"": ""react-native start"",    ""test"": ""jest"",    ""lint"": ""eslint .""  },  ""dependencies"": {    ""@react-native-community/async-storage"": ""^1.12.1"",    ""@tensorflow/tfjs"": ""^3.7.0"",    ""@tensorflow/tfjs-react-native"": ""^0.5.0"",    ""expo-camera"": ""^11.0.3"",    ""expo-file-system"": ""^11.0.2"",    ""expo-gl"": ""^10.3.0"",    ""expo-gl-cpp"": ""^10.3.0"",    ""jpeg-js"": ""^0.4.3"",    ""react"": ""17.0.1"",    ""react-native"": ""0.64.2"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-image-picker"": ""^4.0.3"",    ""react-native-unimodules"": ""^0.13.3"",    ""whatwg-fetch"": ""^3.6.2""  },  ""devDependencies"": {    ""@babel/core"": ""^7.12.9"",    ""@babel/runtime"": ""^7.12.5"",    ""@react-native-community/eslint-config"": ""^2.0.0"",    ""babel-jest"": ""^26.6.3"",    ""eslint"": ""7.14.0"",    ""jest"": ""^26.6.3"",    ""metro-react-native-babel-preset"": ""^0.64.0"",    ""react-test-renderer"": ""17.0.1""  },  ""jest"": {    ""preset"": ""react-native""  }}=====', ""Okay so I found the problem.I tried to merge the metro.config.js from tfjs installation instruction with the existing one, so it become like this```const { getDefaultConfig } = require('metro-config'),module.exports = (async () => {  const defaultConfig = await getDefaultConfig(),  const { assetExts } = defaultConfig.resolver,  return {    transformer: {      getTransformOptions: async () => ({        transform: {          experimentalImportSupport: false,          inlineRequires: true,        },      }),    },    resolver: {      // Add bin to assetExts      assetExts: [...assetExts, 'bin'],    },  },})(),```And it gives me the errors aboveBut when I change it into```const { getDefaultConfig } = require('metro-config'),module.exports = (async () => {  const defaultConfig = await getDefaultConfig(),  const { assetExts } = defaultConfig.resolver,  return {    resolver: {      // Add bin to assetExts      assetExts: [...assetExts, 'bin'],    },  },})(),```It worked, no error shown.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5179"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5179"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4466,Failure to Convert Tensorflow Saved Model to model.json ValueError: cannot create an OBJECT array from memory buffer,11,open,2020-12-30T07:44:43Z,2021-03-08T21:50:00Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow.js installed from (npm or script link): pip installed-  keras 2.2.4-tf- tensorflow 2.1.0- TensorFlow.js version (use command below): 2.8.1- Browser version: chrome 87.0.4280.88- Tensorflow.js Converter Version: 2.8.1**Issue**_**ValueError: cannot create an OBJECT array from memory buffer**_**Describe the current behavior**I have trained a model in Tensorflow and have saved the model. I want to convert the tf_saved_model to model.json Model is created using the following code:```num_steps_2=200classifier2 = tf.estimator.DNNClassifier(    feature_columns=create_feature_columns(input_features, feature_spec),    hidden_units=[128, 64, 32])classifier2.train(train_inpf, steps=num_steps_2)#saving the modelimport shutilinputFn = tf.estimator.export.build_parsing_serving_input_receiver_fn(  tf.feature_column.make_parse_example_spec(feature_columns_needed))OUTDIR = module_pathshutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each timemodelBasePath = os.path.join(OUTDIR, ""model"")modelPath = classifier2.export_saved_model(module_path, inputFn)print(""model is saved at :"", modelPath)```I used the following command within my notebook:`!tensorflowjs_converter --input_format=tf_saved_model --skip_op_check --saved_model_tags=serve --signature_name=serving_default --strip_debug_ops=True --weight_shard_size_bytes=4194304  ""/Users/data/data/ckd/models/ckd_Dec-29-2020-00-00-00/1609297437/"" ""/Users/data/data/ckd/models/output/""`I get the following error:`2020-12-29 19:05:03.076396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-12-29 19:05:03.092798: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8ee0ce94d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-12-29 19:05:03.092818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default VersionWARNING: Logging before flag parsing goes to stderr.W1229 19:05:03.280285 4582796736 deprecation.py:506] From /Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.W1229 19:05:04.476931 4582796736 meta_graph.py:436] Issue encountered when serializing global_step.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.W1229 19:05:04.477355 4582796736 meta_graph.py:436] Issue encountered when serializing variables.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.W1229 19:05:04.477463 4582796736 meta_graph.py:436] Issue encountered when serializing trainable_variables.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.2020-12-29 19:05:04.477655: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)2020-12-29 19:05:04.477732: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session2020-12-29 19:05:04.486826: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node dnn/input_from_feature_columns/input_layer/age_indicator_1/age_lookup/hash_table/table_init/LookupTableImportV2 doesn't exist in graphW1229 19:05:04.489723 4582796736 deprecation.py:323] From /Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:388: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.Instructions for updating:This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.W1229 19:05:04.827945 4582796736 deprecation.py:323] From /Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:393: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`W1229 19:05:04.828099 4582796736 deprecation.py:323] From /Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`2020-12-29 19:05:05.153410: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize2020-12-29 19:05:05.153433: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   debug_stripper: debug_stripper did nothing. time = 0.022ms.2020-12-29 19:05:05.153437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 502 nodes (-8), 597 edges (-8), time = 6.245ms.2020-12-29 19:05:05.153441: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 498 nodes (-4), 593 edges (-4), time = 30.05ms.2020-12-29 19:05:05.153444: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (-226), 593 edges (0), time = 9.77ms.2020-12-29 19:05:05.153448: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 4.877ms.2020-12-29 19:05:05.153451: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0), 593 edges (0), time = 2.654ms.2020-12-29 19:05:05.153509: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0), 593 edges (0), time = 9.233ms.2020-12-29 19:05:05.153514: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 6.764ms.2020-12-29 19:05:05.153517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 4.599ms.2020-12-29 19:05:05.153520: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   debug_stripper: debug_stripper did nothing. time = 0.66ms.2020-12-29 19:05:05.153524: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0), 593 edges (0), time = 2.362ms.2020-12-29 19:05:05.153527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0), 593 edges (0), time = 9.562ms.2020-12-29 19:05:05.153707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 6.718ms.2020-12-29 19:05:05.153713: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 4.525ms.2020-12-29 19:05:05.153716: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0), 593 edges (0), time = 2.69ms.2020-12-29 19:05:05.153719: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0), 593 edges (0), time = 9.015ms.2020-12-29 19:05:05.153723: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 6.161ms.2020-12-29 19:05:05.153869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0), 593 edges (0), time = 4.296ms.2020-12-29 19:05:05.217058: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize2020-12-29 19:05:05.217086: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   remapper: Graph size after: 265 nodes (-7), 586 edges (-7), time = 2.129ms.2020-12-29 19:05:05.217091: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 265 nodes (0), 586 edges (0), time = 7.309ms.2020-12-29 19:05:05.217095: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 265 nodes (0), 586 edges (0), time = 5.159ms.2020-12-29 19:05:05.217099: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 265 nodes (0), 586 edges (0), time = 3.959ms.2020-12-29 19:05:05.217102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   remapper: Graph size after: 265 nodes (0), 586 edges (0), time = 2.478ms.2020-12-29 19:05:05.217106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 265 nodes (0), 586 edges (0), time = 8.296ms.2020-12-29 19:05:05.217193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 265 nodes (0), 586 edges (0), time = 6.055ms.2020-12-29 19:05:05.217205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 265 nodes (0), 586 edges (0), time = 4.521ms.Traceback (most recent call last):  File ""/Users/taposh/anaconda3/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 813, in pip_main    main([' '.join(sys.argv[1:])])  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 817, in main    convert(argv[0].split(' '))  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 804, in convert    weight_shard_size_bytes, metadata_map)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 533, in _dispatch_converter    metadata=metadata_map)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 652, in convert_tf_saved_model    metadata=metadata)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 210, in optimize_graph    initializer_graph_def, metadata=metadata)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 266, in extract_weights    global_manifest = extract_const_nodes(graph_def.node)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 231, in extract_const_nodes    'data': graph_rewrite_util.values_from_const(const)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/graph_rewrite_util.py"", line 62, in values_from_const    tensor_value = tensor_util.MakeNdarray(input_tensor)  File ""/Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 598, in MakeNdarray    dtype=dtype).copy().reshape(shape))ValueError: cannot create an OBJECT array from memory buffer`Added this on StackOverflow for more eyes:https://stackoverflow.com/questions/65501730/tensorflowjs-converter-giving-valueerror-cannot-create-an-object-array-from-mem**Describe the expected behavior**expected model.json and weight files to be created in output location","['Any work around for this? =====', '@taposh Thank you for reporting this issue. It would be very helpful if you can provide the full code to allow us to reproduce the problem. If you can provide a colab or repl.it example would be great. thanks.=====', '@pyu10055  here is my code in colab https://colab.research.google.com/drive/1cPmtlV7LMGAJ16WHBEeRko37zBNwWlda?usp=sharing=====', '@pyu10055  were you able to review this?=====', '@pyu10055  @rthadur any update on this bug !=====', 'Thank you for sharing the colab, I think the input dataset is missing:```---------------------------------------------------------------------------FileNotFoundError                         Traceback (most recent call last)<ipython-input-10-6f3b48cf182c> in <module>()      6 csv_path = \'sample_data/train.csv\'      7 ----> 8 data = pd.read_csv(csv_path)      9 # Set the column in the dataset you wish for the model to predict     10 label_column = \'class\'4 frames/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self, src, **kwds)   2008         kwds[""usecols""] = self.usecols   2009 -> 2010         self._reader = parsers.TextReader(src, **kwds)   2011         self.unnamed_cols = self._reader.unnamed_cols   2012 pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source()FileNotFoundError: [Errno 2] No such file or directory: \'sample_data/train.csv\'```=====', 'The uploaded file gets removed on Colab when runtime is recycled. I will add code to read from my drive. =====', '@pyu10055  code in colab is updated to read file directly from github. You will need to pass to tensorflow converter the file name manually since its on command/shell as opposed to in python.=====', '@pyu10055 any update=====', '@pyu10055  any update?=====', '@taposh Sorry for the delay, I took a close look at your colab, I think the problem is caused by use tf.example as the input of the inference graph, which is not supported by TFJS right now. You need to create a signature that has input with the node after the tf.example has been processed. Here are some example code that might help you to create custom input function:```def _tfjs_serving_input_fn(feature_spec):  """"""Creates an input function for TFJS-node serving.""""""  def raw_transforming_serving_input_fn():    placeholder_features = {}    for name, value in feature_spec.items():      batch_dim = tf.TensorShape([None])      placeholder_features[name] = tf.compat.v1.placeholder(          dtype=value.dtype,          shape=batch_dim.concatenate(value.shape),          name=name)    return tf.estimator.export.ServingInputReceiver(placeholder_features,                                                    placeholder_features)  return raw_transforming_serving_input_fnexport_path = classifier.export_saved_model(""trained-model"", _tfjs_serving_input_fn(tf.feature_column.make_parse_example_spec(my_feature_columns)))```=====']",1
https://github.com/tensorflow/tfjs/issues/5705,Landmarks not placing correctly on back camera,4,closed,2021-10-09T00:21:58Z,2021-12-13T17:39:54Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  Copied from [here](https://github.com/jinjingforever/tfjs-react-native-pose-detection/blob/main/App.tsx)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  macOS Big Sur 11.2.1- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone SE- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0- Browser version:Chrome 94- Tensorflow.js Converter Version:3.9.0**Describe the current behavior**When using back camera, landmarks are not placing correctly. They are placing off to the side. When camera is not pointed towards a person, landmarks are being placed randomly. Using the front camera works without any issue.![IMG_8438](https://user-images.githubusercontent.com/47552416/136633167-31ace200-4404-45bb-bc60-d643f3fa857d.PNG)![IMG_8455](https://user-images.githubusercontent.com/47552416/136636950-972865e4-e1cb-4d3d-b344-ba60299b5f56.jpg)**Describe the expected behavior**Landmarks expected to be placed in correct position and only when facing a person**Code**```import '@mediapipe/pose'import ""@tensorflow/tfjs-backend-webgl"",import React, { useEffect, useState, useRef } from 'react',import { StyleSheet, Text, View, Dimensions, Platform } from 'react-native',import { Camera } from 'expo-camera',import * as tf from '@tensorflow/tfjs',import * as posedetection from '@tensorflow-models/pose-detection',import { cameraWithTensors } from '@tensorflow/tfjs-react-native',import Svg, { Circle } from 'react-native-svg',import { ExpoWebGLRenderingContext } from 'expo-gl',const TensorCamera = cameraWithTensors(Camera),const WIN_WIDTH = Dimensions.get('window').width,const CAM_TEXTURE_WIDTH =  Platform.OS === 'ios'    ? Platform.isPad && WIN_WIDTH === 768      ? 720      : 1080    : 1600,const CAM_TEXTURE_HEIGHT =  Platform.OS === 'ios'    ? Platform.isPad && WIN_WIDTH === 768      ? 1280      : 1920    : 1200,const USE_PRESET = false,const CAM_TEXTURE_INITIAL_SIZE = 2000,const CAM_PREVIEW_WIDTH = Dimensions.get('window').width,const CAM_PREVIEW_HEIGHT =  CAM_PREVIEW_WIDTH / (Platform.OS === 'ios' ? 9 / 16 : 3 / 4),// The score threshold for pose detection results.const MIN_KEYPOINT_SCORE = 0.3,// The size of the resized output from TensorCamera.//// For movenet, the size here doesn't matter too much because the model will// preprocess the input (crop, resize, etc).const OUTPUT_TENSOR_WIDTH = 240,const OUTPUT_TENSOR_HEIGHT = 320,// Whether to auto-render TensorCamera preview.const AUTO_RENDER = false,export default function ScanScreen() {  const cameraRef = useRef(null),  const [camTextureSizeAvailable, setCamTextureSizeAvailable] =    useState(USE_PRESET),  const [camTextureWidth, setCamTextureWidth] = useState(    USE_PRESET ? CAM_TEXTURE_WIDTH : CAM_TEXTURE_INITIAL_SIZE  ),  const [camTextureHeight, setCamTextureHeight] = useState(    USE_PRESET ? CAM_TEXTURE_HEIGHT : CAM_TEXTURE_INITIAL_SIZE  ),  const [outputTensorWidth, setOutputTensorWidth] = useState(    USE_PRESET ? OUTPUT_TENSOR_WIDTH : CAM_TEXTURE_INITIAL_SIZE  ),  const [outputTensorHeight, setOutputTensorHeight] = useState(    USE_PRESET ? OUTPUT_TENSOR_HEIGHT : CAM_TEXTURE_INITIAL_SIZE  ),  const [tfReady, setTfReady] = useState(false),  const [model, setModel] = useState<posedetection.PoseDetector>(),  const [poses, setPoses] = useState<posedetection.Pose[]>(),  const [fps, setFps] = useState(0),  useEffect(() => {    async function prepare() {      // Camera permission.      await Camera.requestPermissionsAsync(),      // Setup tfjs.      await tf.setBackend('rn-webgl'),      await tf.ready(),      // Load movenet model.      const model = await posedetection.createDetector(        posedetection.SupportedModels.MoveNet,        {          modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING,          enableSmoothing: true,          // minPoseScore: 1,        }      ),      setModel(model),      // Ready!      setTfReady(true),    }    prepare(),  }, []),  const calculateCameraTextureSize = async (    images: IterableIterator<tf.Tensor3D>,    updatePreview: () => void,    gl: ExpoWebGLRenderingContext  ) => {    console.log('Start calculating camera texture size'),    const loop = async () => {      // Wait and throttle.      await new Promise<void>((resolve) => {        setTimeout(() => {          resolve(),        }, 200),      }),      // Get the tensor.      const imageTensor = images.next().value as tf.Tensor3D,      // Get the tensor values.      const imageTensorVals = imageTensor.arraySync(),      tf.dispose([imageTensor]),      // A quick check to make sure the camera feed is ready (not black).      //      // Only check 50 pixels in the first row.      let ready = false,      for (let i = 0, i < 50, i++) {        if (!checkPixelsMatch(imageTensorVals[0][i], [0, 0, 0])) {          ready = true,          break,        }      }      if (ready) {        // Check 10 rows and columns for edges, and store the possible        // edges in the arrays below. Eventually we will use the max values        // as the final edges.        const horizontalEdges = [],        const verticalEdges = [],        const sampleCount = 10,        for (let sampleIndex = 0, sampleIndex < sampleCount, sampleIndex++) {          // Focus on the first 500 pixels to get sample rows/columns..          const rcIndex = Math.floor((500 / sampleCount) * sampleIndex),          let curHorizontalEdge = -1,          let curVerticalEdge = -1,          // In the current sample row/column, check 50 consecutive pixels          // to see if they have the same color or not.          const pixelsToCheck = 50,          for (            let anchorPixelIndex = 200,            anchorPixelIndex < CAM_TEXTURE_INITIAL_SIZE - pixelsToCheck - 1,            anchorPixelIndex++          ) {            const curRowAnchorPixel =              imageTensorVals[rcIndex][anchorPixelIndex],            const curColAnchorPixel =              imageTensorVals[anchorPixelIndex][rcIndex],            let foundHorizontalEdge = true,            let foundVerticalEdge = true,            for (let i = 1, i <= pixelsToCheck, i++) {              const rowPixelToCheck =                imageTensorVals[rcIndex][anchorPixelIndex + i],              const colPixelToCheck =                imageTensorVals[anchorPixelIndex + i][rcIndex],              if (!checkPixelsMatch(rowPixelToCheck, curRowAnchorPixel)) {                foundHorizontalEdge = false,              }              if (!checkPixelsMatch(colPixelToCheck, curColAnchorPixel)) {                foundVerticalEdge = false,              }              if (!foundHorizontalEdge && !foundVerticalEdge) {                break,              }            }            if (foundHorizontalEdge && curHorizontalEdge < 0) {              curHorizontalEdge = anchorPixelIndex,            }            if (foundVerticalEdge && curVerticalEdge < 0) {              curVerticalEdge = anchorPixelIndex,            }            if (curHorizontalEdge > 0 && curVerticalEdge > 0) {              break,            }          }          horizontalEdges.push(curHorizontalEdge),          verticalEdges.push(curVerticalEdge),        }        // Find the edges.        const horizontalEdge = Math.max(...horizontalEdges) + 1,        const verticalEdge = Math.max(...verticalEdges) + 1,        console.log(`Camera texture size: ${horizontalEdge}x${verticalEdge}`),        setCamTextureWidth(horizontalEdge),        setCamTextureHeight(verticalEdge),        setOutputTensorWidth(OUTPUT_TENSOR_WIDTH),        setOutputTensorHeight(OUTPUT_TENSOR_HEIGHT),        setCamTextureSizeAvailable(true),      } else {        requestAnimationFrame(loop),      }    },    loop(),  },  const handleCameraStream = async (    images: IterableIterator<tf.Tensor3D>,    updatePreview: () => void,    gl: ExpoWebGLRenderingContext  ) => {    const loop = async () => {      // Get the tensor and run pose detection.      const imageTensor = images.next().value as tf.Tensor3D,      const startTs = Date.now(),      const poses = await model!.estimatePoses(        imageTensor,        undefined,        Date.now()      ),      const latency = Date.now() - startTs,      setFps(Math.floor(1000 / latency)),      setPoses(poses),      tf.dispose([imageTensor]),      // Render camera preview manually when autorender=false.      if (!AUTO_RENDER) {        updatePreview(),        gl.endFrameEXP(),      }      requestAnimationFrame(loop),    },    loop(),  },  const renderPose = () => {    if (poses != null && poses.length > 0) {      const keypoints = poses[0].keypoints        .filter((k) => (k.score ?? 0) > MIN_KEYPOINT_SCORE)        .map((k) => {          // Flip horizontally on android.          const x = Platform.OS === 'android' ? outputTensorWidth - k.x : k.x,          const y = k.y,          return (            <Circle              key={`skeletonkp_${k.name}`}              cx={(x / outputTensorWidth) * CAM_PREVIEW_WIDTH}              cy={(y / outputTensorHeight) * CAM_PREVIEW_HEIGHT}              r='4'              strokeWidth='2'              fill='#4f4bcb'              stroke='white'            />          ),        }),      return <Svg style={styles.svg}>{keypoints}</Svg>,    } else {      return <View></View>,    }  },  const checkPixelsMatch = (p1: number[], p2: number[]) => {    return p1[0] === p2[0] && p1[1] === p2[1] && p1[2] === p2[2],  },  return (    <View style={styles.container}>      <View        style={{          ...styles.camContainer,          opacity: tfReady && camTextureSizeAvailable ? 1 : 0,        }}      >        {/* The hidden camera for figuring out the correct texture size */}        {!camTextureSizeAvailable && (          <TensorCamera            ref={cameraRef}            style={styles.camera}            autorender={true}            type={Camera.Constants.Type.back}            // tensor related props            cameraTextureWidth={camTextureWidth}            cameraTextureHeight={camTextureHeight}            resizeWidth={outputTensorWidth}            resizeHeight={outputTensorHeight}            resizeDepth={3}            onReady={calculateCameraTextureSize}             flashMode='auto'            useCustomShadersToResize={false}          />        )}        {/* The main camera */}        {camTextureSizeAvailable && tfReady && (          <TensorCamera            ref={cameraRef}            style={styles.camera}            autorender={AUTO_RENDER}            type={Camera.Constants.Type.back}            // tensor related props            cameraTextureWidth={camTextureWidth}            cameraTextureHeight={camTextureHeight}            resizeWidth={outputTensorWidth}            resizeHeight={outputTensorHeight}            resizeDepth={3}            onReady={handleCameraStream}             flashMode='auto'            useCustomShadersToResize={false}          />        )}        {renderPose()}        {/* {renderFps()} */}      </View>      {(!tfReady || !camTextureSizeAvailable) && (        <View style={styles.loadingMsg}>          <Text>Loading...</Text>        </View>      )}    </View>  ),}```","['Hi @eledahl, thank you for the report. tfjs-react-native 0.7.0 should fix this problem. You no longer need to specify camera texture size. Please see an updated example [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection). Thank you!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5705"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5705"">No</a>=====', 'Hi @jinjingforever , thank you for the answer and methodology. Do you know how one can implement the skeleton or is there any thought of implementing that. Perhaps the performance goes down ?=====', ""Hi @pmahan00 we have a posenet/movenet demo app and you can see how it draws the skeleton [here](https://github.com/tensorflow/tfjs-models/blob/master/posenet/demo/demo_util.js#L100). The app is not in react native but you can use the similar techniques. It shouldn't affect the performance too much. Hope it helps! =====""]",1
https://github.com/tensorflow/tfjs/issues/5063,React Native with TypeScript tfjs-models/universal-sentence-encoder error on model load,1,open,2021-05-12T16:19:41Z,2021-05-14T14:03:23Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 14.5- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 12- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: n/a- Tensorflow.js Converter Version: @tensorflow/tfjs-react-native@0.5.0**Describe the current behavior**I'm trying to use tfjs-models/universal-sentence-encoder within a React Native app by following these instructions. However, I get the following error when I try to load the model:`TypeError: undefined is not an object (evaluating '_universalSentenceEncoder.default.load'`**Describe the expected behavior**It should load the use model, compute embeddings, and print them to the console.**Standalone code to reproduce the issue**```import React, { useEffect, useState } from 'react',require('@tensorflow/tfjs'),const use = require('@tensorflow-models/universal-sentence-encoder'),export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            'Hello.',            'How are you?'          ],          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2, 512].            embeddings.print(true /* verbose */),          }),        }),      }      catch (err) {        console.log(err),      }    },  }, []),````","['This was fixed via some code changes and downgrading @tensorflow/tfjs to 3.0.0. See [this post](https://stackoverflow.com/questions/67507110/react-native-with-typescript-tfjs-models-universal-sentence-encoder-error-on-mod/67514193#67514193) for details on change. Code changed to the following:```import React, { useEffect, useState } from \'react\',import ""@tensorflow/tfjs-react-native"",import * as tf from \'@tensorflow/tfjs\',import * as use from \'@tensorflow-models/universal-sentence-encoder\',export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            await tf.ready(),            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            \'Hello.\',            \'How are you?\'          ],          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2, 512].            embeddings.print(true /* verbose */),          }),        }),      }      catch (err) {        console.log(`ERROR: ${err}`),      }    },  }, []),```I am, however, still only able to produce valid embeddings when running the code on an emulator. Running on physical iOS device produces an embeddings vector full of NaNs:`[javascript] Tensor    [[NaN, NaN, NaN, ..., NaN, NaN, NaN],     [NaN, NaN, NaN, ..., NaN, NaN, NaN]]`On an iOS emulator, the vector is full of valid floating point values.iOS Emulator: iPhone 11, iOS 14.5Physical iPhone: iPhone 11, iOS 14.5.1=====']",1
https://github.com/tensorflow/tfjs/issues/5800,"When in webgl backend,  tf.isNaN() produces incorrect results",8,open,2021-11-01T08:23:59Z,2021-11-29T19:25:45Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 3.11.0- Browser version:   Google Chrome  93.0.4577.63- Tensorflow.js Converter Version:- **Describe the current behavior**tf.isNaN() has different results when running on different backends. For example, for the following code:```     const y = tf.tensor1d([Infinity, .5, 4, -38.8, NaN]),     y.isNaN().print(),```In **webgl backend**, the output is ```    Tensor    [false, false, false, false, false]```In **cpu backend**, the output is ```    Tensor    [false, false, false, false, true]```In **wasm backend**, get the error is ```Uncaught (in promise) Error: Kernel 'IsNan' not registered for backend 'wasm'```**These results indicate that tf.isNaN()cannot correctly recognize NaN in webgl backend.****Describe the expected behavior**According to the official documentation，tf.isNaN(x) RReturns which elements of x are NaN.Therefore, the correct result should be```    Tensor    [false, false, false, false, true]```**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.the code to reproduce the problem is```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>Title</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""> </script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl""></script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js""></script></head><body><script> async function nan_test() {     const y = tf.tensor1d([Infinity, .5, 4, -38.8, NaN]),       tf.setBackend(""webgl""),     await tf.ready(),     console.log(""webgl backend result:""),     y.isNaN().print(),     tf.setBackend(""cpu""),     await tf.ready(),     console.log(""cpu backend result:""),     y.isNaN().print(),     tf.setBackend(""wasm""),     await tf.ready(),     console.log(""wasm backend result:""),     y.isNaN().print(),    }    nan_test(),</script></body></html>```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attachedthe code to reproduce the problem[nan_test.zip](https://github.com/tensorflow/tfjs/files/7451685/nan_test.zip).","['I tried in lates chrome browser(MAC OS) and was not able to see above behavior ![image](https://user-images.githubusercontent.com/43972606/139707905-c6f8ae3c-60c1-4bfe-b35a-073a58afa264.png)=====', ""@rthadur Thank you for your prompt reply.  I did the test again, and still got incorrect results on **Ubuntu** (ThinkPad and Dell).  I also tested it on Mac OS and Windows, and I really can't see this behavior. This is strange, what is the reason?The output of chrome on Ubuntu is as follows:![image](https://user-images.githubusercontent.com/68681463/139823893-6d9d3d10-15a5-419c-8ca6-a836f29003fd.png)The same error appears on Tensorflow.js API web page (https://js.tensorflow.org/api/latest/?hl=zh-cn#isNaN)![image](https://user-images.githubusercontent.com/68681463/139839444-55aca7ac-ea51-420d-a05e-bd121499e1fb.png)====="", 'Can you try run this https://js.tensorflow.org/debug/ and provide screenshot ?=====', 'This is screenshot:![image](https://user-images.githubusercontent.com/68681463/139906826-4084707e-32d6-4ecf-b0ae-3c50b8c852bc.png)=====', 'can you please upgrade to Chrome latest and try ?=====', 'I have updated Chrome to version 95.0.4638.69 and the error still exists. In addition, there is the same error on firefox. result on chrome:![image](https://user-images.githubusercontent.com/68681463/139910674-87055484-340c-4ee3-a5d4-6d693b4debd0.png)result on Firefox:![image](https://user-images.githubusercontent.com/68681463/139910363-8ab8ebc6-b30e-4957-96cb-48250ce313b7.png)The debug information is as follows![Selection_139](https://user-images.githubusercontent.com/68681463/139910003-bfa858c6-1076-44db-b814-7848697c9016.png)=====', '![image](https://user-images.githubusercontent.com/68681463/143088806-027cedbc-eea4-4c5d-b58a-86d1bb1eab8c.png)Thank you for fixing this issue, but I have some doubts. First , I feel that the current changes cannot explain the incorrect results under Linux. In your fix,  delete ` return (val > 0.0 || val < 0.0) ? false : val != 0.0,` and add `return val != val,` . However, in Linux, when ` val=NaN`, the result of `(val > 0.0 || val < 0.0) ? false : val != 0.0` a is TRUE, as expected . So is this issue caused by the insufficient webgl support under Linux? as stated by ahmedsabie “Modify the implementation to also support UHD Graphics on Linux” .  Second, I would like to ask if this fix will be updated to the future version？ Thanks again.=====', ""The issue is caused by WebGL being optimized and supported slightly differently across different GPUs so behavior isn't always consistent. The issue is more of an Intel UHD Graphics issue rather than a Linux one. Yes the fix will be added to the next release once merged!  =====""]",1
https://github.com/tensorflow/tfjs/issues/3734,"Conversion to tfjs: make_image_classifier gives correct results, notebook version incorrect ones?",2,closed,2020-08-03T11:43:00Z,2020-08-11T18:27:16Z,"I have used `make_image_classifier` to make my own classifier and converted that to Tensorflow.js with success, giving me confidence rates like A: 0.67XXX, B: 0.10XXX, C: 0.23XXXX (together 1.0)I want to do the same with the corresponding notebook to have more control (https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb)but if I use it with default settings, I get weird predictions after conversion then, like:A: 0.09XXX, B: 2.69XXX, C: 0.77XXXXI wonder why this is happening - how can I use the notebook and convert succesfully?For both I used this conversion format from https://github.com/tensorflow/tfjs/tree/master/tfjs-converter: tensorflowjs_converter \    --input_format=tf_saved_model \    --output_format=tfjs_graph_model \    --signature_name=serving_default \    --saved_model_tags=serve \    /mobilenet/saved_model \    /mobilenet/web_model","['@vjuss,Transferred the issue to this (TF JS) Repository, as the issue is related to TF JS. Thanks!@rthadur,PTAL.  =====', 'Closing this issue to better track in #3717=====']",0
https://github.com/tensorflow/tfjs/issues/3716,Tensor.clipByValue does not work for  tensors of dtype int32 in Node backend,2,closed,2020-08-03T07:35:35Z,2020-08-18T18:04:55Z,"TensorFlow.js version 2.0.1node version 12.13.0backend: `""tensorflow""`OS: Windows 10Issue: Tensor.clipByValue does not work for  tensors of dtype `int32`. Code to reproduce issue:```const x = tf.tensor([-1, 2, -3, 4], undefined, 'int32'),x.clipByValue(-2, 4).print(),```This throws an error with the stack trace:```Thrown:Error: Invalid TF_Status: 3Message: cannot compute Minimum as input #1(zero-based) was expected to be a int32 tensor but is a float tensor    at NodeJSKernelBackend.executeSingleOutput (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\dist\nodejs_kernel_backend.js:192:43)    at NodeJSKernelBackend.minimum (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\dist\nodejs_kernel_backend.js:558:21)    at NodeJSKernelBackend.clip (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\dist\nodejs_kernel_backend.js:636:25)    at ENGINE.runKernelFunc.x (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:6882:27)    at C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3229:55    at C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3075:22    at Engine.scopedRun (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3085:23)    at Engine.tidy (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3074:21)    at kernelFunc (C:\Users\Ryan\projects\pysc2\node_modules\@tensorflow\tfjs-node\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3229:29)```Please note that it appears to work just fine in the browser using the `webgl` backend.","[""I'm not sure exactly where the issue is in the code base, but just writing this helper method gets me around the issue for now-```function clipByValue(x, min, max) {  const minT = tf.fill(x.shape, min, x.dtype),  const maxT = tf.fill(x.shape, max, x.dtype),  return x.where(x.greaterEqual(min), minT).where(x.lessEqual(max), maxT),}```I noticed that when you rely on broadcasting to add a scalar value to an existing tensor, the resulting tensor dtype can change. For example:```const x = tf.tensor([-1, 2, -3, 4], undefined, 'int32'),const y = x.add(4), // tensor y has dtype float32```I think this may have something to do with the issue I'm experiencing.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3716"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3716"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/501,Move TFJSBackend to a factory with provider,0,closed,2018-07-10T19:04:19Z,2018-07-25T18:18:45Z,The current constructor/destructor for TFJSBackend is currently public. Those functions should become private and move to a factory to provide to the backend.,[],0
https://github.com/tensorflow/tfjs/issues/4839,"Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D.",0,open,2021-03-19T16:54:26Z,2021-06-03T16:15:39Z,"When using Xception in TensorflowJS, SeparableConv2D gives this error! If it's invalid then how am I supposed to use Xception architecture that I have already trained from Keras.applications.Any idea how to fix this issue!?![Capture](https://user-images.githubusercontent.com/48760936/111815396-9ef8a500-8901-11eb-9cd5-ad8b3bd13d06.PNG)",[],0
https://github.com/tensorflow/tfjs/issues/5192,tfjs-layers bundles contain a copy of tfjs-core,1,open,2021-06-08T15:16:24Z,2021-06-08T19:56:25Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): Github- TensorFlow.js version (use command below): 11ea58c8fe2ba94d6cb27d9714a5941601ae62a7- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**tfjs-layers bundles contain a copy of tfjs-core.Looking at the node bundle, we see that it includes the full definition for the `Tensor` class (search for `class Tensor`), which is defined in tfjs-core. I'm pretty sure this should not be bundled in tfjs-layers. Strangely, the node bundle still `require`s tfjs-core in `var tfc = require('@tensorflow/tfjs-core'),`. **Describe the expected behavior**tfjs-core should be external to the tfjs-layers bundle. None of the classes defined in tfjs-core should be duplicated in the tfjs-layers bundle.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.All commands run in `tfjs-layers`:To enable visualization for all tfjs-layers bundles, replace `visualize` with `true` in the rollup config:`sed -i 's/if (visualize)/if (true)/g' rollup.config.js`Then, run `yarn build-deps && yarn build-npm` to build the bundles.Then, open the generated visualization website at `dist/tf-layers.*.html`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""This is caused by including gradients from tfjs-core's `dist` directory in the layers bundle. The gradients reference other tfjs-core files in `dist`, like `tensor.js`, and these are redundant with importing the `tfjs-core` bundle itself.To keep bundle size low, `register_all_gradients` is not actually included in the `tf-core*.js` bundles, so the import from `dist` is required. Should gradients be in their own bundle (which would depend on tfjs-core as an external dependency)? Is it possible to factor them out? If so, then tfjs-layers could include the gradients bundle instead of importing them from dist. The bundle would know that `tfjs-core` is external, and wouldn't include any extra copies of `Tensor` or other core classes / objects from `tfjs-core`.=====""]",1
https://github.com/tensorflow/tfjs/issues/4659,[Feature] Auto check for version compatibility when individual package is loaded,4,open,2021-02-09T05:23:04Z,2021-06-29T01:35:37Z,"**System information**Browser: Chrome `88.0.4324.96`OS: macOS `10.15.7`Copying and pasting the [Via a script tag](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md#via-a-script-tag) section of the WASM readme throws the following errors:```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.4.0/dist/tf.min.js"" type=""text/javascript""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js""></script><script>tf.setBackend('wasm').then(() => {}),</script>``````kernel_registry.ts:111 Uncaught Error: The kernel 'undefined' for backend 'wasm' is already registered    at Object.c [as registerKernel] (kernel_registry.ts:111)    at register_all_kernels.ts:200    at tf-backend-wasm.js:20    at tf-backend-wasm.js:21``````engine.ts:235 Uncaught (in promise) Error: Backend name 'wasm' not found in registry    at t.<anonymous> (engine.ts:235)    at tf.min.js:2    at Object.next (tf.min.js:2)    at tf.min.js:2    at new Promise (<anonymous>)    at r (tf.min.js:2)    at t.setBackend (engine.ts:233)    at Object.t.setBackend (globals.ts:276)    at index.html:3```","[""you're force-loading `tfjs` 1.4.0 while `wasm` is loaded without version string, so defaults to latest 3.0.0 - no chance those two can work together.====="", 'Ah, that makes sense, thanks! Is there a way to catch that incompatible versions of `tfjs*` are being used and surface an error that reflects that?=====', ""Each module has its own version string, so it should be easy to compare if major version matches or not.  But that's a feature request for devs.====="", '@phoenix-meadowlark That is a great idea, we will look into that.=====']",1
https://github.com/tensorflow/tfjs/issues/897,Calling executeAsync more than once fails,5,closed,2018-11-10T06:57:28Z,2018-11-16T17:23:08Z,"#### TensorFlow.js version0.13.4#### Browser versionChrome 70#### Describe the problem or feature requestAny calls after the first to executeAsync will throw the error  `Error: Tensor is disposed.`. This happens when using the same input tensor, or a new one. I'm using a pre-trained object detection model converted using `tfjs-converter`. The model works fine, but only once.#### Code to reproduce the bug / link to feature request```javascriptconst MODEL_URL = '/coco_model/web_model/tensorflowjs_model.pb',const WEIGHTS_URL = '/coco_model/web_model/weights_manifest.json',tf.loadFrozenModel(MODEL_URL, WEIGHTS_URL).then(async (model) => {    const image_tensor = tf.expandDims(tf.fromPixels(inputVideoElement), 0)    const result = await model.executeAsync({ image_tensor }),    const result2 = await model.executeAsync({ image_tensor }), // <- ERROR})```","[""My guess is that some intermediate tensors are being disposed of - but I'm not sure how to prevent that. I can't use `tidy()` around `executeAsync()`, nor can I call `keep()` on tensors within a loaded model.Here's the model used: http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gzAnd conversion command:```tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='num_detections,detection_classes,detection_boxes,detection_scores' --saved_model_tags=serve coco_model/frozen_inference_graph.pb coco_model/web_model```====="", ""After digging through Github, I've found that sticking `await model.load()` before every call to `model. executeAsync` will avoid the issue. Unfortunately `model.load()` takes more than twice as long as the actual execution... I shouldn't need to reload the whole model every time, right?====="", ""I can confirm that this error doesn't occur in version `0.13.1`, but does in `0.13.4`. Definitely a regression.====="", '@t-mullen looks like the weights tensor might have been disposed prematurely, can you confirm if it works on 0.13.3? I notice this only happens on 0.13.4. Thanks!=====', 'Works, thanks! 👍 =====']",0
https://github.com/tensorflow/tfjs/issues/5345,Broadcast over complex Values on CPU backend does not work well,7,open,2021-07-19T10:18:24Z,2021-09-27T22:38:29Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Professionnal (build 19041.1110)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not on Mobile device, but here are the laptop's specs :    - Intel Core i7-10875H CPU 2.30GHz    - 32Go RAM    - Operating System x64- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.7.0- Browser version: -- Tensorflow.js Converter Version: -**Describe the current behavior**While trying to use some ""simple"" operations on a complex tensor such as add, multiply or substract, only 1 over 2 term of the tensor is actually computed, the second is marked as NaN + NaNj.Example : const test = tf.complex([1, 1, 1, 2, 3, 1], [1, 4, 5, 6, 8, 1]),test.mul(2).print() // [2 + 2j, NaN + NaNj, 2 + 10j, NaN + NaNj, 6 + 16j, NaN + NaNj]test.add(1).print() // [2 + 1j, NaN + NaNj, 2 + 5j, NaN + NaNj, 4 + 8j, NaN + NaNj]However if I use a Tensor of the same size from the complex one to execute this operation, the problem does not appear anymore.**Describe the expected behavior**When using a simple digit instead of a tensor of the same size, the operation should be computed properly.Example :const test = tf.complex([1, 1, 1, 2, 3, 1], [1, 4, 5, 6, 8, 1]),test.mul(2).print() // [2 + 2j, 2 + 8j, 2 + 10j, 4+ 12j, 6 + 16j, 2+ 2]**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.in node console :const tf = require(""@tensorflow/tfjs"")const test = tf.complex([1, 1, 1, 2, 3, 1], [1, 4, 5, 6, 8, 1]),test.mul(2).print()**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['I tried on tfjs [version](https://js.tensorflow.org/api/3.7.0/) @3.7.0 it is working as expected , please see below , ![image](https://user-images.githubusercontent.com/43972606/126213900-82eee957-371c-4811-a054-7baab19a2308.png)=====', 'So it must be a node console or NodeJS problem, because when I do it in the console, the problem is here.![image](https://user-images.githubusercontent.com/14946801/126972920-3dc0ac7a-fd46-42c0-9446-cb6603c598ab.png)=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'This is working as expected on console as well , please check![image](https://user-images.githubusercontent.com/43972606/128402337-3bbcbb1f-face-4b19-9d6f-fad4e5cd15d2.png)=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5345"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5345"">No</a>=====', ""Hello,I ran several tests to see where that would go wrong and obtained these results :- In the browser (backends webgl) with tensorflow editor or console in the navigator -> works fine- With modules @tensorflow/tfjs-node | @tensorflow/tfjs-node-gpu with node.js (script or console) -> works fine- In the browser with a **cpu** backend the problem occurs (@tensorflow/tfjs) (see image below)![image](https://user-images.githubusercontent.com/14946801/134887376-dd5aa703-10d5-47d4-8947-dec0d1c4489e.png)- In a **node console**, importing just **@tensorflow/tfjs** that registers a **cpu** backend the problem occurs (see image below)![image](https://user-images.githubusercontent.com/14946801/134887148-0e58b730-673c-4666-a23b-939a0d243fb1.png)As shown above, the broadcast operation (and probably other operations as well) on complex numbers don't work as expected when using the vanilla backend.Thank you for your understanding.=====""]",1
https://github.com/tensorflow/tfjs/issues/5301,error on mobile devices,7,closed,2021-07-08T12:22:53Z,2021-07-16T07:01:07Z,"**System information**- OS Platform and Distribution: Android 11- Mobile device : Samsung galaxy S10, iphone 7- TensorFlow.js installed from npm:- TensorFlow.js 3.7.0:- tfjs-tflite.js 0.0.1-alpha.4:- Browser version: Chrome 91**Describe the current behavior**- tflite_web_api_cc_simd.js:9 Uncaught (in promise) RuntimeError: abort(undefined). Build with -s ASSERTIONS=1 for more info.    at abort (tflite_web_api_cc_simd.js:9)    at _abort (tflite_web_api_cc_simd.js:9)    at tflite_web_api_cc_simd.wasm:0x188bf    at tflite_web_api_cc_simd.wasm:0x1806fa    at tflite_web_api_cc_simd.wasm:0x181025    at tflite_web_api_cc_simd.wasm:0x181123    at tflite_web_api_cc_simd.wasm:0x26afd3    at tflite_web_api_cc_simd.wasm:0x28262f    at tflite_web_api_cc_simd.wasm:0x28268b    at tflite_web_api_cc_simd.wasm:0x33555**Standalone code to reproduce the issue**import { loadTFLiteModel, TFLiteModel, setWasmPath, getWasmFeatures } from '@tensorflow/tfjs-tflite',setWasmPath('tflite-wasms/'),","[""Hi @clancyGruver,Thanks for the report! I tried the following code on my Android 11 emulator with Chrome 91 (SIMD enabled) and it seems to be working fine ([screenshot](https://drive.google.com/file/d/1HRrbWqP9Dx83ddEeCQ0XJa7aKrKi_Gmz/view?usp=sharing)). Maybe there was more code after `setWasmPath` you didn't post? (looks like the error occurred when you were trying to load a model?).```js// Import @tensorflow/tfjs-tflite.import {getWasmFeatures, loadTFLiteModel, setWasmPath} from '@tensorflow/tfjs-tflite',setWasmPath('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/'),async function start() {  console.log('start'),  const wasmFeatures = await getWasmFeatures(),  console.log(      'simd :' + wasmFeatures.simd +      ', multi-threading: ' + wasmFeatures.multiThreading),  // Load model runner with the cartoonizer tflite model.  await loadTFLiteModel(      'https://tfhub.dev/sayakpaul/lite-model/cartoongan/fp16/1',  ),  console.log('model loaded'),}start(),```====="", ""Hello, thanks for your answer.An emulator isn't the best solution for testing phones, because you still use your processor instead of phone's one.Here is a console screenshot for a mobile phone. It's Galaxy S10.The same code works perfectly on a desktop.![image](https://user-images.githubusercontent.com/14860440/125576570-22ef4c0e-c0e2-4f46-b2f4-2861adc63935.png)====="", 'Thank you! You are right about the emulator. I will try to find an Android device to test this (works fine on my iPhone). From the error message, it looks like something inside the TFLite runtime. I will ask the team about this and report back if I get anything. Thanks!=====', 'Thank! It will be great.=====', ""I was able to use the browserstack's Real Device Cloud to run the same code on Galaxy S10 (with Android 9 and Chrome 89), but I didn't see the error you saw. I also tried Galaxy S21 with Android 11 and Chrome 91 and it seems to be working fine as well. Is it possible that the model is not downloaded successfully from tfhub? (or are you using a different tflite model?) Thanks!![Screen Shot 2021-07-15 at 11 39 06 AM](https://user-images.githubusercontent.com/8752427/125840348-96d0603f-63e4-4459-9a96-c9fd889a8d28.png)![Screen Shot 2021-07-15 at 11 33 52 AM](https://user-images.githubusercontent.com/8752427/125840361-9304bb2c-830c-4016-bd74-b19b9d0e625e.png)====="", ""Thanks a lot for your answer. It's an awesome explanation. You're right I have used my own model. On cartoonize model from tfhub all works fine. It looks like something is wrong with my model.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5301"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5301"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4715,Help wanted in using posenet in React Native.,22,closed,2021-02-18T20:05:12Z,2021-09-28T16:41:56Z,"I am unable to render the skeleton on the actual background image. I tried a lot of different ways to do so. I am currently using <Svg> box as provided by the react native documentation example of posenet, but that is for a small screen and I want to present the posenet on a larger view on the screen.","['In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks! =====', 'https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsxThe code provided here does not work well enough. Skeleton visualisation is not good ( it does not plot the points where they belong. there is always some displacement of the skeleton during visualization. I am using a 256x300 tensor[widthxheight] for visualization.=====', ""@ahtrahdis7, any updates on it? Were you able to resolve it? I'm facing the same issue.====="", '@abhinavchawla13 Sorry bro, I tried to fix that issue for about 3 months, with no positive results.=====', 'Thanks for your patience , I had offline chat with @jinjingforever and he will be able to get to it next week. Thank you =====', '@jinjingforever, @rthadur any updates on it? Thanks for the help!=====', 'Sorry for the delay @abhinavchawla13, I was too busy last week. Will take a look this week=====', 'Hi @abhinavchawla13, I took a look at this issue and looks like the key is to figure out the relationship between camera texture and camera preview, how to position and size them, etc. I extracted the pose detection demo to make it more self-contained, and replaced the old posenet with the newer [movenet](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection) model. Here is the [repo](https://github.com/jinjingforever/tfjs-react-native-pose-detection) you can check out. It should correctly render the results for any camera preview sizes ([screenshots](https://photos.app.goo.gl/JngVWy3DoN2DSx696)). Hopefully the comments in the code make sense. Please feel free to let me know if you have any questions. Thanks!=====', '@jinjingforever thanks for resolving this. =====', '@jinjingforever, thank you so much! This is super helpful, especially the comments around the sizing. 🙏=====', '@ahtrahdis7 can we close this issue ?=====', ""@jinjingforever The repository you provided, It doesnot work well with android. I tested it now and found some issues with it.- With `autorender={false}`, the camera preview is not re-rendered or updated.- With `autorender={true}`, The camera preview and the skeleton does not coincide.- WebGl is not getting configured with Expo, this will probably be resolved by using native installation and build.I had assumed that it would work, but it's not the case.====="", '@jinjingforever , I tried it out as well on Android (OnePlus 6T) and an iPad - same issues as @ahtrahdis7 mentioned. I do not need `autorender` to be set `true` for iOS but do need it for Android. And on both, the alignment is coming out to be incorrect. Also, I had to upgrade to use `""expo-camera"": ""^11.2.2""` for the camera to load properly and to resolve TS errors.Any help would be appreciated, again! =====', 'Thanks for testing it @ahtrahdis7 and @abhinavchawla13! I will try to test it more on Android/iPad and will report back (probably next week). Thanks!=====', ""Hi @ahtrahdis7 and @abhinavchawla13, I updated the [repo](https://github.com/jinjingforever/tfjs-react-native-pose-detection) with some new fixes. Last time I forgot to call `gl.endFrameEXP(),` when doing the manual rendering, so some of my assumptions were wrong.. I updated the code comments with my new understanding. Now it should work at least on Pixel 2, newer iPhones, iPad, and iPad Pro, with or without [autorender](https://github.com/jinjingforever/tfjs-react-native-pose-detection/blob/main/App.tsx#L141).After some research and experimenting, I found that it is very tricky to get the correct camera texture size for all kinds of devices, which is the key to get the correct inference results. Initially we thought iOS devices have 1080x1920 textures, and android devices have 1600x1200 textures. But turns out there are many exceptions. For example, for older iPad and iPhones, the texture size is 720x1080. On android, the camera texture size can even change for different preview sizes... Unfortunately, `expo-gl` doesn't have any APIs to query the camera texture size. To work around this, I added a fairly experimental way to figure out the texture size automatically. Set [`USE_PRESET`](https://github.com/jinjingforever/tfjs-react-native-pose-detection/blob/main/App.tsx#L84) to false to try it. See more details in the comments. I tried it on various devices and it seems to be working fine. It does make the initialization time a bit longer though (less noticeable on more modern devices).We will raise some issues for `expo-gl` and hopefully they can implement the APIs we want, which is the ideal solution to this. We've asked for help [before](https://forums.expo.dev/t/getting-expo-gl-createcameratextureasync-texture-dimensions/32226) but got no response..(P.S. I also upgraded `expo-camera` to 11.2.2)Thank you! ====="", ""Hello @jinjingforever, thanks for the updated repo. I tested it out, it still wasn't setting up the camera texture correctly on my Android (OnePlus 6T), but it did work perfectly on iPhone. I will try to test it out on iPad and my Android again tonight and update you guys as well! ====="", 'Please also try setting [USE_PRESET](https://github.com/jinjingforever/tfjs-react-native-pose-detection/blob/main/App.tsx#L84) to false to see if it works. Thank you=====', ""@jinjingforever, I tried setting it to both false and true - didn't work on my initial try but will update you after giving another shot soon. Thanks so much!====="", ""@jinjingforever, I actually tested it out again and somehow it started working perfectly fine on both my iPad and Android phone - sorry to bother above. I'm assuming my expo project was being cached on both (or some funky stuff), it seems to be working fine for now. Thanks a ton! If something breaks while I continue my test, I shall let you know in this thread. [Example On OnePlus 6T](https://user-images.githubusercontent.com/10625526/133189478-12442017-f4a0-43be-9505-f3fa5190a822.png)====="", 'Great to know! Thank you=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/333,Throw a user-friendly error when converting a model that lacks topology,3,closed,2018-05-23T20:37:56Z,2018-05-23T20:52:05Z,"From the tfjs discuss mailing list:Timothée BERNARD:> While developing a side-project with Keras, I accidentally only saved the weights of my model (save_weights_only=True).> Then, I noticed that tensorflowjs_converter does not prevent or warn the user from exporting the h5 to the web format. After that, when loading the model, it results in a ""modelTopology: null"" in the model.json and an error when calling loadModel().> > I'd like to know if only exporting the weights without the topology is relevant or should the converter warn or prevent this?","['cc @caisq if you foresee us allowing ever to export only weights from Keras, without the topology=====', ""Hi @dsmilkov, I've already filed an issue here: #332 Let me know if you want me to close it for avoiding the duplication.====="", 'Closing this one. Duplicate of #332=====']",0
https://github.com/tensorflow/tfjs/issues/2687,Running on GPU freezes,4,closed,2020-01-21T02:45:47Z,2020-06-05T18:46:16Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version6.13.4#### Browser versionnode.js v13.6.0#### Describe the problem or feature requestRunning using tfjs-node finishes very quickly without any noticable freezes.Running with tfjs-node-gpu freezes on:""Epoch 1/100""After about a minute or two, the next line is:2020-01-20 20:44:36.479151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0#### Code to reproduce the bug / link to feature request// THIS IS A TEST FILE!!!const tf = require('@tensorflow/tfjs-node-gpu'),// Optional Load the binding:// Use '@tensorflow/tfjs-node-gpu' if running with GPU.// require('@tensorflow/tfjs-node'),// Train a simple model:const model = tf.sequential(),model.add(tf.layers.dense({units: 100, activation: 'relu', inputShape: [10]})),model.add(tf.layers.dense({units: 1, activation: 'linear'})),model.compile({optimizer: 'sgd', loss: 'meanSquaredError'}),const xs = tf.randomNormal([100, 10]),const ys = tf.randomNormal([100, 1]),model.fit(xs, ys, {  epochs: 100,  callbacks: {    onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`)  }}),If you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['Freezes on the first epoc for 2 mIns or so for me and then runs through all the training. I am seeing this on other model trainings as well. it is allocating 10Gb of my GPU memory with tfjs-node-js and only 1Gb with thensorflow-gpu in python. If I set the epoc and batch to 1 it will start faster. If I then set it back to say 256 batch and 25 epocs  it just runs immediately but still allocates most of the GPU memory. =====', '@tensorflow/tfjs-node-gpu@1.4.0 seems to start immediately. My testing with fashion-mnist-vae also has the same behavior.=====', 'hi @JeffreyXBao , can you provide the version of the library where you encounter this issue?=====', 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/4761,symlink ./lib/napi-v7 failed:  null,7,closed,2021-02-27T18:49:17Z,2021-03-04T18:39:42Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution : Windows 10  home- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: @tensorflow/tfjs-node@3.2.1- CUDA/cuDNN version: none**Describe the problem**I am trying to install the package noted above in a brand new project (right after `npm init`) and I am getting the output below.I have installed visual studio with the build tools required and python 2.7 and looked through the various similar issues but haven't managed to make any of the solutions there work for me,**Provide the exact sequence of commands / steps that you executed before running into the problem**`npm install @tensorflow/tfjs-node` (also tried with `--build-from-source` flag, and with yarn)**Any other info / logs**`$ npm install @tensorflow/tfjs-nodenpm WARN deprecated node-pre-gyp@0.14.0: Please upgrade to @mapbox/node-pre-gyp: the non-scoped node-pre-gyp package is deprecated and only the @mapbox scoped package will recieve updates in the future> @tensorflow/tfjs-node@3.2.0 install C:\Users\dark_\Desktop\projects\tfjs2\node_modules\@tensorflow\tfjs-node> node scripts/install.jsCPU-windows-3.2.0.zip* Downloading libtensorflow[==============================] 4845107/bps 100% 0.0s* Building TensorFlow Node.js bindingssymlink ./lib/napi-v7 failed:  nullnpm WARN tfjs2@1.0.0 No descriptionnpm WARN tfjs2@1.0.0 No repository field.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@3.2.0 install: `node scripts/install.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node@3.2.0 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     C:\Users\dark_\AppData\Roaming\npm-cache\_logs\2021-02-27T18_45_25_103Z-debug.log`","['@Diabl0269 thank you for reporting, can you share the complete log: C:\\Users\\dark_\\AppData\\Roaming\\npm-cache_logs\\2021-02-27T18_45_25_103Z-debug.log=====', '@pyu10055 Here is the complete log:[2021-02-27T18_45_25_103Z-debug.log](https://github.com/tensorflow/tfjs/files/6065107/2021-02-27T18_45_25_103Z-debug.log)=====', 'I am experiencing the same issue. Installation of @tensorflow/tfjs-node@latest fails with the exact same console output on Windows 10 Pro, npm@6.14.8 with the following node versions:node@v10.22.1node@v12.21.0node@v14.16.0Using an elevated command prompt did not alter the outcome.Attempting installation of @tensorflow/tfjs-node@3.1.0 did not alter the outcome.However, installation of @tensorflow/tfjs-node@3.0.0 succeeded.=====', 'Facing exact same issue=====', 'This is fixed with this PR https://github.com/tensorflow/tfjs/pull/4754, should be available in the next release. Meanwhile, can you use tfjs-node@3.0.0?=====', '@pyu10055 Great to hear, I will wait. Anyway version 3.0.0 does work so i guess we can close this issue,=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4761"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4761"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/3437,wechat plugin support for tfjs 2.0,2,closed,2020-06-12T03:16:21Z,2020-06-18T23:41:34Z,This is not a issue but a feature needed.When can we use facemesh with tfjs 2.0 ? And also the wechat-plugin with tfjs 2.0 ?Looking forward to it ~: ),"['You should be able to use facemesh with tfjs 2.0 today, just make sure to include a backend if you are using tfjs-core directly. Have you tried it and run into a problem?=====', '@tafsiri Thx for the reply. I tried but with wechat plugin. I guess it is because of the wechat-plugin: https://github.com/tensorflow/tfjs-wechatwhich is not updated to fit 2.0 yet.=====']",0
https://github.com/tensorflow/tfjs/issues/437,Add Support for BilinearUpSampling2D Layer for Keras model,6,closed,2018-06-16T18:22:12Z,2019-02-12T15:32:29Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.11.6#### Browser versionVersion 67.0.3396.62 (Official Build) (64-bit)#### Describe the problem or feature requestWhen I import my model in tfjs, I get the error: `Error: Unknown layer: BilinearUpSampling2D`#### Code to reproduce the bug / link to feature request","[""@jmlb Do you know whether `BilinearUpSampling2D` is an official layer in Keras. It doesn't seem to be in the documentation at https://keras.io/ or in the source code in https://github.com/tensorflow/tensorflowIs it possible that your Keras model artifacts are saved from Python code using Keras and a customly defined layer (`BilinearUpSampling2D`)?====="", 'BilinearUpSampling2D is not an official layer in Keras.Here\'s are two implementations I found:- https://github.com/akirasosa/mobile-semantic-segmentation/blob/master/layers/BilinearUpSampling.py#L45-L110- https://github.com/penny4860/FCN/blob/master/keras_fcn/layers.py#L8-L41Interestingly, neither of them set `align_corners=True`. From my read of issues like https://github.com/tensorflow/tensorflow/issues/6720 it\'s an open question whether the default behavior is desired or not going forward. Another description of this ""bug"" is provided here https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35The right place to get this function added first would be Keras. That said, if you can convert your model from Keras to straight Tensorflow, then it looks like tfjs already supports bilinear resizing via https://js.tensorflow.org/api/0.11.6/#image.resizeBilinear=====', 'Thanks for the reply. I am using the same implementation as in https://github.com/akirasosa/mobile-semantic-segmentation/blob/master/layers/BilinearUpSampling.py#L45-L110 (@kylemcdonald ). Great suggestion: I will convert the model to TF.BTW, thank you for the discussion and links about `align_corners`...something I was not aware of. Very interesting reading.=====', '@jmlb You may be able to get this working sooner by using a custom layer in tfjs.  See here for an example custom layer https://github.com/tensorflow/tfjs-examples/tree/master/custom-layer=====', 'There is no upstream bandwidth to review https://github.com/keras-team/keras/pull/9303.I think that it is ""a general status"" of keras.=====', 'Closing the issue as the layer is still not a standard layer in Keras. Users can implement the custom layer. See example at https://codepen.io/caisq/pen/jdBgwB=====']",0
https://github.com/tensorflow/tfjs/issues/806,ScatterND tests don't match expected TensorFlow output,2,open,2018-10-18T17:38:43Z,2019-03-11T18:27:54Z,"I'm updating to @tensorflow/tfjs 0.13.2 on the Node bindings and it looks like the scatterND tests are incorrect.```Failures:1) scatterND test-tensorflow {} should work for 2d  Message:    Error: Invalid TF_Status: 3    Message: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5,3] updates: [3,3]  Stack:    Error: Invalid TF_Status: 3        at <Jasmine>        at NodeJSKernelBackend.executeSingleOutput (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:124:41)        at NodeJSKernelBackend.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:955:17)        at environment_1.ENV.engine.runKernel.$indices (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:49:33)        at /usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:197:22        at Engine.scopedRun (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:168:19)        at Engine.runKernel (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:193:10)        at scatterND_ (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:48:21)        at Object.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:46:24)        at UserContext.<anonymous> (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd_test.ts:28:23)        at <Jasmine>        at runCallback (timers.js:696:18)        at tryOnImmediate (timers.js:667:5)        at processImmediate (timers.js:649:5)        at process.topLevelDomainCallback (domain.js:121:23)2) scatterND test-tensorflow {} should work for simple 1d  Message:    Error: Invalid TF_Status: 3    Message: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5] updates: [1]  Stack:    Error: Invalid TF_Status: 3        at <Jasmine>        at NodeJSKernelBackend.executeSingleOutput (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:124:41)        at NodeJSKernelBackend.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:955:17)        at environment_1.ENV.engine.runKernel.$indices (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:49:33)        at /usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:197:22        at Engine.scopedRun (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:168:19)        at Engine.runKernel (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:193:10)        at scatterND_ (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:48:21)        at Object.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:46:24)        at UserContext.<anonymous> (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd_test.ts:40:23)        at <Jasmine>        at runCallback (timers.js:696:18)        at tryOnImmediate (timers.js:667:5)        at processImmediate (timers.js:649:5)        at process.topLevelDomainCallback (domain.js:121:23)3) scatterND test-tensorflow {} should work for multiple 1d  Message:    Error: Invalid TF_Status: 3    Message: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5] updates: [3]  Stack:    Error: Invalid TF_Status: 3        at <Jasmine>        at NodeJSKernelBackend.executeSingleOutput (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:124:41)        at NodeJSKernelBackend.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:955:17)        at environment_1.ENV.engine.runKernel.$indices (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:49:33)        at /usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:197:22        at Engine.scopedRun (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:168:19)        at Engine.runKernel (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:193:10)        at scatterND_ (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:48:21)        at Object.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:46:24)        at UserContext.<anonymous> (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd_test.ts:50:23)        at <Jasmine>        at runCallback (timers.js:696:18)        at tryOnImmediate (timers.js:667:5)        at processImmediate (timers.js:649:5)        at process.topLevelDomainCallback (domain.js:121:23)4) scatterND test-tensorflow {} should sum the duplicated indices  Message:    Error: Invalid TF_Status: 3    Message: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [8] updates: [6]  Stack:    Error: Invalid TF_Status: 3        at <Jasmine>        at NodeJSKernelBackend.executeSingleOutput (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:124:41)        at NodeJSKernelBackend.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:955:17)        at environment_1.ENV.engine.runKernel.$indices (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:49:33)        at /usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:197:22        at Engine.scopedRun (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:168:19)        at Engine.runKernel (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:193:10)        at scatterND_ (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:48:21)        at Object.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:46:24)        at UserContext.<anonymous> (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd_test.ts:89:23)        at <Jasmine>        at runCallback (timers.js:696:18)        at tryOnImmediate (timers.js:667:5)        at processImmediate (timers.js:649:5)        at process.topLevelDomainCallback (domain.js:121:23)5) scatterND test-tensorflow {} should work for tensorLike input  Message:    Error: Invalid TF_Status: 3    Message: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5] updates: [3]  Stack:    Error: Invalid TF_Status: 3        at <Jasmine>        at NodeJSKernelBackend.executeSingleOutput (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:124:41)        at NodeJSKernelBackend.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/src/nodejs_kernel_backend.ts:955:17)        at environment_1.ENV.engine.runKernel.$indices (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:49:33)        at /usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:197:22        at Engine.scopedRun (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:168:19)        at Engine.runKernel (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/engine.ts:193:10)        at scatterND_ (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd.ts:48:21)        at Object.scatterND (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:46:24)        at UserContext.<anonymous> (/usr/local/google/home/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/src/ops/scatter_nd_test.ts:99:23)        at <Jasmine>        at runCallback (timers.js:696:18)        at tryOnImmediate (timers.js:667:5)        at processImmediate (timers.js:649:5)        at process.topLevelDomainCallback (domain.js:121:23)Ran 2656 of 2688 specs2656 specs, 5 failuresFinished in 3.068 secondserror Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.``` Our op signature matches the generated Python code. I'll look to see if any wrapper APIs do any shaping.","['cc @pyu10055 =====', 'Did we ever figure this out, @pyu10055?=====']",0
https://github.com/tensorflow/tfjs/issues/3197,Support for AutoML Vision Edge models with sharded weights in React Native,12,closed,2020-05-01T17:38:53Z,2020-11-20T21:40:29Z,"#### TensorFlow.js version@tensorflow/tfjs : 1.7.3@tensorflow/tfjs-react-native : 0.2.3#### Browser versionexpo : 37.0.8#### Describe the problem or feature requestSince you: - Can't load sharded weights using the [bundleResourceIO](https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/src/bundle_resource_io.ts#L32) function provided in tfjs-react-native.- [Can't run your own tensorflowjs conversion](https://github.com/tensorflow/tfjs/issues/1513#issuecomment-546449981) to change the `weight_shard_size_bytes` on a TF Saved Model [exported from AutoML](https://cloud.google.com/vision/automl/docs/export-edge#container).You get caught at an impasse when trying to use an AutoML Vision Edge model through the React Native APIs. It seems to me the options for someone with this issue are:- Attain the ability to turn weight sharding off within AutoML's Cloud UI.- Attain a way to reproduce the exact `tensorflowjs_converter` command that AutoML runs on their end. Or however they make that conversion.- Attain a way to load multiple shards within the tfjs-react-native `bundleResourceIO`.- Attain a way to rejoin shards. I'm out of my depth, would that even be possible? I've experimented with taking non-sharded weights from my own conversion runs and splicing them into the model.json produced by AutoML with no luck.#### Code to reproduce the bug / link to feature requestThere's not too much relevant code to show since it's explicitly unsupported in tfjs-react-native. However, fwiw, here's the conversion code I attempted to use when I realized direct support for the artifacts produced by AutoML tfjs export were not usable.```bash# This produces significantly different results than what is produced through the AutoML tfjs export.tensorflowjs_converter \  --input_format=tf_saved_model \  --output_format=tfjs_graph_model \  --weight_shard_size_bytes=10485760 \  --skip_op_check \  00000123 \  test```","[""You can load shared model via https I'm facing the same issue using bundleResourse====="", 'Hi. I think the path you want is your second point, converting the model to use a single shard. Sharding is primarily advantageous for web browsers and caching, when the model is local we generally want to fetch that in one request/read. `weight_shard_size_bytes` should the path you want. Could you elaborate on your last point about the model having different results when you run it through the tfjs converter? Would you be able to post some sample code somewhere with your converted model to reproduce?cc @pyu10055 @dsmilkov.=====', ""Yea, I agree that path makes the most sense.As for the different results, I'm referring to the model.json that's created and the ability to get a functioning result back. In the process of setting up a sample to share I managed to sidestep some of the issues I'd seen previously -- specifically around the input signature, I had to sleuth around to find the magic `ToFloat` key -- but the differences are still present. I'm only assuming that this is what's responsible for the malfunction.See https://github.com/nicholas/tfjs_issue_3197Notice the differences between the [exported model](https://github.com/nicholas/tfjs_issue_3197/blob/master/assets/exported_tfjs/model.json) & the [converted model](https://github.com/nicholas/tfjs_issue_3197/blob/master/assets/converted_tfjs/model.json).I've added some commentary about the errors I'm seeing [here](https://github.com/nicholas/tfjs_issue_3197/blob/master/App.js#L54).Note that this same model, the weight sharded, direct AutoML tfjs export, functions as expected using the the instructions laid out in [tfjs-automl](https://github.com/tensorflow/tfjs/tree/master/tfjs-automl).Side note 1: Over the weekend I played around with the [sample integration](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native/integration_rn59). I had issues getting that to work so I spun up a brand new React Native 62 project and ported everything over with success -- with some minor issues pending. Is that something you'd be interested in a PR for?Side note 2: I should mention, I've only focused on iOS. There's likely build errors with Android.====="", 'For the conversion step have you tried converting the auto-ml tfjs export from a sharded model to a non sharded model directly (no savedmodel). The converter can take tfjs models as inputs as well, see https://github.com/tensorflow/tfjs/tree/master/tfjs-converter#javascript-to-javascript.=====', 'In this case both the input and output type would be tfjs_graph_model.=====', ""Hey, thanks for taking a look. That does look promising, I'll run some more experiments and report my findings back here.====="", ""Unfortunately, tfjs_graph_model to tfjs_graph_model conversion does not appear to be a supported feature.```tensorflowjs_converter \\  --input_format=tfjs_graph_model \\  --output_format=tfjs_graph_model \\  --weight_shard_size_bytes=10485760 \\  --skip_op_check \\  assets/exported_tfjs/model.json \\  assets/converted_tfjs...TensorFlow.js model converters.: error: argument --input_format: invalid choice: 'tfjs_graph_model' (choose from 'tf_hub', 'keras_saved_model', 'keras', 'tf_frozen_model', 'tf_saved_model', 'tfjs_layers_model')```And after some review, I don't see that option in what I think is the [main entry point for conversion](https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/python/tensorflowjs/converters/converter.py#L587).====="", '@pyu10055 Could we support a configuration in the converter like the one above to support unsharding a sharded tfjs_graph_model? Or is there another option for unsharding.=====', ""@tafsiri @nicholas Unsharding a tfjs_graph_model is possible, but I don't think we should provide it as converter option, since allowing tfjs_graph_model as a input format and supporting all options are beyond the need for fixing this issue. we can create an js util class to perform the unsharding. ====="", ""What if it were just a specific unsharding command in the converter so they do not have to get a separate tool (but it doesn't otherwise allow full conversion of graph models)?====="", '@tafsiri after thinking about this more, it might make sense to provide graph_model to graph_model conversion, the allowed features could include sharding and quantization.I will look into that.=====', 'Closing, sharded models are now supported in tfjs-react-native https://github.com/tensorflow/tfjs/pull/4113We also updates the instructions on the [automl README](https://github.com/tensorflow/tfjs/tree/master/tfjs-automl#loading-the-model-1) to describe how to load a model with a custom IOHandler (i.e. something other than a string).=====']",0
https://github.com/tensorflow/tfjs/issues/4595,tf.unique failing on Node only with `Error running Unique: Neither modular kernel nor forward func passed`,4,open,2021-01-25T02:35:24Z,2021-08-16T21:03:37Z,"The code ```tsconst tf = require('@tensorflow/tfjs-node')const a = tf.tensor1d([1, 1, 2, 4, 4, 4, 7, 8, 8]),const {values, indices} = tf.unique(a),```Fails on **windows** and **mac** for Node.jsIf you switch backend to CPU it works.Here it is failing on node<img width=""1456"" alt=""Screen Shot 2021-01-24 at 8 31 28 PM"" src=""https://user-images.githubusercontent.com/997157/105654244-35b96000-5e83-11eb-8d30-90c5ba88379d.png"">But when using CPU backend it works.See https://github.com/GantMan/unique_fail_poc for failure and success reproductions with latest release of TFJS Node.  Let me know if I can help with anything.   Tested on two machines, and it fails on quite a few versions of TFJS, btw.  Lots of stuff works just fine, it's just tf.unique that seems angry.","[""@GantMan thank you for detailed report , tf.unique is only supported in 'cpu' and 'webgl' for now , this would be a feature request to add it to Node. ====="", 'cc @jinjingforever =====', ""I think it would be wise to add some kind of notation or caveat to the docs where functions are unsupported.  A small note on the [docs](https://js.tensorflow.org/api/latest/#unique) would identify this is not a bug.Secondly, as another feature, better error messages would be a good feature as well.  It took me a while to find out what the issue was.  I'll make a note of this and try to contribute back if this issue is still outstanding when I get some cycles.====="", 'Hello, any news relating to this feature? I think getting the unique values in a tensor in a core method and should be also implemented for node=====']",1
https://github.com/tensorflow/tfjs/issues/3961,bodyPix not working even a little bit correct,10,closed,2020-09-23T10:45:19Z,2021-01-15T06:31:44Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address **System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 7 ultimate i7 36000 intel hd built in graphics- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm install @tensorflow/tfjs- TensorFlow.js version (use command below):- Browser version:chrome 85.0- Tensorflow.js Converter Version: ^2**Describe the current behavior**I am using bodypix in node.js using electron-framework. I am using simple example given on github with this backend.It is not detecting any human on any simple video I have tried. If I reduce the segmentation threshold  then also it is detecting completely wrong(walls in the video). However once through script tag(only once in 10 tries) it has detected humans exactly right with a console message of ""hello seems that you are trying node. Try installing....""**Describe the expected behavior**It should simply detect humans as in the demo site using webcam**Standalone code to reproduce the issue**You can reproduce the problem by loading a video, drawing it to a canvas on video play per second and then using bodypix on the canvas to output mask data to another canvas**Other info / logs** If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Sometimes give vertex and shader not linked(something else like this) error. Once it worked with the same settings giving a console message starting with hello, looks like you are using node.... other times no error","['@HemantKumarProgrammer we do not have Windows machine to reproduce the error , I tried on my Mac and could not reproduce, It is working as expected.=====', ""> @HemantKumarProgrammer we do not have Windows machine to reproduce the error , I tried on my Mac and could not reproduce, It is working as expected.@rthadur I dont know but even using script tag, it is not working. Once it worked exactly the right way. Maybe it is a server problem. Or I don't have a gpu, just inbuilt native Intel hd graphics, is it a problem.I have @tensorflow/tfjs , @tensorflow/tfjs-node,@tensorflow/tfjs-convertor,@tensorflow/tfjs -core installed along with bodypix. However while installing tfjs-node it is trying to install some other build tools which were not installed due to an error,saying latest visual studio needed.====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Is a GPU must for using tensorflow bodypix? Then it may be the reason for this issue because I dont have a gpu, just inbuilt intel hd graphics , which is not very good=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Just writing this to avoid being stalled.***Progress:*** thinking to try again, maybe after 10-15 days=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3961"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3961"">No</a>=====', 'It worked after setting ` tf.setBackend(""cpu"")`=====']",0
https://github.com/tensorflow/tfjs/issues/5325,add a input size flag for topK cpu forwarding,1,closed,2021-07-13T20:49:56Z,2021-07-14T19:29:33Z,"TopK CPU forwarding requires more fine tuning on the input size and K, create a flag would allow users to customize the mechanism.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5325"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5325"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5698,Compatibility with newer version of react native and expo,7,open,2021-10-07T14:34:53Z,2021-11-03T16:52:57Z,"While installing tfjs-react-native on my expo project, i've got this error```➜  smartstudio-app git:(feat/A410) ✗ npm install @tensorflow/tfjs-react-nativenpm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! While resolving: undefined@undefinednpm ERR! Found: expo-asset@8.3.3npm ERR! node_modules/expo-assetnpm ERR!   expo-asset@""~8.3.3"" from the root projectnpm ERR! npm ERR! Could not resolve dependency:npm ERR! peer expo-asset@""^7.0.0"" from @tensorflow/tfjs-react-native@0.7.0npm ERR! node_modules/@tensorflow/tfjs-react-nativenpm ERR!   @tensorflow/tfjs-react-native@""*"" from the root project```my config```   ""expo"": ""^42.0.0"",    ""expo-asset"": ""~8.3.3"",    ""expo-auth-session"": ""~3.3.1"",    ""expo-av"": ""~9.2.3"",    ""expo-camera"": ""~11.2.2"",    ""expo-constants"": ""~11.0.1"",```i also believe this library not to be comptabile with expo-camera over version 7 while i have version 11.forcing peer deps doesn't look like a good idea, but maybe we can write dependencies of tfjs-react-native like    ""expo-camera"": "">=7.0.0"",if expo does not break api between major versionThanks a lot for the good work, team !","['expo-camera latest version is supported , please check out this repo https://github.com/jinjingforever/tfjs-react-native-pose-detection which was recently tested.  Thank you =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', ""it works only with yarn. installing with npm, even with --force, gives plenty of errors at runtime and actually don't work====="", 'Can you try with this latest example https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification, right now it only works with yarn. It might take some time to support npm as this is throwing some errors.=====', '@BigZ this looks like the infamous peer dependency issue, npm changed the behavior of peer dependencies:- `<=6.x.x` - peer dependencies are optional, when missing or ""incompatible"" it gives you a warning.- `>=7.x.x` - peer dependencies are enforced during install, when missing or ""incompatible"" it throws exactly this error.You should be able to install it using [`npm install --legacy-peer-deps`](https://docs.npmjs.com/cli/v7/using-npm/config#legacy-peer-deps). Fixing this issue takes a long time, all peer dependencies from all libraries used in the Expo or React Native community now have to _exactly match_ the right versions. It\'s [something more people aren\'t happy about](https://twitter.com/markdalgleish/status/1325755229883621376), but on the other hand, [they had some reasons to do it anyways](https://twitter.com/notbrent/status/1357481746841804800).=====', '@byCedric yes, yarn or legacy peer deps work, but signify there should be a change in package.jsonDo you think i should do a PR for the change i suggested in my original message ? (ie peerdeps using >= instead of ^)=====', '@BigZ that sounds good to me. Thank you!=====']",1
https://github.com/tensorflow/tfjs/issues/4386,"Tensorflow.js with wasm backend is trying to use unsupported function 'resizeNearestNeighbor', even though I used bilinear interpolation in all the places",6,closed,2020-12-10T16:30:16Z,2020-12-17T11:25:42Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): pip install tensorflowjs- TensorFlow.js version (use command below): 2.7.0- Browser version: Google Chrome Version 84.0.4147.89 (Official Build) (64-bit)- Tensorflow.js Converter Version: v2.4.0**Describe the current behavior**Tensorflow.js with wasm backend is trying to use unsupported function 'resizeNearestNeighbor', even though I used bilinear interpolation in all the places. I get the following error when trying to run inference in the browser ""Error: 'resizeNearestNeighbor' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen"".Code runs smoothly but slowly when switching to ""cpu"" backend.**Describe the expected behavior**I would like the inference to run without errors and use Bilinear interpolation which is supported on all the backends.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.I can not share the trained weights, but the rest of inference code is here. You can see the model.json file, which does not contain any nearest neighbor interpolation. https://github.com/martun/tensorflow_js_issue**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""Related to https://github.com/tensorflow/tfjs/issues/4340 and https://github.com/tensorflow/tfjs/issues/4212The underlying issue is the UpSampling2D layer (used in your model0 directly calls `resizeNearestNeighbor` and doesn't seem handle the bilinear interpolation option. @pyu10055 @caisq, should this be an enhancement to the UpSampling2D layer?====="", ""@tafsiri I agree this should be considered as an enhancement. It is currently unimplemented is most likely lack of support by tfjs-core. So it's possible that the nearest-neighbor resampling support is still unavailable from tfjs-core and that'll need to be addressed first. ====="", '@caisq tfjs-core has both resizeNearestNeighbour (currently called) and resizeBilinear, so we should be able to switch on that unless resizeBilinear does not support something required by upsampling2d.=====', '@caisq Actually looks like its been added!=====', 'Thanks guys, it started to work after I updated to tfjs version: 2.8.0=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4386"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4386"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4832,How does tfjs handle memory when GC occurs,4,closed,2021-03-18T11:49:36Z,2021-04-05T16:59:13Z,"From the official document, we find that tfjs can automatically handle tensor memory management when GC occurs. However, this does not happen in our practice.Below is a snippet of code to reproduce it:```jsconst tf = require('@tensorflow/tfjs'),const wait = () => new Promise((resolve) => {  setTimeout(resolve, 1000),}),const main = async () => {  const makeFeature = () => tf.ones([100]),  const makeLabel = () => 1,  for (let i = 0, i < 1, i++) {    for (let j = 0, j < 2, j++) {      let xs = tf.stack([makeFeature(), makeFeature(), makeFeature()]),      let ys = tf.stack([tf.oneHot(makeLabel(), 2), tf.oneHot(makeLabel(), 2), tf.oneHot(makeLabel(), 2)]),      // this will not delete the tensor from memory automatically.      //xs = null      xs.dispose()      await wait()          }    console.log(tf.memory())  }}main(),```","['@WenheLI I believe if you are using WebGL backend, which is the default backend if you using the union package, the tensor dispose needs to be manually initiated or using the `tf.tidy()` block.Can you link to the document where we stated differently? thanks=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4832"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4832"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4947,Delete called on a Tensor not referenced with Node.js worker_threads,2,closed,2021-04-16T18:46:49Z,2021-06-03T14:58:15Z,"Using `tf.tidy` in a Node.js worker thread sometimes causes `Error: Delete called on a Tensor not referenced` if two instances of the thread are running at the same time.- OS: Windows 10 20H2- TensorFlow.js version: v3.3.0- Node version: v14.16.0```js// master.jsconst { Worker } = require('worker_threads'),const w1 = new Worker('worker.js'),const w2 = new Worker('worker.js'),// worker.jsconst { tidy, scalar } = require('@tensorflow/tfjs-node'),tidy(() => {    const s = scalar(3.14),}),``````Error: Delete called on a Tensor not referenced (tensor_id: 2392)    at Object.<anonymous> (<anonymous>)    at NodeJSKernelBackend.disposeData (node_modules\@tensorflow\tfjs-node\dist\nodejs_kernel_backend.js:264:30)    at Engine.disposeTensor (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3381:26)    at Tensor.dispose (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:2361:21)    at Engine.scopedRun (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3017:13)    at Engine.tidy (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3001:21)    at Object.tidy (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:8744:19)```","['related PR has been merged , @bmcdorman thanks for the contribution.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4947"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4947"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/552,Neither LSTMCell nor RNNCell are exported in `.d.ts` file,2,closed,2018-07-27T05:12:50Z,2018-07-27T16:39:17Z,"#### TensorFlow.js version0.12.3#### Browser versionNode 10.5.0#### Describe the problem or feature requestDocumentation says that `tf.RNNCell` is a return type of `tf.layers.lstmCell`. However, neither `RNNCell` nor `LSTMCell` are exported.#### Code to reproduce the bug / link to feature request```jsconsole.log(tf.RNNCell === undefined),console.log(tf.LSTMCell === undefined),```","['@indutny Thanks for reporting this issue. We will export `RNNCell` as `tf.layers.RNNCell` and export `RNN` as `tf.layers.RNN`.=====', 'Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/5242,`tfjs-automl` image classification memory leak on Node.js,4,open,2021-06-22T16:36:16Z,2021-08-09T23:37:00Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Repo link below- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin 20.5.0 x64 (macOS Big Sur 11.4)- TensorFlow.js installed from (npm or script link): npm- System information:```jsconst os = require('os'),const { node, v8 } = process.versions,const plat = `OS: ${os.type()} ${os.release()} ${os.arch()}\nNode.js: ${node}\nV8: ${v8}`,const cpus = os.cpus().map(cpu => cpu.model).reduce((o, model) => {  o[model] = (o[model] || 0) + 1,  return o,}, {}),const cpusInfo = Object.keys(cpus).map((key) => {  return `${key} x ${cpus[key]}`,}).join('\n'),console.log(`${plat}\nCPU: ${cpusInfo}\nMemory: ${os.totalmem() / (1024 * 1024)} MiB\n`),// OS: Darwin 20.5.0 x64// Node.js: 14.17.0// V8: 8.4.371.23-node.63// CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz x 16// Memory: 32768 MiB```**Describe the current behavior**`Resident Set Size` keeps getting bigger.**Standalone code to reproduce the issue**https://github.com/SukkaW/tfjs-memory-leak-example**Other info / logs** Include any logs or source code that would be helpful to```shgit clone https://github.com/SukkaW/tfjs-memory-leak-example && cd tfjs-memory-leak-examplenpm inode index.js``````Platform info:OS: Darwin 20.5.0 x64Node.js: 14.17.0V8: 8.4.371.23-node.63CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz x 16Memory: 32768 MiBdaisy 146083840 2035808daisy 152354816 2035808daisy 153202688 2035808daisy 157728768 2035808daisy 163180544 2035808daisy 167120896 2035808daisy 169332736 2035808daisy 169385984 2035808daisy 170770432 2035808daisy 172281856 2035808daisy 174080000 2035808daisy 174120960 2035808daisy 176160768 2035808.....```","['The above GitHub link does not work , please check, meanwhile can you check this comment https://github.com/tensorflow/tfjs/issues/4015#issuecomment-704335135 if it helps , thank you =====', '> The above GitHub link does not work , please check, meanwhile can you check this comment [#4015 (comment)](https://github.com/tensorflow/tfjs/issues/4015#issuecomment-704335135) if it helps , thank you@rthadur The repo should be available now.I will look into the link you provided, thanks for your reply!=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', ""Why the issue is marked as stale? I don't see the issue is being solved, or if it is a dupe of #3061=====""]",1
https://github.com/tensorflow/tfjs/issues/5936,Use bodypix model on react native,2,closed,2021-12-11T17:05:22Z,2021-12-12T15:08:08Z,"Hello. I am trying to use tensorflow bodypix model on my app on react native, I want to use it for segment a body person from a local image. Could anyone say me how can I do it? Thank you so much","['You can use movenet instead of bodypix , please check out this demo https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection here , thank you=====', 'Thank you so much, I will try it=====']",1
https://github.com/tensorflow/tfjs/issues/1311,[question{ Which version of Tensorflow (libtensorflow) does tfjs-node/-gpu depend on?,8,closed,2019-03-03T02:25:11Z,2020-08-27T20:45:43Z,"hi,I noticed that I can’t install tfjs-node-gpu unless I first build libtensorflow from Tensorflow-GPU. Given the dependency, what version of the Tensorflow backend is supported by Tensorflow.JS?I could be misunderstanding the relationship between the two but I’m not sure how tfjs-node can be isolated from change to the Tensorflow backend. For example, what happens when Tensorflow transitions to 2.0 with many breaking changes? Does the tfjs API change too? Any clarification would be appreciated. Thank you.EDIT:I believe node-gles will free tfjs from the libtensorflow dependency? But for now until node-goes is ready for general use, can I just install the Python Tensorflow-GPU 1.9 wheel (I think there is a 1.11 wheel too for my platform: aarch64 / Tegra) Will that install ltensorflow.so?","['cc @nkreeger =====', 'Updated:Looks like I have to build tensorflow-gpu on my device in order to get libtensorflow since the Python wheel does not seem to install it. In my docker, I install cuda 9 / cudnn 7, and the official NVIDIA tensorflow-gpu for Jetson TX2 SBC (aarch64/Tegra): Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 markdown-3.0.1 numpy-1.16.2 protobuf-3.7.0 setuptools-39.1.0 six-1.12.0 tensorboard-1.9.0 tensorflow-gpu-1.9.0+nv18.8 termcolor-1.1.0 werkzeug-0.14.1But when I try to install tfjs-node-gpu, I immediately get the message that ld cannot find the tensorflow C library.I\'ll try building it manually and will report back. I can appreciate why node-gles is so vital given how difficult it is to get tfjs-node working for aarch64.> @tensorflow/tfjs-node-gpu@0.3.1 install /node_modules/@tensorflow/tfjs-node-gpu> node scripts/install.js gpu download* Downloading libtensorflow* Building TensorFlow Node.js bindings/node_modules/@tensorflow/tfjs-node-gpu/scripts/install.js:161      throw new Error(\'node-gyp rebuild failed with: \' + err),      ^Error: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuild/usr/bin/ld: skipping incompatible ./Release/libtensorflow.so when searching for -ltensorflow/usr/bin/ld: cannot find -ltensorflow/usr/bin/ld: skipping incompatible ./Release/libtensorflow_framework.so when searching for -ltensorflow_framework/usr/bin/ld: cannot find -ltensorflow_frameworkcollect2: error: ld returned 1 exit statusmake: *** [Release/obj.target/tfjs_binding.node] Error 1gyp ERR! build error gyp ERR! stack Error: `make` failed with exit code: 2gyp ERR! stack     at ChildProcess.onExit (/usr/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:262:23)gyp ERR! stack     at emitTwo (events.js:126:13)gyp ERR! stack     at ChildProcess.emit (events.js:214:7)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)gyp ERR! System Linux 4.4.38-tegragyp ERR! command ""/usr/bin/node"" ""/usr/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild""gyp ERR! cwd /node_modules/@tensorflow/tfjs-node-gpugyp ERR! node -v v8.15.1gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok     at cp.exec (/node_modules/@tensorflow/tfjs-node-gpu/scripts/install.js:161:13)    at ChildProcess.exithandler (child_process.js:288:5)    at emitTwo (events.js:126:13)    at ChildProcess.emit (events.js:214:7)    at maybeClose (internal/child_process.js:915:16)    at Socket.stream.socket.on (internal/child_process.js:336:11)    at emitOne (events.js:116:13)    at Socket.emit (events.js:211:7)    at Pipe._handle.close [as _onclose] (net.js:561:12)npm WARN enoent ENOENT: no such file or directory, open \'/package.json\'npm WARN !invalid#1 No descriptionnpm WARN !invalid#1 No repository field.npm WARN !invalid#1 No README datanpm WARN !invalid#1 No license field.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node-gpu@0.3.1 install: `node scripts/install.js gpu download`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the @tensorflow/tfjs-node-gpu@0.3.1 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /root/.npm/_logs/2019-03-03T19_53_37_760Z-debug.log=====', ""@innerop, just curious if you made any progress on this. I've been sitting on a TX2 dev kit for a while and was wanting to test it out with tfjs-node-gpu :)====="", 'Same here, does someone successfully run tfjs-node-gpu on an arm64 like Jetson Nano / TX2 ?  =====', 'I\'m not sure if my issue is related, but I\'ve followed the instructions at [tensorflow.org/install/docker#gpu_support](https://www.tensorflow.org/install/docker#gpu_support) to install docker, and it seems to work fine with python:```bashsudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu  python -c ""import tensorflow as tf, print(tf.reduce_sum(tf.random.normal([1000, 1000])))""``````2020-01-19 03:42:35.391400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.62020-01-19 03:42:35.392368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.62020-01-19 03:42:35.743097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.12020-01-19 03:42:35.775753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.777170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5coreClock: 1.44GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s2020-01-19 03:42:35.777235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.12020-01-19 03:42:35.777304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.102020-01-19 03:42:35.778680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.102020-01-19 03:42:35.779089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.102020-01-19 03:42:35.780770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.102020-01-19 03:42:35.781782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.102020-01-19 03:42:35.781843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-19 03:42:35.781927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.782791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.783445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 02020-01-19 03:42:35.783888: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-01-19 03:42:35.790412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz2020-01-19 03:42:35.791136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de5a9c0610 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-01-19 03:42:35.791147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version2020-01-19 03:42:35.868398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.868726: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de5aa25fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:2020-01-19 03:42:35.868740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.52020-01-19 03:42:35.868848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.869143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5coreClock: 1.44GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s2020-01-19 03:42:35.869162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.12020-01-19 03:42:35.869168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.102020-01-19 03:42:35.869177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.102020-01-19 03:42:35.869183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.102020-01-19 03:42:35.869189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.102020-01-19 03:42:35.869194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.102020-01-19 03:42:35.869199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-19 03:42:35.869226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.870044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:35.870298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 02020-01-19 03:42:35.870317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.12020-01-19 03:42:36.048792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:2020-01-19 03:42:36.048819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 2020-01-19 03:42:36.048824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 2020-01-19 03:42:36.048955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:36.049400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 03:42:36.049666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4344 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)tf.Tensor(-1445.1068, shape=(), dtype=float32)```But when I try running this with node within the same docker container it doesn\'t work:```jsrequire(""@tensorflow/tfjs-node-gpu""),``````2020-01-19 04:06:33.990265: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-01-19 04:06:34.019307: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz2020-01-19 04:06:34.020647: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x75cbaa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-01-19 04:06:34.020691: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version2020-01-19 04:06:34.022833: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.12020-01-19 04:06:34.064030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-19 04:06:34.064586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.44pciBusID: 0000:01:00.02020-01-19 04:06:34.064734: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcudart.so.10.0\', dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.064838: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcublas.so.10.0\', dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.064878: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcufft.so.10.0\', dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.064922: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcurand.so.10.0\', dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.064961: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcusolver.so.10.0\', dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.065007: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libcusparse.so.10.0\', dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory, LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib642020-01-19 04:06:34.067293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-19 04:06:34.067304: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.Skipping registering GPU devices...2020-01-19 04:06:34.142644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:2020-01-19 04:06:34.142663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 2020-01-19 04:06:34.142668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N```Does the docker image approach not work with tfjs? I.e. will I instead need to install the Nvidia software (mentioned [here](https://www.tensorflow.org/install/gpu#software_requirements)) directly on my machine?=====', 'As mentioned [here](https://github.com/tensorflow/tfjs/issues/3287#issuecomment-656935658) , tfjs-node depend on tf core version 1.5.0 =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/1497,Add WASM backend,30,closed,2019-04-05T02:51:13Z,2019-12-14T20:45:40Z,"#### TensorFlow.js version1.0.4#### Browser versionChrome 73#### Describe the problem or feature requestHas there been any further discussion about using WASM to optimize performance? I noticed there was discussion in #36, but it was decided to wait and eventually `tfjs-node` was created.**Possible Benefits of WASM**- Run in any WASM environment- Maybe higher speed than WebGL if users have slow graphics?","['Hi, thanks for filing this issue. A WASM backend is on our roadmap for this quarter. We can use this issue for updates.=====', 'Folks at intel have been making webml/webnn and recently made some amazing strides of hitting 31fps    running coco SSD models. https://github.com/intel/webml-polyfill/issues/650#issuecomment-475127456They are implementing it natively mimicking the android NN API and for the changes to reach end user, they have to be merged into chromium source code. Where as  WASM backend would be awesome and I hope it can hit as much performance as they are able to=====', 'Hi folks update on this, curious to see the performance compared to tflite and tensorflow=====', ""Thanks for the reference. We are closely following the amazing work that Intel has done, led by @huningxin. Our wasm backend will most likely not be able to reach that level of performance since we won't be able to access hardware acceleration beyond SIMD 128bit, but it will provide other benefits, such as: - acceleration for lower-end mobile devices that don't have webgl float support. - acceleration on higher-end devices through SIMD and threading. - consistent performance on cpu, agnostic of the JIT compilation. - open an avenue for adding iterative linear algebra primitives such as SVD, eigenvectors, matrix inversion, that cannot be done efficiently on the GPU.We will start developing our WASM backend in the open, on GitHub, very soon. Stay tuned!====="", ""Thanks for the update @dsmilkov . As you know, [webml-polyfill](https://github.com/intel/webml-polyfill) currently reuses tfjs-core WebGL kernels for [GPU implementation](https://github.com/intel/webml-polyfill/tree/master/src/nn/webgl) and compiles tf-lite kernels to WASM for [CPU implementation](https://github.com/intel/webml-polyfill/tree/master/src/nn/wasm/src). It's a great news that tfjs will have a WASM backend soon. It would simplify the webml-polyfill and benefit the [webnn spec](https://webmachinelearning.github.io/webnn/) development. Highly looking forward to that.====="", 'Hey, guys. Are there any updates on this issue? =====', 'We just started by adding the framework (skeleton) code around it: https://github.com/tensorflow/tfjs-core/pull/1863 We will be slowly ramping up on this.=====', 'While fully understanding that this a new and still experimental backend, are you planning on publishing the alpha/beta builds to npm?=====', ""We are still very early in the process so publishing something to npm is not useful at this time. I'll ping this issue when we have more updates.====="", 'For out of browser WASI I just opened https://github.com/tensorflow/mlir/issues/140 in the MLIR repo.=====', 'This has been done and should be available in latest releases.Thank you.=====', 'Great news! Which release has now Wasm support? And are there any instructions about it? Looking forward to trying it out.=====', 'This is actually still in progress we’ll update this when we’re ready to release :)=====', '> This is actually still in progress we’ll update this when we’re ready to release :)Hello, do you have any updates?=====', 'Hi all,We are excited to share that we just released our first alpha release of the WASM backend.See the [README.md](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md) in the `tfjs-backend-wasm` folder for how to use it. At this point we have about ~30 kernels - see [all_kernels.ts](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/src/kernels/all_kernels.ts#L21) for the list.We would love to get your feedback, testing, and help as we are polishing towards a general release!Thanks!=====', ""Closing this issue since it's a general issue tracking our WASM release. Feel free to file separate issues if you run into problems.====="", 'Thanks @dsmilkov This is great news. =====', '@dsmilkov Are you involved as team in https://bytecodealliance.org/?=====', ""There appears to be a directory called `wasm-out` missing in the published npm version, I get an error:```Module not found: Error: Can't resolve '../wasm-out/tfjs-backend-wasm' in '/var/www/node_modules/@tensorflow/tfjs-backend-wasm/dist'```Here is a screenshot of installed items:![image](https://user-images.githubusercontent.com/833485/70281945-2b18a480-1771-11ea-85e1-0d60ea9ebcfe.png)====="", 'Thanks for finding that out. We just released `1.4.0-alpha2` with the fix. Let us know how it goes.=====', ""It looks to be working so far in the sense that our application hasn't crashed or thrown errors. I will you know how it goes :+1: ====="", 'Great new congrats on alpha, Cameron, pls do share the performance improvements you notice during your experiments with wasm backend. =====', 'Did everything following the example from README and got an error. Do I need some special build of tfjs-core? Any Ideas? ![Screenshot from 2019-12-06 15-30-44](https://user-images.githubusercontent.com/18259976/70323312-84e6a080-183d-11ea-8e41-0305af502265.png)=====', '@chirva-ivan which browser are you using? If you open the initialization error what does it say?=====', '@nsthorat using Chrome 78![Screenshot from 2019-12-06 18-26-16](https://user-images.githubusercontent.com/18259976/70334182-faf70180-1855-11ea-9fd3-de7636c34a5f.png)=====', '@chirva-ivan is it possible for you to stand up a demo for us to look at?=====', 'Are you using Parcel? Can you share a small project/build script that we can reproduce? Thanks!=====', '@dsmilkov  Here is a codesandbox with above error:https://codesandbox.io/s/naughty-montalcini-s0gjk=====', '@dsmilkov I am still getting the error in the browser, was the fix just for node?=====', 'I just sent https://github.com/tensorflow/tfjs/pull/2559 for review where I document how to use the WASM backend with bundlers and also added starter projects for webpack and parcel. See [update readme from the PR](https://github.com/tensorflow/tfjs/tree/9c11267db3f3562756440450945a4801d80d2690/tfjs-backend-wasm#using-bundlers).Note also that you have to depend on the latest alpha `1.4.0-alpha3`, and potentially we might need to release `alpha4` since we made some updates to the `browser` field in `package.json`.=====']",0
https://github.com/tensorflow/tfjs/issues/106,Consider putting math kernels into standalone module?,4,closed,2018-04-05T17:59:51Z,2019-07-24T22:17:35Z,"_From @ofrobots on January 3, 2018 21:53_Have we considered extracting the math functionality from this module and putting it in a standalone module by itself. Math functionality seems independently useful._Copied from original issue: tensorflow/tfjs-core#508_","[""_From @dsmilkov on January 4, 2018 0:50_Hi, to better understand, can you share your use case? `NDArrayMath` doesn't make sense  without `NDArray`s. Did you mean `NDArrayMath` independent of the graph/gradient/backpropagation code?====="", '_From @ofrobots on January 4, 2018 1:6_Right, sorry. Yeah, I think we have a really good implementation of a accelerated math library here. I think it can be useful independently of the remainder of the ML code.=====', '_From @ofrobots on January 22, 2018 18:55_Some experiences from over this weekend. I was working a backend (Node.js) project where I just needed a 3D Array structure, I started with deeplearn as it has a really nice accelerated implementation. Here\'s the friction log:* It would be good to be able to use just the NDArray and associated code without depending upon the entirety of deeplearn. (This issue)* In my Node.js typescript project, I had to enable `lib: [""dom""]` in `tsconfig.json`. This module directly depends upon browser APIs which makes adds to the friction of using it on the backend. Only the CPU Math backend is available on Node.js presently, and from a JavaScript point of view, things work. From a types point of view they don\'t. (I can extract this into a standalone issue).* My typescript project follows the Google TypeScript style guide which uses `strict` compiler flag. The compiler complains of type errors in the deeplearn.js code. The work-around is to disable strict mode which is highly undesirable. (issue #576)=====', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/1658,Will posenet work on landscape mode?,1,closed,2019-06-12T10:17:20Z,2019-06-12T18:17:02Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser version#### Describe the problem or feature request#### Code to reproduce the bug / link to feature request",['This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow.js) since it is not a bug or feature request. There is also a larger community that reads questions there.====='],0
https://github.com/tensorflow/tfjs/issues/481,Out of memory exception when fitting model,11,closed,2018-06-30T11:25:07Z,2020-08-14T17:27:23Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version5.6.0#### Browser versionVersion 67.0.3396.99 (Official Build) (64-bit)#### Describe the problem or feature requestWhen fitting a fairly simple dense model (100-125-75 nodes per layer), we hit an out of Memory exception.  Note: performance noticeably drops over time too.I'm training multiple models, small dataset (~100 rows, 16 inputs per row).  We get this exception on the second run, on epoch ~1000The machine has 16 G of ram, and AMD RX-480 video cardIf we continue the exception, progress continues, but still very slowly.#### Code to reproduce the bug / link to feature requestI'm training for a lot of epochsHere is the full training function.      async trainModel(model, Xdata, Ydata) {        const xs = tf.tensor2d(Xdata),        const ys = tf.tensor2d(Ydata),        const yscaled = ys.mul(this.scaleFactor),        await model.fit(xs, yscaled, {            batchSize: 25,            epochs: this.epochs,            callbacks: {                onEpochEnd: async (epoch, log) => {                    console.log(`Epoch ${epoch}: loss = ${log.loss}`),                }            }        }),        // Do a quick test on the setting value        let ypred = model.predict(xs),        let ypredDescaled = ypred.div(this.scaleFactor),        let pdata = ypredDescaled.dataSync(),        for (let i = 0, i < pdata.length, i++) {            // TODO: evaluate differences        }        // As this is an async operation, manually dispose of allocated memory        xs.dispose(),        ys.dispose(),        yscaled.dispose(),        ypred.dispose(),        ypredDescaled.dispose(),    }","[""I removed the logging function (and the predict post-train) and lowered the epochs to 2500.  This improved things considerably - trained about 14 models before the exception threw.  Again, continuing seemed to resume training (although I haven't yet verified the trained models).====="", ""Having a similar issue with a super basic model. I'm running on a 2014 Dell XPS 15 though (i7-4712HQ, GT750M).**TensorFlow.js version**0.11.7**Browser Version**Version 67.0.3396.99 (Official Build) (64-bit)**Describe the problem**Performance degrades quickly and it soon completely locks up my chrome tab (even though it should be Async?), after which I have to kill it with the Chrome task manager. I'm not getting any 'this tab is slowing down your browser' alerts either.```const xs = tf.tensor2d([[0], [1]])const ys = tf.tensor2d([[1], [0]])const predictXs = tf.tensor2d([[0], [0.5], [1]])const model = tf.sequential()const hidden = tf.layers.dense({    units: 4,    inputShape: [1],    activation: 'sigmoid'})model.add(hidden)const output = tf.layers.dense({    units: 1,    activation: 'sigmoid'})model.add(output)const sgdOpt = tf.train.sgd(0.1)model.compile({    optimizer: sgdOpt,    loss: 'meanSquaredError'})const fitConfig = {    epochs: 4000}model.fit(xs, ys, fitConfig).then((result)=>{    console.log(result.history.loss[0])        let outputs = model.predict(predictXs)    outputs.print()})```I know I'm doing a sigmoid activation on a linear regression. I'm just trying to figure out how this all works. It doesn't have to be sensible as long as I can get a grasp on what's going on.====="", ""It would be great for tfjs to have some sort of basic memory check against the target user's system, I've seen issues on low-spec systems such as Microsoft Surface tablets where they technically have *some* VRAM, but the amount is so low that it should just use the CPU. From what I've read it's impossible for WebGL to reveal how much VRAM is available to it (security issue?), but it's a real pain in the ass, the only way around it is to identify the User Agent as a low-spec system.====="", 'This looks like it might be a memory leak. Could you check how many tensors are created as your program runs using [tf.memory()](https://js.tensorflow.org/api/0.11.7/#memory), this will return an object that has a numTensors property, if that is always increasing that would indicate a memory leak. @nwesthoff for you program you may want to add an onEpochEnd callback to print the number of tensors after each epoch.We do have some work coming in future releases on preventing runaway memory leaks from bringing down the program (though it is still ideal to not leak memory).cc @caisq, does `model.fit` call `tf.nextFrame` internally?=====', '@tafsiri I\'m having trouble with the `onEpochEnd ` callback, I can make it fire by running `fitConfig.onEpochEnd()`, but it isn\'t fired by the training function.```const fitConfig = {    epochs: 2000,    shuffle: true,    onEpochEnd: () => {        console.log(tf.memory().numTensors + "" tensors"")    }}model.fit(tf.tensor(xs), tf.tensor(ys), fitConfig).then(() => {    // stuff happens}```Running `fitConfig.onEpochEnd()` manually after 2000 epochs returns 48 tensors, though I\'m not sure how useful this is, as it doesn\'t show progress during training.=====', '@nwesthoff the syntax for onEpochEnd looks more like this (it is part of a callbacks option, [see model.fit](https://js.tensorflow.org/api/0.11.7/#tf.Model.fit) for more details)```jsconst fitConfig = {    epochs: 2000,    shuffle: true,    callbacks: {\t\tonEpochEnd: () => {    \t    console.log(tf.memory().numTensors + "" tensors"")\t    }\t}}model.fit(tf.tensor(xs), tf.tensor(ys), fitConfig).then(() => {    // stuff happens}```Try that and see if it works.=====', ""@tafsiri Ah, misunderstood the docs, that works indeed. When running 2000 epochs it goes up from 279 tensors all the way to 2278 tensors (curiously close to one tensor per epoch). It doesn't reset after running the fit function for new data either. So this does indeed look like a memory leak. I've just read about the `tf.dispose` and `tf.tidy` functions, though I'm not sure how to apply them, as they happen during the model fitting. Is this something I can fix using the layers API?====="", 'Going to refer this to @caisq in case this because of a memory leak inside `model.fit`. We did jave one that was fixed here https://github.com/tensorflow/tfjs-layers/pull/252 so a fix may be incoming.=====', ""Any update on this issue? I'm also having issues with fix() leaking memory. after 10 epochs I run out of memory. Each epoch is very small too. 16 32x32 RGB images. I've 2gb of VRAM + 8gb shared.Thanks====="", ""Ok, so not sure if this helps, but I tested my code running on nodejs/ubuntu as well.  I had the similar sorts of results there, with my entire 12G of ram being eaten when trying to train a moderate sized graph (65K rows, 2000 epochs).  I would start with around 500megs of memory, then grow till well over 2 gigs per process (running 4 simultaneous trainings).NOTE: I'm running on the CPU for this test.It appears that (for node anyway) the leak is on the JS side.  Periodically printing process.memoryUsage() shows the heap and RSS would grow continuously while training, while external is fairly constant.   I assume tensors refer to external memory, rather than anything on the JS heap.Some things I tried:  Calling GC() in the epoch callback did not resolve the issue.  Running multiple iterations of model.fit, with a GC between iterations did not resolve the issue   Running multiple iterations of model.fit by saving the model, releasing it, calling GC(), calling tf.disposeVariables(), reloading the model, and continuing training, still did not fix the issue.  All of the above, plus calling tf.setBackend(tf.getBackend())  FINALLY FIXED IT!I now do a full flush of the system every 250 epochs.  All of this significantly improved the training performance and dropped my total memory usage to a nice low 200m - even lower than starting conditions.====="", ""I got the same problem, specifically with tfjs-node backend, it turned up that the easiest solution that worked for me was to use [jemalloc](http://jemalloc.net/), in this [Stack Overflow answer](https://stackoverflow.com/questions/53234410/how-to-use-node-js-with-jemalloc) it's pretty well explained what you have to do.=====""]",0
https://github.com/tensorflow/tfjs/issues/3341,hope tensorflow suport  deno,5,closed,2020-05-28T06:08:13Z,2021-09-17T04:32:27Z,[deno](https://github.com/denoland/deno) is here.,"['@chenxyzl this will not supported anytime in near future , so closing this request.=====', 'Thanks for your response=====', '@rthadur Deno added support for webgpu in 1.8. Would there be any chance that support for it would be provided now?=====', 'cc @pyu10055 @lina128 =====', 'I was able to run tfjs-core with CPU backend in Deno with the following import map configuration.import_map.json```json{  ""imports"": {    ""crypto"": ""./empty.js"",    ""seedrandom"": ""https://esm.sh/seedrandom"",    ""@tensorflow/tfjs-core"": ""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core/dist/tf-core.fesm.js"",    ""@tensorflow/tfjs-backend-cpu"": ""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu/dist/tf-backend-cpu.fesm.js""  }}```test.js```jsimport * as tf from ""@tensorflow/tfjs-core"",import ""@tensorflow/tfjs-backend-cpu"",tf.randomGamma([2, 2], 1).print(),```empty.js```jsexport default undefined,``````shellsession$ deno run --import-map import_map.json --location https://example.com test.jsTensor    [[0.5183429, 4.3179331],     [0.4852145, 0.1412067]]```=====']",0
https://github.com/tensorflow/tfjs/issues/514,UnhandledPromiseRejectionWarning: ReferenceError: XMLHttpRequest is not defined,6,closed,2018-07-13T22:32:21Z,2018-11-14T14:07:41Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js 0.11.4#### Chrome Version 67.0.3396.99### Using Node v9.4.0#### Describe the problem or feature request**_const net  = await posenet.load(),  <== Getting error here..._**```async function machineLearning(imageElement) {        const imageScaleFactor = 0.5,        const flipHorizontal = true,        outputStride: 16,        const net  = await posenet.load(),  <== Getting error here...        const pose = await net.estimateSinglePose(imageElement, imageScaleFactor, flipHorizontal, outputStride),        console.log(pose),        return pose,    }```**getting UnhandledPromiseRejectionWarning: ReferenceError : XMLHttpRequest is not definedand then** ```at /Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:445:27    at new Promise (<anonymous>)    at CheckpointLoader.loadManifest (/Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/p```Getting Error on Promise#### Code to reproduce the bug / link to feature requestLogs:```Listening on port 8080(node:42295) UnhandledPromiseRejectionWarning: ReferenceError: XMLHttpRequest is not defined    at /Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:445:27    at new Promise (<anonymous>)    at CheckpointLoader.loadManifest (/Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:444:20)    at /Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:461:27    at new Promise (<anonymous>)    at CheckpointLoader.getCheckpointManifest (/Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:460:24)    at /Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:478:23    at new Promise (<anonymous>)    at CheckpointLoader.getAllVariables (/Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:477:20)    at Object.<anonymous> (/Users/rpatel/dev/elvis/delete/express-babel/node_modules/@tensorflow-models/posenet/dist/posenet.js:922:53)(node:42295) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 2)(node:42295) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```","['Solved after  putting global variable in node_modules=====', 'Can you please show where did you add this global variable?=====', '@lucasgaspar22 install **xhr2** `npm install xhr2`and put this on first line of this file `node_modules/@tensorflow-models/posenet/dist/posenet.js`**`global.XMLHttpRequest = require(""xhr2""),`**=====', '@lucasgaspar22  Overall, my suggestion is to go back to python Tensorflow instead of using tf.js Because I personally feel they are still developing and you have to solve so many problems by your self.=====', ""Thank you man. I agree with you. I'm just using node because my companysaid I needed to .. :(Thank you a million2018-07-18 13:55 GMT-03:00 Rutul Patel <notifications@github.com>:> @lucasgaspar22 <https://github.com/lucasgaspar22> Overall, my suggestion> is to go back to python Tensorflow instead of using tf.js>> Because I personally feel they are still developing and you have to solve> so many problems by your self.>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/514#issuecomment-406001619>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AQ9o1mmxA-u49dxnX4hwSF6lS6JjnqnOks5uH2iMgaJpZM4VPlk1>> .>====="", ""Why is this closed? I'm running in to the same issue today.=====""]",0
https://github.com/tensorflow/tfjs/issues/5670,Cannot convert Keras saved model,5,closed,2021-09-27T20:01:57Z,2021-10-12T18:44:31Z,"**System information**- Ubuntu 18.04 WSL- tensorflow 2.5.0**Describe the problem**Model convertion using `tensorflowjs_converter --input_format=keras_saved_model ./model_autokeras ./converted_model` throws following error: `RuntimeError: Unable to restore a layer of class Custom>MultiCategoryEncoding. Layers of class Custom>MultiCategoryEncoding require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.`My model has been generated using Autokeras StructuredDataRegressor.**Provide the exact sequence of commands / steps that you executed before running into the problem**This can be reproduced by following this tutorial: https://autokeras.com/tutorial/structured_data_regression/And then run `tensorflowjs_converter --input_format=keras_saved_model ./model_autokeras ./converted_model`","['Looks like you might have a custom layer that is not supported ""natively"" in Tensorflow.js. Have you tried changing the output format to `--output_format=tfjs_graph_model`?=====', 'It seems that this combination is not supported :( `ValueError: Unsupported input_format - output_format pair: keras_saved_model - tfjs_graph_model`=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5670"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5670"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/677,Missing gradient: tf.max and tf.min,1,closed,2018-09-05T01:34:04Z,2018-09-06T19:02:58Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.12.7#### Browser versionDoesn't matter#### Describe the problem or feature requesttf.max() doesn't have a gradient defined. See code in tfjs-core:https://github.com/tensorflow/tfjs-core/blob/master/src/ops/reduction_ops.ts#L283",['cc @dsmilkov @nsthorat ====='],0
https://github.com/tensorflow/tfjs/issues/5496,backend tfjs-backend-webgpu is missing several common kernel ops,21,closed,2021-08-14T21:57:06Z,2021-10-22T06:37:33Z,"after converting my existing app to run with `tfjs-backend-webgpu` built from main branch  (latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old),  i've run into issues running some of the models (all of them work perfectly using `webgl` backend):> Kernel 'TopK' not registered for backend 'webgpu'> Kernel 'SplitV' not registered for backend 'webgpu'> Kernel 'FlipLeftRight' not registered for backend 'webgpu'> Kernel 'RotateWithOffset' not registered for backend 'webgpu'note: with remaining running models, with `webgpu` inference is 300-400ms while with `webgl` it's 400-550ms  which makes `webgpu` ~30% faster than `webgl`!btw, i'll try using webgpu in web workers next as single thread does not come even close of saturating gpu  (average utilization is ~25%)environment: `chrome/94 canary` with `tfjs` 3.8.0 and `tjfs-backend-webgpu` built from main","['@xhcao Please take a look, thanks. =====', 'I can work on this if you say...=====', '@carrycooldude, any contribution to WebGPU backend is highly appreciated! Once you have something ready for review, you may @qjia7 and we can invite other related reviewers. Thanks!=====', 'Hi, @carrycooldude Are you implementing these kernel ops? You could submit several patches, and each patch only implements one kernel op.I know It is difficult to implement these ops, if you have some problems, we could discuss the solutions. =====', 'Sure @xhcao I started working on it, I will let you know if I having any issue and Also If possible can you make a project tab for me for the missing kernels=====', 'Hi, @carrycooldude, How many operators had been implemented? Do you need help from me to implement them? I think @vladmandic is waiting us to implement these operator, and verify it work on his refactoring. https://github.com/tensorflow/tfjs/issues/5468You could refer the implementations from webgl backend to implement the first version and optimize them in feature.=====', 'Can you give me some PR for reference=====', '@carrycooldude Below links implement these kernels on webgl backend, you could refer them.https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/TopK.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/SplitV.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/FlipLeftRight.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/RotateWithOffset.ts=====', 'Thanks I will start working on this=====', 'Hi, @carrycooldude, how about the process of these kernels? Currently, I have spare time, and could help you to implement the kernels, which have been not started to implement.=====', 'Yes , Actually Want to understand the webgl code implementation with webgpu in order to get which Files I have to import to implement those kernels=====', ""Could I take over these two kernels ('FlipLeftRight' and  'RotateWithOffset')? We could implement them synchronously.====="", 'Sure , Also if possible can you guide me through your implementation it will be great too 😇=====', '@carrycooldude You can refer to this one #5585, which implements a new kernel `DepthToSpace` for webgpu.=====', 'Thanks @qjia7 =====', ""@carrycooldude , I had implemented 'FlipLeftRight' and 'RotateWithOffset' kernels, https://github.com/tensorflow/tfjs/pull/5649 ,you could refer to them.====="", 'Thanks yunfei for implementing the `split` kernel. Now, only `topK` kernel is unimplemented, but it is the most difficult one, the algorithm of webgl backend is based on the paper https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf, I will  investigate and implement this operator with @carrycooldude, and make some optimizations in future according to webgpu features.=====', 'Hi, @vladmandic , all these four kernel ops had been implemented now, you could try your app on master branch. If there are some other issues, please tell us, thank you.=====', ""I just run first sanity test:- no missing kernel ops- quite a lot of refactoring on my side to avoid `dataSync`    (more than I wanted since async functions cannot be used inside `tf.tidy()`)- just submitted a trivial PR #5756i'll test performance and precision next...====="", '**webgpu** implementation of `tf.image.fromPixels()` is inverted on y-axis, so that corrupts all model outputs  ive just created issue #5757 to track thatregarding performance, its still pretty bad compared to **webgl** - that is being tracked under #5689**webgl**- warmup initial **18sec**- warmup cached **5sec****webgpu**- warmup initial **21sec**- warmup cached **20sec**recent work on `WEBGL_PACK_DEPTHWISECONV` makes **webgl** warmup much faster than in 3.9.0 release  does that have any effect on **webgpu** or should it be implemented there separately as well?btw, im using `WEBGL_USE_SHAPES_UNIFORMS=true` since that has significant performance advantages for **webgl** warmup=====', '@vladmandic Thanks for the feedback. In fact, webgpu already uses the uniforms by default. The fix on webgl is not suitable for webgpu since the shader is very different from the problem one in webgl. But we are actively looking at the warmup issue. Hope we can give a solution soon. And for the browser cache issue, the webgpu developer in chromium replied us that this features is expected to be finished in this quarter.Close this one and use the separate bugs you reported to track the left issues. Thanks.=====']",1
https://github.com/tensorflow/tfjs/issues/5054,Too many duplicated code in all backends,2,open,2021-05-08T06:58:06Z,2021-05-12T15:28:54Z,"There are too many duplicated codes in all backends. For example:1. Validation code. For the parameter validation, we can do it in the front end instead of doing it in all backends. 2. For some ops, they are using *ImplCPU for all backends. 3.  For some ops, they don't need a backend specific kernel. For example, `Identity`, `ExpandDims`.4. For some common optimizations, for example, in some situations, this operation is just return an Identity of itself or reshape. All of above situations, maybe we can put them in the front end (`fjs-core/src/ops/`) instead of making a copy in all backends.Take #5025 as an example,  it seems that `SparseSegmentMean` is using a cpu implementation. I suppose it will be same for all backends although only cpu/webgl was covered in that PR.","['@pyu10055 @lina128 Any thoughts on this?=====', '@qjia7 Thank you for raising this issue. The backend kernel is designed this way to allow tree shaking and custom bundle. We can discuss more in our weekly sync.=====']",1
https://github.com/tensorflow/tfjs/issues/1753,"how to make index in tfjs like python :  such as : boxes[:, :, 0]",10,closed,2019-07-17T08:27:34Z,2019-07-24T02:34:07Z,"how to make index in tfjs like python :  such as : boxes[:, :, 0]","['hi @aohan237  are you trying to index in tensor or array?=====', '@kangyizhang  yes, it seems that there is no way to do this=====', 'and such method as tf.meshgrid does not exist in tfjs, which makes hard to impl some custom layer in tfjs from python model=====', '@kangyizhang  @rthadur  is there any solution about this? thanks in advance=====', '@aohan237 - For the `index in tensor or array` thing,I think you can use [TensorBuffer](https://js.tensorflow.org/api/0.11.2/#buffer) to achieve. Also, one possible workaround regarding with pythonic index would be using babel to overwrite the [] operator. But I did not see any babel plugin serves for that aim. Hope that gives you some clue.=====', ""@aohan237 There is no support for Python-style slicing in TensorFlow.js's Tensor of TensorBuffer at this moment.However, something like `boxes[:, :, 0]` can be achieved using `tf.gather()`, e.g., `tf.gather(boxes, 0, 2)`, or equivalently `boxes.gather(0, 2)`. In both cases, the argument (`0`) is the index along the last dimension, and the argument (`2`) refers to the 3rd dimension.====="", '@caisq  thanks. i will try.and there is someting missing in tfjs . meshgrid, for example in tfpy, but not found in tfjs=====', '@caisq  and what if i want a boxes[:, :, 2:4], gather cant do this..how can i get this?=====', '@aohan237 Questions like this might be more suitable for StackOverflow, under the tag ""tensorflow.js"":https://stackoverflow.com/questions/tagged/tensorflow.jsFor `boxes[:, :, 2:4]`, you can use `tf.slice()`, although it\'s slightly tedious.```tf.slice(boxes, [0, 0, 2], [boxes.shape[0], boxes.shape[1], 4]),```Also, feel free to file a feature request issues for meshgrid.=====', '@caisq  Thanks, use tf.slice can make this, although it is not that user-friendly.and a small suggestion:if something needs one to find out how to impl an function from your api docment, may be add something example to the doc for people to find out.such as : add the following to the beginner guides(from python) > For boxes[:, :, 2:4], you can use tf.slice().`tf.slice(boxes, [0, 0, 2], [boxes.shape[0], boxes.shape[1], 4]),`all new people will know it, rather than try to find out how to use this.  =====']",0
https://github.com/tensorflow/tfjs/issues/5839,Cannot get WebGL context in Pop!_OS firefox,9,closed,2021-11-12T22:31:38Z,2021-11-15T20:08:34Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes, but not relating to this bug- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Pop!_OS 21.04- TensorFlow.js installed from (npm or script link): HTML Tag- TensorFlow.js version (use command below): tfjs 3.1.0- Browser version: Firefox 94.0**Describe the current behavior**I am getting the below error: Error when getting WebGL context:  Error: Cannot create a canvas in this context**Describe the expected behavior**tfjs should be able to get to get WebGL context. I am able to run use my website on chrome on the same machine with no issues.**Standalone code to reproduce the issue**Error arises during initialization, so just import tfjs via html tag.","[""Unable to reproduce on PopOS 20.04 with Firefox v94:![image](https://user-images.githubusercontent.com/1167575/141646188-28c938bc-dd35-4e97-9bce-80d747d815f6.png)https://jsbin.com/wimogumana/edit?html,outputI'm not able to test PopOS 21.04 right now. Have you tried restarting your computer to see if the error persists? I sometimes get problems like this in Chrome (WebGL context lost), but restarting fixes it. I'm not sure whether it's a problem with tfjs or browser WebGL implementation, or nvidia linux drivers.Are you using a nvidia card? If so, and if the problem persists after restarting, try switching to integrated graphics if available (as a test to see if the bug is nvidia-driver-related).====="", ""Modern Firefox disables WebGL by itself if it doesnt detect that GPU is adequate - so make sure you're running good display drivers as majority of Linux default ones are anything but ok  And do a quick test by navigating to <https://get.webgl.org/webgl2/> and <https://webglreport.com/?v=2>====="", 'Thanks all, the tests that you have given are working for me, which indicates that webgl is actually working with firefox and that the issue is elsewhere.I\'m actually having the error when importing tfjs in a webworker. `importScripts(\'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js\'),`It is the line above, in a webworker that I have having the issue. I assume when it says ""Error when getting WebGL context:  Error: Cannot create a canvas in this context"", it is referring to the webworker context? I am unsure. =====', 'nothing to do with with **PopOs** or **TFJS**.by definition, **DOM** is not available in web workers (and **Canvas** is a DOM object)  alternative is `OffscreenCanvas` that TFJS is happy to use, but Firefox keeps that disabled  (I have no idea why would they still keep it disabled since it was added years ago)  navigate to `about:config` and enable `gfx.offscreencanvas.enabled`=====', ""I've enabled `gfx.offscreencanvas.enabled` but now firefox is crashing whenever I open the page.====="", ""That's up to Firefox. Use some other browser if you want web workers to work. ====="", 'Alright then, thanks for your help. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5839"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5839"">No</a>=====', 'Alright then, thanks for your help. =====']",1
https://github.com/tensorflow/tfjs/issues/4710,BlazeFace demo freeze with wasm simd option,1,closed,2021-02-17T23:05:47Z,2021-02-24T01:10:39Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): Tested two versions: 3.1.0 and 2.8.0- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**The blazeface [demo](https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html) freeze with wasm simd option. **Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4710"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4710"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3550,Uncaught (in promise) Error: Argument tensors passed to stack must be a Tensor[] or TensorLike[],7,closed,2020-07-03T16:46:23Z,2021-09-07T20:01:30Z,"#### TensorFlow.js version2.0.0#### Browser versionMicrosoft Edge Version 83.0.478.58 (Official build) (64-bit)#### Describe the problem or feature requestUncaught (in promise) Error: Argument tensors passed to stack must be a `Tensor[]` or `TensorLike[]`Error occurs every time the model is fit with tensor arrays or by zipping as dataset.Error Stack :tensor_util_env.js:125 Uncaught (in promise) Error: Argument tensors passed to stack must be a `Tensor[]` or `TensorLike[]`    at jg (tensor_util_env.js:125)    at stack_ (array_ops.js:134)    at stack (operation.js:45)    at Object.x (array_ops.js:186)    at tape.js:167    at engine.js:425    at t.e.scopedRun (engine.js:436)    at t.e.tidy (engine.js:423)    at engine.js:1005    at s (tape.js:167)#### Code to reproduce the bug / link to feature request```<script>    async function trainmodel() {        const model = tf.sequential({            layers:[tf.layers.lstm({units:100,activation:'relu',returnSequences:true,inputShape:[1,1]}),                    tf.layers.lstm({units:10,activation:'relu',returnSequences:true}),                    tf.layers.dense({units:1})            ]        }),        const X = tf.tensor3d([0.1,0.4,0.7,0.5],[4,1,1]),        const y = tf.tensor3d([0.05,0.5,0.95,0.8],[4,1,1]),        function onBatchEnd(batch, logs) {            console.log('Loss', logs.acc),        }        model.compile({            optimizer: tf.train.sgd(0.01),            loss: 'meanSquaredError',            metrics: ['accuracy']        }),        model.fit(X, y, {            epochs: 3,            callbacks: {onBatchEnd}        }).then(info => {            console.log('Final accuracy', info.history.acc),        }),           }    trainmodel(),</script>```","['cc @caisq I was able to reproduce but I noticed this happens on these values for units on LSTM but not other ones (e.g. lower values), ccing you in case you have ideas of where this might lead? =====', '@tafsiri Thanks for letting me know. I can reproduce it too. An additional interesting observation is that the model can always run `predict()` correctly even though its `fit()` method throws the error. This seems to indicate that the problem is somewhere in the gradient tape code related to the `convertToTensorArray` method in tfjs-core. This is consistent with the stack trace of the error. In addition, I observed that if the input shape is `[2, 1]`, instead of `[1, 1]`, then the model trains without error. This again is a hint that the bug is an edge case in the gradient tape code. cc @dsmilkov =====', 'More info: `convertToTensorArray()`, which is a part of the gradient tape mechanism, expects its argument to be an array of Tensor or TensorLike. But for some reason, when input shape of the LSTM is `[1, 1]`, i.e., only one temporal step, `convertToTensorArray()` receives a single Tensor as argument. If the argument is an Array<Tensor> of length 1, then everything works.=====', 'Thanks for the pointers @caisq!=====', '@caisq I am running into this too. I have implemented stateful lstm layers and removed all my timesteps (seconds dimension is now of length 1) which caused the error. I have found that moving my number of features from the third dimension to the second dimension no longer causes this error (shape=100,4,1 instead of 100,1,4). I hope that extra small bit of info is helpful in trouble shooting.Side question: Is it valid to move variables to different dimensions in this sense, or do these lstm layers not treat each dimension identical? (does each dimension in a stateful lstm have specific purpuses)? In other words, is this a valid temporary fix for this bug, or would you not expect it to predict properly?=====', ""I was getting the same error and solved it by placing the tensor argument into an array in the following function:` @tensorflow/tfjs/dist/tf.node.js/** * Stacks a list of rank-`R` `tf.Tensor`s into one rank-`(R+1)` `tf.Tensor`. * * ```js * const a = tf.tensor1d([1, 2]), * const b = tf.tensor1d([3, 4]), * const c = tf.tensor1d([5, 6]), * tf.stack([a, b, c]).print(), * ``` * * @param tensors A list of tensor objects with the same shape and dtype. * @param axis The axis to stack along. Defaults to 0 (the first dim). * * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'} */function stack_(tensors, axis = 0) {    const $tensors = convertToTensorArray([tensors], 'tensors', 'stack', 'string_or_numeric'),`====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3550"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3550"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4269,Uncaught (in promise) TypeError: Cannot read property 'length' of null,1,closed,2020-11-19T04:01:14Z,2020-11-20T18:37:30Z,"<img width=""510"" alt=""Screen Shot 2020-11-19 at 11 54 16 AM"" src=""https://user-images.githubusercontent.com/11975415/99619229-f4a96780-2a5d-11eb-9e8a-43c19b6af25e.png"">I was following the [TensorFlow.js - Audio recognition using transfer learning](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#7) tutorial. When I called the train() function by pressing the 'train' button, an error came as above. Was it me or was there a mistake in the tutorial?  (I did follow the guide step by step...and test it with a localhost in the latest version of chrome)[code_from_tutorial.zip](https://github.com/tensorflow/tfjs/files/5564636/code_from_tutorial.zip)",['https://stackoverflow.com/questions/64905349/uncaught-in-promise-typeerror-cannot-read-property-length-of-null problem solved====='],1
https://github.com/tensorflow/tfjs/issues/1931,gyp ERR! configure error gyp ERR! stack Error: Command failed,3,closed,2019-08-28T01:04:41Z,2019-08-28T02:05:59Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version 1.2.8#### Browser versionFirefox/chrome#### Describe the problem or feature requestCant install tfjs-node with this errorPS C:\Connect\Teste01> npm install @tensorflow/tfjs-node> @tensorflow/tfjs-node@1.2.8 install C:\Connect\Teste01\node_modules\@tensorflow\tfjs-node> node scripts/install.jsCPU-windows-1.2.8.zip* Downloading libtensorflow[==============================] 11129583/bps 100% 0.0s[==============================] 1338573/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https downloadnode-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v4/1.2.8/tfjs_binding-v1.2.8-node-v64-win32-x64.tar.gznode-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@1.2.8 and node@10.16.3 (node-v64 ABI, unknown) (falling back to source compile with node-gyp)gyp ERR! configure errorgyp ERR! stack Error: Command failed: C:\Users\artur\Anaconda3\python.EXE -c import sys, print ""%s.%s.%s"" % sys.version_info[:3],gyp ERR! stack   File ""<string>"", line 1gyp ERR! stack     import sys, print ""%s.%s.%s"" % sys.version_info[:3],gyp ERR! stack                                ^gyp ERR! stack SyntaxError: invalid syntaxgyp ERR! stackgyp ERR! stack     at ChildProcess.exithandler (child_process.js:294:12)gyp ERR! stack     at ChildProcess.emit (events.js:198:13)gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:259:5)gyp ERR! System Windows_NT 10.0.18362gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\Connect\\Teste01\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v4\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=C:\\Connect\\Teste01\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v4"" ""--napi_version=4"" ""--node_abi_napi=napi"" ""--napi_build_version=4"" ""--node_napi_label=napi-v4""gyp ERR! cwd C:\Connect\Teste01\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v10.16.3gyp ERR! node-gyp -v v3.8.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=C:\Connect\Teste01\node_modules\@tensorflow\tfjs-node\lib\napi-v4\tfjs_binding.node --module_name=tfjs_binding --module_path=C:\Connect\Teste01\node_modules\@tensorflow\tfjs-node\lib\napi-v4 --napi_version=4 --node_abi_napi=napi --napi_build_version=4 --node_napi_label=napi-v4' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (C:\Connect\Teste01\node_modules\node-pre-gyp\lib\util\compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:198:13)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:259:5)node-pre-gyp ERR! System Windows_NT 10.0.18362node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Connect\\Teste01\\node_modules\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-node-pre-gyp ERR! cwd C:\Connect\Teste01\node_modules\@tensorflow\tfjs-nodenode-pre-gyp ERR! node -v v10.16.3node-pre-gyp ERR! node-pre-gyp -v v0.13.0node-pre-gyp ERR! not oknpm WARN saveError ENOENT: no such file or directory, open 'C:\Connect\Teste01\package.json'npm WARN enoent ENOENT: no such file or directory, open 'C:\Connect\Teste01\package.json'npm WARN Teste01 No descriptionnpm WARN Teste01 No repository field.npm WARN Teste01 No README datanpm WARN Teste01 No license field.+ @tensorflow/tfjs-node@1.2.8updated 1 package and audited 533 packages in 6.855sfound 0 vulnerabilities#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['duplicate of https://github.com/tensorflow/tfjs/issues/1912=====', 'Use:npm install @tensorflow/tfjs@1.2.7 andnpm install @tensorflow/tfjs-node@1.2.7This solved my problem.=====', '> duplicate of #1912Thanks=====']",0
https://github.com/tensorflow/tfjs/issues/4540,Error: The dict provided in model.execute(dict) has keys: [ToFloat] that are not part of graph,1,closed,2021-01-15T15:46:04Z,2021-01-15T17:56:10Z,"hi all, I have got the captioned error when running the following scripts for Google AutoML multiple object detection. Is there any clue ? <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-automl""></script><img id=""mahjong"" src=""tiles2.jpg""><script>async function run() {  //const model = await tf.automl.loadImageClassification('model.json'),  const model = await tf.automl.loadObjectDetection('model.json'),  const image = document.getElementById('mahjong'),  const options = {score: 0.5, iou: 0.5, topk: 20},  //const predictions = await model.classify(image),  const predictions = await model.detect(image, options),  console.log(predictions),  // Show the resulting object on the page.  const pre = document.createElement('pre'),  pre.textContent = JSON.stringify(predictions, null, 2),  document.body.append(pre),}run(),</script>","[""It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tfjs/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.=====""]",0
https://github.com/tensorflow/tfjs/issues/5942,Npm installation fails.,19,open,2021-12-14T15:26:25Z,2021-12-21T18:40:08Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - - TensorFlow.js installed from (npm or script link): npm install @tensorflow/tfjs-node-gpu- TensorFlow.js version: 3.12.0- CUDA/cuDNN version: CUDA@11.5, cuDNN@8.3.1#define CUDNN_MAJOR 8#define CUDNN_MINOR 3#define CUDNN_PATCHLEVEL 1**Describe the problem**I opened up a default react ts project in vs 2022 stable version. View -> terminal. In terminal, npm install @tensorflow/tfjs or tfjs-node or tfjs-node-gpu. None of them works.I also tried npm install in the terminal directly opened from Windows 10 and failed.I don't know if this issue is caused by the internet since I'm in China. The familiar 404, you know.**Provide the exact sequence of commands / steps that you executed before running into the problem**As above.**Any other info / logs** npm install @tensorflow/tfjs-node-gpu> @tensorflow/tfjs-node-gpu@3.12.0 install E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu> node scripts/install.js gpu downloadGPU-windows-3.12.0.zip* Downloading libtensorflowhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.7.0.zip[==============================] 11266495/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/GPU-windows-3.12.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node-gpu@3.12.0 and node@14.17.3 (node-v83 ABI, unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/GPU-windows-3.12.0.zipgyp ERR! find VSgyp ERR! find VS msvs_version not set from command line or npm configgyp ERR! find VS VCINSTALLDIR not set, not running in VS Command Promptgyp ERR! find VS unknown version ""undefined"" found at ""C:\Program Files\Microsoft Visual Studio\2022\Community""gyp ERR! find VS checking VS2019 (16.11.31911.196) found at:gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community""gyp ERR! find VS - found ""Visual Studio C++ core features""gyp ERR! find VS - found VC++ toolset: v142gyp ERR! find VS - missing any Windows SDKgyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usegyp ERR! find VS looking for Visual Studio 2015gyp ERR! find VS - not foundgyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8gyp ERR! find VSgyp ERR! find VS **************************************************************gyp ERR! find VS You need to install the latest version of Visual Studiogyp ERR! find VS including the ""Desktop development with C++"" workload.gyp ERR! find VS For more information consult the documentation at:gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsgyp ERR! find VS **************************************************************gyp ERR! find VSgyp ERR! configure errorgyp ERR! stack Error: Could not find any Visual Studio installation to usegyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16gyp ERR! stack     at ChildProcess.exithandler (child_process.js:326:5)gyp ERR! stack     at ChildProcess.emit (events.js:375:28)gyp ERR! stack     at maybeClose (internal/child_process.js:1055:16)gyp ERR! System Windows_NT 10.0.19043gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""gyp ERR! cwd E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpugyp ERR! node -v v14.17.3gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:375:28)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1055:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:288:5)node-pre-gyp ERR! System Windows_NT 10.0.19043node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpunode-pre-gyp ERR! node -v v14.17.3node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not oknpm WARN @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@7.16.0 requires a peer of @babel/core@^7.13.0 but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""win32"",""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules\webpack-dev-server\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""win32"",""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules\watchpack-chokidar2\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""win32"",""arch"":""x64""})npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node-gpu@3.12.0 install: `node scripts/install.js gpu download`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node-gpu@3.12.0 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:One last line is note copied. It's only a path.Tomorrow I'll try installing the python version with pip to check if both ways fail. Btw, would you please add some guide for typescript with tf. The type system is too helpful. Thanks.","['Yes this is due to links not accessible from China , @pyu10055 @jasonmayes is there any way we can install tfjs from China ?=====', 'If you could try the python way to see if that fails too so we can narrow down if this is specific to TFJS or NPM that would be useful.=====', '> Yes this is due to links not accessible from China , @pyu10055 @jasonmayes is there any way we can install tfjs from China ?> If you could try the python way to see if that fails too so we can narrow down if this is specific to TFJS or NPM that would be useful.Hi. I just started the installing of python version. The downloading speed is 7 to 40 kB/s, and it probably takes 4 to 8 hours according to the terminal. Offline installer is way better in such a case. Offline installer or any zip is definitely way better for js version since even the link is eventually accessible in China, the downloading speed would probably be very slow.If any issue occurs half way, all the efforts before the issue are wasted. Which could lengthen the installation dramatically.When I downloaded CUDA and cuDNN, the speed is 11 MB/s ish. I downloaded them via firefox. Even for those I have to download with the proxy, the speed is at least 400kB and in avg of 1MB/s.So, please consider offline installer. Thanks.=====', ""py 3.9.6pip 21.3.1Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.9 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.8 six-1.16.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.0 termcolor-1.1.0 typing-extensions-4.0.1 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.13.3 zipp-3.6.0WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)and then, import tensorflow as tf>>> print(tf.__version__)2.7.0I believe it works.Now I plan to update the npm and try again. But since vs2022 also failed, I don't expected the newest npm would work.====="", ""Update:I reinstall node.js. Updated the npm (npm: '8.3.0',). Then it's very easy to install tensorflow with npm. Sorry for disturbing all of you. By the way, would you remind people to make sure they installed the newest npm. Because as you see, the log I posted yesterday didn't mention the version. Npm works for all the other cases expect for tensorflow in my computer. So I didn't doubt the npm to be out dated at all.Anyway, I don't have to learn python. :)====="", 'Thank you @YagaoDirac. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5942"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5942"">No</a>=====', ""> Update: I reinstall node.js. Updated the npm (npm: '8.3.0',). Then it's very easy to install tensorflow with npm. Sorry for disturbing all of you. By the way, would you remind people to make sure they installed the newest npm. Because as you see, the log I posted yesterday didn't mention the version. Npm works for all the other cases expect for tensorflow in my computer. So I didn't doubt the npm to be out dated at all. Anyway, I don't have to learn python. :)I've upgraded to npm 8.3.0 and I'm on node 16.13.1.  I'm still getting the same error you were getting before. Are there any additional steps you did besides upgrading? ====="", '@yagaodirac Very happy to see you managed to resolve by using a higher version of NPM. I shall relay this message to the team to update this. Can you also let me know which tutorials you were following so I can check the documentation for those too.@deadly Are you also based in China? =====', 'No, I am in America.=====', '@deadly Can you confirm more details about your system. OS / Node + NPM version etc =====', '@jasonmayes My OS is Windows 10 (19043.1348). Running the node -v command, my node version is 16.3.1. Running the npm -v command, my npm version is 8.3.0.Here is the full error I get when trying to install:```npm ERR! path C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! command failednpm ERR! command C:\\WINDOWS\\system32\\cmd.exe /d /s /c node scripts/install.jsnpm ERR! CPU-windows-3.12.0.zipnpm ERR! https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.7.0.zipnpm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnpm ERR! node-pre-gyp info it worked if it ends with oknpm ERR! node-pre-gyp info using node-pre-gyp@1.0.4npm ERR! node-pre-gyp info using node@16.13.1 | win32 | x64npm ERR! node-pre-gyp info check checked for ""C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" (not found)npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! node-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! node-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.12.0 and node@16.13.1 (node-v93 ABI, unknown) (falling back to source compile with node-gyp)npm ERR! node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@8.3.0npm ERR! gyp info using node@16.13.1 | win32 | x64npm ERR! gyp info oknpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@8.3.0npm ERR! gyp info using node@16.13.1 | win32 | x64npm ERR! gyp info find Python using Python version 3.10.1 found at ""C:\\Python310\\python.exe""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS msvs_version was set from command line or npm confignpm ERR! gyp ERR! find VS - looking for Visual Studio version 2015npm ERR! gyp ERR! find VS VCINSTALLDIR not set, not running in VS Command Promptnpm ERR! gyp ERR! find VS checking VS2019 (16.4.29806.167) found at:npm ERR! gyp ERR! find VS ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community""npm ERR! gyp ERR! find VS - found ""Visual Studio C++ core features""npm ERR! gyp ERR! find VS - found VC++ toolset: v142npm ERR! gyp ERR! find VS - found Windows SDK: 10.0.18362.0npm ERR! gyp ERR! find VS - msvs_version does not match this versionnpm ERR! gyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usenpm ERR! gyp ERR! find VS looking for Visual Studio 2015npm ERR! gyp ERR! find VS - not foundnpm ERR! gyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS valid versions for msvs_version:npm ERR! gyp ERR! find VS - ""2019""npm ERR! gyp ERR! find VS - ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! configure errornpm ERR! gyp ERR! stack Error: Could not find any Visual Studio installation to usenpm ERR! gyp ERR! stack     at VisualStudioFinder.fail (C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:121:47)npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:74:16npm ERR! gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:351:14)npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:70:14npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:372:16npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\util.js:54:7npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\util.js:33:16npm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:404:5)npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)npm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)npm ERR! gyp ERR! System Windows_NT 10.0.19043npm ERR! gyp ERR! command ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\\\Users\\\\redacted\\\\node_modules\\\\@tensorflow\\\\tfjs-node\\\\lib\\\\napi-v8\\\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=C:\\\\Users\\\\redacted\\\\node_modules\\\\@tensorflow\\\\tfjs-node\\\\lib\\\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8"" ""--python=C:\\\\Users\\\\redacted\\\\.windows-build-tools\\\\python27\\\\python.exe"" ""--msvs_version=2015""npm ERR! gyp ERR! cwd C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! gyp ERR! node -v v16.13.1npm ERR! gyp ERR! node-gyp -v v8.3.0npm ERR! gyp ERR! not oknpm ERR! node-pre-gyp ERR! build errornpm ERR! node-pre-gyp ERR! stack Error: Failed to execute \'C:\\Program Files\\nodejs\\node.exe C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js configure --fallback-to-build --module=C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node --module_name=tfjs_binding --module_path=C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8 --python=C:\\Users\\redacted\\.windows-build-tools\\python27\\python.exe --msvs_version=2015\' (1)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (C:\\Users\\redacted\\node_modules\\@mapbox\\node-pre-gyp\\lib\\util\\compile.js:89:23)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.emit (node:events:390:28)npm ERR! node-pre-gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)npm ERR! node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)npm ERR! node-pre-gyp ERR! System Windows_NT 10.0.19043npm ERR! node-pre-gyp ERR! command ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Users\\\\redacted\\\\node_modules\\\\@mapbox\\\\node-pre-gyp\\\\bin\\\\node-pre-gyp"" ""install"" ""--fallback-to-build""npm ERR! node-pre-gyp ERR! cwd C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! node-pre-gyp ERR! node -v v16.13.1npm ERR! node-pre-gyp ERR! node-pre-gyp -v v1.0.4npm ERR! node-pre-gyp ERR! not oknpm ERR! * Downloading libtensorflownpm ERR!npm ERR! * Building TensorFlow Node.js bindingsnpm ERR! A complete log of this run can be found in:npm ERR!     C:\\Users\\redacted\\AppData\\Local\\npm-cache\\_logs\\2021-12-15T02_26_21_080Z-debug.log```The Visual Studio C++ workload error confused me, so I tried to reinstall the correct workload for Visual Studio, but this did not fix the issue.=====', 'Can you try using the windows sub system for linux. You should be able to install popular distros via the windows store for example like Ubuntu. If you install TensorFlow on those it should work just fine.=====', 'This tutorial may help if you have not done that before: https://ubuntu.com/tutorials/ubuntu-on-windows=====', 'same issue here cannot solve=====', '@jasonmayes installing the library from the ubuntu subsystem worked (node version 17.3.0 and npm 8.3.0). I upgraded to node 17.3.0 on my main windows system and still continued to get the same error.=====', 'It seems there may be an issue with Windows that needs further investigation however the recommended way to use for Windows 10 users for now would be to use the Ubuntu subsystem which seems to work for all.=====', 'Running into the same error on Node v14, Windows 11.=====', 'Re-opening for team to investigate once folk are back in the office after holidays as it seems to be effecting a lot of folk.=====']",1
https://github.com/tensorflow/tfjs/issues/2459,tf.browser.fromPixel(video) causes memory leak,1,closed,2019-12-02T16:27:17Z,2019-12-23T17:46:44Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.3.1#### Browser versionChrome Version 75.0.3770.142#### Describe the problem or feature requestI wanted to get the current frame data from a video using tf.browser.fromPixels(video), but every time this function is called, there is a memory leak. #### Code to reproduce the bug / link to feature request`async function  drawScreen () {        	console.log(tf.memory()),		var frame = tf.tidy( () => {			return  tf.browser.fromPixels (videoElement, 3).expandDims(0).toFloat().div(tf.scalar(255))			}),		console.log(tf.memory()),		hr_image_0 = tf.tidy( () => {			return net.execute(frame).squeeze().clipByValue(0,1),		}),		tf.dispose(frame),		console.log(tf.memory()),		tf.browser.toPixels(hr_image_0, ccanvas),		await tf.tidy( () =>  {			 tf.browser.toPixels(hr_image_0, ccanvas),		}),		console.log(tf.memory()),	}`I used > requstAnimationFrameto call the drawScreen function.Running the code on Chrome, the following is printed on the consoleObject { unreliable: false, numBytesInGPU: 210113944, numTensors: 172, numDataBuffers: 172, numBytes: 211660580 }Object { unreliable: false, numBytesInGPU: 210344344, numTensors: 173, numDataBuffers: 173, numBytes: 211833380 }Object { unreliable: false, numBytesInGPU: 212187544, numTensors: 173, numDataBuffers: 173, numBytes: 213215780 }Object { unreliable: false, numBytesInGPU: 213745048, numTensors: 173, numDataBuffers: 173, numBytes: 213215780 }If you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","[""You need to dispose hr_image_0, this memory allocation is expected to increase without disposing that variable since it's the output of the second tf.tidy().=====""]",0
https://github.com/tensorflow/tfjs/issues/5143,Unhandled Rejection (Error): No backend found in registry on Tensorflow.js using mobilenet model and KNN classifier,1,closed,2021-05-30T11:46:07Z,2021-05-30T17:41:18Z,"After I import into my project ReactJS `import * as mobilenet from '@tensorflow-models/mobilenet',``import * as knnClassifier from '@tensorflow-models/knn-classifier',`And later I create model mobilenet and knn classifier `  const classifier = knnClassifier.create(),``const mobilenetModule = await mobilenet.load(),`then it errors as image under.Screenshot: ![image](https://user-images.githubusercontent.com/70431419/120102719-98df3a00-c176-11eb-96a3-689e8d1a95d6.png)",['I have fixed the bug already. Only need to reinstall the package ` mobilenet ` with version 2.0.4 and package ` tfjs ` version 1.6.0 are handled issue. ====='],1
https://github.com/tensorflow/tfjs/issues/3504,Unable to load model using asyncStorageIO API in react-native,12,closed,2020-06-24T06:14:27Z,2021-10-15T09:20:05Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version0.3.0#### Browser versionReact Native: v0.61.5#### Describe the problem or feature requestI am getting the following error while loading a model using AsyncStorageIO API.```Row too big to fit into CursorWindow requiredPos=0, totalRows=1* http://192.168.1.7:19001/index.bundle?platform=android&dev=true&minify=false&hot=false:203062:24 in convertError* [native code]:null in map- node_modules/@react-native-community/async-storage/src/AsyncStorage.native.js:97:32 in RCTAsyncStorage.multiGet$argument_1- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:483:4 in __invokeCallback- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:135:28 in __guard$argument_0- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:384:10 in __guard- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:134:17 in __guard$argument_0* [native code]:null in invokeCallbackAndReturnFlushedQueue```In my project, I have a model with json and weights which I have loaded using bundleResourceIO API. Then, I am updating the model using the given code and saving it with asyncStorageIO API. This flow goes well and good. But, I try to load the model using asyncStorageIO API, I get the previously described error.``` const optimizer = tf.train.adam(),      model.compile({        optimizer: optimizer,        loss: ""categoricalCrossentropy"",        metrics: [""accuracy""],      }),model        .trainOnBatch(          tf            .tensor2d(imageTensors, [numOfElems, 784])            .reshape([numOfElems, 28, 28, 1]),          tf.tensor2d(labelTensors, [numOfElems, 10]).reshape([numOfElems, 10])        )        .then((d) => {          console.log(""Model updated""),          model            .save(asyncStorageIO(""mnist-model""))            .then((res) => console.log(""Model Saved"", res))            .catch((e) => console.log(e)),        }),```","['How big is the model? Does the same error occur if you try to save and load from Async storage after loading the model without doing any further training?=====', '@tafsiri I am using the official Mnist Model which is trained using around 60000 images. Here\'s the code for the loading the model with model\'s json & weights using bundleResourceIO, saving it using asyncStorageIO without any predictions and then loading using asyncStorageIO.```const modelJson = require(""../assets/models/mnist.json""),const modelWeights = require(""../assets/models/mnist_weights.bin""),tf.loadLayersModel(bundleResourceIO(modelJson, modelWeights))    .then((model) => {        console.log(""Loaded model using bundleResourceIO""),        setModel(model),        model            .save(asyncStorageIO(""model-async-storage""))            .then((res) => {                console.log(""Saved model using asyncStorageIO"", res),                tf.loadLayersModel(asyncStorageIO(""model-async-storage""))                    .then((d) => console.log(""Loaded model using asyncStorageIO""))                    .catch((e) => console.log(""Error loading model error using asyncStorageIO"",e)),            })            .catch((e) => console.log(""Error saving model using asyncStorageIO"", e)),    })    .catch((e) => {        throw `Error loading model using bundleResourceIO: ${e}`,    }),```Here\'s the error console output when I ran the above code without clearing the app storage. ```Loaded model using bundleResourceIOSaved model using asyncStorageIO Object {  ""modelArtifactsInfo"": Object {    ""dateSaved"": 2020-06-24T14:22:09.590Z,    ""modelTopologyType"": ""JSON"",    ""weightDataBytes"": 2379688,  },}Error loading model error using asyncStorageIO [Error: In local storage, the binary weight values of model \'model-async-storage\' are missing.][Unhandled promise rejection: Error: database or disk is full (code 13 SQLITE_FULL)]* http://192.168.1.7:19001/index.bundle?platform=android&dev=true&minify=false&hot=false:203048:24 in convertError* [native code]:null in map- node_modules/@react-native-community/async-storage/src/AsyncStorage.native.js:121:22 in Promise$argument_0- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:483:4 in __invokeCallback- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:135:28 in __guard$argument_0- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:384:10 in __guard- node_modules/react-native/Libraries/BatchedBridge/MessageQueue.js:134:17 in __guard$argument_0* [native code]:null in invokeCallbackAndReturnFlushedQueue```Here\'s the error console output when I ran the above code after clearing storage. ```Loaded model using bundleResourceIOSaved model using asyncStorageIO Object {  ""modelArtifactsInfo"": Object {    ""dateSaved"": 2020-06-24T14:42:22.839Z,    ""modelTopologyType"": ""JSON"",    ""weightDataBytes"": 2379688,  },}Error loading model error using asyncStorageIO [Error: Row too big to fit into CursorWindow requiredPos=0, totalRows=1]```=====', 'I basically want to further train the pre-trained model using new images and update the model to be used later on. =====', '> How big is the model? Does the same error occur if you try to save and load from Async storage after loading the model without doing any further training?=====', '> > How big is the model? Does the same error occur if you try to save and load from Async storage after loading the model without doing any further training?@tafsiri  The model size is 2379688 bytes. If I try to create a new custom model of size approx 111234 bytes and save/load using asyncStorage, it works.The original model from mnist if loaded using bundleResource and then with asyncStorage creates error.I am using tfjs-react-native as a PoC for my project. For that, I will need to retrain the model with new data on mobile and then send the updated weights to the cloud. It would be great if you can help me resolve the issue asap.=====', ""It looks like your device just won't let you save a model of that size locally to async storage. If you are on android you can try adjusting the limits for async storage size https://react-native-community.github.io/async-storage/docs/advanced/db_size (though it appears your model should be under that limit but it's worth a try). Have you tried this on any other device? Do you see the same behaviour?In the near term, I'd probably suggest you use your idea of pushing the saved weights to the cloud. The http IO handler might help, see https://www.tensorflow.org/js/guide/save_load (search for https request). It wasn't written explictly for react native but might work. Else you might need to write a custom IO handler (aslo described in that guide).====="", 'Also could you post device/os information?=====', 'I am running the react-native application in Redmi Note 5 Pro 4GB RAM/ 64GB Internal Storage with OS version Android 10.=====', ""@geetesh-gupta The AsyncStorage buffer has a limit of 2MB per entry so any value bigger that that will cause this error.Here's some things you can try:1) use this library that stores the data on the file systemhttps://www.npmjs.com/package/react-native-fs-storeDownside: app will be slower while reading/writing.2) this method will work depending on the structure of the model. If the model object is consisted of many key values then it should work well, otherwise, try the first methodUse this code in your app to wrap the AsyncStorage: https://gist.github.com/bureyburey/2345dfa88a31e00a514479be37848d42It's a wrapper for completely different usage but it may be useful here as well, depending on the case i stated above.====="", 'Hi, thanks for your responses 😊. Currently due to time constraint, I created a different model with less layers that can work with the project, so reducing the size to 172 KB. But, when I will get time, I will try to incorporate the above suggested solutions and will respond back.=====', ""Hey, sorry for bringing this up again, but I have the same problem as @geetesh-gupta on **Android** only. iOS works just fine.For the first run, I download my model directly from the cloud using `const model = tf.loadLayersModel(modelUri)`.Then, I save it on asyncStorage doing `model.save(asyncStorageIO('custom-model'))`. For the second run and on, I have a logic to check if I have any model saved on the asyncStorage and load it if there is. This is working very nicely on iOS, though I'm facing the same problems that @geetesh-gupta reported on **Android**.I have a 20MB sized TF custom model.I have updated my `gradle.properties` file, as @tafsiri suggested from the AsyncStorage docs, to have a max of 50 MB in the SQLite DB: `AsyncStorage_db_size_in_MB=50`, but even though, it looks like this is an inner limitation of SQLite that can only work with a maximum **2MB** in the Cursor Window buffer.Even though it feels like **I can save** the model in the first trial, when I try to load it I get this: `[Error: Row too big to fit into CursorWindow requiredPos=0, totalRows=1]`And if I try to save again the Model with the same path Key (to overwrite it), then I get this:`Out of memory stringify@[native code]`@tafsiri - right now, I'm trying to save/retrieve model to the FileSystem, using `react-native-fs`.Do you think this approach could work? Or do you have any other workaround suggestion?Best regards.====="", ""> Hey, sorry for bringing this up again, but I have the same problem as @geetesh-gupta on **Android** only. iOS works just fine.> > For the first run, I download my model directly from the cloud using `const model = tf.loadLayersModel(modelUri)`. Then, I save it on asyncStorage doing `model.save(asyncStorageIO('custom-model'))`. For the second run and on, I have a logic to check if I have any model saved on the asyncStorage and load it if there is. This is working very nicely on iOS, though I'm facing the same problems that @geetesh-gupta reported on **Android**.> > I have a 20MB sized TF custom model. I have updated my `gradle.properties` file, as @tafsiri suggested from the AsyncStorage docs, to have a max of 50 MB in the SQLite DB: `AsyncStorage_db_size_in_MB=50`, but even though, it looks like this is an inner limitation of SQLite that can only work with a maximum **2MB** in the Cursor Window buffer.> > Even though it feels like **I can save** the model in the first trial, when I try to load it I get this: `[Error: Row too big to fit into CursorWindow requiredPos=0, totalRows=1]`> > And if I try to save again the Model with the same path Key (to overwrite it), then I get this: `Out of memory stringify@[native code]`> > @tafsiri - right now, I'm trying to save/retrieve model to the FileSystem, using `react-native-fs`. Do you think this approach could work? Or do you have any other workaround suggestion?> > Best regards.Exact same for me, my model is 12MB =====""]",0
https://github.com/tensorflow/tfjs/issues/5935,"tf-backend-wasm.node.js  occurred  ""memory out of bounds""",1,open,2021-12-10T17:50:41Z,2021-12-10T22:38:23Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): tfjs-backend-wasm  3.12.0- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**When running under the wasm backend many times, ""memory out of bounds"" will always appear.  At the beginning, the program can run normally, and after a short time, ""memory out of bounds"" appears all the time.  The strange thing is that when the shape of the tensor does not change, there is no such issue. **Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.```var tf= require(""@tensorflow/tfjs""),const wasm= require(""@tensorflow/tfjs-backend-wasm""),wasm.setWasmPaths('./node_modules/@tensorflow/tfjs-backend-wasm/dist/'),async function fuzz() {        await tf.setBackend('wasm'),        await tf.ready(),        var random1=Math.floor(Math.random()*5+1),        var random2=Math.floor(Math.random()*35+1),        var random3=Math.floor(Math.random()*35+1),        var random4=Math.floor(Math.random()*35+1),        var input_array=new Array(random1*random2*random3*random4),        for (var i=0,i<input_array.length,i++){            input_array[i]=3,        }        try {            var input_tensor=await tf.tensor(input_array,[random1,random2,random3,random4],""float32""),            layer =  await tf.layers[""conv2d""]({filters:3,kernelSize:3,strides:1}),            var prediction = await layer.apply(input_tensor),            tf.dispose(input_tensor),        }catch (e){           console.log(e),        }    }async function f(){    while (1){        await fuzz(),    }}f(),```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['my best-guess is that conv layer gets created with a kernel func for a specific shape and if shape is the same, it gets reused. but when shape changes, it creates new kernel func while previous one never got disposed.=====']",1
https://github.com/tensorflow/tfjs/issues/4313,"tf.tensor([ 17378479 ]).print(),returns 17378480",2,open,2020-11-27T01:19:45Z,2021-11-05T19:00:11Z,"Thanks for all tensorflow jobs!I'm trying to modify image with using tensorflow.js.and i found that some integer will change via tensorflow object.But I know that it is not critical for machine learning.So, I want to know ...Is it a important bug or inevitable spec for tensorflow.js?**System information**- Have I written custom code    e.g.)   tf.tensor([ 17378479 ]).print(), // => 17378480  tf.tensor1d([ 17378479 ], 'int32').print() // => 17378479  tf.tensor1d([ 17378479 ], 'float32').print() // => 17378480  tf.tensor1d([ 17378479 ], 'int32').slice([0], [1]).print() // => 17378479 (In some case, changes to 17378480)- OS Platform and Distribution:  macOS 11.0.1 (20B29)- Mobile device:  MacBookAir9,1- TensorFlow.js version:  v2.6.0 / v2.7.0- Browser version:  Chrome 87.0.4280.67（Official Build）beta （x86_64）  MSEdge 87.0.664.47 (Official Build) (x86_64)  Or another","[""By default tensors will be created at float32 tensors, which will have some degree of numerical imprecision, so i'm not particularly surprised with your first three lines, however could you say more about the last line.`tf.tensor1d([ 17378479 ], 'int32').slice([0], [1]).print() // => 17378479 (In some case, changes to 17378480)`In particular you say in some cases it changes, Could you elaborate on when that happens? Does that happen when repeatedly running the same line over and over again in the same environment?====="", ""@tafsiri Thanks for your reply.Not by a repeatedly running. Run the code below. You can see the value change via .slice( ) method.```jslet x = 127,tf.tensor1d(new Array(x).fill(0).concat(17378479), 'int32').slice(0, x + 1).dataSync()[x], // => 17378480```And the case below, the matter is not occured.```jslet x = 127,tf.tensor1d(new Array(x).fill(0).concat(17378479), 'int32').dataSync()[x], // => 17378479``````jslet x = 126,tf.tensor1d(new Array(x).fill(0).concat(17378479), 'int32').slice(0, x + 1).dataSync()[x], // => 17378479```=====""]",1
https://github.com/tensorflow/tfjs/issues/1770,[integration_tests] Browser benchmarks fails due to node-pre-gyp and aws-sdk,1,closed,2019-07-23T14:42:51Z,2019-07-24T22:06:39Z,Step to reproduce:1. Checkout the tfjs (union) repo at head2. cd integration_tests3. yarn benchmark --layersNotice the error message:23 07 2019 10:54:53.444:ERROR [karma-server]: Error: Unable to resolve module [aws-sdk] from [.../tfjs/tfjs/integration_tests/node_modules/node-pre-gyp/lib/unpublish.js],['closing this as the related PR has been merged. Thank you ====='],0
https://github.com/tensorflow/tfjs/issues/4149,"model.fit() reports non-NaN float losses, but returns NaN when inferencing even though weights haven't changed",4,closed,2020-10-28T05:11:20Z,2021-03-26T10:30:24Z,"**System information**- Have I written custom code: Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS ~13- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: yes, iPhone X- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.7.0- Browser version: Safari - Tensorflow.js Converter Version: N/A**Describe the current behavior**I'm trying to fine-tune a complex computer vision neural net on the mobile browser, specifically iOS's Safari. The model architecture is really long and I don't think it's the problem, but basically it takes in two 128x128x3 images and an 8 vector, runs each input through some hidden layers, concats them and then runs them through some more hidden layers, and then outputs two numbers representing two normalized coordinates. My backend is webgl, and it's set before any other operations happen. I'm loading in the model (it was converted from a keras model using the python tfjs converter), then freezing every layer except the last few (everything here before the dense_3 has their trainable set to false): ![Screen Shot 2020-10-28 at 12 04 37 AM](https://user-images.githubusercontent.com/29759597/97389833-baf79d80-18b1-11eb-981a-5a2a2a245325.png)I then try to fit it using this code. It just compiles the model using sgd and runs fit, and at the end of every epoch evaluates it on random inputs and prints the current loss, :```javascriptnaturemodel.predict([tf.randomNormal([1,128,128,3]), tf.randomNormal([1,128,128,3]), tf.randomNormal([1,8])]).print()var epochCount = 0// Compile the modelnaturemodel.compile({  optimizer: tf.train.sgd(0.0000268),  loss: 'meanSquaredError',  metrics: ['mae', 'mse']}),var n = 2,console.log(tf.memory())naturemodel.fit([tf.randomNormal([n,128,128,3]), tf.randomNormal([n,128,128,3]), tf.randomNormal([n,8])], [tf.randomNormal([n,2])], {       epochs: 3,       callbacks: {          onEpochEnd: async (batch, logs) => {                console.log('in batch predict ', naturemodel.predict([tf.randomNormal([1,128,128,3]),                                                tf.randomNormal([1,128,128,3]), tf.randomNormal([1,8])]).arraySync())                console.log(epochCount++, 'Loss: ' + logs.loss.toFixed(5)),}                }     })naturemodel.predict([tf.randomNormal([1,128,128,3]), tf.randomNormal([1,128,128,3]), tf.randomNormal([1,8])]).print()```So, originally, the losses, weights, and biases were all NaN after training no matter how small the learning rate was: ![naturemodel.fit attempt with WEBGL_CPU_FORWARD left default, true](https://user-images.githubusercontent.com/29759597/97389970-1e81cb00-18b2-11eb-89d6-6c56c2486df1.png)I then turned off the WEBGL_CPU_FORWARD flag with tf.ENV.set('WEBGL_CPU_FORWARD', false),. The model started reporting float losses (good!), but when evaluated on any inputs, the model returned NaN. This behavior continues even if I turn the WEBGL_CPU_FORWARD flag back to true. This is especially weird when you consider that the weights and biases of the last few layers were left unchanged (towards the bottom of the pic), so the model shouldn't have NaN output since it should be doing the same math as it was before. And also, if the loss is not NaN, why would the outputs be?![naturemodel.fit attempt with WEBGL_CPU_FORWARD false](https://user-images.githubusercontent.com/29759597/97392697-cac6b000-18b8-11eb-9ab0-d63dacb0d3ce.png)**Describe the expected behavior**After running model.fit, I should to be able to use the model to do inference (especially if the weights are all still the same). I believe this has something to do with where the tensors are stored, or where the operations are occurring (cpu vs webgl), but I really don't know.","[""Have made some progress on a workaround. Duplicating the model after training doesn't fix the problem (making a new ```tf.model({inputs: naturemodel.inputs, outputs:naturemodel.outputs}),```.However, replacing the unfrozen hidden layers (everything in the architecture screenshow) with other dense layers does allow me to retrain and use the outputs. The only thing different is there's no batch norm layers in the new layers, however that could be a red herring.EDIT: This doesn't work on mobile, I forgot I was testing on my desktop browser. What you can do is take the embeddings out from an intermediate layer and train a new model on that. ```javascriptvar modelcopy = tf.model({inputs: naturemodel.inputs, outputs: naturemodel.layers[33].output}), // This is the dense_4 layer's output var tmplayer1 = tf.layers.dense({units: 32, activation: 'relu'}),var tmplayer2 = tf.layers.dense({units: 2}),var newoutput = tmplayer2.apply(tmplayer1.apply(naturemodel.layers[33].output))var modelcopy = tf.model({inputs: naturemodel.inputs, outputs: newoutput}), // Model with new regression head attachednaturemodel = modelcopy,```====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4149"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4149"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5180,[webgpu]fall back to cpu or webgl,12,closed,2021-06-05T11:36:55Z,2021-06-15T05:55:10Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):yes, tfjs functional api model with custom layersself browserified tfjs-backend-webgpu github pull- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):chrome canary windows- TensorFlow.js installed from (npm or script link):link https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js at time of writing 3.7- TensorFlow.js version (use command below):3.7- Browser version:Version 93.0.4533.0 (Official Build) canary (64-Bit)**Describe the current behavior**model is running fine with webglif i change backend to webgpu on unsafe canary, following error shows up:Error: Kernel 'LogicalAnd' not registered for backend 'webgpu'**Describe the expected behavior**i know webgpu is at an alpha stage - but why doesn't it fall back to cpu or webgl for that part? am i missing something**Standalone code to reproduce the issue**i can provide the browserified version of the webgpu-backend, if needed.","[""i also tried with tf.env().set('WEBGPU_CPU_FORWARD', true), but no effect.====="", ""@BenjaminWegener Thanks for trying the webgpu backend. Current, the supported kernels are limited in webgpu. You can find all supported kernels [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgpu/src/register_all_kernels.ts). `LogicalAnd` is not supported yet. But it's easier to be added. >i also tried with tf.env().set('WEBGPU_CPU_FORWARD', true), but no effect.Setting `WEBGPU_CPU_FORWARD` true doesn't mean to fall back to cpu. It's just one of the conditions that it will run to cpu. Only if [all conditions](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgpu/src/backend_webgpu.ts#L837) are satisfied (it is based on performance consideration), it will go the cpu path. But all of these are based on that the corresponding kernel has been registered. > fall back to cpu or webgl.In our current design, we should register the kernel first. Then we can choose to fall back to cpu or use the backend implementation in the kernel config file. So the fix is to register the missed kernel for webgpu backend. ====="", 'Thank you for your answer. I need `tf.LogicalAnd` and `tf.Tile`. I will have to wait.=====', '@BenjaminWegener `tf.LogicalAnd` and `tf.Tile` are supported now in webgpu. Please let us know if you still meet any problems. Thanks.=====', 'Thank you very much, it really helps.If i run my model, the next Problem is`UnsortedSegmentSum`.=====', '...=====', ""@BenjaminWegener Do you mind to share more information about your model? Do you see any bad performance on webgl so that you want to try webgpu? Currently our main efforts are on performance . We want to make sure that webgpu really brings great performance when it's officially released.  But the supported ops may be not full. So I want to know the gap to enable your model. ====="", ""no problem. i am trying to implement a transformer like model in tfjs. just download or clone the repo an try it out. use chrome w/ webgpu .https://github.com/BenjaminWegener/transformer-tfjsthe model runs fine with ~6gig of GPU memory with webgl, i just wanted to try the speed of webgpu. so no, it isn't crucial to me, that it runs with webgpu (yet).thank you for your support-====="", ""@BenjaminWegener Can you review your model and list all the needed ops so that we can know the overall gap in webgpu? It's not efficient for us to implement these ops one by one. If you can provide the overall status, we can evaluate our efforts and give you better support.====="", 'I stripped everything unneccesary from the model. Looks like tf.layers.embedding uses `UnsortedSegmentSum`, which is the only OP my model is missing. But I cannot tell, if tf.layers.embedding uses another unsupported OP.=====', ""i replaced the embedding layer with a feedforward net. (dense-activation-dense). but if i use 'relu' activation like `tf.layers.dense({units: UNITS, activation: 'relu'}).apply(x)` i get the error that op `Step` isn't implemented. so i replace that with a custom gelu activation, which seems to be working. so for now i'm closing this. thank you for your help!====="", ""@BenjaminWegener Glad to know that you work around this issue. We'd like to hear your experience on webgpu. Do you see any perf issue with your model on webgpu? Please feel free to file bugs if you see any problems :) =====""]",1
https://github.com/tensorflow/tfjs/issues/164,Without browser,1,closed,2018-04-10T07:21:09Z,2018-04-10T14:04:26Z,Can I use it on a server?,['We will be tracking nodejs bindings here: https://github.com/tensorflow/tfjs/issues/36====='],0
https://github.com/tensorflow/tfjs/issues/2443,Add support for keras layer DenseFeatures,7,open,2019-11-27T17:38:29Z,2021-10-20T20:14:53Z,"#### TensorFlow.js version1.3.2#### Browser versionChome 78.0.3904.108#### Describe the feature requestAdd support for the [keras layer DenseFeatures](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures).#### Code to reproduce the bug / link to feature request##### Step 1:Create a model in Python using the DenseFeatures layer.```          feature_columns = [tf.feature_column.numeric_column('foo')]       feature_layer = tf.keras.layers.DenseFeatures(feature_columns)       model = tf.keras.Sequential([            feature_layer,            tf.keras.layers.Dense(8),            tf.keras.layers.Dense(1)      ])```##### Step 2:Train, fit and then save the model using `tfjs.converters.save_keras_model(model, 'path_to_model')`##### Step 3:Load the model into tfjs: `tf.loadLayersModel('./model.json'),`It will throw: `Error: Uncaught (in promise): Error: Unknown layer: DenseFeatures.`","['Will this be part of any upcoming tfjs release?=====', ""@caisq I don't think anyone is actively working on this. Mind if I add a contributions welcome tag here?====="", '@tafsiri Please go ahead.=====', 'For any interested, a workaround is to freeze the model. That way you can use DenseFeatures and give predictions in tfjs:```model.save(temp_path, overwrite=True)tfjs.converters.convert_tf_saved_model(temp_path, path)```and then```const model = await tf.loadGraphModel(path),```=====', '@blidblid, For some reasons suggested workaround does not apply for my model with the error:`AssertionError: Identity is not in graph`Meanwhile I have started porting `DenseFeatures` layer to tfjs in this branch:https://github.com/djemeljanovs/tfjs/tree/feature/dense-feature-layer-supportIt seems to be excessive amount of work, since not only `DenseFeatures`, but whole `FeatureColumn` API has not been ported to tfjs yet.@tafsiri, I tried to create branch for this on tfjs, but git did not allow me that, so I created a fork. Could you please check out if this is not the right place to do, I am quite new to contributions. Thank you!=====', '> For any interested, a workaround is to freeze the model. That way you can use DenseFeatures and give predictions in tfjs:> > ```> model.save(temp_path, overwrite=True)> tfjs.converters.convert_tf_saved_model(temp_path, path)> ```> > and then> > ```> const model = await tf.loadGraphModel(path),> ```hey @blidblid in which version of tensorflow have you tested this trick?=====', '```Python                   3.7.5tensorflow               2.3.0tensorflowjs             1.5.2```Not all feature columns seem to work with this method. I had to use `categorical_column_with_identity` over `categorical_column_with_vocabulary_list`.=====']",0
https://github.com/tensorflow/tfjs/issues/5701,TFJS-TFLITE error when model outputs quantized values when calling predict(...),1,closed,2021-10-08T04:51:50Z,2021-10-08T04:58:40Z,"**System information**Linux Ubuntu 18.04.03, Chrome 93TensorFlow.js installed from (npm or script link):Latest from NPM via script tag:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.js""></script>```**Tensorflow.js Converter Version:**Model created via TFLITE converter via python file (quantization aware training)**Describe the current behavior:**When running a model that outputs int8 values via TFJS-TFLITE, such as an image generation model, the stack crashes with the error:`Error: values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray`This occurs at: `TFLiteModel.predict (tflite_model.ts:134)`**Possible source of issue:**It appears the model is outputting an Int8Array that is passed to tensorflow's `tensor(...)` method, however that method appears to only accept `Float32Array, Int32Array, or Uint8Array` as per https://github.com/tensorflow/tfjs/issues/4009#issue-713346237**Describe the expected behavior:**TFJS-TFLITE should ensure that quantized image model's output are casted to UInt8Array, instead of Int8Array... or should expand support in tensorflowJS to accept Int8Array **Standalone code to reproduce the issue**```// Produce quantized 8 bit model that outputs imageslet model = await tflite.loadTFLiteModel('URL TO 8 bit quantized image generation model'),let cameraImage = new tf.tensor4d(      new Int32Array(        ctx.getImageData(0, 0, 256, 256).data),      [1, 256, 256, 4])cameraImage = tf.slice(cameraImage, [0, 0, 0, 0], [1, 256, 256, 3]) // a faster way to slice off the alpha channel instead of loop+iflet result = model.predict(cameraImage) // crash```**Other info / logs**```VM9506:1 Uncaught TypeError: Cannot read properties of undefined (reading 'data')    at eval (eval at TFLiteModel.predict (tflite_model.ts:133), <anonymous>:1:13)    at TFLiteModel.predict (tflite_model.ts:133)```","['duplicate of https://github.com/tensorflow/tfjs/issues/5700 , closing this issue.=====']",1
https://github.com/tensorflow/tfjs/issues/5486,Blazeface result is not correct on WASM backend,3,closed,2021-08-13T05:37:07Z,2021-08-13T19:57:27Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  This is happening even in demo page for blazeface model (i.e. https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html).- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Haven't checked yet on mobile, but probably it will exist in all the devices- TensorFlow.js installed from (npm or script link):  https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0- TensorFlow.js version (use command below):  3.1.0- Browser version: Chrome 91**Describe the current behavior**  Open the blazeface model demo page on `https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html`.   It is detecting the face on hands kept in front of webcam. If the face is present in the frame, then it detects properly the face and results are as expected. But if we don't have any face in the frame, in that case the blazeface detection results shows that face exists while it is Hand/mobile, etc.**Describe the expected behavior**  It should show result of face detected only if any face is present in frame. If there is no face in frame, then there is no point to show face detected.**Other info / logs** ![Screenshot from 2021-08-13 11-03-57](https://user-images.githubusercontent.com/41855523/129309933-86df836d-f3b2-4e0b-8a15-8fab539dc4cf.png)![Screenshot from 2021-08-13 11-04-43](https://user-images.githubusercontent.com/41855523/129309935-04ec7f13-ff90-4f0d-8b3a-586488615063.png)","['It tries to detect all the object but you need to check the probability , which will vary for actual face and some random objects. You can try to get the code locally and check for percentage accuracy.This question is better asked on [Discourse-TF](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5486"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5486"">No</a>=====', ""@rthadur Can you please tell me whats the probability of actual face should be then?As, we are calculating the probablity of face using the face landmarks. But with those, it's giving 98% probability of actual face on mobile device.Attaching the image for same.![Screenshot from 2021-08-14 01-16-28](https://user-images.githubusercontent.com/41855523/129411377-b094200b-25f3-4a54-8545-2c809b22a9dc.png)=====""]",1
https://github.com/tensorflow/tfjs/issues/4751,How to build tfjs locally ?,6,closed,2021-02-26T04:26:57Z,2021-03-11T16:43:17Z,**System information**- OS Platform and Distribution: Linux Ubuntu 18.04**Describe the problem**- I modified some files under directory of `tfjs` and `tfjs-backend-webgl` and I wanna build it. Is there a script to build  `tfjs`  ?- Do I have to add a new webpack and add some rules there? Thanks,"['not an official way, but my quick hack:```bashgit clone --depth 1 https://github.com/tensorflow/tfjscd tfjsfind . -maxdepth 1 -type d | while read d, do  pushd $PWD  cd $d  if [ -f package.json ], then    yarn install --ignore-optional --ignore-engines  fi  popddonecd ./tfjsyarn run build-depsyarn run buildyarn run build-npmcd ../tfjs-nodeyarn run build-npmcd ../tfjs-node-gpu./prep-gpu.shyarn run build-npmcd ../tfjs-backend-webgpuyarn run build-npm```=====', '@vladmandic  Thanks for your reply. I followed above commands. Error happens when  `yarn  run build-deps` is executed. I think this is something related to node or tsc, but I\'m not quite sure. By the way, I am building tfjs-v2.8.4 tag version.--- error message: ```bash/home/xlpiao/tfjs/tfjs:$ yarn run build-depsyarn run v1.22.5$ yarn build-core && yarn build-layers && yarn build-converter && yarn build-data && yarn build-backend-cpu && yarn build-backend-webgl$ cd ../tfjs-core && yarn && yarn build[1/5] Validating package.json...[2/5] Resolving packages...success Already up-to-date.$ node ./scripts/enumerate-tests.js && tsc && yarn bundle../../node_modules/@types/eslint/index.d.ts:260:41 - error TS2694: Namespace \'""/home/xlpiao/tfjs/tfjs-core/node_modules/@types/estree/index""\' has no exported member \'ChainExpression\'.260         ChainExpression?: (node: ESTree.ChainExpression & NodeParentExtension) => void,                                            ~~~~~~~~~~~~~~~../../node_modules/@types/eslint/index.d.ts:283:42 - error TS2694: Namespace \'""/home/xlpiao/tfjs/tfjs-core/node_modules/@types/estree/index""\' has no exported member \'ImportExpression\'.283         ImportExpression?: (node: ESTree.ImportExpression & NodeParentExtension) => void,                                             ~~~~~~~~~~~~~~~~Found 2 errors.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```=====', 'Hi Xianglan,I checked out tfjs-v2.8.4 but didn\'t see any issues when running ""yarn build-deps"" from tfjs/tjfs. It is a little bit strange to see that the error is in `tfjs-core/node_modules/@types/eslint`, because I don\'t actually see it on my machine. Here is everything under `tfjs-core/node_modules/@types`:```color-nameestreejasminelongminimatchnodenode-fetchoffscreencanvasresolveseedrandomwebgl-ext```Is it possible that this might be caused by some new code you wrote locally? Thanks! =====', '@jinglescode Thanks for you reply. I put `tfjs` under some other project and that might be the reason for the building error. Since I moved tfjs out of that project directory, above commands works well. Issue resolved. =====', 'Thank you for confirming , closing this issue.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4751"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4751"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4654,TF.js Models Handpose live example link is broken,1,closed,2021-02-08T16:28:36Z,2021-02-10T01:36:32Z,on the README for [TF.js-models](https://github.com/tensorflow/tfjs-models) HandPose live example link is broken,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4654"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4654"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5446,Allow setting number of threads programmatically for WASM backend,2,closed,2021-08-09T12:45:12Z,2021-10-15T21:24:12Z,"Currently number of WASM threads for `tfjs-backend-wasm` is fixed in build process for WASM files:`tfjs-backend-wasm/src/cc/BUILD`:```shellcc_binary(    name = ""tfjs-backend-wasm-threaded-simd.js"",    srcs = [""backend.cc""] + KERNELS_WITH_KEEPALIVE,    linkopts = BASE_LINKOPTS + [        ""-s EXPORT_NAME=WasmBackendModuleThreadedSimd"",        ""-s MALLOC=emmalloc"",        ""-s USE_PTHREADS=1"",        ""-s PROXY_TO_PTHREAD=1"",        # Many x86-64 processors have 2 threads per core, so we divide by 2.        ""-s PTHREAD_POOL_SIZE="" +        ""'Math.min(4, Math.max(1, (navigator.hardwareConcurrency || 1) / 2))'"",)```On the other hand, it can be freely set programmatically for `tfjs-tflite` backend:`tfjs-tflite/src/tflite_model.ts`:```js  if (options && options.numThreads !== undefined) {    curOptions.numThreads = options.numThreads,  } else {    curOptions.numThreads = await getDefaultNumThreads(),  }```Plus since it evaluates `navigator.hardwareConcurrency` during runtime, it auto-adjusts to users hardware much better,  (unlike `tfjs-backend-wasm` which just uses 4 threads regardless of available hardware)  Environment: TFJS 3.8.0CC @jinjingforever as author of code for TFLite","['cc @ahmedsabie @jinjingforever =====', '@ahmedsabie @jinjingforever @mattsoulanille any chance of getting this? should be trivial since it already works in `tflite` backend.=====']",1
https://github.com/tensorflow/tfjs/issues/5532,Unable to use the @tensorflow/tfjs-tflite package outside of a web browser,3,open,2021-08-25T18:37:52Z,2021-08-26T16:46:31Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): `yarn add @tensorflow/tfjs-tflite@0.0.1-alpha.4`- TensorFlow.js version: N/A- CUDA/cuDNN version: N/A**Describe the problem**The `@tensorflow/tfjs-tflite` package can't be used outside of a web browser due to its use of browser-specific APIs.**Provide the exact sequence of commands / steps that you executed before running into the problem**Create an empty NPM package and populate it with the following:```shell$ mkdir tensorflow-lite-repro && cd tensorflow-lite-repro$ yarn init $ cat package.json{  ""name"": ""tensorflow-lite-repro"",  ""version"": ""1.0.0"",  ""main"": ""index.js"",  ""license"": ""MIT"",  ""type"": ""module"",  ""dependencies"": {    ""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"",    ""@tensorflow/tfjs-core"": ""^3.8.0"",    ""@tensorflow/tfjs-tflite"": ""^0.0.1-alpha.4""  }}$ cat index.js// Adds the CPU backend.import '@tensorflow/tfjs-backend-cpu',// Import @tensorflow/tfjs-coreimport * as tf from '@tensorflow/tfjs-core',// Import @tensorflow/tfjs-tflite.import {loadTFLiteModel, TFLiteModel} from '@tensorflow/tfjs-tflite',```Next try to execute `index.js` using Node.```$ node --versionv16.7.0$ yarn --version1.22.5$ yarn install(snip)$ node index.js/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1261module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.getInstance=function(a,b,c){var d=a+b,c=URL.createObjectURL(new Blob([c],{type:""application/javascript""})),module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.instances.has(d)||(a=new module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader(a,                                                                                                                                                                                                   ^ReferenceError: Blob is not defined    at Function.module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.getInstance (/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1261:196)    at Object.<anonymous> (/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1331:447)    at Module._compile (node:internal/modules/cjs/loader:1101:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)    at Module.load (node:internal/modules/cjs/loader:981:32)    at Function.Module._load (node:internal/modules/cjs/loader:822:12)    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:196:29)    at ModuleJob.run (node:internal/modules/esm/module_job:183:25)    at async Loader.import (node:internal/modules/esm/loader:178:24)    at async Object.loadESM (node:internal/process/esm_loader:68:5)```You can monkey-patch `Blob` (e.g. by adding `window.Blob = require(""blob-polyfill"")` before the `@tensorflow/tfjs-tflite` import), but it then fails because [`Window.self`](https://developer.mozilla.org/en-US/docs/Web/API/Window/self) doesn't exist.","['There is an older version on npm which is 3 months old , we have a bunch of similar issues where users are using older version via npm , requesting @jinjingforever to push newer version to npm. Thank you =====', 'In the meantime, @rthadur do you know if it is possible to add the `@tensorflow/tfjs-tflite` package directly as a git dependency?=====', ""Thank you @Michael-F-Bryan for the report. Your are right. We actually haven't worked on nodejs support for tfjs-tflite yet (hence the alpha version). I will take a look at this soon (already in my todo list). Thanks! =====""]",1
https://github.com/tensorflow/tfjs/issues/989,Optimize addN in the WebGL,1,closed,2018-12-12T16:48:40Z,2019-10-08T20:32:25Z,Similarly to how we optimize concat: https://github.com/tensorflow/tfjs-core/pull/1449,"['related PR has been merged , will be closing this issue. =====']",0
https://github.com/tensorflow/tfjs/issues/5182,Incorrect SideEffects settings in TFJS cause bad builds due to tree shaking,1,open,2021-06-06T16:20:20Z,2021-06-08T16:27:51Z,"TFJS changed behavior several times how to deal with tree-shaking and side-effects,  but it's still broken.For example, `tfjs-backend-webgl:package.json` has this:```json  ""sideEffects"": [    ""./dist/register_all_kernels.js"",    ""./dist/flags_webgl.js"",    ""./dist/base.js"",    ""./dist/index.js""  ]```What is the point of `sideEffects` pointing to JS files in `/dist/`? 1. If user is using prebuilt bundles of tfjs from `/dist`, it's irrelevant2. If user is trying to build `@tensorflow/tfjs-backend-webgl`, it will be **broken**3. If user is trying to build `@tensorflow/tfjs`, then this is relevant     since the meta-bundle imports from `@tensorflow/tfjs-backend-webgl/dist/index.js` which references `./dist`At minimum, sideEffects should point to TS files in `./src`, as well as JS `./dist`  Note that `tfjs-backend-webgl:package.json` is used only as an example, same bad behavior is present in all `tfjs` packages.  Proof that it's broken?  Building custom TFJS bundle and build works just fine, but then fails in runtime with.  ```logUncaught Error: Cannot evaluate flag 'CPU_HANDOFF_SIZE_THRESHOLD': no evaluation function found.    at Environment.evaluateFlag (environment.ts:138)```Why? Because entire import `./src/flags_webgl.ts` was dropped due to tree-shaking  and flags never got registered to start with.If I re-run build with bundler set to simply ignore any `sideEffects` fiels within TFJS,  it works just fine, but bundle is massive.On the other hand, even better solution is not to use `sideEffects` at all by moving flat code in those files into a method (e.g. `init()` and calling that method from single place in the parent code where needed. I don't see why should any code be immediately executed during import - that is just a bad practice.  Environment: TFJS 3.7.0 on Ubuntu 21.04",['cc @tafsiri due to previous work on sideEffects====='],1
https://github.com/tensorflow/tfjs/issues/5757,WebGPU implementation of tf.image.fromPixels is inverted on y-axis,5,closed,2021-10-21T14:20:54Z,2021-10-25T01:16:38Z,"environment:- tfjs 3.9.0 using tfjs-backend-webgpu 0.0.1-alpha8- tfjs and tfjs-backend-webgpu fresh build from main branch as of todaybasically, if i use `tf.image.fromPixels`, resulting tensor is inverted on y-axis - thus fully corrupted  if i construct tensor manually, it works fine  (i havent noticed before since i'm mostly using tfjs in a web worker so not relying on tf.image.fromPixels)","[""@shaoboyan Please take a look if it's related with your change in chromium.====="", '@vladmandic  May I know the input of you `tf.image.fromPixels`? Is the input WebGL Canvas? Do you use chrome canary? If you could provide the program that would be appreciatate, I could have a try locally. Because the flip issue may related to recently chromium changes for copyExternalImageToTexture which tfjs.fromPixels relies on..The reason for that change is WebGL uses a coordinate which origin is bottom-left, but WebGPU uses coordinate which origin is top-left. In previous CopyExternalImageToTexture impl, we copy WebGL Canvas from bottom-left to align with WebGL coordinate.But in practical, WebGL developers render things with flipY correction to ensure the canvas generate correct result , so the previous behaviour makes the final copy visual result weired. After discussion, community decides to make copyExternalImageToTexture copy pixels from top-left origin always.But it also be possible that you hit the issue due to the canary is not update to (>= 97.0.4675.0). So if you could provide the sample, I could have a double check.@qjia7  Pls assign this issue to me. Also cc @kainino0x for FYI.=====', '> May I know the input of you tf.image.fromPixels? > Is the input WebGL Canvas? yes, input is `OffscreenCanvas` with `webgl` context  (im using a simple GLSL programs on it to do some basic corrections like brightness/contrast/etc. before passing it to TF)> Do you use chrome canary?  ah, in **chrome 96.0.1053.0** image is inverted  but in **chrome 97.0.4678.0** its all good  i was aware of coordinate system difference, but wasnt aware of the recent change in chrome  thanks for the hint, issue is not really an issue - feel free to close it=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5757"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5757"">No</a>=====', '@vladmandic  Thanks for your check and happy to know the fix in chromium helps!=====']",1
https://github.com/tensorflow/tfjs/issues/5700,TFJS-TFLITE error when model outputs quantized values when calling predict(...),1,closed,2021-10-08T04:51:46Z,2021-10-20T17:48:16Z,"**System information**Linux Ubuntu 18.04.03, Chrome 93TensorFlow.js installed from (npm or script link):Latest from NPM via script tag:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.js""></script>```**Tensorflow.js Converter Version:**Model created via TFLITE converter via python file (quantization aware training)**Describe the current behavior:**When running a model that outputs int8 values via TFJS-TFLITE, such as an image generation model, the stack crashes with the error:`Error: values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray`This occurs at: `TFLiteModel.predict (tflite_model.ts:134)`**Possible source of issue:**It appears the model is outputting an Int8Array that is passed to tensorflow's `tensor(...)` method, however that method appears to only accept `Float32Array, Int32Array, or Uint8Array` as per https://github.com/tensorflow/tfjs/issues/4009#issue-713346237**Describe the expected behavior:**TFJS-TFLITE should ensure that quantized image model's output are casted to UInt8Array, instead of Int8Array... or should expand support in tensorflowJS to accept Int8Array **Standalone code to reproduce the issue**```// Produce quantized 8 bit model that outputs imageslet model = await tflite.loadTFLiteModel('URL TO 8 bit quantized image generation model'),let cameraImage = new tf.tensor4d(      new Int32Array(        ctx.getImageData(0, 0, 256, 256).data),      [1, 256, 256, 4])cameraImage = tf.slice(cameraImage, [0, 0, 0, 0], [1, 256, 256, 3]) // a faster way to slice off the alpha channel instead of loop+iflet result = model.predict(cameraImage) // crash```**Other info / logs**```VM9506:1 Uncaught TypeError: Cannot read properties of undefined (reading 'data')    at eval (eval at TFLiteModel.predict (tflite_model.ts:133), <anonymous>:1:13)    at TFLiteModel.predict (tflite_model.ts:133)```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5700"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5700"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3755,tf.losses.MeanSquaredError doesn't work for multi-dimensional labels and predictions,2,closed,2020-08-08T08:36:33Z,2020-11-20T21:39:51Z,"#### TensorFlow.js version""@tensorflow/tfjs"": ""^2.0.1""""@tensorflow/tfjs-react-native"": ""0.3.0""#### Browser versionNA (iOS 10.0 on iPad 6th generation)#### Describe the problem or feature requestI am trying to use **tf.losses.MeanSquaredError()** provided by tfjs to calculate the loss between 4-dimensional labels and predictions. We wouldn't ideally use 4-dimensional tensors while calculating the loss, but need it for a custom use case. The problem is that for any value of given label and prediction tensors [shape = (20, 32, 14, 14)], the loss is being returned as 1.0.#### Code to reproduce the bug / link to feature requestJavaScript:``` javascript    import * as tf from '@tensorflow/tfjs',      const js_a = Array(125440).fill(200),    const a = tf.tensor4d(js_a, [20, 32, 14, 14]),    const js_b = Array(125440).fill(1),    const b = tf.tensor4d(js_b, [20, 32, 14, 14]),    const mse_loss = tf.losses.meanSquaredError(a, b),    mse_loss.print(),    // 1.0```Python:```python import tensorflow as tf import numpy as np  np_a = np.empty([20, 32, 14, 14]) np_b = np.empty([20, 32, 14, 14]) np_a.fill(200) np_b.fill(1)  tf_a = tf.convert_to_tensor(np_a, np.float32) tf_b = tf.convert_to_tensor(np_b, np.float32)  mse = tf.keras.losses.MeanSquaredError() print(mse(tf_a, tf_b).numpy()) # 39600.758```GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['Even I faced this issue with the same version of tfjs.=====', 'UPDATE: The issue seems to be solved for ""@tensorflow/tfjs"": ""^2.1.0"". Closing it.=====']",0
https://github.com/tensorflow/tfjs/issues/5821,"Unhandled Rejection (Error): Based on the provided shape, [4,4,64,128], the tensor should have 131072 values but has 14636",1,closed,2021-11-07T06:13:00Z,2021-11-07T06:20:50Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.11.0- Browser version: Chrome 94.0.4606.81- Tensorflow.js Converter Version: **Describe the current behavior**```def Generator():    inputs = tf.keras.layers.Input(shape=[256,256,3])    down_stack = [        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)        downsample(128, 4), # (bs, 64, 64, 128)        downsample(256, 4), # (bs, 32, 32, 256)        downsample(512, 4), # (bs, 16, 16, 512)        downsample(512, 4), # (bs, 8, 8, 512)        downsample(512, 4), # (bs, 4, 4, 512)        downsample(512, 4), # (bs, 2, 2, 512)        downsample(512, 4), # (bs, 1, 1, 512)    ]    up_stack = [        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)        upsample(512, 4), # (bs, 16, 16, 1024)        upsample(256, 4), # (bs, 32, 32, 512)        upsample(128, 4), # (bs, 64, 64, 256)        upsample(64, 4), # (bs, 128, 128, 128)    ]#    initializer = tf.random_normal_initializer(0., 0.02)    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,                                         strides=2,                                         padding='same',                                         activation='tanh') # (bs, 256, 256, 3)    x = inputs    # Downsampling through the model    skips = []    for down in down_stack:        x = down(x)        skips.append(x)    skips = reversed(skips[:-1])    # Upsampling and establishing the skip connections    for up, skip in zip(up_stack, skips):        x = up(x)        x = tf.keras.layers.Concatenate()([x, skip])    x = last(x)    return tf.keras.Model(inputs=inputs, outputs=x)``````tfjs.converters.save_keras_model(generator, tfjs_target_dir)```exported the model in python, trying to load it in tensorflow js, but got an error: Unhandled Rejection (Error): Based on the provided shape, [4,4,64,128], the tensor should have 131072 values but has 14636**Describe the expected behavior**Should be able to load the model.**Standalone code to reproduce the issue**https://colab.research.google.com/drive/1ivkK-P9VJyRmK_X4UBEAnKc6iRbYjnRR?usp=sharingin the javascript I'm simply loading the model like this:```import * as tf from ""@tensorflow/tfjs"",const MODEL_URL = ""/colorization-model.json"",const loadModel = async () => {  const model = await tf.loadLayersModel(MODEL_URL),},loadModel(),``` **Other info / logs** Include any logs or source code that would be helpful toNo more info","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5821"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5821"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4796,Threaded SIMD tests for the wasm backend crash the browser,3,closed,2021-03-09T19:02:07Z,2021-04-22T19:40:08Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): 3.2- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian rodete- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: Chrome ~88- Tensorflow.js Converter Version:**Describe the current behavior**Threaded SIMD tests for the wasm backend create a new webworker for each test, crashing the page with hundreds of webworkers.**Describe the expected behavior**Each test does not have its own webworker, or webworkers are cleaned up after the test is finished.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Bundling all tests (with e.g. esbuild) does not fix this. While test files are no longer individually requested from Karma, each test still re-initializes the wasm backend (requesting the `.wasm` file each time) and creates a new webworker.=====', 'Hey @mattsoulanille, Actually for test and fixed-width SIMD maybe there is some issues with Web Assembly , Can you help me out in this![image](https://user-images.githubusercontent.com/41143496/110775224-ade9b280-8284-11eb-8ec7-f0e95abb9f78.png)=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4796"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4796"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/850,tfjs-vis: allow adjustment of font size on charts,1,closed,2018-10-26T19:01:15Z,2018-11-06T20:26:50Z,Whether this is possible through CSS requires investigation. Please also investigate whether it's possible on both the `render` API and the `fitCallbacks` API.,['https://github.com/tensorflow/tfjs-vis/pull/23 addresses this for render functions.====='],0
https://github.com/tensorflow/tfjs/issues/5869,"tf.gather is needlessly slow, CPU-intensive",2,open,2021-11-18T23:24:23Z,2021-11-19T16:16:07Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 Home- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): [npm](https://unpkg.com/@tensorflow/tfjs@3.11.0/dist/tf.js)- TensorFlow.js version (use command below): 3.11.0- Browser version: Google Chrome 96.0.4664.45- Tensorflow.js Converter Version: ???**Describe the current behavior**`tf.gather` seems disproportionately slow, taking  33x the wall time of `tf.cumsum` with comparably sized input and output. When viewing performance data, runtime seems dominated by uploading and downloading textures, with much of the CPU time spent looping over indices to merely check that they are within range.It seems also that `tf.gatherND` does not suffer similar performance problems. So replacing ` tf.gather(x,ix)` with `tf.gatherND(x,ix.expandDims(1))` seems an appropriate workaround when gathering on the first axis.**Describe the expected behavior**I expect `tf.gather` to perform gathering on the GPU without synchronous operations. Any index checks (if necessary) should be done on GPU.**Standalone code to reproduce the issue**```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">    <title>Document</title></head><body>    <script crossorigin=""anonymous"" src=https://unpkg.com/@tensorflow/tfjs@3.11.0/dist/tf.js></script>    <script>        window.addEventListener(""load"",()=>{            const x = tf.range(0, 10)            const ix = tf.randomUniform([50000],0,10).cast('int32')            for (let i=0, i<1000, ++i){                tf.tidy(()=>{ tf.gather(x,ix),                tf.cumsum(ix), })            }        })    </script></body></html>```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.See screenshot and attached Chrome devtools profile.![image](https://user-images.githubusercontent.com/119948/142503841-13f871df-ed34-46eb-9d8a-b293589b896a.png)[Profile-20211118T172039.zip](https://github.com/tensorflow/tfjs/files/7566680/Profile-20211118T172039.zip)","['The slowness is caused by reading data back from gpu to cpu for out of bounds checking. We ever talked about it to skip this checking in production mode or do clamping in shader. @pyu10055=====', '@qjia7 thanks. that makes sense. I don’t understand your last sentence though.It would be nice to bring performance in line with `gatherND` by either eliminating bounds checking or doing it in the GPU kernel.=====']",1
https://github.com/tensorflow/tfjs/issues/4803,[tfjs-react-native] Build and setup issue with camera,3,closed,2021-03-10T21:11:39Z,2021-03-11T17:40:18Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04`- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `Pixel_3a_API_30_x86.avd`- TensorFlow.js installed from (npm or script link):- TensorFlow.js version: `npm install @tensorflow/tfjs`, - CUDA/cuDNN version:**Describe the problem**I am following the steps to setup tfjs react native, ive gone from step 1 - step 4 however i am blocked with an error message after i run when i use either `npm run web` or `npm run android`. The problem both relate to the camera, i have installed the prequisite `expo install expo-camera`.**Provide the exact sequence of commands / steps that you executed before running into the problem**1. `npm run web````Failed to compile./home/kay/kay/powerlevelai/mobileapp/node_modules/@tensorflow/tfjs-react-native/dist/camera/camera_stream.js 281:12Module parse failed: Unexpected token (281:12)You may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders|             const cameraComp = (|             //@ts-ignore see https://github.com/microsoft/TypeScript/issues/30650>             <CameraComponent key='camera-with-tensor-camera-view' {...(cameraProps)} ref={(ref) => (this.camera = ref)}/>),|             // Create the glView if the camera has mounted.|             let glViewComponent = null,```alternatively i have setup an android emulator using AVD.1. `npm run android````Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.Use '--warning-mode all' to show the individual deprecation warnings.See https://docs.gradle.org/6.3/userguide/command_line_interface.html#sec:command_line_warningsFAILURE: Build failed with an exception.* What went wrong:Could not determine the dependencies of task ':app:compileDebugJavaWithJavac'.> Could not resolve all task dependencies for configuration ':app:debugCompileClasspath'.   > Could not find com.google.android:cameraview:1.0.0.     Required by:         project :app > project :expo-camera```**Any other info / logs****package.json**```{  ""main"": ""index.js"",  ""scripts"": {    ""android"": ""react-native run-android"",    ""ios"": ""react-native run-ios"",    ""web"": ""expo start --web"",    ""start"": ""react-native start"",    ""test"": ""jest""  },  ""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.14.1"",    ""@react-native-community/async-storage"": ""^1.12.1"",    ""@tensorflow/tfjs"": ""^3.3.0"",    ""@tensorflow/tfjs-react-native"": ""^0.5.0"",    ""expo"": ""~40.0.0"",    ""expo-camera"": ""~9.1.0"",    ""expo-gl"": ""~9.2.0"",    ""expo-gl-cpp"": ""~9.2.0"",    ""expo-splash-screen"": ""~0.8.0"",    ""expo-updates"": ""~0.4.0"",    ""react"": ""16.13.1"",    ""react-dom"": ""16.13.1"",    ""react-native"": ""~0.63.4"",    ""react-native-fs"": ""^2.16.6"",    ""react-native-gesture-handler"": ""~1.8.0"",    ""react-native-reanimated"": ""~1.13.0"",    ""react-native-screens"": ""~2.15.0"",    ""react-native-unimodules"": ""~0.12.0"",    ""react-native-web"": ""~0.13.12""  },  ""devDependencies"": {    ""@babel/core"": ""~7.9.0"",    ""@types/react"": ""~16.9.35"",    ""@types/react-dom"": ""~16.9.8"",    ""@types/react-native"": ""~0.63.2"",    ""babel-preset-expo"": ""~8.3.0"",    ""jest-expo"": ""~40.0.0"",    ""typescript"": ""~4.0.0""  },  ""jest"": {    ""preset"": ""react-native""  },  ""private"": true}```**App.tsx**```import * as tf from '@tensorflow/tfjs',import '@tensorflow/tfjs-react-native',import React, { Component } from 'react',export default class App extends React.Component {  constructor(props: any) {    super(props),    this.state = {      isTfReady: false,    },  }  async componentDidMount() {    // Wait for tf to be ready.    await tf.ready(),    // Signal to the app that tensorflow.js can now be used.    this.setState({      isTfReady: true,    }),  }  render() {    return null,  }}```","['Hi Kay,I haven\'t set up Android emulator on my machine, but for the web, I actually encountered the same issue. It is fixed by the steps below:- Run: `expo customize:web`  - Use the space key to select the `webpack.config.js` entry, then press ""enter""  - This will create a bare-minimum `webpack.config.js` file.- Edit the `webpack.config.js` file to make webpack transpile all `@tensorflow` packages. Here is what my file looks like```const createExpoWebpackConfigAsync = require(\'@expo/webpack-config\'),module.exports = async function(env, argv) {  const config = await createExpoWebpackConfigAsync(      {        ...env,        babel: {          dangerouslyAddModulePathsToTranspile: [            // Ensure that all packages starting with @tensorflow are            // transpiled.            \'@tensorflow\',          ],        },      },      argv),  return config,},``` - Now `npm run web` should workPlease check if this helps with the Android build as well. Thanks!=====', ""@jinjingforever Hey,I followed your steps and got `npm run web` working. Is this some extra setup step that needs to now be added to the readme?The `npm run android` still does not work for another reason which i'll make a new issue for when i revist it.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4803"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4803"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/955,body-pix demo does not work on phones,14,closed,2018-11-29T22:36:06Z,2021-09-03T19:55:06Z,The predictions seem to be working but it looks like the overlay is not properly placed on the image.cc @oveddan,"['+1 on this bug I tried both Chrome on Pixel and Safari on IOS on portrait mode, none of them works due to the overlay placement issue. However everything seem to work fine on landscape mode. =====', '@huan =====', ""Also this demo doesn't work on Chrome for IPhone. Error message: his browser does not support video capture,or this device does not have a camera@airlrj====="", 'I have a Pixel 2 XL and it seems that run this demo in chrome browser will have trouble when the phone in verticle:![image](https://user-images.githubusercontent.com/1361891/50437889-a3060a00-0926-11e9-981b-ae99353b8ca7.png)But it works perfectly when the phone in horizontal:![image](https://user-images.githubusercontent.com/1361891/50437897-ab5e4500-0926-11e9-8c0e-0dab1223a134.png)=====', ""Thanks for reporting.  I'll take a look.  As far as chrome on ios - we haven't supported this on the demos because as far as I understand it is not possible to access the camera in chrome on ios.====="", ""@huan @wangtz I've made a quick fix for safari ios, as well as all mobile. * The text at the bottom is responsive and doesn't appear on mobile.  * The rendered video has the same size as the captured video.  This allows for full screen on mobile and gets rid of the bug you reported.* The gui appears on mobile - this allows different options to be changed.  In the future would be ideal to have a mobile menu made up of icons at the bottom.Can you please test?  Particularly on Android?  I tried on the IPhone XR and it works fine now.In the future I want to add the ability to switch to the rear camera.====="", '@huan Huan, do you want to give it a try?=====', 'I still have the same issue on my pixel 2 XL woth chrome in vertical mode.=====', '@huan @wangtz I forgot to give you the link to the demo with the fix, try going here:https://body-pix-mobile-fix.netlify.com/=====', 'With your new link, the vertical is ok but the horizontal comes with issue that the canvas position is not right.=====', '@huan the same issue that happened before for vertical?  Can you include a screenshot?=====', '![image](https://user-images.githubusercontent.com/1361891/50544083-5cb1f300-0c25-11e9-8d72-0f43d88d001d.png)![image](https://user-images.githubusercontent.com/1361891/50544085-63406a80-0c25-11e9-9d11-5176799311c6.png)=====', ""I've been working on an improved responsive version of the demo.  It may have fixed the issues. Could you check the latest at:https://body-pix-mobile-fix.netlify.com/It also lets you switch the cameras now.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/955"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/955"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/1151,Guide: Model Conversion,1,closed,2019-01-28T21:37:25Z,2019-07-09T19:57:04Z,- Models,['Link to doc https://github.com/tensorflow/tfjs-website/blob/devsite/docs/guides/conversion.md====='],0
https://github.com/tensorflow/tfjs/issues/5313,Outdated Documentation,1,closed,2021-07-12T02:58:55Z,2021-07-14T17:56:19Z,"The documentation appears to be outdated.The main readme states:> ⚠️ We recently released TensorFlow.js 2.0. If you have been using TensorFlow.js via a script tag without specifying a version and see an error saying no backends are found, then you should read our release notes for instructions on how to upgrade.npmjs shows that v2.0.0 was one year ago. The current ""latest"" version is 3.7.0, which was published last month.- https://www.npmjs.com/package/@tensorflow/tfjs/v/3.7.0- https://www.npmjs.com/package/@tensorflow/tfjs/v/2.0.0The concern is that developers are likely using the README for v2 code while using v3.Additionally, [WINDOWS_TROUBLESHOOTING](https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md) is outdated.> Currently, node-gyp requires Python 2.x to work properly. If Python 3.x is installed, you will see build failures. Also double check your python version python --version and update the Windows $PATH as needed.That statement is inaccurate. See https://github.com/nodejs/node-gyp/issues/1977#issuecomment-567131298> You do not need Python 2.7 in order to run node-gyp. **_Python 3 has been supported since version 5.0.5_**. If you have verified that your issue is python related, as in that node-gyp is trying to run a script which contains errors according to the python 3 interpreter, then try the following:...","['@djbreen7 thanks for checking this , I have submitted 2 separate PRs to remove above lines =====']",1
https://github.com/tensorflow/tfjs/issues/5900,tf.min on bool type tensor under wasm backend return wrong result.,1,closed,2021-11-26T06:58:41Z,2021-12-07T17:41:28Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): custom code- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.4- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ""@tensorflow/tfjs"": ""^3.11.0"",  ""@tensorflow/tfjs-backend-wasm"": ""^3.11.0"",- Browser version:""electron"": ""^13.0.0"",**Describe the current behavior**With wasm backend, the tf.min function on bool type tensor return wrong result.**Describe the expected behavior**The tf.min on bool type tensor  return false or 0.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.```jsimport * as tf from '@tensorflow/tfjs-core',import {setWasmPaths} from '@tensorflow/tfjs-backend-wasm',import wasmSimdPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-simd.wasm',import wasmSimdThreadedPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-threaded-simd.wasm',import wasmPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm.wasm',setWasmPaths({    'tfjs-backend-wasm.wasm': wasmPath,    'tfjs-backend-wasm-simd.wasm': wasmSimdPath,    'tfjs-backend-wasm-threaded-simd.wasm': wasmSimdThreadedPath }),tf.setBackend('wasm').then(()=>console.log(tf.min(tf.tensor([true, false], [2], 'bool')).arraySync()))```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.The above code will print a non-zero number in console with a warn:![截屏2021-11-26 下午2 53 49](https://user-images.githubusercontent.com/30199027/143539324-71295895-4129-46d2-8823-10ba0d16bdeb.png)In the CPU backend, `console.log(tf.min(tf.tensor([true, false], [2], 'bool')).arraySync())` will print 0.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5900"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5900"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5584,Breaking Change v3.8.0 => v3.9.0,10,closed,2021-09-05T05:19:28Z,2021-12-21T08:00:31Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0**Describe the current behavior**Upgraded from 3.8.0 to 3.9.0, and typescript throws an error:> Error: node_modules/@tensorflow/tfjs-core/dist/hash_util.d.ts:2:49 - error TS2304: Cannot find name 'Long'.![image](https://user-images.githubusercontent.com/5757359/132116104-1a06c0df-e90b-48d9-9cc2-db62244a6b10.png)Using `typescript` 4.3.5.Perhaps missing `import Long from 'long',` in ![image](https://user-images.githubusercontent.com/5757359/132116245-cf1c1688-fbf8-48ab-91d8-84c0be4702c6.png)**Describe the expected behavior**No breaking change","['Same issue=====', 'Looks like `hash_util.d.ts` somehow lost its reference types triple slash. Here it is at 3.8.0:```/// <reference types=""long"" />export declare function hexToLong(hex: string): Long,export declare function fingerPrint64(s: Uint8Array, len?: number): Long,```It [should have been automatically added by tsc](https://www.typescriptlang.org/docs/handbook/triple-slash-directives.html#-reference-types-), and I\'m not yet sure why `tsc` isn\'t including it in our Bazel compile.=====', ""I've tried to reproduce this locally by compiling the following:```import {hexToLong} from '@tensorflow/tfjs-core/dist/hash_util',console.log(hexToLong('0xABCDEF')),```tsc successfully compiled the above. Could you please send me a reproduction of this issue where tsc fails? Thanks!====="", ""I am going to remove the release blocker tag from this issue since I have been unable to reproduce it. [Here's my attempt](https://github.com/mattsoulanille/tfjs-issue-5584-test).====="", 'I was not able to reproduce the same error, will be closing this request , please @mention to reopen. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5584"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5584"">No</a>=====', 'I am also having the same issue on a fresh Angular 12 install...Version is 3.11.0When I manually add `/// <reference types=""long"" />`to  the `hash_util.d.ts` it works as expected.Before```/// <amd-module name=""@tensorflow/tfjs-core/dist/hash_util"" />export declare function hexToLong(hex: string): Long,export declare function fingerPrint64(s: Uint8Array, len?: number): Long,```After```/// <amd-module name=""@tensorflow/tfjs-core/dist/hash_util"" />/// <reference types=""long"" />export declare function hexToLong(hex: string): Long,export declare function fingerPrint64(s: Uint8Array, len?: number): Long,```=====', 'This is still an issue for me.I have tried:1. `npm I -D @types/long`2. add `""types"": [""long""]` to `compilerOptions`But that did not help.=====', 'v3.12.0Added `@types/long` to my `compilerOptions.types` worked.=====', 'Step 1. npm I -D @types/longStep 2.  go to tsconfig.app.jsonStep 3.  add   ""types"": [""@types/long""]----->tsconfig.app.json this file will looks like{  ""extends"": ""./tsconfig.json"",  ""compilerOptions"": {    ""outDir"": ""./out-tsc/app"",   ###  _**""types"": [""@types/long""]**_  },  ""files"": [    ""src/main.ts"",    ""src/polyfills.ts""  ],  ""include"": [    ""src/**/*.d.ts""  ]}-------> it worked for me, hope will work for you as well. =====']",1
https://github.com/tensorflow/tfjs/issues/4405,Uncaught TypeError: webglBackend.incRef is not a function for tfjs-modes/face-landmarks-detection,7,closed,2020-12-15T04:57:26Z,2020-12-15T18:42:28Z,"**System information**package.json dependencies:```  ""dependencies"": {    ""@tensorflow-models/face-landmarks-detection"": ""^0.0.2"",    ""@tensorflow/tfjs"": ""^2.8.0"",    ""@tensorflow/tfjs-backend-webgl"": ""2.4.0"",    ""@tensorflow/tfjs-converter"": ""2.4.0"",    ""@tensorflow/tfjs-core"": ""2.4.0"",    ""p5"": ""^1.1.9"",    ""stats.js"": ""^0.17.0""  },```**Describe the current behavior**When importing another .js file that imports '@tensorflow/tfjs' it throws the error:```Uncaught TypeError: webglBackend.incRef is not a function    at reshape (Reshape.ts:50)    at Object.oneHot [as kernelFunc] (OneHot.ts:36)    at kernelFunc (engine.ts:576)    at engine.ts:634    at Engine.scopedRun (engine.ts:439)    at Engine.runKernelFunc (engine.ts:631)    at Engine.runKernel (engine.ts:508)    at oneHot_ (one_hot.ts:58)    at Object.oneHot__op (operation.ts:51)    at Object.parcelRequire.wailaModel.js.@tensorflow/tfjs (wailaModel.js:14)```These warnings are also displayed before the error is thrown:![image](https://user-images.githubusercontent.com/32425272/102172729-58f5f600-3e67-11eb-8db7-138ccf03e13b.png)**Standalone code to reproduce the issue**Here are the imports to the modified demo file from tfjs-models/face-landmarks-detection/demo/index.js:```import {load, SupportedPackages} from '@tensorflow-models/face-landmarks-detection',import '@tensorflow/tfjs-backend-webgl'import * as tf from '@tensorflow/tfjs-core',import someObject from './someFileThatImportsTfjs'```And here are the imports in someFileThatImportsTfjs:`import * as tf from '@tensorflow/tfjs'`","['Similar issue has been reported here https://github.com/tensorflow/tfjs/issues/4309 , will leave it to @annxingyuan for further comments, thank you =====', ""@rthadur Thanks for the response. Just to give some more information:The demo worked out of the box for me, but the problem occurs when I add `import someObject from './someFileThatImportsTfjs'` in demo/index.js====="", 'One potential issue is that you are importing the backends twice with two different versions.This package:```""@tensorflow/tfjs"": ""^2.8.0"",```Includes copies of these packages (but at version 2.8.0):```""@tensorflow/tfjs-backend-webgl"": ""2.4.0"",""@tensorflow/tfjs-converter"": ""2.4.0"",""@tensorflow/tfjs-core"": ""2.4.0"",```Since you are already using ""@tensorflow/tfjs"": ""^2.8.0"", you can remove the others from your package json and just `import @tensorflow/tfjs` in your project.=====', ""@tafsiri Okay I see. I removed those redundant packages and the previous error does not get thrown. However this gets thrown:```Uncaught (in promise) Error: method must be bilinear or nearest, but was undefinedassert                                                            \tutil_base.ts:108cropAndResize_                                                    \tcrop_and_resize.ts:85cropAndResize__op                                                 \toperation.ts:51y.startPoint                                                      \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tengine.ts:442scopedRun                                                         \tengine.ts:453tidy                                                              \tengine.ts:440tidy                                                              \tglobals.ts:192(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17s                                                                 \tface-la…-detection.esm.js:17Async call from Promise.thensetUp                                                             \tindex.js:61Async call from async functionsetUp                                                             \tindex.js:47main                                                              \tindex.js:76parcelRequire.index.js.@tensorflow-models/face-landmarks-detection\tindex.js:170newRequire                                                        \teye_tra…_main.e31bb0bc.js:47(anonymous function)                                              \teye_tra…_main.e31bb0bc.js:81(anonymous function)                                              \teye_tra…_main.e31bb0bc.js:120```My imports in demo/index.js are now:```import {load, SupportedPackages} from '@tensorflow-models/face-landmarks-detection'import * as tf from '@tensorflow/tfjs',import someObject from './someFileThatImportsTfjs'```I am trying to follow the stack trace, but it is a little difficult with face-landmarks-detection.esm.js. I'm not too familiar with Parcel, but it seems like I do not have sourcemapping set up properly?====="", 'Hi @scottzockoll - this is indeed a separate issue, which we are working on a fix for: https://github.com/tensorflow/tfjs/pull/4407=====', '@annxingyuan Great! Thanks for the information. I will close this since it is a duplicate.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4405"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4405"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5529,[tfjs-backend-webgpu] Fail to build webgpu backend after upgrading @webgpu/types to version 0.1.6,3,closed,2021-08-25T08:59:49Z,2021-10-01T00:33:44Z,"After upgrading `@webgpu/types to 0.1.6` in `package.json`, I got build error messages below:```src/index.ts → dist/tf-backend-webgpu.node.js...[!] (plugin typescript) Error: @rollup/plugin-typescript TS2322: Type 'CanvasRenderingContext2D | WebGLRenderingContext' is not assignable to type 'GPUCanvasContext'.  Type 'CanvasRenderingContext2D' is missing the following properties from type 'GPUCanvasContext': __brand, configure, unconfigure, getPreferredFormat, getCurrentTexturesrc\backend_webgpu.ts (135:7) 'GPUCanvasContext': __brand, configure, unconfigure, getPreferredFormat, getCurrentTexture135       this.dummyContext = this.dummyCanvas.getContext('gpupresent'),          ~~~~~~~~~~~~~~~~~Error: @rollup/plugin-typescript TS2322: Type 'CanvasRenderingContext2D | WebGLRenderingContext' is not assignable to type 'GPUCanvasContext'.  Type 'CanvasRenderingContext2D' is missing the following properties from type 'GPUCanvasContext': __brand, configure, unconfigure, getPreferredFormat, getCurrentTexture    at error (C:\code\tfjs\tfjs-backend-webgpu\node_modules\rollup\dist\shared\rollup.js:10149:30)```Notice that build works fine if we use `@webpu/types` version 0.1.4.The error is report from `@rollup/plugin-typescript` and a simple fix by upgrading `@rollup/plugin-typescript` to latest version still cannot fix it.@kainino0x Do you have any suggestions?","[""I think it may be because `@webgpu/types@0.1.6` is not for the version of `webgpu` that `tfjs-backend-webgpu` uses. I'm not sure though.====="", 'I am pretty sure this was resolved as I remember discussing the workaround.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5529"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5529"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1360,Tensorflow converter not working with tensorflow r1.13,4,closed,2019-03-12T12:22:53Z,2019-03-13T12:04:52Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser version#### Describe the problem or feature requestTensorflow converter does not work with Tensorflow 1.13TypeError: Expected config_proto to be a ConfigProto, saw type <class 'tensorflow.core.protobuf.rewriter_config_pb2.RewriterConfig'>Installing Tensorflowjs automatically removes Tensorflow 1.13 and installs 1.12 instead, whichcan break a code for people developing for the latest stable version.### Code to reproduce the bug / link to feature request","['@pyu10055 This sounds like a version mismatch.  Is this something we expect to work?=====', '@batrlatom Sorry for the inconvenience. Please create a clean virtualenv or pipenv and install the latest release of tensorfowjs in it. It should work with tensorflow 1.12. We are working on pushing out a new version of tensorflowjs, which will depend on 2.0.0-alpha0.=====', '@batrlatom can you specify the version of your converter? As @caisq mentioned, for 1.0.x releases going forward, we will be depending on TF 2.0 nightly pip, while for 0.15.x, it will be on TF 1.12.If you are on tfjs 0.15.x and require some features from TF 1.13, we can try to fix this issue, otherwise you should wait for the tfjs 1.0.1 release.=====', 'Thanks for the responses.Using `tensorflowjs_converter --version `gave me> tensorflowjs 0.8.0> Dependency versions:>   keras 2.2.2>   tensorflow 1.13.1But when I install tensorflow 1.12, it works. So I solved it by using a different virtual environment and converting it there. I think it is not needed to split resources to solve the problem now. Migratingto tf r2.0 will be better. =====']",0
https://github.com/tensorflow/tfjs/issues/4316,Unknown op TensorListReserve,2,closed,2020-11-28T02:56:07Z,2020-11-30T21:23:09Z,"Hello,I created a LSTM model, converted it using the CLI and serving the same using node http-server.Below is the code that attempts to run prediction from within html code and below error was shown on the console. Can you pls provide guidance?PS: Earlier, I tried model.predict and it recommended using executeAsync.<img width=""1074"" alt=""Screen Shot 2020-11-27 at 8 53 52 PM"" src=""https://user-images.githubusercontent.com/10361685/100492532-adbd1100-30f2-11eb-80c1-ab6f0f9fe670.png""><img width=""989"" alt=""Screen Shot 2020-11-27 at 8 49 40 PM"" src=""https://user-images.githubusercontent.com/10361685/100492535-b7467900-30f2-11eb-9503-03cc7ff3fbd7.png"">","['@sahas- Looks like you are using an old version of TFJS (tfjs@2.0.0), TensorListReserve op should have been supported, can you upgrade to the latest version of the TFJS (2.7.0)? thanks=====', 'It works with 2.7.0. Thanks.=====']",1
https://github.com/tensorflow/tfjs/issues/4716,"Error: ""Cannot read property '0' of undefined"" for texShape",9,closed,2021-02-18T20:32:40Z,2021-07-21T19:49:13Z,"Somehow `inputInfo.shapeInfo.texShape` is undefined when running inference on `COCO SSD` model on a specific image (and works for 99% of other images)  Actual backtrace is:```log  TypeError: Cannot read property '0' of undefined    at LCe (shader_compiler.js:555)    at G4 (shader_compiler.js:91)    at dCe (shader_compiler.js:103)    at shader_compiler.js:36    at Array.map (<anonymous>)    at fCe (shader_compiler.js:36)    at KCe (gpgpu_math.js:43)    at backend_webgl.js:692    at $N.getAndSaveBinary (backend_webgl.js:720)    at $N.runWebGLProgram (backend_webgl.js:691)```Matching code is in `shader_compiler.js:getPackedSampler1D()`:```jsfunction getPackedSampler1D(inputInfo) {    const texName = inputInfo.name,    const funcName = 'get' + texName.charAt(0).toUpperCase() + texName.slice(1),    const texShape = inputInfo.shapeInfo.texShape,    const packedTexShape = [Math.ceil(texShape[0] / 2), Math.ceil(texShape[1] / 2)],    const glsl = getGlslDifferences(),    return `    vec4 ${funcName}(int index) {      vec2 uv = packedUVfrom1D(        ${packedTexShape[0]}, ${packedTexShape[1]}, index),      return ${glsl.texture2D}(${texName}, uv),    }  `,}```Somehow `inputInfo.shapeInfo` is defined, but does not have a valid `texShape` property.  Unfortunately, entire code is too long to post for quick reproduction, so if additional info is needed please suggest diagnostic steps.  So far I've found several images (out of thousands) where this occurs 100%.Environment: `TFJS` 3.1.0 with `WebGL` backend and `Chrome` 88 browser","[""update with backtrace using non-minified code:```logprocessImage.js:299 TypeError: Cannot read property '0' of undefined    at getPackedSampler1D2 (shader_compiler.js:555)    at getPackedSamplerFromInInfo2 (shader_compiler.js:91)    at getInputSamplingSnippet2 (shader_compiler.js:103)    at shader_compiler.js:36    at Array.map (<anonymous>)    at makeShader2 (shader_compiler.js:36)    at compileProgram2 (gpgpu_math.js:43)    at backend_webgl.js:692    at MathBackendWebGL2.getAndSaveBinary (backend_webgl.js:720)    at MathBackendWebGL2.runWebGLProgram (backend_webgl.js:691)```====="", 'Hi @vladmandic Can you share the image with us? Can we reproduce it with tfjs coco ssd model?=====', ""@lina128 seems that root cause is another model (custom one) and the fact that coco ssd model runs concurrently is pretty much a timing thing. the custom model definitely experiences this problem, like i said about 1 image in every 500 so far.if you're willing to dig deeper, i can create a test project and upload that custom model with it (it's about 72mb quantized to f16).====="", 'i\'ve created a self-contained minimal repository containing reproduction code, test images (one ok and one producing error) and model itself: <https://github.com/vladmandic/tfjs-error-test>when run with `npm start` it produces following output in the browser:```logInit TFJS: 3.2.0 Backend: webglLoading model...Model loaded: ./model/model.jsonEngine state: 146462740 bytesImage loaded: ""imgok"" 300 x 300OKResult[0] shape: 1,300,4Result[1] shape: 1,300Image loaded: ""imgerr"" 300 x 600ErrorTypeError: Cannot read property \'0\' of undefined    at getPackedSampler1D (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72172:51)    at getPackedSamplerFromInInfo (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71708:24)    at getInputSamplingSnippet (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71720:20)    at https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:23    at Array.map ()    at makeShader (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:14)    at compileProgram (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72888:24)    at https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74315:24    at MathBackendWebGL.getAndSaveBinary (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74352:41)    at MathBackendWebGL.runWebGLProgram (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74314:33)```=====', ""@rthadur @lina128is there any chance of this being looked at?it's open since Feb with full reproduction available?====="", 'Getting below error after passing one result,I guess this is related the to image `imagerr`````Init TFJS: 3.2.0 Backend: webglLoading model...Model loaded: ./model/model.jsonEngine state: 146462740 bytesImage loaded: ""imgok"" 300 x 300OKResult[0] shape: 1,300,4Result[1] shape: 1,300Image loaded: ""imgerr"" 300 x 600ErrorgetPackedSampler1D@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72172:33getPackedSamplerFromInInfo@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71708:24getInputSamplingSnippet@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71720:20makeShader/inputSamplingSnippet<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:46makeShader@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:14compileProgram@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72888:34runWebGLProgram/binary<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74315:24getAndSaveBinary@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74352:41runWebGLProgram@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74314:33l$@https://localhost:8001/dist/index.js:3465:186kernelFunc@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2920:34runKernelFunc/<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2981:31scopedRun@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2794:29runKernelFunc@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2977:18runKernel@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2850:25Dk@https://localhost:8001/dist/index.js:24:7887r@https://localhost:8001/dist/index.js:12:26001u_@https://localhost:8001/dist/index.js:35:20238Aw/r````=====', '@rthadur exactly - some sample image work fine, some cause this error - and it\'s really up to images, i\'d say about ~5% of my test set causes this problem and i cannot figure out why. `imgok` is an example of ""good"" input image and `imgerr` is an example of input image that causes problems.=====', ""I'll look into this next week. Stay tuned. Thank you for reporting the bug and the reproducible code.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4716"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4716"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2390,tfjs-node can not install,4,closed,2019-11-14T09:00:18Z,2019-11-15T06:04:38Z,"While I was trying to install `tfjs-node` on GCP, I got an error:```* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download node-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v4/1.3.2/CPU-linux-1.3.2.tar.gz ```This seems to be a problem with the server that hosts the TensorFlow-binary. I have also tried it with the GPU version and my laptop. All emit similar errors. **Update:** I can install the `tfjs-node` with an older version like `1.3.1` for mac and `1.3.0` for Linux.","['hi @WenheLI this is a warning when pre-build node addon is not available and it should build the node addon from source (you can tell from the command `node-pre-gyp install --fallback-to-build`). Did you see any log marked with error? Does the package have any issue when import and use it?=====', ""@kangyizhang - Thanks for your response! I think the `napi-v4` module/binary just can not be accessed via the link: `https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v4/1.3.2/CPU-linux-1.3.2.tar.gz `. And it triggers the warning. Also, if I require the package in node, I got the following errors:```js> require('@tensorflow/tfjs-node')Thrown:Error: Cannot find module '/Users/eric/Desktop/dev/training-node/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node'Require stack:- /Users/eric/Desktop/dev/training-node/node_modules/@tensorflow/tfjs-node/dist/index.js- <repl>    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:625:15)    at Function.Module._load (internal/modules/cjs/loader.js:527:27)    at Module.require (internal/modules/cjs/loader.js:683:19)    at require (internal/modules/cjs/helpers.js:16:16) {  code: 'MODULE_NOT_FOUND',  requireStack: [    '/Users/eric/Desktop/dev/training-node/node_modules/@tensorflow/tfjs-node/dist/index.js',    '<repl>'  ]}```It suggests the napi-v4 is not there.====="", '@WenheLI I have just uploaded prebuild addon module for napi-v4, please try again.=====', 'Thanks! It has been fixed!=====']",0
https://github.com/tensorflow/tfjs/issues/4978,Installation issue @tensorflow/tfjs-node-gpu with npm on Win 10,2,closed,2021-04-23T21:10:42Z,2021-04-26T21:17:22Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.5.0- CUDA/cuDNN version: 11.2**Describe the problem**I can't install tfjs-node-gpu through npm.npm ERR! code 1npm ERR! path c:\IA\node_modules\@tensorflow\tfjs-node-gpunpm ERR! command failednpm ERR! command C:\Windows\system32\cmd.exe /d /s /c node scripts/install.js gpu downloadnpm ERR! GPU-windows-3.5.0.zipnpm ERR! * Downloading libtensorflownpm ERR!npm ERR! * Building TensorFlow Node.js bindingsnpm ERR! symlink ./lib/napi-v8 failed:  Error: Command failed: node scripts/deps-stage.js symlink ./lib/napi-v8npm ERR!   * Symlink of lib\napi-v8\tensorflow.dll failed, creating a copy on disk.npm ERR! node:internal/process/promises:245npm ERR!           triggerUncaughtException(err, true /* fromPromise */),npm ERR!           ^npm ERR!npm ERR! [Error: ENOENT: no such file or directory, copyfile 'c:\IA\node_modules\@tensorflow\tfjs-node-gpu\deps\lib\tensorflow.dll' -> 'c:\IA\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8\tensorflow.dll'] {npm ERR!   errno: -4058,npm ERR!   code: 'ENOENT',npm ERR!   syscall: 'copyfile',npm ERR!   path: 'c:\\IA\\node_modules\\@tensorflow\\tfjs-node-gpu\\deps\\lib\\tensorflow.dll',npm ERR!   dest: 'c:\\IA\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8\\tensorflow.dll'npm ERR! }npm ERR!npm ERR!     at ChildProcess.exithandler (node:child_process:326:12)npm ERR!     at ChildProcess.emit (node:events:369:20)npm ERR!     at maybeClose (node:internal/child_process:1067:16)npm ERR!     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5) {npm ERR!   killed: false,npm ERR!   code: 1,npm ERR!   signal: null,npm ERR!   cmd: 'node scripts/deps-stage.js symlink ./lib/napi-v8'npm ERR! }**Provide the exact sequence of commands / steps that you executed before running into the problem**`npm install @tensorflow/tfjs-node-gpu`I've got the same issue with ""npm install @tensorflow/tfjs-node""**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.[2021-04-23T21_07_53_494Z-debug.log](https://github.com/tensorflow/tfjs/files/6368115/2021-04-23T21_07_53_494Z-debug.log)","['I tested older tfjs-node-gpu versions and the installation is working until 3.1.0The issue is present since 3.2.0=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4978"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4978"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1421,Tensorflow Node.js and Java Bindings,2,closed,2019-03-19T18:27:26Z,2020-11-17T17:08:29Z,"#### TensorFlow.js version1.13.1#### Browser versionNode.js#### Describe the problem or feature requestConflicts between Tensorflow Java and JavaScript bindings.In my project I'm running Tensorflow Java bindings (because it existed before TF.JS bindings).It seems that as soon as I load the Node.js bindings, the Java bindings will fail to load:In `@tensorflow/tfjs-node/deps/lib` I have the node bindings```ls -l node_modules/@tensorflow/tfjs-node/deps/libtotal 388432-r-xr-xr-x  1 loretoparisi  staff  182985692  1 Gen  1970 libtensorflow.so-r-xr-xr-x  1 loretoparisi  staff   14136480  1 Gen  1970 libtensorflow_framework.so```while let's say in `/root/jni` there are the Java bindings:```total 294152-r-xr-xr-x  1 loretoparisi  staff     331921 28 Giu  2018 LICENSE-r-xr-xr-x  1 loretoparisi  staff   12019756 28 Giu  2018 libtensorflow_framework.so-rw-r--r--  1 loretoparisi  staff   12019756 28 Giu  2018 libtensorflow_framework_darwin.so-rw-r--r--  1 loretoparisi  staff  124911544 28 Giu  2018 libtensorflow_jni.dylib```where Java bindings are loaded through the `-Djava.library.path=` library path (see the repo below for an example).What happens is this 1. The Node.js Tensorflow loads perfectly through the `require('@tensorflow/tfjs-node'),`. I can see the typical logging.```2019-03-19 19:21:41.644954: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA``` And also the module works in inference. As soon as I load the java module, I get the `UnsatisfiedLinkError` caused by the native library:```	... 14 moreCaused by: java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: darwin, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)	at org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:101)	at serving_utils.SentenceClassificationModel.<init>(SentenceClassificationModel.java:42)	at serving_utils.SentenceClassificationModel.<init>(SentenceClassificationModel.java:25)	at musixmatch_nlp.MXMSentimentTensorflowAnnotator.<init>(MXMSentimentTensorflowAnnotator.java:32)	... 19 more```**NOTE**Without the TF.JS module running, the Java bindings works, i.e. the `UnsatisfiedLinkError` is not thrown and the module is loaded correctly.#### Code to reproduce the bug / link to feature requestRun Java Bindings from here https://github.com/loretoparisi/tensorflow-javaand in the same run-time run the TF.JS bindings","[""**[UPDATE]**Further findings: the issue happens in both cases when reversing the order of Java / Node.js modules loading into the system:1. Load TF.JS `require('@tensorflow/tfjs-node'),` **then** Load Tensorflow Java (with ` -Djava.library.path=`)The error was:```shCaused by: java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: darwin, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.```2. Load Tensorflow Java (with ` -Djava.library.path=`) **then**  Load TF.JS `require('@tensorflow/tfjs-node'),` The error was raised by `dlopen`: **Symbol not found: __ZN10tensorflow10DeviceBase16eigen_cpu_deviceEv**full stack trace:```shRegistration of backend tensorflow failedError: dlopen(/node_modules/@tensorflow/tfjs-node/build/Release/tfjs_binding.node, 1): Symbol not found: __ZN10tensorflow10DeviceBase16eigen_cpu_deviceEv  Referenced from: /node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so  Expected in: /Users/loretoparisi/webservice/jni/darwin/libtensorflow_framework.so in /node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so    at Object.Module._extensions..node (internal/modules/cjs/loader.js:707:18)    at Module.load (internal/modules/cjs/loader.js:589:32)    at tryModuleLoad (internal/modules/cjs/loader.js:528:12)    at Function.Module._load (internal/modules/cjs/loader.js:520:3)    at Module.require (internal/modules/cjs/loader.js:626:17)    at require (internal/modules/cjs/helpers.js:20:18)    at bindings (/node_modules/bindings/bindings.js:84:48)    at /node_modules/@tensorflow/tfjs-node/dist/index.js:49:60    at Environment.registerBackend (/node_modules/@tensorflow/tfjs-core/dist/environment.js:439:27)    at Object.<anonymous> (/node_modules/@tensorflow/tfjs-node/dist/index.js:48:8)    at Module._compile (internal/modules/cjs/loader.js:678:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:689:10)    at Module.load (internal/modules/cjs/loader.js:589:32)    at tryModuleLoad (internal/modules/cjs/loader.js:528:12)    at Function.Module._load (internal/modules/cjs/loader.js:520:3)    at Module.require (internal/modules/cjs/loader.js:626:17)============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================```====="", 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/1165,Improve shape mismatch error in layers.,11,open,2019-01-28T21:50:08Z,2021-09-08T12:09:11Z,"When calling fit, we get errors like this:```tf-layers.esm.js:17 Uncaught (in promise) Error: Error when checking target: expected sampling_layer_SamplingLayer1 to have shape [,4], but got array with shape [1,512].```This is mostly fine, but it would be good to log the name of the other layer that this connects with that is causing the error.Maybe also instead of ""target"" we could say something like ""output""? How about this error message:```sampling_layer_SamplingLayer1 is expected to have an input shape of [null, 4] but the output of previous_layer has an output shape of [null, 512].```","['Hey! Interested in this. Any starting point that I should be looking at?=====', '@caisq are you working on this one? If not, is it okay for the community to send a PR?=====', '@duhaime @usmanmuhd Go for it.=====', 'I had hoped to but am bound up at the moment!=====', '@nsthorat @duhaime : i have sent a PR for this issue https://github.com/tensorflow/tfjs/pull/2848 Please review and let me know if there are any changes required. :smile: =====', 'I will work on this.=====', 'Hi, is anybody working on this issue? If not, I would like to work on this issue.=====', '@sourabh112 please go ahead with the changes , thank you for your interest.=====', '@sourabh112 if you are stuck somewhere could i help? Or if you are busy at the moment can i pick it up?=====', ""@mdaj06 Thanks for tagging me. My semester has started and I totally missed updates this issue. I'm busy right now and can't take on this issue for now, you can go ahead and work on it. ====="", ""@sourabh112 sure no problem! @rthadur i'll have a go at this issue then=====""]",0
https://github.com/tensorflow/tfjs/issues/5573,Propagates NaNs failure on WebGL1,2,open,2021-09-02T01:12:27Z,2021-09-14T18:30:55Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: 95.0.4629.2 (Official Build) canary (64-bit) (cohort: Clang-64)- Tensorflow.js Converter Version:**Describe the current behavior**Unit test has 14 failures on WebGL1 backend, most of them related to NaN propagation **Describe the expected behavior**All passed**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.`cd tfjs-backend-webgl && yarn && yarn karma start --testEnv webgl1`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Chrome 95.0.4629.2 (Windows 10) step kernel webgl1 {""WEBGL_VERSION"":1,""WEBGL_CPU_FORWARD"":false,""WEBGL_SIZE_UPLOAD_UNIFORM"":0} propagates NaNs FAILED        Error: Arrays differ: actual[4] = 0, expected[4] = NaN.            at <Jasmine>            at expectArraysPredicate (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:77834:23)            at expectArraysClose (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:77788:16)            at C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65502:28            at step (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65425:23)            at Object.next (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65406:53)            at fulfilled (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65397:58)......```","['In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",1
https://github.com/tensorflow/tfjs/issues/5539,Support L2-Pooling.,2,closed,2021-08-27T08:05:40Z,2021-09-22T00:55:59Z,"Hi,Is there any plan to support L2-Pooling?  Thanks.Link https://github.com/webmachinelearning/webnn-polyfill/pull/112.","['There are suggestion that you can decompose L2 pooling with following:```tf.sqrt(tf.avgPool(tf.square(h)) ```=====', 'Thanks much @pyu10055. Close it.  =====']",1
https://github.com/tensorflow/tfjs/issues/3901,[BodyPix] Improve body segmentation(flickering edges) updates for a video stream,0,closed,2020-09-10T10:19:11Z,2020-09-10T10:26:51Z,"This is related to body segmentation using body-pix.body segmentation has a flickering effect around the edges for a video stream.Even setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video.Had similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?**System information**- TensorFlow.js version (you are using): tfjs@2.1.0, body-pix@2.0.5- Are you willing to contribute it (Yes/No):Yes, I'm willing to help with whatever i can. But, I'm not familiar with tensorflow models though.**Describe the feature and the current behavior/state.**This is related to body segmentation using body-pix.body segmentation has a flickering effect around the edges for a video stream.Even setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video.Had similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?**Will this change the current api? How?**Not sure. Might change current api if additional masks are added.**Who will benefit with this feature?**Everyone in the community who uses body-pix model.**Any Other info.**",[],0
https://github.com/tensorflow/tfjs/issues/3717,Wrong predictions after converting from tf2_image_retraining.ipynb to tfjs_graph_model,4,closed,2020-08-03T11:58:50Z,2020-09-11T18:27:14Z,"TensorFlow.js version: 2.0Browser version: Chrome Problem: Wrong results after conversionCode to reproduce the bug / link to feature request:https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifierhttps://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynbI have used make_image_classifier to make my own classifier and converted that to Tensorflow.js with success, giving me confidence rates like A: 0.67XXX, B: 0.10XXX, C: 0.23XXXX (together 1.0)I want to do the same with the corresponding Colab notebook to have more control but if I use it with default settings, I get weird predictions after conversion then, like:A: 0.09XXX, B: 2.69XXX, C: 0.77XXXXI wonder why this is happening - how can I use the notebook and convert succesfully?For both I used this conversion format from https://github.com/tensorflow/tfjs/tree/master/tfjs-converter:tensorflowjs_converter--input_format=tf_saved_model--output_format=tfjs_graph_model--signature_name=serving_default--saved_model_tags=serve/mobilenet/saved_model/mobilenet/web_model","['@vjuss can you share the saved models from both make_image_classifier script and colab? =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', ""@pyu10055 I'm sorry to reply this late! What eventually happened was that I followed advice from this thread https://stackoverflow.com/questions/55849309/retrain-image-detection-with-mobilenet/56340676#56340676 (by you, for example!) and used retrain.py script for my training, as I realized that make_image_classifier was more optimized for TF Lite. I could try to provide the make_image_classifier and colab models for further research - now in the middle of a final project deadline so will look into that once this is over!=====""]",0
https://github.com/tensorflow/tfjs/issues/4719,"speech-commands demo failing with large training sets - with ""TypeError: n.array is not a function""",5,closed,2021-02-19T19:55:56Z,2021-02-27T13:13:45Z,"I'm loading speech-commands from npm, using version 0.5.2. If I revert to 0.4.2, I don't see the error shown below, so this looks like a recent regression. (Could it be a build problem, similar to the one fixed in https://github.com/tensorflow/tfjs/issues/4709 ?) ### When do I get the error?When I call `transferRecognizer.train` You can see an example of what I'm doing at https://github.com/tensorflow/tfjs-models/blob/62f4f9c20db0eef4cae1ee82d000b9cd69696a58/speech-commands/demo/index.js#L456-L466 Calling `train()` results in an uncaught exception### What error am I getting?```Uncaught (in promise) TypeError: n.array is not a function    at speech-commands.min.js:17    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at e.getData (speech-commands.min.js:17)    at r.collectTransferDataAsTfDataset (speech-commands.min.js:17)    at r.<anonymous> (speech-commands.min.js:17)    at speech-commands.min.js:17    at Object.next (speech-commands.min.js:17)```### How to recreate I _think_ this is only happening with training data that is large enough to trigger this:```Detected large dataset: total duration = 98900 ms > 60000 ms. Training transfer model using fitDataset() instead of fit()```When I use training data small enough to not see that message, I don't see the error.### More info about the exceptionI tried rebuilding the app with speech-commands.js (instead of the minified version) to get a more useful stack trace:```Uncaught (in promise) TypeError: tfd.array is not a function    at speech-commands.js:1067    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at Dataset.getData (speech-commands.js:966)    at TransferBrowserFftSpeechCommandRecognizer.collectTransferDataAsTfDataset (speech-commands.js:2536)    at TransferBrowserFftSpeechCommandRecognizer.<anonymous> (speech-commands.js:2616)    at step (speech-commands.js:95)    at Object.next (speech-commands.js:76)```Catching the error in a debugger, I can see it's caused by this line:![image](https://user-images.githubusercontent.com/1444788/108553596-d2d4b080-72ea-11eb-9380-967e4cc841c4.png)`tfd` isn't null/undefined, and looks like it's the right sort of shape, but sure enough, it doesn't have an `array` function.![image](https://user-images.githubusercontent.com/1444788/108553702-fbf54100-72ea-11eb-8b62-3a66a6ca28c8.png)And if I let it try to step past that line, I get the stack trace copied above![image](https://user-images.githubusercontent.com/1444788/108553762-0ca5b700-72eb-11eb-8173-e0cdfb7c9bbe.png)","['@dalelane Thank you for the detailed debugging session, this seems to be kinda weird, the tfd (tfjs-data) object seems to become tfjs-core. Is it possible for you to create a codepen example of this? thanks!=====', ""Not easily, sorry. But essentially I'm just doing this:```jsvar baseRecognizer = speechCommands.create('BROWSER_FFT'),baseRecognizer.ensureModelLoaded()    .then(function () {         var transferRecognizer = baseRecognizer.createTransfer('uniqueid'),                  // collect at least 60 seconds worth of examples         // e.g.  many times calling             //  transferRecognizer.collectExample('label'),                               return transferRecognizer.train(),    }),```====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">No</a>=====', '@dalelane Please verify with the new release 0.5.3, thanks.=====', '@pyu10055 That seems to have done the trick - thanks!=====']",1
https://github.com/tensorflow/tfjs/issues/5765,Can a graph model loaded with tensorflow.js use data on GPU without transferring it to the CPU first?,9,open,2021-10-23T02:00:45Z,2021-12-20T22:28:07Z,"Hi, I am currently using tfjs 3.8 to load a segmentation model (loaded as a tf.GraphModel) on the client side. To create the input Tensor, I call browser.fromPixels(imageData), which creates the Tensor on CPU from the ImageData object that is also on CPU. Since I'm using tfjs' webgl backend, the data is sent to the GPU when calling the model.predict(tensor) function. All of this works well, excepted that my ImageData object is created from an image on a canvas with a WebGLRenderingContext, meaning it comes from the GPU. This GPU->CPU->GPU data transfer is slowing down my process, which I am trying to optimize.I briefly searched tfjs and could not find a way to create a Tensor on GPU to prevent the GPU->CPU data transfer. Is there a way I could keep my data on the GPU?","['if you call `browser.fromPixels(canvas)` and it can get `gl` context on it, it will use small GSLS program inside `fromPixels` implementation for `webgl`, so its GPU -> GPU. any other variation, it has to be prepared on CPU first.=====', 'Thanks a lot! I will try that :)=====', ""Very cool. Is there any TFJS documentation or examples for the fast GPU <--> GPU transfer paths?Is there a path that let's me access a texture ID from a Tensor. I.e. to take the result from, say predict() and continue other GPU processing without transferring to the CPU?====="", ""@danwexler `fromPixels()` is an exception to otherwise rule that tensors start in heap space and then get created whatever the *backend* is in use - as only with `canvas` with `gl` context its actually possible to read data from canvas using *gsls*   but once tensor is in gpu, for majority of operations it stays there without transfers back and forth unless:- operation you're attempting doesn't have a native `gsls` implementation and instead uses cpu fallback    (afaik, there are no docs which ops use fallback implementation and there is no way to tell without looking at sources)    (afaik again, none of the core ops allow for fallback - you'd get op not implemented error, its primarily some auxiliary ops like `nonMaxSuppression`)- operation is deemed tiny and its faster to perform it on cpu than use shaders  (controllable via tf env variable, default threshold is 128 elements)  one sure way to break this pipeline it is to download tensor (`.data()` or `.array()` methods), do something with that data and then create new tensor from it. thats why i try to get as much done via tf ops until very last moment (even if its sometimes tedious) to download data.====="", ""@vladmandic  Understood. In addition, I want to get the GPU texture ID for the final tensor so that I can, say, pass it to another library that performs image processing using WebGL shaders. FWIW, I'm also investigating[ implementing traditional image processing as TF models](https://discuss.tensorflow.org/t/tf-image-processing/5166), but if I can just get the texture ID and I implicitly know the format, it would make integration with many other GPU processing tools much easier.====="", ""@danwexler only starting point i can think of is `tensor.id`, so where does it lead?  took a quick look, it seems that **tfjs** uses `weakMap` to map `tensor.id` to instance of  `DataStorage: {texture, dtype, texShape, usage, isPacked, slice}`  where `texture` is an actual instance of a `WebGLTexture`  and instance of `DataStorage` is private to each backend instance as `this.texData`  so you can actually read `tf.engine().backendInstance.texData.data` and remap it with `tensor.id`  and you have your `WebGLTexture`  btw, why a `weakMap` and not have those properties directly under `tensor`?  **tfjs** maintains its own ref counters for memory management and caches textures even when tensor is released  alternatively, leave texture management to **tfjs** and implement your `glsl` code as a custom tf operation using `tf.registerKernel({ kernelName, backendName, kernelFunc }),`i'm curious what do you do with it - keep me posted?  ====="", '@vladmandic  Great info. Will keep you posted! =====', ""Hi @vladmandic, I did try passing a `canvas` with a `webgl` context to the `browser.fromPixels` function and it looks like it effectively uses only the GPU to create the tensor. So thanks a lot for that!Now I'm trying to keep the model output on the GPU. I saw that `browser.toPixels` had a `canvas` parameter but it looks like it's used only for drawing on the `2d` context of the canvas with [the data copied on CPU](https://github.com/tensorflow/tfjs-core/blob/master/tfjs-core/src/ops/browser.ts#L97).I see the info you gave about the `texture` and `texData`, I will try using it but I fear this is a little much for someone without lots of experience with webgl like me.====="", 'crosslinking feature request #5918 =====']",1
https://github.com/tensorflow/tfjs/issues/5115,React Native : Huge Memory Leak when using face-landmark-detection in estimateFaces() function,3,closed,2021-05-24T21:11:05Z,2021-06-06T20:28:27Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Tablet S6- TensorFlow.js installed from (npm or script link): yarn add @tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: -- Tensorflow.js Converter Version: -**Describe the current behavior**Huge Memory Leak happening when calling estimateFaces() inside TensorCamera Loop(**Memory usage reach 3.5 GB in less than 1 Minute !!!!**)**Describe the expected behavior**No Memory Leak **Standalone code to reproduce the issue**```""dependencies"": {    ""@react-native-community/async-storage"": ""^1.12.1"",    ""@tensorflow-models/face-landmarks-detection"": ""^0.0.3"",    ""@tensorflow/tfjs"": ""^3.6.0"",    ""@tensorflow/tfjs-react-native"": ""^0.5.0"",    ""expo"": ""~41.0.1"",    ""expo-camera"": ""~11.0.2"",    ""expo-gl"": ""~10.2.0"",    ""expo-gl-cpp"": ""~10.1.0"",    ""expo-splash-screen"": ""~0.10.2"",    ""expo-status-bar"": ""~1.0.4"",    ""expo-updates"": ""~0.5.4"",    ""react"": ""16.13.1"",    ""react-dom"": ""16.13.1"",    ""react-native"": ""~0.63.4"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-gesture-handler"": ""~1.10.2"",    ""react-native-reanimated"": ""~2.1.0"",    ""react-native-screens"": ""~3.0.0"",    ""react-native-unimodules"": ""~0.13.3"",    ""react-native-web"": ""~0.13.12""  },``````import { StatusBar } from 'expo-status-bar',import React, {useState, useEffect} from 'react',import { StyleSheet, Text, View } from 'react-native',import {Camera} from 'expo-camera',import * as tf from '@tensorflow/tfjs',import {cameraWithTensors, bundleResourceIO} from '@tensorflow/tfjs-react-native',import * as FaceModel from '@tensorflow-models/face-landmarks-detection',const modelJson = require('./assets/model.json'),const modelWeights = require('./assets/group1-shard1of1.bin'),export default function App() {  const [tensorReady, setTensorReady] = useState(false),  const [modelReady, setModelReady] = useState(null),  const TensorCamera = cameraWithTensors(Camera),  useEffect(()=>{    (async () => {      await tf.ready(),      await tf.loadGraphModel(bundleResourceIO(modelJson, modelWeights)),      setTensorReady(true),      setModelReady(await FaceModel.load(FaceModel.SupportedPackages.mediapipeFacemesh, {maxFaces: 1, shouldLoadIrisModel: true})),      console.log(tf.getBackend()),      console.log(""READY!!!""),    })(),  }, [])  const handleImage = (images, updatePreview, gl) => {              const loop = async () => {                let nextImageTensor = images.next().value,                const result = await modelReady.estimateFaces({input: nextImageTensor}),                console.log(result),                // if autorender is false you need the following two lines.                // updatePreview(), gl.endFrameEXP(),                requestAnimationFrame(loop),              },              loop(),            },  return (    <View style={styles.container}>      <Text>Open up App.js to start working on your app!</Text>      <StatusBar style=""auto"" />      {(modelReady &&           <TensorCamera            style={[styles.camera, {height: 200, width: 200}]}            type={Camera.Constants.Type.front}            onReady={handleImage}            cameraTextureHeight={1080}            cameraTextureWidth={1920}            resizeHeight={200}            resizeWidth={152}            resizeDepth={3}            autorender={true}          />      )}    </View>  ),}const styles = StyleSheet.create({  container: {    flex: 1,    backgroundColor: '#fff',    alignItems: 'center',    justifyContent: 'center',  },}),```**Other info / logs** TensorFlow was loaded properly (with 'rn-webgl' backend engine)The Model is also loaded properly (Attached)Setup steps are following: https://www.npmjs.com/package/@tensorflow/tfjs-react-native[facemesh_1_default_1.tar.gz](https://github.com/tensorflow/tfjs/files/6535008/facemesh_1_default_1.tar.gz)","['@OckyKristanto-TomTom Please dispose the image tensors from TensorCamera.Please take a look at the example [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsx#L101)=====', '@pyu10055 . Indeed that dispose line solves the Memory Leak. Thanks a lot for the swift reaction and helpSuggestion: maybe its worth it to add the dispose line on the documentation: https://js.tensorflow.org/api_react_native/0.5.0/#cameraWithTensors=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5115"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5115"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5319,Trying to use ML5.js with Posenet,2,closed,2021-07-12T21:36:27Z,2021-07-15T00:42:20Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOs Big Sur- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):script link- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**I am trying to use two different tfjs libraries ml5.js and posenet in a single project. They both work in seperate projects, but I get errors building when they are used together. To run https://unpkg.com/ml5@latest/dist/ml5.min.js I need https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5, but that version of tfjs is incompatible with https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection and https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core.**Provide the exact sequence of commands / steps that you executed before running into the problem**Right now I am using the following to load a ml model:https://unpkg.com/ml5@latest/dist/ml5.min.js with https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5and in a different project the following to do pose detectionhttps://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection and https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core with https://cdn.jsdelivr.net/npm/@tensorflow/tfjsEach project works separately, but not together.**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.When I run the pose detection project with https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5 I get the following error:<img width=""665"" alt=""Screen Shot 2021-07-12 at 2 32 12 PM"" src=""https://user-images.githubusercontent.com/20036774/125358652-06f25180-e31e-11eb-975b-d89acafa1f3b.png"">","[""This question is better suited for https://github.com/ml5js/ml5-library/issues , we will not be able to support ML5.js. One thing to mention is you can't use tfjs and tfjs-core together because tfjs is a union package which includes tfjs-core.So please use only one package. ====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5319"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5319"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4720,the speed is too slow,4,closed,2021-02-20T03:00:38Z,2021-03-08T19:20:50Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 7- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.8.0- Browser version:Chrome 88.0.4324.150- Tensorflow.js Converter Version:- PC:lenovo y7000,- cpu: i7 6700HQ- backend:wasm**Describe the current behavior**I tested the blazeface demo on my own PC,but the inference was 35ms,it was much slower than the official announcement，can you help me to solve it？**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['@Kevin-1993-35 can you try with latest version 3.0.0-rc.1 ? and please let us know which announcement you are referring to ?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4720"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4720"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/740,@tensorflow-models/mobilenet does not work under Node.js,14,closed,2018-09-27T22:26:13Z,2020-05-26T17:43:43Z,"Trying to use the `@tensorflow-models/mobilenet` package under Node.js throws the following exception:2018-09-27 15:25:56.699190: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA(node:98265) Warning: N-API is an experimental feature and could change at any time.(node:98265) UnhandledPromiseRejectionWarning: Error: browserHTTPRequest is not supported outside the web browser without a fetch polyfill.    at new BrowserHTTPRequest (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:46:19)    at Object.browserHTTPRequest (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:247:12)    at Object.<anonymous> (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:98:50)    at step (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:42:23)    at Object.next (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:23:53)    at /Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:17:71    at new Promise (<anonymous>)    at __awaiter (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:13:12)    at Object.loadModelInternal (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/models.js:92:12)    at Object.loadModel (/Users/kreeger/workspace/mn-test/node_modules/@tensorflow/tfjs-layers/dist/exports.js:16:21)(node:98265) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 4)(node:98265) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.","[""If people come across this issue, I found a (relatively) simple workaround using the `node-fetch` library from NPM. https://www.npmjs.com/package/node-fetch```jsconst mobilenet = require('@tensorflow-models/mobilenet')global.fetch = require('node-fetch')const model = await mobilenet.load()```====="", '@jthomas hi, is this because the CPU is not supported in tf? would you please give the complete code about how to make node-fetch work? it should be inside the async function? thank you very much.=====', ""@dontmarryme this is because `fetch` does not exist by default on node. So adding this line```jsglobal.fetch = require('node-fetch')```To the top of your file after installing https://www.npmjs.com/package/node-fetch should do the trick====="", ""Thank you for the help! but still couldn't get it right, I followed everything here (hope I didn't miss anything): http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/@tafsiri I put global.fetch = require('node-fetch') on top inside the script.js already, still don't work, by the way I am on Mac, does it make any difference?@jthomas also I found I missed the downloading of mobilenet model part, it's pretty manual process, still trying to figure out what's there... is there a easier way to make mobilenet running with node.js?this all my filehttps://gitlab.com/greenhouseinfo/mobilenet-on-node.jsAPPLEde-MacBook-Pro-2:mobilenet apple$ node script.js mobilenet/model.json panda.jpgcpu backend was already registered. Reusing existing backend2018-10-23 18:52:03.945011: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA(node:44077) UnhandledPromiseRejectionWarning: TypeError: Only HTTP(S) protocols are supported    at getNodeRequestOptions (/Users/apple/Desktop/mobilenet/node_modules/node-fetch/lib/index.js:1261:9)    at /Users/apple/Desktop/mobilenet/node_modules/node-fetch/lib/index.js:1322:19    at new Promise (<anonymous>)    at fetch (/Users/apple/Desktop/mobilenet/node_modules/node-fetch/lib/index.js:1319:9)    at BrowserHTTPRequest.<anonymous> (/Users/apple/Desktop/mobilenet/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:163:40)    at step (/Users/apple/Desktop/mobilenet/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:32:23)    at Object.next (/Users/apple/Desktop/mobilenet/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:13:53)    at /Users/apple/Desktop/mobilenet/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:7:71    at new Promise (<anonymous>)    at __awaiter (/Users/apple/Desktop/mobilenet/node_modules/@tensorflow/tfjs-core/dist/io/browser_http.js:3:12)(node:44077) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 4)(node:44077) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.====="", ""@dontmarryme `fetch` only works for http(s) protocol. It looks like you changed the protocol to `file://` which wouldn't work with fetch. I'd say you can either go the route of loading the hosted model (and not changing the protocol to file) or following the tutorial code (which doesn't need node-fetch). Have you tried running the code from that tutorial as is (after downloading the necessary files)?====="", ""@tafsiri thanks for the reply, no, I haven't tried the tutorial, which tutorial link you are referring to? what I want is to run tfjs on node.js, actually I have no background for js so all are very confusing for me, thank you for your guide again!====="", '@dontmarryme i was referring to the tutorial you linked above http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-jsHave you tried it when everything is exactly similar to that tutorial? The code you posted has some differences.=====', 'I now follow the tutorial code loading models from file with file:// without node-fetch: ""script.js model.json panda.jpg"", but still getting error messages, I also have downloaded the models manually.=====', 'I have exactly the same issue with @dontmarryme . Even following the [tutorial](http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js), I still meet with error `browserHTTPRequest is not supported outside the web browser without a fetch polyfill.`.=====', 'I finally worked it out.I use `global.fetch = require(\'node-fetch\')`. In `script.sh` I change the loading mode script from `file` to `https`:```  mn.path = `https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json`  await mn.load()```Then I met with an error `Error: Unknown feature TENSORLIKE_CHECK_SHAPE_CONSISTENCY.`And then , I bump tfjs to the latest release:```    ""@tensorflow/tfjs"": ""^0.13.0"",    ""@tensorflow/tfjs-node"": ""^0.1.19"",```It works for me now.=====', ""@daisy-ycguo thank you for the replies! it works for me now, but do you know why there's still the compiling error messages? 2018-10-27 18:54:05.844070: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAclassification results: [ { className: 'running shoe', probability: 0.7699763774871826 },  { className: 'sandal', probability: 0.2003212571144104 },  { className: 'shoe shop, shoe-shop, shoe store',    probability: 0.013504834845662117 } ]====="", ""@dontmarryme That isn't a compile error - the TensorFlow library we ship with isn't fully optimized for your machine (but runs on all hardware). It is just a harmless warning - see https://github.com/tensorflow/tfjs/issues/571 for more info.====="", 'Thanks @daisy-ycguo that worked well. I am trying to make it work still with local models, did anyone make it work?I get the same error message `(node:20391) UnhandledPromiseRejectionWarning: TypeError: Only HTTP(S) protocols are supported` when following the aforementioned tutorial.=====', '@aperkaz I found this guy has modified the script and pointed out the modification!https://sean12697.github.io/blog/2019/07/15/mobilenet-in-nodejs.html=====']",0
https://github.com/tensorflow/tfjs/issues/4680,tf.nonmaxsuppression() in webgl locks the ui thread. call tf.nonmaxsuppressionasync() instead,4,closed,2021-02-14T06:20:00Z,2021-07-14T17:19:49Z,Getting this warning when using Cocossd in tfjs/react-native- TensorCamera preview looks laggy for real-time object detection- Getting only one object in prediction array all timePlease give any solution for smooth camera view and fast object detection through Cocossd,"[""It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tfjs/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.====="", '> Getting this warning when using Cocossd in tfjs/react-native> > * TensorCamera preview looks laggy for real-time object detection> * Getting only one object in prediction array all time> > Please give any solution for smooth camera view and fast object detection through CocossdDid you got the solution ??=====', 'Any update on this?=====', '> > > Getting this warning when using Cocossd in tfjs/react-native> >     * TensorCamera preview looks laggy for real-time object detection> >     * Getting only one object in prediction array all time> > > Please give any solution for smooth camera view and fast object detection through CocossdDid you find a solution?=====']",1
https://github.com/tensorflow/tfjs/issues/4871,running some models using webgl is 10x slower than using nodejs,16,closed,2021-03-27T13:03:32Z,2021-11-26T16:45:18Z,"Model executes in NodeJS using `tensorflow` backend in ~100ms, but above 1sec in browser using `WebGL` backendThat is 10x difference in performance between NodeJS and `WebGL`  I've tried to figure out why a model is executing an order of magnitude slower than expected using `WebGL` backend,  but profiler info for it makes no sense```const t0 = performance.now(),const res = await tf.profile(() => model.executeAsync(input)),const t1 = performance.now(),const wallTime = t1 - t0,const kernelTime = res.kernels.reduce((a, b) => a += b.kernelTimeMs, 0),```wallTime is 900-1200ms  kernelTime is ~20ms  I'm re-running inference on the same input twice and looking at second run to allow for warmup time of `WebGL` backend (shader compile, etc.)  I even tried `tf.enableDebugMode()` and I still don't see anything that gets even close to overall wall time  And I have no idea where is time spent?  Model in question: <https://github.com/vladmandic/nanodet>Environment: TFJS 3.3.0 on Ubuntu 20.10 and Chrome 89","['@vladmandic Have you tried with our [benchmark tool](https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html), I tried the file in model-tfjs-graph-m/ directory, it runs around 79ms on my mac pro 2018, but the profile time is around 1sec.The profile time is different from the real inference time, since it is wait for GL time query for each kernel.=====', ""haven't used that tool before, will try.what confuses me that i have two variations of the model `model-tfjs-graph-m` and `model-tfjs-graph-g`  they both have comparable execution time in `nodejs`, but, *g-variation* executes 10x faster than *m-variation* using `webgl` backend - and i can't figure out why the *m-variation* would be so slow in browser and totally fine otherwise.====="", ""Using benchmark tool, setting backend to `webgl` and leaving everything else at default values:Model URL: <https://raw.githubusercontent.com/vladmandic/nanodet/main/model-tfjs-graph-m/nanodet.json>Result: Subsequent average(5 runs)\t114.8 msModel URL: <https://raw.githubusercontent.com/vladmandic/nanodet/main/model-tfjs-graph-g/nanodet.json>Result: Subsequent average(5 runs)\t65.6 msSo difference is ~2x which is expected and not even close to ~10x that is seen in reality  I still don't know why the model inference for m-variation is that slow - about 5x slower than expected  ====="", 'Another model with same symptoms: <https://github.com/vladmandic/efficientpose>Looking at simplest ""I-Lite"" variation:<https://raw.githubusercontent.com/vladmandic/efficientpose/main/model-tfjs-graph-i-lite/efficientpose.json>Wall times (measured in JS exactly before and after call to `executeAsync()`):- **NodeJS**: ~150 ms- **WASM**: ~350 ms- **WebGL**: ~500 msAnd what does benchmark tool report?- **WASM**: subsequent average = 300 ms    So only slight overhead between benchmark and actual wall time - completely within reason- **WebGL**: subsequent average = 30 ms    So where does the 15x difference between benchmark and wall time come from???  And even if we say that kernel benchmarking is just wrong,    there is no chance that model should execute this slowly with `WebGL` backendThere is something really wrong with WebGL when executing specific models=====', '@vladmandic The profile() method has much more overhead in WebGL comparing to WASM. It aims to measure kernel speed, no to measure inference speed. It adds GL time query at each kernel execution. The average inference time in benchmark is the more accurate measurement for deployment.=====', ""@pyu10055 I only brought up `profile()` method because I was trying to figure out why some models are  5-10x slower for inference using `WebGL` backend than using NodeJS `tensorflow` backend (that should definitely not be the case, especially how low-profile models are) - that is the key question  And when possible, I've tried using same models through other frameworks - for example, inference time using NodeJS with `tensorflow` backend is comparable to TF2 when used directly via Python or even with PyTorch - so no issues other than `WebGL`!But given they profile shows total kernel time 15x lower than inference time, how can I get to bottom of this?I've posted here two very simple models, `NanoDet` and `EfficientDet` that exhibit that behavior  ====="", '@vladmandic Can you clarify on how are you benchmarking WebGL that give you 15x slowness?For example what is the total kernel time you are referring to?and how are you measuring the inference time?=====', ""inference time is simple:in browser:```jsconst t0 = performance.now(),const res = await model.executeAsync(input)),const t1 = performance.now(),const inferenceTime = Math.round(t1 - t0), // elapsed time in ms```and in nodejs:```jsconst t0 = process.hrtime.bigint(),const res = await model.executeAsync(input)), // also can be model.predict(input),const t1 = process.hrtime.bigint(),const inferenceTime = Math.round((t1 - t0) / 1000 /1000), // to convert to ms```and i'm only measuring subsequent runs, skipping first run so webgl has time to warmup (compile&upload shaders)for most models, webgl is fast and it's the best option (since node-gpu doesn't work due to obsolete cuda dependency from tf1).however, for some models (i've provided examples), webgl is 10x slower than nodejs.but running `profile()` shows nothing useful.====="", "">  (since node-gpu doesn't work due to obsolete cuda dependency from tf1).Note that tfjs-node-gpu [will soon be updated to use TF 2](https://github.com/tensorflow/tfjs/pull/4810), and it can be used with TF 1.15.====="", ""@DanielRuf yup, i'm monitoring that. still need to fix whatever is wrong with `webgl`====="", '@vladmandic based on my test the inference time on webgl (79ms) for nanodet is faster than node.js (100ms).I am still confused about your claim that webgl is 10x slower than nodejs.To reconciling the conceptual difference, can you provide the exact inference time you getting and how are you measure that for a single model? thanks=====', ""i've re-converted the models while changing some resource types  it seems there was some serious clipping in the converted model (original implementation heavily uses `int64`)  which was causing trouble for `webgl` inference as the model was branching differently and doing far more work  right now, results are as expected - `webgl` is faster than `nodejs` in pretty much all cases  sorry for the false leads and time wasted, this took me a while to figure out and outputs of `profile()` were throwing me off  ====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4871"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4871"">No</a>=====', '@vladmandic @pyu10055  any comments on this issue https://github.com/tensorflow/tfjs/issues/4907. Do you notice any problems between benchmark tool and own profiling for first run?=====', ""> i've re-converted the models while changing some resource types it seems there was some serious clipping in the converted model (original implementation heavily uses `int64`) which was causing trouble for `webgl` inference as the model was branching differently and doing far more workHey @vladmandic , I think I am facing similar issues. I'm trying to do some inference with an EfficientNet model, and I get slower inference time with the webgl backend. Could you please give some details on how you re-converted the models to make it work ? Thank you!====="", ""@OlivierMns I never stated I saw the problem with EfficientNet, I spoke about EfficientDet. But in general, issue in my case was clipping due to quantization - try unquantized model first. Also, EfficientNet is notoriously slow for warmup, I suggest to enable WebGL uniforms which speeds up warmup 2-4x (no difference on actual inference)> tf.ENV.set('WEBGL_USE_SHAPES_UNIFORMS', true),=====""]",1
https://github.com/tensorflow/tfjs/issues/1469,Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR,8,closed,2019-03-29T18:30:04Z,2020-02-14T17:30:04Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version{ 'tfjs-core': '1.0.3',  'tfjs-data': '1.0.3',  'tfjs-layers': '1.0.3',  'tfjs-converter': '1.0.3',  tfjs: '1.0.3',  'tfjs-node': '1.0.2' }#### Browser versionRunning on nodeUbuntu 18.04> $ nvidia-smi> Fri Mar 29 19:25:37 2019       > +-----------------------------------------------------------------------------+> | NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |> |-------------------------------+----------------------+----------------------+> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |> |===============================+======================+======================|> |   0  GeForce RTX 2070    Off  | 00000000:01:00.0  On |                  N/A |> | N/A   46C    P8     9W /  N/A |    879MiB /  7952MiB |      3%      Default |> +-------------------------------+----------------------+----------------------+#### Describe the problem or feature requestI'm unable to use cudnn convolutional layers in my model on `tfjs-node-gpu` Possibly related due to issues with RTX series in this tensorflow [workaround](https://github.com/tensorflow/tensorflow/issues/24496)  there is suggestion to use `config.gpu_options.allow_growth = True`Is there such option in tensorflow js?#### Code to reproduce the bug / link to feature request```const tf = require('@tensorflow/tfjs-node-gpu'),const model =  tf.sequential({    layers: [            tf.layers.conv2d({        inputShape:[32, 32, 3],        filters: 32,         kernelSize: [3, 3],        activation: 'relu',      }),      tf.layers.maxPooling2d([2, 2]),          ],  }),model.predict(tf.randomNormal([4, 32, 32, 3]))     .then((res) => {         res.print(),     })```> $ node index.js > 2019-03-29 19:22:37.112495: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA> 2019-03-29 19:22:37.249964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2019-03-29 19:22:37.250443: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3aa4000 executing computations on platform CUDA. Devices:> 2019-03-29 19:22:37.250458: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5> 2019-03-29 19:22:37.271245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz> 2019-03-29 19:22:37.271958: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3aa2750 executing computations on platform Host. Devices:> 2019-03-29 19:22:37.271972: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>> 2019-03-29 19:22:37.272241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: > name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.44> pciBusID: 0000:01:00.0> totalMemory: 7.77GiB freeMemory: 6.80GiB> 2019-03-29 19:22:37.272275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0> 2019-03-29 19:22:37.273295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:> 2019-03-29 19:22:37.273308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 > 2019-03-29 19:22:37.273314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N > 2019-03-29 19:22:37.273435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6612 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)> 2019-03-29 19:22:38.761993: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR> 2019-03-29 19:22:38.763178: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR> /home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/engine.js:132>             throw ex,>             ^> > Error: Invalid TF_Status: 2> Message: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.>     at NodeJSKernelBackend.executeSingleOutput (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:192:43)>     at NodeJSKernelBackend.conv2d (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:700:21)>     at environment_1.ENV.engine.runKernel.x (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/ops/conv.js:152:27)>     at /home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/engine.js:171:26>     at Engine.scopedRun (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/engine.js:126:23)>     at Engine.runKernel (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/engine.js:169:14)>     at conv2d_ (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/ops/conv.js:151:40)>     at Object.conv2d (/home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:46:29)>     at /home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-layers/dist/layers/convolutional.js:198:17>     at /home/bobi/Desktop/cudnn/node_modules/@tensorflow/tfjs-core/dist/engine.js:116:22","[""Same error happens even when there is no convolutional layers in model.Models```const actor = () => tf.sequential({    layers: [      tf.layers.inputLayer({inputShape: STATE_SIZE}),      tf.layers.batchNormalization(),      tf.layers.dense({units: ACTION_SIZE*2, activation:'relu'}),      tf.layers.dense({units: ACTION_SIZE, activation:'softmax'}),    ],  }),  const critic = () => {    const stateInput = tf.input({shape: [STATE_SIZE]}),    const actionInput = tf.input({shape: [ACTION_SIZE]}),    const bn = tf.layers.batchNormalization().apply(stateInput),    const d1 = tf.layers.dense({units: ACTION_SIZE*2, activation: 'relu'})      .apply(bn),    const d2 = tf.layers.dense({units: ACTION_SIZE,      activation: 'softmax'}).apply(d1),    const concat = tf.layers.concatenate().apply([d2, actionInput]),    const d3 = tf.layers.dense({units: ACTION_SIZE,       activation: 'relu'}).apply(concat),    const output = tf.layers.dense({units: 1}).apply(d3),    return tf.model({inputs: [stateInput, actionInput], outputs: output}),  }```> $ node server/start.js > 2019-04-03 20:26:24.022854: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA> 2019-04-03 20:26:24.151743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero> 2019-04-03 20:26:24.152219: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3ac43c0 executing computations on platform CUDA. Devices:> 2019-04-03 20:26:24.152233: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5> 2019-04-03 20:26:24.171244: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz> 2019-04-03 20:26:24.171685: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3ac2b10 executing computations on platform Host. Devices:> 2019-04-03 20:26:24.171699: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>> 2019-04-03 20:26:24.171843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: > name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.44> pciBusID: 0000:01:00.0> totalMemory: 7.77GiB freeMemory: 6.57GiB> 2019-04-03 20:26:24.171855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0> 2019-04-03 20:26:24.172565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:> 2019-04-03 20:26:24.172575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 > 2019-04-03 20:26:24.172579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N > 2019-04-03 20:26:24.172688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6389 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)> Starting with random weights.> (node:20980) ExperimentalWarning: The fs.promises API is experimental> Listening on 3000> connection> 2019-04-03 20:26:27.335442: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR> 2019-04-03 20:26:27.335505: W ./tensorflow/stream_executor/stream.h:2099] attempting to perform DNN operation using StreamExecutor without DNN support> 2019-04-03 20:26:27.346775: I tensorflow/stream_executor/stream.cc:2079] [stream=0x4a7f370,impl=0x4a7f410] did not wait for [stream=0x4a7ed90,impl=0x4a76260]> 2019-04-03 20:26:27.346799: I tensorflow/stream_executor/stream.cc:5027] [stream=0x4a7f370,impl=0x4a7f410] did not memcpy host-to-device, source: 0x4a02a980> 2019-04-03 20:26:27.346837: F tensorflow/core/common_runtime/gpu/gpu_util.cc:339] CPU->GPU Memcpy failed====="", ""@bobiblazeski, have you found any resolution to this issue?I'm currently blocked by this same error.Ubuntu 18.04GTX 1660, Driver 418.56, CUDA 10.1 (even though I followed the instructions for 10.0...)====="", ""@adwellj  Nope I'm training on CPU until this is resolved.====="", '@bobiblazeski, I punted over to trying on Windows and finally just got this working. I had to drop down to tfjs-node-gpu version 0.3.2 due to node-gyp issues.However, once I finally got it to install, I later ran in to this same CuDNN issue! Fortunately, using CUDA 9.0 (needed for 0.3.2 compatibility) I got a better error message before the ""This is probably because cuDNN failed to initialize..."" message, stating that tfjs-node-gpu was built against CuDNN version 7.2. Once I downloaded that version, everything is working.I haven\'t went back to see if I could get it to work on the LINUX install, but I\'m hoping that this could just be a CuDNN version incompatibility issue that you could experiment with. Luckily CuDNN doesn\'t have an install / uninstall process, it\'s simply copying the extracted files in to a dedicated directory that you include in your system path.I hope that helps give you some possible direction!=====', 'As explained in https://github.com/tensorflow/tfjs/issues/671#issuecomment-494832790There is workaround by setting global variable `export TF_FORCE_GPU_ALLOW_GROWTH=true`=====', ""> > > @adwellj Nope I'm training on CPU until this is resolved.have you solve to use GPU ?====="", 'I am having this issue too, but it seems to resolve itself **only when I restart my computer**. This seems rather odd to me. I notice that the issue tends to happen after terminating my application(s) that utilize tfjs.EDIT: I tried adding `TF_FORCE_GPU_ALLOW_GROWTH=true` as an environment variable, and it seemed to have worked briefly, but upon trying to run my program once more, the error started appearing again. =====', 'this seems to be a duplicate of #671 , we will close this and track the issue at one place.Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/5203,tfjs-backend-wasm: Webpack can't resolve 'os' error,2,open,2021-06-09T15:07:05Z,2021-07-01T18:47:05Z,"**System information**- macOS Catalina 10.15.7- TensorFlow.js installed from NPM- TensorFlow.js version: 3.7.0- Node version: 14.17.0- NPM version: 7.16.0**Describe the problem**After installing package `npm i @tensorflow/tfjs-backend-wasm` and adding[ all required settings ](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/starter/webpack/README.md) for Webpack bundler it fails with an error:```ERROR in ./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js 9:26970-26988Module not found: Error: Can't resolve 'os' in '/Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out'BREAKING CHANGE: webpack < 5 used to include polyfills for node.js core modules by default.This is no longer the case. Verify if you need this module and configure a polyfill for it.If you want to include a polyfill, you need to:	- add a fallback 'resolve.fallback: { ""os"": require.resolve(""os-browserify/browser"") }'	- install 'os-browserify'If you don't want to include a polyfill, you can use an empty module like this:	resolve.fallback: { ""os"": false }resolve 'os' in '/Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out'  Parsed request is a module  using description file: /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/package.json (relative path: ./wasm-out)    resolve as module      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/node_modules doesn't exist or is not a directory      looking for modules in /Users/Bob/Node/tensorflow/node_modules        single file module          using description file: /Users/Bob/Node/tensorflow/package.json (relative path: ./node_modules/os)            no extension              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os doesn't exist            .js              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.js doesn't exist            .json              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.json doesn't exist            .wasm              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.wasm doesn't exist        /Users/Bob/Node/tensorflow/node_modules/os doesn't exist      /Users/Bob/Node/node_modules doesn't exist or is not a directory      /Users/Bob/node_modules doesn't exist or is not a directory      /Users/Bob/Documents/node_modules doesn't exist or is not a directory      /Users/Bob/node_modules doesn't exist or is not a directory      /Users/node_modules doesn't exist or is not a directory      /node_modules doesn't exist or is not a directory @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/backend_wasm.js 19:0-85 261:20-52 262:19-42 @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/base.js 19:0-51 20:0-72 20:0-72 20:0-72 20:0-72 24:27-31 25:15-26 @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/index.js 18:0-23 18:0-23 @ ./src/index.js 2:0-39```---**U.P.D.**And the deps of my `package.json` file:```{  ...  ""devDependencies"": {    ""html-webpack-plugin"": ""^5.3.1"",    ""webpack"": ""^5.38.1"",    ""webpack-cli"": ""^4.7.2"",    ""webpack-dev-server"": ""^3.11.2""  },  ""dependencies"": {    ""@tensorflow/tfjs"": ""^3.7.0"",    ""@tensorflow/tfjs-backend-wasm"": ""^3.7.0""  }}```My `webpack.config.js` file:```const HtmlWebpackPlugin = require('html-webpack-plugin'),const path = require('path'),module.exports = {   module: {      rules: [         {            test: /\.wasm$/i,            type: 'javascript/auto',            use: [               {                  loader: 'file-loader',               },            ],         },      ],   },   ...}```My `index.js` file:```import * as tf from '@tensorflow/tfjs',import '@tensorflow/tfjs-backend-wasm',import wasmSimdPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-simd.wasm',import wasmSimdThreadedPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-threaded-simd.wasm',import wasmPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm.wasm',setWasmPaths({  'tfjs-backend-wasm.wasm': wasmPath,  'tfjs-backend-wasm-simd.wasm': wasmSimdPath,  'tfjs-backend-wasm-threaded-simd.wasm': wasmSimdThreadedPath}),...```---Please help.","['see <https://github.com/tensorflow/tfjs/issues/4745> for details.=====', 'Thank you @vladmandic can we close this to track the issue in #4745 ?=====']",1
https://github.com/tensorflow/tfjs/issues/4525,wasm backend returns incorrect values as result,7,closed,2021-01-13T15:11:44Z,2021-02-24T01:10:42Z,"I have a very simple float32 model (link provided in the code below) that takes image with values in range of 0..255 as input and returns a single float32 value.  Works perfectly with `webgl` backend, but values explode using `wasm` backend.  E.g., instead of returning value in range of 1-99, it returns values like 700+.  This is just one more case that looks like incorrect cast inside `wasm` backend, likely related to #4326 and #4311 that I've reported earlier.  simple reproduction code:```jsconst resizeT = tf.image.resizeBilinear(image, [64, 64], false), // image is any picture of a faceconst normalizeT = tf.mul(resizeT, [255.0]),resizeT.dispose(),const model = await tf.loadGraphModel('https://vladmandic.github.io/human/models/age-ssrnet-imdb.json'),const ageT = await model.predict(normalizeT),normalizeT.dispose(),const age = ageT.dataSync(),ageT.dispose(),console.log(age),```Environment: TFJS 2.8.3 on Chrome 87 / Windows 10","['@rthadur @annxingyuan @pyu10055 @lina128 sorry for the spam,  but i have 3 issues filled (#4525, #4326, #4311) for `wasm` backend (with full reproduction code)  that make `wasm` pretty much unusable.  and all likely have the same root cause, just symptoms are different.but none received any updates since they were filed (oldest was 2.5 months ago).=====', 'Hi, VladimirThank you for your patience!I took a look at this and found that: if I disable simd, the result from WASM backend matches the webgl result. If simd is enabled (with or without multi-threading support), the result is wrong, like you mentioned. Our code uses different flags/options to build WASM module for different combinations of simd+multi-threading support. At this point it looks like that there are some potential bugs in xnnpack/simd. I will follow up with my colleagues and will update here if more comes up.Thanks! =====', '@jinjingforever thanks for the update=====', 'BTW we further noticed that the wrong results come from some of the ""div"" nodes. With SIMD enabled, those nodes produce the results with the numerator and the denominator swapped.. I am asking folks from the xnnpack project to investigate further. Thanks!=====', ""@jinjingforever any update from xnnpack team?i've chedked xnnpack git repository and there is nothing simmilar mentioned there.====="", '(sorry, I was on vacation last week)Today we figured out that the error was caused by out-of-date xnnpack+emscripten. I am in the process of updating our toolchain to the latest xnnpack and emscripten. Hopefully it can be done this week. Thanks!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4525"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4525"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2750,missing binding error,2,closed,2020-02-13T07:31:14Z,2020-02-27T18:19:24Z," [`tensorflow.js`]#### TensorFlow.js version1.5.2#### Browser versionChrome 80.0.3987.100#### Describe the problem or feature requestafter installing tfjs, while runningError: Cannot find module ..../node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.nodefolder is empty![Screenshot 2020-02-13 at 3 23 37 PM](https://user-images.githubusercontent.com/4047303/74410769-d7af8a80-4e74-11ea-9f94-fbbfc3739610.png)```/project_folder/node_modules/@tensorflow/tfjs-node/dist/index.js:1Error: Cannot find module '/project_folder/node_modules/@tensorflow/tfjs-node/lib/napi-v4/tfjs_binding.node'Require stack:- /project_folder/node_modules/@tensorflow/tfjs-node/dist/index.js- /project_folder/profile-maker.js- /project_folder/server.js- /project_folder/tests/routes/user-routes.js- /project_folder/node_modules/mocha/lib/mocha.js- /project_folder/node_modules/mocha/lib/cli/one-and-dones.js- /project_folder/node_modules/mocha/lib/cli/options.js- /project_folder/node_modules/mocha/lib/cli/cli.js    at Object.<anonymous> (/project_folder/node_modules/@tensorflow/tfjs-node/dist/index.js:46:16)    at Generator.next (<anonymous>)    at Object.<anonymous> (/project_folder/profile-maker.js:1)    at Generator.next (<anonymous>)    at Object.<anonymous> (/project_folder/server.js:1)    at Generator.next (<anonymous>)    at Object.<anonymous> (/project_folder/tests/routes/user-routes.js:1)    at Generator.next (<anonymous>)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:947:10)    at /project_folder/node_modules/mocha/lib/mocha.js:334:36    at Array.forEach (<anonymous>)    at Mocha.loadFiles (/project_folder/node_modules/mocha/lib/mocha.js:331:14)    at Mocha.run (/project_folder/node_modules/mocha/lib/mocha.js:809:10)    at Object.exports.singleRun (/project_folder/node_modules/mocha/lib/cli/run-helpers.js:108:16)    at exports.runMocha (/project_folder/node_modules/mocha/lib/cli/run-helpers.js:142:13)    at Object.exports.handler (/project_folder/node_modules/mocha/lib/cli/run.js:292:3)    at Object.runCommand (/project_folder/node_modules/yargs/lib/command.js:242:26)    at Object.parseArgs [as _parseArgs] (/project_folder/node_modules/yargs/yargs.js:1087:28)    at Object.parse (/project_folder/node_modules/yargs/yargs.js:566:25)    at Object.exports.main (/project_folder/node_modules/mocha/lib/cli/cli.js:68:6)    at Object.<anonymous> (/project_folder/node_modules/mocha/lib/cli/cli.js:73:11)    at Generator.next (<anonymous>)    at internal/main/run_main_module.js:17:11 {  code: 'MODULE_NOT_FOUND',  requireStack: [    '/project_folder/node_modules/@tensorflow/tfjs-node/dist/index.js',    '/project_folder/profile-maker.js',    '/project_folder/server.js',    '/project_folder/tests/routes/user-routes.js',    '/project_folder/node_modules/mocha/lib/mocha.js',    '/project_folder/node_modules/mocha/lib/cli/one-and-dones.js',    '/project_folder/node_modules/mocha/lib/cli/options.js',    '/project_folder/node_modules/mocha/lib/cli/cli.js'  ]}```#### Code to reproduce the bug / link to feature requestInstall via yarnOSX 10.14.6node v12.9.1yarn 1.21.1D iu, [13.02.20 14:32]node v12.9.1D iu, [13.02.20 14:32]yarn 1.21.1D iu, [13.02.20 14:44][ Photo ]Note to the issue:a) in package.json we did import dependencies ""@tensorflow/tfjs-node"": ""^1.5.1"",b) it only happens on some of the dev machinesthanks!","['@kennyviperhk thanks for reporting , can you please try these things 1. remove dependency of @tensorflow/tfjs in package.json. tfjs-node will install @tensorflow/tfjs with corresponding version.2. use the exact version of @tensorflow/tfjs-node (remove ^ before the version number). Delete node_modules, yarn.lock and package-lock.json, then install dependencies.=====', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/684,ValueError: Unsupported Ops in the model before optimization LogSoftmax,3,closed,2018-09-06T13:32:33Z,2018-11-08T00:24:35Z,"Hi, I'm trying to convert a .pb file using```tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='LogSoftmax,concat_40,concat_41' --saved_model_tags=serve test.pb testout/test```and get an error```Using TensorFlow backend.Traceback (most recent call last):  File ""/home/foo/opt/anaconda5/envs/nlp-onnx/bin/tensorflowjs_converter"", line 11, in <module>    sys.exit(main())  File ""/home/foo/opt/anaconda5/envs/nlp-onnx/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 272, in main    strip_debug_ops=FLAGS.strip_debug_ops)  File ""/home/foo/opt/anaconda5/envs/nlp-onnx/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 325, in convert_tf_frozen_model    skip_op_check=skip_op_check, strip_debug_ops=strip_debug_ops)  File ""/home/foo/opt/anaconda5/envs/nlp-onnx/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 113, in optimize_graph    ', '.join(unsupported))ValueError: Unsupported Ops in the model before optimizationLogSoftmax```tensorflow.js version ```Using TensorFlow backend.tensorflowjs 0.5.7Dependency versions:  keras 2.1.6  tensorflow 1.9.0```Is that operation not satisfied? Any way I could add it?Thanks a lot!/ J","['@rumschuettel It would be great f you would like to contribute, please refer to this issue for guidance.tensorflow/tfjs#624=====', 'Worked after using --skip_op_check=SKIP_OP_CHECK flag=====', 'Hi @pyu10055, I open a PR for LogSoftmax op and you can see the [link](https://github.com/tensorflow/tfjs-core/pull/1342) above. Hope you can provide review/comment. Thanks=====']",0
https://github.com/tensorflow/tfjs/issues/5092,moveData doesn't support read data asynchronously,1,open,2021-05-19T04:37:07Z,2021-05-20T05:26:08Z,"When we switch backend, for some global tensors, it needs to call [moveData](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine.ts#L422) to move data from the original backend to the current backend by using [readSync](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine.ts#L425). However, for webgpu backend, it doesn't support read data back synchronously from GPU to CPU. So if the data is already in GPU, and we switch backend from webgpu to other backends. The readSync error will be thrown.We should provide a solution to support moving data asynchronously.","[""@pyu10055 @lina128 @jinjingforever Let's use this issue to track the dataSync problem in moveData, which happens when we switch backend from webgpu to others.=====""]",1
https://github.com/tensorflow/tfjs/issues/180,Tensor indexing like numpy,4,closed,2018-04-12T04:58:20Z,2018-04-13T15:47:43Z,"#### TensorFlow.js version 0.9.0#### Browser version Google Chrome 64.0.3282.186#### Describe the problem or feature requestI am trying to set values to index of tensor but I am not sure which method to use.#### Code to reproduce the bug / link to feature request```shconst a = tf.variable(tf.tensor1d([1,2,3,4])const ix  = tf.tensor1d([1,2])```How can I change/mutate `ix` indices in `a` ? I am unable to find any method.In python tensorflow I can do:```sha = tf.Variable([1,2,3,4])a = a[0].assign(100)with tf.Sesssion() as sess:   sess.run(tf.global_variables_initializer())   print sess.run(a)```This prints `[100, 2, 3, 4]`. I am unable to do this in tensorflow js`a[0]` returns `undefined`","[""You can use [variable.assign](https://js.tensorflow.org/api/0.9.0/#tf.Variable.assign) to assign new values to the variable, this would need to be a new tensor of the same shape with your modified data, as you cannot modify individual parts of a tensor. Tensor's are immutable and Variable is a fairly thin wrapper around that to allow reassignment.====="", 'Also to get your data out of the tensor you should use [.data method](https://js.tensorflow.org/api/0.9.0/#tf.Tensor.data) on tensors.=====', 'A couple more thoughts courtesy of @nsthorat.You could use [.buffer](https://js.tensorflow.org/api/0.9.0/#tf.Tensor.buffer) to convert your variable to a [TensorBuffer](https://js.tensorflow.org/api/0.9.0/#class:TensorBuffer), this is a mutable data structure with some handy set methods. Once the modification is done you would turn it back into a tensor and update your variable. Note that this approach does download the tensor from GPU to CPU memory.To do it all in GPU you could probably use a combination of [.mulStrict](https://js.tensorflow.org/api/0.9.0/#mul) and [.addStrict](https://js.tensorflow.org/api/0.9.0/#add) to mask out the values you want to change from the source tensor and add them from your target tensor. This involves creating the mask manually though.=====', 'Thanks that helps.=====']",0
https://github.com/tensorflow/tfjs/issues/1453,Unknown layer: SpatialDropout2D,3,closed,2019-03-27T10:30:35Z,2020-06-05T05:40:46Z,"The following error appears to me when trying to import a model in tensorflow.jsUncaught (in promise) Error: Unknown layer: SpatialDropout2D. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().It seems like SpatialDropout isn't implemented yet. Is anyone working on it?","['Hi @caisq , are you working on it? I would love to implement SpatialDropout2D layer and noise shape.=====', ""In case it helps anyone with preexisting models: I found this issue because I had trained a model in Keras, used tensorflowjs_converter, and then encountered this error only when trying to infer in JS. In this situation, you can remove the layer in the converted model by simply modifying model.json to remove the layer reference and pull the output of the previous layer to be the input of the next. That way you don't have to do the surgery in Keras as a post-processing step on model saving.====="", 'Hi @caisq , I took a look into this issue, it seems that to implement `SpatialDropout2D`, we need to make `tf.layers.dropout` support `noiseShape` first. While dropout layer only supports default `noiseShape` now (as `dropout` op in `tfjs-core` only support default `noiseShape`), I sent two PRs to make the `dropout` op and `dropout` layer support non-default `noiseShape`. Could you please take a look into these PRs? Thanks!* Make dropout op support non-default noiseShape [tensorflow/tfjs-core#1782](https://github.com/tensorflow/tfjs-core/pull/1782)* Make dropout layer support non-default noiseShape and seed [tensorflow/tfjs-layers#556](https://github.com/tensorflow/tfjs-layers/pull/556)=====']",0
https://github.com/tensorflow/tfjs/issues/5641,wasm backend produces incorrect results for int32 tensors,3,closed,2021-09-19T17:14:31Z,2021-10-17T11:51:46Z,"i was wondering why my app produces results with lower precision when using `wasm` backend and finally narrowed it down to `wasm` handling of **int32** tensors*environment: tfjs 3.9.0 on ubuntu 21.04**note: ive tried wasm with and without simd support*below is a very simple reproduction  (and i suggest to add something like this to automated tests in the future)  note that `tf.sum()` is just easiest way to reproduce, this issue is accross the board in tfjs```jsconst data = Array.from(imageData.data), // in my case data is imageData array so each value is just 0..255, but can be any datasetdata.length = 1024 * 1024 * 4, // lets crop array to specific size, just for testlet sumJS = 0,for (let i = 0, i < data.length, i++) sumJS += data[i],const tI32 = tf.tensor(data, [data.length], 'int32'),const tF32 = tf.tensor(data, [data.length], 'float32'),console.log({ backend: tf.getBackend(), arrayLength: data.length, jsSum: sumJS, tfSumI32: tf.sum(tI32).dataSync()[0], tfSumF32: tf.sum(tF32).dataSync()[0] }),```this is ok output when using `tfjs-node` as both js and tf produce same sum:```js{  backend: 'tensorflow',  arrayLength: 4194304,  jsSum: 1000114465,  tfSumI32: 1000114465,  tfSumF32: 1000114432 // sum of float is slightly off, but close}```but when using `wasm` backend, its completely **broken**:```js{  backend: 'wasm',  arrayLength: 4194304,  jsSum: 1000114465,  tfSumI32: 66326844, // completely broken  tfSumF32: 1023692544 // slightly off as well, but close}```and this is not a case of `int32` overflow as its reproducible with much lower values as well:(it seems that error occur with tensors with more than 64k values)```js {  backend: 'wasm',  arrayLength: 65536,  jsSum: 16042946,  tfSumI32: 16042946, // this is correct  tfSumF32: 16042946}``````js{  backend: 'wasm',  arrayLength: 131072,  jsSum: 31840717,  tfSumI32: 24302074, // this is incorrect!!!  tfSumF32: 31826932}```and those numbers are faaaar off any `int32` limitsand yes, i've checked that tensors get created correctly in both cases by downloading all values from it using `dataSync()` and comparing to original array","[""one more trivial reproduction that shows exactly when errors starts to occure:```jsconst fs = require('fs'),const tf = require('@tensorflow/tfjs'),const wasm = require('@tensorflow/tfjs-backend-wasm'),async function main() {  wasm.setWasmPaths('node_modules/@tensorflow/tfjs-backend-wasm/dist/'),  await tf.setBackend('wasm'),  await tf.ready(),  console.log('tfjs:', { version: tf.version_core, backend: tf.getBackend() }),  const t = {},  const data = fs.readFileSync('dist/tfjs.esm.js.map'),  for (let i = 0, i <= 22, i++) {    const arr = Array.from(data),    const size = 2 ** i,    arr.length = size,    t.i32 = tf.tensor(arr, [size], 'int32'),    t.f32 = tf.tensor(arr, [size], 'float32'),    t.sumI = tf.sum(t.i32),    t.sumF = tf.sum(t.f32),    const JS = arr.reduce((prev, curr) => prev += curr, 0),    const I32 = t.sumI.dataSync()[0],    const F32 = t.sumF.dataSync()[0],    console.log({ size, JS, I32, F32, ok: JS === I32 }),    Object.keys(t).forEach((tensor) => tf.dispose(t[tensor])),  }}main(),```output:```jstfjs: { version: '3.9.0', backend: 'wasm' }{ size: 1, JS: 123, I32: 123, F32: 123, ok: true }{ size: 2, JS: 133, I32: 133, F32: 133, ok: true }{ size: 4, JS: 197, I32: 197, F32: 197, ok: true }{ size: 8, JS: 564, I32: 564, F32: 564, ok: true }{ size: 16, JS: 1180, I32: 1180, F32: 1180, ok: true }{ size: 32, JS: 2319, I32: 2319, F32: 2319, ok: true }{ size: 64, JS: 5041, I32: 5041, F32: 5041, ok: true }{ size: 128, JS: 10828, I32: 10828, F32: 10828, ok: true }{ size: 256, JS: 22156, I32: 22156, F32: 22156, ok: true }{ size: 512, JS: 45456, I32: 45456, F32: 45456, ok: true }{ size: 1024, JS: 91536, I32: 91536, F32: 91536, ok: true }{ size: 2048, JS: 184851, I32: 184851, F32: 184851, ok: true }{ size: 4096, JS: 371489, I32: 371489, F32: 371489, ok: true }{ size: 8192, JS: 742567, I32: 742567, F32: 742567, ok: true }{ size: 16384, JS: 1486349, I32: 1486349, F32: 1486349, ok: true }{ size: 32768, JS: 2999662, I32: 2999662, F32: 2999662, ok: true }{ size: 65536, JS: 6039441, I32: 6039441, F32: 6039441, ok: true }{ size: 131072, JS: 12112329, I32: 12112329, F32: 12112329, ok: true }{ size: 262144, JS: 23178955, I32: 19976980, F32: 23176744, ok: false }{ size: 524288, JS: 46226017, I32: 28329405, F32: 46208756, ok: false }{ size: 1048576, JS: 91071178, I32: 36552479, F32: 91093240, ok: false }{ size: 2097152, JS: 180992048, I32: 44872176, F32: 181083904, ok: false }{ size: 4194304, JS: 358417967, I32: 53091244, F32: 356742528, ok: false }```====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5641"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5641"">No</a>=====', ""@jinjingforever there's a leftover line in the fix:```jsconsole.log('reduceshape', reduceShape),````=====""]",1
https://github.com/tensorflow/tfjs/issues/5291,About the Bug of executeAsync Asynchronous Function,5,open,2021-07-05T03:08:47Z,2021-07-30T14:23:52Z,"@rthadur @lina128I studied the object detection project of tfjs and found that the code of `tf.loadGraphModel(weights).executeAsync(input)` has some bugs when it is called.Bug description:When there is a detected object in the input picture, the program is normal, as shown in the figure.![ksnip_20210705-103113](https://user-images.githubusercontent.com/41098760/124410046-27e60000-dd7c-11eb-8934-4bc6145d336f.png)As can be seen from the above figure, the object has been detected normally, and there is no error message in the chrome console.Let's look at a bug, as shown in the figure below:![ksnip_20210705-103511](https://user-images.githubusercontent.com/41098760/124410277-b6f31800-dd7c-11eb-809d-822f94fc5d5a.png)When I upload a picture without an object to be detected by the detector, the `executeAsync` function will throw an error message and terminate the program at the same time.This happened when I used the camera to detect it. When there is no object to be detected in the first frame or other frames in the camera, an error will be reported and the program will be terminated.##### This is my complete project code:https://drive.google.com/file/d/11uuOqKprEc5Ot30WuWRwwxMbSaDb7Lk-/view?usp=sharing- There is my code and model in this project- Have my profile- Steps for usage:```shell# 1. Unzip# 2. Install dependencies with yarnyarn# 3. Runyarn start```I want the program to not display this kind of error message in pictures without detected objects, and the program can run normally.I tried many exception catching methods, such as try catch, ErrorBoundary tag, etc., but nothing worked. I think it’s not Reactjs's problem, but tfjs's problem.So please experts help me, thank you.","['see #4716 for details (still open)=====', '> see #4716 for details (still open)I have read your question before, but I still want to ask you, is this error because there is no object to be detected by the model in the picture you entered? In other words, is the problem you encountered because executeAsync function cannot handle the returned empty object? Thank you.=====', ""I'm guessing it's about empty return as TFJS is connecting layers, it doesn't necessarily has to be output layer. ====="", 'Thank you @vladmandic , I will this close this issue and track the same at #4716 =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5291"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5291"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5150,Error when trying to use tfjs-node,15,closed,2021-05-31T18:37:41Z,2021-07-02T12:15:20Z,"**System information**- OS Platform Windows 10 1909- TensorFlow.js (@tensorflow/tfjs-node) installed from npm- TensorFlow.js (@tensorflow/tfjs-node) version: 3.6.1- Node.js version: v14.17.0 (x86)**My code**`require('@tensorflow/tfjs-node')`**Error**```    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +    ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: C:\Users\Programmer.exe\Desktop\Stuff\Cafe-Manager\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node --build-addon-from-source' to rebuild the native addon module.If you have problem with building the addon module, please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (C:\Users\Programmer.exe\Desktop\Stuff\Cafe-Manager\node_modules\@tensorflow\tfjs-node\dist\index.js:49:11)```**What I already tried**- Deleting node_modules and reinstalling the module- Upgrading to latest node.js- Adding python 2 to path**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.Also, I tried `npm rebuild @tensorflow/tfjs-node --build-addon-from-source`. The error is attached below.[2021-05-31T18_35_31_293Z-debug.log](https://github.com/tensorflow/tfjs/files/6571816/2021-05-31T18_35_31_293Z-debug.log)","['You need to copy files from  deps/libs to lib/napi-v8, please check for similar issues raised [here](https://github.com/tensorflow/tfjs/issues?q=is%3Aissue+The+Node.js+native+addon+module+%28tfjs_binding.node%29+can+not+be+found+at+path+is%3Aclosed)  , Thank you.=====', 'I copiled tensorflow.dll and tensorflow.lib from deps/lib to lib/napi-v8. Still getting the same error=====', 'did you get chance to check related issues ?=====', 'yeah, none of them worked=====', 'For some reason, after I reinstalled tfjs-node, deps/lib and lib/napi-v8 vanished=====', 'Thank you , closing the issue , feel free to mention@ so that we can reopen if you see any further issues.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5150"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5150"">No</a>=====', '@rthadur Still getting the same error, could you please reopen.=====', ""I'm not sure how to fix this, I looked up all the solutions I could. ====="", 'Can you please list down the steps you were following to install ?=====', '- npm install @tensorflow/tfjs-node - npm install @tensorflow-models/toxicity=====', '@CoolingJam after you installed @tensorflow/tfjs-node and cd into `node_modules/@tensorflow/tfjs-node`,can you share the log from running `yarn build-addon-from-source`?=====', 'I dont have yarn, but I attached an log of `npm rebuild @tensorflow/tfjs-node --build-addon-from-source`.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5150"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5150"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4964,Browser hangs during model loading,3,closed,2021-04-21T15:40:33Z,2021-04-21T17:21:28Z,"While initalizing `new Holistic(...)` model, browser hangs for 10+ seconds with following output```holistic_solution_wasm_bin.js:9 I0421 21:07:14.783000       1 gl_context_webgl.cc:149] Successfully created a WebGL context with major version 3 and handle 3put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 I0421 21:07:14.786000       1 gl_context.cc:348] GL version: 3.0 (OpenGL ES 3.0 (WebGL 2.0 (OpenGL ES 3.0 Chromium)))put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 W0421 21:07:14.790000       1 gl_context.cc:802] Drishti OpenGL error checking is disabledput_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 ERROR: Following operations are not supported by GPU delegate:put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 DEQUANTIZE: put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 577 operations will run on the GPU, and the remaining 0 operations will run on the CPU.put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 ERROR: Following operations are not supported by GPU delegate:put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 DEQUANTIZE: put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 164 operations will run on the GPU, and the remaining 0 operations will run on the CPU.```Is there any way to load the model without making the browser hang?Thanks,Rakesh","['In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks! =====', 'Hi, I used js solution api fromhttps://google.github.io/mediapipe/solutions/holistic.html with zerochanges.On Wed 21 Apr, 2021, 21:54 Rajeshwar Reddy T, ***@***.***>wrote:> In order to expedite the trouble-shooting process, please provide a code> snippet to reproduce the issue reported here. Thanks!>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4964#issuecomment-824192926>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AK4GSXOMMKANTP6JLGHGQA3TJ334XANCNFSM43KTU2IA>> .>=====', 'This issue better asked in this repo https://github.com/google/mediapipe/issues it is not related to tfjs directly.=====']",1
https://github.com/tensorflow/tfjs/issues/4310,Error: The Node.js native addon module (tfjs_binding.node) can not be found,6,closed,2020-11-26T12:00:50Z,2020-12-02T17:39:58Z,"I have installed TFJS Node GPU , it gets stucked at end , i tried this mutlipe of times .![bug](https://user-images.githubusercontent.com/45932883/100348141-83912380-300c-11eb-9ff7-5335bb329149.PNG) i tried reinstalling , rebuilding , looked for similar issues but no replay contained solution.now I can 't compile with GPU backend, For some reason CPU backend works.following error occurs every time I try to compile with it with GPU with following import```jsconst tf=require(""@tensorflow/tfjs-node-gpu"")``````Node version 15.0.1``` ```C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\dist\index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +    ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v6\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source' to rebuild the native addon module.If you have problem with building the addon module, please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\dist\index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1083:30)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1112:10)    at Module.load (node:internal/modules/cjs/loader:948:32)    at Function.Module._load (node:internal/modules/cjs/loader:789:14)    at Module.require (node:internal/modules/cjs/loader:972:19)    at require (node:internal/modules/cjs/helpers:88:18)    at Object.<anonymous> (C:\Users\shive\Desktop\Test\nodeapp\index.js:1:10)    at Module._compile (node:internal/modules/cjs/loader:1083:30)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1112:10)npm ERR! code 1npm ERR! path C:\Users\shive\Desktop\Test\nodeappnpm ERR! command failednpm ERR! command C:\Windows\system32\cmd.exe /d /s /c ""node index.js""npm ERR! A complete log of this run can be found in:npm ERR!     C:\Users\shive\AppData\Local\npm-cache\_logs\2020-11-26T11_46_53_847Z-debug.log```","['Can you please try delete node-modules folder and rebuild ?=====', 'I tried reinstalling nothing worked ,Tried cleaning modules nothing worked,Tried creating new node project nothing worked,Tried Rebuilding with `npm rebuild @tensorflow/tfjs-node-gpu --build-from-source`Then i downgraded to Node 12.20 from v15 now post install script worked but still i am not able to compile to GPU```jstf=require(""@tensorflow/tfjs-node-gpu"")```Not able to import like this, it throws napi error,I will post error in couple of minutes.=====', 'Error```Error: The specified module could not be found.\\\\?\\C:\\Users\\shive\\Desktop\\Test\\nodeapp\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v6\\tfjs_binding.node    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1057:18)    at Module.load (internal/modules/cjs/loader.js:863:32)    at Function.Module._load (internal/modules/cjs/loader.js:708:14)    at Module.require (internal/modules/cjs/loader.js:887:19)    at require (internal/modules/cjs/helpers.js:74:18)    at Object.<anonymous> (C:\\Users\\shive\\Desktop\\Test\\nodeapp\\node_modules\\@tensorflow\\tfjs-node-gpu\\dist\\index.js:58:16)    at Module._compile (internal/modules/cjs/loader.js:999:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)    at Module.load (internal/modules/cjs/loader.js:863:32)    at Function.Module._load (internal/modules/cjs/loader.js:708:14)```=====', 'please take a look at similar issue [here](https://github.com/tensorflow/tfjs/issues/4171) , thank you=====', 'It did .Thanks =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4310"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4310"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5861,[webgl] shape mismatch between texture creation and data upload,1,closed,2021-11-17T04:48:31Z,2021-11-19T19:42:48Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Macosx 15- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): 3.11- TensorFlow.js version (use command below): 3.11- Browser version: latest- Tensorflow.js Converter Version: 3.11**Describe the current behavior**In WebGL uploadToGPU logic, the data is first uploaded to a texture in a dense format.But the texture is created using `createPackedMatrixTexture` method, which is half the size of the request width and height. This results a mismatch of the texture and upload size in the `uploadDenseMatrixToTexture`.This is somehow covered up by the fact that both texture creations are using `texImage2D` method, which creates a mutable texture that its shape/format can be changed later during the data upload.Given texImage2D manipulate mutable textures, it is not as efficient as texStorage2D + texSubImage2D, which will be able to catch the shape mismatch between creation and data upload.**Describe the expected behavior**match up the shape/format between texture creation and data upload.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5861"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5861"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5124,Problem installing tfjs-node and tfjs-node-gpu on Windows,6,closed,2021-05-25T20:20:17Z,2021-05-28T19:09:17Z,"**System information**- OS Platform and Distribution: Windows 10 20H2 (19042.985)- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.6.0- CUDA/cuDNN version: 11.3/8.2**Describe the problem**Whenever I try installing tfjs-node (or tfjs-node-gpu) using npm (or yarn), I get the following error:```npm WARN deprecated node-pre-gyp@0.14.0: Please upgrade to @mapbox/node-pre-gyp: the non-scoped node-pre-gyp package is deprecated and only the @mapbox scoped package will recieve updates in the futurenpm ERR! code 1npm ERR! path C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-nodenpm ERR! command failednpm ERR! command C:\WINDOWS\system32\cmd.exe /d /s /c node scripts/install.jsnpm ERR! CPU-windows-3.6.1.zipnpm ERR! * Downloading libtensorflownpm ERR!npm ERR! C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-node\node_modules\adm-zip\zipFile.js:107npm ERR!                        throw new Error(Utils.Errors.INVALID_FORMAT),npm ERR!                              ^npm ERR!npm ERR! Error: Invalid or unsupported zip format. No END header foundnpm ERR!     at readMainHeader (C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-node\node_modules\adm-zip\zipFile.js:107:10)npm ERR!     at new module.exports (C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-node\node_modules\adm-zip\zipFile.js:19:3)npm ERR!     at new module.exports (C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-node\node_modules\adm-zip\adm-zip.js:20:11)npm ERR!     at WriteStream.<anonymous> (C:\Users\usr\AppData\Roaming\npm\node_modules\@tensorflow\tfjs-node\scripts\resources.js:69:29)npm ERR!     at WriteStream.emit (node:events:365:28)npm ERR!     at emitCloseNT (node:internal/streams/destroy:174:10)npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)npm ERR! A complete log of this run can be found in:npm ERR!     C:\Users\usr\AppData\Local\npm-cache\_logs\2021-05-25T19_59_31_630Z-debug.log```**Provide the exact sequence of commands / steps that you executed before running into the problem**```bashnpm install -g @tensorflow/tfjs-node```","['@ai-machine-42 are you behind firewall, can you access files on google cloud storage?https://storage.googleapis.com/=====', 'I am not sure, but this is what I get when I go to https://storage.googleapis.com/:![image](https://user-images.githubusercontent.com/81181661/119780286-4b42a300-bed2-11eb-8462-90d31d105eeb.png)=====', 'How about this https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html ? can you access ?=====', 'Apparently, my ISP blocks google cloud storage, so I used a VPN to bypass the blocking and now I can access it and the above error is gone. However, now I\'m getting a new error:```npm ERR! code 1npm ERR! path C:\\Users\\usr\\Documents\\repos\\tfjs-test\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! command failednpm ERR! command C:\\WINDOWS\\system32\\cmd.exe /d /s /c node scripts/install.jsnpm ERR! CPU-windows-3.6.1.zipnpm ERR! * Downloading libtensorflownpm ERR! node:events:342npm ERR!       throw er, // Unhandled \'error\' eventnpm ERR!       ^npm ERR!npm ERR! Error: read ECONNRESETnpm ERR!     at TLSWrap.onStreamRead (node:internal/stream_base_commons:211:20)npm ERR! Emitted \'error\' event on ClientRequest instance at:npm ERR!     at TLSSocket.socketErrorListener (node:_http_client:447:9)npm ERR!     at TLSSocket.emit (node:events:365:28)npm ERR!     at emitErrorNT (node:internal/streams/destroy:193:8)npm ERR!     at emitErrorCloseNT (node:internal/streams/destroy:158:3)npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:83:21) {npm ERR!   errno: -4077,npm ERR!   code: \'ECONNRESET\',npm ERR!   syscall: \'read\'npm ERR! }```and now I\'m stuck here.I\'ve tried all the following but none helped:* `npm config set registry ""http://registry.npmjs.org/""` (using http instead of https)* `npm config set strict-ssl false`* using powershell as adminstrator=====', 'This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5124"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5124"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4309,Facemesh/Face-Landmarks-Detection model crashes on WebGL Backend,3,open,2020-11-26T09:45:39Z,2021-04-26T14:02:16Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):iOS 14.2.1- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone 12 Pro Max, iPhone 11 Pro Max- TensorFlow.js installed from (npm or script link):https://unpkg.com/@tensorflow/tfjs-core@2.7.0/dist/tf-core.jshttps://unpkg.com/@tensorflow/tfjs-converter@2.7.0/dist/tf-converter.jshttps://unpkg.com/@tensorflow/tfjs-backend-webgl@2.7.0/dist/tf-backend-webgl.jshttps://unpkg.com/@tensorflow-models/face-landmarks-detection@0.0.2/dist/face-landmarks-detection.js- TensorFlow.js version (use command below):- Browser version: Safari 14.0- Tensorflow.js Converter Version: 2.7.0**Describe the current behavior**Page refreshes after approx 15-18min of usage or 18,000 requestAnimationFrames. Perhaps this is a memory leak within the Face-landmarks-detection library?**Describe the expected behavior**Page does not crash when using WASM backend with Face-landmarks-detection.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.https://glitch.com/edit/#!/facemesh-bug**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Any update on this issue? I have since updated to version 2.8.4 with the same results. Is #4405 still relevant to this issue? Or perhaps #4407 ? =====', '@rthadur could you please look again on that issue? We have problems with it too =( =====', '@pyu10055 It will be great if you could check it too =)=====']",1
https://github.com/tensorflow/tfjs/issues/4907,Difference between First Run on Benchmark Run and In Code,22,open,2021-04-07T03:53:28Z,2021-04-16T23:59:23Z,"When I run a model for first time, it is slow. This is expected. However, the difference in times I get for the first time is way different in benchmarking setup(https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html) and my own custom setup https://github.com/rohanmuplara/tester/tree/test_tfjs/graph I see the time difference for warmup inference differ between 200ms and 2000ms. like 10 to 20x difference. I observe this for almost all my models.  I have tried to record a screencast of exactly steps that I followed https://share.descript.com/view/4UBmlV8jmqd. All these experiments were on chrome and on mac with newest tfjs build","['I have simplified it even more. I am using tfhub mobilenet and basic predict https://github.com/rohanmuplara/tester/tree/test_tfjs2/graph and am noticing 10x difference.I was wondering any suggestions how to reduce this warmup time. Also, when does this warmup copying to the gpu terminate. From my observation, reloading the page causes it to be slow again. Let say I have an iframe same domain on many pages. Is there a way to take advantage of this prerunning one page when another users switches to another page but the same iframe will it work? Is there a way to cache stuff in gpu?Thanks,Rohan=====', 'Sometimes `async` is an interesting source of non-determinism. I also noticed that it appears you are getting the Tensor but never actually getting the data out of it - which is an important step that takes time. What happens if you run the following code for benchmark.js? For me times increased, times were already fairly smooth anyways. (FF87, Win 10)```function runModel(model, tensors, returnTensorReferences ) {    let num_outputs = model.outputs.length,    const predictionsTensor =  model.predict(tensors).dataSync(),    return predictionsTensor,}async function benchmarkInput (model_path, tensors, num_runs) {    console.time(""model loading time""),  let model = await tf.loadGraphModel(model_path, { fromTFHub: true }),  console.timeEnd(""model loading time""),  console.time(""first prediction""),  const predictions = runModel(model, tensors, false),  console.timeEnd(""first prediction""),  let subsequent_times =new Float32Array(num_runs - 1),  for (let i = 0, i < num_runs - 1 , i++) {    let begin= window.performance.now(),    const predictions = runModel(model, tensors, true),    let end= window.performance.now(),    let time = (end-begin) ,    subsequent_times[i] = time,  }  console.log(""subsequent predictions are in ms"", subsequent_times),  console.log(""the average of the subequent predictions are"", average(subsequent_times)),}function average(array) {    let average = array.reduce((a, b) => a + b) / array.length,    return average,}function benchmarkInputDefininedInCode() {    let tensor1 = tf.ones([224, 224,3]),    tensor1 = tensor1.expandDims(0),    benchmarkInput(""https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/feature_vector/2/default/1"", [tensor1], 100),}benchmarkInputDefininedInCode(),```=====', ""@ wingman-jr-addon. Totally get your point about datasync. If you look at first post above https://github.com/tensorflow/tfjs/issues/4907#issue-851978678, there is a more complicated setup where I do do datasync. I just didn't do datasync above to make it easier, ie is syncing time causing delays. I do agree subsequent iterations are really fast and I tried your setup above and doesn't really make a difference.I do agree that subsequent times are good and only really considering the first time.   The problem for me is the first iteration time.  It is (10 to 20 more times slower than subequent runs).  My two questions are a. https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html this is way slower(10-20x)  than I get in benchmarking tool for first run.  Subsequent runs in my setup are relative same time as benchmarking tool but the first one don't agree. and b.  I was wondering any suggestions how to reduce this warmup time. Also, when does this warmup copying to the gpu terminate. From my observation, reloading the page causes it to be slow again. Let say I have an iframe same domain on many pages. Is there a way to take advantage of this prerunning one page when another users switches to another page but the same iframe will it work? Is there a way to cache stuff in gpu?====="", '@rohanmuplara  Ah, well if you\'re wondering about the specifically why the first inference is so slow, that was discussed over in #1715 - basically the shaders are compiling if you\'re using WebGL the first time through.  As a side note: try the WASM backend as a comparison once, but be aware performance differences on backends vary greatly from machine to machine. As an example, WASM is much slower on my machine the but first inference has very little penalty.Regarding caching on the GPU ... Well, maybe if you were able to do some type of communication between web pages and indicated that one was the ""server"" and the others decided to become ""clients"" when they opened and detected there was already a ""server"" present.=====', ""@wingman-jr-addon My question is twofold a. what is the difference between the benchmark tool and my setup The problem for me is the first iteration time. It is (10 to 20 more times slower than subequent runs). My two questions are a. https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html this is way slower(10-20x) than I get in benchmarking tool for first run. Subsequent runs in my setup are relative same time as benchmarking tool but the first one don't agree. on b.  I do agree first inference on wasm is quicker and than much slower for subsequent runs.  My question is what the default behavior of tfjs in terms of reloading the page. Additionally, in tfjs, is there a way to cache the shaders compilation or let the tfjs code know that the tfjs shaders are already present. ====="", '@rohanmuplara I see, sorry for being dense.Well, I may be able to solve part of your mystery but not all of it. If I\'m not mistaken, the benchmark may actually be doing a prediction inside the model load itself. From https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html :```    async function loadModelAndRecordTime() {      updateStateFromURLState(),      const benchmark = benchmarks[state.benchmark],      state.modelType = benchmark[\'type\'],      state.isModelChanged = false, // used to clean the performance history      if (benchmark[\'load\'] == null) {        throw new Error(`Please provide a load method for \'${state.benchmark}\' model.`),      }      await showMsg(\'Loading the model\'),      let start = performance.now(),      const inputSize = parseFloat(state.inputSize),      model = await benchmark.load(inputSize, state.architecture, state.inputType),      state.inputs = [],      if (model.inputs) {        // construct the input state for the model        for (let modelInputIndex = 0, modelInputIndex < model.inputs.length, modelInputIndex++) {          let modelInput = model.inputs[modelInputIndex],          if (modelInput.shape == null) continue,          let shape = modelInput.shape.map(e => e == null ? -1 : e),          state.inputs.push({            name: modelInput.name,            shape: [...shape],            dtype: modelInput.dtype,            range: [0, 1000]          }),        }      }      predict = benchmark.predictFunc(inputSize),      const elapsed = performance.now() - start,      await showMsg(null),      appendRow(timeTable, \'Model load\', printTime(elapsed)),    }```Now looking at your video I see that the model load time is still too small to account for this, and I see similar for my own models (<200ms). Notably, it also does not match my experience in using the model in my own context either: I expected load plus first inference times of >10 seconds.I\'m pretty sure this is an artifact of the benchmark tool somehow because if I load the standard `mobilenet_v2` benchmark the times are much longer than `custom` despite the model being ""smaller"" than my model. Do you experience an unexpected difference in timing between your custom benchmark and `mobilenet_v2`? I\'m not sure how big your model is supposed to be.One more thing I find that makes me suspicious of the benchmark. I watched your video closely and while the warmup time is ~150ms, it actually looks like it was working from about 1:49 to 1:53, which would be consistent with your ~3500ms in your own code. So... I think the benchmark may have a bug?=====', ""Hey man no need to apologize. I am super appreciative of all your help! Yes in the benchmark tool I get this discrepancy every time. I used a custom model (have tried at least 20.)  and have observed this error in all of them.  I think you are correct that line shouldn't be there or more documentation be around it. I think the problem is there is no await for that prediction. Please see this video https://share.descript.com/view/0Fqye7z8hGW where I try to explain the bug. I am very unsure to be clear.So I just want to be clear in my own javascript setup(non-benchmark tool) this is the expected behavior and this is how long it is supposed to take. Is there any good workaround for webgl. Is there a way to preserve the textures in gpu(caching whatever) after page reload or a new page with same iframe enabled on both tabs. I will be frank I don't much about this so any help here would be appreciated about how tfjs behaves and how to optimize this====="", ""@rohanmuplaraNo idea why benchmarking tool would result such a huge difference on first inference other than minor difference that:a) benchmark tool uses `model.executeAsyc()` for custom graph-based models and `model.predict()` only for layers-based models     while it uses `model.predict()` for all predefined modelsb) benchmark tool uses `tf.randomNormal()` to setup input tensor (vs your use of `tf.ones()`)On the topic of GL shader caching, this gets tricky:- TFJS maintains `numBytesAllocated` and if this is higher than threshold,    it simply calls GL function `unbindColorTextureFromFramebuffer`    However:  - Default threshold is infinite by default      (see `WEBGL_FLUSH_THRESHOLD` and `WEBGL_DELETE_TEXTURE_THRESHOLD`)      So that's not an issue (unless you're low on GPU memory)    - TFJS cannot maintain allocation table between page refreshes or between different iframes      There is simply no way to do it in JS due to page isolation enforced by all modern browsers      Just check `tf.engine().state.numBytes`, `tf.engine().backendInstance.numBytesInGPU` and `tf.engine().backendInstance.textureManager.numBytesAllocated`      values on page refresh - it's all zeroes- After `unbind`, it's up to the browser to perform actual GPU memory garbage collection    I've looked around and there is no way to tune it    (and different browsers do behave differently, for example Firefox is much more aggressive than Chrome)Now, one way that comes to mind would be to try save entire state of the `tf.engine().backendInstance.textureManager`  (bit more complicated as most values are read-only and would need to access them on a lower level)  to somewhere like LocalStorage in browser and upon page load to restore it - but that would get very messy very fast  And there is still no guarantees when browser's garbage collection would kick in as there would always be a delay between page load and before state is restored, so should probably download all GL textures, save them as well and restore them on load.====="", ""@vladmandic thanks for all your points again. not sure if you care about the details but I **think not sure ** issue is https://share.descript.com/view/0Fqye7z8hGW. the load model calls (predictFunction) with no await and in custom models these are async (so they return once executeSync is called not returned) so this time doesn't get accounted for. On the point of optimization, I get all your points about multiple tabs and not having reference.  I don't 1. On the textureManager suggestion writing to disk, is time consuming part creating the gl textures from the model or is it copying to the gpu(this impacts whether writing to local storage is helpful.) 2. Could we maybe precompute these ie maybe there is a few differences per browser, but precompute this beforehand offline and load this directly from cloud storage. 3. changes required to tfjs: In addition to making some of these writeonly, I guess some code would have to be changed in tfjs to not reallocate and compile everything to the gpu when calling predict for the first time and to use this texture manager. . ====="", '`predictFunction` has variable definition:- for predefined models it\'s simply `model.predict()` which is a sync call- for custom models, if model is of type layers, it\'s also just `model.predict()`- for custom models, if model is of type graph, it wraps it in a try/catch block trying `model.execute()` (which is also a sync function) first and `await model.executeAsync()` in catch block if model fails because it has any async ops  (which does mean that for models that have async ops, it will try twice and totally skew results.)Regarding ""optimizations"", yes, I think it would be much faster to store them to disk and reload them - but the scope is massive, it would pretty much become a new type of a precompiled model  Need to download all precompiled shaders and textures after the first run and then restore them as part of model loading. And not just GL save/restore, but also make sure that TFJS state (textureManager) is valid.Why do I think it would be much faster?a) initial compile of shaders is time consumingb) uploading unchanged parts of textures is very time consuming    (just try setting WEBGL_DELETE_TEXTURE_THRESHOLD=0 so textures are deallocated on each frame and see the difference)Also note that a shader code has a lot of conditional statements that test GPU GL capabilities and use appropriate functions accordingly, so such ""precompiled"" model would not be a generic WebGL model and would only work on newer GPUs (and on newer browsers and with modern drivers)I love the idea, but I think the scope is just massive...If I have to look forward to something, it would be the `WebGPU` backend - currently it\'s in early stages of development (and it only works on debug versions of browsers), but it could help significantly in the future.=====', ""Yes I agree with everything with you're saying.   As you mentioned, in custom graph models is as async functions because it is calling executeAsync.  Although there is an await within this function, there is no await outside it I think main culprit is this line   https://github.com/tensorflow/tfjs/blob/38f8462fe642011ff1b7bcbb52e018f3451be58b/e2e/benchmarks/local-benchmark/index.html#L469. So in the case of custom graph models because it is an async function(using await internally) and there is no await outside it behaves unexpectedly.Your point on the webgpu makes  sesnse====="", '`predictFunc` would not return `timeInfo` without await - see :```jstimeInfo = await timeInference(() => predict(model), numRuns),```where `predict` is:```js    const start = performance.now(),    const res = await predict(),    const elapsedTime = performance.now() - start,    times.push(elapsedTime),```where this `predict` is a conditional function that uses `model.predict()` or `model.execute()` or `model.executeAsync()`also, initial inference time is just `timeInfo.times[0]`, there is no other handling for it.=====', ""Sure I think the point is that predict is called in the load function itself so the warmup time is actually the second time it is called which is a little unintuitive. In this call, there is no await used so for custom models this time of the first run isn't even counted in the load function. Ihttps://github.com/tensorflow/tfjs/blob/38f8462fe642011ff1b7bcbb52e018f3451be58b/e2e/benchmarks/local-benchmark/index.html#L469====="", ""possibly - i never relied on that e2e benchmark other when tfjs staff asked me to run it - i prefer to run my own benchmarks  anyhow, the core here is slow initial inference when using `webgl` backend and we've covered that  ====="", '@rohanmuplara @vladmandic @wingman-jr-addon great discussion here, seems the inference time measurements are bit confusing here. I Agree we should not pack the first inference into the model loading and there might be a bug in measuring as well. We will address those.=====', '@vladmandic @pyu10055 @wingman-jr-addon thanks for all your help!=====', ""@vladmandic @pyu10055 @wingman-jr-addon https://share.descript.com/view/YT5OGQnajc3. To be clear, I only care about first time inference speed.  I noticed if I have two models with same architectures, the second one's (first time) inference is really fast. I was wondering why that happened and how to reason about it. I believe that they have some weights that are similar and some weights that are different. I have a pipeline of 5 models so was wondering stuff such as if they have same exact architectures/different weights does it help with first time inference speed.  https://github.com/rohanmuplara/tester/blob/tfjs_weird/graph/stitched.js====="", ""I was pretty sure I'd read someplace that the shader compilation happens the first time through. If so, I would expect that the weights are only data rather than a difference in shader, so that would align with your experience. I'm not familiar with the guts though.As an experiment, I suspect that with the WASM backend you will likely not see as much of a difference between model runs.====="", '@wingman-jr-addon I am not too interested in wasm backend as it is significantly slower. So my question on your response is a. I had two different model references so somehow either tfjs or gpu have to figure out that these are the same b. when does it do this, ie if models have on more layer that is different? like how exactly same do the architectures have to be, trying to get intuition on this.  Who would be best person to ping about this?=====', ""it's not per-model or per-layer, it's per kernel op - each kernel op is compiled into a shader.  so there is a lot that can be reused between models  and regarding weights, note that although that bin files may look very different, a lot of models start in the same place (same base weights which are only added on during training)  so if models are based on the same architecture (e.g. mobilenet v2 being extremely common starting point), they likely share >50% of weights as well  ====="", '@vladmandic quick follow up. a. who does this kernel op duplication check is it it tfjs code/ or gpu? b. totally agree that weights are very similar, ie the architecture I used had was 99% mobilenet which by default loads pretrained weights,  I was wondering if the weights were different, do you still get savings. If I have a pipeline of 5 models, they will all need to have different weights(they do different things) but if architectures are same do I get savings=====', ""it's not deduplication, it's simple compile of any used op on its first use - then it's registered in tfjs as existing so whoever calls it next, doesn't need to compile it again. that can be second usage of the same op from the same model or different model, doesn't matter. and there is always just one implementation of any given op - just try importing multiple instances of tfjs and you'll get tons of warnings in the console about 'op already registered'regarding weights, they are not a monolithic thing, they are extracted as needed from a weights file to be used by a specific ops.  and when an op is compiled to a gl shader and it it has weights as param, those weights are uploaded as a gl texture and to a gpu. and tfjs maintains a map of uploaded textures. so if a different model has 50% of different weights, but 50% of same weights as they are inherited from a common model, you do get significant saving.to summarize - you get savings on a) compiling ops as shaders (which is irrelevant of weights), b) uploading weights as textures (which do must match)*disclaimer: all this is unofficial and comes from my experience with tfjs*=====""]",1
https://github.com/tensorflow/tfjs/issues/5775,tsjs-node: sh: line 1: 18590 Illegal instruction: 4,7,closed,2021-10-27T14:14:49Z,2021-10-28T22:35:47Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6 (Big Sur)- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.10.0- CUDA/cuDNN version: N/A**Describe the problem**Getting error `sh: line 1: 18590 Illegal instruction: 4` when trying to use tfjs-node. Works with tfjs (non-node version).**Provide the exact sequence of commands / steps that you executed before running into the problem**Just initializing instance using `const tf = require('@tensorflow/tfjs-node')`.**Any other info / logs**N/A.","['Please provide codepen example or code snippet for us to reproduce. =====', ""All I did was import tfjs-node `const tf = require('@tensorflow/tfjs-node')`====="", ""my guess this is on Apple's M1 hardware? if so, `tensorflow` shared library does not support it (and thus neither can `tfjs-node`). search in older posts for alternatives - none great, but there are some ways to make it work.====="", 'Yes, this is M1, when will this be supported? Mind looking to one of the older posts?=====', 'Thats a question for underlying `tensorflow` shared library, not `tfjs`  See <https://github.com/tensorflow/tensorflow/issues?q=M1>  =====', 'Thank you @vladmandic , we will track this request at one place here https://github.com/tensorflow/tfjs/issues/4514#issuecomment-953889743 , will close this issue. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5775"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5775"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5409,hub.kerasLayer support,3,closed,2021-07-31T11:02:35Z,2021-08-09T20:50:04Z,"**System information**- TensorFlow.js version (you are using): tfjs@2.0.0- Are you willing to contribute it (Yes/No): Yes, however I do not have in depth knowledge of the inner workings of TF yet. Definitely would be happy to help out in testing.**TensorflowJS converted model throwing “Cannot read property ‘producer’ of undefined” error**[https://discuss.tensorflow.org/t/tensorflowjs-converted-model-throwing-cannot-read-property-producer-of-undefined-error/3132](https://discuss.tensorflow.org/t/tensorflowjs-converted-model-throwing-cannot-read-property-producer-of-undefined-error/3132)The above issue is thrown when trying to load a model converted from:https://www.tensorflow.org/text/tutorials/classify_text_with_bert**Will this change the current api? How?****Who will benefit with this feature?**People training models from TF Hub and converting it to TFJS for browser usage.**Any Other info.**","['please check this tutorial as how to train and convert to tfjs for above model https://alexfi.dev/blog/tensorflowjs-bert-train.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5409"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5409"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5482,fromPixels results are inconsistent for image input,5,closed,2021-08-13T03:01:37Z,2021-11-22T20:25:25Z,"Reproduce steps:  1. Build and run e2e(yarn build-deps, npx-http-server), then load bodypix or posenet, select input type image.   2. Click “Test correctness”, when the test is done,  click it again.   3. Check the results from console, ypu will see the results mismatch.Problem investigation:  This problem is caused by chromium's canvas implementation. It can be workaround with recreate canvas context every time, or turn on chrome://flags/#new-canvas-2d-api. I have filed a bug for chromium here: https://bugs.chromium.org/p/chromium/issues/detail?id=1239467, and WIP fix here: https://chromium-review.googlesource.com/c/chromium/src/+/3092523.Test case:     Besides above reproduce steps, you can also use below single page case to reproduce this issue:```<meta charset=""utf-8""><meta name=""viewport"" content=""width=device-width,initial-scale=1,maximum-scale=1.0, user-scalable=no""><body>    <div id=""info"" style='display:none'></div>    <div id=""predictions""></div>    <video id=""video"" playsinline style=""    -webkit-transform: scaleX(-1),    transform: scaleX(-1),    display: none,    width: auto,    height: auto,    "">    </video>    <canvas id=""output"" style=""""></canvas>    <canvas id=""hand_cut"" style=""""></canvas>    <div id=""status""></div>    <script src=""https://unpkg.com/@tensorflow/tfjs-core@latest/dist/tf-core.js"" crossorigin></script>    <script>        function getURLState(url) {            let params = new URLSearchParams(url),            const keys = [...params.keys()],            if (keys.length === 0) return 0,            if (params.has('recreate')) {                return true,            }            return false,        }        async function loadImage(imagePath) {            const imageBucket =                'https://storage.googleapis.com/tfjs-models/assets/posenet/',            const image = new Image(),            const promise = new Promise((resolve, reject) => {                image.crossOrigin = '',                image.onload = () => {                    resolve(image),                },            }),            image.src = `${imageBucket}${imagePath}`,            return promise,        }        function consoleLogArray(array, printLen = 3, comment = '') {            if (printLen > array.length) {                throw new Error(""Print len is bigger than array.length""),            }            console.log(comment + array.slice(0, printLen).toString()),        }        async function fromPixelWrap(recreateContext = false) {            const image = await loadImage('tennis_standing.jpg'),            const fpImage = tf.browser.fromPixels(image),,            return fpImage,        }        async function testFromPixel() {            const recreateContext = getURLState(window.location.search),            console.log(recreateContext),            const fpImage1 = await fromPixelWrap(recreateContext),            const fpImage2 = await fromPixelWrap(recreateContext),            consoleLogArray(await fpImage1.data()),            consoleLogArray(await fpImage2.data()),            tf.test_util.expectArraysClose(await fpImage1.data(), await fpImage2.data()),        }        const bindPage = async () => {            await testFromPixel(),        }        bindPage(),    </script></body>```<html><body><!--StartFragment-->Google Chrome | 94.0.4605.0 (Official Build) canary (64-bit) (cohort: Clang-64)-- | --Revision | a03befa72fc2a3d296e323116474694029ca21aa-refs/branch-heads/4605@{#1}OS | Windows 10 OS Version 2004 (Build 19041.1110)JavaScript | V8 9.4.146<!--EndFragment--></body></html>","['Hi @axinging, will clearRect() before drawing another image help?=====', '@lina128 , indeed I have tried the clearRect, not work. Currently the workarounds are : recreate canvas context every time, or turn on chrome://flags/#new-canvas-2d-api.=====', '@axinging Will adopt the recreate canvas context every time for now and mitigate to new API when it is released based on https://bugs.chromium.org/p/chromium/issues/detail?id=1239467#c5=====', ""Hi, we tested this behavior on three browsers. Safari and Firefox don't have this issue, the first draw are both on GPU. Only Chrome has this issue. For Chrome, we explored three methods, all of them have some caveats. (See this PR for detail: https://github.com/tensorflow/tfjs/pull/5864) Method 1 is the most promising, but the problem is that it is still not very reliable. In some environments, the first few drawImage calls happen in CPU (e.g. if they are in the same scope in JS), so drawing one more time still doesn't work in these cases. Also this approach will incur duplicate fromPixel call to draw, download and upload data. We think what you observed most likely only happens in test, and in reality it's unlikely someone wants to call fromPixels on the same image twice. But it's a nice finding, we can add a warning in this method's doccomment, so that people are aware of this, and if they really want the two calls to yield exactly same result, they can call fromPixels three times and only use the last two results for comparison. The same applies to your test. We can change the benchmark test logic so that it creates tensor from a data array (basically ImageData) that represents the image. After all, we don't want to compare different rendering engine, we want to compare,whether there's difference between backends.Given that we have a workaround, we prefer not to modify the original behavior, as stated above the fix would add unnecessary computation and is not reliable.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5482"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5482"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5390,Different output in wasm vs webgl backend,1,closed,2021-07-28T10:54:23Z,2021-08-25T17:58:56Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Also on iPhone X- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js- TensorFlow.js version (use command below): Latest (3.8.0)- Browser version: Chrome 92.0.4515.107 (latest)- Tensorflow.js Converter Version: Latest (3.8.0)**Describe the current behavior**On the wasm backend the model output varies radically compared to the webgl output (without no warnings or errors logged). The below screenshot shows output using the wasm backend, which comes from here: https://replit.com/@epi-morphism/tfjs-bug2#index.html![Screenshot (2849)](https://user-images.githubusercontent.com/74825640/127310476-e0f0096e-3b2b-449c-88d9-428f91b0d72b.png)**Describe the expected behavior**Using webgl or cpu as the backend produces the expected proper output, as shown in the screenshot below, which comes from here https://replit.com/@epi-morphism/tfjs-bug#index.html.![Screenshot (2850)](https://user-images.githubusercontent.com/74825640/127310703-0d1d5e4c-c922-4b8e-83ac-7a68864d1930.png)**Standalone code to reproduce the issue**Incorrect output (wasm backend): https://replit.com/@epi-morphism/tfjs-bug2#index.htmlCorrect ouput (webgl or cpu backend): https://replit.com/@epi-morphism/tfjs-bug#index.html","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5390"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5390"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3950,can't convert h5 file to json with tensorflowjs converter,6,closed,2020-09-20T06:56:37Z,2021-03-04T05:52:36Z,"I have downloaded vgg.h5 file and try to convert it. with command tensorflowjs_converter --input_format keras ./vgg16.h5 ./my python version is 3.8 and I also tried 3.7.3 but it's same. tf.__version__ is 2.3.0  what maybe the problem? ```(base) pedrojung@pedroui-MacBookPro-2 ~ % tensorflowjs_converter --input_format keras \                       /Users/pedrojung/vgg16.h5 \                       ./Traceback (most recent call last):  File ""/Users/pedrojung/opt/anaconda3/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 757, in pip_main    main([' '.join(sys.argv[1:])])  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 761, in main    convert(argv[0].split(' '))  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 666, in convert    dispatch_keras_h5_to_tfjs_layers_model_conversion(  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 83, in dispatch_keras_h5_to_tfjs_layers_model_conversion    groups = conversion.h5_weights_to_tfjs_format(  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 233, in h5_weights_to_tfjs_format    _check_version(h5file)  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 100, in _check_version    keras_version = as_text(h5file.attrs['keras_version'])  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper  File ""/Users/pedrojung/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/attrs.py"", line 60, in __getitem__    attr = h5a.open(self._id, self._e(name))  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper  File ""h5py/h5a.pyx"", line 77, in h5py.h5a.openKeyError: ""Can't open attribute (can't locate attribute: 'keras_version')""```","['@kotran88 can you please check for a similar issue [here](https://github.com/tensorflow/tfjs/issues/785), what is the version of tfjs ?=====', 'the most recent one. 2.4.0 =====', 'can you please let me know steps you followed to convert ?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Hi, recently I was using the tensorflowjs_converter to convert VGG16.h5 file. I ran the sollowing script:$  tensorflowjs_converter --input_format=keras /c/Users/aksha/Desktop/vgg16_weights_tf_dim_ordering_tf_kernels.h5 /c/Users/aksha/tensorflow_filesThe output that I got:2021-03-04 11:16:47.747396: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library \'cudart64_110.dll\', dlerror: cudart64_110.dll not found2021-03-04 11:16:47.747743: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 192, in _run_module_as_main    return _run_code(code, main_globals, None,  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 85, in _run_code    exec(code, run_globals)  File ""C:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\tensorflowjs_converter.exe\\__main__.py"", line 7, in <module>  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\converter.py"", line 813, in pip_main    main([\' \'.join(sys.argv[1:])])  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\converter.py"", line 817, in main    convert(argv[0].split(\' \'))  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\converter.py"", line 803, in convert    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\converter.py"", line 496, in _dispatch_converter    dispatch_keras_h5_to_tfjs_layers_model_conversion(  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\converter.py"", line 85, in dispatch_keras_h5_to_tfjs_layers_model_conversion    groups = conversion.h5_weights_to_tfjs_format(  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py"", line 233, in h5_weights_to_tfjs_format    _check_version(h5file)  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py"", line 100, in _check_version    keras_version = as_text(h5file.attrs[\'keras_version\'])  File ""h5py\\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper  File ""h5py\\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper  File ""c:\\users\\aksha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\h5py\\_hl\\attrs.py"", line 60, in __getitem__    attr = h5a.open(self._id, self._e(name))  File ""h5py\\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper  File ""h5py\\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper  File ""h5py\\h5a.pyx"", line 77, in h5py.h5a.openKeyError: ""Can\'t open attribute (can\'t locate attribute: \'keras_version\')""When I ran the same command for MobileNet.h5, I got no error. I\'m not able to understand what exactly is the problem. Please help.Akshay V.R.=====']",0
https://github.com/tensorflow/tfjs/issues/621,Layers - convert executeInternal from recursion to iteration,2,closed,2018-08-20T14:42:23Z,2018-11-20T05:11:11Z,"Profiling a real model that is ~130 layers deep revealed that a significant time is spend on the recursive calls of `executeInternal`, where the call stack increases by 130 levels.To allow execution of arbitrarily deep models, and improve performance, we should convert the `executeInternal` to an iterative approach, by implementing our own stack.","['How complicated would the implementation be?  Keep in mind that models in general are DAGs of layers, potentially with inputs/outputs at multiple depths, not only linear stacks.What state needs to be maintained at each stack level?=====', ""This shouldn't be too difficult even for the general DAG case. A depth-first search with a stack keeping track of the current status should suffice.=====""]",0
https://github.com/tensorflow/tfjs/issues/5906,The output of tf.tensor() is different when dtype is int32 and float32,2,closed,2021-11-30T16:43:05Z,2021-11-30T17:40:56Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):  script link- TensorFlow.js version (use command below): 3.11.0- Browser version:  chrome  95.0.4638.69 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**For uninitialized input array, `tf.tensor(input,""int32"")` returns 0, but `tf.tensor (input, ""float32"")` returns NaN.```    var data=new Array(5),    var i32 = tf.tensor(data, [5], 'int32'),    var f32 = tf.tensor(data, [5], 'float32'),    console.log(i32.arraySync())    console.log(f32.arraySync())```for example, the output of above code is:![image](https://user-images.githubusercontent.com/68681463/144089486-1ee7a962-20ae-42b6-8f24-10a927e60e0a.png)**Describe the expected behavior**For any dtype, the returned result should be the same.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['@liliquan0118 Thank you for reporting this, this seems to be caused by javascript difference between Int32Array and Float32Array:```var data=new Array(5),console.log(new Int32Array(data))VM301:1 Int32Array(5)\xa0[0, 0, 0, 0, 0, buffer: ArrayBuffer(20), byteLength: 20, byteOffset: 0, length: 5]undefinedconsole.log(new Float32Array(data))VM360:1 Float32Array(5)\xa0[NaN, NaN, NaN, NaN, NaN, buffer: ArrayBuffer(20), byteLength: 20, byteOffset: 0, length: 5]```I would recommend not to use uninitialized array, since the behavior of the system is undefined.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5906"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5906"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1528,Arbitrary style transfer is slower in tfjs v1,8,closed,2019-04-18T17:19:32Z,2021-09-10T00:33:43Z,"#### TensorFlow.js versiontfjs 1.0.4#### Browser versionChrome Version 70.0.3538.77 (Official Build) (64-bit)#### Describe the problem or feature requestI decided to try updating my arbitrary style transfer demo to tfjs v1.0.4 from v0.14.1.Unfortunately, it seemed much slower + more memory-hungry. I haven't had the time to do more detailed benchmarks yet but this is a PR with the update to tfv1 if you want to try things out locally or check for errors in my migration. https://github.com/reiinakano/arbitrary-image-stylization-tfjs/pull/23. I haven't touched the original demo with v0.14.1 yet and it's still at https://reiinakano.github.io/arbitrary-image-stylization-tfjs/.","['I wonder if it is related to the performance drop I saw too in #1530 ? Still ongoing investigation but it appears forcing WEBGL_PACK to false may be a suitable workaround.=====', '@jessetrana Thanks for the heads up! I will try it and report back.=====', ""It works! I can confirm it's back to normal (maybe even a tiny bit faster)====="", 'Excellent! If the team feels there is a singular root cause, would it be appropriate to close either this one or #1530 as a duplicate?=====', 'I think leaving this one open might be helpful for now, as it gives us more examples to look at for the impact of WEBGL_PACK.cc @annxingyuan =====', 'And to clarify i meant leaving both of them open for now.=====', 'Closing this issue as this has been fixed based on the comment here https://github.com/tensorflow/tfjs/issues/1530#issuecomment-575655744 , thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/1528"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/1528"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/317,Support non-batched inputs for layers model.predict().,5,open,2018-05-20T17:17:11Z,2019-02-12T15:33:32Z,Right now we expect a batched input when making a prediction.We should allow predicting without a batch dimension (single example).,"['Maybe add an optional parameter batch = true/false ? =====', ""@zaidalyafeai Thanks for the suggestion. I think that won't be as easy-to-use as letting the method determine whether the input is a batch or an individual example automatically. The method should be able to do that based on the shape. One thing I want to explore is whether we can get Keras to make the same change on the Python side as well. @fchollet====="", ""+1 to doing this automatically. No need for an explicit bit when it's pretty easy to infer.====="", 'Maybe instead of supporting non-batched,  single-example input in `predict`, which will involve some ambiguity for models with multiple inputs, we should add a method called `predictOnExample()`=====', 'Can you remind me what the problem with multi-input models is? I still think we should do this as part of `predict`.=====']",0
https://github.com/tensorflow/tfjs/issues/1691,Drop shelljs package from tfjs-node.,0,closed,2019-06-24T19:59:05Z,2019-06-25T02:44:28Z,"There is a security problem with `shelljs` and that package has not been published in 7 months.It looks like we do this only for tensorboard tests - we can do these actions with existing Node.js APIs already:```src/tensorboard_test.ts:24:const shelljs = require('shelljs'),src/tensorboard_test.ts:37:      shelljs.rm('-rf', tmpLogDir),src/tensorboard_test.ts:133:    shelljs.mkdir('-p', tmpLogDir),src/tensorboard_test.ts:156:      shelljs.rm('-rf', tmpLogDir),```",[],0
https://github.com/tensorflow/tfjs/issues/5071,Audio is glitching/distortion using playRawAudio,8,closed,2021-05-14T01:43:08Z,2021-06-11T22:01:25Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- speech-command version: 0.5.4- Browser version: Version 90.0.4430.212 (Official Build) (x86_64)**Describe the current behavior**I'm using basic code from [the example in README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) and the audio doesn't sound right. You can hear glitching/distortion in the audio**Describe the expected behavior**Expected to have audio without glitching/distortion**Standalone code to reproduce the issue**[Example in Codesandbox](https://codesandbox.io/s/tfjs-play-raw-audio-v0y12?file=/src/App.tsx:1100-1112)","[""@lina128 I tried [this solution](https://stackoverflow.com/a/38555027/9340882), but it didn't work..====="", '<!--/* Font Definitions */@font-face\t{font-family:""Cambria Math"",\tpanose-1:2 4 5 3 5 4 6 3 2 4,}@font-face\t{font-family:Calibri,\tpanose-1:2 15 5 2 2 2 4 3 2 4,}/* Style Definitions */p.MsoNormal, li.MsoNormal, div.MsoNormal\t{margin:0in,\tfont-size:11.0pt,\tfont-family:""Calibri"",sans-serif,}a:link, span.MsoHyperlink\t{mso-style-priority:99,\tcolor:blue,\ttext-decoration:underline,}.MsoChpDefault\t{mso-style-type:export-only,}@page WordSection1\t{size:8.5in 11.0in,\tmargin:1.0in 1.0in 1.0in 1.0in,}div.WordSection1\t{page:WordSection1,}-->I would sat stop but its to late now\xa0Sent from Mail for Windows 10\xa0From: adotnusiyanSent: Thursday, May 20, 2021 11:54 PMTo: tensorflow/tfjsCc: SubscribedSubject: Re: [tensorflow/tfjs] Audio is glitching/distortion using playRawAudio ***@***.***I tried this solution, but it didn\'t work..—You are receiving this because you are subscribed to this thread.Reply to this email directly, view it on GitHub, or unsubscribe.\xa0=====', '@lina128 Is there any updates on this issue? Thanks =====', 'Hi, I took a look at this, not sure why audio glitches. This is not a typical use of the API and our model seems run well. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5071"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5071"">No</a>=====', ""@lina128 What's the typical use?I did not do any custom code. It's what you suggested [here](https://github.com/tensorflow/tfjs/issues/4736#issuecomment-802300190). And just like what's written in [README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) ====="", ""I mean playing back the raw audio is not the main purpose of this API. As long as the model is working well, there's nothing to fix.====="", '@lina128 If it\'s not the ""main"" purpose, it doesn\'t mean there\'s ""nothing"" to fix. Maybe it should have a low priority, but not ""closed""! =====']",1
https://github.com/tensorflow/tfjs/issues/332,Prevent users from converting a model that has no topology,7,closed,2018-05-23T17:14:27Z,2020-08-14T19:21:38Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).Hi everyone and thanks for the great work!Following this discussion https://groups.google.com/a/tensorflow.org/d/msg/tfjs/QV3n4NBDi70/pg2gCtdhCAAJ I'm creating this issue.#### Describe the problem or feature requesttensorflowjs_converter does not prevent the user from exporting a keras h5 that only contains the weights (ex: keras save_weights_only=True). I've been able to export my ""weights only"" h5 to the web format, it resulted in a ""modelTopology: null"" in the model.json and an error when calling loadModel() (missing topology).According to @caisq, it is a bug as the converter expects both the topology and the weights to be present.If no one has any objection, I think I will be able to come up with a pull request by the end of this week.","[""Hi again,I must admit I'm a little bit confused. I started digging into the code and here's what I rapidly found: https://github.com/tensorflow/tfjs-converter/blame/master/python/tensorflowjs/converters/converter.py#L144According to this code, a weights-only HDF5 file is one of the expected input format which contradicts @caisq message in the google groups: https://groups.google.com/a/tensorflow.org/d/msg/tfjs/QV3n4NBDi70/pg2gCtdhCAAJEdit: it is also contradictory to this line of code: https://github.com/tensorflow/tfjs-layers/blob/dff535b89b242ef143c08fbbb5c892a92536c853/src/models.ts#L205====="", ""@timotheebernard Sorry - I gave misleading information before. tfjs-converter supports converting a weights-only HDF5 file, as you read correctly from the source code. In the conversion result, model.json will contain `null` in the `modelTopology` field. I meant to say that the JavaScript library currently doesn't support loading such converted files. The JavaScript library has a number of files and methods in which this check should be done, in https://github.com/tensorflow/tfjs-core/tree/master/src/io But I believe that only one of the (browser_files.ts) is actually doing the check. The check should be added to other files, including local_storage.ts, indexed_db.ts and browser_http.ts). In addition, I think a check needs to be added here as well:https://github.com/tensorflow/tfjs-layers/blob/master/src/models.ts#L40====="", 'Hi @caisq, thanks for the precision.So, if my understanding is correct, one should be able to export a model without topology but tfjs should check the presence of the topology for every IO (which is currently not the case).Since the issue name is ""Prevent users from converting a model that has no topology"", do you want me to close it or rename it ""Ensure model has topology for every IO""?=====', 'Using currently available version on pip (0.8), produces a `model.pb` and `weights_manifest.json`, not a `model.json`. Assuming the latest version produces a `model.json` as the docs promise, is there an ETA on publishing latest to pip?=====', '@caisq do you have any comments on this or can we close this issue ?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/5343,WEBGL_PACK_DEPTHWISECONV=true seems to cause significant first inference performance drop,17,closed,2021-07-19T05:30:37Z,2021-10-21T12:41:59Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes. Production code at [https://github.com/wingman-jr-addon/wingman_jr/pull/135](https://github.com/wingman-jr-addon/wingman_jr/pull/135), minimal reproduction at [https://github.com/wingman-jr-addon/wingman_jr/pull/136](https://github.com/wingman-jr-addon/wingman_jr/pull/136)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 Home 21H1- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A, but laptop specs:```ideapad FLEX5-1570Processor	Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz   2.90 GHzInstalled RAM	16.0 GB (15.9 GB usable)System type	64-bit operating system, x64-based processorPen and touch	Pen and touch support with 10 touch points```- TensorFlow.js installed from (npm or script link):https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.4.0/dist/tf.min.jshttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tf-backend-wasm.jshttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm.wasmhttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm-simd.wasmhttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm-threaded-simd.wasm- TensorFlow.js version (use command below): 3.4.0- Browser version: Firefox 90.0 64 bit- Tensorflow.js Converter Version: Unknown, but probably 2.7.0Current behavior - Upgrading from 3.3.0 to 3.4.0 experienced major performance drop on load+first inference time. 3.3.0 sees times of about 8.8s, 3.4.0 sees times about 14.4s. It pains me to report a bug related to WEBGL_PACK as so much work has gone into this feature, but ... It appears that setting `WEBGL_PACK_DEPTHWISECONV=false` on 3.4.0 returns to performance found in 3.3.0. Regression with default flags has been found to exist in at least 3.6.0 and 3.8.0 as well. (This was found on a bisection to upgrade from 2.7.0 to 3.8.0 to get the new shader compilation performance improvements started in #5205 )Expected behavior - 3.4.0 with the flag default `WEBGL_PACK_DEPTHWISECONV=true` offers similar or better performance to 3.3.0.Minimal reproduction: [https://github.com/wingman-jr-addon/wingman_jr/pull/136](https://github.com/wingman-jr-addon/wingman_jr/pull/136)Note this is a Firefox plugin, but TF.js is loaded via a content tab rather in the background context so it should be acting quite similarly to a normal browsing context.Attached is output from Firefox's about:support, which includes more detailed graphics issues that may be relevant to the matter at hand.[FF90_about_support.txt](https://github.com/tensorflow/tfjs/files/6838167/FF90_about_support.txt)","['Hi Ahmed,I think you probably have more knowledge about the webgl backend so assigning this to you:) Please help take a look when you have a chance. Really appreciate it!=====', ""(Also, I see that I neglected to link off to the PR that led me to this issue - [https://github.com/tensorflow/tfjs/pull/4909](https://github.com/tensorflow/tfjs/pull/4909) - I don't know that the work done in the bulk of the PR is by any means the cause, but the change of the flag's default option is what caused the regression.)====="", '@wingman-jr-addon If the initialization is larger than before, there are possibly two causes:1. There are configuration of the depthwise conv2d ops that require more packed shaders to be compiled comparing to unpacked depthwise kernel. But I do see any setup could cause that.2.  The packed shader takes much longer to compile compares to unpacked version.Item 2 might be browser specific, can you help to verify if this behavior occurs on firefox and chrome web page? Thanks=====', 'Thanks for idea to try @pyu10055 . I changed the plugin to run in a web page and then ran across Firefox 90 and Chrome 92.In general, Chrome had significantly better performance, but there was still a performance gap between the flag being on and off for reload times. I would run the test by opening the browser, loading the web page, recording the time, and then hitting refresh and making note of times after that. The first load times tended to be much higher than subsequent times in Chrome. Also, I was only running one browser at a time during testing in case of some sort of GPU contention.Here are the raw times in seconds:FF 90, defaults: 14.5, 13.5, 14.2FF 90, flag=false: 9.6, 8.9, 9.1, 8.5Chrome 92, defaults: 13.9, 7.0, 6.8, 6.9, 7.0Chrome 92, flag=false: 14.9, 5.2, 4.8I reloaded the last Chrome test and tried it several more times, seeing reload times of 4.5-5.5 seconds.So, I guess I\'m not sure what to make of the results for Chrome - I\'m not sure if I should trust the reload results or if I should run a bunch of ""first run"" tests. For Firefox, the results are slower across the board but consistent on reload.Let me know what you think. Thanks!=====', ""My guess is that Chrome have better caching on the shader across page reloads. One thing you could try, not necessarily relates to depthwise conv2d, use uniform variables for unary and binary ops.```tf.env().set('WEBGL_USE_SHAPES_UNIFORMS', true),```We have observe significant reduction of initial loading time.====="", ""Thanks for the tip, I am seeing about a 0.5-1.0s reduction in load time on FF 90!But getting back to the matter at hand - any clue why we might be seeing such a performance gap on `WEBGL_PACK_DEPTHWISECONV=true` though?It could be that it just depends on hardware and mine is not the primary target, I'd just like to make sure there isn't some other issue hanging out.====="", ""@wingman-jr-addon @pyu10055  My guess `WEBGL_PACK_DEPTHWISECONV=true` brings more shaders to compile. You can compare the `binaryCaches`'s size in `backend_webgl.ts` between `WEBGL_PACK_DEPTHWISECONV=true` and `WEBGL_PACK_DEPTHWISECONV=false` to see how many shaders are added (they probably are encode/decode matrix shaders. If that's the case, I think this PR #5297 may resolve your issue by using uniforms to encode/decode matrix programs). And, in another side, we should check why so many encode/decode shaders are introduced. Can them be avoided/reduced? In my opinion, if we go to the packed path, it will be best if we only pack data at the beginning and unpack the data at the last and try to avoid encode/decode the data in the middle. Maybe we can check if there is any chance to optimize it.====="", ""Cross-posting from #5205 I've tested this on my notebook with 3 different models of medium-high complexity  | Model | DataSet | WEBGL_PACK_DEPTHWISECONV | WEBGL_USE_SHAPES_UNIFORMS | Warmup | Execution | Note || --- | --- | --- | --- | --- | --- | --- || Inception-v4 | ImageNet | True | False | 11.2sec | 42ms | Default| Inception-v4 | ImageNet | False | False | 10.8sec | 45ms || Inception-v4 | ImageNet | False | True | 10.8sec | 45ms || Inception-v4 | ImageNet | True | True | 11.2sec | 42ms || SSD/MobileNet-v2 | OpenImages | True | False | 14.7 | 2.1sec | Default| SSD/MobileNet-v2 | OpenImages | False | False | 13.3sec | 2.2sec || SSD/MobileNet-v2 | OpenImages | False | True | 12.7sec | 2.1sec || SSD/MobileNet-v2 | OpenImages | True | True | 13.6sec | 2.1sec || EfficientDet-D4 | CoCo | True | False | 23.1sec | 12.9sec | Default| EfficientDet-D4 | CoCo | False | False | 16.1sec | 14.5sec || EfficientDet-D4 | CoCo | False | True | 15.9sec | 14.0sec || EfficientDet-D4 | CoCo | True | True | 21.1sec | 13.00sec |All-in-all:- WEBGL_USE_SHAPES_UNIFORMS helps to significantly reduce warmup with *NO* negative impact on subsequent inference  - WEBGL_PACK_DEPTHWISECONV increases warmup too much even if subsequent inference is slightly fasterAs it is, I'll be setting `WEBGL_USE_SHAPES_UNIFORMS=True` and `WEBGL_PACK_DEPTHWISECONV=False` on my projects as even with uniforms enabled (which does help), it's still too slow on warmup  *Note: Chrome does extensive shader caching between sessions, so simple page reload is not sufficient and full browser restart is needed between tests*====="", '@rthadur Would you agree the awaiting response label can probably be removed from this issue now?=====', ""@ahmedsabie @qjia7 @rthadur @pyu10055 Any updates on this? As you can see, WEBGL_PACK_DEPTHWISECONV=True (which is default value) has a **massive** negative performance impact - and it's gotten far worse in newer versions of TFJS. This is a major regression and it has very little updates.And yes, using WEBGL_USE_SHAPES_UNIFORMS is much better, but - a) it's not a solution, it's an alternative, b) it's not widely implemented, c) almost nobody knows about it.====="", 'see #5689 for fully reproducible code and additional performance notes.=====', '@vladmandic @wingman-jr-addon I think this could be caused by the packed depthwise conv2d shader could be much larger in size than unpacked depthwise conv2d version. This could be related to the filter size, since the packed version expand the loop of the filter width into code. Can you share what is the filter size for depthwise conv2d in your model? And the other question is, we have a way to make the initial warm non UI blocking, basically by yielding the JS thread and removing all GL block calls (parallel shader compilation). But the overall warm time might still be similar. Will this behavior be helpful for your use cases?=====', ""> Can you share what is the filter size for depthwise conv2d in your model?I have seen the same behavior in almost **every off-the-shelf model**  Just pick any from TFHub - I've provided performance data for `Inception-v4`, `EfficientDet-D4`, `EfficientNet-B5`, `MobileNet-v2`And only model that doesn't have the problem is ancient `MobileNet-v2`  (and since I have an automated test for this, I can reproduce using any given model)  What is the intended benefit of the packed conv2d shader? I don't see much benefit of it: - few percent faster inference (never more than 2-5%, hardly worth it)- few hundred percent slowdown in warmup (average 50-400%, massive)> And the other question is, we have a way to make the initial warm non UI blocking, basically by yielding the JS thread and removing all GL block calls (parallel shader compilation). But the overall warm time might still be similar. Will this behavior be helpful for your use cases?Yes and No :)- Yes for overall usage as **any** reduction of blocking calls is very welcome.- No as I already use web workers in most cases anyhow,    so my UI is not really blocked - but since app requires models to work, not much use of app until warmup has finished  ====="", 'We have seen significant performance gain on mobile device with the packed depthwise conv2d shader, especially for android devices.Thank you for the insights, we will see if we can avoid the slow start up time it introduces.As for UI blocking, even with web worker, the UI can be blocked, since chrome has a single GL command queue, any blocking GL call will prevent new GL commands to be flushed to GPU.  =====', ""Thanks @pyu10055,  Perhaps as a start, you could do conditional `WEBGL_PACK_DEPTHWISECONV = isMobile()` ?  It doesn't solve a problem, just makes less people immediately affected  I just tried on Android. Yes, inference performance difference on Android is visible (unlike on desktop)  IMO - still, negative impact of slow warmup causing users to simply give up and close the app outweighs performance benefits of actual inference  Re: UI Blocking - true, if there is any GL usage elsewhere. Anyhow, if you can make it non-blocking, that is very much welcomeAnd when will WEBGL_USE_SHAPES_UNIFORMS become a default?====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5343"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5343"">No</a>=====', 'closing the loop after testing using todays code in main branch:warmup is now about 2x fasterno material difference regardless if `WEBGL_PACK_DEPTHWISECONV` is enabled or disabeld so that issue is resolved  do note that enabling `WEBGL_USE_SHAPES_UNIFORMS` performs much better *(2x faster warmup)* regardless of packing (actually packing improvements make it even faster)!**webgl** default - warmup initial 53sec in 3.9.0 -> 32sec in main branch- warmup cached 15sec -> 12sec**webgl** with uniforms enabled- warmup initial 23sec -> 18sec- warmup cached 14sec > 5sec@pyu10055 please consider enabling uniforms as default=====']",1
https://github.com/tensorflow/tfjs/issues/5275,Build tfjs-core and tfjs-backend-cpu with ts_library,1,closed,2021-07-01T20:26:26Z,2021-07-19T20:31:31Z,"This issue is part of the ""Adopt Bazel"" project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package, take a look at the [Bazel Migration doc](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md).",['Closed by #5133====='],1
https://github.com/tensorflow/tfjs/issues/4656,Please publish face-landmarks-detection 0.0.3 to npm,8,closed,2021-02-08T22:29:08Z,2021-02-18T02:01:15Z,"It looks like the face-landmarks-detection model was updated to work with TFJS 3.0 a few weeks agohttps://github.com/tensorflow/tfjs-models/blob/2af288d98609b32df2075cf7226c694989a5d476/face-landmarks-detection/package.json#L3But npm still only has the previous versionhttps://www.npmjs.com/package/@tensorflow-models/face-landmarks-detectionCan someone publish the latest TFJS 3.0-compatible version to npm, please?","['plz ^^=====', ""Wait there was a 0.0.3??? I was getting a huge bug with the npm version that toFloat does not exist on type Tensor3d and it had to do with a line in the package. What's going on?====="", 'Our team will publish the tfjs 3.x compatible version today. Stay tuned.=====', '🤩=====', '0.0.3 is published.=====', 'Thank you so much. You guys are literal superheroes. 🚀 Love the work you do and keep it up!=====', 'Thank you closing this as we have published to npm.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4656"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4656"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2077,"lstm-text-generation example - training with node.js results in error: ""UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'dtype' of undefined""",1,closed,2019-09-21T20:24:17Z,2019-09-23T17:33:16Z,"I'm following the instructions to to run the training-lstm-example, using @tensorflow/tfjs-node (on cpu) on Mac OSX.When I run the command, as instructed in the tutorial:```shyarn train shakespeare \    --lstmLayerSize 128,128 \    --epochs 120 \    --savePath ./my-shakespeare-model```I get an error:```Epoch 1 of 120:(node:28883) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'dtype' of undefined    at NodeJSKernelBackend.batchMatMul (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:336:37)    at NodeJSKernelBackend.fusedBatchMatMul (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:370:27)    at /Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/ops/fused_ops.ts:172:23    at /Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/engine.ts:462:22    at Engine.scopedRun (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/engine.ts:404:19)    at Engine.runKernel (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/engine.ts:459:10)    at matMul_ (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/ops/fused_ops.ts:171:22)    at Object.matMul (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:45:24)    at Object.dot (/Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-layers/src/backend/tfjs_backend.ts:407:22)    at /Users/danoved/Source/lstm-tutorial/tfjs-examples/lstm-text-generation/node_modules/@tensorflow/tfjs-layers/src/layers/recurrent.ts:1897:17```For reference, before this error it says:```> cpu backend was already registered. Reusing existing backend factory.> Platform node has already been set. Overwriting the platform with [object Object].> 2019-09-21 16:21:04.042496: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA> Orthogonal initializer is being called on a matrix with more than 2000 (65536) elements: Slowness may result.> Orthogonal initializer is being called on a matrix with more than 2000 (65536) elements: Slowness may result.> Compiled model with learning rate 0.01> _________________________________________________________________> Layer (type)                 Output shape              Param #> =================================================================> lstm_LSTM1 (LSTM)            [null,60,128]             112640> _________________________________________________________________> lstm_LSTM2 (LSTM)            [null,128]                131584> _________________________________________________________________> dense_Dense1 (Dense)         [null,91]                 11739> =================================================================> Total params: 255963> Trainable params: 255963> Non-trainable params: 0```",['This is resolved.  The issue occured when using node v 12.10.0.  When I downgraded to 10.15.3 the issue went away.====='],0
https://github.com/tensorflow/tfjs/issues/1075,Undocumented exposed API : csvDataset.columnNames(),2,closed,2019-01-09T16:22:57Z,2019-01-17T01:27:21Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js versiontfjs: ""0.14.1""tfjs-converter: ""0.7.1""tfjs-core: ""0.14.2""tfjs-data: ""0.1.5""tfjs-layers: ""0.9.1""#### Browser versionirrelevant#### Describe the problem or feature requestWant to know how to get the column names of a csv file.  The api is referenced in the code sample, but never included as a first class api item.#### Code to reproduce the bug / link to feature request![image](https://user-images.githubusercontent.com/547150/50912759-e5d4e100-1400-11e9-8c4e-1362101e1f99.png)","['cc: @tafsiri @kangyizhang =====', 'Also unexposed is `prefetch`=====']",0
https://github.com/tensorflow/tfjs/issues/1293,Add WebGL for onesLike,2,closed,2019-02-26T21:16:33Z,2019-03-11T18:19:50Z,#### TensorFlow.js version1.0.0-alpha3,"['Do we need to do something more?Since `onesLike` already uses `fill` kernel internally, it already uses the WebGL specific implementation. Is it necessary to add another kernel `onesLike`?=====', ""I don't think we need to do anything more, the fill kernel is fine as is.=====""]",0
https://github.com/tensorflow/tfjs/issues/826,Have the WebGL backend send certain ops to the CPU if it would be faster,1,closed,2018-10-24T11:36:30Z,2019-09-11T22:50:34Z,"Develop a heuristic for identifying smaller ops that would always be faster on the CPU. In `backend_webgl:compileAndRun`, check whether the incoming op meets the criteria and if so send it to the CPU.From Nikhil:For small operations in WebGL, we currently upload them to textures and call a shader. This can be much slower than just running operations on the CPU.It would be great to have a heuristic where an op that is executing on CPU-resident Tensors will simply execute on the CPU.Let's run an experiment doing this with coco-ssd and see what kind of wins we get!","['looks like the related PR has been merged , so closing this issue.=====']",0
https://github.com/tensorflow/tfjs/issues/4593,[tfjs-react-native] [iOS only] Initialization of backend rn-webgl failed [native code],5,closed,2021-01-24T17:29:12Z,2021-08-13T19:33:38Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 14.3 Simulator / Device- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 11```yarn list v1.22.10├─ @tensorflow/tfjs-automl@1.0.0├─ @tensorflow/tfjs-backend-cpu@2.7.0├─ @tensorflow/tfjs-backend-webgl@2.7.0├─ @tensorflow/tfjs-converter@2.7.0├─ @tensorflow/tfjs-core@2.7.0├─ @tensorflow/tfjs-data@2.7.0├─ @tensorflow/tfjs-layers@2.7.0├─ @tensorflow/tfjs-react-native@0.5.0└─ @tensorflow/tfjs@2.7.0```**Describe the current behavior**On launch, when I tf.ready(), I get the warning Initialization of backend rn-webgl failed**Describe the expected behavior**rn-webgl platform loads**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.```// index.jsimport '@tensorflow/tfjs-react-native'``````// App.tsximport * as tf from '@tensorflow/tfjs'...const [tfReady, setTfReady] = useState(false)useEffect(() => {  tf.ready()  // <-- this is when the warning happens    .then(() => setTfReady(true))}, [])if (!tfReady) {  return null}// render app```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```yarn list v1.22.10├─ expo-asset@8.2.1├─ expo-camera@9.0.0├─ expo-constants@9.2.0├─ expo-file-system@9.2.0├─ expo-font@8.3.0├─ expo-gl-cpp@9.1.2├─ expo-gl@9.1.1├─ expo-image-loader@1.2.0├─ expo-linking@2.0.0│  └─ expo-constants@9.3.5├─ expo-location@9.0.1├─ expo-media-library@9.2.1├─ expo-network@2.4.0├─ expo-permissions@9.3.0├─ expo-splash-screen@0.8.1└─ expo-store-review@2.3.0```All of these dependencies are configured and working on their own. WebGL initializes and works properly on Android emulator and device. This is an issue I'm encountering only for iOS, both on simulator and device.","[""Standing up a new project with `npx create-react-native-app` and doing the minimum dependency installations allowed me to get the rn-webgl backend loading. Closing the issue for now until I can figure out why it's not working in my app.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4593"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4593"">No</a>=====', 'Did you ever figure out what caused it? =====', '@Cannonball2134 > Did you ever figure out what caused it?No idea, but creating a new RN project and copying over all my code fixed it=====', 'Hi @zholmes1, I face the same error like you did. From your case, I kinda found out a trick which can be a temporary solution for this problem.I manually install @tensorflow/tfjs-backend-wasm and run `pod install`. The problem fixed by itself.After a few successful build, the error show up again. I uninstall @tensorflow/tfjs-backend-wasm and run `pod install`. The problem fixed again. @Cannonball2134 hopefully this is still helpful for you=====']",1
https://github.com/tensorflow/tfjs/issues/5000,posenet\demo\coco.js: function __clone()... could not be cloned error when trying to start dev demo server,4,closed,2021-04-28T04:26:19Z,2021-05-06T22:46:13Z,"Hi, I cloned the tfjs-models repository and was about to try out posenet demo.I followed the instructions as per GitHub: https://github.com/tensorflow/tfjs-models/tree/master/posenet/demoeverything went well but when I did yarn watch, I got the following error:""PS C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo> yarn watchyarn run v1.22.10$ cross-env NODE_ENV=development parcel index.html --no-hmr --openServer running at http://localhost:1234 ×  C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\coco.js: function __clone() {    var node2 = new Node(),    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    var node2 = new Node(),    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    at Object.serialize (v8.js:256:7)    at _default (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\util\clone-deep.js:16:30)    at normalizeFile (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\normalize-file.js:52:36)    at normalizeFile.next (<anonymous>)    at run (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\index.js:31:50)    at run.next (<anonymous>)    at Function.<anonymous> (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transform-ast.js:20:41)    at Generator.next (<anonymous>)    at evaluateSync (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\gensync\index.js:251:28)    at Function.sync (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\gensync\index.js:89:14)""a browser tab also loaded up with the same build error.I am running on Windows 10 and my editor is Visual Studio Code:Version: 1.55.2 (user setup)Commit: 3c4e3df9e89829dce27b7b5c24508306b151f30dDate: 2021-04-13T09:35:57.887ZElectron: 11.3.0Chrome: 87.0.4280.141Node.js: 12.18.3V8: 8.7.220.31-electron.0OS: Windows_NT x64 10.0.19041Please advise.","['@FlyWong thanks for reporting , i could reproduce the same in MAC OS , will dig deep more in to actual issue  and keep you posted.=====', 'Hi,is there any update on this issue?________________________________From: Rajeshwar Reddy T ***@***.***>Sent: Thursday, April 29, 2021 1:10 AMTo: tensorflow/tfjs ***@***.***>Cc: FlyWong ***@***.***>, Mention ***@***.***>Subject: Re: [tensorflow/tfjs] posenet\\demo\\coco.js: function __clone()... could not be cloned error when trying to start dev demo server (#5000)@FlyWong<https://github.com/FlyWong> thanks for reporting , i could reproduce the same in MAC OS , will dig deep more in to actual issue and keep you posted.—You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tfjs/issues/5000#issuecomment-828626998>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACPS7UWSWOQPFTPJBQXSCWTTLA6QNANCNFSM43WIMJ3Q>.=====', 'this is a duplicate of https://github.com/tensorflow/tfjs/issues/4921 , will track the same issue at one place. Closing this issue. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5000"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5000"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4596,How to read modelWeights.bin from file system,19,open,2021-01-25T05:29:16Z,2021-01-28T04:27:08Z,"As it is raised before, I am not able to load model as below. It gives `TypeError: Network request failed `error.  `const model = await tf.loadLayersModel('file://path/to/my-model/model.json'),`I need to read model.json and model_weights.bin manually and bundle them together. I am able to read model.json without any issues but I am having troubles to get right type for bin file. ```const rootPath = Platform.OS === 'ios' ? RNFS.MainBundlePath : RNFS.DocumentDirectoryPath,const modelWeights = await RNFS.readFile(`${rootPath}/model_weights.bin`, 'base64'),```I am not sure how to convert modelWeights (string) into number to use `bundleResourceIO(modelJson: io.ModelJSON, modelWeightsId: number)`I saw suggestions to convert base64 string into Uint8Array but I couldn't find a way to convert it into 'number' type.```const buffer = tf.util.encodeString(modelWeights, 'base64').buffer,const arr = new Uint8Array(buffer),```Any suggestions how to bundle them together or any other ways loading model from native file system? PS: Statically bundle works without any issues. ```import modelJson from './assets/model.json',import modelWeights from './assets/model_weights.bin',const model = await tf.loadGraphModel(bundleResourceIO(modelJson, modelWeights)),```**Standalone code to reproduce the issue**```import React from 'react',import {Button, View} from 'react-native',import * as tf from '@tensorflow/tfjs',import {bundleResourceIO} from '@tensorflow/tfjs-react-native',import modelJson from './assets/model/model.json',import modelGroup from './assets/model/group1-shard1of1.bin',import RNFS from 'react-native-fs',class Home extends React.Component {  triggerModel = async () => {    await tf.ready(),    //Successful    const model = await tf.loadGraphModel(      bundleResourceIO(modelJson, modelGroup),    ),    //model.json and group1-shard1of1.bin exists in file system    //returns model.json file successfully    const modelJson1 = await RNFS.readFile(      `${RNFS.DocumentDirectoryPath}/model/model.json`,    ),    //Throws TypeError: Network request failed    const model1 = await tf.loadGraphModel(modelJson1, {      weightUrlConverter: async (weightFileName) => {        return `file://${RNFS.DocumentDirectoryPath}/model/group1-shard1of1.bin`,      },    }),    //Throws TypeError: Network request failed    const model2 = await tf.loadGraphModel(      `${RNFS.DocumentDirectoryPath}/model/model.json`,    ),  },  render() {    return (      <View>        <Button onPress={() => this.triggerModel()}>Trigger Model</Button>      </View>    ),  }}export default Home,```","['@kscgl Please update if you get any solution on this.I am working on something similar.=====', 'Also, I have a similar question. That is how to load a model with two different links to json and weights file. If anybody here knows how to do it, do let me know.Thanks.=====', 'There is an optional argument in the loadGraphModel called `weightUrlConverter` which can be used to modified the weights path. It worked for me. Hope this helps.=====', ""Thank you @nischal-sanil for suggestion. I tried as below but I got `TypeError: Network request failed````const modelLoad = await tf.loadGraphModel(modelJson, {            weightUrlConverter:              'file://' +              RNFS.DocumentDirectoryPath +              '/model/group1-shard1of1.bin',          }),```====="", ""@kscgl You should pass an async function to the `weightUrlConverter`. This function will receive a `weightFileName: string` as input and should return path to the weights. If you path is correct something like this should work.```const modelLoad = await tf.loadGraphModel(modelJson, {            weightUrlConverter: async function (weightFileName) => {              return 'file://' +              RNFS.DocumentDirectoryPath +              '/model/group1-shard1of1.bin',          }}),```====="", ""@nischal-sanil I am getting the same network request failed error. ```  const modelLoad = await tf.loadGraphModel(modelJson, {            weightUrlConverter: this.getWeightFile(              'model/group1-shard1of1.bin',            ),          }),  getWeightFile = async (weightFileName) => {    return `file://${RNFS.DocumentDirectoryPath}/${weightFileName}`,  },```====="", 'Do not call the function. By calling it you are again passing a string as an input. Try something like this.```  const modelLoad = await tf.loadGraphModel(modelJson, {            weightUrlConverter : async (weightFileName) => {                  return `file://${RNFS.DocumentDirectoryPath}/${weightFileName}`,          }}),```Tensorflowjs will call this function behind the scenes.=====', '@nischal-sanil I see. I tried that one but same error. ```const modelLoad = await tf.loadGraphModel(modelJson, {            weightUrlConverter: async (weightFileName) => {              return `file://${RNFS.DocumentDirectoryPath}/model/group1-shard1of1.bin`,            },          }),```=====', 'hmmm, Not sure why this is the case then. If the path is right it should have worked. May be you can try another optional argument called `weightPathPrefix` instead of `weightUrlConverter`. Check out the [docs](https://js.tensorflow.org/api/latest/#loadGraphModel)=====', ""@nischal-sanil That won't work either because it always give `TypeError: Network request failed` wherever I provide a location from file system. ====="", ""Aren't you using a local server?====="", '@nischal-sanil For what purpose? I provide the path of phone file system itself. =====', '@kscgl can you please share minimal code for reproduction ? if you are loading local file system , you should not see above error, are you trying to load using script tags ?=====', '@rthadur I updated the initial post to add standalone code. Please let me know if you need more details.  =====', ""@HarshalRohit I decided to go with [asyncStorageIO handler](https://js.tensorflow.org/api_react_native/0.2.1/#asyncStorageIO) until I figure this out. ```...const modelName = 'customModelName',await model.save(asyncStorageIO(modelName)),//Save modelName somewhere. I use local database...//Fetch your model name then call your modelconst modelName = fetchModelName(),const model = await tf.loadGraphModel(asyncStorageIO(modelName)),...```====="", '@kscgl Thanks!!=====', 'I am getting **Error** `Row too big to fit into CursorWindow requiredPos=0, totalRows=1` on calling `tf.LoadLayersModel`.@kscgl How much is your model size (as returned by  `model.save(asyncStorageIO(modelName))` ? Mine is 6 MB.@jinjingforever I understand this is due sqlite limits, but is there any workaround for this?? =====', ""@HarshalRohit Mine is around 1 MB. It's probably due to Android AsyncStorage limit which is 6 MB. You can look into how to increase AsyncStorage size and try if that works. ====="", ""@kscgl Increasing the AsyncStorage size doesn't work.I tried increasing the size and `model.save` works fine but`model.load` throws the error `Row too big to fit into CursorWindow requiredPos=0, totalRows=1`.I guess I will have to write custom IO handler.=====""]",1
https://github.com/tensorflow/tfjs/issues/4343,frequent getaddrinfo ENOTFOUND tfhub.dev on models,15,closed,2020-12-03T11:54:34Z,2021-01-29T22:27:07Z,"**System information**this is just to do with loading models, basic TF js code.**Describe the current behavior**frequently when loading models, I'll get an error like the below:```Uncaught Promise Rejection FetchError: request to https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1/group1-shard1of7?tfjs-format=file failed, reason: getaddrinfo ENOTFOUND tfhub.dev    at ClientRequest.<anonymous> (/Users/dc/dev/nlp/TfClassifier/node_modules/node-fetch/lib/index.js:1461:11)    at ClientRequest.emit (events.js:314:20)    at TLSSocket.socketErrorListener (_http_client.js:469:9)    at TLSSocket.emit (events.js:314:20)    at emitErrorNT (internal/streams/destroy.js:100:8)    at emitErrorCloseNT (internal/streams/destroy.js:68:3)    at processTicksAndRejections (internal/process/task_queues.js:80:21) {  type: 'system',  errno: 'ENOTFOUND',  code: 'ENOTFOUND'}```**Describe the expected behavior**Ideal would be a way to download the models locally for both speed and reliability.otherwise, less network fails!**Standalone code to reproduce the issue**```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"",await sentenceEncoder.load()```but this is hard to repro as it seems to be a networking issues**Other info / logs** Include any logs or source code that would be helpful tosee above","['@dcsan can you please share code snippet and model which you are trying to load ?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@rthadur there is a code snippet above```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"",await sentenceEncoder.load()```around here is where it times out.is there any way to grab/cache the models locally rather than a network import every runtime?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Not stale botOn Thu, Dec 17, 2020, 8:47 PM tensorflow-butler[bot] <notifications@github.com> wrote:> This issue has been automatically marked as stale because it has not had> recent activity. It will be closed in 7 days if no further activity occurs.> Thank you.>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4343#issuecomment-747865182>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAD5PUXGKEODVNGRP62AC3DSVLNGXANCNFSM4UL3Z5YQ>> .>=====', 'you can save the model using model.save(""localstorage://model"") locally. =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Actually I was asking about the encoder not my own models, as from the example above.```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"",await sentenceEncoder.load()```I don\'t think `sentenceEncoder.save()` will work in the same way as for a model. I do of course use that already for saving and caching models.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'not stale :D=====', 'May i know from which location you are trying to load tfhub models ?=====', 'import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"",like written in the issue above=====', 'some links does not work in china , I would like to know if you are accessing from china and one more thing we will not be supporting custom save function in universal-sentence-encoder in near future.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4343"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4343"">No</a>=====', ""No I'm not accessing from China.So there is no way to pull the model down locally, it has to be re-downloaded for every run?meaning this function:`await sentenceEncoder.load()`pulls a large model over the network every time I run my process?=====""]",1
https://github.com/tensorflow/tfjs/issues/5382,Webpack fails when using Bazel's `.mjs` output files,1,closed,2021-07-26T21:40:13Z,2021-08-11T17:35:27Z,"**Describe the current behavior**Webpack fails to bundle when run on Bazel-created `.mjs` outputs.**Describe the expected behavior**Webpack successfully bundles when given Bazel outputs.**Standalone code to reproduce the issue**In the [webpack_test](https://github.com/mattsoulanille/tfjs/tree/webpack_test/e2e/webpack_test) branch, I've created a simple webpack bundle that depends on the Bazel-compiled outputs of `tfjs-core` and `tfjs-backend-cpu`. To reproduce the bug, run the following in that directory:```shyarnyarn build-depsyarn build```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```$ yarn buildyarn run v1.22.10$ webpackassets by status 673 bytes [cached] 1 assetorphan modules 4.55 KiB [orphan] 2 modulesruntime modules 274 bytes 1 module./app.js + 2 modules 5.37 KiB [built] [code generated]WARNING in ./app.js 22:10-21export 'tensor1d' (imported as 'tf') was not found in '@tensorflow/tfjs-core' (module has no exports)ERROR in ../../link-package/node_modules/@tensorflow/tfjs-backend-cpu/dist/index.mjs 18:0-23Module not found: Error: Can't resolve './base' in '/home/msoulanille/tfjs/link-package/node_modules/@tensorflow/tfjs-backend-cpu/dist'Did you mean 'base.js'?BREAKING CHANGE: The request './base' failed to resolve only because it was resolved as fully specified(probably because the origin is a '*.mjs' file or a '*.js' file where the package.json contains '""type"": ""module""').The extension in the request is mandatory for it to be fully specified.Add the extension to the request. @ ./app.js 19:0-38...```We are likely misusing the `.mjs` extension by not including the file extension in relative imports.A possible workaround is to add the following to the webpack config when bundling tfjs, but we still need to fix our outputs to work without this: https://github.com/graphql/graphql-js/issues/2721#issuecomment-723008284```jsmodule: {  rules: [{    test: /\.m?js/,    resolve: {      fullySpecified: false    }  }],}```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5382"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5382"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5725,"Bodypix - Uncaught (in promise) Error: Could not initialize any backends, all backend initializations failed",3,closed,2021-10-14T04:36:51Z,2021-10-15T01:14:33Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No (Demo page from bodypix readme - https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu  20.04.3 LTS- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA (Laptop)- TensorFlow.js installed from (npm or script link): suspect NPM- TensorFlow.js version (use command below): - Browser version: Google Chrome Version 94.0.4606.81 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Javascript Error - Uncaught (in promise) Error: Could not initialize any backends, all backend initializations failed. And hang at Loading BodyPix model...**Describe the expected behavior**The loading Bodypix model... shall disappear and the webcam screen shall appear**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html![error](https://user-images.githubusercontent.com/14855901/137252753-f62893f0-0f5f-4b60-bdef-e8545525beca.png)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""`WebGL is not supported on this device` means exactly that - and since WASM backend is not loaded by default, there is no meaningful fallback  You can load and enable WASM backend and it will work, but I'd rather fix your notebookYour Ubuntu installation probably did not detect video card correctly so its running with default non-accelerated display drivers.If drivers dont suport hardware accelerated operations, WebGL in browser is not available - check by navigating to `about://gpu`  And on Ubuntu, to check which drivers you're using run `sudo lshw -c video`====="", ""Hi Vladmandic,Thanks for your kind assistance.It's confirmed to be the video card issue.I tried loading up on windows 10 for same laptop and able to access same page without any issues.Sorry that never tried investigating along that line as for another bodypix page, the demo is working on my ubuntu setup.https://qumoptly.github.io/body-pix/Thus I never thought that it is due to the WebGL issues-----------------------------------Hmm, seems that  sudo lshw -c video is detecting the NVIDIA graphic card.However, that is unrelated to bodypix and will investigate/search elsewhere  *-display                        description: VGA compatible controller       product: NVIDIA Corporation       vendor: NVIDIA Corporation       physical id: 0       bus info: pci@0000:01:00.0       logical name: /dev/fb0       version: a1       width: 64 bits       clock: 33MHz       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom fb       configuration: depth=32 driver=nvidia latency=0 mode=1920x1080 visual=truecolor xres=1920 yres=1080       resources: iomemory:600-5ff iomemory:600-5ff irq:206 memory:a3000000-a3ffffff memory:6030000000-603fffffff memory:6040000000-6041ffffff ioport:6000(size=128) memory:a4000000-a407ffff  *-display       description: VGA compatible controller       product: UHD Graphics       vendor: Intel Corporation       physical id: 2       bus info: pci@0000:00:02.0       version: 05       width: 64 bits       clock: 33MHz       capabilities: pciexpress msi pm vga_controller bus_master cap_list rom       configuration: driver=i915 latency=0       resources: iomemory:600-5ff iomemory:400-3ff irq:204 memory:6044000000-6044ffffff memory:4000000000-400fffffff ioport:7000(size=64) memory:c0000-dffff====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5725"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5725"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/268,Can I code Tensorflow Js In Google Colab ?,7,closed,2018-05-05T15:30:21Z,2021-01-14T06:18:53Z,"Hi there, I like Tensorflow JS. But I want to use Tensorflow JS in Google Colab for research. Maybe you can explain some solutions. Thank you very much!","[""Hi @LunaLuan, yes it is possible but currently a little bit tricky.Taking inspiration from [this conversation](https://github.com/googlecolab/colabtools/issues/13), I just created an example that you can access here: [Google Colaboratory + TensorflowJS example](https://colab.research.google.com/drive/1PH7VCBQHeDbvGi3F3m-pRADRvDxxPfY1).Currently, you won't be able to fork it and run within **your** Google Drive but I'll later explain how to proceed (this is the tricky part).====="", ""Colab isn't quite ready for full-fledged JavaScript notebooks. I wouldrecommend using https://beta.observablehq.com/ for now.On Sat, May 5, 2018 at 12:32 PM, Timothée Bernard <notifications@github.com>wrote:> Hi @LunaLuan <https://github.com/LunaLuan>, yes it is possible but> currently a little bit tricky.>> Taking inspiration from this conversation> <https://github.com/googlecolab/colabtools/issues/13>, I just created an> example that you can access here: Google Colaboratory + TensorflowJS> example> <https://colab.research.google.com/drive/1PH7VCBQHeDbvGi3F3m-pRADRvDxxPfY1>> .>> Couple of comments:>>    - Installed tfjs version is 0.7.0.>    - I invite you to fork this colab and try more complex operations like>    loadModel(), predict() to see how far we can go!>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/268#issuecomment-386817703>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/ABDLzVHgj04CzXB-0tn3GFkTnhXZkOBrks5tvdQfgaJpZM4TzreX>> .>====="", ""But ObservableHQ doesn't have the GPU advantages of training a model on Google's servers...?====="", ""Running TensorFlow.js in Jupyter Notebooks require a special Jupyter kernelfor Node.js (iJavaScript <https://github.com/n-riesco/ijavascript>). Ihaven't figured out a way to install that in Google CoLab directly yet.However, you can run this from your own Jupyter Notebook environment. Seethe thread I posted not long ago on Twitter:https://twitter.com/sqcai/status/1104475647391481856On Mon, Apr 8, 2019 at 7:46 AM Erik Katerborg <notifications@github.com>wrote:> But ObservableHQ doesn't have the GPU advantages of training a model on> Google's servers...?>> —> You are receiving this because you modified the open/close state.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/268#issuecomment-480798426>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AQC5fjrAsY7cLWtDZTqlfe1Z-tpV8WObks5veywxgaJpZM4TzreX>> .>-- ---Shanqing CaiSoftware EngineerGooglecais@google.com====="", ""# Docs for importing modules into your JavaScript Notebook once setupWorking on an iPad Pro today and managed to get this going. Was not sure how to require node modules as shown in the tf comment above, but figured out a solution.Will describe my whole flow since the iPad required a work around which may be useful to someone.## Installing ijavascriptJust as shown above, but with one difference - note the `familiar-log` module I installed, which is to be used in the final step.```!npm init -y!npm install ijavascript -g zeromq familiar-log --unsafe-perm!ijsinstall!jupyter-kernelspec list!npm root -g```## Modifying your `.ipynb` file without a computerMy solution was to save the notebook to GitHub Gists, edit the file there to set the kernel as described [here](https://github.com/googlecolab/colabtools/issues/13), save it, then tap the `Open in Colab` button to send it back.## Importing npm modules into your scriptSee in step 1 where I installed the `familiar-log` module. Note the `!npm root -g`.After confirming that JS works in my new notebook `console.log(‘hello world’)` I imported the module by using the value output by the root command. In my case: `/tools/node/lib/node_modules `. So instead of `require(‘familiar-log’)` I specified the location.```var log = require('/tools/node/lib/node_modules/familiar-log').loglog('hello'),log('hello').json({ world: true }),```====="", '@jasonhargrove @LunaLuan @KokoDoko I made a simple to bootstrap template that allows you to run TensroflowJS on Google Colab, install modules in notebook, and run long running async code https://dev.to/obenjiro/silence-of-the-fans-part-1-javascript-quickstart-5f3m=====', 'Hi All, @jasonhargrove @timotheebernard @nsthorat @obenjiro @KokoDokoI like python running in the google colab- What is the best way to do the same for javascript/node.js in 2021?thanks in advance---Here is what I found till now:- Jupyter related (Not google colaboratory)1. Running javascript (client-side only, not node.js) in Jupyter using %%jshttps://youtu.be/DTco1nTTlzE?t=432. Running Node.js (server only, not client-side) using pixiedust_node libraryHow to use a Jupyter Notebook to run Node.js code in Watson Studio or a local environment using the pixiedust_node libraryhttps://developer.ibm.com/patterns/run-nodejs-code-in-jupyter-notebooks/- Node.js Notebookshttps://runkit.com/skiphttps://observablehq.com/demohttps://alpha.iodide.io/tryit/---Related: https://github.com/googlecolab/colabtools/issues/13#issuecomment-759946089=====']",0
https://github.com/tensorflow/tfjs/issues/5607,TFJS TFLite installation with script tags not working,3,closed,2021-09-11T10:23:53Z,2021-09-13T21:02:42Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**I have tried to use the ""import via script tag"" section from https://www.npmjs.com/package/@tensorflow/tfjs-tflite/v/0.0.1-alpha.2 in a very basic index.html as follows:```<!DOCTYPE html><html><head>	<meta charset=""utf-8"">	<title>TFJS TFLite</title></head><body>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite""></script></body></html>```but the `tfjs-tflite` script raises an error:```Request URL: https://cdn.jsdelivr.net/npm/@tensorflow/tflite_web_api_cc_simd.jsRequest Method: GETStatus Code: 404 ```**Any other info / logs**See https://replit.com/@ClementWalter/tfjs-tflite#index.html","[""This looks like a typical problem with relative path references. If the file loads another file, it needs to be loaded via a fully resolved URL, which in this case is https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.6/dist/tf-tflite.min.js or https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.min.js if you don't request a specific version. Then, the requested relative path will be correctly resolved to https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.6/dist/tflite_web_api_cc_simd.js====="", '@MartinKolarik thats correct , please also refer to note here https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite#via-a-script-tag , thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5607"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5607"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5661,"Export function ""nameScope"" in ""tfjs-layers"" module",2,open,2021-09-24T09:42:31Z,2021-09-25T06:20:29Z,"**System information**- TensorFlow.js version: 3.9.0- Are you willing to contribute it: YesFeature: Expose the function ""nameScope"" from ""tfjs-layers/src/common.ts"" to the tfjs-layers module. This enables the serialization/deserialization of custom layers that contain other layers with weights.It will add the function ""nameScope"" to the publicly visible API of tfjs-layers.This will benefit anybody who want the serialize/deserialize more complex custom layers.","['@JanKaul thank you , do you wish to contribute for this ?=====', 'Yes, I will try to prepare a PR.=====']",1
https://github.com/tensorflow/tfjs/issues/4765,Backend WASM does not implement kernel LRN,1,open,2021-02-28T13:52:00Z,2021-06-03T16:15:38Z,"attempting to run places365 model, googlenet variation  converted from caffe -> tf saved -> tfjs graph fails using `wasm` backend:```logUncaught (in promise) Error: Kernel 'LRN' not registered for backend 'wasm'    at Engine2.runKernel (engine.js:391)    at localResponseNormalization_ (local_response_normalization.js:53)```same graph model works fine in `nodejs` using `tensorflow` backend.environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10",['cc @pyu10055 @lina128 @mattsoulanille ====='],1
https://github.com/tensorflow/tfjs/issues/5652,[wasm] OOM for some conv2d input on build bots,2,closed,2021-09-23T02:24:44Z,2021-11-19T21:31:27Z,"Test case: https://github.com/tensorflow/tfjs/pull/5637/commits/976623953e724be45730f8259a0d0944243d4afcBuild log: https://console.cloud.google.com/cloud-build/builds/46b2a97c-0c12-494a-9e7a-51e1627fe072,step=14?project=learnjs-174218```$ ./scripts/test-ci.sh$ tsc && ts-node --transpile-only --skip-ignore -P tsconfig.test.json src/test_node.tsPlatform node has already been set. Overwriting the platform with [object Object].Started(node:48) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 uncaughtException listeners added. Use emitter.setMaxListeners() to increase limit(node:48) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 unhandledRejection listeners added. Use emitter.setMaxListeners() to increase limitXNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_max_pooling2d_nhwc_f32 is not successful. XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.XNN status for xnn_create_max_pooling2d_nhwc_f32 is not successful. XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.failed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:54                throw ex,                ^RuntimeError: abort(RangeError: WebAssembly Instantiation: Out of memory: wasm memory). Build with -s ASSERTIONS=1 for more info.    at abort (/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:9596)    at /workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:11594error Command failed with exit code 7....................................**..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 7.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```","['Fixed in #5852.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5652"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5652"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5872,[wasm] Incorrect check for 'process' in pre.js,1,closed,2021-11-19T19:25:42Z,2021-11-22T23:05:03Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below): b2f3664a32a0119a1acf93756cc8b930b8de860f- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**#5852 made it impossible to load tfjs-backend-wasm on the web. It checks for the existence of the global `process` variable incorrectly in `pre.js`:```javascriptif (process && process.listeners) {...}```This was not caught by unit tests because Karma adds a global `process` variable to the browser.**Describe the expected behavior**`pre.js` correctly checks for the existence of a `process` variable.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5872"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5872"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5613,tensor dimension need to be 2d,1,closed,2021-09-13T16:50:04Z,2021-09-13T22:53:27Z,"https://github.com/tensorflow/tfjs/blob/4dfcd782deda0a9fbe9f3cc5c60bbf4293e14577/tfjs-core/src/ops/diag.ts#L41-L43Code snippet error on official docs of tensorflowjs [here](https://js.tensorflow.org/api/latest/#diag)**It should be**` const x = tf.tensor2d([1, 2, 3, 4, 5, 6, 6, 8], [4, 2]) tf.diag(x).print()`","['Thanks for reporting , submitted a fix =====']",1
https://github.com/tensorflow/tfjs/issues/5447,blazeface's result is not correct on benchmarks test on webgl backend,3,closed,2021-08-10T08:17:27Z,2021-08-23T00:51:54Z,"Steps to reproduce:1. git clone https://github.com/tensorflow/tfjs2. cd tfjs/tfjs-backend-webgl3. yarn && yarn build-npm4. Do below changes to use your own build npm package.```--- a/e2e/benchmarks/local-benchmark/index.html+++ b/e2e/benchmarks/local-benchmark/index.html@@ -88,9 +88,8 @@ limitations under the License.   </div>   <script src=""https://unpkg.com/@tensorflow/tfjs-core@latest/dist/tf-core.js"" crossorigin></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-backend-cpu@latest/dist/tf-backend-cpu.js"" crossorigin></script>-  <script src=""https://unpkg.com/@tensorflow/tfjs-backend-webgl@latest/dist/tf-backend-webgl.js"" crossorigin></script>+  <script src=""../../../bazel-out/x64_windows-fastbuild/bin/tfjs-backend-webgl/tf-backend-webgl.js"" crossorigin></script>```5. cd ..6. npx http-server7.  Open the browser and navigate to http://localhost:8080/e2e/benchmarks/local-benchmark/8. Choose `blazeface` and `webgl` backend, click `Test correctness`. Expect the result is `true`, but get `false`.It's a regression. The result is correct with the latest publish npm package.","[""Will take a look if it's caused by the uniform change. If not, will un-assign myself.====="", ""It seems that it's caused by PR #5422. Will continue this tomorrow.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5447"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5447"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2976,[webgpu] nonMaxSuppression doesn't work with other operators in sync mode,0,closed,2020-03-27T10:33:15Z,2020-06-05T18:09:52Z,"The root cause is: In sync mode (Can not call await), the scores generated by div is resided in the GPU. But for NMS,  it requires the input data is ready in the CPU. So this may fail in WebGPU but pass in WebGL.```    it('works with other operators, div', async () => {      const boxes = tf.tensor2d(          [            0, 0,  1, 1,  0, 0.1,  1, 1.1,  0, -0.1, 1, 0.9,            0, 10, 1, 11, 0, 10.1, 1, 11.1, 0, 100,  1, 101          ],          [6, 4]),      const a = tf.tensor1d([0, 1, -2, -4, 4, -4]),      const b = tf.tensor1d([0.15, 0.2, 0.25, 0.5, 0.7, 1.2]),      const scores = a.div(b) as tf.Tensor1D,      const maxOutputSize = 2,      const iouThreshold = 0.5,      const scoreThreshold = 0,      const indices = tf.image.nonMaxSuppression(          boxes, scores, maxOutputSize, iouThreshold, scoreThreshold),      expect(indices.shape).toEqual([2]),      expectArraysEqual(await indices.data(), [4, 1]),    }),```The work around is before passing any data to NMX in WebGPU, await it first. This passes in both WebGL and WebGPU:```    it('works with other operators, div', async () => {      const boxes = tf.tensor2d(          [            0, 0,  1, 1,  0, 0.1,  1, 1.1,  0, -0.1, 1, 0.9,            0, 10, 1, 11, 0, 10.1, 1, 11.1, 0, 100,  1, 101          ],          [6, 4]),      const a = tf.tensor1d([0, 1, -2, -4, 4, -4]),      const b = tf.tensor1d([0.15, 0.2, 0.25, 0.5, 0.7, 1.2]),      const scores = a.div(b) as tf.Tensor1D,      const maxOutputSize = 2,      const iouThreshold = 0.5,      const scoreThreshold = 0,      // await is only necessary for WebGPU.      await scores.data(),      const indices = tf.image.nonMaxSuppression(          boxes, scores, maxOutputSize, iouThreshold, scoreThreshold),      expect(indices.shape).toEqual([2]),      expectArraysEqual(await indices.data(), [4, 1]),    }),```To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser version#### Describe the problem or feature request#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).",[],0
https://github.com/tensorflow/tfjs/issues/3949,using tfjs in node js cause error.,4,closed,2020-09-20T05:53:28Z,2020-10-05T18:01:15Z,"hello I'm doing tensorflow js following https://www.youtube.com/watch?v=EoYfa6mYOG4&t=2221sbut <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.11.7""></script>with above, it cause below error in `model = await tf.loadModel(""images/VGG16/model.json""),````Uncaught (in promise) Error: Unknown layer: Functional    at new t (tfjs@0.11.7:2)    at deserializeKerasObject (tfjs@0.11.7:2)    at deserialize (tfjs@0.11.7:2)    at tfjs@0.11.7:2    at tfjs@0.11.7:2    at Object.next (tfjs@0.11.7:2)    at o (tfjs@0.11.7:2)```and I changed it to 2.0  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js""></script> and then it cause in`model = await tf.loadLayersModel(""images/VGG16/model.json""),````Uncaught (in promise) Error: Unknown layer: Functional. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().    at jN (generic_utils.js:242)    at GI (serialization.js:31)    at models.js:295    at u (runtime.js:45)    at Generator._invoke (runtime.js:274)    at Generator.forEach.t.<computed> [as next] (runtime.js:97)    at Wm (runtime.js:728)    at o (runtime.js:728)```how can I solve this problem?I made h5 file through colab```import keras!pip install -q tensorflowjsfrom google.colab import drivefrom google.colab import filesdrive.mount('/gdrive')import tensorflowjs as tfjsvgg16 = keras.applications.vgg16.VGG16()a=tfjs.converters.save_keras_model(vgg16,'tfjs/tfjs-models/VGG16')```with converted model.json and bin files, it cause above error both for 1.x and 2.0 version. ","['In your model.json ""Functional"" should be under model_config.class_name change it to ""Model""=====', 'This issue has been fixed [here](https://github.com/tensorflow/tfjs/pull/3863) , please use latest version to convert and load model and let us know ?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/4052,"Can't build tfjs-node on Windows 10, module_name not defined in binding.gyp.",11,closed,2020-10-10T01:41:33Z,2021-04-12T22:46:18Z,"I'm unable to run tfjs-node. It reports a missing bindings file. I see the file in my project, but I haven't found a way to include it (via import or require).I'm not certain, but it looks like ""module_name"" is referenced without being defined in binding.gypHope this helps.VS CodeWindows 10x64node 14.13.1python 2.7.17Stack:(WORKSPACE)\node_modules\@tensorflow\tfjs-node>node-gyp configure --verbosegyp info it worked if it ends with okgyp verb cli [gyp verb cli   'C:\\Program Files\\nodejs\\node.exe',gyp verb cli   'C:\\...\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js',gyp verb cli   'configure',gyp verb cli   '--verbose'gyp verb cli ]gyp info using node-gyp@7.1.0gyp info using node@14.13.1 | win32 | x64gyp verb command configure []gyp verb find Python Python is not set from command line or npm configurationgyp verb find Python Python is not set from environment variable PYTHONgyp verb find Python checking if ""python3"" can be usedgyp verb find Python - executing ""python3"" to get executable pathgyp verb find Python - ""python3"" is not in PATH or produced an errorgyp verb find Python checking if ""python"" can be usedgyp verb find Python - executing ""python"" to get executable pathgyp verb find Python - executable path is ""C:\Python27\python.exe""gyp verb find Python - executing ""C:\Python27\python.exe"" to get versiongyp verb find Python - version is ""2.7.17""gyp info find Python using Python version 2.7.17 found at ""C:\Python27\python.exe""gyp verb get node dir no --target version specified, falling back to host node version: 14.13.1gyp verb command install [ '14.13.1' ]gyp verb install input version string ""14.13.1""gyp verb install installing version: 14.13.1gyp verb install --ensure was passed, so won't reinstall if already installedgyp verb install version is already installed, need to check ""installVersion""gyp verb got ""installVersion"" 9gyp verb needs ""installVersion"" 9gyp verb install version is goodgyp verb get node dir target node version installed: 14.13.1gyp verb build dir attempting to create ""build"" dir: C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-node\buildgyp verb build dir ""build"" dir needed to be created? C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-node\buildgyp verb find VS msvs_version not set from command line or npm configgyp verb find VS VCINSTALLDIR not set, not running in VS Command Promptgyp verb find VS checking VS2017 (15.9.28307.960) found at:gyp verb find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""gyp verb find VS - found ""Visual Studio C++ core features""gyp verb find VS - found VC++ toolset: v141gyp verb find VS - found Windows SDK: 10.0.17763.0gyp info find VS using VS2017 (15.9.28307.960) found at:gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""gyp info find VS run with --verbose for detailed informationgyp verb build/config.gypi creating config filegyp verb build/config.gypi writing out config file: C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-node\build\config.gypigyp verb config.gypi checking for gypi file: C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-node\config.gypigyp verb common.gypi checking for gypi file: C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-node\common.gypigyp verb gyp gyp format was not specified, forcing ""msvs""gyp info spawn C:\Python27\python.exegyp info spawn args [gyp info spawn args   'C:\\Users\\...\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\gyp\\gyp_main.py',gyp info spawn args   'binding.gyp',gyp info spawn args   '-f',gyp info spawn args   'msvs',gyp info spawn args   '-I',gyp info spawn args   'C:\\Users\\...\\OneDrive\\...\\node_modules\\@tensorflow\\tfjs-node\\build\\config.gypi',gyp info spawn args   '-I',gyp info spawn args   'C:\\Users\\...\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\addon.gypi',gyp info spawn args   '-I',gyp info spawn args   'C:\\Users\\...\\AppData\\Local\\node-gyp\\Cache\\14.13.1\\include\\node\\common.gypi',gyp info spawn args   '-Dlibrary=shared_library',gyp info spawn args   '-Dvisibility=default',gyp info spawn args   '-Dnode_root_dir=C:\\Users\\...\\AppData\\Local\\node-gyp\\Cache\\14.13.1',gyp info spawn args   '-Dnode_gyp_dir=C:\\Users\\...\\AppData\\Roaming\\npm\\node_modules\\node-gyp',gyp info spawn args   '-Dnode_lib_file=C:\\\\Users\\\\...\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\14.13.1\\\\<(target_arch)\\\\node.lib',gyp info spawn args   '-Dmodule_root_dir=C:\\Users\\...\\OneDrive\\...\\node_modules\\@tensorflow\\tfjs-node',gyp info spawn args   '-Dnode_engine=v8',gyp info spawn args   '--depth=.',gyp info spawn args   '--no-parallel',gyp info spawn args   '--generator-output',gyp info spawn args   'C:\\Users\\...\\OneDrive\\...\\node_modules\\@tensorflow\\tfjs-node\\build',gyp info spawn args   '-Goutput_dir=.'gyp info spawn args ]gyp: Undefined variable module_name in binding.gyp while trying to load binding.gypgyp ERR! configure errorgyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (C:\Users\...\AppData\Roaming\npm\node_modules\node-gyp\lib\configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:314:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:276:12)gyp ERR! System Windows_NT 10.0.18362gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\...\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--verbose""gyp ERR! cwd C:\Users\...\PROJECT\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v14.13.1gyp ERR! node-gyp -v v7.1.0gyp ERR! not ok","['Please check related issue [here](https://github.com/tensorflow/tfjs/issues/3999) ! Thank you=====', ""I looked through the related issue, but was not able to fix the problem, unfortunately. I still see the same 'module_name undefined' error when I run node-gyp.This is what I tried after reading the related issue:TRIED    npm rebuild @tensorflow/tfjs-node build-addon-from-sourceRESULT  no changeTRIED    npm install npm@latest -g  // note that I already had 6.14.8, but ran this anywayRESULT  no change====="", 'can you please share your package.json ?=====', 'Sure, it\'s below. It\'s very simple. I have rebuilt it many (many) times. I keep deleting node_modules and reinstalling, and trying different folders, and global vs local packages. For the record, I have been using basic @tensorflow/tfjs without any problems, as a node program. Now I have a bug with model save/load, so I\'m trying to move to tfjs-node (...and of course I want the performance boost from C++  :)  )package.json{  ""name"": ""PROJECT"",  ""version"": ""1.0.0"",  ""description"": """",  ""main"": ""index.js"",  ""scripts"": {    ""test"": ""echo \\""Error: no test specified\\"" && exit 1""  },  ""author"": """",  ""license"": ""ISC"",  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^2.5.0"",    ""simple-statistics"": ""^7.3.0""  }}=====', ""same with me it's seems some of variables in `binding.gyp` are not defined?====="", '@joemullenix-ks can you use the latest tfjs-node 2.7.0? I have uploaded the pre-compiled windows addons for 2.7.0, please let me know if that solve the problem for you. =====', ""It looks like the problem still exists. I cleared my project, and reinstalled tfjs-node. My dependency is now 2.7.0.When I run in VS Code, I still see the same error ('tfjs_binding.node not found'). Also, when I run node-gyp configure, I see the same undefined variable error.Thanks for trying! Please let me know if I can do anything to help track this down.====="", '@joemullenix-ks Looks like your issue is related to this bug https://github.com/tensorflow/tfjs/issues/4257, basically the dll file is not copied properly, when the node version is > 14.x=====', 'Looks good! I manually copied the dll as mentioned in #4257, and tfjs-node now imports and runs correctly. Thanks!!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4052"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4052"">No</a>=====', ""> Looks good! I manually copied the dll as mentioned in #4257, and tfjs-node now imports and runs correctly. Thanks!!Hi, I am encountering the same problem. How did you copied the dll? I cannot find it anywhere.Edit: Well, I just forced everything to look the same as it requires, like I see napi-v7 in my module and I created a soft link name napi-v8. After that, it seems like installed without a problem. In case anyone is facing the same issue and doesn't known much about node.js just like me, here is the code in powershell.```{posh}npm install @tensorflow/tfjs-node --ignore-scripts #prevent downloaded files to be deleted on failed installcd .\\node_modules\\@tensorflow\\tfjs-node\\lib\\sudo New-Item -Path .\\napi-v8 -ItemType SymbolicLink -Value .\\napi-v7cd ..\\..\\..\\..\\npm rebuild @tensorflow/tfjs-node --build-from-source```=====""]",0
https://github.com/tensorflow/tfjs/issues/4811,TypeError: Cannot read property 'length' of null (dilations is null in depthwiseConv2dNativeBackpropInput_),8,closed,2021-03-14T21:54:51Z,2021-08-17T14:35:59Z,"**System information**- From the [simple-object-detection](https://github.com/tensorflow/tfjs-examples/tree/master/simple-object-detection) example- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tried both on Mac OS and Windows  and Windows WSL- TensorFlow.js installed from (npm or script link): npm library- TensorFlow.js version (use command below): tested on ^2.6.0 and ^3.3.0**Describe the current behavior**Playing with the [simple-object-detection](https://github.com/tensorflow/tfjs-examples/tree/master/simple-object-detection) example. Ran `npm run train` and the following happensStep 1: (Phase 1 of 2: initial transfer learning) - this runs without any problems```javascript  const tBegin = tf.util.now(),  console.log(`Generating ${args.numExamples} training data...`),  const { images, targets } = await trainingData(),  const { model, fineTuningLayers } = await buildObjectDetectionModel(),  model.compile({ loss: customLossFunction, optimizer: tf.train.rmsprop(5e-3) }),  model.summary(),  // Initial phase of transfer learning.  // console.log('Images Shape:', images.shape),  // console.log('Targets Shape', targets.shape),  console.log('Phase 1 of 2: initial transfer learning'),  await model.fit(images, targets, {    epochs: args.initialTransferEpochs,    batchSize: args.batchSize,    validationSplit: args.validationSplit,    callbacks: args.logDir == null ? null : tfn.node.tensorBoard(args.logDir, {      updateFreq: args.logUpdateFreq    })  }),```Step 2: (Phase 2 of 2: fine-tuning phase) - this is where it breaks```javascript  // Fine-tuning phase of transfer learning.  // Unfreeze layers for fine-tuning.  for (const layer of fineTuningLayers) {    console.log(fineTuningLayers),    layer.trainable = true,  }  model.compile({ loss: customLossFunction, optimizer: tf.train.rmsprop(2e-3) }),  model.summary(),  // Do fine-tuning.  // The batch size is reduced to avoid CPU/GPU OOM. This has  // to do with the unfreezing of the fine-tuning layers above,  // which leads to higher memory consumption during backpropagation.  console.log('Phase 2 of 2: fine-tuning phase. Top Layer Name:', topLayerName, 'Group Names:', topLayerGroupNames),  await model.fit(images, targets, {    epochs: args.fineTuningEpochs,    batchSize: Math.round(args.batchSize / 2),    validationSplit: args.validationSplit,    callbacks: args.logDir == null ? null : tfn.node.tensorBoard(args.logDir, {      updateFreq: args.logUpdateFreq    })  }),```I get this error after `model.fit(images, targets, {...` is executed:![image](https://user-images.githubusercontent.com/7416676/111085676-9bb28300-84d5-11eb-9cdf-af32be17e081.png)I debugged down a bit further - turns out that somewhere at the kernel input (I think??), it's giving a dilation value of `null` (should be `number` or `number[]`).![image](https://user-images.githubusercontent.com/7416676/111085448-ab7d9780-84d4-11eb-8226-fe246256050d.png)![image](https://user-images.githubusercontent.com/7416676/111085562-13cc7900-84d5-11eb-82f8-31eed68c301c.png)Please advise, I'm going off the tfjs-examples repo simple-object-dectectionhttps://github.com/tensorflow/tfjs-examples/blob/master/simple-object-detection/package.json...then just running `npm install` and `npm run train`","[""Still broken, but found a workaround (though probably the model is not even transfer learning)1. train.js change the tensorflow import from `@tensorflow/tfjs` to `@tensorflow/tfjs-node`2. replaced `const topLayerGroupNames = ['conv_pw_9', 'conv_pw_10', 'conv_pw_11'],` to `const topLayerGroupNames = [ 'conv_pw_11'],`Both changes are required, one without the other leads to the same problem. I have no idea why this fixes it.====="", '@opiepj #2 did it for me. You are my hero!=====', '@opiepj can we close this issue ? =====', ""No it doesn't work, I had to keep layers 10-11 frozen... was only able to transfer learn on the 12th layer.Outputs are still wack ====="", '+1=====', 'After five month, anything new?=====', 'Related PR has been merged , the change will be available in next release. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4811"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4811"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2959,Update proto info for TF SavedModel input/output TensorShape,0,open,2020-03-24T17:54:38Z,2021-11-05T18:56:43Z,"The TensorShape field in TF SavedModel SignatureDef has been updated: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto#L257The corresponding part need to be updated in tfjs-node, and perhaps tfjs-converter.",[],0
https://github.com/tensorflow/tfjs/issues/98,loss in ModelCompileConfig doesn't accept functions,2,closed,2018-04-05T05:25:59Z,2018-10-26T19:10:04Z,"#### TensorFlow.js version- 0.7.0#### Browser version- N/A#### Describe the problem or feature request> String (name of objective function) or objective function.> https://github.com/tensorflow/tfjs-layers/blob/4905b2b5106f33df2ff80c091b655703ec016094/src/engine/training.ts#L591The `loss` property in `ModelCompileConfig` looks to say accepting objective functions, but the implementation accepts only strings.#### Code to reproduce the bug / link to feature request- https://github.com/tensorflow/tfjs-layers/blob/4905b2b5106f33df2ff80c091b655703ec016094/src/engine/training.ts#L597","['It looks like Keras supports losses as a non-string argument: https://keras.io/losses/#usage-of-loss-functions.We can easily extend this to take a function that allows you to define a loss at the ops-level.Thanks for filing the issue.=====', 'Confirming this is fixed now.See codepen: https://codepen.io/caisq/pen/QZYZEZ=====']",0
https://github.com/tensorflow/tfjs/issues/3085,Possible to run prediction directly in server side only?,2,closed,2020-04-16T04:21:11Z,2020-06-05T18:36:02Z,"I have scripts that can run in browsers. However, I would like to enhance the performance by migrating the scripts to the server side. It seems not working.**UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'loadObjectDetection' of undefined**```const tf = require('@tensorflow/tfjs'),automl = require('@tensorflow/tfjs-automl'),fs = require('fs'),fs.readFile('./image.jpg', function (err, data) {    if (err) throw err,    async function run() {        const model = await tf.automl.loadObjectDetection( './model.json' ),        const options = {score: 0.1, iou: 0.1, topk: 20},        async function prediction(){            const predictions = await model.detect(data, options),            if(predictions.length>0){                //console.log(predictions.length),                for(var i=0, i<predictions.length, i++){                    console.log( predictions[i].label + "" "" + predictions[i].score ),                }            }            prediction(),        }        prediction(),    },    run(),}),```","[""@kezmanh, I did not try this myself, but following line looks wrong```const model = await tf.automl.loadObjectDetection( './model.json' ),```can you replace it with```const model = await automl.loadObjectDetection( './model.json' ),```====="", 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/1191,loadFrozenModel now returns Promise<tf.FrozenModel | tf.FrozenModelJSON> and breaks TypeScript users.,0,closed,2019-02-02T20:16:44Z,2019-02-04T19:19:30Z,This PR changed the return type (which forces users to now cast as tf.FrozenModel): https://github.com/tensorflow/tfjs-converter/pull/264Let's fix this and get a patch out ASAP.,[],0
https://github.com/tensorflow/tfjs/issues/4706,Lint with gts,0,open,2021-02-17T20:04:46Z,2021-06-03T16:15:37Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): N/A- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**Code linting is currently done by vscode, but we should probably lint with something like [gts](https://github.com/google/gts).**Will this change the current api? How?**It may change how code is formatted, but I think the current vscode formatting is similar to gts.**Who will benefit with this feature?**Everyone.**Any Other info.**",[],0
https://github.com/tensorflow/tfjs/issues/5632,"[webgl] ""Switching WebGL + CPU backends"" Tests do not Run",2,open,2021-09-16T17:06:03Z,2021-09-20T17:52:22Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 5.10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): Local build- TensorFlow.js version (use command below): Tried on [fe0dcb738daec71f950d93fd466825e21a915e25](https://github.com/tensorflow/tfjs/commit/fe0dcb738daec71f950d93fd466825e21a915e25) and [19ca872919c196787c2fbf9a8aa57000cd12dff0](https://github.com/tensorflow/tfjs/commit/19ca872919c196787c2fbf9a8aa57000cd12dff0).- Browser version: Chrome 93- Tensorflow.js Converter Version: n/a**Describe the current behavior**[Switching WebGL + CPU backends](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine_test.ts#L587-L601) tests do not run when running `karma start` (or `yarn test-dev`) in `tfjs-backend-webgl`. They seem to not run because `_engine.ENGINE.backendNames()` does not contain `webgl` even though the webgl tests are being run.**Describe the expected behavior**The above tests run.**Standalone code to reproduce the issue**In `tfjs-backend-webgl`, ```yarn build-depsyarn karma start```Then, in the Chrome debugger, open `engine_test.js` and set a breakpoint in the `predicate` function of the `Switching WebGL + CPU backends` tests. Check `_engine.ENGINE.backendNames()` and you'll see that `webgl` is missing.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.![webgl-no-test](https://user-images.githubusercontent.com/1474501/133653885-d68894d7-b523-4344-84f3-723895a63ec4.png)","['@mattsoulanille backend webgl register itself in the tfjs-backend-webgl/src/base.ts,is that file included in the test suite?=====', ""@pyu10055 backend webgl does seem to register itself in `base.ts`, but this happens after the engine test runs, which seems to be the wrong order. I'll see if I can get it to run before.=====""]",1
https://github.com/tensorflow/tfjs/issues/557,UglifyJs error during build,3,closed,2018-07-28T19:20:56Z,2018-08-10T14:48:23Z,"I use tfjs in a Webpack project and got this error message during build:```ERROR in main-c0b5163b85a17b3c3caa.js from UglifyJsUnexpected token: name ($oneOfFields) [./node_modules/@tensorflow/tfjs-converter/dist-es6/data/compiled_api.js:324,0][main-c0b5163b85a17b3c3caa.js:112140,12]```I don't have dependency on UglifyJs in my project, so I assume this is a problem internal to tfjs.#### TensorFlow.js version`""@tensorflow/tfjs"": ""^0.11.7""`","[""Uglify is Webpack's default config in  'production' mode.(For Webpack version 4.0 +)I think you should not build tfjs node_module, just ignore it ....I also use webpack and it works well.====="", '@dsmilkov I think this issue can be closed.=====', 'Thanks. Closed=====']",0
https://github.com/tensorflow/tfjs/issues/4725,tensorflowjs_converter: Add Support for Operations used by BigQuery ML models,4,closed,2021-02-22T17:51:25Z,2021-04-26T16:16:05Z,"**System information**- TensorFlow.js version (you are using): 3.1.0 (version 2.4.1 of tensorflow)- Are you willing to contribute it (Yes/No): I'm not familiar enought with this or tensorflow's codebase so I don't think I could write the code to fix this issue. But I think I can contribute in anything else you need, like testing, reviewing the code or giving more context. **Describe the feature and the current behavior/state.**The company I work for is building a Deep Neural Network Regressor (DNN_REGRESSOR) using BigQuery ML and we plan on deploying in a Node.js environment. The initial tests worked fine but recently when we tried to convert the Saved Model to Tensorflowjs it failed with the following error:```shValueError: Unsupported Ops in the model before optimizationSparseSegmentMean, LookupTableSizeV2, StringToHashBucketFast, SparseReshape, SparseFillEmptyRows```I'm converting the model in this way: ```shtensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --signature_name=predict --weight_shard_size_bytes=25000000 --saved_model_tags=serve source target```As a result we can't convert model nor deploy it, rendering it unusable for us. I'm using tabular data with high cardinality categorical features and these ones seem to be at the root of the problem.BigQuery ML doesn't give any options in which operations to include or not, so we depend on you giving support for these operations or maybe offering a workaround that I don't know of.**Will this change the current api? How?**Definitely, because it will require adding those Operations as supported, but that shouldn't break any working code.**Who will benefit with this feature?**Every user that is producing *BigQuery ML* models and then using them with Tensorflowjs can benefit from this. We're using tabular data in a pretty straightforward way so I don't think this is an edge case that you won't encounter anymore and it would probably be a nice synergy to have ML models created in BigQuery that work smoothly in Tensorflowjs.","['cc @pyu10055 =====', ""@rthadur @pyu10055 I have been familiarizing myself with the library. I'll try adding at least LookupTableSizeV2 and StringToHashBucketFast myself. I'll start with LookupTableSizeV2. I think this only requires returning the size of the Hash Table and should be pretty similar to other operations like LookupTableFind. Do you think this approach is OK? Do you agree on adding those operations?I read the contributing guidelines but would appreciate any help or guidance you can offer. Thanks in advance!====="", ""@rthadur @pyu10055 [This PR](https://github.com/tensorflow/tfjs/pull/4755) adds one of the operations I mentioned previously. I haven't signed the CLA on behalf of my employer yet but will do it soon. I'd appreciate your review. Thanks in advance!====="", '@fneubaum-sulvo Thank you, will review the PR.=====']",1
https://github.com/tensorflow/tfjs/issues/5663,interoperability - onxx exchange format,2,open,2021-09-25T15:58:08Z,2021-09-25T16:53:47Z,i would like to pre-train network models on the backend using just any other library (i prefer using spark for this operations) band then use the results within the tensor.js Framework in the frontend ...can this already be achieved somehow? i am talking about importing / deserializationi think this feature would be a game changer for this library.,"['**onnx** provides python package that converts onnx model to tensorflow format,  check onnx documentation (good starting point is <https://github.com/onnx/onnx-tensorflow/>)  once model is in tensorflow format, use `tensorflowjs_converter` from python package (maintained by tfjs team) to convert from tf saved model format to tfjs graph model  =====', 'sounds like a promising approach. i will surely have a look. thanks for the advice. i have tried to find this information on. the official tensorflow.js doc but it seemed to me that only Json (de)serialization was possible, which is not true. . probably a note in the doc to this issue would be good for fiture readers.=====']",0
https://github.com/tensorflow/tfjs/issues/1181,feature request : extra information to model,4,closed,2019-02-01T12:57:32Z,2019-02-06T00:25:45Z,"#### TensorFlow.js versionNA#### Browser versionNA#### Describe the problem or feature requestHello,I'd like to have a standardized way to save extra_information inside a model in python and have it passed all the way to tensorflowjs.The use case for example is when I have ""Valid"" Conv1D but None as time dimension, I need to know what the minimal size to pass as input that won't result in an error.An other use case, is specifying a License/ author for a model.But simply a way of passing a dictionary as json inside a field would suffice.#### Code to reproduce the bug / link to feature requestIn python : ```model.extraInfo = {""license"":""MIT"", ""minInputShape"":(None,10,None),""author"":""unrealwill""}tfjs.converters.save_keras_model(model, url)```and then accessing it in javascript using : ```tf.loadmodel( url ) .then( function(model){ console.log(model.extraInfo.license) , console.log(model.extraInfo.minInputShqpe) , console.log(model.extraInfo.author) , }```Thanks","['Hi @unrealwill,Your use case looks best handled by having a custom solution on top of tf.js. Can you serve your own metadata (e.g. `metadata.json`) along with the model?@davidsoergel and @caisq who might have some thoughts about custom metadata.=====', 'Please see #762 for a prior discussion of this.=====', 'Thanks. Closing this as a duplicate of #762=====', 'Thanks=====']",0
https://github.com/tensorflow/tfjs/issues/5704,GPU utilization is very poor on multi-process or concurrently trained models,4,open,2021-10-09T00:16:32Z,2021-10-21T23:57:11Z,"**System information**- TensorFlow.js v3.9.0 (Windows 10), `tfjs-node-gpu`- Are you willing to contribute it (Yes/No): Possibly if not overly complex changes**Describe the feature and the current behavior/state.**On average most model training greatly underutilizes the GPU, due to the nature of how GPU's scale. We all know that large models (tens or hundreds of millions of params) perform far more efficient (ops/watt) than smaller models (< 10M params), that is simply the nature of accelerators. The issue with tfjs is that there is no way for these smaller models to be trained in parallel (across multiple worker processes) OR concurrently with any significant benefits (more on this later).Let's take a look at the state of parallel vs concurrent training in tensorflow:**Multi-process**Multi-process scales linearly in the case of multiple GPU's, but this story is about the utilization of each of those individual GPU's. Due to tfjs lack of memory allocation limits, multi-process training on a GPU is not possible today. The simple solution would be to add a [similar interface to manage memory in Tensorflow 2 core](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)**Concurrency**Unlike multi-process training on a single GPU, concurrent training of models within a single process is actually possible today. In fact I've trained dozens and even hundreds of (<10M param) models, concurrently. While I can't speak to exactly why the performance and GPU utilization is so poor, I've run dozens of simple tests where-in at best I'll see 50-70% **increase** in GPU utilization (not to be confused with total utilization). While these benefits sound nice, it's quite devastating when we're talking sub 10% GPU utilization. It's not clear to me if this is a bug (or design flaw) in tfjs or tfcore, but I see no reason why we should not be able to fully utilize a GPU if we have enough models to saturate the GPU.Consider this contrived example (based on actual testing):* I can train a single ~130M param model to exceed 85% of GPU utilization (rtx 3080 ti for reference)* But training two ~65M param models concurrently will utilize less than 55% of the GPU* Adding additional ~65M param models may see slightly higher GPU utilization, but never more than 60%* For even smaller models, any amount of concurrency (even in the hundreds) will never even see 25% utilizationThe expectation is that so long as we've not run out of GPU memory, or have some sort of model or CPU inefficiency that prevents full utilization, we should be able to take full advantage of the GPU by running 2 or more models, concurrently.**Will this change the current api? How?**1. Enable multi-process GPU training by adding a [similar interface to manage memory in Tensorflow 2 core](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth).2. Resolve concurrency performance -- unclear if this would require an interface change or bug fix**Who will benefit with this feature?**This could benefit a lot of engineers, especially those that need to improve GPU utilization with smaller models.**Any Other info.**I've put hundreds of hours into tfjs and **love** it! Would love to make tfjs a more serious real-world training solution and a true first-class alternative to Python-based training. Thank you!","['@asilvas Thank for providing the detail performance analysis of concurrent training with TFJS. One thing I want to confirm first, are you running training within browser or using our tfjs-node backend on nodejs?This would give us better idea where to investigate. thanks=====', 'Hi @pyu10055 -- This is `tfjs-node-gpu` with CUDA. =====', '@asilvas Can you share your setup for training multiple models with TFJS? It will help us to identify the bottleneck quicker. Thanks!=====', ""Sure thing. Here's a contrived test: https://gist.github.com/asilvas/29c566709d0565a9638b7605ecf8e283Produced results:```node gpu-concurrency-test.js 1Training 1 x 24M param models took 25194ms, or 25194ms per modelnode gpu-concurrency-test.js 2Training 2 x 24M param models took 45338ms, or 22669ms per modelnode gpu-concurrency-test.js 5Training 5 x 24M param models took 102100ms, or 20420ms per modelnode gpu-concurrency-test.js 10Training 10 x 24M param models took 191775ms, or 19178ms per model```Despite using less than 5% of the GPU with a single model, training two concurrently netted only ~11% gains (despite a near idle GPU). Increasing concurrency slightly increases overall throughput, but by very insignificant gains.I didn't do much testing as I wasn't worried about the usefulness of these test models, but the results were consistent with real-world tests I've done.=====""]",1
https://github.com/tensorflow/tfjs/issues/5336,WASM throws memory access out of bounds in Chrome but not Firefox/Edge,6,closed,2021-07-16T13:02:48Z,2021-08-12T18:35:06Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):Yes, at least as pertains to the web part.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10- TensorFlow.js installed from (npm or script link):importScripts(""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""),importScripts(""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js""),- Browser version:Chrome 91.0.4472.164**Describe the current behavior**When running the ProGan model on a website in web workers, when using WASM backend, Firefox and Edge will use tfjs-backend-wasm-simd.wasm while Chrome will use tfjs-backend-wasm-threaded-simd.wasm. The code will execute fine on Firefox and Edge but fail on Chrome with unclear exceptions. Best guess, multithreaded only works on the main js module and fails through workers.However, when setting WASM_HAS_MULTITHREAD_SUPPORT to false, Chrome uses the same tfjs-backend-wasm-simd.wasm file and throws a memory access out of bounds exception. Again, this works fine with the other browsers. This is the same with WASM_HAS_SIMD_SUPPORT set to false as well.**Describe the expected behavior**Chrome should load the model instead of throwing the error.NOTE: I'm running wasm in worker threads not only to achieve multithreading on browsers other than Chrome, but primarily to not block the UI thread. This is for benchmarking, not meant to be real-time.**Standalone code to reproduce the issue**https://github.com/tauber/proganYou can run the demo from http://objectcoded.com/progan/progan.htmlSelect any number of threads and WASM for the backend. Approx. ~100MB model.Exception details:Uncaught (in promise) RuntimeError: memory access out of bounds    at tfjs-backend-wasm-simd.wasm:0xa8b    at Pa (tfjs-backend-wasm-simd.wasm:0x1aaa2)    at BackendWasm.move (backend_wasm.ts:83)    at BackendWasm.write (backend_wasm.ts:54)    at e.t.makeTensor (engine.js:813)    at Sk (tensor_ops_util.js:75)    at Tk (tensor.js:56)    at Ak (io_utils.js:224)    at e.t.loadSync (graph_model.js:160)    at e.<anonymous> (graph_model.js:134)(anonymous) @ tfjs-backend-wasm-simd.wasm:0xa8bPa @ tfjs-backend-wasm-simd.wasm:0x1aaa2BackendWasm.move @ backend_wasm.ts:83BackendWasm.write @ backend_wasm.ts:54t.makeTensor @ engine.js:813Sk @ tensor_ops_util.js:75Tk @ tensor.js:56Ak @ io_utils.js:224t.loadSync @ graph_model.js:160(anonymous) @ graph_model.js:134c @ runtime.js:63(anonymous) @ runtime.js:293(anonymous) @ runtime.js:118bv @ runtime.js:747o @ runtime.js:747async function (async)render_gan_image @ render-gan.js:73","[""Hi @tauber,Thanks for the report! Looks like the problem is version mismatch. The `progan/modules/tfjs-backend-wasm-simd.wasm` file in your [repo](https://github.com/tauber/progan) is from 3.7.0, but the `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js` is loading 3.8.0 (which was released 23 days ago, 2 days before your repo was created). I replaced `tfjs-backend-wasm-simd.wasm` with the one from 3.8.0 and your app works without problems (great app btw!). I am not entirely sure why things work on other browsers though.. One thing to do to help with this is to call the following in your worker script (maybe after the two `importScripts` calls). This will ask the wasm backend to load the wasm binary files from jsdelivr so you don't need to copy them manually. I tried it and it seems to [work](https://drive.google.com/file/d/1utcV8WEiG6AQlkdYJbV7dCyazt1jCyML/view?usp=sharing).```jstf.wasm.setWasmPaths(    'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/'),```Thanks!====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5336"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5336"">No</a>=====', ""Great work! Thank you! I wanted to use the setWasmPaths api but couldn't because I didn't find any documentation that uses the wasm object. Existing blog posts call the function directly from tf, which did not work.====="", 'Thank you @tauber , did you mean to say the documentation here https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md#using-bundlers  is not clear ?=====', ""@rthadur Looks like the doc didn't mention what to do when importing the wasm backend through the script tag. I will update it. Thanks!====="", ""Now I see the comment: '// or tf.wasm.setWasmPaths when using <script> tags.'I don't know why I missed this. Making it more obvious would definitely help exhausted developers :)Thank you again for your help with this issue.=====""]",1
https://github.com/tensorflow/tfjs/issues/5492,posibility of a 3.8.1 release with updated adm-zip,3,closed,2021-08-14T14:09:19Z,2021-08-17T16:58:51Z,"Hi, @lina128 @pyu10055, there is a high severity vulnerability introduced by package **adm-zip**:### Issue Description   I noticed that a vulnerability is introduced in **_@tensorflow/tfjs-node@3.8.0_**:     Vulnerability **SNYK-JS-ADMZIP-1065796** (high severity) affects package **_adm-zip_** (versions:<0.5.2):  [https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796)  The above vulnerable package is referenced by **_@tensorflow/tfjs-node@3.8.0_** via:`@tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`Since **_@tensorflow/tfjs-node@3.8.0_** ([2,176 downloads per week](https://www.npmjs.com/package/@tensorflow/tfjs-node/v/3.8.0?activeTab=versions)) is referenced by **18** downstream projects (e.g., [stills 21.3.6](https://www.npmjs.com/package/stills) (latest version), [node-red-contrib-facial-recognition 0.30.105](https://www.npmjs.com/package/node-red-contrib-facial-recognition) (latest version), [@nlpjs/open-question 4.22.0](https://www.npmjs.com/package/@nlpjs/open-question) (latest version), [twitterfollowerexplorer 3.1.11](https://www.npmjs.com/package/twitterfollowerexplorer) (latest version), [roboflow-node 0.2.16](https://www.npmjs.com/package/roboflow-node) (latest version)), the high severity vulnerability [**SNYK-JS-ADMZIP-1065796**](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796) can be propagated into these downstream projects and expose security threats to them via the following package dependency paths: (1)`stills@21.3.6 ➔ @tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`(2)`twitterfollowerexplorer@3.1.11 ➔ @tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`  **......**If **_@tensorflow/tfjs-node@3.8.\*_** removes the vulnerable package from the above version, then its fixed version can help downstream users decrease their pain.**Given the large number of downstream users, could you help update your package to remove the vulnerability from @tensorflow/tfjs-node@3.8.0 ?**### Fixing suggestionsIn **_@tensorflow/tfjs-node@3.8.1_**, maybe you can kindly try to perform the following upgrade(**not crossing major version**) :`adm-zip ^0.4.11 ➔ ^0.5.2`, **Note:**_**adm-zip@0.5.2**(>=0.5.2) has fixed the vulnerability [**SNYK-JS-ADMZIP-1065796**](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796)._Thank you for your attention to this issue and welcome to share other ways to resolve the issue.Best regards,^_^","[""Thank you @evansrobert , submitted a fix , let's wait for PR to get reviewed.====="", '@rthadur Thanks for your feedback.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5492"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5492"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1042,Converter should warn users if their model weights overflow or underflow float16 bounds,0,open,2018-12-20T12:07:42Z,2021-09-03T17:44:59Z,This way users have advance notice of the fact that their model may not work as expected on mobile devices that only support half-precision floating point.,[],0
https://github.com/tensorflow/tfjs/issues/5716,Error when running yarn test in tfjs-layers,4,closed,2021-10-12T13:08:14Z,2021-10-13T21:24:41Z,"- TensorFlow.js version: 3.9.0Whenever I try to run the command ``` yarn test```inside the tfjs-layers folder I get this error:```yarn run v1.22.15$ yarn test-dev$ ibazel run :tfjs-layers_test --test-output=streamediBazel [1:02PM]: Querying for files to watch...Loading: 0 packages loadedERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelLoading: 0 packages loadedLoading: 0 packages loadediBazel [1:02PM]: Bazel query failed: exit status 7Loading: 0 packages loadedERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelLoading: 0 packages loadedLoading: 0 packages loadediBazel [1:02PM]: Bazel query failed: exit status 7iBazel [1:02PM]: Running :tfjs-layers_testLoading: Loading: 0 packages loadedERROR: Skipping ':tfjs-layers_test': no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelWARNING: Target pattern parsing failed.ERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelINFO: Elapsed time: 0.074sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)FAILED: Build did NOT complete successfully (0 packages loaded)iBazel [1:02PM]: Error running Bazel exit status 1error Command failed with exit code 4.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 4.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```I tried with a freshly cloned tfjs and the same error happened, what is the missing step","[""Thanks for the bug report! This is a bug in the `package.json` test script. `tfjs-layers_test` should be `tfjs-layers_webgl2_test`. It was not caught by CI since CI does not run tests through `yarn test` and uses a different method instead. I'll submit a fix.====="", ""Thank you @mattsoulanille. After changing my package.json locally and running 'yarn test' i got another error:```'external' is not recognized as an internal or external command, operable program or batch file. ```My os is Windows 10====="", ""This looks like an issue with Bazel Karma tests on Windows. We're working on fixing this in #5660, and you can see any updates we post in that issue. At the moment, there's not a good workaround that we know of, but fixing this is high priority since it blocks almost all Windows development.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5716"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5716"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2130,Feature request: `tf.spectral.fft2d`,0,open,2019-10-02T17:37:03Z,2019-10-02T21:13:42Z,A 2-dimensional fft would be helpful for some image processing use-cases. One example would be for object tracking. I can work on adding this op.,[],0
https://github.com/tensorflow/tfjs/issues/1121,tfjs-node-gpu 0.2.3 fails to install on Windows,16,closed,2019-01-23T15:06:32Z,2019-03-16T06:24:16Z,"#### TensorFlow.js versiontensorflow/tfjs-node-gpu: 0.2.3#### Browser versionnode: v8.15.0#### Describe the problem or feature request`npm i @tensorflow/tfjs-node-gpu` results in an error.0.2.2 also fails, while 0.2.1 works, this means the breaking change was added in 0.2.2.If I download the .zip of 0.2.3 manually and then run `node scripts/install.js` it works.#### Code to reproduce the bug / link to feature request```D:\tmp\tfjs_0_2_3>npm i @tensorflow/tfjs-node-gpu> @tensorflow/tfjs-node-gpu@0.2.3 install D:\tmp\tfjs_0_2_3\node_modules\@tensorflow\tfjs-node-gpu> node scripts/install.js gpu download* Downloading libtensorflow[==============================] 11080854/bps 100% 0.0s* Building TensorFlow Node.js bindingsD:\tmp\tfjs_0_2_3\node_modules\@tensorflow\tfjs-node-gpu\scripts\install.js:171      throw new Error('node-gyp rebuild failed with: ' + err),      ^Error: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuildgyp ERR! build errorgyp ERR! stack Error: `C:\Program Files (x86)\MSBuild\14.0\bin\msbuild.exe` failed with exit code: 1gyp ERR! stack     at ChildProcess.onExit (C:\Users\pavel\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\build.js:262:23)gyp ERR! stack     at emitTwo (events.js:126:13)gyp ERR! stack     at ChildProcess.emit (events.js:214:7)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)gyp ERR! System Windows_NT 10.0.17134gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\pavel\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild""gyp ERR! cwd D:\tmp\tfjs_0_2_3\node_modules\@tensorflow\tfjs-node-gpugyp ERR! node -v v8.12.0gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok    at cp.exec (D:\tmp\tfjs_0_2_3\node_modules\@tensorflow\tfjs-node-gpu\scripts\install.js:171:13)    at ChildProcess.exithandler (child_process.js:283:5)    at emitTwo (events.js:126:13)    at ChildProcess.emit (events.js:214:7)    at maybeClose (internal/child_process.js:915:16)    at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5)npm WARN tfjs_0_2_3@1.0.0 No descriptionnpm WARN tfjs_0_2_3@1.0.0 No repository field.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node-gpu@0.2.3 install: `node scripts/install.js gpu download`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node-gpu@0.2.3 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     C:\Users\pavel\AppData\Roaming\npm-cache\_logs\2019-01-23T14_41_47_959Z-debug.log```[2019-01-23T14_41_47_959Z-debug.log](https://github.com/tensorflow/tfjs/files/2787764/2019-01-23T14_41_47_959Z-debug.log)","['Hi @iacovlev-pavel Can you install `windows-build-tools` (https://github.com/tensorflow/tfjs-node/blob/master/WINDOWS_TROUBLESHOOTING.md) and try again? =====', 'Hello @kangyizhang Fresh VM from https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/Installed `node v8.15.0`Ran `npm --vs2015 install --global windows-build-tools`Ran `npm i @tensorflow/tfjs-node-gpu`and got the same error.![virtualbox_msedge - win10_28_01_2019_11_40_47](https://user-images.githubusercontent.com/1787775/51827243-95f78480-22f1-11e9-844e-0b7f07f97ef6.png)I am doing 2 non standard things here: node 8 and vs2015. Will do additional tests and see if using the latest versions of these applications change anything.=====', 'Tried with Node 10 and vs 2017, got the same result.![virtualbox_msedge - win10_28_01_2019_15_08_49](https://user-images.githubusercontent.com/1787775/51838343-ad913600-230e-11e9-9746-3b52174e2ed7.png)=====', ""The tool for building native Node.js plugins `node-gyp` currently has a variety of issues with Visual Studio 2017. I'd recommend following this issue for some solutions - https://github.com/nodejs/node-gyp/issues/1056 - not a great option. I feel your pain because I have two Windows workstations and each one required a different solution.====="", '@nkreeger Thank you for the reply, will look into the recommended issue. I also forgot to restart the VM after installing vs2015 and later vs2017, will give another go Tomorrow.=====', 'I still can\'t seem to install latest tfjs-node-gpu. Tried fresh VMs with both vs2015 and vs2017 and I always end up with the above mentioned error.I looked at it a bit more Today, and here is the actual `node-gyp` error:```D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package>if not defined npm_config_node_gyp (node ""C:\\Users\\pavel\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\npm-lifecycle\\node-gyp-bin\\\\..\\..\\node_modules\\node-gyp\\bin\\node-gyp.js"" rebuild )  else (node ""C:\\Users\\pavel\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" rebuild )Warning: Missing input files:D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\deps\\include\\tensorflow\\c\\eager\\c_api.hD:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\deps\\include\\tensorflow\\c\\c_api.hBuilding the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.  deps-stage  (node:3480) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, rename \'D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\deps\\lib\\tensorflow.dll\' -> \'D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\build\\Release\\tensorflow.dll\'  (node:3480) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)  (node:3480) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.  generate_def  fs.js:646    return binding.open(pathModule._makeLong(path), stringToFlags(flags), mode),                   ^CUSTOMBUILD : error : ENOENT: no such file or directory, open \'D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\deps\\include\\tensorflow\\c\\c_api.h\' [D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\build\\tfjs_binding.vcxproj]      at Object.fs.openSync (fs.js:646:18)      at Object.fs.readFileSync (fs.js:551:33)      at files.map (D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\scripts\\generate_defs.js:23:40)      at Array.map (<anonymous>)      at Object.<anonymous> (D:\\tmp\\tfjs-install\\tensorflow-tfjs-node-gpu-0.2.3\\package\\scripts\\generate_defs.js:23:23)      at Module._compile (module.js:653:30)      at Object.Module._extensions..js (module.js:664:10)      at Module.load (module.js:566:32)      at tryModuleLoad (module.js:506:12)      at Function.Module._load (module.js:498:3)```![image](https://user-images.githubusercontent.com/1787775/52200013-49232900-2870-11e9-8ba9-7b96d9d445ce.png)I don\'t get a `tensroflow` folder in the deps directory, and the c_api.h is in the root of deps folder.NOTE: The way I tested this I used: `npm pack @tensorflow/tfjs-node-gpu` to download the package locally, then modified the scripts/install.js to log the stdout output of `node-gyp rebuild` command.=====', 'More info:This line checks if tensorflow.dll exist:https://github.com/tensorflow/tfjs-node/blob/master/scripts/install.js#L186it does not so it empties the deps directory:https://github.com/tensorflow/tfjs-node/blob/master/scripts/install.js#L191then it downloads:https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.12.0.zipNow the GPU zip has a different layout then CPU zip, and fails to install.Here are the packages unzipped side by side.![image](https://user-images.githubusercontent.com/1787775/52522795-be3c9700-2c92-11e9-9daf-d5ba02414c44.png)=====', 'Same here. Node v10, VS 2015. Cannot install 0.2.2, 0.2.3, and 0.3.0 while 0.2.1 works.=====', ""Looks like some more Windows build fun - I had something similar and this fixed it for me:Open an administrative prompt and run the following:```shnpm --add-python-to-path='true' --debug install --global windows-build-tools```Then try re-installing/yarn'ing. If that does not work - try running `node-gyp configure --verbose` from your `node_modules` or a clone of tfjs-node.From a clone of `tfjs-node` you can enable gpu locally by running:```shyarn enable-gpu```That will download the GPU binaries and re-compile. Let me know if this helps. More info here: https://github.com/tensorflow/tfjs-node/pull/206====="", ""@iacovlev-pavel Oh OK sorry I missed the part about the GPU layout. I'll try and fix that this week. We switched to different hosted binaries for libtensorflow. I don't know why GPU Win64 builds are packaged differently...====="", '@nkreeger Thank you=====', 'Fix in here https://github.com/tensorflow/tfjs-node/pull/208=====', 'Please try with 0.3.1 (just published)=====', ""It's working. v0.3.1 on Windows 10.====="", 'v0.3.1 contains the following, which surprisingly enough doesn\'t compile well...    copyModel: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/model_management"").copyModel,    listModels: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/model_management"").listModels,    moveModel: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/model_management"").moveModel,    removeModel: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/model_management"").removeModel,    browserFiles: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/browser_files"").browserFiles,    browserHTTPRequest: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/browser_http"").browserHTTPRequest,    concatenateArrayBuffers: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/io_utils"").concatenateArrayBuffers,    decodeWeights: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/io_utils"").decodeWeights,    encodeWeights: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/io_utils"").encodeWeights,    fromMemory: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/passthrough"").fromMemory,    getLoadHandlers: (url: string | string[], onProgress?: Function) => tf.io.IOHandler[],    getModelArtifactsInfoForJSON: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/io_utils"").getModelArtifactsInfoForJSON,    getSaveHandlers: (url: string | string[]) => tf.io.IOHandler[],    isHTTPScheme: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/browser_http"").isHTTPScheme,    loadWeights: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader"").loadWeights,    registerLoadRouter: (loudRouter: import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/router_registry"").IORouter) => void,    registerSaveRouter: (loudRouter: import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/router_registry"").IORouter) => void,    weightsLoaderFactory: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/weights_loader"").weightsLoaderFactory,    withSaveHandler: typeof import(""../../../../../Users/kreeger/workspace/tfjs-node/node_modules/@tensorflow/tfjs-core/dist/io/passthrough"").withSaveHandler,I\'m guessing that\'s from your machine, mr kreeger?=====', ""Aren't there tests for this? Something that tries to install and build from a windows machine?=====""]",0
https://github.com/tensorflow/tfjs/issues/4976,knn classifier fails with npm,2,closed,2021-04-23T09:31:16Z,2021-05-19T23:38:15Z,"**System information**- OS : Linux n-Lenovo-ideapad-320-15ISK 5.4.0-70-generic #78-Ubuntu SMP Fri Mar 19 13:29:52 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux- TensorFlow.js installed from (npm or script link): `npm install '@tensorflow/tfjs'`- TensorFlow.js version:                 ""@tensorflow-models/coco-ssd"": ""^2.2.2"",        ""@tensorflow-models/mobilenet"": ""^2.1.0"",        ""@tensorflow/tfjs"": ""^3.4.0"",        ""@tensorflow/tfjs-node"": ""^3.4.0"",- CUDA/cuDNN version: NA**Describe the problem**npm installation fails **Exact sequence of steps  executed before running into the problem**Have coco-ssd and mobilenet already installed and working smoothly also want to add knn classifier `npm i @tensorflow-models/knn-classifier`**Any other info / logs** npm i @tensorflow-models/knn-classifiernpm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! Found: @tensorflow/tfjs-core@3.4.0npm ERR! node_modules/@tensorflow/tfjs-corenpm ERR!   peer @tensorflow/tfjs-core@""^3.3.0"" from @tensorflow-models/coco-ssd@2.2.2npm ERR!   node_modules/@tensorflow-models/coco-ssdnpm ERR!     @tensorflow-models/coco-ssd@""^2.2.2"" from the root projectnpm ERR!   peer @tensorflow/tfjs-core@""^3.1.0"" from @tensorflow-models/mobilenet@2.1.0npm ERR!   node_modules/@tensorflow-models/mobilenetnpm ERR!     @tensorflow-models/mobilenet@""^2.1.0"" from the root projectnpm ERR!   6 more (@tensorflow/tfjs, @tensorflow/tfjs-backend-cpu, ...)npm ERR! npm ERR! Could not resolve dependency:npm ERR! @tensorflow-models/knn-classifier@""*"" from the root projectnpm ERR! npm ERR! Conflicting peer dependency: @tensorflow/tfjs-core@1.7.4npm ERR! node_modules/@tensorflow/tfjs-corenpm ERR!   peer @tensorflow/tfjs-core@""^1.2.1"" from @tensorflow-models/knn-classifier@1.2.2npm ERR!   node_modules/@tensorflow-models/knn-classifiernpm ERR!     @tensorflow-models/knn-classifier@""*"" from the root projectnpm ERR! npm ERR! Fix the upstream dependency conflict, or retrynpm ERR! this command with --force, or --legacy-peer-depsnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.npm ERR! npm ERR! See /home/n/.npm/eresolve-report.txt for a full report.npm ERR! A complete log of this run can be found in:","['I guess there are duplicate dependencies in the package.json , which needs to be resolved , please check. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4976"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4976"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5400,BlazePose Mediapipe-gpu official demo no longer works,1,closed,2021-07-29T16:40:48Z,2021-07-29T21:31:46Z,When I go to this link provided by the official TFJS models repo for pose-detection: https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=blazepose it throws this error on the console instead:![image](https://user-images.githubusercontent.com/33485877/127531130-4dd91a2c-10a0-42a0-9387-15b756e1c237.png)And nothing loads. This only happens when using blazepose with mediapipe-gpu backend.It was working before a couple days ago then I cleared my cache and it no longer works. So if it's still working for you make sure you clear your cache or change your browser and then try that link.**System information**Tried on Chrome 91 on Ubuntu 21.04 and Microsoft Edge Latest on Windows 10 2004,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5400"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5400"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4870,Cannot debug model containing Infinity as tensor value,2,closed,2021-03-27T12:33:41Z,2021-03-27T20:09:01Z,"Original issue in #4764 was that models containing **Infinity** value could not run.Work by @mattsoulanille in #4779 and #4783 fixed that issue.However, if TFJS debug mode is enabled using `tf.enableDebugMode(),`,  TFJS still runs checks and prevents model from running:```logUncaught (in promise) Error: A tensor of type float32 being uploaded contains -Infinity.    at checkConversionForErrors (util_base.ts:477)    at toTypedArray (util.ts:50)    at makeTensor (tensor_ops_util.ts:73)    at tensor (tensor.ts:56)    at Object.decodeWeights (io_utils.ts:225)    at GraphModel.loadSync (graph_model.ts:160)    at GraphModel.load (graph_model.ts:134)    at async loadGraphModel (graph_model.ts:440)    at async load12 (nanodet.ts:15)    at async Human.load (human.ts:243)```Model in question is at <https://github.com/vladmandic/nanodet>Environment: TFJS 3.3.0 using WebGL backend on Chrome 89 on Windows 10","[""closing this as it turns out, this is not an incompatibility with INF, it's the INF that results due to overflow when executing model quantized to float16.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4870"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4870"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3226,converter: ValueError: Unsupported Ops in the model before optimization,1,closed,2020-05-07T08:36:39Z,2020-11-20T21:40:30Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version`-- @tensorflow/tfjs@1.7.4  +-- @tensorflow/tfjs-converter@1.7.4  +-- @tensorflow/tfjs-core@1.7.4  | +-- @types/offscreencanvas@2019.3.0  | +-- @types/seedrandom@2.4.27  | +-- @types/webgl-ext@0.0.30  | +-- @types/webgl2@0.0.4  | +-- node-fetch@2.1.2  | `-- seedrandom@2.4.3  +-- @tensorflow/tfjs-data@1.7.4  | +-- @types/node-fetch@2.5.7  | | +-- @types/node@13.13.5  | | `-- form-data@3.0.0  | |   +-- asynckit@0.4.0  | |   +-- combined-stream@1.0.8  | |   | `-- delayed-stream@1.0.0  | |   `-- mime-types@2.1.27  | |     `-- mime-db@1.44.0  | `-- node-fetch@2.1.2 deduped  `-- @tensorflow/tfjs-layers@1.7.4#### Browser version#### Describe the problem or feature request#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.I save a linear estimator as follow`linear_est.export_saved_model(  ""./model/"", serving_input_fn)`with the result`INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']INFO:tensorflow:Signatures INCLUDED in export for Train: NoneINFO:tensorflow:Signatures INCLUDED in export for Eval: NoneINFO:tensorflow:Restoring parameters from C:\Users\nibur\AppData\Local\Temp\tmpvn9274v2\model.ckpt-9384INFO:tensorflow:Assets added to graph.INFO:tensorflow:No assets to write.INFO:tensorflow:SavedModel written to: ./model\temp-1588619275\saved_model.pb`When I then try to convert this model to tfjs I got the following error`(venv) PS C:\predict\web> tensorflowjs_wizardWelcome to TensorFlow.js Converter.? Please provide the path of model file or the directory that contains model files.If you are converting TFHub module please provide the URL.  sold\model\1588619275\saved_model.pb? What is your input model format? (auto-detected format is marked with *)  Tensorflow Saved Model *? What is tags for the saved model?  serve? What is signature name of the model?  signature name: predict? Do you want to compress the model? (this will decrease the model precision.)  No compression (Higher accuracy)? Please enter shard size (in bytes) of the weight files?  4194304? Do you want to skip op validation?This will allow conversion of unsupported ops,you can implement them as custom ops in tfjs-converter.  No? Do you want to strip debug ops?This will improve model execution performance.  Yes? Which directory do you want to save the converted model in?  web? The output already directory exists, do you want to overwrite it?  Yesconverter command generated:tensorflowjs_converter --input_format=tf_saved_model --saved_model_tags=serve --signature_name=predict --strip_debug_ops=True --weight_shard_size_bytes=4194304 sold\model\1588619275 web2020-05-07 10:25:23.621058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2WARNING:tensorflow:From c:\predict\web\venv\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:Issue encountered when serializing global_step.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing variables.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing trainable_variables.Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.2020-05-07 10:25:24.401911: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)2020-05-07 10:25:24.412308: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node linear/linear_model/linear/linear_model/linear/linear_model/category_id/category_id_lookup/hash_table/table_init/LookupTableImportV2 doesn't exist in graphWARNING:tensorflow:From c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py:313: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.Instructions for updating:This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.WARNING:tensorflow:From c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py:315: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`WARNING:tensorflow:From c:\predict\web\venv\lib\site-packages\tensorflow_core\python\framework\graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`Conversion failed, please check error log file C:\Users\nibur\AppData\Local\Temp\converter_error.log.(venv) PS C:\predict\web> python --versionPython 3.6.8`This is the related log file`Traceback (most recent call last):  File ""c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\wizard.py"", line 562, in run    converter.convert(arguments)  File ""c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\converter.py"", line 618, in convert    weight_shard_size_bytes=weight_shard_size_bytes)  File ""c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py"", line 462, in convert_tf_saved_model    weight_shard_size_bytes=weight_shard_size_bytes)  File ""c:\predict\web\venv\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py"", line 142, in optimize_graph    ', '.join(unsupported))ValueError: Unsupported Ops in the model before optimizationParseExampleV2, Unique, HashTableV2, SparseReshape, LookupTableFindV2, AsString, SparseFillEmptyRows, SparseSegmentSum`Here is the saved model[model.zip](https://github.com/tensorflow/tfjs/files/4591728/model.zip)GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).",['Those ops are not supported in tfjs and there is no roadmap as of now to support them. ====='],0
https://github.com/tensorflow/tfjs/issues/5808,tfjs_converter output format incorrect.,3,open,2021-11-03T16:59:47Z,2021-11-04T02:27:47Z,"Hello i'm working with `tfjs_converter` on a saved model via the following command`tensorflowjs_converter --input_format=tf_saved_model --signature_name=serving_default --saved_model_tags=serve ./saved_model ./tfjs_model`This seems to convert my model fine however there is some oddity in the outputs when looking at the output tensors from the saved model.json.If I run the following command I get the following output tensors from the original saved model:```saved_model_cli show --dir ./saved_model --all``````MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:signature_def['__saved_model_init_op']:  The given SavedModel SignatureDef contains the following input(s):  The given SavedModel SignatureDef contains the following output(s):    outputs['__saved_model_init_op'] tensor_info:        dtype: DT_INVALID        shape: unknown_rank        name: NoOp  Method name is:signature_def['serving_default']:  The given SavedModel SignatureDef contains the following input(s):    inputs['input_tensor'] tensor_info:        dtype: DT_UINT8        shape: (1, -1, -1, 3)        name: serving_default_input_tensor:0  The given SavedModel SignatureDef contains the following output(s):    outputs['detection_anchor_indices'] tensor_info:        dtype: DT_FLOAT        shape: (1, 100)        name: StatefulPartitionedCall:0    outputs['detection_boxes'] tensor_info:        dtype: DT_FLOAT        shape: (1, 100, 4)        name: StatefulPartitionedCall:1    outputs['detection_classes'] tensor_info:        dtype: DT_FLOAT        shape: (1, 100)        name: StatefulPartitionedCall:2    outputs['detection_multiclass_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1, 100, 3)        name: StatefulPartitionedCall:3    outputs['detection_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1, 100)        name: StatefulPartitionedCall:4    outputs['num_detections'] tensor_info:        dtype: DT_FLOAT        shape: (1)        name: StatefulPartitionedCall:5    outputs['raw_detection_boxes'] tensor_info:        dtype: DT_FLOAT        shape: (1, 51150, 4)        name: StatefulPartitionedCall:6    outputs['raw_detection_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1, 51150, 3)        name: StatefulPartitionedCall:7  Method name is: tensorflow/serving/predict```Now using the converted tfjs model in node, I run the following command to get the outputNodes with the following output.```let model = await tf.loadGraphModel(handler),console.log(model.outputNodes)``````[  'detection_multiclass_scores',  'raw_detection_boxes',  'Identity:0',  'Identity_2:0',  'detection_boxes',  'raw_detection_scores',  'num_detections',  'Identity_4:0']```You can see that some of the output tensors didn't carry the names from the save model, they are also not ordered as the original saved model. Why do the output tensors: `'Identity:0', 'Identity_2:0', 'Identity_4:0'` not have the same names as  `detection_anchor_indices, detection_scores, detection_classes`. It's difficult to determine the output. This seems like a bug in the converter. Any insight on this issue is appreciated.","['mapping output node names is a best effort and only added in recent versions, previously it was all just `identity_n`  but for mapping to work, tensors must have unique shape so they can be determined without errors. if two tensors have the same shape, there is no chance to know which one is which auto-magically  you can see that `detection_anchor_indices` and `detection_classes` have the same shape `[1, 100]`its just how converter works, not a bug as such  and order of tensors is not something to ever rely on. that is not even a limitation.=====', '@vladmandic thanks for that explanation that helps. Is there any way to prune or remove output tensors that may not be needed during the conversion using `tensorflowjs_converter` or a way to name or rename the outputs tensors or manually map them during the conversion?The problem is that when using these outputs in `tfjs` knowing which output to use is important, if we have some arbitrary output names it makes interpreting the output difficult, unless I can rely on `Identity_2:0` always mapping to `detection_anchor_indices`.=====', ""A) yes, you can rely on it since it's now defined in model.jsonB) you can run model.json through a prettyfier and then edit signature part to write down anything you want.=====""]",1
https://github.com/tensorflow/tfjs/issues/532,Support for non-placeholders as inputs to FrozenModel,1,closed,2018-07-19T22:16:05Z,2018-09-01T03:23:09Z,"It would be great if `tensorflowjs_converter` could support points of entry to graphs other than `tf.Placeholder`. The current behavior of the converter adds all placeholders that are part of the `--output_node_names` operation subgraphs as inputs. It would be better if the converter could have an `--input_node_names` field to allow user specification.Tensorflow vanilla supports the pattern of overriding intermediate graph tensors rather than placeholders. In the following example, I would like to be able to specify `--input_node_names=a_plus_1`, whereas currently my only option is to use `a`.```pya = tf.placeholder(tf.int32, [], ""a"")a_plus_1 = a + 1c = a_plus_1sess = tf.InteractiveSession()print sess.run(c, {a: 1}) # prints 2print sess.run(c, {a_plus_1: 2}) # prints 2```As a realistic example, the Tensorflow core operation `tf.nn.dynamic_rnn` uses a similar pattern. In this case the problem is that the hidden state vector of the RNN is a tensor (not a placeholder). It needs to be overridden differently during generation. However, it is very convenient to use the `dynamic_rnn` operation to have only one codebase for training and inference.In the following example I would like to be able to specify `--input_node_names=input_data,initial_state`.```pyinput_data = tf.placeholder(tf.int32, [None, None], name='input_data')batch_size = tf.shape(input_data)[0]input_data_onehot = tf.one_hot(input_data, 3)rnn_cell = tf.nn.rnn_cell.BasicRNNCell(3)initial_state = rnn_cell.zero_state(batch_size, tf.float32)outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, input_data_onehot, initial_state=initial_state)output_sample = tf.distributions.Categorical(probs=tf.nn.softmax(outputs)).sample()sess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())# During training one might do something like this (also using some kind of optimizer)_outputs = sess.run(outputs, {input_data: [[0, 2, 1], [0, 1, 1]]})# During inference (generation) one might do something like this_state = sess.run(initial_state, {input_data: [[0]]})_last_output = [[0]]for i in xrange(30):  _state, _last_output = sess.run([final_state, output_sample], {initial_state: _state, input_data: _last_output})  print _last_output```The problem is in the second to last line, where I have to override the state tensor rather than initializing it from zero.",['I think this is very important in order to bridge the gap between tfjs and vanilla tensorflow and not particulary  time-consuming to address. Is there any roadmap about it?====='],0
https://github.com/tensorflow/tfjs/issues/3945,"The behavior of the three resize methods (cropAndResize, resizeNearestNeighbor and resizeBilinear)",16,open,2020-09-18T08:34:34Z,2021-12-06T20:11:22Z,"# problemWhat is the difference between the three resize methods (cropAndResize, resizeNearestNeighbor and resizeBilinear)?CropAndResize and resizeNearestNeighbor do not return the expected results. Why is the bottom half of my image lost?# backgroundWe would like to develop an application that performs image processing in Tensorflow.js.To do so, we need to match the size of the image to be input to the trained model.The image to be input is large, so I want to simply scale it down to 128x128.I have found 3 resize methods on the following page, but they do not give me the results I expect.The bottom half of the image is lost. Does anyone have any good ideas?# codeThis is the source code I wrote.```<!DOCTYPE html><html><head></head><meta charset=""utf-8""/><body><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""></script><canvas id=""canvas1"" width=""256"" height=""256"" style=""border: 2px solid,""></canvas><canvas id='canvas2' width=""128"" height=""128"" style=""border: 2px solid,""></canvas><canvas id='canvas3' width=""128"" height=""128"" style=""border: 2px solid,""></canvas><canvas id='canvas4' width=""128"" height=""128"" style=""border: 2px solid,""></canvas><script>const canvas1 = document.getElementById('canvas1'),const canvas2 = document.getElementById('canvas2'),const canvas3 = document.getElementById('canvas3'),const canvas4 = document.getElementById('canvas4'),const SIZE = 128,const COL_CHANNEL = 3,const inputImg = new Image(), inputImg.src = ""../00_resources/woman172.jpg"",const inCtx = canvas1.getContext('2d'),// image1 (original image)inCtx.drawImage(inputImg, 0, 0, canvas1.width, canvas1.height),let img1 = tf.browser.fromPixels(canvas1, COL_CHANNEL),// image2 (resized image - using 'cropAndResize')let img2 = tf.image.cropAndResize(img1.expandDims(0), [[0, 0, 1, 1]], [0], [SIZE,SIZE]).div(tf.scalar(255)).squeeze(),tf.browser.toPixels(img2, canvas2),// NG// image3 (resized image - using 'resizeNearestNeighbor')let img3 = tf.image.resizeNearestNeighbor(img1, [SIZE, SIZE]).toFloat().div(tf.scalar(255)),tf.browser.toPixels(img3, canvas3),// NG// image4 (resized image - using 'resizeBilinear')let img4 = tf.image.resizeBilinear(img1, [SIZE, SIZE]).toFloat().div(tf.scalar(255)),tf.browser.toPixels(img4, canvas4),// OK?</script></body></html>```![Capturing the result](https://user-images.githubusercontent.com/69997979/93575531-bd262c80-f9d4-11ea-953d-dca60e046507.png)From left to right.- Original Image.- Resized image using cropAndResize (half of the image is black) - Resized image using resizeNearestNeighbor (half of the image is black)- Resized image using resizeBilinear (appears to be the correct result)# referencehttps://js.tensorflow.org/api/latest/","['I wasn\'t able to reproduce this exactly, but did run into problem if I didn\'t wait for the image to finish loading. Could you try the following?```const inputImg = new Image(),   inputImg.onload = function () {  const inCtx = canvas1.getContext(\'2d\'),  // image1 (original image)  inCtx.drawImage(inputImg, 0, 0, canvas1.width, canvas1.height),  let img1 = tf.browser.fromPixels(canvas1, COL_CHANNEL),  // image2 (resized image - using \'cropAndResize\')  let img2 = tf.image.cropAndResize(img1.expandDims(0), [[0, 0, 1, 1]], [0], [SIZE,SIZE]).div(tf.scalar(255)).squeeze(),  tf.browser.toPixels(img2, canvas2),// NG  // image3 (resized image - using \'resizeNearestNeighbor\')  let img3 = tf.image.resizeNearestNeighbor(img1, [SIZE, SIZE]).toFloat().div(tf.scalar(255)),  tf.browser.toPixels(img3, canvas3),// NG  // image4 (resized image - using \'resizeBilinear\')  let img4 = tf.image.resizeBilinear(img1, [SIZE, SIZE]).toFloat().div(tf.scalar(255)),  tf.browser.toPixels(img4, canvas4),// OK?}  inputImg.src = ""your url here"" ,```Notice that here we wait for the onload event on the image before trying to draw it or read it. If that doesn\'t work could you paste a codepen or glitch example that we can use to try and reproduce the issue.My attempt to reproduce this: https://glitch.com/edit/#!/humorous-silky-hacksaw?path=index.html%3A12%3A23 (code)https://humorous-silky-hacksaw.glitch.me (output)=====', ""Dear tafsiri,Thank you for your reply.Your point is correct.The lack of onload in the code in the first post is my mistake. My apologies.Back on topic, I am wondering why image2 and image3 are not fully drawn.It's reproduced in the example you gave me.It doesn't draw image2 and image3 despite image1 being drawn and image4 attempted as well is drawn.Why is this?Can you help me?Thank you and best regards,====="", 'Could you send me an online example (in codepen, glitch, or something similar) that demonstrates the problem? I am no able to reproduce it. This is what i see in [the link](https://humorous-silky-hacksaw.glitch.me/) I posted above.![Screen Shot 2020-09-30 at 11 15 01 AM](https://user-images.githubusercontent.com/26408/94704727-38a9a700-030e-11eb-99d0-1c5779d042db.png)=====', ""Thank you for your reply.I was surprised to hear that it can't be reproduced, but I'm fortunate that you attached the image.I seem to be seeing different results with the same source code.For example, the image looks incomplete to me even in the link you provided to me.I'll send you the image I'm seeing.Even in the source code, I'm using what you contributed to glitch.(Glitch is very useful. I got an account there too.)That's my first source code with an ’onload' added to it, butThe result looks as imperfect to me as what I see in the link you provided.Is it an environmental issue?Is there anything else I can offer besides the source code?It may be a very difficult problem, but please help me.Thank you and best regards,![image](https://user-images.githubusercontent.com/69997979/94759158-1e3cff80-03da-11eb-95a4-257c3fb05f77.png)====="", 'I\'ll report what I\'ve found.1. it doesn\'t seem to be due to the type of browser2. it doesn\'t seem to be due to the tfjs versionAbout 1.I checked with three different browsers - firefox, chrome and edge - and the results were the same.(I\'m attaching a picture just in case)All browsers are kept up to date.![image](https://user-images.githubusercontent.com/69997979/94769756-c5776200-03ed-11eb-8b22-7db69fc35d2c.png)About 2.I specified the version of tensorflowjs explicitly.For example, to specify the latest version 2.4.0, I used the following<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js""></script>What I\'ve found here is that the same phenomenon exists if you go back to 1.1.2.I\'ve attached an image of this as well.![image](https://user-images.githubusercontent.com/69997979/94769769-cc05d980-03ed-11eb-9667-c22cb56d3ad1.png)I don\'t know if it\'s related, but if you go back to 0.15.3, \'resizeBilinear\' is also incomplete in the same way.I\'ve attached an image of this as well.![image](https://user-images.githubusercontent.com/69997979/94769784-d1fbba80-03ed-11eb-8aa9-467d48e8b1fe.png)It might give you a clue if there was a feature update for \'resizeBilinear\' in the 0.15.3 to 1.1.2 versions.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', ""@tafsiri @rthadur This post has not yet been resolved. Please do not close it.'Installed' is not an appropriate action.Also, I have presented the information I can report.'stat:awiting response' is also not appropriate.Please reply if there is anything I can do to help.====="", '@rthadur are you able to reproduce the issue described here when visiting this link https://humorous-silky-hacksaw.glitch.me/=====', '> Could you send me an online example (in codepen, glitch, or something similar) that demonstrates the problem? I am no able to reproduce it. This is what i see in [the link](https://humorous-silky-hacksaw.glitch.me/) I posted above.> > ![Screen Shot 2020-09-30 at 11 15 01 AM](https://user-images.githubusercontent.com/26408/94704727-38a9a700-030e-11eb-99d0-1c5779d042db.png)@tafsiri I am seeing the same in all browsers.=====', 'Hi @its-ogawa, from looking at this and a few other issues we are beginning to suspect an issue with particular integrated graphics chipsets, could you tell us what hardware/graphics card you are running on?Could you also post a screenshot of what you see here https://js.tensorflow.org/debug/=====', '@tafsiri Dear tafsiri,Thank you for your reply.I\'m glad you are continuing your research.I\'ll send you the hardware/graphics card information.Please check the attached ""CPU-Z_report.txt"".[CPU-Z_report.txt](https://github.com/tensorflow/tfjs/files/5426732/CPU-Z_report.txt)Also attached is a capture of the results of the Debug info (https://js.tensorflow.org/debug/).![image](https://user-images.githubusercontent.com/69997979/96946300-f2192800-151a-11eb-8343-5662a8fd0978.png)I hope this helps with the solution.I know it\'s a lot of work, but please help me out.Thank you and best regards,=====', 'Thanks, just pulling out the graphics card info for other to see. Display adapter 0\t\tID\t\t\t0x4000000\tName\t\t\tIntel(R) HD Graphics 4000\tBoard Manufacturer\t0x10CF (0x176D)\tPCI device\t\tbus 0 (0x0), device 2 (0x2), function 0 (0x0)\tVendor ID\t\t0x8086 (0x10CF)\tModel ID\t\t0x0162 (0x176D)\tRevision ID\t\t0x9\tPerformance Level\t0cc @annxingyuan @pyu10055 for comparison with other related issues.=====', 'can you please try with latest version , if the issue still exists ?=====', '@rthadur Thank you for contacting us.I am glad that you continue to work on this issue.Now, as for the results, resizeNearestNeighbor works, but cropAndResize does not.The source code is the same as the old one.I think tfjs has been uploaded. I am running the following command.```>npm install -g tfjs+ tfjs@0.6.0added 4 packages from 4 contributors in 4.044s``````>npm install -g @tensorflow/tfjsC:\\Users\\ogawa\\AppData\\Roaming\\npm\\tfjs-custom-module -> C:\\Users\\ogawa\\AppData\\Roaming\\npm\\node_modules\\@tensorflow\\tfjs\\dist\\tools\\custom_module\\cli.js> core-js@3.19.1 postinstall C:\\Users\\ogawa\\AppData\\Roaming\\npm\\node_modules\\@tensorflow\\tfjs\\node_modules\\core-js> node -e ""try{require(\'./postinstall\')}catch(e){}""Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js:> https://opencollective.com/core-js> https://patreon.com/zloirock> https://paypal.me/zloirock> bitcoin: bc1qlea7544qtsmj2rayg0lthvza9fau63ux0fstczAlso, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)+ @tensorflow/tfjs@3.11.0added 49 packages from 84 contributors in 16.554s```![image](https://user-images.githubusercontent.com/69997979/140676212-e751ff89-7fc2-4650-9a84-66c26a8142c5.png)The result of [this link](https://humorous-silky-hacksaw.glitch.me/) is as follows.![image](https://user-images.githubusercontent.com/69997979/140676360-8935efcc-215e-47c6-90e3-f5dbe3ea9bb9.png)=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@tafsiri @rthadurThis post has not yet been resolved. Please do not close it.I have presented the information I can report.Please reply if there is anything I can do to help.=====']",0
https://github.com/tensorflow/tfjs/issues/2255,tensorflow.js demo-Not allowed to load local resource on chrome,1,closed,2019-10-22T07:09:33Z,2019-10-22T17:48:32Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser versionChrome  76.0.3809.100#### Describe the problem or feature requestI'm running the Tensorflow.js demo downloaded from github which called Iris. When I follow the command of README.md, the server was set up and the chrome opened automatically, **but it failed to call the css and js file. Thus, the demo didn't run successfully.**I have downloaded ""yarn"". I ran the command on Powershell. When I enter""yarn watch"", the Git window came out to set up the server and the chrome came out to show the html file(website).source code is like this<img width=""500"" alt=""1"" src=""https://user-images.githubusercontent.com/48885821/67264509-d6c9a600-f4dd-11e9-841b-bef0292d4977.png""><br><br>but when i check on chrome's console, it became like that<img width=""500"" alt=""1"" src=""https://user-images.githubusercontent.com/48885821/67264541-ed6ffd00-f4dd-11e9-99ba-62c3ac92ed6b.png""><br><br>and the console told some error:<img width=""500"" alt=""1"" src=""https://user-images.githubusercontent.com/48885821/67264464-b3066000-f4dd-11e9-827b-39594d5ec7e7.png"">I expect the result to be that I can run the demo on Chrome, but the actual result is the console of chrome told me""Not allowed to load local resource on chrome"" ","[""@kingsleyljc The files (html, js, css) are packaged and hosted by `http-server`, instead of loading from local resource directly. I think the demo didn't work on your machine because the `yarn watch` command runs `serve.sh` shell script but it may not work on windows.=====""]",0
https://github.com/tensorflow/tfjs/issues/5191,[Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'graph.versions.producer')],2,open,2021-06-08T14:00:01Z,2021-06-09T19:48:05Z,"I have used graph-model while loading the loading i m getting this error   [Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'graph.versions.producer')]Code Referenceasync componentDidMount() {    await tf.ready()    this.setState({      isTfReady: true     })      const modelJson = require('./assets/model/model.json'),      const modelWeights = require('./assets/model/final.bin'),            this.model = await tf.loadGraphModel(bundleResourceIO(modelJson, modelWeights)),      console.log('loadGraphModel', this.model)            this.setState({ isModelReady: true })    // await this.getPermissionAsync()  }","['Can you share full log and reproducible code ,and also looking at the error message this is related to exported model not tfjs , please check for a related issue here https://github.com/tensorflow/tfjs/issues/5122=====', '[<img width=""1440"" alt=""Screenshot 2021-06-09 at 2 38 42 PM"" src=""https://user-images.githubusercontent.com/84903872/121326855-8f04c600-c930-11eb-8143-1f1d8d601755.png"">](url)import React from \'react\'import {  StyleSheet,  Text,  View,  ActivityIndicator,  StatusBar,  Image,  TouchableOpacity} from \'react-native\'import * as tf from \'@tensorflow/tfjs\'import * as jpeg from \'jpeg-js\'import * as ImagePicker from \'expo-image-picker\'import Constants from \'expo-constants\'import * as Permissions from \'expo-permissions\'import * as FileSystem from \'expo-file-system\',import {bundleResourceIO} from ""@tensorflow/tfjs-react-native"",class App extends React.Component {  state = {    isTfReady: false,    isModelReady: false,    predictions: null,    image: null  }  async componentDidMount() {    await tf.ready()    this.setState({      isTfReady: true     })      const modelJson = require(\'./assets/model/model.json\'),      const modelWeights = require(\'./assets/model/final.bin\'),            this.model = await tf.loadGraphModel(bundleResourceIO(modelJson, modelWeights)),      console.log(\'loadGraphModel\', this.model)            this.setState({ isModelReady: true })    // await this.getPermissionAsync()  }  getPermissionAsync = async () => {    if (Constants.platform.ios) {      const { status } = await Permissions.askAsync(Permissions.CAMERA_ROLL)      if (status !== \'granted\') {        alert(\'Sorry, we need camera roll permissions to make this work!\')      }    }  }  imageToTensor(rawImageData) {    const TO_UINT8ARRAY = true    const { width, height, data } = jpeg.decode(rawImageData)    // Drop the alpha channel info for mobilenet    const buffer = new Uint8Array(width * height * 3)    let offset = 0 // offset into original data    for (let i = 0, i < buffer.length, i += 3) {      buffer[i] = data[offset]      buffer[i + 1] = data[offset + 1]      buffer[i + 2] = data[offset + 2]      offset += 4    }    return tf.tensor3d(buffer, [height, width, 3])  }  classifyImage = async () => {    try {      const imageAssetPath = Image.resolveAssetSource(this.state.image)      // const response = await fetch(imageAssetPath.uri, {}, { isBinary: true })      const imgB64 = await FileSystem.readAsStringAsync(imageAssetPath.uri, {        encoding: FileSystem.EncodingType.Base64,      }),      const imgBuffer = tf.util.encodeString(imgB64, \'base64\').buffer,      // const rawImageData = await response.arrayBuffer()      const raw = new Uint8Array(imgBuffer)      // const imageTensor = jpeg.decode(raw),      const imageTensor = this.imageToTensor(raw).resizeBilinear([224,224]).reshape([1,224,224,3]),      // const predictions = await this.model.classify(imageTensor)      // const zeros = tf.zeros([1, 224, 224, 3]),      // model.predict(zeros).print(),       const res = this.model.predict(imageTensor),      this.setState({ res })      console.log(res, \'success\')    } catch (error) {      console.log(error, \'failure\')    }  }  selectImage = async () => {    try {      let response = await ImagePicker.launchImageLibraryAsync({        mediaTypes: ImagePicker.MediaTypeOptions.All,        allowsEditing: true,        aspect: [4, 3]      })      if (!response.cancelled) {        const source = { uri: response.uri }        this.setState({ image: source })        this.classifyImage()      }    } catch (error) {      console.log(error)    }  }  renderPrediction = prediction => {    return (      <Text key={prediction.className} style={styles.text}>        {prediction.className}      </Text>    )  }  render() {    const { isTfReady, isModelReady, predictions, image } = this.state    console.log(predictions, \'predictionspredictionspredictionspredictions\')    return (      <View style={styles.container}>        <StatusBar barStyle=\'light-content\' />        <View style={styles.loadingContainer}>          <Text style={styles.text}>            TFJS ready? {isTfReady ? <Text>✅</Text> : \'\'}          </Text>          <View style={styles.loadingModelContainer}>            <Text style={styles.text}>Model ready? </Text>            {isModelReady ? (              <Text style={styles.text}>✅</Text>            ) : (              <ActivityIndicator size=\'small\' />            )}          </View>        </View>        <TouchableOpacity          style={styles.imageWrapper}          onPress={isModelReady ? this.selectImage : undefined}>          {image && <Image source={image} style={styles.imageContainer} />}          {isModelReady && !image && (            <Text style={styles.transparentText}>Tap to choose image</Text>          )}        </TouchableOpacity>        <View style={styles.predictionWrapper}>          {isModelReady && image && (            <Text style={styles.text}>              Predictions: {predictions ? \'\' : \'Predicting...\'}            </Text>          )}          {isModelReady &&            predictions &&            predictions.map(p => this.renderPrediction(p))}        </View>      </View>    )  }}const styles = StyleSheet.create({  container: {    flex: 1,    backgroundColor: \'#171f24\',    alignItems: \'center\'  },  loadingContainer: {    marginTop: 80,    justifyContent: \'center\'  },  text: {    color: \'#ffffff\',    fontSize: 16  },  loadingModelContainer: {    flexDirection: \'row\',    marginTop: 10  },  imageWrapper: {    width: 280,    height: 280,    padding: 10,    borderColor: \'#cf667f\',    borderWidth: 5,    borderStyle: \'dashed\',    marginTop: 40,    marginBottom: 10,    position: \'relative\',    justifyContent: \'center\',    alignItems: \'center\'  },  imageContainer: {    width: 250,    height: 250,    position: \'absolute\',    top: 10,    left: 10,    bottom: 10,    right: 10  },  predictionWrapper: {    height: 100,    width: \'100%\',    flexDirection: \'column\',    alignItems: \'center\'  },  transparentText: {    color: \'#ffffff\',    opacity: 0.7  },  footer: {    marginTop: 40  },  poweredBy: {    fontSize: 20,    color: \'#e69e34\',    marginBottom: 6  },  tfLogo: {    width: 125,    height: 70  }})export default App=====']",1
https://github.com/tensorflow/tfjs/issues/4534,@tensorflow/tfjs-node package.json file missing license value,1,closed,2021-01-14T18:57:26Z,2021-01-15T17:48:17Z,"Hi, I'm trying to get tfjs-node added into our corporate repository, but our process requires the package.json file to have the license header included.@tensorflow/tfjs can be added easily, because it has the value ""Apache-2.0"" in the license header, but tfjs-node doesn't.Is it intended that tfjs-node would not have the license header within its package.json file?Thanks in advance! ",['Added license filed in tfjs-node package.json====='],1
https://github.com/tensorflow/tfjs/issues/5860,Building tfjs-converter in a Google Colab environment,2,open,2021-11-16T21:39:33Z,2021-11-18T17:07:49Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab- TensorFlow.js installed from (npm or script link): pip- TensorFlow.js version: 3.11.0**Describe the problem**Having had my tfjs-converter issue very promptly solved (thank you!), I've tried to build tfjs-converter following [these instructions](https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/DEVELOPMENT.md). Here's the Colab I'm using:https://colab.research.google.com/drive/1bmMVsBE8pMwEspEeQCoCHg7gxELV7dHMAs you can see (the error logs are statically saved in that doc - no need to run it), it results in an error like this:```.........created virtual environment CPython2.7.17.final.0-64 in 1007ms  creator CPython2Posix(dest=/tmp/tmp.6SvzefkP81, clear=False, no_vcs_ignore=False, global=False)  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)    added seed packages: pip==20.3.4, setuptools==44.1.1, wheel==0.37.0  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivatorLooking for wheel for python2: Python 2.7.17 ...The wheel should be build with 'bazel build python2_wheel python3_wheel' commandls: cannot access '../../dist/bin/tfjs-converter/python/*py2*.whl': No such file or directory```From that error message it seems like it has to do with Python versions, but Colab has binaries for python2 and python3 (in `/usr/bin`), so I don't see why that should be a problem.**Provide the exact sequence of commands / steps that you executed before running into the problem**See Colab link above.**Any other info / logs**N/A","[""The `build_pip_package.sh` script is actually a misnomer, and should probably be renamed to `copy_pip_package.sh`. The pip package is actually first built with Bazel, with the command `bazel build python2_wheel python3_wheel`, but we exclude that command from `build_pip_package.sh` because in CI, we have already build the wheel with Bazel. @pyu10055 knows more about this build process.I gave this a try in the Colab, but I ran into a PyInquirer install error:```Collecting PyInquirer==1.0.3  Using cached PyInquirer-1.0.3.tar.gz (27 kB) (WARNING: Discarding https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz#sha256=c9a92d68d7727fbd886a7908c08fd9e9773e5dc211bf5cbf836ba90d366dee51 (from https://pypi.org/simple/pyinquirer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.ERROR: Could not find a version that satisfies the requirement PyInquirer==1.0.3ERROR: No matching distribution found for PyInquirer==1.0.3```Edit: [Here's a link to the colab](https://colab.research.google.com/drive/1QcpIC3EBAK68Ci1yGEEsdfT2pHMKc_1W#scrollTo=4dc0mnNuqSRs)====="", ""@pyu10055 I added `pip install tensorflowjs` before the `bazel` command in mattsoulanille's notebook and got the same `PyInquirer` error. I may have misunderstood your instruction? The series of commands in the notebook are:```!wget https://github.com/bazelbuild/bazelisk/releases/download/v1.10.1/bazelisk-linux-amd64!chmod +x bazelisk-linux-amd64!mv bazelisk-linux-amd64 /usr/local/bin/bazel!git clone https://github.com/tensorflow/tfjs%cd tfjs/tfjs-converter/python!pip install tensorflowjs!bazel build python2_wheel python3_wheel!./build-pip-package.sh /content/my_tensorflowjs_pip```and it fails at the `bazel` command.=====""]",1
https://github.com/tensorflow/tfjs/issues/1724,[Codelab]: Making Predictions from 2D Data,1,closed,2019-07-06T08:38:19Z,2019-07-08T14:23:27Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.0.0#### Browser versionfirefox quantum Ver 67.04 64bit#### Describe the problem or feature requestI ussually blame myself when an error happens because I'm not a seasoned coder. However, while trying to learn ML with tensorflowJS, the code provided in the tensorflow codelab returns an error when I run it with firefox. I'm copying the code exact, so I don't think it's on my end, but feel free to correct me.Erros like ""await can only be called within an async function"" and ""data is not defined"". I did notice that the variable data is not defined in the code, however it is passed into the function 'convertToTensor'. I tried creating a variable data and removing the await that was causing the code to error, but that opens a bigger can of worms.hopefully someone will look into this and find a solutions.#### Code to reproduce the bug / link to feature request/** * Get the car data reduced to just the variables we are interested * and cleaned of missing data. */async function getData() {  const carsDataReq = await fetch('https://storage.googleapis.com/tfjs-tutorials/carsData.json'),  const carsData = await carsDataReq.json(),  const cleaned = carsData.map(car => ({    mpg: car.Miles_per_Gallon,    horsepower: car.Horsepower,  }))  .filter(car => (car.mpg != null && car.horsepower != null)),  return cleaned,}async function run() {  // Load and plot the original input data that we are going to train on.  const data = await getData(),  const values = data.map(d => ({    x: d.horsepower,    y: d.mpg,  })),  tfvis.render.scatterplot(    {name: 'Horsepower v MPG'},    {values},    {      xLabel: 'Horsepower',      yLabel: 'MPG',      height: 300    }  ),  // More code will be added below}document.addEventListener('DOMContentLoaded', run),function createModel() {  // Create a sequential model  const model = tf.sequential(),  // Add a single hidden layer  model.add(tf.layers.dense({inputShape: [1], units: 1, useBias: true})),  // Add an output layer  model.add(tf.layers.dense({units: 1, useBias: true})),  return model,}function createModel() {  // Create a sequential model  const model = tf.sequential(),  // Add a single hidden layer  model.add(tf.layers.dense({inputShape: [1], units: 1, useBias: true})),  // Add an output layer  model.add(tf.layers.dense({units: 1, useBias: true})),  return model,}const model = createModel(),tfvis.show.modelSummary({name: 'Model Summary'}, model),/** * Convert the input data to tensors that we can use for machine * learning. We will also do the important best practices of _shuffling_ * the data and _normalizing_ the data * MPG on the y-axis. */function convertToTensor(data) {  // Wrapping these calculations in a tidy will dispose any  // intermediate tensors.  return tf.tidy(() => {    // Step 1. Shuffle the data    tf.util.shuffle(data),    // Step 2. Convert data to Tensor    const inputs = data.map(d => d.horsepower)    const labels = data.map(d => d.mpg),    const inputTensor = tf.tensor2d(inputs, [inputs.length, 1]),    const labelTensor = tf.tensor2d(labels, [labels.length, 1]),    //Step 3. Normalize the data to the range 0 - 1 using min-max scaling    const inputMax = inputTensor.max(),    const inputMin = inputTensor.min(),    const labelMax = labelTensor.max(),    const labelMin = labelTensor.min(),    const normalizedInputs = inputTensor.sub(inputMin).div(inputMax.sub(inputMin)),    const normalizedLabels = labelTensor.sub(labelMin).div(labelMax.sub(labelMin)),    return {      inputs: normalizedInputs,      labels: normalizedLabels,      // Return the min/max bounds so we can use them later.      inputMax,      inputMin,      labelMax,      labelMin,    }  }),}async function trainModel(model, inputs, labels) {  // Prepare the model for training.  model.compile({    optimizer: tf.train.adam(),    loss: tf.losses.meanSquaredError,    metrics: ['mse'],  }),  const batchSize = 28,  const epochs = 50,  return await model.fit(inputs, labels, {    batchSize,    epochs,    shuffle: true,    callbacks: tfvis.show.fitCallbacks(      { name: 'Training Performance' },      ['loss', 'mse'],      { height: 200, callbacks: ['onEpochEnd'] }    )  }),}// Convert the data to a form we can use for training.const tensorData = convertToTensor(data),const {inputs, labels} = tensorData,// Train the modelawait trainModel(model, inputs, labels),console.log('Done Training'),","[""I think you missed a step. The code you put at the end of the file should be added to the bottom of your **run function** (right below the comment). Feel free to re-open this issue if that doesn't work for you. =====""]",0
https://github.com/tensorflow/tfjs/issues/5897,React Native - Back Camera Issue + Unhandled promise rejection: TypeError: null is not an object,5,closed,2021-11-25T01:01:41Z,2021-12-09T20:39:58Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Copied from [Here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection)- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Big Sur 11.2- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone SE- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ""^0.8.0""- Browser version: Chrome 96- Tensorflow.js Converter Version: ""^3.9.0""**Describe the current behavior**I am still having issues with the Back camera not placing landmarks correctly. The Front camera works great. On Back camera, they place when no person is in the camera view and off to the side on a ghost pose when a person is in view. I had the same issue with version 0.6.0. I've taken out useCustomShadersToResize={false}, cameraTextureWidth{OUTPUT_TENSOR_WIDTH} and cameraTextureHeight={OUTPUT_TENSOR_HEIGHT} but it is still requesting these.**Describe the expected behavior**Expected behavior would be for the landmarks to place correctly.I am getting this error currently in VS Code:![Screen Shot 2021-11-27 at 3 11 29 PM](https://user-images.githubusercontent.com/47552416/143723008-cdb87926-38f7-4bbc-b20b-572b0b823373.png)**Standalone code to reproduce the issue**```import React, { useEffect, useState, useRef } from 'react',import { StyleSheet, Text, View, Dimensions, Platform } from 'react-native',import { Camera } from 'expo-camera',import * as tf from '@tensorflow/tfjs',import * as posedetection from '@tensorflow-models/pose-detection',import * as ScreenOrientation from 'expo-screen-orientation',import { cameraWithTensors } from '@tensorflow/tfjs-react-native',import Svg, { Circle } from 'react-native-svg',import { ExpoWebGLRenderingContext } from 'expo-gl',// tslint:disable-next-line: variable-nameconst TensorCamera = cameraWithTensors(Camera),const IS_ANDROID = Platform.OS === 'android',const IS_IOS = Platform.OS === 'ios',// Camera preview size.//// From experiments, to render camera feed without distortion, 16:9 ratio// should be used fo iOS devices and 4:3 ratio should be used for android// devices.//// This might not cover all cases.const CAM_PREVIEW_WIDTH = Dimensions.get('window').width,const CAM_PREVIEW_HEIGHT = CAM_PREVIEW_WIDTH / (IS_IOS ? 9 / 16 : 3 / 4),// The score threshold for pose detection results.const MIN_KEYPOINT_SCORE = 0.3,// The size of the resized output from TensorCamera.//// For movenet, the size here doesn't matter too much because the model will// preprocess the input (crop, resize, etc). For best result, use the size that// doesn't distort the image.const OUTPUT_TENSOR_WIDTH = 180,const OUTPUT_TENSOR_HEIGHT = OUTPUT_TENSOR_WIDTH / (IS_IOS ? 9 / 16 : 3 / 4),// Whether to auto-render TensorCamera preview.const AUTO_RENDER = false,export default function App() {  const cameraRef = useRef(null),  const [tfReady, setTfReady] = useState(false),  const [model, setModel] = useState<posedetection.PoseDetector>(),  const [poses, setPoses] = useState<posedetection.Pose[]>(),  const [fps, setFps] = useState(0),  const [orientation, setOrientation] =    useState<ScreenOrientation.Orientation>(),  useEffect(() => {    async function prepare() {      // Set initial orientation.      const curOrientation = await ScreenOrientation.getOrientationAsync(),      setOrientation(curOrientation),      // Listens to orientation change.      ScreenOrientation.addOrientationChangeListener((event) => {        setOrientation(event.orientationInfo.orientation),      }),      // Camera permission.      await Camera.requestPermissionsAsync(),      // Wait for tfjs to initialize the backend.      await tf.ready(),      // Load movenet model.      // https://github.com/tensorflow/tfjs-models/tree/master/pose-detection      const model = await posedetection.createDetector(        posedetection.SupportedModels.MoveNet,        {          modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING,          enableSmoothing: true,        }      ),      setModel(model),      // Ready!      setTfReady(true),    }    prepare(),  }, []),    const handleCameraStream = async (    images: IterableIterator<tf.Tensor3D>,    updatePreview: () => void,    gl: ExpoWebGLRenderingContext  ) => {    const loop = async () => {      // Get the tensor and run pose detection.      const imageTensor = images.next().value as tf.Tensor3D,      const startTs = Date.now(),      const poses = await model!.estimatePoses(        imageTensor,        undefined,        Date.now()      ),      const latency = Date.now() - startTs,      // setFps(Math.floor(1000 / latency)),      setPoses(poses),      tf.dispose([imageTensor]),      // Render camera preview manually when autorender=false.      if (!AUTO_RENDER) {        updatePreview(),        gl.endFrameEXP(),      }      requestAnimationFrame(loop),    },        loop(),  },    const renderPose = () => {    if (poses != null && poses.length > 0) {      const keypoints = poses[0].keypoints        .filter((k) => (k.score ?? 0) > MIN_KEYPOINT_SCORE)        .map((k) => {          // Flip horizontally on android.          const x = IS_ANDROID ? OUTPUT_TENSOR_WIDTH - k.x : k.x,          const y = k.y,          const cx =            (x / getOutputTensorWidth()) *            (isPortrait() ? CAM_PREVIEW_WIDTH : CAM_PREVIEW_HEIGHT),          const cy =            (y / getOutputTensorHeight()) *            (isPortrait() ? CAM_PREVIEW_HEIGHT : CAM_PREVIEW_WIDTH),          return (            <Circle              key={`skeletonkp_${k.name}`}              cx={cx}              cy={cy}              r='4'              strokeWidth='2'              fill='#00AA00'              stroke='white'            />          ),        }),      return <Svg style={styles.svg}>{keypoints}</Svg>,    } else {      return <View></View>,    }  },  const renderFps = () => {    return (      <View style={styles.fpsContainer}>        <Text>FPS: {fps}</Text>      </View>    ),  },  const isPortrait = () => {    return (      orientation === ScreenOrientation.Orientation.PORTRAIT_UP ||      orientation === ScreenOrientation.Orientation.PORTRAIT_DOWN    ),  },  const getOutputTensorWidth = () => {    // On iOS landscape mode, switch width and height of the output tensor to    // get better result. Without this, the image stored in the output tensor    // would be stretched too much.    //    // Same for getOutputTensorHeight below.    return isPortrait() || IS_ANDROID      ? OUTPUT_TENSOR_WIDTH      : OUTPUT_TENSOR_HEIGHT,  },  const getOutputTensorHeight = () => {    return isPortrait() || IS_ANDROID      ? OUTPUT_TENSOR_HEIGHT      : OUTPUT_TENSOR_WIDTH,  },  const getTextureRotationAngleInDegrees = () => {    // On Android, the camera texture will rotate behind the scene as the phone    // changes orientation, so we don't need to rotate it in TensorCamera.    if (IS_ANDROID) {      return 0,    }    // For iOS, the camera texture won't rotate automatically. Calculate the    // rotation angles here which will be passed to TensorCamera to rotate it    // internally.    switch (orientation) {      // Not supported on iOS as of 11/2021, but add it here just in case.      case ScreenOrientation.Orientation.PORTRAIT_DOWN:        return 180,      case ScreenOrientation.Orientation.LANDSCAPE_LEFT:        return 270,      case ScreenOrientation.Orientation.LANDSCAPE_RIGHT:        return 90,      default:        return 0,    }  },  if (!tfReady) {    return (      <View style={styles.loadingMsg}>        <Text>Loading...</Text>      </View>    ),  } else {    return (      // Note that you don't need to specify `cameraTextureWidth` and      // `cameraTextureHeight` prop in `TensorCamera` below.      <View        style={          isPortrait() ? styles.containerPortrait : styles.containerLandscape        }      >        <TensorCamera           ref={cameraRef}          style={styles.camera}          autorender={AUTO_RENDER}          type={Camera.Constants.Type.front}          // tensor related props          resizeWidth={getOutputTensorWidth()}          resizeHeight={getOutputTensorHeight()}          resizeDepth={3}          rotation={getTextureRotationAngleInDegrees()}          onReady={handleCameraStream}                  />        {renderPose()}        {renderFps()}       </View>    ),  }}const styles = StyleSheet.create({  containerPortrait: {    position: 'relative',    width: CAM_PREVIEW_WIDTH,    height: CAM_PREVIEW_HEIGHT,    marginTop: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2,  },  containerLandscape: {    position: 'relative',    width: CAM_PREVIEW_HEIGHT,    height: CAM_PREVIEW_WIDTH,    marginLeft: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2,  },  loadingMsg: {    position: 'absolute',    width: '100%',    height: '100%',    alignItems: 'center',    justifyContent: 'center',  },  camera: {    width: '100%',    height: '100%',    zIndex: 1,  },  svg: {    width: '100%',    height: '100%',    position: 'absolute',    zIndex: 30,  },  fpsContainer: {    position: 'absolute',    top: 10,    left: 10,    width: 80,    alignItems: 'center',    backgroundColor: 'rgba(255, 255, 255, .7)',    borderRadius: 2,    padding: 8,    zIndex: 20,  },}),```Here is my package.json:```{  ""name"": ""mobile"",  ""version"": ""1.0.0"",  ""main"": ""node_modules/expo/AppEntry.js"",  ""scripts"": {    ""start"": ""expo start"",    ""android"": ""expo start --android"",    ""ios"": ""expo start --ios"",    ""web"": ""expo start --web"",    ""eject"": ""expo eject""  },  ""dependencies"": {    ""@mediapipe/pose"": ""^0.5.1635988162"",    ""@react-native-async-storage/async-storage"": ""^1.15.14"",    ""@react-navigation/core"": ""^6.1.0"",    ""@react-navigation/drawer"": ""^6.1.8"",    ""@react-navigation/material-bottom-tabs"": ""^6.0.9"",    ""@react-navigation/native"": ""^6.0.6"",    ""@react-navigation/stack"": ""^6.0.11"",    ""@tensorflow-models/pose-detection"": ""0.0.6"",    ""@tensorflow/tfjs"": ""^3.11.0"",    ""@tensorflow/tfjs-react-native"": ""^0.8.0"",    ""axios"": ""^0.24.0"",    ""expo"": ""~43.0.2"",    ""expo-camera"": ""~12.0.3"",    ""expo-font"": ""^10.0.3"",    ""expo-gl"": ""~11.0.3"",    ""expo-gl-cpp"": ""~11.0.1"",    ""expo-status-bar"": ""~1.1.0"",    ""formik"": ""^2.2.9"",    ""react"": ""17.0.1"",    ""react-dom"": ""17.0.1"",    ""react-native"": ""0.64.3"",    ""react-native-animatable"": ""^1.3.3"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-gesture-handler"": ""^1.10.2"",    ""react-native-paper"": ""^4.10.1"",    ""react-native-reanimated"": ""^2.2.4"",    ""react-native-safe-area-context"": ""^3.3.2"",    ""react-native-svg"": ""^12.1.1"",    ""react-native-vector-icons"": ""^9.0.0"",    ""react-native-web"": ""0.17.1"",    ""react-navigation"": ""^4.4.4"",    ""react-redux"": ""^7.2.6"",    ""redux"": ""^4.1.2"",    ""redux-thunk"": ""^2.4.1""  },  ""devDependencies"": {    ""@babel/core"": ""^7.12.9"",    ""@types/react"": ""~17.0.21"",    ""@types/react-native"": ""~0.64.12"",    ""typescript"": ""~4.3.5""  },  ""private"": true}```Example video of landmarks:https://user-images.githubusercontent.com/47552416/144524649-b1c1239f-edc2-4971-928d-03c7016506a6.mp4**Additional Issue**I am also dealing with issues when changing screens. Anytime I exit the screen with the pose detection or even close down the app, I receive this error:![Screen Shot 2021-11-27 at 5 46 24 PM](https://user-images.githubusercontent.com/47552416/143725958-ed60743b-d27c-4051-b176-b56128525c5b.png)On occasion I get his error with it:![Screen Shot 2021-11-27 at 5 46 32 PM](https://user-images.githubusercontent.com/47552416/143725950-c357d0ab-c5ae-4af4-80e5-9cc7a2ec64cf.png)","['Hi @eledahl,I need to look more on those errors (please ignore them for now). For the back camera, I think you need to flip the X coordinates on iOS when back camera is being used. Please see this [draft PR](https://github.com/tensorflow/tfjs-examples/pull/637) where I added a button to switch between front and back camera (still work in progress, e.g. back camera landscape mode not working correctly).Tested on iPhone 13.Thanks!=====', '@jinjingforever thank you! The changes in your draft fixed the issue for the back camera. Please let me know if you find anything about the unmounting error!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5897"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5897"">No</a>=====', 'Hi @eledahl I updated the demo with back camera support and I also added logic to properly clean up update loop (cancelAnimationFrame) when the app is unmounted.I took a look at the ""null is not an object"" error. Looks like it is caused by weird interactions between expo\'s ""fast refresh"" and the app that I don\'t quite understand yet. I tried unmounting the app/camera component in a regular app and didn\'t see this error. For now please ignore that error. Thanks!=====', 'Thank you @jinjingforever! I am not seeing the unmounting error anymore with the updates.=====']",1
https://github.com/tensorflow/tfjs/issues/3826,tfjs-node: is there a way to install and build from source always,15,closed,2020-08-19T07:00:13Z,2020-09-15T13:31:16Z,#### TensorFlow.js versionLatest#### Browser versionNode.js#### Describe the problem or feature requestSometimes we need to build from source for addon because of the different Glibc version between pre-builtin machine and target.,"['please check this link https://www.npmjs.com/package/@tensorflow/tfjs-node#mnist-demo-for-nodejs for building from source.=====', '@rthadur This looks a little bit complicated and not matched, and what this issue request is just like node-pre-gyp `--build-from-source` option which disables downloading the pre-compiled addon, which the idea is not the same as the demo.=====', '@yorkie what demo you are referring ?=====', ""It's https://www.npmjs.com/package/@tensorflow/tfjs-node#mnist-demo-for-nodejs :)====="", ""Could you elaborate a bit more on what you would like to do and how it differs from https://github.com/tensorflow/tfjs/blob/master/tfjs-node/package.json#L35 (`node-pre-gyp install --build-from-source`). I don't quite understand the request.====="", '@tafsiri This is what I want, but it seems no way to use it via `npm-install`.=====', 'Have you tried cloning this repository and running that script? You can checkout specific versions of tfjs via tags https://github.com/tensorflow/tfjs/tags=====', ""@tafsiri The tag doesn't work for me, because it can't run the script in `npm-script`.====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', '@yorkie. I don\'t understand what you mean by ""it can\'t run the script"". What output do you get when you run `npm run build-addon-from-source` (within the tfjs-node subfolder of this repository)?=====', 'What I mean is to able to execute `npm run build-addon-from-from-source` when running `npm install`.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', ""I'm not sure what you mean, there isn't enough detail in the issue. Could you provide a repository that we can clone with steps to reproduce the problem and what output you are getting vs what you would expect/want to see?====="", 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3826"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3826"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4824,Backend WASM does not implement kernel SparseToDense,1,open,2021-03-17T15:15:39Z,2021-06-03T16:15:38Z,attempting to run nanodet object detection model  converted from pytorch - conversion notes and example of model usage are at:  <https://gist.github.com/vladmandic/d0115e3899860391382d58550bd49b3f>```logengine.ts:540 Uncaught (in promise) Error: Kernel 'SparseToDense' not registered for backend 'wasm'    at Engine.runKernel (engine.ts:540)    at sparseToDense_ (sparse_to_dense.ts:88)    at sparseToDense__op (operation.ts:51)    at executeOp15 (slice_join_executor.ts:183)    at operation_executor.ts:92    at engine.ts:467    at Engine.scopedRun (engine.ts:478)    at Engine.tidy (engine.ts:465)    at tidy (globals.ts:192)    at operation_executor.ts:91```same graph model works without issues in nodejs with `tensorflow` backend and browser with `webgl` backendenvironment: tfjs 3.3.0 on ubuntu 20.10 with chrome 88,['cc @lina128 @jinjingforever ====='],1
https://github.com/tensorflow/tfjs/issues/1287,Missing call to tf.shuffle() in tfjs-examples/iris-fitDataset,1,closed,2019-02-26T04:33:52Z,2019-02-26T05:50:14Z,#### Describe the problem or feature requestThe [iris dataset](https://github.com/tensorflow/tfjs-examples/tree/master/iris-fitDataset) example in tfjs-examples is potentially [missing a tf.shuffle() call](https://github.com/tensorflow/tfjs-examples/blob/master/iris-fitDataset/data.js#L135) when using the new fitDataset API.,"['Shuffling appears on [line 115](https://github.com/tensorflow/tfjs-examples/blob/master/iris-fitDataset/data.js#L115).  It is an operation on the array itself, rather than the dataset.  This is a rather unusual case, as the entire dataset is so small.  See the TODO comment for a discussion of a better way to handle this, once dataset.cache() is implemented.If this answer is insufficient, please feel free to reopen.=====']",0
https://github.com/tensorflow/tfjs/issues/5713,Uncaught (in promise) TypeError: model.predict(...).data is not a function,3,closed,2021-10-11T11:55:14Z,2021-10-12T05:03:54Z,"I am trying to implement EAST text detection using tensorflowjs and after reading an image when I try to predict using the below code :```$(""#predict-button"").click(async function () {        let image = $(""#selected-image"").get(0),        let tensor = tf.browser.fromPixels(image)            .resizeNearestNeighbor([640, 320])            .expandDims(),    tnsr = tf.cast(tensor, 'float32')    let predictions = await model.predict(tnsr).data(),    const text  = Array.from(predictions)    console.log(text)```i am getting the above-mentioned error but when i remove the .data() part from```let predictions = await model.predict(tnsr).data(),```then code is working fine with output```(2) [e, e]```but the output is not what I am expecting, I am expecting the probabilities.Can anyone help me with this?Thanks.","['your model outputs TWO tensors, not one.```jsconst [output1, output2] = await model.predict(tnsr),const data1 = await output1.data()const data2 = await output2.data(),```which one is *probabilities* and what is the other one (maybe *classes* so its probability-per-class), that is up to your model.btw, imo, this type of a question is better suited for <https://stackoverflow.com/questions/tagged/tensorflow.js>=====', '@vladmandic you are correct , please refer at tfjs example for similar implementation. https://github.com/tensorflow/tfjs-examples/blob/master/mobilenet/index.js=====', 'Thanks you very much @vladmandic =====']",1
https://github.com/tensorflow/tfjs/issues/5932,tfjs-v3.12.0 and Wasm 3.12.0 for tfjs-models/pose-detection/ MoveNet multipose wasm,1,open,2021-12-09T17:03:33Z,2021-12-13T23:18:10Z,When I try to run the demo of the model : tfjs-models/pose-detection/ : https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenetWith tfjs-v3.12.0 and Wasm 3.12.0 and configuration model MoveNet type Multipose and backend Wasm i got the following error :Error: Kernel 'Reciprocal' not registered for backend 'wasm'![145433791-63df6bdb-87a1-4e08-a7cb-b72e41773c18](https://user-images.githubusercontent.com/32233417/145442149-e4f4c8e7-6c06-417b-92a3-8b591a4b64b1.png)The demo Woks well for the singlepose type (lightning).Regards,"[""Hi Na , the demo needs to be updated to latest version https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet and v3.12.0 is complaining about missing op 'Reciprocal'=====""]",1
https://github.com/tensorflow/tfjs/issues/4542,Error: The dict provided in model.execute(dict) has keys: [ToFloat] that are not part of graph,3,open,2021-01-15T18:14:54Z,2021-10-13T15:13:21Z,"This template is for miscellaneous issues not covered by the other issue categories.For questions on how to work with TensorFlow.js, or support for problems that are not verified bugs in TensorFlow.js, please go to [StackOverflow](https://stackoverflow.com/tags/tensorflow.js).hi all, I have got the captioned error when running the following scripts for Google AutoML multiple object detection. Is there any clue ?[index.txt](https://github.com/tensorflow/tfjs/files/5822253/index.txt)tfjs:17 Uncaught (in promise) Error: The dict provided in model.execute(dict) has keys: [ToFloat] that are not part of graph    at e.t.checkInputs (tfjs:17)    at e.<anonymous> (tfjs:17)    at u (tfjs:17)    at Generator._invoke (tfjs:17)    at Generator.forEach.e.<computed> [as next] (tfjs:17)    at Um (tfjs:17)    at o (tfjs:17)    at tfjs:17    at new Promise (<anonymous>)    at e.<anonymous> (tfjs:17)","['@startonggithub is it possible to share the model which you are using ?=====', 'hi rthadur, here is the model. Thanks. [model.zip](https://github.com/tensorflow/tfjs/files/5845242/model.zip)=====', 'having same issue please i need help=====']",1
https://github.com/tensorflow/tfjs/issues/2361,[webgpu] Add nonMaxSuppression kernel.,0,closed,2019-11-08T19:42:00Z,2020-05-20T15:49:25Z,Used in handtracking.,[],0
https://github.com/tensorflow/tfjs/issues/5493,tfjs-backend-webgpu incorrect tree shaking when bundling,2,closed,2021-08-14T14:58:29Z,2021-08-14T21:53:48Z,"`tfjs-backend-webgpu` does not define `sideEffects` in it's `package.json` causing incorrect tree shaking to occur  result is that entire `flags_webgpu.ts` gets dropped so none of the global variables get registeredat minimum, it should follow similar `sideEffects` settings as defined in `tfjs-backend-webgl` package  but also take a look at open issue #5182environment: chrome/94 canary, tfjs 3.8.0, tfjs-backend-webgpu 0.0.1-alpha.7cc @qjia7","['closing as resolved when building `tfjs-backend-webgpu` from main branch,  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5493"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5493"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5112,use posenet.js with flutter mobile,2,closed,2021-05-23T20:05:17Z,2021-06-10T15:30:09Z,"I realy want to use    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet""></script> in flutter web , so I put scripts  and this test code in index.html :    posenet.load().then(function(net) {      const pose = net.estimateSinglePose(imageElement, {        flipHorizontal: true      }),      return pose,    }).then(function(pose){      console.log(pose),    })but how to call it in dart? ","['Hi @R-abodyak, to call it in dart, you can use dart:js library (https://api.dart.dev/stable/2.12.2/dart-js/dart-js-library.html) to wrap essential TF.js methods. We can also provide such dart wrapper if needed, please let us know if you want to request this feature.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/3618,I'm having issue when trying to convert keras model to json,14,closed,2020-07-17T08:29:36Z,2021-12-19T12:20:02Z,"I found an error where it could not find my keras model (input path) and the output path, I think it's because of the space character between ""My Drive"", since I was using google drive to load and save the model. And I think you can't change the name ""My Drive"".So Instead of using google drive, I was using the local google colab's drive. So then I uploaded the keras model and convert it and save it in the google colab's drive and downloaded it.So If it's a bug, I hope in the future it will be fixed. Thank you.EnvironmentGoogle ColabSystem informationTensorFlow 2.2tensorflowjs ver2.0.1.post1Codefrom google.colab import drivedrive.mount('/content/gdrive')!tensorflowjs_converter --input_format=keras '/content/gdrive/My Drive/model/model.h5' '/content/gdrive/My Drive/json'Errorusage: TensorFlow.js model converters. [-h][--input_format {keras,tf_frozen_model,keras_saved_model,tf_hub,tfjs_layers_model,tf_saved_model}][--output_format {keras_saved_model,tfjs_graph_model,tfjs_layers_model,keras}][--signature_name SIGNATURE_NAME][--saved_model_tags SAVED_MODEL_TAGS][--quantize_float16 [QUANTIZE_FLOAT16]][--quantize_uint8 [QUANTIZE_UINT8]][--quantize_uint16 [QUANTIZE_UINT16]][--quantization_bytes {1,2}][--split_weights_by_layer] [--version][--skip_op_check][--strip_debug_ops STRIP_DEBUG_OPS][--weight_shard_size_bytes WEIGHT_SHARD_SIZE_BYTES][--output_node_names OUTPUT_NODE_NAMES][--control_flow_v2 CONTROL_FLOW_V2][input_path] [output_path]TensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My Drive/json","['@lexms that is expected , try to add backslash before space or addd quoted surrounding the whole path , then you will be able to execute the command =====', ""As you can see in the example above I've quoted the whole path, but it still gave an error.And also I've tried adding a backslash before space and it doesn't work out.or perhaps I've mistaken, could you give me an example? Thanks====="", 'please try forward slash or try to run as below !tensorflowjs_converter --input_format=keras \'/content/gdrive/My Drive/model/model.h5\' ""/content/gdrive/My Drive/json""or !tensorflowjs_converter --input_format=keras \'/content/gdrive/My Drive/model/model.h5\' \'/content/gdrive/My\\ Drive/json\'=====', 'It still gave the same errorFirst code error-----------------------TensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My Drive/jsonSecond code error-----------------------TensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My\\ Drive/json=====', ""sorry , can you try this `!tensorflowjs_converter --input_format=keras '/content/gdrive/My\\ Drive/model/model.h5' '/content/gdrive/My\\ Drive/json'` i guess there needs one more slash ====="", 'Still the same errorTensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My\\ Drive/json=====', '@lexms can you move the model file to a directory without spaces as part of the name, and output to the same directory? =====', ""> @lexms can you move the model file to a directory without spaces as part of the name, and output to the same directory?Yes, actually that's how I resolve the problem, as I explained it in the description above. But, I find it weird that I couldn't use the google drive====="", '@lexms So the basic problem is how to represent directory with space as a single param to the command line, you can use double quotes ""/content/gdrive/My\\ Drive/model/model.h5"" instead of single quote. You might not need the backslash.This is based on some [quick googling](https://stackoverflow.com/questions/18639663/how-to-pass-command-line-parameters-with-space-in-batch-file).=====', ""@pyu10055 Well yes you're right, but it seems it doesn't work in Colab, It doesn't recognize the argument even with double quotes or with a backslash, but thank you for the sources.**error:**TensorFlow.js model converters.: error: unrecognized arguments: /content/gdrive/My\\ Drive/json====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'same issue, dobule quotes and backslash do not work in Colab=====', 'issue still exists. =====']",0
https://github.com/tensorflow/tfjs/issues/5816,is:issue is:open UnhandledPromiseRejectionWarning: TypeError: forwardFunc is not a function: need to update package.json version of @tensorflow/tfjs-node,4,closed,2021-11-05T12:52:11Z,2021-11-19T16:50:41Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem****Provide the exact sequence of commands / steps that you executed before running into the problem****Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.","['As the error suggests please update your package.json to latest version and include only `tfjs-node` in the packages.Finally run npm install.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5816"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5816"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5007,Possible memory leak in tfjs-node,1,closed,2021-04-28T23:10:46Z,2021-04-29T15:37:15Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA- TensorFlow.js installed from (npm or script link): NA- TensorFlow.js version (use command below): 3.5- Browser version: NA- Tensorflow.js Converter Version: NA**Describe the current behavior**`process.memoryUsage().rss` in node.js shows a steady increase with time. All the tensors created are wrapped inside `tf.tidy()````jsconst result = tf.tidy(() => {    const x = {        'Age': tf.tensor([1]),        ...      }      // logger(`x = ${JSON.stringify(x)}`)    return predict(model, x)  }),```**Describe the expected behavior**RSS memory should be freed up regularly by the node.js process to prevent it from crashing due OOM**Standalone code to reproduce the issue**Simply push the **Run** button in https://replit.com/@NitinPasumarthy/memory-leak-tfjs-node#index.js and observe the latest memory-usage-[[timestamp]].log CSV file**Other info / logs** Include any logs or source code that would be helpful tohttps://replit.com/@NitinPasumarthy/memory-leak-tfjs-node#memory-usage-1619649567559.logMany such logs are available with small variations of code","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5007"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5007"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/868,Optimize the tf.rfft / tf.irfft op by calling an rfft kernel.,0,open,2018-11-01T14:45:49Z,2018-11-01T14:46:18Z,Currently we call into the fft kernel and then throw out the parts we don't need. We can optimize this by running a shader in WebGL and calling into the correct rfft kernel in Node.js.,[],0
https://github.com/tensorflow/tfjs/issues/694,Packed matMul should work even when we can't render to floating point textures,0,closed,2018-09-11T18:41:11Z,2018-10-11T12:09:37Z,"Currently, we only turn on packed matMul when `WEBGL_RENDER_FLOAT32_ENABLED` is true.",[],0
https://github.com/tensorflow/tfjs/issues/5172,Issue converting keras model to JavaScript and subsequent loading,2,open,2021-06-04T12:59:02Z,2021-08-31T04:54:57Z,"I am having difficulty loading a model that has been converted using the 'TensorFlow.js converter' . Any help would be appreciated!**System information**Google ColabP5jsTensorflow 2.5.0TensorFlow.js Converter v3.7.0**Describe the current behavior**- Unable to load a model that has been converted from 'keras_saved_model' into 'tfjs_layers_model' format using 'TensorFlow.js converter'. The following errors appear in console upon attempts to load via the P5.js editor & 'tf.loadLayersModel(MODEL_PATH)':initializers.js:131 The shape of the input tensor ([null,148,148,256]) does not match the expectation of layer conv2d_1: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,73,73,256]) does not match the expectation of layer conv2d_2: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,69,69,128]) does not match the expectation of layer conv2d_3: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,32,32,128]) does not match the expectation of layer conv2d_4: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,30,30,64]) does not match the expectation of layer conv2d_5: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,14,14,64]) does not match the expectation of layer conv2d_6: [null,150,150,3]initializers.js:131 The shape of the input tensor ([null,5,5,32]) does not match the expectation of layer conv2d_7: [null,150,150,3]util_base.js:154 Uncaught (in promise) Error: Based on the provided shape, [256], the tensor should have 256 values but has 0    at Wv (util_base.js:154)    at Mw (engine.js:1086)    at Lw (tape.js:54)    at Ww (engine.js:1190)    at RM (container.js:211)    at container.js:196    at c (runtime.js:63)    at Generator._invoke (runtime.js:293)    at Generator.next (runtime.js:118)    at bv (runtime.js:747)- Saving the model as 'keras' format (model.h5) and converting into 'tfjs_layers_model' format does not resolve the issue- Manually editting the model.json file resolves the ""initializers.js:131..."" errors but not the ""util_base.js:154..."" error**Standalone code to reproduce the issue**https://colab.research.google.com/drive/1EhMHvteiQqPGvZKti6cvjGuDspDXzDXP?usp=sharing**Other info / logs**[model.zip](https://github.com/tensorflow/tfjs/files/6598599/model.zip)[modelConverted.zip](https://github.com/tensorflow/tfjs/files/6598601/modelConverted.zip)[modelDescription.zip](https://github.com/tensorflow/tfjs/files/6598625/modelDescription.zip)","['@josephzonghaozhu sorry for the delay , were you able to run the model before converting ? =====', 'Thanks for taking a look! I was able to run the model with no issues before converting, I added an example at the bottom of the colab =====']",1
https://github.com/tensorflow/tfjs/issues/4622,Tensorflow.js implimentation error on Blazor Server Application,4,closed,2021-01-31T05:48:22Z,2021-02-16T17:00:24Z,"Hi, Need one help, am trying to implement tensorflow.js into my Blazor Server application.  But in the java script  Promise.all([        faceapi.nets.tinyFaceDetector.loadFromUri('models'),        faceapi.nets.faceLandmark68Net.loadFromUri('models'),        faceapi.nets.faceExpressionNet.loadFromUri('models')    ]).then(beginVideo),getting an error face-api.min.js:1 Uncaught (in promise) Error: Based on the provided shape, [3,3,32,1], the tensor should have 288 values but has 281https://github.com/felixkjoseph/BlazorTensorflowJS.gitThanks in advance","['@felixkjoseph Not familiar with Blazor, purely based on your description, this could be an network issue, (i.e. CORS etc) You should check network tab (if this runs in browser) to see if the model files are downloaded correctly.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4622"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4622"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5040,The Node.js native addon module (tfjs_binding.node) can not be found,4,closed,2021-05-05T17:33:54Z,2021-05-07T02:42:29Z,"**System information**- OS Platform and Distribution: Windows 10 version 1909- TensorFlow.js installed from: `npm i @tensorflow/tfjs-node`- TensorFlow.js version: none- tfjs-node version: `3.6.1`**Describe the problem**I successfully installed tfjs-node after installing `windows-build-tools@5.2.2` (2017) and Python v2.7.2. When I run the app, it says that the native addon module `tfjs_binding.node` can not be found. I've tried install `@mapbox/node-pre-gyp@1.0.4` globally but it doesn't help. When I run rebuild from source, it returns errors as below. I've also read and tried all solutions for previous relative issues but none works, including copying `deps/libs` to `lib/napi-v7`.**Any other info / logs**- `node app.js````node-pre-gyp info This Node instance does not support builds for N-API version 8node-pre-gyp info This Node instance does not support builds for N-API version 8C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\dist\index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +    ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node --build-addon-from-source' to rebuild the native addon module.If you have problem with building the addon module, please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\dist\index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at Module.require (node:internal/modules/cjs/loader:997:19)    at require (node:internal/modules/cjs/helpers:92:18)    at Object.<anonymous> (C:\Users\User1\Desktop\mnist\app.js:2:1)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)```- `npm rebuild @tensorflow/tfjs-node build-addon-from-source````...npm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnpm ERR! node-pre-gyp info it worked if it ends with oknpm ERR! node-pre-gyp info using node-pre-gyp@0.14.0npm ERR! node-pre-gyp info using node@15.5.0 | win32 | x64npm ERR! node-pre-gyp WARN Using needle for node-pre-gyp https downloadnpm ERR! node-pre-gyp info This Node instance does not support builds for N-API version 8npm ERR! node-pre-gyp info check checked for ""C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node"" (not found)npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp http 404 https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@3.6.1 and node@15.5.0 (node-v88 ABI, unknown) (falling back to source compile with node-gyp)npm ERR! node-pre-gyp http 404 status code downloading tarball https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@7.1.2npm ERR! gyp info using node@15.5.0 | win32 | x64npm ERR! gyp info oknpm ERR! node-pre-gyp info This Node instance does not support builds for N-API version 8npm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@7.1.2npm ERR! gyp info using node@15.5.0 | win32 | x64npm ERR! gyp info find Python using Python version 2.7.2 found at ""C:\Python27\python.exe""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS msvs_version was set from command line or npm confignpm ERR! gyp ERR! find VS - looking for Visual Studio version 2017npm ERR! gyp ERR! find VS VCINSTALLDIR not set, not running in VS Command Promptnpm ERR! gyp ERR! find VS checking VS2017 (15.9.28307.1500) found at:npm ERR! gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""npm ERR! gyp ERR! find VS - found ""Visual Studio C++ core features""npm ERR! gyp ERR! find VS - found VC++ toolset: v141npm ERR! gyp ERR! find VS - missing any Windows SDKnpm ERR! gyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usenpm ERR! gyp ERR! find VS looking for Visual Studio 2015npm ERR! gyp ERR! find VS - not foundnpm ERR! gyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS valid versions for msvs_version:npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! configure errornpm ERR! gyp ERR! stack Error: Could not find any Visual Studio installation to usenpm ERR! gyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16npm ERR! gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16npm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:340:5)npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:376:20)npm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1063:16)...```","['@uahnbu The root cause seems to be related to your visual studio installation:npm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************=====', ""@pyu10055 There's no instruction to install VS on `tfjs-node` gitpage, which is really obstructive. The `node-gyp` link instructed that **_VS Build 2017_** with **_Visual C++ build tools_** is enough as a replacement for **_Desktop development with C++_**. I've tried installing both **_VS Build_** and **_VS Community latest 2019_** version from external website for 3GB instead of `npm install` above but I can't install `tfjs-node` at all with them. I've currently installed the following components:![image](https://user-images.githubusercontent.com/27907396/117342345-560aa900-aecd-11eb-88ce-0bfca8f6beef.png)The error says that Windows SDK is missing and I don't really know what it means.====="", ""Hmm, I think that I have to restart the computer to make VS Build Tools (2017) work. That's weird but it works now. The 2019 versions doesn't work still although it has forced a restart.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5040"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5040"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3788,Facemesh model (const model = await facemesh.load()) stopped working after this weeks update,1,closed,2020-08-13T14:17:17Z,2020-11-20T21:39:20Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.1/dist/tf.min.js""></script><script src=""https://unpkg.com/@tensorflow/tfjs-converter@2.1.0/dist/tf-converter.js""></script>#### Browser versionVersion 84.0.4147.125 (Official Build) (64-bit)#### Describe the problem or feature requestHave a project that uses TF JS for object detection and facial landmark detection. After this weeks update (for face rotation) facemesh.load no longer works? code taken from the demo and docs: https://github.com/tensorflow/tfjs-models/tree/master/facemeshWorked perfectly until the update, I've tried setting it to a previous version but its throwing different errors including:facemesh:17 Uncaught (in promise) TypeError: e.image.rotateWithOffset is not a function    at facemesh:17    at Array.map (<anonymous>)    at facemesh:17    at engine.js:425    at t.e.scopedRun (engine.js:436)    at t.e.tidy (engine.js:423)    at Object.oS [as tidy] (globals.js:182)    at t.<anonymous> (facemesh:17)    at facemesh:17    at Object.next (facemesh:17)Does anyone have a work around? New to JS and TensorFlow but have tried everything I can think of to fix the issue and can't get anywhere. Thanks in advance ","[""Hi @Laurenceldp - thanks for filing this issue. I noticed that you're using TFJS 2.0.1 - I think the issue will go away if you upgrade to TFJS 2.1.0.Also wanted to note in your snippet you're loading both tf.js and tf-converter.js - the converter is actually included in tf.js, so might be able to remove that script tag!=====""]",0
https://github.com/tensorflow/tfjs/issues/1864,PoseNet models don't accept batch sizes greater than 1.,3,open,2019-08-19T21:33:55Z,2019-09-23T18:57:02Z,"I'm writing some custom code that is to feed a batch of images into the PoseNet models for inference, in my case its a series of frames from long videos.  I would eventually  open a PR that supports this functionality.However, when feeding a batch size of greater than one to both the MobileNet and ResNet models, you get an error:> Error: The shape of dict['sub_2'] provided in model.execute(dict) must be [1,-1,-1,3], but was [5,537,953,3]A way to simply reproduce the error source is:```  const url =      'https://storage.googleapis.com/tfjs-models/savedmodel/posenet/resnet50/quant1/model-stride16.json'  const graphModel = await tf.loadGraphModel(url),  const inputTensor = tf.zeros([5, 100, 100, 3]),  const result = graphModel.predict(inputTensor),```The above code results in an error:> UnhandledPromiseRejectionWarning: Error: The shape of dict['sub_2'] provided in model.execute(dict) must be [1,-1,-1,3], but was [5,100,100,3]The same thing occurs for other model urls.CC @tylerzhu-github","['CC @tylerzhu-github gentle ping to check this error.=====', 'Thanks for the reminder!Yes that will require exporting a model that support batch size > 1.Will handle that in the next release, timeline would be end of next quarter.=====', '@tylerzhu-github thanks!  Does this require re-training the model or just a modification to how the model is exported?=====']",0
https://github.com/tensorflow/tfjs/issues/5603,Installation failed for tfjs-node 3.9.0 on windows with Node 12,3,closed,2021-09-10T12:09:18Z,2021-09-13T12:56:55Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.9.0- CUDA/cuDNN version: NAnpm install of tfjs-node fails on windows**Provide the exact sequence of commands / steps that you executed before running into the problem**npm i @tensorflow/tfjs-nodeLogs> @tensorflow/tfjs-node@3.9.0 install C:\Users\xxxxxxxxxxxxxxxxxxxx\node_modules\@tensorflow\tfjs-node> node scripts/install.jsCPU-windows-3.9.0.zip* Downloading libtensorflowhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.5.0.zip[==============================] 8649874/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.9.0/CPU-windows-3.9.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.9.0 and node@12.22.5 (node-v72 ABI, unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.9.0/CPU-windows-3.9.0.zipgyp WARN install got an error, rolling back installgyp ERR! configure errorgyp ERR! stack Error: self signed certificate in certificate chaingyp ERR! stack     at TLSSocket.onConnectSecure (_tls_wrap.js:1502:34)gyp ERR! stack     at TLSSocket.emit (events.js:314:20)gyp ERR! stack     at TLSSocket._finishInit (_tls_wrap.js:937:8)gyp ERR! stack     at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:711:12)gyp ERR! System Windows_NT 10.0.18363gyp ERR! command ""C:\\Users\\xxxxxxxxxxxxx\\node.exe"" ""C:\\Users\\txxxxxxxxxxx\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\Users\\xxxxxxxxxxxxxx\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=C:\\Users\\xxxxxxxxxxx\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""gyp ERR! cwd C:\Users\xxxxxxxx\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v12.22.5gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Users\xxxxxxxxx\node.exe C:\Usersxxxxxxxxxx\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=C:\Users\xxxxxxxxx\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=C:\Users\xxxxxxxx\node_modules\@tensorflow\tfjs-node\lib\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (C:\Users\xxxxx\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:314:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1022:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:287:5)node-pre-gyp ERR! System Windows_NT 10.0.18363node-pre-gyp ERR! command ""C:\\Users\\xxxxxxxx\\node.exe"" ""C:\\Users\\xxxxxxxxnode_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd C:\Users\xxxxxxxxx\node_modules\@tensorflow\tfjs-nodenode-pre-gyp ERR! node -v v12.22.5node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not ok","[""The 404 on the prebuilt binary is actually expected since I don't believe we ship Node-API version 8 binaries yet. This should not matter though, since the install should fall back to compiling from source, and it looks like that's where it failed.This looks like it could be related to certificates, since it's complaining about a self-signed certificate in the chain (`gyp ERR! stack Error: self signed certificate in certificate chain`). Apparently, this can be caused by being behind a corporate firewall. Certificates are outside my area of expertise, but you might find an answer on [this issue](https://github.com/nodejs/node-gyp/issues/695#issuecomment-177260446).====="", ""Hello, thanks for the link where I find a work around to install for the first time the tensorflow nodejs librariries.Using set NODE_TLS_REJECT_UNAUTHORIZED=0 before launching the install. Didn't succeed with the solution of default certificates  up to now.Thank you====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5603"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5603"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5004,tensorflow/tfjs import error in react js,7,closed,2021-04-28T17:28:03Z,2021-05-13T00:20:24Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- Windows 10- TensorFlow.js installed from npm - TensorFlow.js version: 3.5.0- CUDA/cuDNN version: NO**Describe the problem**As I try to import  import tensorflow into my component i get the error as such : ./node_modules/@tensorflow/tfjs-core/dist/io/http.jsError: C:\Users\sony\Desktop\rohit\decentragram-master\node_modules\@tensorflow\tfjs-core\dist\io\http.js: Expected type ""Expression"" with option {}, but instead got ""SpreadElement"".    at Array.map (<anonymous>)    at Array.forEach (<anonymous>)**Provide the exact sequence of commands / steps that you executed before running into the problem**const tf = require('@tensorflow/tfjs-node'),**Any other info / logs**I have installed @tensorflow/tfjs-node in my project but the dependency is not visible in the package.json file ","['@RohitGamare please provide the steps you are following to install and sample code if possible. Thank you =====', '> @RohitGamare please provide the steps you are following to install and sample code if possible. Thank younpm install @tensorflow/tfjsnpm install @tensorflow/tfjs-corenpm install @tensorflow/tfjs-nodenpm install @tensorflow/tfjs-node --unsafe-perm=true --allow-rootabove been the commands i tried =====', 'npm install @tensorflow/tfjs-node will install all the dependencies for tfjs and tfjs core ,also check windows trouble shooting guide here https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md=====', '> npm install @tensorflow/tfjs-node will install all the dependencies for tfjs and tfjs core ,also check windows trouble shooting guide here https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.mdThank you for the response =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5004"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5004"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5797,@tensorflow/tfjs-node installation failed in windows system,4,open,2021-10-30T13:21:35Z,2021-12-23T18:22:26Z,"I am using node version v10.24.1, Win 10 OS, VSCode IDE 1.61.2.  I have installed C/C++ extension to VSCode but still when I try to install @tensorflow/tfjs-node npm package it throws me error.node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.10.0/CPU-windows-3.10.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.10.0 and node@10.24.1 (node-v64 ABI, unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.10.0/CPU-windows-3.10.0.zipgyp ERR! find VSgyp ERR! find VS msvs_version not set from command line or npm configgyp ERR! find VS VCINSTALLDIR not set, not running in VS Command Promptgyp ERR! find VS checking VS2019 (16.11.31729.503) found at:gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""gyp ERR! find VS - ""Visual Studio C++ core features"" missinggyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usegyp ERR! find VS looking for Visual Studio 2015gyp ERR! find VS - not foundgyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8gyp ERR! find VSgyp ERR! find VS **************************************************************gyp ERR! find VS You need to install the latest version of Visual Studiogyp ERR! find VS including the ""Desktop development with C++"" workload.gyp ERR! find VS For more information consult the documentation at:gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsgyp ERR! find VS **************************************************************gyp ERR! find VSgyp ERR! configure errorgyp ERR! stack Error: Could not find any Visual Studio installation to usegyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)gyp ERR! stack     at findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16)gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)gyp ERR! stack     at findVisualStudio2015 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14)gyp ERR! stack     at regSearchKeys (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16)gyp ERR! stack     at regGetValue (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16gyp ERR! stack     at ChildProcess.exithandler (child_process.js:301:5)gyp ERR! stack     at ChildProcess.emit (events.js:198:13)gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)gyp ERR! System Windows_NT 10.0.10240gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=E:\\learning\\ReactJS\\nextapp\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v7\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=E:\\learning\\ReactJS\\nextapp\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v7"" ""--napi_version=7"" ""--node_abi_napi=napi"" ""--napi_build_version=7"" ""--node_napi_label=napi-v7""gyp ERR! cwd E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v10.24.1gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node --module_name=tfjs_binding --module_path=E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-node\lib\napi-v7 --napi_version=7 --node_abi_napi=napi --napi_build_version=7 --node_napi_label=napi-v7' (1)node-pre-gyp ERR! stack     at ChildProcess.cmd.on (E:\learning\ReactJS\nextapp\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:198:13)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:259:5)node-pre-gyp ERR! System Windows_NT 10.0.10240node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""E:\\learning\\ReactJS\\nextapp\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-nodenode-pre-gyp ERR! node -v v10.24.1node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not oknpm WARN @babel/plugin-syntax-jsx@7.14.5 requires a peer of @babel/core@^7.0.0-0 but none is installed. You must install peer dependencies yourself.npm WARN @typescript-eslint/parser@4.33.0 requires a peer of eslint@^5.0.0 || ^6.0.0 || ^7.0.0 but none is installed. You must install peer dependencies yourself.npm WARN eslint-config-next@11.1.2 requires a peer of eslint@^7.23.0 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-jsx-a11y@6.4.1 requires a peer of eslint@^3 || ^4 || ^5 || ^6 || ^7 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-react@7.26.1 requires a peer of eslint@^3 || ^4 || ^5 || ^6 || ^7 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-react-hooks@4.2.0 requires a peer of eslint@^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 but none is installed. You must install peer dependencies yourself.npm WARN tsutils@3.21.0 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-darwin-x64@11.1.2 (node_modules\@next\swc-darwin-x64):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-darwin-x64@11.1.2: wanted {""os"":""darwin"",""arch"":""x64""} (current: {""os"":""win32"",""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-darwin-arm64@11.1.2 (node_modules\@next\swc-darwin-arm64):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-darwin-arm64@11.1.2: wanted {""os"":""darwin"",""arch"":""arm64""} (current: {""os"":""win32"",""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-linux-x64-gnu@11.1.2 (node_modules\@next\swc-linux-x64-gnu):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-linux-x64-gnu@11.1.2: wanted {""os"":""linux"",""arch"":""x64""} (current: {""os"":""win32"",""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""win32"",""arch"":""x64""})npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@3.10.0 install: `node scripts/install.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node@3.10.0 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.","['Can you please try below commands : ```sudo apt-get updatesudo apt-get install build-essentialnpm i node-pre-gyp -g(if your using tfjs-node-gpu)npm rebuild @tensorflow/tfjs-node-gpu --build-from-source(or if your using tfjs-node instead)npm rebuild @tensorflow/tfjs-node --build-from-source```=====', 'Thanks for reply. sudo commands not working, I think it is linux based command I am windows user. and second thing I am not able to run ""npm install @tensorflow/tfjs-node"" command I throws error above. I am windows user (windows 10)=====', ""I've had a similar issue so hopefully this will work for you.First check these guidelines on [how to set up](https://github.com/Microsoft/nodejs-guidelines/blob/master/windows-environment.md#environment-setup-and-configuration) your computer for node,js  You need Visual Studio (msvs) 2017 or 2019 since I've personally ran into some issues with the 2022 and 2015 versions.Option 1 seems to not be working for a lot of people, so just download Visual Studio Community 2017 or 2019 from option 2 and make sure you have python 2.7 installed on your computer.Then you either configure your npm to run said version of msvs`npm config set msvs_version 2019`Or you just force the node installation with it`npm install @tensorflow/tfjs-node --msvs_version=2019`====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",1
https://github.com/tensorflow/tfjs/issues/4592,Minor typo (I believe),4,closed,2021-01-23T23:08:26Z,2021-02-02T18:51:33Z,```Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: [path]\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v7\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source' to rebuild the native addon module.```> Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source'should be> Please run command 'npm rebuild @tensorflow/tfjs-node-gpu --build-addon-from-source'if I'm not mistaken...I'd make a Pull Request myself if I knew which file to change.,"['Here is the file https://github.com/tensorflow/tfjs/blob/master/tfjs-node/src/index.ts#L43 , where you can submit a PR. Thank you =====', ""Oh, is that file used for both the GPU and non-GPU versions? Would the best solution be to add a conditional like```js// error message`'npm rebuild @tensorflow/tfjs-node`+(String(bindingPath).indexOf('@tensorflow\\tfjs-node-gpu\\') > 0 ? `-gpu` : ``) +` --build-addon-from-source' to ` +// error message```?====="", 'cc @pyu10055 @lina128 =====', '@EFHIII looks good, I will suggest to just check for string `tfjs-node-gpu` instead to avoid dealing with file separators.=====']",1
https://github.com/tensorflow/tfjs/issues/5856,results of `nonMaxSuppression` are incorrect for non-clamped boxes,2,closed,2021-11-16T14:43:12Z,2021-11-16T15:54:30Z,"some models return non-clamped outputs (example is very popular `blazeface` detector) and in that case, running `nonMaxSuppression` will produce incorrect results *if* box coordinates are bellow 0.below are example input image and two boxes that are returned as over `iouThreshold`  ![in](https://user-images.githubusercontent.com/57876960/142005818-9d62f380-5fa2-43d1-8bdf-aa64e05dc3ca.jpg)![out](https://user-images.githubusercontent.com/57876960/142005826-a289e6bf-86f1-421a-940d-f2e82baa6dd4.jpg)reducing `iouThreshold` to any value does not help as `nonMaxSuppression` calculates it as `0` while its clear that first box is almost completely overlapped with second boxcapturing values inside `tfjs-core/src/backends/non_max_suppression_impl.ts:intersectOverUnion`:```json{  ""iCoord"": { ""0"": 115.93872833251953, ""1"": 42.05706787109375, ""2"": 172.44512939453125, ""3"": 98.56344604492188 },  ""jCoord"": { ""0"": -25.357688903808594, ""1"": -12.717769622802734, ""2"": 97.00607299804688, ""3"": 109.66532897949219 },  ""areaI"": 3192.9720676520374,  ""areaJ"": 14975.256338182517,  ""intersectionYmin"": 115.93872833251953,  ""intersectionYmax"": 97.00607299804688,  ""intersectionXmin"": 42.05706787109375,  ""intersectionXmax"": 98.56344604492188,  ""intersectionArea"": 0}```shows that `intersectionArea` is incorrectly calculatedenvironment: tfjs 3.11backend or os are not relevant since `nms` implementation is in `tfjs-core`","['ignore, my fault for incorrect box transformation before nms.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5856"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5856"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3999,node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-build,6,closed,2020-10-01T09:03:16Z,2021-07-24T21:53:09Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):`NO`- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):```ProductName:    Mac OS XProductVersion: 10.14.5BuildVersion:   18F132```- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:`Not applicable`- TensorFlow.js installed from (npm or script link):Latest version from command `npm i @tensorflow/tfjs-node`- TensorFlow.js version (use command below):- Browser version:`Not applicable` using `Node.JS`- Tensorflow.js Converter Version:`Not applicable`**Describe the current behavior**Installation error after `npm i @tensorflow/tfjs-node````> @tensorflow/tfjs-node@2.4.0 install /Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-darwin-2.4.0.tar.gz* Downloading libtensorflow[==============================] 8801882/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download node-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v5/2.4.0/CPU-darwin-2.4.0.tar.gz node-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@2.4.0 and node@12.16.1 (node-v72 ABI, unknown) (falling back to source compile with node-gyp) Traceback (most recent call last):  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/gyp_main.py"", line 50, in <module>    sys.exit(gyp.script_main())  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 554, in script_main    return main(sys.argv[1:])  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 547, in main    return gyp_main(args)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 532, in gyp_main    generator.GenerateOutput(flat_list, targets, data, params)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 2215, in GenerateOutput    part_of_all=qualified_target in needed_targets)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 845, in Write    mac_bundle_deps, extra_outputs, part_of_all)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1539, in WriteTarget    self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1896, in GetSortedXcodePostbuildEnv    additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1885, in GetSortedXcodeEnv    additional_settings)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1616, in GetSortedXcodeEnv    additional_settings)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1527, in _GetXcodeEnv    if XcodeVersion() >= '0500' and not env.get('SDKROOT'):TypeError: '>=' not supported between instances of 'tuple' and 'str'gyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:311:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12)gyp ERR! System Darwin 18.6.0gyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5"" ""--napi_version=5"" ""--node_abi_napi=napi"" ""--napi_build_version=5"" ""--node_napi_label=napi-v5""gyp ERR! cwd /Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v12.16.1gyp ERR! node-gyp -v v5.0.5gyp ERR! not ok node-pre-gyp ERR! build error node-pre-gyp ERR! stack Error: Failed to execute '/usr/local/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --fallback-to-build --module=/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5 --napi_version=5 --node_abi_napi=napi --napi_build_version=5 --node_napi_label=napi-v5' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:311:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1021:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:286:5)node-pre-gyp ERR! System Darwin 18.6.0node-pre-gyp ERR! command ""/usr/local/bin/node"" ""/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/.bin/node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd /Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v12.16.1node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok```**Describe the expected behavior**the command `npm i @tensorflow/tfjs-node` should succeed.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.I have used to following install command in a clean folder```npm i @tensorflow/tfjs-node```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.`Not applicable` - log reported above","['@loretoparisi can you please check for a similar issue [here](https://github.com/tensorflow/tfjs/issues/3577) =====', '@rthadur Thanks please wait let me check if is related to one of both issues mentioned.=====', '@rthadur my first attempt was trying the suggestion in the first of your links [here](https://github.com/tensorflow/tfjs/issues/3016#issuecomment-608003327) so doing `@tensorflow/tfjs-node build-addon-from-source````$ npm rebuild @tensorflow/tfjs-node build-addon-from-source> @tensorflow/tfjs-node@2.4.0 install /Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-darwin-2.4.0.tar.gz* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download node-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v5/2.4.0/CPU-darwin-2.4.0.tar.gz node-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@2.4.0 and node@12.16.1 (node-v72 ABI, unknown) (falling back to source compile with node-gyp) Traceback (most recent call last):  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/gyp_main.py"", line 50, in <module>    sys.exit(gyp.script_main())  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 554, in script_main    return main(sys.argv[1:])  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 547, in main    return gyp_main(args)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 532, in gyp_main    generator.GenerateOutput(flat_list, targets, data, params)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 2215, in GenerateOutput    part_of_all=qualified_target in needed_targets)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 845, in Write    mac_bundle_deps, extra_outputs, part_of_all)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1539, in WriteTarget    self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1896, in GetSortedXcodePostbuildEnv    additional_settings={\'CHROMIUM_STRIP_SAVE_FILE\': strip_save_file})  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 1885, in GetSortedXcodeEnv    additional_settings)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1616, in GetSortedXcodeEnv    additional_settings)  File ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/xcode_emulation.py"", line 1527, in _GetXcodeEnv    if XcodeVersion() >= \'0500\' and not env.get(\'SDKROOT\'):TypeError: \'>=\' not supported between instances of \'tuple\' and \'str\'gyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:311:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12)gyp ERR! System Darwin 18.6.0gyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5"" ""--napi_version=5"" ""--node_abi_napi=napi"" ""--napi_build_version=5"" ""--node_napi_label=napi-v5""gyp ERR! cwd /Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v12.16.1gyp ERR! node-gyp -v v5.0.5gyp ERR! not ok node-pre-gyp ERR! build error node-pre-gyp ERR! stack Error: Failed to execute \'/usr/local/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --fallback-to-build --module=/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5/tfjs_binding.node --module_name=tfjs_binding --module_path=/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node/lib/napi-v5 --napi_version=5 --node_abi_napi=napi --napi_build_version=5 --node_napi_label=napi-v5\' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:311:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1021:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:286:5)node-pre-gyp ERR! System Darwin 18.6.0node-pre-gyp ERR! command ""/usr/local/bin/node"" ""/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/.bin/node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd /Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v12.16.1node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok @tensorflow/tfjs-node@2.4.0 /Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-node```=====', ""The [other](https://github.com/netlify-templates/victor-hugo/issues/186) approach I have used was to reinstall `npm` doing `sudo npm install npm@latest -g` so I have```$ npm --version6.14.8```and now it works! 💯 The problem now becomes th 2nd one, I have cited [here](https://github.com/tensorflow/tfjs/issues/4000)```ip-192-168-178-22:toxicity loretoparisi$ node test.js Platform node has already been set. Overwriting the platform with [object Object].Platform node has already been set. Overwriting the platform with [object Object]./Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-backend-cpu/dist/tf-backend-cpu.node.js:268var nonMaxSuppressionV3Impl = tf.kernel_impls.nonMaxSuppressionV3Impl,                                              ^TypeError: Cannot read property 'nonMaxSuppressionV3Impl' of undefined    at Object.<anonymous> (/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs-backend-cpu/dist/tf-backend-cpu.node.js:268:47)    at Module._compile (internal/modules/cjs/loader.js:1158:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1178:10)    at Module.load (internal/modules/cjs/loader.js:1002:32)    at Function.Module._load (internal/modules/cjs/loader.js:901:14)    at Module.require (internal/modules/cjs/loader.js:1044:19)    at require (internal/modules/cjs/helpers.js:77:18)    at Object.<anonymous> (/Users/loretoparisi/Documents/MyProjects/AI/tfjs-models/toxicity/node_modules/@tensorflow/tfjs/dist/tf.node.js:25:22)    at Module._compile (internal/modules/cjs/loader.js:1158:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1178:10)```Closing this specific one then.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3999"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3999"">No</a>=====', 'To fix the issue (worked for me on Ubuntu 18)```sudo apt-get updatesudo apt-get install build-essentialnpm i node-pre-gyp -g(if your using tfjs-node-gpu)npm rebuild @tensorflow/tfjs-node-gpu --build-from-source(or if your using tfjs-node instead)npm rebuild @tensorflow/tfjs-node --build-from-source=====']",0
https://github.com/tensorflow/tfjs/issues/5655,Unhandled promise rejection: Error: tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported,1,closed,2021-09-23T14:09:17Z,2021-09-23T16:44:53Z,"I have a created react native expo managed app. And tried to use mobile net. ```import * as mobilenet from '@tensorflow-models/mobilenet',import * as tf from '@tensorflow/tfjs',import { decodeJpeg } from '@tensorflow/tfjs-react-native',import * as FileSystem from 'expo-file-system',import * as ImagePicker from 'expo-image-picker',import React, { useEffect, useState } from 'react',import { Button, Image, StyleSheet, View } from 'react-native',const BACKEND_TO_USE = 'rn-webgl',export default function App() {  const [prediction, setPrediction] = useState<[]>(),  const [image, setImage] = useState<string>(''),  async function init() {    console.log('Loading mobilenet...'),    const model = await mobilenet.load(),    const fileUri = image,    const imgB64 = await FileSystem.readAsStringAsync(fileUri, {      encoding: FileSystem.EncodingType.Base64,    }),    const imgBuffer = tf.util.encodeString(imgB64, 'base64').buffer,    const newData = new Uint8Array(imgBuffer),    const imageTensor = decodeJpeg(newData), // transforms byte array into 3d tensor    const prediction = await model.classify(imageTensor),    console.log(prediction),    // Use prediction in app.    setPrediction(prediction),  }  const pickImage = async () => {    let result = await ImagePicker.launchImageLibraryAsync({      mediaTypes: ImagePicker.MediaTypeOptions.All,      quality: 1,    }),    console.log(result),    if (!result.cancelled) {      setImage(result.uri),    }  },  useEffect(() => {    async function init() {      await tf.ready(),    }    init(),  }, []),  return (    <View style={styles.container}>      <Button title=""Pick image"" onPress={pickImage} />      <Button title=""RUn"" onPress={init} />    </View>  ),}const styles = StyleSheet.create({  container: {    flex: 1,    backgroundColor: '#fff',    alignItems: 'center',    justifyContent: 'center',  },}),```**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: android emulator- TensorFlow.js installed from (npm or script link): yarn- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**`mobilenet.load()` is giving error [Unhandled promise rejection: Error: tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported.]at node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:5834:34 in __generator$argument_1at node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:84:17 in stepat node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:58:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:54:11 in __awaiterat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3700:31 in __generator$argument_1at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:129:21 in stepat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:71:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:57:11 in __awaiterat node_modules\@tensorflow-models\mobilenet\dist\index.js:177:45 in __generator$argument_1at node_modules\@tensorflow-models\mobilenet\dist\index.js:48:17 in stepat node_modules\@tensorflow-models\mobilenet\dist\index.js:20:47 in fulfilledat node_modules\react-native\node_modules\promise\setimmediate\core.js:37:13 in tryCallOneat node_modules\react-native\node_modules\promise\setimmediate\core.js:123:24 in setImmediate$argument_0    at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:130:14 in _callTimerat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:181:14 in _callImmediatesPassat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:441:30 in callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:387:6 in __callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:135:6 in __guard$argument_0at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:364:10 in __guardat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:134:4 in flushedQueue**Describe the expected behavior**mobile net should load **Standalone code to reproduce the issue**My dependency list ```""@react-native-async-storage/async-storage"": ""~1.15.0"",    ""@tensorflow-models/mobilenet"": ""^2.1.0"",    ""@tensorflow/tfjs"": ""^3.9.0"",    ""@tensorflow/tfjs-react-native"": ""^0.7.0"",    ""expo"": ""~42.0.1"",    ""expo-camera"": ""~11.2.2"",    ""expo-file-system"": ""~11.1.3"",    ""expo-gl"": ""~10.4.2"",    ""expo-gl-cpp"": ""~10.4.1"",    ""expo-image-picker"": ""~10.2.2"",    ""expo-status-bar"": ""~1.0.4"",    ""jpeg-js"": ""^0.4.3"",    ""react"": ""16.13.1"",    ""react-dom"": ""16.13.1"",    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-42.0.0.tar.gz"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-web"": ""~0.13.12""```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5655"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5655"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/732,Export tfjs symbols in tfjs-node and tfjs-node-gpu package.,7,closed,2018-09-25T16:58:08Z,2019-02-27T21:41:39Z,Should be as easy as: export * from '@tensorflow/tfjs' - let's consider doing this.,"[""+1 That'll simplify users' import code from two lines to one line and reduce the chance of mistakes. Let's give it a try (and test it out thoroughly, needless to say). ====="", '+1. This makes sense now that tfjs is a regular dependency of tfjs-node (upvoted by emoji :))=====', 'Related thought: when we make this change, we should probably switch the versioning scheme of tfjs-node so that it is in sync with tfjs. For example, minor version changes in tfjs should be reflected in the minor version of tfjs-node.=====', ""That's a good point, though sometimes we are fixing bugs in tfjs-node that are unreleated to tfjs, and we want to have the ability to release that bug fix independently of tfjs. EDIT: It's ok for tfjs-node sometimes to get ahead of tfjs due to bug fixes, but in general, releasing a new tfjs package should be followed by releasing a new tfjs-node package so we are all in sync.General point about exposing API: Exposing the API would also reduce problems of [this nature](https://github.com/tensorflow/tfjs/issues/410#issuecomment-425902276) since people won't explicitly import `@tensorflow/tfjs`, thus they are less likely to run `npm install @tensorflow/tfjs` which is what results in having 2 separate versions of tfjs that are not compatible.@caisq also has a great suggestion that after this issue is done, we should communicate this change more effectively via twitter, READMEs, mailing list etc.====="", 'This is done, right?=====', 'I think so.=====', 'This is done.=====']",0
https://github.com/tensorflow/tfjs/issues/5546,tf.fused.conv2d doesn't support 'NCHW' data format,1,open,2021-08-28T02:03:40Z,2021-08-28T12:37:06Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.8.0- TensorFlow.js version (use command below): 3.8.0- Browser version:  Google Chrome  92.0.4515.159- Tensorflow.js Converter Version: N/A**Describe the current behavior**This issue could be reproduced by following code snippet```jslet x = tf.ones([1, 2, 5, 5]),  // `NCHW`let filter = = tf.ones([3, 3, 2, 3]), // `HWIO`let y = tf.fused.conv2d({x, filter, strides: 1, pad: 'same', dataFormat: 'NCHW'}),```The error will be thrown:```Uncaught Error: Error in conv2d: depth of input (5) must match input depth for filter 2.```**Describe the expected behavior**No error is thrown and the result is expected to be same as tf.conv2d.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.See above code snippet.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.See error message above.cc @pyu10055 . Thanks.","[""Just my $0.02, almost none of TFJS ops support any other format than *NHWC* (which is defacto a standard for TF in general), not just `Conv2D`. And it's easy (and fast) to transpose tensor before using it. Even in TF itself (using Python bindings), very few ops support anything other than *NHWC* (although more than in TFJS).=====""]",1
https://github.com/tensorflow/tfjs/issues/5167,undefined is not an object (evaluating 'env().platform.fetch') for poseNet in React Native,6,closed,2021-06-03T19:02:26Z,2021-06-09T04:11:53Z,**System information**- OS : Windows 10- Platform : Android - TensorFlow.js installed from npm - TensorFlow.js version:3.6.0- @tensorflow-models/tfjs-react-native version:0.5.0- @tensorflow-models/posenet  version: 2.2.2 **Describe the problem**I always get this response when I tried to `load()` the posenet from  `@tensorflow-models/posenet`. I am using React Native CLI not Expo.I installed and configured the packages as per the document.Is TensorFlow supported for React Native CLI or just Expo projects?,"['Yes React Native CLI is supported https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native#step-1-create-your-react-native-app , you need to install all the expo dependencies even though you are not using the expo. Please refer a related issue  here https://github.com/tensorflow/tfjs/issues/4475=====', 'Thanks for your response @rthadur =====', 'Can we have the . tflite file for poseNet bundled with the app ? If yes how do I get it. I can only download the .tflite file for it.  And in the docs they specify some path for .json and .bin  How would I generate .json and .bin from .tflite for pose estimation?=====', 'Sorry,I did not quite get it , are you trying to use tflite models in tfjs ? if yes please refer here https://github.com/tensorflow/tfjs/blob/master/tfjs-tflite/README.mdcc @jinjingforever =====', 'Thanks again @rthadur =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5167"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5167"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4623,Performance issue with tf.signal.stft,8,closed,2021-01-31T18:37:47Z,2021-06-29T01:52:16Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm i @tensorflow/tf-node- TensorFlow.js version (use command below): 2.8.5, but master branch is affected as well- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**The tf.signal.stft function takes a 1D tensor, and calculates the FFT over a sliding window. The expected result is a 2D Tensor where all the strides contain the FFT according to the input parameters. I believe there's a performance bottleneck in the stft.ts implementation, in the stft_ function: the input (and processed) tensor is sliced into 1D tensors, and rfft is called in a loop. The output is aggregated into an output buffer, which is concatenated and then returned. **Describe the expected behavior**The called tf.spectra.rfft function is batch-aware. There's no reason to slice the processed tensor in stft_, you can pass windowedSignal to the rfft function, still get the same result. The benefit is, that there's no extra memory and processing required to slice/concat the results. Also, in rfft (where fft is called, which is a kernel function) gets a boost (may the backend be in node-cpu / -gpu / webgl / etc) from the calls working on larger set of data. My measurements show at least 3-fold performance increase (see example) using tf-node-cpu.Please consider reviewing the sfft_ function.**Standalone code to reproduce the issue**https://gist.github.com/harangp/b4c21a6c1ce9e2c8b0c1da804dde784d**Other info / logs**Output on my machine (i5 7th gen running at 3Ghz):```Standard aproach ----------------2021-01-31T18:17:31.568Z 'ellapsed time:' 1102021-01-31T18:17:31.650Z 'ellapsed time:' 802021-01-31T18:17:31.790Z 'ellapsed time:' 1392021-01-31T18:17:31.847Z 'ellapsed time:' 562021-01-31T18:17:31.903Z 'ellapsed time:' 552021-01-31T18:17:31.961Z 'ellapsed time:' 572021-01-31T18:17:32.010Z 'ellapsed time:' 482021-01-31T18:17:32.059Z 'ellapsed time:' 482021-01-31T18:17:32.113Z 'ellapsed time:' 532021-01-31T18:17:32.161Z 'ellapsed time:' 472021-01-31T18:17:32.210Z 'ellapsed time:' 482021-01-31T18:17:32.262Z 'ellapsed time:' 512021-01-31T18:17:32.314Z 'ellapsed time:' 512021-01-31T18:17:32.362Z 'ellapsed time:' 472021-01-31T18:17:32.415Z 'ellapsed time:' 522021-01-31T18:17:32.465Z 'ellapsed time:' 492021-01-31T18:17:32.510Z 'ellapsed time:' 442021-01-31T18:17:32.552Z 'ellapsed time:' 422021-01-31T18:17:32.607Z 'ellapsed time:' 542021-01-31T18:17:32.655Z 'ellapsed time:' 48Sped up approach ----------------2021-01-31T18:17:32.667Z 'ellapsed time:' 112021-01-31T18:17:32.685Z 'ellapsed time:' 92021-01-31T18:17:32.696Z 'ellapsed time:' 102021-01-31T18:17:32.717Z 'ellapsed time:' 92021-01-31T18:17:32.728Z 'ellapsed time:' 102021-01-31T18:17:32.746Z 'ellapsed time:' 122021-01-31T18:17:32.768Z 'ellapsed time:' 92021-01-31T18:17:32.779Z 'ellapsed time:' 112021-01-31T18:17:32.799Z 'ellapsed time:' 112021-01-31T18:17:32.808Z 'ellapsed time:' 92021-01-31T18:17:32.828Z 'ellapsed time:' 112021-01-31T18:17:32.848Z 'ellapsed time:' 122021-01-31T18:17:32.871Z 'ellapsed time:' 182021-01-31T18:17:32.884Z 'ellapsed time:' 122021-01-31T18:17:32.903Z 'ellapsed time:' 92021-01-31T18:17:32.915Z 'ellapsed time:' 122021-01-31T18:17:32.938Z 'ellapsed time:' 112021-01-31T18:17:32.964Z 'ellapsed time:' 112021-01-31T18:17:32.984Z 'ellapsed time:' 122021-01-31T18:17:32.993Z 'ellapsed time:' 9Does it return the same results?Tensor    true```","[""Hi, @harangp!I have taken the liberty of creating a test of this issue for web client. Also I've added 3 new test tensors (zeros, randomNormal and randomUniform) to verify that all is working at it should.In my case I've noticed a ~10x improvement on webgl backend. I was unable to test it with the wasm backend as it doesn't currently support complex tensors.My standalone code to reproduce the issue:https://gist.github.com/JPery/70df24354427475e72d2adcf33e94928Output log on my machine (i7 4th gen running at 2,3Ghz with Nvidia 750M) using Google Chrome (v88.0.4324.96):```index.html:108 Backend: webglindex.html:67 ****************  testTensor: ones ****************index.html:69 ---------------- Warming up ----------------DevTools failed to load SourceMap: Could not load content for https://cdn.jsdelivr.net/npm/@tensorflow/tf.min.js.map: HTTP error: status code 404, net::ERR_HTTP_RESPONSE_CODE_FAILUREindex.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 76index.html:53 Elapsed time: 66index.html:53 Elapsed time: 48index.html:53 Elapsed time: 42index.html:53 Elapsed time: 39index.html:53 Elapsed time: 42index.html:53 Elapsed time: 53index.html:53 Elapsed time: 52index.html:53 Elapsed time: 57index.html:53 Elapsed time: 55index.html:53 Elapsed time: 54index.html:53 Elapsed time: 46index.html:53 Elapsed time: 51index.html:53 Elapsed time: 47index.html:53 Elapsed time: 49index.html:53 Elapsed time: 64index.html:53 Elapsed time: 46index.html:53 Elapsed time: 44index.html:53 Elapsed time: 58index.html:53 Elapsed time: 51index.html:78 AVG tf.signal.stft 52index.html:79 STDev tf.signal.stft 8.79772697916911index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 22index.html:53 Elapsed time: 3index.html:53 Elapsed time: 2index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 13index.html:53 Elapsed time: 9index.html:53 Elapsed time: 62index.html:53 Elapsed time: 132index.html:53 Elapsed time: 7index.html:53 Elapsed time: 43index.html:53 Elapsed time: 2index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 2index.html:90 AVG custom_stft 5.2index.html:91 STDev custom_stft 3.8026306683663087index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: zeros ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 67index.html:53 Elapsed time: 68index.html:53 Elapsed time: 61index.html:53 Elapsed time: 58index.html:53 Elapsed time: 51index.html:53 Elapsed time: 64index.html:53 Elapsed time: 62index.html:53 Elapsed time: 49index.html:53 Elapsed time: 41index.html:53 Elapsed time: 68index.html:53 Elapsed time: 65index.html:53 Elapsed time: 67index.html:53 Elapsed time: 88index.html:53 Elapsed time: 78index.html:53 Elapsed time: 68index.html:53 Elapsed time: 65index.html:53 Elapsed time: 58index.html:53 Elapsed time: 78index.html:53 Elapsed time: 59index.html:53 Elapsed time: 68index.html:78 AVG tf.signal.stft 64.15index.html:79 STDev tf.signal.stft 10.209187039132939index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 19index.html:53 Elapsed time: 9index.html:53 Elapsed time: 5index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 9index.html:53 Elapsed time: 6index.html:53 Elapsed time: 10index.html:53 Elapsed time: 9index.html:53 Elapsed time: 5index.html:53 Elapsed time: 6index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 7index.html:53 Elapsed time: 53index.html:53 Elapsed time: 4index.html:90 AVG custom_stft 6.65index.html:91 STDev custom_stft 3.439113257803529index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: randomNormal ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 49index.html:53 Elapsed time: 57index.html:53 Elapsed time: 45index.html:53 Elapsed time: 55index.html:53 Elapsed time: 68index.html:53 Elapsed time: 45index.html:53 Elapsed time: 53index.html:53 Elapsed time: 39index.html:53 Elapsed time: 42index.html:53 Elapsed time: 43index.html:53 Elapsed time: 52index.html:53 Elapsed time: 38index.html:53 Elapsed time: 43index.html:53 Elapsed time: 44index.html:53 Elapsed time: 482index.html:53 Elapsed time: 44index.html:53 Elapsed time: 95index.html:53 Elapsed time: 51index.html:53 Elapsed time: 59index.html:78 AVG tf.signal.stft 50.7index.html:79 STDev tf.signal.stft 12.446284586172695index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 8index.html:53 Elapsed time: 4index.html:53 Elapsed time: 10index.html:53 Elapsed time: 14index.html:53 Elapsed time: 6index.html:53 Elapsed time: 8index.html:53 Elapsed time: 5index.html:53 Elapsed time: 9index.html:53 Elapsed time: 6index.html:53 Elapsed time: 7index.html:53 Elapsed time: 6index.html:53 Elapsed time: 42index.html:53 Elapsed time: 5index.html:53 Elapsed time: 6index.html:53 Elapsed time: 4index.html:53 Elapsed time: 5index.html:53 Elapsed time: 15index.html:90 AVG custom_stft 6.95index.html:91 STDev custom_stft 2.991237202229205index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: randomUniform ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 34index.html:53 Elapsed time: 48index.html:53 Elapsed time: 49index.html:53 Elapsed time: 63index.html:53 Elapsed time: 91index.html:53 Elapsed time: 81index.html:53 Elapsed time: 70index.html:53 Elapsed time: 57index.html:53 Elapsed time: 75index.html:53 Elapsed time: 88index.html:53 Elapsed time: 106index.html:53 Elapsed time: 3702index.html:53 Elapsed time: 50index.html:53 Elapsed time: 59index.html:53 Elapsed time: 72index.html:53 Elapsed time: 86index.html:53 Elapsed time: 69index.html:53 Elapsed time: 56index.html:53 Elapsed time: 57index.html:78 AVG tf.signal.stft 81.55index.html:79 STDev tf.signal.stft 68.38309367087744index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 8index.html:53 Elapsed time: 53index.html:53 Elapsed time: 4index.html:53 Elapsed time: 32index.html:53 Elapsed time: 6index.html:53 Elapsed time: 42index.html:53 Elapsed time: 3index.html:53 Elapsed time: 8index.html:53 Elapsed time: 11index.html:53 Elapsed time: 73index.html:53 Elapsed time: 5index.html:90 AVG custom_stft 5.15index.html:91 STDev custom_stft 1.9817921182606413index.html:93 Does it return the same results?index.html:102 true```Thanks so much for reporting this issue and I hope my work helps.Best regards!====="", ""Hi @JPery , thanks for taking the time and confirming my find. Also, I like the code you've brought to the web. I've run your code on my machine in a Chrome (v: 87.0.4280.141) as well for reference, here are the results (see below). The interesting thing is my GPU is an integrated Intel HD Graphics 620, but the results are roughly the same. So I would say WebGL definitely packs a punch here.```stftLayerExperiment_2.html:108 Backend: webglstftLayerExperiment_2.html:67 ****************  testTensor: ones ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------DevTools failed to load SourceMap: Could not load content for https://cdn.jsdelivr.net/npm/@tensorflow/tf.min.js.map: HTTP error: status code 404, net::ERR_HTTP_RESPONSE_CODE_FAILUREstftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 122stftLayerExperiment_2.html:53 Elapsed time: 101stftLayerExperiment_2.html:53 Elapsed time: 75stftLayerExperiment_2.html:53 Elapsed time: 91stftLayerExperiment_2.html:53 Elapsed time: 112stftLayerExperiment_2.html:53 Elapsed time: 83stftLayerExperiment_2.html:53 Elapsed time: 60stftLayerExperiment_2.html:53 Elapsed time: 59stftLayerExperiment_2.html:53 Elapsed time: 38stftLayerExperiment_2.html:53 Elapsed time: 45stftLayerExperiment_2.html:53 Elapsed time: 47stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 43stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 30stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:78 AVG tf.signal.stft 57.3stftLayerExperiment_2.html:79 STDev tf.signal.stft 28.81683535713108stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 11stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 73stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 52stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:53 Elapsed time: 6stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:90 AVG custom_stft 5.4stftLayerExperiment_2.html:91 STDev custom_stft 2.267156809750927stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: zeros ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 50stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 37stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 51stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:53 Elapsed time: 352stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:53 Elapsed time: 29stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 20stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 95stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 24stftLayerExperiment_2.html:53 Elapsed time: 240stftLayerExperiment_2.html:78 AVG tf.signal.stft 46.8stftLayerExperiment_2.html:79 STDev tf.signal.stft 47.226687370595876stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 10stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 6stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 3.65stftLayerExperiment_2.html:91 STDev custom_stft 1.796524422322168stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: randomNormal ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 51stftLayerExperiment_2.html:53 Elapsed time: 442stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 282stftLayerExperiment_2.html:53 Elapsed time: 19stftLayerExperiment_2.html:53 Elapsed time: 16stftLayerExperiment_2.html:53 Elapsed time: 20stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 26stftLayerExperiment_2.html:53 Elapsed time: 44stftLayerExperiment_2.html:53 Elapsed time: 206stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:78 AVG tf.signal.stft 38.2stftLayerExperiment_2.html:79 STDev tf.signal.stft 39.60378769764327stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 55stftLayerExperiment_2.html:53 Elapsed time: 80stftLayerExperiment_2.html:53 Elapsed time: 15stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 13stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 10.35stftLayerExperiment_2.html:91 STDev custom_stft 19.71363741170056stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: randomUniform ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 27stftLayerExperiment_2.html:53 Elapsed time: 30stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 29stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 41stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 19stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 82stftLayerExperiment_2.html:53 Elapsed time: 853stftLayerExperiment_2.html:53 Elapsed time: 40stftLayerExperiment_2.html:53 Elapsed time: 27stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:78 AVG tf.signal.stft 74.45stftLayerExperiment_2.html:79 STDev tf.signal.stft 179.1715588479377stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------2stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 52stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 8stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 3.1stftLayerExperiment_2.html:91 STDev custom_stft 1.5459624833740309stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 true```====="", '@harangp @JPery Awesome work for great perf improvement, any of you want to contribute the change to the library? thanks!=====', '> @harangp @JPery Awesome work for great perf improvement, any of you want to contribute the change to the library? thanks!Yes, happy to (will be first time, but one must start somewhere).=====', 'https://github.com/tensorflow/tfjs/pull/4790Every check is green. Is there anything else I should do with this?=====', '@pyu10055 gentle ping to review the PR=====', 'The related PR has been merged , so closing the issue. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4623"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4623"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4293,AutoML Vision Tensorflowjs -  Cannot read property 'reduce' of undefined,4,closed,2020-11-22T01:22:38Z,2020-11-23T22:42:58Z,"When using the latest model created by automl for tensorflowjs for image classification or object detection, getting the following error when running:`const model = await tf.automl.loadImageClassification('/model/model.json'),`Getting the following error:```Uncaught (in promise) TypeError: Cannot read property 'reduce' of undefined    at e.t.transformGraph (tfjs:17)    at e.t.loadSync (tfjs:17)    at e.<anonymous> (tfjs:17)    at u (tfjs:17)    at Generator._invoke (tfjs:17)    at Generator.forEach.e.<computed> [as next] (tfjs:17)    at Um (tfjs:17)    at o (tfjs:17)```Using```""@tensorflow/tfjs"": ""^2.7.0"",""@tensorflow/tfjs-automl"": ""^1.0.0""```Full code```<script src=""https://unpkg.com/@tensorflow/tfjs""></script><script src=""https://unpkg.com/@tensorflow/tfjs-automl""></script><img id=""daisy"" src=""test2.jpg""><script>    async function run() {        const model = await tf.automl.loadImageClassification('/model/model.json'),        const image = document.getElementById('daisy'),        const predictions = await model.classify(image),        console.log(predictions),        // Show the resulting object on the page.        const pre = document.createElement('pre'),        pre.textContent = JSON.stringify(predictions, null, 2),        document.body.append(pre),    }    run(),</script>```","['Lets close this in and track it at single place [here](https://github.com/tensorflow/tfjs/issues/4293)=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">No</a>=====', 'sorry wrong issue reference earlier , same issue exists [here](https://github.com/tensorflow/tfjs/issues/4105) =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1844,Possible reduction in accuracy with the PoseNet 2.0 Mobilenet vs PoseNet 1.0 Mobilenet models,15,closed,2019-08-16T12:35:04Z,2020-08-12T15:08:45Z,"@joeyklee who is working on upgrading ml5 to use posenet 2.0 has noticed a potential reduction in accuracy when upgrading to 2.0 and using the same MobileNet model and output stride, even with input resolution of 513 (the highest).See: https://github.com/ml5js/ml5-library/issues/510It could have to do with #1681 - since in 2.0 the image is resized to be a square by adding padding, resolution is lost when it's scaled down to match the resolution and black pixels are added.","['According to @joyeklee, resizing the input image to a square ahead of time fixed the accuracy issues:https://github.com/ml5js/ml5-library/issues/510So we should consider in posenet instead of resizing to a square, resizing using input resolution to the maximum largest dimension of the image.=====', 'Thanks @oveddan for noting this! Happy to provide feedback or test anything :) =====', 'https://github.com/tensorflow/tfjs/issues/1830 , there is a related issue here =====', 'Should be fixed by https://github.com/tensorflow/tfjs-models/pull/283 , once submitted.=====', '@bileschi it will certainly be helped by that PR, but it would also be helped by resizing as close to match the original aspect ratio instead of adding pixels to match a square (and causing lost resolution along the shorter dimension).=====', 'PR has been submitted , closing this =====', ""Hi @rthadur the PR certainly helps with accuracy, but doesn't address the cause of the accuracy issue that this PR was opened for.  See https://github.com/ml5js/ml5-library/issues/510In the debugging of accuracy issues, @joeyklee discovered that resizing to a square fixed the accuracy issues, if PoseNet resizes to match the original image dimensions (as it did in v1), or can accept a width and height instead of a single size parameter, the image won't be forced to be resized to a square, which it does currently, causing resolution to be lost on the longer dimension.  ====="", '@oveddan i see #296 merged , can we close this or will there be follow up changes.=====', 'You can close itOn Mon, Oct 14, 2019 at 9:32 PM Rajeshwar Reddy T <notifications@github.com>wrote:> @oveddan <https://github.com/oveddan> i see #296> <https://github.com/tensorflow/tfjs/issues/296> merged , can we close> this or will there be follow up changes.>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/1844?email_source=notifications&email_token=AAGZW22P2ZR7BJ25Y4OU4KDQOUMTRA5CNFSM4IMHCEOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBHDK7Q#issuecomment-541996414>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAGZW23RP4JZ353HBI6PXP3QOUMTRANCNFSM4IMHCEOA>> .>-- Sent from Gmail Mobile=====', 'thank you=====', '@nsthorat can you please publish posenet to npm so that the latest version is there?Thanks!=====', 'Just a small note that also body-pix 2.0 has not yet been published to npm yet either - https://github.com/tensorflow/tfjs-models/blob/master/body-pix/package.json. Do you know if this is intentional? Thanks so much! =====', 'Yep intentional, still working on some last minute API and bug fixes!=====', ""Thanks for the update @nsthorat! I'll keep my eyes out for news. ====="", 'Tengo el mismo problema al actualizar a la versión 2.0, saben si se ha podido solucionar?=====']",0
https://github.com/tensorflow/tfjs/issues/513,model.save(url) isn't saving weights file with dowloads://,12,closed,2018-07-13T19:06:55Z,2018-10-26T19:20:59Z,"Let me preface this by saying I am a beginner in the TensorFlow community, although I am familiar with the concepts. This is my first project with TensorFlow JS.#### TensorFlow.js version0.12.0#### Browser versionChrome Version 67.0.3396.99#### Describe the problem or feature requestI am running this webpage with [http-server](https://www.npmjs.com/package/http-server).I am using `model.save()` to save the model to local storage, but it only saves a JSON file and never saves the weights.bin file. Now when I try and load the model, it can't find the weights file becuase it was never created.#### Code to reproduce the bug / link to feature request```javascriptasync function train() {  const options = {    epochs: epochs,    validationSplit: 0.1,    shuffle: true,    callbacks: {      onTrainBegin: () => {        console.log('training start')        lossVals = []      },      onTrainEnd: () => {        console.log('training complete')        saveModel().then((results) => {          console.log(results)        })      },      onEpochEnd: async (num, logs) => {        lossVals.push({          epoch: num,          loss: logs.val_loss        })        drawLoss(lossVals)        if (!lowest_loss || logs.val_loss < lowest_loss.loss) {          lowest_loss = {            epoch: num,            loss: logs.val_loss          }        }        console.log('Epoch: ' + num)        console.log('Loss: ' + logs.val_loss)        $('#loss').text('latest loss: ' + logs.val_loss + '   |  epoch ' + lowest_loss.epoch + ' had the lowest loss of ' + lowest_loss.loss)        await tf.nextFrame()      }    }  }  return model.fit(xs, ys, options)}async function saveModel() {  return await model.save(`downloads://${hidden_units}_hidden_units_${epochs}_epochs_${lr*100}_lr`)}```","['Hi @BradHacker , as a diagnostic step, can you go to this link below and run ""Example 3?"" Let me know whether it downloads one or two files.https://js.tensorflow.org/api/0.12.0/#tf.Model.save=====', 'I use this code: ```html<!DOCTYPE html><html><head>  <meta charset=""utf-8"" />  <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">  <title>TensorFlow Test</title>  <meta name=""viewport"" content=""width=device-width, initial-scale=1"">  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0""> </script></head><body>  <script>    const model = tf.sequential(      {layers: [tf.layers.dense({units: 1, inputShape: [3]})]}),    const saveResults = model.save(\'downloads://my-model-1\'),  </script></body></html>```It still only downloads one file called `my-model-1`=====', ""@BradHacker You didn't answer my question about how many files are downloaded when you run Example 3 at https://js.tensorflow.org/api/0.12.0/#tf.Model.saveSome browser settings will prevent >1 file to be downloaded at the same time. Can you check that your browser is not set like that? For more details see:https://ccm.net/faq/32938-google-chrome-allow-websites-to-perform-simultaneous-downloads====="", 'It only downloads **one file**. My chrome settings should allow multiple files to be downloaded. I attached a screenshot of my automatic download settings in chrome.![image](https://user-images.githubusercontent.com/22984089/42719012-840e2e78-86dd-11e8-9473-2f4edbbdcc14.png)=====', ""So it seems this might be an issue with chrome because there is a number on the chrome icon signifying active downloads, but when I go to the downloads page (Mac: Command + Shift + J), there are no active downloads and the number doesn't go away when I clear my downloads list. So the download for the weights file seems to be initiated, but it never finishes.Using macOS High Sierra 10.13.5====="", 'That does sound like there may be something screwy with your Chrome.  Can you try reinstalling it?=====', 'Oh, also: You mentioned ""one file called my-model-1"".  Did you mean my-model-1.json?=====', ""Yes I meant to say my-model-1.json. Reinstalling chrome didn't fix the issue. ====="", 'Hm, that is puzzling.  Is there any chance that you have some other process removing files from your downloads directory?  E.g. a virus checker might be spuriously quarantining my-model-1.weights.bin, or a tool like Hazel might be automatically sorting your downloads into folders, etc.Does the downloads list indicate that the weights file was downloaded, or does it just not show up there at all?  E.g., after running Example 3 that @caisq pointed at, mine looks like this:![screen shot 2018-07-25 at 9 42 16 am](https://user-images.githubusercontent.com/1347381/43204527-210c2f44-8fef-11e8-9cf4-3f4dbd3e9c23.png)=====', ""I tried it twice. The bottom download being with running on http-server and the second was just navigating to the `file:///` path of the html file. When navigating to the file I was asked if I wanted to allow multiple files to download and I selected 'Allow', yet it yielded no new results. My chrome icon now says I have 2 new unfinished downloads to make a total of 5 since I've started chrome.![image](https://user-images.githubusercontent.com/22984089/43213266-35cee0b0-9004-11e8-8f26-9266aa7fb4f0.png)![image](https://user-images.githubusercontent.com/22984089/43213536-d3cc6468-9004-11e8-8bb2-dab5b95729a0.png)====="", ""I had the same problem, but managed to resolve it with the following steps:- Go to chrome://settings >> Content Settings >> Automatic downloads, or just go straight to chrome://settings/content/automaticDownloads - Look under 'Allow' for the address of the local file where you are running the tensorflow.js scripts- Remove the address- Train your tensorflow model again- There will be a prompt for you to download the json file- Followed by a pop-up asking for permission to download multiple files- Click 'Allow'- There will now be  prompt for you to download the bin file with the weights.====="", ""Great, closing this out since it's resolved :)=====""]",0
https://github.com/tensorflow/tfjs/issues/496,Website: survey div renders over api version selector,0,closed,2018-07-06T18:22:41Z,2018-07-10T18:52:04Z,#### TensorFlow.js versionsite#### Browser version68.0.3440.42 (Official Build) beta (64-bit)#### Describe the problem or feature request![image](https://user-images.githubusercontent.com/547150/42394395-fc359082-8127-11e8-8887-e8f52ef44b5f.png)#### Code to reproduce the bug / link to feature requesthttps://js.tensorflow.org/api/0.11.6/,[],0
https://github.com/tensorflow/tfjs/issues/959,Allow users to pass a list of TypedArray in tf.tensor(),1,closed,2018-12-03T18:04:35Z,2018-12-05T14:56:32Z,"Allow users to a pass a list of TypedArray in tf.tensor():```jsa = new Float32Array([1, 2]),b = new Float32Array([3, 4]),c = tf.tensor([a, b]), // 2x2 matrix```",['cc @shiffman====='],0
https://github.com/tensorflow/tfjs/issues/1375,Model accessibility issues in China,17,closed,2019-03-13T18:44:33Z,2020-06-05T18:35:36Z,"Currently for pre-tained models, the weights and model graph is not shipped with NPM and needs to fetched at runtime from GCS. However GCS is not available in China or very slow which hurts user experiences. Do we have a plan to make pre-trained models available in China? I would suggest to embed pre-trained weights in a fat NPM (probably new NPMs to avoid breaking existing use cases). But we could definitely find a place to host these content in China as well.","['@pyu10055 @nsthorat @dsmilkov @caisq @kangyizhang Any thoughts?=====', '@huan=====', ""Yes, that's a problem and the developers will be very sad if it could not be solved.For example, when we want to use the Google Chrome Puppeteer in China, what we expect the developer to do is a `npm install puppeteer` but it will fail eventually because after installing all the JS files from NPM server then it needs run a post-install script and wants to download the binary from the GCS storage server, which is not accessible in China because of the GFW.There is a workaround for the puppeteer at [here](https://github.com/Chatie/wechaty-puppet-puppeteer#note-for-developers-in-china). I believe we should solve this kind of problem for the developers in China, for spreading the tfjs faster. (at least we should provide a workaround for installing tfjs-models)====="", ""+1 for solving this for developers in China. Most of our tfjs-models actually download weights from GCS at runtime. So I'm not sure if the puppeteer workaround @huan pointed out would work here.====="", 'I think our company can provide the models storage and CDN services in China . may accelerate this.=====', '@chenqing That would be great if the ~~npm.taobao.com~~`npm.taobao.org/mirrors` could provide the CDN service for our tfjs models.Do you know any stakeholders from `npm.taobao.com` who can make decisions on this issue? If you could introduce a key person for us, then we would be able to solve this issue easier.=====', ""@huan I think what's your said is https://npm.taobao.org/mirrors . I have talked with the maintainer. He promised to support it. The next question is what and how to synchronize.====="", ""That'll be awesome. Thanks Chenqing.The first one coming out of my mind is probably rsync if that works on GCS?====="", '@chenglu=====', 'Thanks @chenqing and @huan for the suggestion, as @caisq mentioned, our model npms do not have a post install script to download model files from gcs, and people can use jsdelivr directly on their pages, which will download the model files on the fly. Feels like the better way is to mirror GCS bucket, but not sure if that is possible.=====', 'thanks @wangtz,I was wondering if we could try the feature of ""mirror storage""--镜像存储 that alioss or qiniu provided for unblocking the tfjs-models bucket?e.g.http://tfjs.oss-cn-hongkong.aliyuncs.com/tfjs-models/weights/posenet/mobilenet_v1_075/MobilenetV1_Conv2d_8_pointwise_biases=====', '> Thanks @chenqing and @huan for the suggestion, as @caisq mentioned, our model npms do not have a post install script to download model files from gcs, and people can use jsdelivr directly on their pages, which will download the model files on the fly. Feels like the better way is to mirror GCS bucket, but not sure if that is possible.I think so .=====', '@wangtz @huan @caisq  https://cnpmjs.org/mirrors/tfjs-models/  has sync ok.  and will automatic synchronization when update.I think may mention this in some parts of the document? =====', '@chenqing awesome, good job! =====', ""I found when installing ttf-node npm will down libtensorflow from googleapis.com too. see https://github.com/tensorflow/tfjs-node/blob/master/scripts/install.js#L33 Unfortunately, this url can't be visited normally in China.upload libtensorflow to npm may be a solution.@caisq ====="", 'I came to ask where we can download those models to load them locally easier, but seeing this issue is very related I\'ll join here.As a workaround what I did is opening the network developer tab, and copying the body of all the requests (one JSON, then N ""shards""):<img width=""549"" alt=""Screen Shot 2019-09-24 at 0 25 12"" src=""https://user-images.githubusercontent.com/2801252/65439237-ce8c3580-de61-11e9-83bf-9e17502f4b37.png"">Then put them in your `public` folder as static file:<img width=""173"" alt=""shards"" src=""https://user-images.githubusercontent.com/2801252/65439496-3478bd00-de62-11e9-9c10-812665be8c72.png"">Then you can load them using the option `{ modelUrl: \'/objects/model.json\' }`. Does anyone know a better way? Seems like an easy way of downloading these models would help OP as well. I\'m looking to improve this for a React Hook I made, [`use-tensorflow`](https://www.npmjs.com/package/use-tensorflow) and currently I\'m telling devs [this way of loading those manually](https://www.npmjs.com/package/use-tensorflow#loading-local-models) but it is painfully:```jsimport React, { useRef } from ""react"",import { useObjects } from ""use-tensorflow"",import { Container, Box } from \'./components\', export default () => {  const ref = useRef(null),  const objects = useObjects(ref, { modelUrl: ""/objects/model.json"" }),  return (    <Container>      <img ref={ref} src=""/living-room.jpg"" />      {objects.map(({ left, top, width, height, label, score }) => (        ...      ))}    </Container>  ),},```=====', 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/452,Missing documentation for model.save,0,closed,2018-06-19T18:01:45Z,2018-06-20T15:19:30Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.11.6#### Describe the problem or feature requestLooking at the documentation, I see model.save being referenced, but its not actually listed in the documentation. Am I misreading the documentation, or are there public functions missing from it? https://js.tensorflow.org/api/0.11.6/#io.removeModel",[],0
https://github.com/tensorflow/tfjs/issues/4339,Incorrect predictions for a known precise model (React Native),2,closed,2020-12-02T20:19:12Z,2020-12-02T22:20:12Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): React Native 0.63.3- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Galaxy S20+- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):- Tensorflow.js Converter Version: 2.7.0""@tensorflow/tfjs"": ""^2.7.0"",""@tensorflow/tfjs-react-native"": ""^0.5.0"",**Describe the current behavior**Graph model is predicting incorrectly, but consistently. If I take a picture of a black background, it outputs index 120 with ~.5 confidence. If I take a picture of anything not black, it outputs index 1034 with confidence of 1, and all other indexes are 0.**Describe the expected behavior**Correct predictions. I know this is possible because I uploaded images in the google cloud vision console and this same model was predicting accurately.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.Load model from assets on launch. I did change my metro config to allow loading of .bin assets.```useEffect(() => {  tfReady()    .then(async () => {      return loadGraphModel(        bundleResourceIO(          require('../../../../assets/model.json'),          require('../../../../assets/model_weights.bin')        )      )    })    .then(setModel)    .catch(e => console.error(e))}, [])```After base64 comes in from camera```const raw = Uint8Array.from(toByteArray(base64)) // from 'base64-js' npm packageconst imageTensor = decodeJpeg(raw)  .expandDims(0)  .resizeBilinear([224, 224], false)const output = model.predict(imageTensor)// @ts-ignoreconst data: number[] = output.dataSync()console.log(data.length)````data.length` is the correct number of classes. But it always contains incorrect data as I stated in the section above.I know this model works because after I trained it on Google Cloud AutoML, I was able to upload images (that the model had never seen) of all different classes and it was predicting correctly with high confidence.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4339"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4339"">No</a>=====', 'It must have been something to do with the way I was processing my image. I switched to using the `ImageClassificationModel` from @tensorflow/tfjs-automl and it started classifying the images properly.=====']",1
https://github.com/tensorflow/tfjs/issues/42,IPv6 support,1,closed,2018-03-31T18:41:02Z,2018-03-31T22:28:25Z,"Unlike [the Tensorflow website](https://www.tensorflow.org) itself, [the website for Tensorflow.js](https://js.tensorflow.org) is inaccessible over IPv6.",['Moved the issue to the tfjs-website: tensorflow/tfjs-website/issues/107====='],0
https://github.com/tensorflow/tfjs/issues/5854,Facemesh/Face-Landmarks-Detection memory comparison,1,open,2021-11-16T02:28:35Z,2021-11-21T20:17:24Z,"I’ve been digging into finding a non-leaky way to use Face Landmarks Detections / facemesh.I’ve tested several different approaches all with demo code (no modifications) on chrome on osx using the webgl backend. Here is all the info about my testing incase it helps someone else. In each case, I ran the programs for 10 minutes and polled the JS heap size and GPU Process size (chrome task manager) every minute. ### Face Landmarks Detections [Demo](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection/demo) including the library via script tags/ unpkgVersions: ```tfjs-core@3.1.0tfjs-converter@3.1.0tfjs-backend-webgl@3.1.0tfjs-backend-cpu@3.1.0face-landmarks-detection@0.0.3```Result: ![Screen Shot 2021-11-15 at 7 11 14 PM](https://user-images.githubusercontent.com/3160465/141878431-88fd2fa8-a0e8-4f14-bc1d-5632917d24b0.png)Heap sizes`20.1,27.8,35.3,45.1,53.5,59.4,96.5,73.7,82.0,88.9`Delta- 68.8 mbGPU Process sizes `345,442,551,673,814,943,1000,1200,1300,1400`Delta- 1055 mbConclusion - significant memory problem. Has crashed chrome and caused the system to run out of memory frequently. ### Face Landmarks Detections [Demo](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection/demo) using NPM/Yarn/build tools in repoVersions: ```    face-landmarks-detection"": ""link:../dist"",    ""@tensorflow/tfjs-backend-wasm"": ""^3.1.0"",    ""@tensorflow/tfjs-backend-webgl"": ""^3.1.0"",    ""@tensorflow/tfjs-backend-cpu"": ""^3.1.0"",    ""@tensorflow/tfjs-converter"": ""^3.1.0"",    ""@tensorflow/tfjs-core"": ""^3.1.0"",```Result: ![Screen Shot 2021-11-15 at 6 54 16 PM](https://user-images.githubusercontent.com/3160465/141879236-f0d2bfac-0122-44c3-af0d-d87f436f1560.png)Heap sizes`17.3,18.9,18.7, 19.4,19.6,19.2,19.7,19.8,19.9`Delta- 2.6 mbGPU Process sizes `314,317,318,318,318,319,320,320,320`Delta- 6mbConclusion - still increases, but best option. Is it simply including via NPM or is it also about all the dev tools (babel, rollup, rollup, esbuild, yarn, etc ) that are being used?### Face Mesh [demo](https://codepen.io/mediapipe/pen/KKgVaPJ) run on a local server with linked / non-npm dependenciesVersions```camera_utils@0.3control_utils@0.6drawing_utils@0.3face_mesh@0.4```Result:![Screen Shot 2021-11-15 at 4 15 50 PM](https://user-images.githubusercontent.com/3160465/141879662-9ce94fa5-63f7-46f0-a1c6-0c6e70255286.png)Heap size`31.4,50.3,76.1,88.7,104,121,143,168,168,199`Delta - 167.6 mbGPU Process sizes `240 - 286 `Delta - 46 mb[More info on this issue](https://github.com/google/mediapipe/issues/1937#issuecomment-969348179)- also likely relevant thread about the details of webgl and memory. | demo+links heap delta | demo+links GPU delta | demo+npm heap delta | demo+npm GPU delta | facemesh heap delta | facemesh GPU delta ||-----------------------|----------------------|---------------------|--------------------|---------------------|--------------------|| 68.8 mb               | 1055 mb              | 2.6 mb              | 6 mb               | 167.6 mb            | 46   mb            |To my non expert eyes - using npm + the dev dependencies seems to be the secret sauce. Which of these dev dependencies are making the difference if any? Why is that approach so much better? ","['Hi there, Checking in about this. @mattsoulanille Any thoughts on why the memory management is so different with and without the dev dependencies? =====']",1
https://github.com/tensorflow/tfjs/issues/828,Encode float program should be able to handle packed input textures,0,open,2018-10-24T19:23:12Z,2021-12-15T18:44:12Z,In `backend_webgl:getValuesFromTexture` we need to either unpack first and then encode floats or create a version of `EncodeFloatProgram` that handles packed inputs.,[],0
https://github.com/tensorflow/tfjs/issues/4766,Backend WASM does not implement kernel ResizeNearestNeighbor,1,open,2021-02-28T15:05:06Z,2021-06-03T16:15:38Z,"attempting to run nudenet model using `wasm` backend  converted from tf saved to tfjs graph using tensorflowjs_convert:```logengine.js:391 Uncaught (in promise) Error: Kernel 'ResizeNearestNeighbor' not registered for backend 'wasm'    at Engine2.runKernel (engine.js:391)    at resizeNearestNeighbor_ (resize_nearest_neighbor.js:60)    at Object.resizeNearestNeighbor__op [as resizeNearestNeighbor] (operation.js:44)    at executeOp$9 (image_executor.js:34)    at operation_executor.js:62    at engine.js:327    at Engine2.scopedRun (engine.js:337)    at Engine2.tidy (engine.js:326)    at tidy (globals.js:175)    at operation_executor.js:62```same model works fine using `weblgl` backend  as per #4386 it has been implemented, but it seems it was implemented for `cpu` backend only although title specifies `wasm`?environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10",['cc @lina128 @pyu10055 @mattsoulanille ====='],1
https://github.com/tensorflow/tfjs/issues/5245,Otsu Image Threshold Broken in tfjs-react-native,1,open,2021-06-22T20:48:49Z,2021-06-22T21:52:32Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android 9.0 Samsung Galaxy S9 Plus- TensorFlow.js installed from (npm or script link): master branch- TensorFlow.js version (use command below): master branch- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**In the image threshold op, otsu's method returns a different (incorrect) result for webgl in tfjs-react-native than in the browser.**Describe the expected behavior**Otsu's method returns the correct result in tfjs-react-native.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.Run CI tests on tfjs-react-native. The bug may also appear when running the integration tests of tfjs-react-native on a local android phone.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.I visualized the test results and for some reason, the otsu threshold test output matches the expected output for binary. This is almost certainly a coincidence.![](https://user-images.githubusercontent.com/1474501/122586563-b1be6b00-d011-11eb-99b5-e2523060f2ae.png)Logs:https://console.cloud.google.com/cloud-build/builds/15aa4850-a102-45f4-9525-c06588d6f668,step=7?project=learnjs-174218More details on this PR: https://github.com/tensorflow/tfjs/pull/5217",['[Reference colab contributed to the TF repo](https://colab.sandbox.google.com/drive/1CdVfa2NlkQBga1E9dBwHved36Tk7Bg61)[TF Proposal](https://github.com/tensorflow/addons/issues/358)====='],1
https://github.com/tensorflow/tfjs/issues/1866,Support op TF.unique,3,closed,2019-08-19T23:49:45Z,2020-10-06T22:57:47Z,It would be really great to have support for [tf.unique](https://www.tensorflow.org/api_docs/python/tf/unique).This has been brought up by a number of [previous](https://github.com/tensorflow/tfjs/issues/958) [issues](https://github.com/tensorflow/tfjs/issues/1693) and in general it is a very common operation that many applications rely on.,"['@evancasey thank you , may i know your use case ?=====', ""Hey @rthadur, I'm using tf.unique right now to do a type of one-hot encoding where I have an input vector with a discrete number of unique values but the values are scattered between [0,N] and N is potentially very large. I can't call tf.one_hot directly on this because I want the depth to be equal to the number of unique values.====="", '@pyu10055 any plans to support TF.unique op ?=====']",0
https://github.com/tensorflow/tfjs/issues/402,Add gradients for resizeNearestNeighbor,2,closed,2018-06-07T13:23:42Z,2018-06-26T15:00:12Z,See comment https://github.com/tensorflow/tfjs/issues/159#issuecomment-395342787,"[""cc @tafsiri if you have cycles, since you've implemented gradients for resizeBilinear, and this op is similar.====="", '@dsmilkov I can take a look=====']",0
https://github.com/tensorflow/tfjs/issues/4498,Unhandled Rejecting Error ~ Implicit Shape Can't Be a Fractional Number,1,closed,2021-01-06T20:23:31Z,2021-01-06T23:18:06Z,"Hello everyone, sosI am following an online tutorial on how to run gesture recognition using react and tensor flow. However, I am always seeing this error whenever I play around with the webcam in chrome.Here is my github for what I am working on btw. And here is the tutorial video I'm watching. I got stuck right around minute 10https://github.com/riccrdo5/helphttps://youtu.be/f7uBsb-0sGQTy and happy holidays![image](https://user-images.githubusercontent.com/67179440/103816496-eae1c080-5019-11eb-8e4e-f05b5622c2cd.png)![image](https://user-images.githubusercontent.com/67179440/103816511-f03f0b00-5019-11eb-96f3-d690d5164eba.png)",['This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow.js) since it is not a bug or feature request. There is also a larger community that reads questions there.====='],0
https://github.com/tensorflow/tfjs/issues/536,"CNN Conv Layer output some times mismatch with expected formula, please help!",0,closed,2018-07-22T04:23:02Z,2018-08-30T06:17:14Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version: 0.11.6#### Browser version: Chrome 67.0.3396.99#### Describe the problem or feature requestI am doing a project about machine learning visualisation, but the conv layer output confuses me a lot.Generally for a cnn convolution layer, the output should follow the formula:`output[ x , y ] = Relu( np.sum( input[ x : x + 5 , y : y + 5 , : ] * W ) + Bias)`The strange thing is when filter size is 1 everything is okay. However, when filter size is 2 or larger, the output does not match the formula any more. (The result is still converge and can get good accuracy)When only 1 filter:<img width=""1143"" alt=""screen shot 2018-07-22 at 12 13 02 pm"" src=""https://user-images.githubusercontent.com/14250127/43042170-b403f15e-8da8-11e8-8e81-04d2a71c46fe.png""> When 2 filters<img width=""1142"" alt=""screen shot 2018-07-22 at 12 13 30 pm"" src=""https://user-images.githubusercontent.com/14250127/43042172-c29951dc-8da8-11e8-8607-3c78df38178a.png"">#### Code to reproduce the bug / link to feature requestI have created a simple repo to reproduce this issue.Code is here:```import * as tf from '@tensorflow/tfjs',import {MnistData} from './data',function setupModel(filterSize){  let model = tf.sequential(),  model.add(tf.layers.conv2d({    inputShape: [28, 28, 1],    kernelSize: 5,    filters: filterSize,    strides: 1,    activation: 'relu',    kernelInitializer: 'varianceScaling'  })),  const LEARNING_RATE = 0.15,  const optimizer = tf.train.sgd(LEARNING_RATE),  model.compile({    optimizer: optimizer,    loss: 'categoricalCrossentropy',    metrics: ['accuracy'],  }),  return model,}async function showPredictions(filterSize) {  const batch = data.nextTestBatch(1),  let model = setupModel(filterSize),  tf.tidy(() => {    const input = batch.xs.reshape([-1, 28, 28, 1]),    const output = model.predict(input),    const inputData = input.dataSync(),    const outputData = output.dataSync(),    console.log(""filterSize"",filterSize),    // console.log(""input"",inputData),    // console.log(""output"",outputData),    //fixed input size 28x28, no padding, strides[1,1], kernel size 5    //Going to show first activation with inputs and weights and ouput    for(let y = 0, y < 24, y++){      for(let x = 0, x < 24, x++){        let index = y * 24 + x,        if(outputData[index] > 0){          console.log(`First activation at [${x},${y}]`)          let firstFilterWight = model.layers[0].getWeights()[0].dataSync().slice(0,25),          console.log(""weights of first filter"",firstFilterWight),                let inputWindow = []          for(let yKernel = 0, yKernel < 5, yKernel++ ){            let inputIndex = y * 28 + yKernel * 28 + x ,            inputWindow.push(...inputData.slice(inputIndex,inputIndex+5)),          }          console.log(""Input window values"",inputWindow),          let dotProduct = 0,          for(let i in inputWindow){            dotProduct += inputWindow[i] * firstFilterWight[i],          }          //bias          let bias = model.layers[0].getWeights()[1].dataSync()[0],          dotProduct += bias,          console.log(""dot product"",dotProduct),          console.log(`First activation at [${x},${y}]`,outputData[index])          return,        }      }    }  }),}let data,async function load() {  data = new MnistData(),  await data.load(),}async function mnist() {  await load(),  showPredictions(1),  showPredictions(2),}mnist(),```Here is the result:<img width=""753"" alt=""screen shot 2018-07-22 at 12 01 13 pm"" src=""https://user-images.githubusercontent.com/14250127/43042184-39ef7dd8-8da9-11e8-9dd3-414734ca8f6a.png"">As you can see when filter size is 1, dot prodcut matches output, but when filter size >=2 it does not match.[Repo is here](https://github.com/siwei0729/tfjs_CNN_example2)Clone the repo then run ```yarnyarn watch```",[],0
https://github.com/tensorflow/tfjs/issues/985,Runs layers benchmarks as unit tests and log them in the cron.,0,closed,2018-12-12T16:39:45Z,2019-05-21T16:56:54Z,Move existing layers benchmarks over to tfjs union.,[],0
https://github.com/tensorflow/tfjs/issues/1107,UnicodeEncodeError: 'charmap' codec can't encode characters in position 858-861: character maps to <undefined>,4,closed,2019-01-19T03:50:58Z,2019-08-13T18:08:09Z,"To get help from the community, we encourage using Stack Overflow and the [`tesnorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versiontfjs-node v0.2.3#### Browser versionnode v10.15.0#### Describe the problem or feature requestinstall failed#### Code to reproduce the bug / link to feature requestnode-gyp v3.8.0python v2.7.15 throw new Error('node-gyp rebuild failed with: ' + err),      ^Error: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuildWarning: unrecognized setting VCCLCompilerTool/MultiProcessorCompilationWarning: unrecognized setting VCCLCompilerTool/MultiProcessorCompilationTraceback (most recent call last):  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\gyp_main.py"", line 16, in <module>    sys.exit(gyp.script_main())  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\__init__.py"", line 545, in script_main    return main(sys.argv[1:])  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\__init__.py"", line 538, in main    return gyp_main(args)  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\__init__.py"", line 523, in gyp_main    generator.GenerateOutput(flat_list, targets, data, params)  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\generator\msvs.py"", line 2004, in GenerateOutput    generator_flags))  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\generator\msvs.py"", line 943, in _GenerateProject    return _GenerateMSVSProject(project, options, version, generator_flags)  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\generator\msvs.py"", line 1047, in _GenerateMSVSProject    p.WriteIfChanged()  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\MSVSProject.py"", line 208, in WriteIfChanged    encoding=""Windows-1252"")  File ""C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\easy_xml.py"", line 122, in WriteXmlIfChanged    xml_string = xml_string.decode(default_encoding).encode(encoding)  File ""C:\Python27\lib\encodings\cp1252.py"", line 12, in encode    return codecs.charmap_encode(input,errors,encoding_table)UnicodeEncodeError: 'charmap' codec can't encode characters in position 858-861: character maps to <undefined>gyp ERR! configure errorgyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\configure.js:345:16)gyp ERR! stack     at ChildProcess.emit (events.js:182:13)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:240:12)gyp ERR! System Windows_NT 10.0.17134gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild""gyp ERR! cwd D:\机器学习\test\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v10.15.0gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok    at cp.exec (D:\机器学习\test\node_modules\@tensorflow\tfjs-node\scripts\install.js:171:13)    at ChildProcess.exithandler (child_process.js:301:5)    at ChildProcess.emit (events.js:182:13)    at maybeClose (internal/child_process.js:962:16)    at Process.ChildProcess._handle.onexit (internal/child_process.js:251:5)npm WARN rollback Rolling back @types/seedrandom@2.4.27 failed (this is probably harmless): EPERM: operation not permitted, rmdir 'D:\机器学习\test\node_modules\@types'npm WARN test@1.0.0 No descriptionnpm WARN test@1.0.0 No repository field.","['@kangyizhang @nkreeger This seems to be related to non-ASCII characters in the file path.=====', 'This should be a windows specific issue. @MrNewWorld can you try to install windows-build-tools and try again?=====', '> @kangyizhang @nkreeger> This seems to be related to non-ASCII characters in the file path.thanks, it was caused by the non-ASCII characters in the file path..=====', 'I tried path with non-ASCII characters on both linux and windows, it works fine. `npm install --global windows-build-tools` should fix it on windows.=====']",0
https://github.com/tensorflow/tfjs/issues/1274,Module not found: Error: Can't resolve 'fs',6,closed,2019-02-22T15:01:20Z,2019-03-18T18:37:22Z,"#### TensorFlow.js version    @tensorflow/tfjs@0.15.1#### Browser version    Chrome Version 72.0.3626.109 (Official Build) (64-bit)#### Describe the problem or feature requestMy Angular app was using tfjs@0.13.4. When I try to upgrade to `0.15.1`, I got this error when executing the `ng build` command. But I am not even using the `tfjs-data` component.After I revert tfjs to **0.13.4**, my code compiles and the app worked again.#### Code to reproduce the bug / link to feature requestAll the dependencies in my app.```javascript      ""dependencies"": {	""@angular/animations"": ""^7.2.6"",	""@angular/cdk"": ""^7.3.3"",	""@angular/common"": ""^7.2.6"",	""@angular/compiler"": ""^7.2.6"",	""@angular/core"": ""^7.2.6"",	""@angular/forms"": ""^7.2.6"",	""@angular/http"": ""^7.2.6"",	""@angular/material"": ""^7.3.3"",	""@angular/platform-browser"": ""^7.2.6"",	""@angular/platform-browser-dynamic"": ""^7.2.6"",	""@angular/pwa"": ""^0.13.3"",	""@angular/router"": ""^7.2.6"",	""@angular/service-worker"": ""^7.2.6"",	""@davidshen84/ngx-webcam"": ""^0.2.2"",	""@tensorflow/tfjs"": ""^0.15.1"",	""@types/hammerjs"": ""^2.0.36"",	""angularx-qrcode"": ""^1.5.3"",	""core-js"": ""^2.6.5"",	""hammerjs"": ""^2.0.8"",	""ngx-clipboard"": ""^11.1.9"",	""ngx-markdown"": ""^7.1.3"",	""ngx-mathjax"": ""0.0.4"",	""ngx-store"": ""^2.1.0"",	""normalize.css"": ""^8.0.1"",	""prismjs"": ""^1.15.0"",	""rxjs"": ""^6.4.0"",	""zone.js"": ""^0.8.29""      },      ""devDependencies"": {	""@angular-devkit/build-angular"": ""^0.13.3"",	""@angular/cli"": ""^7.3.3"",	""@angular/compiler-cli"": ""^7.2.6"",	""@angular/language-service"": ""^7.2.6"",	""@types/del"": ""^3.0.1"",	""@types/gulp"": ""^4.0.5"",	""@types/gulp-shell"": ""0.0.31"",	""@types/jasmine"": ""^3.3.9"",	""@types/jasminewd2"": ""^2.0.6"",	""@types/mathjax"": ""0.0.35"",	""@types/node"": ""^11.9.4"",	""ajv"": ""^6.9.1"",	""codelyzer"": ""~4.5.0"",	""del"": ""^3.0.0"",	""gulp"": ""^4.0.0"",	""gulp-shell"": ""^0.6.5"",	""jasmine-core"": ""~3.3.0"",	""jasmine-spec-reporter"": ""~4.2.1"",	""karma"": ""^4.0.0"",	""karma-chrome-launcher"": ""~2.2.0"",	""karma-coverage-istanbul-reporter"": ""^2.0.5"",	""karma-jasmine"": ""^2.0.1"",	""karma-jasmine-html-reporter"": ""^1.4.0"",	""protractor"": ""^5.4.2"",	""rxjs-spy"": ""^7.5.0"",	""ts-node"": ""~8.0.2"",	""tslint"": ""~5.12.1"",	""typescript"": ""^3.2.4""      }```","['Experiencing the exact same issue....reverting back to 0.14.1 works for me=====', 'hi @davidshen84 and @djimoh5 @tensorflow/tfjs-data is part of the @tensorflow/tfjs union package. The error is because of angular-cli does not support modules in node like ""fs"" and ""path"". Original issue: https://github.com/angular/angular-cli/issues/10681I tested locally and the ""fs"" can be resolve by adding `""browser"": { ""fs"": false, ""path"": false, ""os"": false}` in your package.json (original solution: https://github.com/angular/angular-cli/issues/8272#issuecomment-392777980)=====', ""Instead of asking our users to specify the browser field in their `package.json`, we should update our `tfjs-data/package.json` to have 'node-fetch':false, 'string_decoder':false and 'fs':false under `package.browser`.I found those 3 packages when I searched for 'require()' usages in tfjs-data repo.@kangyizhang can you do that? Thanks! ====="", '@dsmilkov totally agree with you. Sent PR https://github.com/tensorflow/tfjs-data/pull/147=====', 'Why this error happens every time updated version??? =====', '@NiNJAD3vel0per are you still seeing this error? Could you make sure you remove node_modules and re-install?=====']",0
https://github.com/tensorflow/tfjs/issues/5421,how can i get the z (depth) value correctly with face-landmarks-detection?,7,closed,2021-08-03T03:29:43Z,2021-08-26T19:52:32Z,"I'm trying to get the depth of the face, but it's seems  wrong about code , i using npm to install tensorflow-model/face-landmarks-detection and npm's examples, but i get the wrong scaleMesh z value in result.","['can you please explain in detail what you are trying to do , I see the demo is working as expected please check here https://storage.googleapis.com/tfjs-models/demos/face-landmarks-detection/index.html , thank you =====', 'Think you for your help!!! ,i am try to do face detect and predicted the distance between face and camera, I got the depth value according to the sample provided by the @tensorflow-models/face-landmarks-detecton is different with @tensorflow-models/facemesh, and @tensorflow-models/face-landmarks-detecton Unable to handle ROLL Angle over 45 degrees(it will detect a wrong small face even face still at same position). =====', 'Can you please post the difference which you are seeing both Tensorflow and tfjs ? Thank you =====', ""> Can you please post the difference which you are seeing both Tensorflow and tfjs ? Thank youok, here is my experiment, I try to get a z value from face-landmark-detection similar to facemesh, It's diden't work, what ever prediction.scaledMesh or prediction.Mesh ,i Can‘t get z value like facemesh </br>### face-landmark-detection0.0.3 ###![face-landmark-detect0 05roll90](https://user-images.githubusercontent.com/32930325/128314707-8f720175-31d6-4ab4-83ad-38c97a9394ce.png)![face-landmark-detection0 05roll0](https://user-images.githubusercontent.com/32930325/128314759-183b75fa-3277-42b1-8754-2f3fc3345226.png)</br>### facemesh0.0.1/0.0.5 ### ![facemesh0 03 rol0](https://user-images.githubusercontent.com/32930325/128314760-d6e2b8cc-d273-4d0e-b00a-6a5db2440f7f.png)![facemesh0 03roll90](https://user-images.githubusercontent.com/32930325/128314758-a002255d-5686-4309-bd7b-887c48c6c9ca.png)====="", 'I see no error here in the demo https://storage.googleapis.com/tfjs-models/demos/facemesh/index.html , can you please verify ? =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/2837,[deleted],2,closed,2020-03-06T16:17:26Z,2020-03-13T04:56:02Z,[deleted],"[""Hi,I noticed in your code:```jsmodel.add(tf.layers.dense({units: 10, activation: 'sigmoid',inputShape: [2]})),model.add(tf.layers.dense({units: 1, activation: 'sigmoid',inputShape: [10]})),```This means that every time you refresh the page, you will get new random set initial weights, which affect the final output. To make it more consistent, provide a fixed initial value for the weights. See the weights argument in https://js.tensorflow.org/api/latest/#layers.denseRegarding speed, WebGL has a fixed overhead, and should only be used for large numerical computation. That is, adding 2 numbers in WebGL is just as fast as adding 100,000 numbers.====="", '[deleted]=====']",0
https://github.com/tensorflow/tfjs/issues/5742,backend tfjs-backend-webgpu correctness is false on e2e|posenet|1024|ResNet50|image,1,open,2021-10-19T09:13:38Z,2021-11-18T20:06:33Z,"Steps:Run Test correctness on e2e posenet model with 1024|ResNet50|image configuration.Device:Tiger-Lake.Result:Prediction matches GPU is false. actual[score] = 0.615322583793279, expected[score] = 0.6023533163482652","['@axinging As we discussed, you had already set up to the framework to debug the similar issue, please see it, thank you.=====']",1
https://github.com/tensorflow/tfjs/issues/4709,Latest version of speech-commands library won't load in browser,3,closed,2021-02-17T22:32:58Z,2021-02-19T10:01:45Z,"For a simple recreate:```<html><head><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js""></script><script src=""https://unpkg.com/@tensorflow-models/speech-commands@0.5.1""></script></head><body></body></html>```The browser console shows the error:```speech-commands@0.5.1:17 Uncaught TypeError: Cannot read property 'Buffer' of undefined    at speech-commands@0.5.1:17    at Ms (speech-commands@0.5.1:17)    at speech-commands@0.5.1:17    at speech-commands@0.5.1:17    at speech-commands@0.5.1:17```This used to work, and if you pin the speech-commands version you can see earlier versions don't have this problem.For example, this works correctly```<html><head><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js""></script><script src=""https://unpkg.com/@tensorflow-models/speech-commands@0.4.2""></script></head><body></body></html>```For real examples of this in the wild, see pages like these:- https://glitch.com/~abrasive-ixora (cf. https://yashints.dev/blog/2019/12/16/tfjs-transfer-learning )- https://livecodestream.dev/post/speech-recognition-with-tensorflowjs/ - https://bensonruan.com/speech-recognition-with-tensorflow-js/- https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab#2(which, at the time of writing, are currently broken because they're using https://unpkg.com/@tensorflow-models/speech-commands to just grab the latest released version)I haven't tried this on every browser, but it's broken on the current versions of Firefox and Chrome on MacOS.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4709"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4709"">No</a>=====', '@dalelane Thank you for reporting this bug, new build 0.5.2 has been release, I have checked some of the website, they seems to be working as expected. =====', '@pyu10055 Fantastic - thanks for fixing it so quickly!=====']",1
https://github.com/tensorflow/tfjs/issues/277,Update docstring for tf.setBackend(): it creates a new Engine and disposes the old one.,2,closed,2018-05-07T22:42:04Z,2018-05-10T10:23:49Z,"It seens that a variable can't be defined before using ""tf.setBackend()"", even if the backend is the same.```// I create a variable tensor ""BARIABLE_TENSOR"".const BARIABLE_TENSOR = tf.variable(tf.fill([1, 2], 0.5)),// It's ""webgl"".console.log('Backend: ' + tf.getBackend()),// It was ""webgl"" before, so tf.setBackend('webgl') probably didn't do anything.tf.setBackend('webgl'),// I try to do some calculations.tf.print(BARIABLE_TENSOR.add(tf.ones([1, 2]))),// All right, I continue.const denseModel = tf.sequential(),denseModel.add(tf.layers.dense({inputShape:[2], units:1})),const optimizer = tf.train.sgd(0.01),const loss = () => {    return denseModel.predict(BARIABLE_TENSOR).asScalar(),},// Something worng happened:// Argument 'b' passed to 'add' must be a Tensor, but got undefined.optimizer.minimize(loss, true, [BARIABLE_TENSOR]),```#### TensorFlow.js 0.10.0#### Browser version Chrome 66.0.3359.139","['This is working as intended.  tf.setBackend() always initializes a new execution environment and disposes any preexisting one.I see we could improve the documentation to clarify that, though.=====', '**Thanks, I see that [1dff6a8](https://github.com/tensorflow/tfjs-core/commit/1dff6a88328d6bb4ee1efc3e8680c25d1b56c777)**> Note this disposes the current backend, if any, as well as any tensors associated with it.  A new backend is initialized, even if it is of the same type as the previous one.**It may be my misunderstanding that `tf.setBackend()` did not clear the tensors:**(1) Obviously, tensor `test` still exists after `tf.setBackend()`, but `tf.memory().numTensors` is 0. Actually, after `tf.setBackend()`, `tf.memory().numTensors` is always 0. So, `tf.memory()` can\'t be use to show how many tensors exists in memory before `tf.setBackend()`, or tensors before `tf.setBackend()` shouldn\'t exists.```tf.setBackend(\'webgl\'),const test = tf.tensor([1,2,3]),console.log(tf.memory().numTensors), // numTensors = 1tf.setBackend(\'webgl\'),console.log(tf.memory().numTensors), // numTensors = 0test.print(), // ""test"" existstest.add(tf.tensor([1,1,1])), // It can even be used to calculatetest.print(),tf.setBackend(\'cpu\'),    // switch to a different backendtf.setBackend(\'webgl\'),  // switch backtest.print(), // ""test"" still exists...```(2) The tensors created before `tf.setBackend()` can use `add()` or `mul()` method after `tf.setBackend()`, but it can\'t be used for Optimizers. In fact, I found that the following code can ""work"":```const BARIABLE_TENSOR = tf.variable(tf.fill([1, 2], 0.5), true, \'tensor_train\'),console.log(\'Backend: \' + tf.getBackend()),tf.setBackend(\'webgl\'),console.log(\'BARIABLE_TENSOR:\'),tf.print(BARIABLE_TENSOR),/* * I just create a new variable tensor. And I set The * name is same with BARIABLE_TENSOR. */const TEST_TENSOR = tf.variable(tf.fill([1, 2], 0.5), true, \'tensor_train\'),const denseModel = tf.sequential(),denseModel.add(tf.layers.dense({inputShape:[2], units:1})),const optimizer = tf.train.sgd(0.01),const loss = () => {    return denseModel.predict(BARIABLE_TENSOR).asScalar(),},optimizer.minimize(loss, true, [BARIABLE_TENSOR]),console.log(\'BARIABLE_TENSOR & TEST_TENSOR:\'),tf.print(BARIABLE_TENSOR),tf.print(TEST_TENSOR),````optimizer.minimize()` want to change BARIABLE_TENSOR, but what is really changed is TEST_TENSOR.**Generally, it shows that your ""tensors manager(or something else, I don\'t know what you call it)"" seems to have some problems. If everything is normal, what puzzles me is that (2) seems to want to say that the tensor previously created will affect the recent tensor in some way.**=====']",0
https://github.com/tensorflow/tfjs/issues/953,Support GLSL ES300 to improve precision on Android devices where WebGL 2.0 is available.,3,closed,2018-11-29T15:52:26Z,2019-03-26T20:18:11Z,"We'll have to do a bit of wrangling to still support the older ES versions (WebGL 1.0), but this will improve precision on Android devices with WebGL 2.0 (iPhones are still on WebGL 1.0).See the commit here: https://github.com/GreyZzzzzzXh/tfjs-core/commit/39a56bf75803ae9327bbf160189f0d08f8a06416cc @GreyZzzzzzXh","[""This commit simply changed the version of GLSL so WebGL 1.0 doesn't work now.Next i'll try to support multiple shading languages with your help and update it in https://github.com/tensorflow/tfjs-core/pull/1421.Hope that will help you, thanks!====="", '@nsthorat i see the [PR](https://github.com/tensorflow/tfjs-core/pull/1421) is been merged , can we close this ?=====', 'Yes, this is fixed!=====']",0
https://github.com/tensorflow/tfjs/issues/5504,Cannot convert model to tensorflowjs from tf hub,1,closed,2021-08-18T04:41:05Z,2021-08-18T12:04:14Z,"I am using tensorflowjs_converter to convert a tflite model to tensorflowjs model.The repository can be download with web browsers but failed when using tensorflowjs_converter.```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/dr/predict/1' ./web_model```It outputs an Http 404 but my connection is fine. When I run the example code it works fine and I downloaded it.```tensorflowjs_converter --input_format=tf_hub 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1' ./web_model  ```If you can tell me the reason or you can successfully convert tflite model from the following URLs, I will appreciate it a lot. Thank you so much. https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/dr/predict/1https://tfhub.dev/sayakpaul/lite-model/cartoongan/dr/1","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5504"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5504"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5212,Example for pose detection in bare react-native,1,closed,2021-06-12T11:05:00Z,2021-06-14T17:32:02Z,Can anyone suggest me a perfect example for pose detection from a  stream in bare react-native platform?,['This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.====='],0
https://github.com/tensorflow/tfjs/issues/304,Add Node.js implementation for resizeBilinearBackprop,1,closed,2018-05-16T16:43:45Z,2018-05-31T22:03:19Z,The core TF library does not have an implementation for this so we'll have to piece it together using a combination of Ops.,['@nkreeger in TF I believe this is called ResizeBilinearGrad.====='],0
https://github.com/tensorflow/tfjs/issues/5377,How to use execute instead of executeAsync for object detection model?,3,open,2021-07-25T12:49:09Z,2021-08-24T20:32:35Z,"Good day!I have integrated Unity WebGL with TensorflowJS.When calling the JavaScript script from Unity, I need to return the search result for objects detection.The result must be obtained synchronously, since it is used in Unity for post-processing (realtime, in the same frame).How can I use execute instead of executeAsync for object detection (mobilenet v3)?Now, when I try to call executeAsync, I get the error:Uncaught (in promise) Error: This execution contains the node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Exit_2', which has the dynamic op 'Exit'. Please use model.executeAsync() instead. Alternatively, to avoid the dynamic ops, specify the inputs [Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3]","['Please provide a codepen example or git repo for us to reproduce the same , meanwhile can you please check below related issue https://github.com/tensorflow/tfjs/issues/4579 , thank you =====', ""**rthadur**,You can see my sample code and model here:https://github.com/tensorflow/tfjs/issues/5259It work with version 3.3.0 when using the executeAsync function and doesn't work with execute function.====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",1
https://github.com/tensorflow/tfjs/issues/352,TensorFlow.js crashes on mobile Safari,6,closed,2018-05-28T10:34:19Z,2018-06-05T15:21:50Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js versiontfjs 0.11.1tfjs-converter 0.4.0#### Browser versionMobile Safari 11#### Describe the problem or feature requestThe browser crashes without any error in console, also loading a simple and small network model converted using tensor flow js converter","['@andreaferracani Can you provide a little more information about the crash? What model were you converting and using in the browser? At what stage does the crashing happen? When you call `tf.loadFrozenModel` or some other method?cc @pyu10055 =====', '1. The model is a simplified version of Yolo v. 12. The crash happens when we use dataSync() as it follows``` var result_t = model.execute({""image_input"" : input}),var result = result_t.dataSync(),```Thank you=====', '@andreaferracani thank you for reporting this bug, in order to properly reproduce the problem, is it possible for you to share the model with us?And does this issue happen only on mobile Safari? How about desktop and mobile chrome?Thanks! =====', 'Hi this is the model which causes the crash and the demo in the zipped folder https://drive.google.com/open?id=1UuWSH3FgIqBoHbPdPJCUA9iGV3fXBu04=====', 'Your process method is calling itself continuously, which causing the browser to freeze.Increase the timeout or set a limit on how many to run would help.```  13        function process(model) { 14             var input = tf.randomNormal([1, 448, 448, 3]) 15             var result = model.execute({""image_input"" : input}).dataSync(), 16             console.log(""done"") 17             setTimeout(process, 0, model), 18         }```=====', ""HiUnfortunately that's not the problem, the bug occurs even without calling the setTimeout function repeatedly. In fact it happens the second time we call the function at line 15. The code is just an example and it works on desktop browsers, in Android but not on mobile SafariThanks=====""]",0
https://github.com/tensorflow/tfjs/issues/4892,conv2dTranspose gives different resutls on WebGL and WASM backends,3,closed,2021-04-01T09:59:13Z,2021-04-24T15:19:05Z,"**System information**- TensorFlow.js version: 3.2- Browser version: Chrome 89 / pop! OS 20.04 LTSOutput of the [tf.layers.conv2dTranspose](https://js.tensorflow.org/api/latest/#layers.conv2dTranspose) on WebGL backend is different from WASM backend even for simple cases. Other layers like conv2d, upsample, etc work fine (with very small numerical differences).Simple reproduction code:```javascriptvar cfg = {    filters: 1,    kernelSize: 2,     strides: 2,     padding: ""same"",     batchInputShape: [1, 144 , 256, 3],    useBias: false,    kernelInitializer: 'ones'}async function main() {    await tf.setBackend('webgl'),    const data_from_webgl = await evaluateModel(),    await tf.setBackend('wasm'),    const data_from_wasm = await evaluateModel(),    console.log(data_from_webgl),    console.log(data_from_wasm),}async function evaluateModel() {    const model = tf.sequential(),    model.add(tf.layers.conv2dTranspose(cfg)),    const prediction = tf.tidy(() => {        const imgTensor = tf.randomNormal([1, 144, 256, 3], 0, 1, 'float32', 11111),        const y = model.predict(imgTensor),        return y,    }),    return prediction.data(),}main(),```","['@drnemor I am curious if you are using tf.randomNormal, the input would change on every execution, it will lead to different output.=====', ""@pyu10055 I used the same random seed, so randomNormal gives same tensor on each execution regardless of chosen backend. You can check that by replacing ```model.add(tf.layers.conv2dTranspose(cfg)) ``` with ```model.add(tf.layers.activation({batchInputShape: [1, 144 , 256, 3], activation: 'linear'}))```. This gives exactly the same tensors on both backends. Also, you can check other layers like ```tf.layers.conv2d```, they also produce the same results, but not ```conv2dTranspose```. ====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4892"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4892"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5349,Is there some way to surpress this warning message?,8,closed,2021-07-19T20:15:35Z,2021-07-23T15:17:36Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 3.8.0- Are you willing to contribute it (Yes/No): maybe but likely no**Describe the feature and the current behavior/state.**👋 from the VS Code teamWe use tfjs in VS Code and see this warning:![image](https://user-images.githubusercontent.com/2644648/126220844-e3eb21e8-85d8-4728-a9ec-9413d0ff2896.png)It would add a lot of extra complexity to include tfjs-node in VS Code so we'd like to not do what it's suggesting and instead would like to suppress this warning.Is this possible?**Will this change the current api? How?**No**Who will benefit with this feature?**People who don't want to use tfjs-node**Any Other info.**","[""The 1st warning is also a little strange... it's something inside of tfjs causing that.====="", ""@TylerLeonhardt what environment are you running in? seems both of the flags IS_BROWSER and IS_NODE are true.```export function isBrowser(): boolean {  return (typeof window !== 'undefined' && window.document != null) ||      //@ts-ignore      (typeof WorkerGlobalScope !== 'undefined'),}/** Whether we are in a browser (as versus, say, node.js) environment. */ENV.registerFlag(    'IS_NODE',    () => (typeof process !== 'undefined') &&        (typeof process.versions !== 'undefined') &&        (typeof process.versions.node !== 'undefined')),```====="", ""@pyu10055 within VS Code. My guess is that since it's an electron app... both return true.![image](https://user-images.githubusercontent.com/2644648/126249367-22179365-f7df-48bf-a612-40f7b4251ed8.png)You can test this by opening up the dev tools in VS Code:* Open the command palette and type: `Dev Tools`* then choose this one:![image](https://user-images.githubusercontent.com/2644648/126249326-bf9b28e1-7886-4d88-98f8-c4b5a34a717d.png)====="", '@TylerLeonhardt in this case, you can just use webgl backend by not including tfjs-backend-cpu package.in you deps, you can add only tfjs-core, tfjs-backend-webgl, tfjs-converter packages instead of tfjs union package.Or if you prefer cpu backend, you can swap webgl dep with cpu in above list.=====', '@pyu10055 thanks for the pointer! I believe you said CPU is best in our use-case in the previous issue, so I\'m trying that. Here\'s what I tried:* Removing `@tensorflow/tfjs` from package.json* Adding these to package.json:  *  `""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"",`  *  `""@tensorflow/tfjs-converter"": ""^3.8.0"",`  * `""@tensorflow/tfjs-core"": ""^3.8.0"",`* Changing this:```import { GraphModel, io, loadGraphModel, Rank, setBackend, tensor, Tensor } from \'@tensorflow/tfjs\',```to```import { Rank, tensor, Tensor, io, setBackend } from \'@tensorflow/tfjs-core\',import { GraphModel, loadGraphModel } from \'@tensorflow/tfjs-converter\',import \'@tensorflow/tf-backend-cpu\',```Unfortunately, this still yields the logs in the original screenshot.For your reference, the code in question is found here: https://github.com/Microsoft/vscode-languagedetectionand is then loaded into vscode. The test in the vscode-languagedetection repo shows the ""hi there 👋 "" log.=====', ""@TylerLeonhardt can you try to set `IS_NODE` to false before you load the model?```tf.env().set('IS_NODE', false),```====="", '@pyu10055 That gets rid of the ""hi there 👋 "" log, great!The last one that shows up is:```Platform browser has already been set. Overwriting the platform with [object Object].```which only shows up when running in VS Code... (in electron, I guess)... in my mocha tests in the vscode-languagedetection repo I do not see it.=====', 'Thanks for this @pyu10055! =====']",1
https://github.com/tensorflow/tfjs/issues/4724,Built in image decoder for note js.,9,closed,2021-02-22T12:52:29Z,2021-04-27T22:36:34Z,"**System information**- Operating system: macOS- TensorFlow.js version (you are using): 2.8.6- Are you willing to contribute it (Yes/No): No**Describe the feature and the current behaviour/state.**A built-in image decoder for node js. At the moment image needs to be decoded through node-canvas, which is extremely slow for real-time detection.```  if(data && data.body) {    const img = new Image()    img.src = data.body,    const canvas = createCanvas(320, 180),    const ctx = canvas.getContext('2d'),    const input = tf.browser.fromPixels(canvas),    try {      predictions = await model.estimateSinglePose(input, {        flipHorizontal: false      }),    } catch(e) {      console.log(e),    }  }```**Will this change the current api? How?**Yes, it'll add another method to decode the image.**Who will benefit with this feature?**Anybody who to use Posenet without sacrificing the client frame rate.**Any Other info.**","['Decode image for Node is already implemented here https://github.com/tensorflow/tfjs/blob/master/tfjs-node/src/image.ts#L155 , please verify=====', 'So I\'ve tried the following code,`CLIENT````    const imageData = context.getImageData(0, 0, 320, 180),    const buffer = imageData.data.buffer,    socket.emit(""signal"", buffer), //Pass it to the server through websocket````BACKEND````socket.on(""signal"", (data)=> {\u3000\u3000const buffer = new Uint8Array(data),\u3000\u3000const imgData = ts.node.decodeImage(buffer), //error throwns here.})```on the backend, I\'ve tried to decode the buffer, but this error was thrown out.```throw new Error(\'Expected image (BMP, JPEG, PNG, or GIF), but got unsupported \' +        ^Error: Expected image (BMP, JPEG, PNG, or GIF), but got unsupported image type    at getImageType (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-node/dist/image.js:351:15)    at Object.decodeImage (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-node/dist/image.js:196:21)    at Socket.socket.on (/Users/xxx/app/server/app.js:37:29)    at Socket.emit (events.js:182:13)    at Socket.emitUntyped (/Users/xxx/app/server/node_modules/socket.io/dist/typed-events.js:69:22)    at process.nextTick (/Users/xxx/app/server/node_modules/socket.io/dist/socket.js:428:39)    at process._tickCallback (internal/process/next_tick.js:61:11)```Is there something I\'ve missed?=====', ""you do not need to explicitly convert to Unit8Array , here is the sample code from one of the [SO questions ](https://stackoverflow.com/questions/65429456/cant-open-jpg-or-png-after-saving-it-using-node-js-fs-and-http)`saveImageToDisk(imageurl,filepath)            const img_buffer = fs.readFileSync(filepath)            const img = tf.node.decodeImage(img_buffer)            coco.load().then(model => {                // detect objects in the image.                model.detect(img).then(predictions => {                    console.log('Predictions: ', predictions),                }),              }),`====="", '@rthadur thanks, but I have a different use case. The reason I need the Unit8Array is that I need to pass the webcam image to the server.Please check my original post.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@bebensiganteng when you use the getImageData() call, it returns the pixel values of the image, which means it is decoded, you can use it directly to create the tensors for model execution.=====', '@pyu10055 could you elaborate, please? And did you actually tried it? because it threw me this error.`CLIENT````const imageData = context.getImageData(0, 0, 320, 180),socket.emit(""signal"", imageData), //Pass it to the server through websocket````BACKEND````  socket.on(""signal"", async (data)=> {    if(model) {      try {        const pose = await model.estimateSinglePose(data, {          flipHorizontal: false        }),        console.log(pose),      } catch(e) {        console.log(e),      }    }  })``````Error: Tensor must have a shape comprised of positive integers but got shape [,,3].   at assert (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:335:15)   at /Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:813:9   at Array.forEach (<anonymous>)   at assertNonNegativeIntegerDimensions (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:812:11)   at makeTensor (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4081:9)   at tensor3d (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:7445:12)   at fromPixels_ (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:7588:12)   at Object.fromPixels__op [as fromPixels] (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3986:29)   at toInputTensor (/Users/xxx/app/server/node_modules/@tensorflow-models/posenet/dist/util.js:211:60)   at /Users/xxx/app/server/node_modules/@tensorflow-models/posenet/dist/util.js:247:27```=====', 'you need to create a tensor from the data, with something as following:height = 180, width=320, channels=4 ```    const temp = tf.tensor(new Uint8Array(data), [height, width, channels]),    const image = tf.slice(temp, [0, 0, 0], [-1, -1, 3]),    temp.dispose(),        const pose = await model.estimateSinglePose(image, {          flipHorizontal: false    }),    console.log(pose),```=====', 'It worked! thank you! `CLIENT````          const imageData = refCam.current.ctx.getImageData(0, 0, 320, 180),          const buffer = imageData.data.buffer,          socket.emit(""signal"", buffer),````BACKEND````    let pose = []    if(model) {      const buffer = new Uint8Array(data),      const temp = tf.tensor(buffer, [HEIGHT, WIDTH, CHANNELS]),      const image = tf.slice(temp, [0, 0, 0], [-1, -1, 3]),      temp.dispose(),      pose = await model.estimateSinglePose(image, {        flipHorizontal: false      }),    }    socket.emit(""messageAll"", pose),```@pyu10055 If you don\'t mind could explain what `tf.slice `means, please?=====']",1
https://github.com/tensorflow/tfjs/issues/3003,[webgpu] Avoid error out if the browser doesn't support WebGPU,4,closed,2020-04-01T00:45:07Z,2020-09-04T07:14:16Z,"Currently there are models  which support dymanically select backend (e.g. https://github.com/tensorflow/tfjs-models/blob/master/facemesh/demo/index.js#L172).But when add the webgpu backend support for the handpose (https://github.com/tensorflow/tfjs-models/pull/427), this may error out when running on a non WebGPU support browser due to the import chain, though the WebGPU codepath is not really hit during run: ```//handpose/src/pipeline.tsimport {rotate as rotateWebgpu} from './rotate_webgpu',     if (backend === 'webgl') {       rotatedImage = rotateWebgl(image, angle, 0, palmCenterNormalized),   } else if (backend === 'webgpu') {     rotatedImage = rotateWebgpu(image, angle, 0, palmCenterNormalized),```In handpose/src/pipeline.ts, it imports rotate_webgpu.ts```import {rotate as rotateWebgpu} from './rotate_webgpu',```In rotate_webgpu.ts, it imports tfjs-backend-webgpu:```import * as tfwebgpu from '@tensorflow/tfjs-backend-webgpu',```The detailed error when running on a non - WebGPU chromium:```backend_webgpu.ts:96 Uncaught ReferenceError: GPUBufferUsage is not defined    at Object.parcelRequire.node_modules/@tensorflow-models/handpose/node_modules/@tensorflow/tfjs-backend-webgpu/dist/backend_webgpu.js../flags_webgpu (backend_webgpu.ts:96)    at newRequire (demo.e31bb0bc.js:47)    at localRequire (demo.e31bb0bc.js:53)    at Object.parcelRequire.node_modules/@tensorflow-models/handpose/node_modules/@tensorflow/tfjs-backend-webgpu/dist/index.js../flags_webgpu (index.ts:23)    at newRequire (demo.e31bb0bc.js:47)    at localRequire (demo.e31bb0bc.js:53)    at Object.parcelRequire.node_modules/@tensorflow-models/handpose/dist/handpose.esm.js.@tensorflow/tfjs-core (handpose.esm.js:17)    at newRequire (demo.e31bb0bc.js:47)    at localRequire (demo.e31bb0bc.js:53)    at Object.parcelRequire.index.js.@tensorflow-models/handpose (index.js:18)```This error may be gone when the WebGPU is default on Chromium.We suggest to workout a method to avoid this error.","['Closing this due to lack of activity, feel to reopen. Thank you=====', ""Hi @rthadur,  we also met this error recently when we're using tfjs-backend-webgpu for POC, would you please help reopen and fix it, thanks.====="", 'Please open a new issue using template , thank you =====', ""Hi @rthadur, we didn't reproduce this issue with enabling experimental 'Unsafe WebGPU' flag of dev chromium on Windows and macOS platforms. I can't find this flag of dev chromium on Linux platform, so currently WebGPU isn't available for Linux platform, right?  thanks.=====""]",0
https://github.com/tensorflow/tfjs/issues/4429,tfjs 2.8.1 change in behavior in tf.reshape causes errors,13,closed,2020-12-18T01:54:11Z,2020-12-22T21:33:34Z,"upon upgrade from 2.7.0 to 2.8.1, my existing app fails with:```Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3    at Object.inferFromImplicitShape (util_base.ts:328)    at Object.reshape76 [as kernelFunc] (Reshape.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)    at Engine.runKernelFunc (engine.ts:657)    at Engine.runKernel (engine.ts:522)    at reshape_ (reshape.ts:60)    at reshape__op (operation.ts:51)```this may be desired behavior, but it breaks several existing models published on `tfhub`!e.g, i'm using `blazeface` where `reshape` is failing on results of model inference, so not like that can be changed easily:```jsconst [contours, confidence, contourCoords] = this.meshDetector.predict(face),const coordsReshaped = tf.reshape(contourCoords, [-1, 3]),let rawCoords = coordsReshaped.arraySync(),```","['Which backend has this behavior?=====', 'webgl backend=====', '@lina128 I am also receiving this error after upgrading to 2.8.1 with webgl.=====', 'Same thing here!=====', ""Same here! When trying to fetch face landmarks :` const predictions = await model.estimateFaces({ input: video }) `Browser returns `util_base.ts:328 Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3`====="", 'cc @pyu10055 @annxingyuan @tafsiri =====', ""Yup, I'm getting the same error as well! I still don't understand why demos like these https://mediapipe-facemesh-3js-tf2.glitch.me/ work. I tried changing the version of my facemesh model but that doesn't do anything====="", ""@dewball345 because it's hard coded to load tfjs 2.4.0. anything up to 2.7.0 works fine, 2.8.0 and 2.8.1 don't.====="", 'yeah i realized that now. A temporary fix would be to just use the older version of tensorflow?=====', ""I'm staying on 2.7.0 until this is resolved.====="", 'Hi @lina128 - I ran a git bisect and it seems to be traceable to this PR: https://github.com/tensorflow/tfjs/pull/4187=====', 'Thanks for the bug report, we will release the fix in 2.8.2 next week.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4844,"When in ""webgl"" backend and image width is odd, the second run of tf.conv2d() may be wrong.",5,closed,2021-03-20T12:48:53Z,2021-07-20T00:09:53Z,"**System information**- OS Platform and Distribution: Chrome OS, Android- Mobile device (the issue happens on mobile device, too): ASUS ZenPhone Max Pro- TensorFlow.js installed from: script link- TensorFlow.js version: 3.2.0- Browser version:  89.0.4389.82 (Stable) (64 bits)**Describe the current behavior**- When tf.gtBackend() is ""webgl"".  - If the width of the input is even, the result of tf.conv2d() is correct.  - If the width of the input is odd, the result of first time tf.conv2d() is correct. But the result of  second times run may be wrong.- When tf.gtBackend() is ""wasm"" or ""cpu"".  - The above problem seems not existed.**Describe the expected behavior**- No matter which backend, what kinds image width (even or odd), how many times to run tf.conv2d(), the result should be same and corrrect.**Standalone code to reproduce the issue**- https://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02- (The code could be pasted into any example console box of Tensorflow.js API (https://js.tensorflow.org/api/3.2.0/) web page to run.)","['I was able to reproduce the same in MAC book pro.cc @lina128 =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4844"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4844"">No</a>=====', 'Thank you all efforts for this strange problem. However, re-try the test codes in the gist (https://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02) in 3.8.0 tensorflow.js (https://js.tensorflow.org/api/3.8.0/) example console box. It seems that the problem still exists.=====', 'Hi @ColorfulCakeChen, the fix has not been released yet. It will be released in 3.9.0.=====', ""> Hi @ColorfulCakeChen, the fix has not been released yet. It will be released in 3.9.0.Oh. Thanks. It's my fault too impatient. XD=====""]",1
https://github.com/tensorflow/tfjs/issues/5891,[Codelab]: TensorFlowJS Comment Spam Detection,1,closed,2021-11-24T19:36:06Z,2021-11-29T21:31:38Z,"At the end of step 9, when I'm supposed to see 2 numbers printed in the console, I don't see any numbers. I do see the comment I post in the console. (I'm using Glitch)There are however 2 errors in the console. > 20:28:04.953 cdn.glitch.me/group1-shard1of1.bin?v=1637779856621:1 Failed to load resource: the server responded with a status of 404 (Not Found)> > 20:28:04.962 io_utils.js:169 Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4>     at new Float32Array (<anonymous>)>     at yw (io_utils.js:169)>     at oT (models.js:334)>     at models.js:316>     at u (runtime.js:45)>     at Generator._invoke (runtime.js:274)>     at Generator.forEach.t.<computed> [as next] (runtime.js:97)>     at Wm (runtime.js:728)>     at o (runtime.js:728)>","['It seems your reference to the binary file is incorrect - you have a 404. Please double check the notes in the codelab which explain how to change the model.json file so it works on Glitch CDN - my guess is that this is why you are getting a 404.Instructions I am referring to are here: https://developers.google.com/codelabs/tensorflowjs-comment-spam-detection#7If you are still having issues after that, please check difference between my working version which can be found here:https://glitch.com/edit/#!/comment-spam-detection-complete=====']",1
https://github.com/tensorflow/tfjs/issues/1611,Doc inconsistency: Latest docs still mention tfl.loadModel which does not exist in 1.1.2,1,closed,2019-05-25T08:56:06Z,2019-05-28T16:31:21Z,"Apologies if this is the wrong place to mention documentation problems.https://js.tensorflow.org/api/latest/#io.browserFiles shows the following code:```js// Note: This code snippet won't run properly without the actual file input//   elements in the HTML DOM.// Suppose there are two HTML file input (`<input type=""file"" ...>`)// elements.const uploadJSONInput = document.getElementById('upload-json'),const uploadWeightsInput = document.getElementById('upload-weights'),const model = await tfl.loadModel(tf.io.browserFiles(    [uploadJSONInput.files[0], uploadWeightsInput.files[0]])),```But `loadModel` no longer exists. I think it should be `loadLayersModel`.I could fix this if someone can point out where I can do it.","['Sent out fixing PR at https://github.com/tensorflow/tfjs-core/pull/1770In the latest API version (1.1.2), it should be `tf.loadLayersModel()`=====']",0
https://github.com/tensorflow/tfjs/issues/5550,Test cases missing for error detection in tfjs-layers (In many functions).,3,open,2021-08-29T05:20:00Z,2021-09-13T18:03:30Z,There are a lot of test cases missing for error detection in tfjs-layers. Some are already added as an issue but others are not yet reported. Are you willing to work on this: YesThese are my initials findings for missing error detection test-cases:- Check Input Data function- Model.Summary- Model.Compile- standardizeDataIteratorOutput,"['I have made tests for the following functions. If anyone can confirm that these are missing. I will make a PR for these. And If anyone wants to add more missing test cases.=====', ""I don't see much movement on this issue. I think this issue might not be correctly portrayed by me. So, If there is no movement on it till tomorrow, I will close it.====="", 'Hi @mattsoulanille can you please help with this request ?=====']",1
https://github.com/tensorflow/tfjs/issues/5156,How to get a spectrogram offline with the right shape as an input to recognize() in node .js?,0,open,2021-06-01T12:34:15Z,2021-06-03T16:15:40Z,"I am trying to perform offline recognition with this doc: https://github.com/tensorflow/tfjs-models/tree/master/speech-commandsI had the same issue as https://github.com/tensorflow/tfjs/issues/3820 described, and I had tried all solutions suggested from there, including the colab support https://colab.research.google.com/github/tensorflow/tfjs-models/blob/master/speech-commands/training/browser-fft/training_custom_audio_model_in_python.ipynb#scrollTo=1AjdTru5NnQQ but got an array with all NaN values. Is there a solution to this problem",[],0
https://github.com/tensorflow/tfjs/issues/2263,Cannot compute the outputs for dynamic ops (TensorArrayStack),13,closed,2019-10-24T00:17:37Z,2019-11-04T23:16:25Z,"### TensorFlow.js version1.2.11#### Browser versionGoogle Chrome Version 77.0.3865.120 (Official Build) (64-bit)#### Describe the problem or feature requestI have a graph model (built using `tf.contrib.seq2seq`) which uses control flow ops for dynamic RNN decoding. I exported it with TF 1.15 (using `simple_save`) and converted with TF.js 1.2.11 (command: `tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --saved_model_tags=serve`).When trying to run the model with `executeAsync`, I get the following error:```Uncaught (in promise) Error: Cannot compute the outputs [decoder/decode_sample/decoder_1/transpose_1] from the provided inputs [inputs,softmax_temperature]. Consider providing the following inputs: []. Alternatively, to avoid the dynamic ops, use model.execute() and specify the inputs [decoder/decode_sample/decoder_1/TensorArrayStack_1/TensorArrayGatherV3]    at t.<anonymous> (graph_executor.ts:318)    at callbacks.ts:253    at Object.next (callbacks.ts:253)    at o (callbacks.ts:253)```Does this mean TF.js currently cannot handle these ops? Is it the `TensorArrayStack`/`TensorArrayGatherV3` operation or something else?","['Hi there is a known bug with `executeAsync` specified here in #1793 , for it to work you may need to specify the output nodes please refer [here](https://github.com/tensorflow/tfjs/issues/2254#issuecomment-545755315). cc @pyu10055 =====', 'Hi, thanks for the response. I\'d seen that issue before and I\'m already specifying the output nodes like this:```model.executeAsync({\'inputs\': tf.zeros([1, 128, 16]), \'softmax_temperature\': tf.scalar(0.6)}, [""decoder/decode_sample/decoder_1/transpose_1""])```I can, for example, get the encoder outputs by passing ""encoder_rnn/rnn/transpose_1"", but there seems to be some issue with the decoder... I\'m guessing the graph may be broken by the conversion.Is there some way to get more information from the executor to see what exactly is preventing it from running? The error says `Consider providing the following inputs: []`, which doesn\'t help much...=====', '@cifkao Thank you for reporting the issue, this might related to our model analyzer having issue with TensorArray ops. Do you have the converted op that can be shared for us to reproduce?=====', '@pyu10055 I was able to get a complete example. I\'m attaching the [SavedModel protobuffer](https://github.com/tensorflow/tfjs/files/3783528/saved_model.pb.gz) and the [tfjs model](https://github.com/tensorflow/tfjs/files/3783529/model.json.gz) (converted using v1.3.0).I run the model with:```jsmodel.executeAsync({\'inputs\': tf.zeros([1, 16], \'int32\'), \'softmax_temperature\': tf.scalar(0.6)}, [""decoder/decode_sample/decoder/transpose_1""])```and get the following error:```Uncaught (in promise) Error: Cannot compute the outputs [decoder/decode_sample/decoder/transpose_1] from the provided inputs [inputs,softmax_temperature]. Consider providing the following inputs: []. Alternatively, to avoid the dynamic ops, use model.execute() and specify the inputs [decoder/decode_sample/decoder/TensorArrayStack_1/TensorArrayGatherV3]    at t.<anonymous> (graph_executor.ts:318)    at callbacks.ts:253    at Object.next (callbacks.ts:253)    at o (callbacks.ts:253)```So this is the op concerned:```python{\'name\': \'decoder/decode_sample/decoder/TensorArrayStack_1/TensorArrayGatherV3\', \'op\': \'TensorArrayGatherV3\', \'input\': [\'decoder/decode_sample/decoder/TensorArray_1\',  \'decoder/decode_sample/decoder/TensorArrayStack_1/range\',  \'decoder/decode_sample/decoder/while/Exit_3\'], \'attr\': {\'element_shape\': {\'shape\': {\'dim\': [{\'size\': \'-1\'}]}},  \'dtype\': {\'type\': \'DT_INT32\'}}}```=====', '@cifkao can you provide the whole directory of the saved_model, also the weights files for the tfjs converter model? Thanks.=====', ""@pyu10055 Sure, here's everything:* [export.tar.gz](https://github.com/tensorflow/tfjs/files/3785892/export.tar.gz)* [export_web.tar.gz](https://github.com/tensorflow/tfjs/files/3785894/export_web.tar.gz)====="", '@cifkao  I was able to run your model with the fix above.I got following result [122, 122, 122, 122, 122, 122, 122, 121, 122, 122, 122, 122, 122, 121, 122, 122, 122, 122, 121, 122, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122, 122, 121, 122, 122]Please let me know if it matches your python output, thanks.=====', '@pyu10055 Perfect!I got a different output, but I assume it\'s because the random generators are different. Can you try the deterministic output `""decoder/decode_greedy/decoder/transpose_1""`? That gave me:[[ 37  37 162  73 162  55  55  55  55  55  55  55  55  55  55  55  55  55   55  55  55  55  55  55  55  55  55  55  55  55  55  55  55  55  55  55   55  55  55  55  55  55  55  55  55  55  55  55  55  55]]=====', '(Updated the comment above, it should be greedy instead of sample.)=====', 'yes, that is the exact result I got[37, 37, 162, 73, 162, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55]length: 1__proto__: Array(0)=====', 'You have some Assert ops in your model, it would be ideal to avoid them, since it will slow down the model. Our converter automatically convert them to no-op by default, which is causing the issue you are having.=====', ""@pyu10055 Thanks! Is there an easy way to remove the asserts from the graph? (They're added by `tf.contrib.seq2seq`.)====="", '@cifkao in that case you can rely on our converter to do that, and the fix I had will fix your issue.=====']",0
https://github.com/tensorflow/tfjs/issues/5508,`RangeError: Maximum call stack size exceeded` executing specific model with a large input,1,closed,2021-08-19T17:58:01Z,2021-08-21T00:38:45Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): - Browser version: 3.8.0- Tensorflow.js Converter Version: 3.8.0**Describe the current behavior**In VS Code, it seems like we hit a `Maximum call stack size exceeded` error when the input into [the model we are using](https://github.com/microsoft/vscode-languagedetection/tree/07d8fc8b7a8e29a4a259664e1d58330fc45f034a/model) is too large. Here's the full stack trace:```ERR Maximum call stack size exceeded: RangeError: Maximum call stack size exceeded    at Xe (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:22892)    at Object.kernelFunc (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:105789)    at o (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:242459)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:243406    at y.scopedRun (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240964)    at y.runKernelFunc (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:243158)    at y.runKernel (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:241470)    at stringSplit_ (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:437551)    at Object.stringSplit__op [as stringSplit] (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:392714)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:212953    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:213212    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240843    at y.scopedRun (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240964)    at y.tidy (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240777)    at Module.f (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:256518)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:212600    at ar (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:215253)    at hr.processStack (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:224679)    at hr.executeWithControlFlow (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:223909)    at async hr._executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:222881)    at async Nt (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:187687)    at async Promise.all (index 0)    at async hr.executeWithControlFlow (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:223941)    at async hr._executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:222881)    at async fr.executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:230456)```from https://github.com/microsoft/vscode/issues/129597I ""fixed"" this by sending only 100000 characters to the model. That seems to be a good enough cut off... but I felt that I should report this anyway.Additionally something interesting is [our integration tests that run on [an input that is way smaller than 100000](https://github.com/microsoft/vscode/blob/main/extensions/vscode-api-tests/src/singlefolder-tests/untitled.languagedetection.test.ts#L27-L55) hit this error every so often and it's not clear why. Here's the stack trace from @bpasero:```Maximum call stack size exceeded: RangeError: Maximum call stack size exceeded    at Xe (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:22892)    at Object.kernelFunc (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:105789)    at o (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:242459)    at eval (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:243406)    at y.scopedRun (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:240964)    at y.runKernelFunc (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:243158)    at y.runKernel (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:241470)    at stringSplit_ (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:437551)    at Object.stringSplit__op [as stringSplit] (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:392714)    at eval (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:212953)```Note that we bundle tfjs into vscode-languagedetection now which is why the stack trace is different.**Describe the expected behavior**Not crash**Standalone code to reproduce the issue*** clone https://github.com/microsoft/vscode-languagedetection* edit [this line](https://github.com/microsoft/vscode-languagedetection/blob/main/test/index.test.ts#L65) to take in `{ maxContentSize: 1000000 }`* run the tests with `npm test`This should cause a test to fail and show the stack trace... let me know if it doesn't.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5508"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5508"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/750,tfjs-node installation error with anaconda3,2,closed,2018-10-01T19:36:48Z,2018-10-02T19:06:25Z,"#### Describe the problem or feature requestI'm getting this error when I'm trying to install '@tensorflow/tfjs-node'. The wierd thing is this line: `Command failed: /Users/jonas/anaconda3/bin/python -c import sys, print ""%s.%s.%s"" % sys.version_info[:3],`.What does anaconda3 to do with this? I think I started to get this error only once I installed anaconda.Any ideas?/Users/jonas/Projects/ML/tfjs-test/ng-tf-handwriting/node_modules/@tensorflow/tfjs-node/scripts/install.js:154      throw new Error('node-gyp rebuild failed with: ' + err),      ^Error: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuildgyp ERR! configure errorgyp ERR! stack Error: Command failed: /Users/jonas/anaconda3/bin/python -c import sys, print ""%s.%s.%s"" % sys.version_info[:3],gyp ERR! stack   File ""<string>"", line 1gyp ERR! stack     import sys, print ""%s.%s.%s"" % sys.version_info[:3],gyp ERR! stack                                ^gyp ERR! stack SyntaxError: invalid syntaxgyp ERR! stackgyp ERR! stack     at ChildProcess.exithandler (child_process.js:275:12)gyp ERR! stack     at emitTwo (events.js:126:13)gyp ERR! stack     at ChildProcess.emit (events.js:214:7)gyp ERR! stack     at maybeClose (internal/child_process.js:925:16)gyp ERR! stack     at Socket.stream.socket.on (internal/child_process.js:346:11)gyp ERR! stack     at emitOne (events.js:116:13)gyp ERR! stack     at Socket.emit (events.js:211:7)gyp ERR! stack     at Pipe._handle.close [as _onclose] (net.js:557:12)gyp ERR! System Darwin 17.5.0gyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild""gyp ERR! cwd /Users/jonas/Projects/ML/tfjs-test/ng-tf-handwriting/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v8.11.2gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok    at cp.exec (/Users/jonas/Projects/ML/tfjs-test/ng-tf-handwriting/node_modules/@tensorflow/tfjs-node/scripts/install.js:154:13)    at ChildProcess.exithandler (child_process.js:282:5)    at emitTwo (events.js:126:13)    at ChildProcess.emit (events.js:214:7)    at maybeClose (internal/child_process.js:925:16)    at Socket.stream.socket.on (internal/child_process.js:346:11)    at emitOne (events.js:116:13)    at Socket.emit (events.js:211:7)    at Pipe._handle.close [as _onclose] (net.js:557:12)npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@0.1.17 install: `node scripts/install.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node@0.1.17 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /Users/jonas/.npm/_logs/2018-10-01T18_34_11_588Z-debug.log","['Hi there, I had this one:node-gyp does not support python 3.x, try with python 2.7=====', 'Thank you @xcapt, seems to work not!=====']",0
https://github.com/tensorflow/tfjs/issues/4591,What information is contained in the part_offsets tensor output?,3,open,2021-01-23T01:42:43Z,2021-01-30T20:03:31Z,"I'm trying to finetune the bodypix model and I cannot for the life of me figure out how to generate the targets for the part_offsets output. Based on its shape ([H,W,48]) I presume it is a pair of numbers for each of the 24 body parts. Furthering that assumption, and the name, an X/Y offset? But it's clamped to 0->1. And body parts are regions, not points, so an offset for a bodypart makes no sense to me. Can you clear this up a little for me?","['@tylerzhu-github Hi Tyler can you help to comment on this question? Thanks.=====', ""Hi @jtrammell-dla,Thanks for your inquiry!That's a very good question. The output node is reserved for future use cases.The best way is to not set any target for that head and ignore it in the total loss.Best,Tyler====="", ""Ah, so!!! That explains it, thank you! Now I'm curious, though. Can you tell us what sort of future use cases you had in mind? And thank you for the quick response.=====""]",0
https://github.com/tensorflow/tfjs/issues/5050,Switch new Buffer to Buffer.from,1,closed,2021-05-07T20:56:30Z,2021-05-10T20:15:08Z,https://github.com/tensorflow/tfjs/blob/e18e0984ab494c18d83b08b7b8bff0fdac79af05/scripts/cloud_funcs/send_email/index.js#L32Refer:- https://nodesource.com/blog/understanding-the-buffer-deprecation-in-node-js-10/- https://docs.guardrails.io/docs/en/vulnerabilities/javascript/insecure_use_of_language_framework_api.html,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5050"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5050"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1178,Conv layer documentation missing required configuraitons,12,open,2019-01-31T16:30:48Z,2021-09-03T17:58:58Z,"#### TensorFlow.js version0.14.0#### Describe the problem or feature request![image](https://user-images.githubusercontent.com/547150/52068878-8d9a8600-254b-11e9-863f-a0178ea8b765.png)Conv layer requires kernelSize, accpets padding, dilation, etc.","['@tafsiri is there some annotation that will allow for the inheretied Conv parameters to thread through to the website?=====', ""@bileschi by inherited conv params do you mean the [BaseConvLayerArgs](https://github.com/tensorflow/tfjs-layers/blob/d90401eebdb28a66802c1fd7566b9a0002d2eecc/src/layers/convolutional.ts#L202). I don't know if there is a way to do this in the current parse but maybe Nikhil knows.@nsthorat [Conv2d](https://github.com/tensorflow/tfjs-layers/blob/d90401eebdb28a66802c1fd7566b9a0002d2eecc/src/layers/convolutional.ts#L558) takes a [ConvLayerArgs](https://github.com/tensorflow/tfjs-layers/blob/d90401eebdb28a66802c1fd7566b9a0002d2eecc/src/layers/convolutional.ts#L298) but most of its parameters are in the interface ConvLayerArgs extends. Do you know if the doc generator can traverse these inheritance chains? BaseLayerArgs also extends LayerArgs so it would be at least a couple of hops to get them all.====="", ""No currently we can't walk inheritance chains (this would require a bunch of non-trivial work to walk a tree which we don't do right now).Can you just copy the interface? I don't think that's unreasonable -- I doubt your params are going to change much.====="", ""I'm not sure I understand I understand the proposal.Copy which interface to where?====="", ""Sorry I just mean have a separate interface for DepthwiseConv and Conv2D that don't use inheritance.====="", 'Two notes on this proposal:1. It encourages duplication in the codebase in service of documentation, and discourages the existing hierarchical construction of related interfaces.2. There are many more extended interfaces in the Layers codebase that are similarly affected.  In order to cover these cases would we similarly need to duplicate instead of inherit all surfaced interfaces?Is it possible instead to manually override the documentation in these cases?Is there some treatment that could be provided at the call site that makes the documented inheritence chain explicit?E.g. For the JSDoc for Conv1D here:https://github.com/tensorflow/tfjs-layers/blob/d90401eebdb28a66802c1fd7566b9a0002d2eecc/src/layers/convolutional.ts#L1019Could we add something like```@ConstructorParam args type ConvLayerArgs extends BaseConvLayerArgs extends LayerArgs```Where would explicitly include the fields defined in the listed types?  (We might or might not, for instance, want to include LayerArgs here)=====', ""While you are right on 1., I don't really think inheritance in this case is very useful. Personally, I find inheritance of user-facing interfaces / classes confusing (they have to walk the inheritance tree and figure out the union of the arguments themselves as opposed to a single hermetic interface that defines exactly API. It's unlikely this API is going to change anyways).Just curious, what are the other interfaces that have this problem?Anyways, this is doable but non-trivial, I don't really have cycles to take this on right now so I would recommend duplicating the type if you want it fixed soon (or if you want to jump into the doc generate codebase you're more than welcome, though it's definitely not going to be an easy change).====="", ""A little grepping around suggests there are approximately 55 such deeply inherited interfaces.```find ./src -iname '*.ts' | grep -v test | xargs grep interface | grep extends | grep -v 'extends LayerArgs' | awk '{print $3}'``````BidirectionalLayerArgsRNNLayerArgsSimpleRNNLayerArgsGRUCellLayerArgsGRULayerArgsLSTMCellLayerArgsLSTMLayerArgsDepthwiseConv2DLayerArgsConvLayerArgsSeparableConvLayerArgsOrthogonalArgsZeroPadding2DLayerConfigDropoutLayerConfigDenseLayerConfigActivationLayerConfigRepeatVectorLayerConfigReshapeLayerConfigPermuteLayerConfigBatchNormalizationLayerConfigEmbeddingLayerConfigConcatenateLayerConfigDotLayerConfigDepthwiseConv2DLayerConfigReLULayerConfigLeakyReLULayerConfigPReLULayerConfigELULayerConfigThresholdedReLULayerConfigSoftmaxLayerConfigBaseRNNLayerConfigSimpleRNNCellConfigSimpleRNNLayerConfigGRUCellConfigGRULayerConfigLSTMCellConfigLSTMLayerConfigStackedRNNCellsConfigBaseConvLayerConfigConvLayerConfigSeparableConvLayerConfigCropping2DLayerConfigUpSampling2DLayerConfigTimeDistributedLayerConfigBidirectionalLayerConfigPooling1DLayerConfigPooling2DLayerConfigGlobalPooling2DLayerConfigTrainingConfigNodeArgsModelSerializationSequentialSerializationLayerConfig```====="", ""If we get rid of the ones from the Keras spec typesystem there are even fewer```$ find ./src -iname '*.ts' | grep -v test | xargs grep interface | grep extends | grep -v 'extends LayerArgs' | grep -v 'keras_format' | awk '{print $3}'``````BidirectionalLayerArgsRNNLayerArgsSimpleRNNLayerArgsGRUCellLayerArgsGRULayerArgsLSTMCellLayerArgsLSTMLayerArgsDepthwiseConv2DLayerArgsConvLayerArgsSeparableConvLayerArgsOrthogonalArgs````====="", 'Okay thanks -- are there any that are multiple levels of inheritance (beyond just extending a single base)?=====', 'Looks like we have to make the change in the doc generator for this to actually be possible :)=====', 'Yeah, looks like, e.g.,SeparableConvLayerArgsextends ConvLayerArgsextends BaseConvLayerArgsextends LayerArgsThis might be the deepest one?On Fri, Feb 1, 2019 at 3:07 PM Nikhil Thorat <notifications@github.com>wrote:> Looks like we have to make the change in the doc generator for this to> actually be possible :)>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/1178#issuecomment-459851531>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AAhZTu1fThJHoSRMXzT5-RAe26glDEltks5vJJ6SgaJpZM4acrTB>> .>-- Stan Bileschi Ph.D.  |   SWE  |  bileschi@google.com |  617-230-8081=====']",0
https://github.com/tensorflow/tfjs/issues/231,Distribution Normal from Tensors,4,closed,2018-04-25T16:38:18Z,2018-07-30T22:46:12Z,"#### TensorFlow.js versionAny#### Browser versionAny#### Feature requestThe only function who refer to a normal distribution is tf.randomNormal, the two parameters: mean and stdDev must be number and not tensor thereby the resulting tensor is not differentiable... Moreover, we can create only a set of values from only one pair of mean and variance instead of using multiple pairs.It would be great to accept tensor too as parameters...#### Code from Python tensorflowUsing tensorflow in python it is possible to create a normal distribution from two tensors (mean and stdDev)Example (with eager execution):```nr = tf.distributions.Normal([0.5, 2.0], [0.5, 0.5])nr._sample_n(1)#<tf.Tensor: id=300, shape=(1, 2), dtype=float32, numpy=array([[ 0.52700466,  2.44701052]], dtype=float32)>```","['I would be ok to implement it=====', 'That would be great. Thank you!=====', 'Hi @dsmilkov  I have a working code! Which branch should I use to make my pull request?=====', '@thibo73800 You should make a PR to master branch.=====']",0
https://github.com/tensorflow/tfjs/issues/4378,GPU memory leak of tf.signal.stft,8,closed,2020-12-09T06:37:36Z,2021-01-21T04:03:56Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0""> </script>- TensorFlow.js version (use command below): 2.7.0- Browser version: Chrome 87.0.4280.88- Tensorflow.js Converter Version: N/A**Describe the current behavior**There is GPU memory leak after use tf.signal.stft. If use it continuously, it leads to crash the Chrome GPU process.  **Describe the expected behavior**There is no GPU memory leak after use tf.signal.stft.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.```jsfunction hanningWindow(N) {  return tf.tidy(() => {    const window = new Float32Array(N),    for (let i = 0, i < N - 1, ++i) {      window[i] = 0.5*(1 - Math.cos(6.283185307179586*i/(N-1))),    }    return tf.sqrt(window),  }),}console.log(tf.engine().memory()),// e.g. {unreliable: false, numBytesInGPU: 0, numBytesInGPUAllocated: 0, numBytesInGPUFree: 0, numTensors: 0, …}const N = 100, // Reproduce the GPU process crash by setting large number, e.g. 10000 for (let i = 0, i < N, ++i) {  const win = 320,  const fft = 320,  const hop = 160,  const input = tf.zeros([1760]),  const output = tf.signal.stft(input, win, hop, fft, hanningWindow),  input.dispose(),  output.dispose(),}console.log(tf.engine().memory()),// e.g. {unreliable: false, numBytesInGPU: 2560000, numBytesInGPUAllocated: 2694080, numBytesInGPUFree: 134080, numTensors: 0, …}```The GPU memory leak can be reproduced by using WebGL backend. The CPU backend doesn't have this issue.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","['@huningxin thanks for reporting, the intermediate tensors seem to have be disposed, but the bytesInGPU has increased, @annxingyuan can you confirm this is normal? Is the numBytesInGPUAllocated counting the texture size?=====', ""Thanks @pyu10055 for your comments. If you'd like to reproduce the Chrome GPU process crash, you can simply set the `N` to a large number, say 100000, in above [sample code](https://github.com/tensorflow/tfjs/issues/4378#issue-760048997).====="", '@huningxin I notice that the `hanningWindow` output is tight to the size `N`, which is also not disposed after stft call.The other thing you can choose to do to set the texture dispose threshold flag `WEBGL_DELETE_TEXTURE_THRESHOLD`, here is modified code:```javascript    function hanningWindow(N) {      const window = new Float32Array(N),      for (let i = 0, i < N - 1, ++i) {        window[i] = 0.5 * (1 - Math.cos(6.283185307179586 * i / (N - 1))),      }      return tf.sqrt(window),    }    console.log(tf.engine().memory()),    // e.g. {unreliable: false, numBytesInGPU: 0, numBytesInGPUAllocated: 0, numBytesInGPUFree: 0, numTensors: 0, …}    // aggressive gpu texture disposal starts at 100M    tf.env().set(""WEBGL_DELETE_TEXTURE_THRESHOLD"", 100 * 1024 * 1024),    const N = 10000, // Reproduce the GPU process crash by setting large number, e.g. 10000     for (let i = 0, i < N, ++i) {      const win = 320,      const fft = 320,      const hop = 160,      const input = tf.zeros([1760]),      //const ident = (length) => tf.ones([length]).as1D(),      const output = tf.tidy(() => {        return tf.signal.stft(input, win, hop, fft, hanningWindow),      })      input.dispose(),      output.dispose(),    }    console.log(tf.engine().memory()),```=====', ""Thanks for your reply, @pyu10055 !> @huningxin I notice that the `hanningWindow` output is tight to the size `N`, which is also not disposed after stft call.I understand the output of `hanningWindow` is used by `tf.signal.stft` internally. I suppose it would be `tf.singla.stft`'s responsibility to dispose it. Is that correct?> The other thing you can choose to do to set the texture dispose threshold flag `WEBGL_DELETE_TEXTURE_THRESHOLD`, here is modified code:I tried the modified code. The `numBytesInGPU` can still go higher than `WEBGL_DELETE_TEXTURE_THRESHOLD`, 100 * 1024 * 1024. e.g. after run the loop several times, the output of `console.log(tf.engine().memory()),` is:```{unreliable: false, numBytesInGPU: 130560000, numBytesInGPUAllocated: 130560000, numBytesInGPUFree: 0, numTensors: 0,\xa0…}numBytes: 0numBytesInGPU: 130560000numBytesInGPUAllocated: 130560000numBytesInGPUFree: 0numDataBuffers: 0numTensors: 0unreliable: false__proto__: Object```====="", '@huningxin I think you are right that, the op is wrapped in the `ENGINE.startScope` and `endScope`, intermediate tensor should have been disposed, which is also verified through the tensor count from the `memory()`result.I curious about the size exceeding the limit as well, possibly there are some large intermediate tensor get allocated, which cause the memory to exceed the limit, maybe the disposal of texture is done before the tensor creation. @annxingyuan =====', ""I'm investigating. This is the first bad commit: https://github.com/tensorflow/tfjs/commit/c537c813ba8234a909d1e0849abfd95998a8fb5b====="", 'I verified with tf.js 2.8.5 release that the memory leak issue has been fixed. Thanks much @pyu10055 and @annxingyuan !=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/727,iOS 12 is producing NaNs for all ops.,4,closed,2018-09-24T14:09:59Z,2018-10-26T12:11:09Z,"When running snippets in the API docs, I'm getting all NaNs for ops that happen on the GPU.","[""Just to clarify this probably isn't just happening in the docs? But rather is general to iOS 12? ====="", ""Good question, I don't actually know if it's just the docs, I only tried it in docs + iOS 12.====="", 'running on IOS over webview? its that possible?=====', 'This was resolved with https://github.com/tensorflow/tfjs-core/pull/1346=====']",0
https://github.com/tensorflow/tfjs/issues/5566,Latest released @tensorflow/tfjs-core 3.9.0 missed some files.,4,closed,2021-09-01T06:09:26Z,2021-09-02T02:52:36Z,Current @tensorflow/tfjs-core 3.9.0 only have following ```shnode_modules/@tensorflow/tfjs-core$ lsdist  package.json  README.md```while @tensorflow/tfjs-core 3.8.0 had```shnode_modules/@tensorflow/tfjs-core$ lsBUILD.bazel  cloudbuild.yml  development  dist  package.json  README.md  scripts  setup_test_bazel.ts  src  test.html  tsconfig.test.json```,"['Link https://github.com/webmachinelearning/webnn-polyfill/issues/114=====', ""Thanks for the bug report! This is actually intentional as part of the Bazel build migration. We omitted TS source files (along with other files) in the most recent release because they are not used by downstream packages (although it seems like your package is an exception).In the issue you linked, I noticed you're using `@tensorflow/tfjs-core/src/ops/conv_util`. Could you instead import `@tensorflow/tfjs-core/dist/ops/conv_util`, or does this not work for your use case?====="", 'Thanks much @mattsoulanille. It works for my case. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5566"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5566"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4830,"Error dumping weights, duplicate weight name block1_conv1/kernel",2,closed,2021-03-18T03:15:10Z,2021-03-20T17:36:48Z,"```import tensorflowjs as tfjstfjs.converters.save_keras_model(Xception_unet, 'connt')#!tensorflowjs_converter --input_format=keras /content/a.h5 /content/```Tried both the ways to convert my keras model into tfjs model. but ended up with the error:**_Error dumping weights, duplicate weight name block1_conv1/kernel_**![Capture](https://user-images.githubusercontent.com/48760936/111568047-08739900-87c6-11eb-9f95-461f1b38f20e.PNG)ANY IDEA HOW TO RESOLVE THIS ISSUE!!?For questions on how to work with TensorFlow.js, or support for problems that are not verified bugs in TensorFlow.js, please go to [StackOverflow](https://stackoverflow.com/tags/tensorflow.js).","['In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!=====', 'I resolved this issue by changing the names of the layers. =====']",0
https://github.com/tensorflow/tfjs/issues/4778,Error when load model using tf.loadLayersModel,10,open,2021-03-04T18:29:06Z,2021-03-26T16:51:33Z,"when run the code `(async function () {  console.log(""wating for model load!""),  const model = await tf.loadLayersModel(""simple/model.json""),  console.log(""model: "", model),})(),`**I got error**Uncaught (in promise) Error: Input 0 is incompatible with layer flatten_3: expected min_ndim=3, found ndim=2.    at new e (errors.ts:48)    at e.assertInputCompatibility (topology.ts:790)    at topology.ts:985    at rh (common.ts:43)    at e.apply (topology.ts:978)    at e.add (models.ts:500)    at e.fromConfig (models.ts:948)    at Rp (generic_utils.ts:277)    at cd (serialization.ts:31)    at models.ts:300I use script tag verson 1.0.0 OR 2.0.0 same error `<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js""></script>`How can fix it ??","['Please use latest version 3.2.0 and let us know , if that does not help , please share reproducible code in codepen or in a GitHub repo. Thank you=====', 'this is my code on github https://github.com/AmmarYasir29/tensorflowJSI try the last verson that you tell me ans the same error: > Uncaught (in promise) Error: Input 0 is incompatible with layer flatten_3: expected min_ndim=3, found ndim=2.>     at t.n.assertInputCompatibility (topology.js:790)=====', '@rthadur any help ??=====', 'i guess the model file is missing , is it fine to share the model also , thank you =====', 'I convert model from .h5 format to .json to loadembedded it to web by`console.log(""wating form model load!""),``const model = await tf.loadLayersModel(""http://localhost:3000/fjs/model.json""),`But the error appear [IMG Error](https://ibb.co/QPP5GGz) and I cant fix that The mdoel in h5 format [the model h5](https://github.com/missdafer/the_Simpsons_classification_project/blob/acb254dd3fbaa4a4c24d7ad00bcf2d5080687530/ResNet50_model.h5)and the [json format ](https://drive.google.com/drive/folders/1RVCzN9Ycx5hE6JiYb0bZaLaq3aStsQ8V?usp=sharing)=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@rthadur the error still appear and the model not load Is there solution ?  =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@lina128 can you help me ? =====', ""if you are loading the model from local folder you need to use below code `model = await tf. loadLayersModel('model/model.json'),`=====""]",1
https://github.com/tensorflow/tfjs/issues/563,Add timing information to tf.profile(),0,closed,2018-07-31T15:39:13Z,2021-03-12T18:53:58Z,"A `tf.profile()` would take a function, similar to `tf.time()` but would give rich information about the function being profiled, like memory information (peak, average, new allocations), timing information (total kernel time), and possibly a list of kernels.For the first pass, let's just compute memory information.For example:```const {profile, result} = tf.profile(() => {  const x = tf.tensor1d([1, 2, 3]),  let x2 = x.square(),  x2.dispose(),  x2 = x.square(),  x2.dispose(),  return x,}),```Then, `profile` would be an object looking like:```{  memory: {    peakBytes: 24,    averageBytes: 44  // (12 + 24 + 24) / 3, after each kernel is run    newTensors: 1,    newTensorsBytes: 12  },  timing: {    totalKernelTimeMs: 100  },  kernels: [    {name: square, kernelTimeMs: 10, inputShapes: [[3]], outputShape: [3]},    {name: square, kernelTimeMs: 12, inputShapes: [[3]], outputShape: [3]},  ],  // Possibly later, as a hierarchy at the op level (via the named scopes).  operations: [    {name: square, kernelTimeMs: 10, inputShapes: [[3]], outputShape: [3]},    {name: square, kernelTimeMs: 12, inputShapes: [[3]], outputShape: [3]},  ]}```",[],0
https://github.com/tensorflow/tfjs/issues/4406,Model is not predicting well after loading from localStorage,1,closed,2020-12-15T09:44:07Z,2020-12-18T14:28:20Z,"I have build model with the following structure in **Node JS** using **TFJS@1.7.0**``` let model = tf.sequential({      layers: [        tf.layers.dense({inputShape: [34], units: 16, activation: 'relu'}),        tf.layers.dense({units: 3, activation: 'softmax'}),      ]    }),    model.compile({      optimizer: tf.train.adam(0.02),      loss: 'categoricalCrossentropy',      metrics: ['accuracy']    }),```Model is giving >90% accuracy. Model has been tested and it is predicting well according to it's accuracy.I saved model in localStorage with **model.save(""localstorage://model"")**.But when i load model with **model.loadLayersModel(""localstorage://model"")** in different page,The performance of model has got down to 50%, it is predicting like **non-trained models**.Any wrong in saving or loading model.","['Model saving and Loading only works in a single page. If moved from one page to another within the same domain, the performance of model decreases to 50% if load from localStorage.=====']",1
https://github.com/tensorflow/tfjs/issues/4775,Tfjs error with macOS 11 on m1 chip,1,closed,2021-03-03T18:27:20Z,2021-03-03T19:19:54Z,Trying to run basic get started example with tfjs in max m1 chip and getting illegal argument : 4 error. Is there anything specific need to be done for Mac m1 chip .,"['There is already issue opened here for M1 Chip , currently tfjs does not work well with M1 , please check back here https://github.com/tensorflow/tfjs/issues/4775=====']",1
https://github.com/tensorflow/tfjs/issues/4327,[Codelab]: Making Predictions from 2D Data,1,closed,2020-12-01T14:42:23Z,2020-12-03T17:35:55Z,"Found inconsistencyhttps://www.tensorflow.org/js/tutorials/setup - script uses tfjs@2.0.0https://codelabs.developers.google.com/codelabs/tfjs-training-regression/index.html#1 - script uses tfjs@1.0.0Also, I see ""55 mins remaining"" on top. Can I not be rushed? :(","['It is fixed now, thanks for reporting.=====']",1
https://github.com/tensorflow/tfjs/issues/1120,tfjs-node-gpu one tensor not getting disposed after executeAsync,3,closed,2019-01-23T14:31:13Z,2019-02-11T07:06:05Z,"#### TensorFlow.js versiontensorflow/tfjs-node-gpu: 0.1.21#### Browser versionelectron: 3.1.0#### Describe the problem or feature requestI am running object detection on a set of images, 10000 in this example.Everything get's really really slow and eventually crashes after ~4000 predictions.After each prediction in the end I call `console.log(tf.memory()),` and it seems that one of the Tensors is not disposed.![leak](https://user-images.githubusercontent.com/1787775/51612790-d7bbb000-1f2a-11e9-89ed-73b833d292e5.png)I can see the orphan tensors in `tf.ENV.engine.tensorInfo` but since it's a WeakMap not sure what would be the best way to handle it.![leak2](https://user-images.githubusercontent.com/1787775/51613195-a68faf80-1f2b-11e9-856e-081ac56593d9.png)Disposing the model does not dispose of this tensors.Here is my weights_manifest.json: https://github.com/tensorflow/tfjs/files/2722928/model.zip#### Code to reproduce the bug / link to feature request```const model = await tf.loadFrozenModel(modelPath, weightsPath),// Runs in a loopconst shape = [1, 2560, 1920, 3],const imageTensor = tf.fill(shape, 0, 'int32'),const predictions = await model.executeAsync(  { image_tensor: imageTensor },  ['detection_boxes', 'detection_scores', 'detection_classes', 'num_detections'],),imageTensor.dispose(),predictions.forEach(prediction => prediction.dispose()),console.log(tf.memory()),```","['The leak is in the `faster_rcnn_resnet101_coco_2018_01_28` model, if I use `ssd_mobilenet_v2_coco_2018_03_29` the number of tensors is constant after each prediction.http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gzthen I used this command to convert to a web_model:`tensorflowjs_converter --input_format=tf_saved_model --output_node_names=""detection_boxes,detection_scores,num_detections,detection_classes"" --saved_model_tags=serve model/faster_rcnn_resnet101_coco_2018_01_28/saved_model model/faster_rcnn_resnet101_coco_2018_01_28/web_model`Is there any way to find out what weight causes the orphan tensor?All I know is:""shape"": [2],""dtype"": ""int32""and that each prediction I leak 8 bytes.There are plenty of weights in faster_rcnn that are not in ssd_mobilenet like `CropAndResize/crop_size` that match the signature.Any way to find out what weight leaks the tensor?=====', 'rfcn_resnet101_coco_2018_01_28 leaks ever more, 18 tensors per prediction are not garbage collected.![03](https://user-images.githubusercontent.com/1787775/51862106-8bb3a580-2346-11e9-9b5e-724d13fca76b.png)I have tried using `tfjs-node` instead of `tfjs-node-gpu`, same amount of tensors leaked.=====', 'Looks like this is a duplicate of #1186. Fixed in 0.3.0.=====']",0
https://github.com/tensorflow/tfjs/issues/5466,tensorflowjs_converter does not work for Tensorflow Object Detection API faster-rcnn model,9,closed,2021-08-11T12:30:39Z,2021-08-26T15:59:08Z,I trained the **faster_rcnn_resnet50_v1_640x640_coco17_tpu-8 model** using custom dataset using my laptop and exported the model into .pb format as shown below.![image](https://user-images.githubusercontent.com/28196102/129028169-1e563618-abe0-43cb-8651-5517ddfa9fd9.png)Then I tried using the 'tensorflowjs_converter' to convert it into .json and bin file. This method works for SSD MobileNet and EfficientDet model but an error pops up whenever I try to convert the faster rcnn .pb model.This is the **error**:![image](https://user-images.githubusercontent.com/28196102/129028666-558462f7-ca05-4240-95ee-4c8f68b2851d.png)Please let me know how can I solve this problem?Thank you in advance.,"['As the error message suggests op `broadcast` is not implemented in webgl and wasm , this would be a feature request. Did you try loading in cpu ?=====', 'related feature request in wasm https://github.com/tensorflow/tfjs/issues/3580=====', 'No, I haven’t try loading it in CPU, may I know what is the issue with loading it in GPU? How should I proceed? Also, I am not sure how to load it in CPU …=====', 'We have a PR that is adding the missing BroadcastArgs op.ref https://github.com/tensorflow/tfjs/pull/4636=====', 'Thank you for replying, may I know if the BroadcastArgs op has been added already? If it has already been added, does that mean I need to uninstall tensorflowjs and reinstall it again?=====', 'Not yet, likely will be in the next release. thanks=====', 'Alrighty, thanks again. Looking forward to the upgraded version.=====', 'Closing this issue as the related PR has been merged , as mentioned above changes will be available in next release. Thank you.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5466"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5466"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5277,Build tfjs-backend-webgl with Bazel,1,closed,2021-07-01T20:31:53Z,2021-09-29T16:57:26Z,"This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package, take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275",['Closed by #5562 ====='],1
https://github.com/tensorflow/tfjs/issues/5520,[tfjs-react-native] Image classification of Livestream in real-time,4,open,2021-08-23T09:41:42Z,2021-08-26T19:58:18Z,"**System information**- TensorFlow.js version: 3.8.0- tfjs-react-native version: 0.6.0- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**I wonder if there is a way to analyze the Livestream in real-time. For example, I've got a task to classify a mini object, such that the mobile's camera is not capable of doing that. In this case, I have an IP camera, which could return a Livestream url for the app. However, currently, there is only the `decodeJpeg` method, which runs statically, not dynamically. Actually, I'm trying to snapshot on the Livestream frame and recursively calling `decodeJpeg` method, however, this is weird and I think the performance interacting with react-native is not good. **Will this change the current API? How?**Yes, and I think it needs two steps:1. Allowing dynamic video streams to be the input (e.g. a Livestream url) of a model (which possibly will be a new API).2. Allowing a real-time callback function for result display.**Who will benefit from this feature?**For users that need to use other cameras instead of mobile native cameras, because not all types of objects or images taken by phones are suitable for particular cases, thanks a lot!","['You can use tf.data.webcam https://js.tensorflow.org/api/latest/#data.webcam to live stream , please check =====', ""> You can use tf.data.webcam https://js.tensorflow.org/api/latest/#data.webcam to live stream , please checkThanks a lot! This is cool, however, I notice that it is only working for webcams (usually link to PC by USB), instead of ip cameras (directly access live streams from a URL under the same network). I'm not quite sure if I could just input the live stream URL (RTSP/HTTP protocol) to `tf.data.webcam`, it seems that the method only accepts input of a DOM element such as `document.createElement('video')`.====="", 'I think this is really close to the use of `cameraWithTensors`:https://js.tensorflow.org/api_react_native/0.6.0/#cameraWithTensorsIf the `onReady` function could be used for a Livestream URL, everything will be perfect. 🤔=====', ""Under the hood, `tf.data.webcam` still [takes individual frames from the camera stream](https://github.com/tensorflow/tfjs/blob/master/tfjs-data/src/iterators/webcam_iterator.ts#L154) and transforms them into tensors. cameraWithTensors [does the same thing](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L278-L283).I'm not sure how you're displaying the livestream on the app, but if you're rendering it in a WebGL2 context and know which texture corresponds to the video, you might be able to use [fromTexture](https://js.tensorflow.org/api_react_native/0.6.0/#fromTexture) to get a tensor representing a single frame of the video. Then, you can run that through your model. The way cameraWithTensors works is it [creates and expo-gl GLView](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L218) and [renders each frame to it](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L314). When the user requests the current frame as a tensor (or if it's set to autoRender), it converts the [WebGL texture into a tensor](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L278-L284). You might be able to do something similar with your video feed. Ideally, we would have a method similar to [tf.browser.fromPixels](https://js.tensorflow.org/api/3.8.0/#browser.fromPixels) that works on React Native components (probably implemented as a higher-order component like [cameraWithTensors](https://js.tensorflow.org/api_react_native/0.6.0/#cameraWithTensors)), but implementing that in React Native is likely a lot more complex than in the browser.=====""]",1
https://github.com/tensorflow/tfjs/issues/5664,Error: Requested texture size [-1536x512] is invalid,2,open,2021-09-25T20:24:22Z,2021-09-27T15:29:27Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows - TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 3.9.0- Browser version: Chrome 94.0.4606.54- Tensorflow.js Converter Version:  Most recent**Describe the current behavior**I am running two object detection models. The first locates where a sudoku board is in an image, and the second model locates where each grid is on the sudoku board. The first model works fine. However, when I call executeAsync to the model to find where each grid is on sliced tensor (This comes from the predictions made from the first model) I get an error. So the first object detection models works, but upon calling the second object detection model I get the error :tf.min.js:17 Uncaught (in promise) Error: Requested texture size [-1536x512] is invalid.    at tf.min.js:17    at jX (tf.min.js:17)    at tf.min.js:17    at e.t.createFloat32MatrixTexture (tf.min.js:17)    at e.t.acquireTexture (tf.min.js:17)    at t.n.acquireTexture (tf.min.js:17)    at t.n.uploadToGPU (tf.min.js:17)    at t.n.runWebGLProgram (tf.min.js:17)    at Object.kernelFunc (tf.min.js:17)    at n (tf.min.js:17)**Describe the expected behavior**It is should work exactly how the first model works. Its weird because the first model is able to make predictions but for the second model it fails.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/CodePen/any notebook.--This is the image I was loading to the html input![test_img](https://user-images.githubusercontent.com/20917134/134791114-2461b4bf-0692-41fe-afae-be9d12b858c3.png)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Code:HTML:```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>TF JS example</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.js""></script></head><body><form method=""POST"" action=""/upload"" enctype=""multipart/form-data"">  <p><input type=""file"" id=""file1"" name=""file""></p></form><canvas style=""border: 10px solid black "" id=""canvas""  >  <img id =""display"" width = ""...""  height=""..."" ></canvas><script src=""main.js""></script>    </body></html>```main.js:`````async function load() {  const model = await tf.loadGraphModel('https://big-g.s3.us-east.cloud-object-storage.appdomain.cloud/model.json'),    return model,  },async function load_s(){  const model_s = await tf.loadGraphModel('https://small-g.s3.us-east.cloud-object-storage.appdomain.cloud/model.json'),  return model_s}async function load_class(){  const model_class = await tf.loadGraphModel(""https://classy.s3.us-east.cloud-object-storage.appdomain.cloud/model.json""),  return model_class}var model = load(),var model_s = load_s(),//var model_class =load_class(),console.log(""laoded"")document.querySelector('input').addEventListener('change', function() {  console.log(""change"")  var reader = new FileReader(),  const fileInput = document.getElementById(""file1"").files[0],  console.log(fileInput),  readerDim = new FileReader(),  readerDim.readAsDataURL(fileInput),  readerDim.onload = function (e) {    //Initiate the JavaScript Image object.    var image = new Image(),    //Set the Base64 string return from FileReader as source.    image.src = e.target.result,    //Validate the File Height and Width.    image.onload = function () {      var height = this.height,      var width = this.width,      console.log(width,height),//width,height        var a = tf.browser.fromPixels(image),      //convert image into 3 channel grayscale      var g_scale =a.mean(2).expandDims(-1),      var a = tf.image.grayscaleToRGB(g_scale).cast(""int32""),      a=a.expandDims(),      model.then(model => {            async function pred() {        console.log('Start'),        const result = await model.executeAsync(a),        console.log('End')        //console.log( await result[6].array()),        //const classes = await result[2].array(),        //const accuracy = await result[4].array(),        const boxes = await result[6].array(), //y,x,height,width                //Time to draw         const canvas = document.querySelector('#canvas'),        const ctx = canvas.getContext('2d'),        //resize        canvas.height = image.height,        canvas.width= image.width,         // ctx.drawImage(image,0,0),       tf.browser.toPixels(a.squeeze(),canvas),        const [y,x,height,width] =boxes[0][0],        //console.log(y),        console.log(""drawing...""),        const xc = Math.round(x*image.width),        const yc = Math.round(y*image.height),        const x2c = Math.round(width*image.width),        const y2c = Math.round(height*image.height),        console.log(xc,yc,x2c ,y2c),//x1, y1, x2, y2, not height or width        //ctx.rect(xc, yc, x2c-xc, y2c-yc),        //ctx.stroke(),        console.log(a.shape)        sliced = a.slice([0,yc,xc,0],[1,y2c-yc,x2c-xc,3] ),        console.log(sliced)        //await new Promise(r => setTimeout(r, 1000)),        //ctx.clearRect(0, 0, canvas.width, canvas.height)        tf.browser.toPixels(sliced.squeeze(),canvas),        model_s.then(model_s =>{          async function small_g(sliced){            console.log(""predicting small"",sliced),            const result_s = await model_s.executeAsync(sliced),            return result_s          }          const result_s = small_g(sliced),          console.log(result_s),        }),        },      console.log('Predicting')      var box =  pred(),      }),    },  },}, false),`````Logs:changemain.js:27 File {name: 'test_img.png', lastModified: 1631819149502, lastModifiedDate: Thu Sep 16 2021 15:05:49 GMT-0400 (Eastern Daylight Time), webkitRelativePath: '', size: 259253, …}main.js:45 1268 784main.js:104 Predictingmain.js:56 Startmain.js:58 Endmain.js:76 drawing...main.js:82 117 81 660 624main.js:85 (4) [1, 784, 1268, 3]main.js:87 e {kept: false, isDisposedInternal: false, shape: Array(4), dtype: 'int32', size: 884547, …}main.js:95 predicting small e {kept: false, isDisposedInternal: false, shape: Array(4), dtype: 'int32', size: 884547, …}main.js:100 Promise {<pending>}tf.min.js:17 Uncaught (in promise) Error: Requested texture size [-1536x512] is invalid.    at tf.min.js:17    at jX (tf.min.js:17)    at tf.min.js:17    at e.t.createFloat32MatrixTexture (tf.min.js:17)    at e.t.acquireTexture (tf.min.js:17)    at t.n.acquireTexture (tf.min.js:17)    at t.n.uploadToGPU (tf.min.js:17)    at t.n.runWebGLProgram (tf.min.js:17)    at Object.kernelFunc (tf.min.js:17)    at n (tf.min.js:17)(anonymous) @ tf.min.js:17jX @ tf.min.js:17(anonymous) @ tf.min.js:17t.createFloat32MatrixTexture @ tf.min.js:17t.acquireTexture @ tf.min.js:17n.acquireTexture @ tf.min.js:17n.uploadToGPU @ tf.min.js:17n.runWebGLProgram @ tf.min.js:17kernelFunc @ tf.min.js:17n @ tf.min.js:17(anonymous) @ tf.min.js:17t.scopedRun @ tf.min.js:17t.runKernelFunc @ tf.min.js:17t.runKernel @ tf.min.js:17stridedSlice_ @ tf.min.js:17stridedSlice__op @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17t.scopedRun @ tf.min.js:17t.tidy @ tf.min.js:17qI @ tf.min.js:17(anonymous) @ tf.min.js:17pW @ tf.min.js:17p @ tf.min.js:17t.processStack @ tf.min.js:17(anonymous) @ tf.min.js:17c @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17bv @ tf.min.js:17o @ tf.min.js:17async function (async)small_g @ main.js:96(anonymous) @ main.js:99Promise.then (async)pred @ main.js:93async function (async)pred @ main.js:57(anonymous) @ main.js:105Promise.then (async)image.onload @ main.js:53load (async)readerDim.onload @ main.js:42load (async)(anonymous) @ main.js:33Show 2 more frames","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5664"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5664"">No</a>=====', ""Any tensor I use even a random one gives this error. I tried recreating the model from the tfjs-converter but that didn't work. I think the error is that this is a center-net model. so I will just use a different model=====""]",1
https://github.com/tensorflow/tfjs/issues/4843,tf.conv2d produces different results for browser and node,6,closed,2021-03-20T01:36:53Z,2021-04-27T15:38:01Z,"i've been chasing down why same image model (object detection) has slightly different results in browser and node environments and it comes down to results of `tf.conv2d` being slightly different for the exactly same inputs.in browser, `cpu`, `webgl` and `wasm` backends produce identical results (and WEBGL_CONV_IM2COL has no effect).but `tfjs-node` using `tensorflow` backend produces different result.example code:```jsconsole.log('Input:', x.shape, x.size, 'sum:', x.reshape([786432]).sum().dataSync()[0]), // input does not change (checked values)console.log('Filter:', params.filters.shape, params.filters.size, 'sum:', params.filters.reshape([864]).sum().dataSync()[0]), // params do not change (checked values)console.log('Strides', strides),let out = tf.conv2d(x, params.filters, strides, 'same'),console.log('Conv2d 1st 5 values:', out.shape, out.size, out.dataSync().slice(0, 5)), // output has different values!console.log('Conv2D sum of all values:', tf.reshape(out, [2097152]).sum().dataSync()[0]), // silly sum just to see how much results diverged```browser output:```logInput: [ 1, 512, 512, 3 ] 786432 sum: -631754.625Filter: [ 3, 3, 3, 32 ] 864 sum: 0.07897007465362549Strides [ 2, 2 ]Conv2d 1st 5 values: [ 1, 256, 256, 32 ] 2097152 Float32Array(5) [ 0.02585916966199875, 0, 0, 0, 0 ]Conv2D sum of all values: -23342.779296875```node output:```logInput: [ 1, 512, 512, 3 ] 786432 sum: -631754.625Filter: [ 3, 3, 3, 32 ] 864 sum: 0.07897007465362549Strides [ 2, 2 ]Conv2d 1st 5 values: [ 1, 256, 256, 32 ] 2097152 Float32Array(5) [ 0.026323730126023293, 0, 0, 0, 0 ]Conv2D sum of all values: -24542.615234375```you can see that value of just first entry is already different and that a simple checksum is off by ~1%environment: tfjs 3.3.0 on chrome 89 and ubuntu 20.10","['@vladmandic I think this might be related to the input data, can you verify the input are the same for node and browser? I suspect fromPixels and decodeJPeg might produce different pixel values.=====', ""@pyu10055 that's the first thing i've thought of as well :)and yes, `decodeJpeg` and `fromPixels` do produce different results - specifically, RGB values in `fromPixels` are offset by +1  i've also double-checked behavior of `alignCorners` and similar items when performing `resizeBilinear`  but i've handled that and that's why i'm printing the checksum of the input (after normalization) now - to confirm input is 100% identical  (if there were any differences, I'd have implemented something like `canvas.js` decoding which is uniform on both platform)====="", '@vladmandic the WebGL has precision loss when stored on texture, it is usually rather small. The input sum is negative seems to be weird, is it overflowing already?=====', ""@pyu10055 > the WebGL has precision loss when stored on texture, it is usually rather smallThe thing is `WebGL` and `WASM` produce results identical up to 5th decimal point (after that it's up to WebGL precision loss)  But `tfjs-node` produces results which are ~2-5% different than either `WebGL` or `WASM` which is not small> The input sum is negative seems to be weird, is it overflowing already?Input here is just an image resized to 1x512x512x3 and normalized  Sum is just a (very) cheap way to do a hash to make sure inputs are same, but given the size of the array no wonder its overflowing.But...I've just tried with *TFJS 3.5.0* where `tfjs-node` ships with **TF2** and difference is almost gone  (sum of conv2d values now shows divergence of ~0.15% - that is at least 25x improvement)  (and more importantly, model predictions actually match)So I guess bug was in TF1 implementation of `conv2d` - and finally updating TFJS to use TF2 resolved this issue as wellFeel free to close the issue====="", 'that is great to know, thanks!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4843"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4843"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5467,webgpu backend incompatible with web workers,6,closed,2021-08-11T16:36:45Z,2021-08-16T20:18:03Z,"trying out `tfjs-backend-webgpu`, there are quite a few instances in the code where it attempts to access **DOM** elements which are by definition *not available* in web workers:```jsif (!(pixels instanceof HTMLVideoElement) ...``````logUncaught (in promise) ReferenceError: HTMLVideoElement is not defined```environment: chrome/94 canary, tfjs 3.8.0, tfjs-backend-webgpu 0.0.1-alpha.7","['Ah, yea. Thanks for catching this.=====', ""@vladmandic  I've made a fix for this issue here https://github.com/tensorflow/tfjs/pull/5472 but I've got some problems to add a worker test to cover this in tfjs for now. I've checked the type checker part in some local framework in worker and it won't report the error.And It is appreciate that if you could help to integrate this fix and build the latest tfjs webgpu backend to see whether there is other issues remain to run this backend in worker. cc @qjia7 too.====="", ""@shaoboyan had some issues building new `tfjs-backend-webgpu` as i didn't realize that your patch is on top of your tree, not main. anyhow, all sorted and tested - it works!  ====="", '@vladmandic Big thanks!=====', 'Thank you , closing this issue as the PR has been merged and latest changes will be available in next release.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5467"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5467"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/494,Module not found: Error: Can't resolve 'crypto' in 'C:,24,closed,2018-07-05T19:19:25Z,2021-10-07T05:30:32Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version""@tensorflow/tfjs"": ""^0.12.0"",#### Browser versionVersion 67.0.3396.99 (Official Build) (64-bit)#### Describe the problem or feature requestI am using angular 6 with tensorflow.js in the latest versions of tensorflow I get a warning in the console that somme crypto module is not found I have this warning all the time, what should I do/?#### Code to reproduce the bug / link to feature requestimport * as tf from '@tensorflow/tfjs',```typescriptimport { Component, OnInit } from '@angular/core',import * as tf from '@tensorflow/tfjs',@Component({  selector: 'app-xor-example',  templateUrl: './xor-example.component.html',  styleUrls: ['./xor-example.component.css']})export class XorExampleComponent implements OnInit {  // TRAINING DATA.  x_train = tf.tensor2d([[0, 0], [0, 1], [1, 0], [1, 1]]),  y_train = tf.tensor2d([[0], [1], [1], [0]]),  // Defining a model.  model: tf.Sequential,  constructor() { }  ngOnInit() {  }  async initModel() {    this.model = tf.sequential(),    this.model.add(tf.layers.dense({ units: 8, inputShape: [2], activation: 'tanh' })), // input layer    this.model.add(tf.layers.dense({ units: 1, activation: 'sigmoid' })), // output layer    const optimizer = tf.train.sgd(0.01),    this.model.compile({      optimizer: optimizer,      loss: 'binaryCrossentropy',    }),    // Creating dataset    const xs = tf.tensor2d([[0, 0], [0, 1], [1, 0], [1, 1]]),    xs.print(),    const ys = tf.tensor2d([[0], [1], [1], [0]]),    ys.print(),    // Train the model    await this.model.fit(xs, ys, {      batchSize: 1,      epochs: 1500    }),    const saveResults = await this.model.save('localstorage://my-model-1'),    const loadedModel = await tf.loadModel('localstorage://my-model-1'),    console.log('Prediction from loaded model:'),    // loadedModel.predict(tf.ones([1, 3])).print(),    this.prediction = this.model.predict(xs),    console.log(this.prediction),  }}```### this is from chrome console```consoleUncaught ReferenceError: global is not defined    at Object../node_modules/protobufjs/src/util/minimal.js (minimal.js:49)    at __webpack_require__ (bootstrap:76)    at Object../node_modules/protobufjs/src/writer.js (writer.js:4)    at __webpack_require__ (bootstrap:76)    at Object../node_modules/protobufjs/src/index-minimal.js (index-minimal.js:13)    at __webpack_require__ (bootstrap:76)    at Object../node_modules/protobufjs/minimal.js (minimal.js:4)    at __webpack_require__ (bootstrap:76)    at Object../node_modules/@tensorflow/tfjs-converter/dist-es6/data/compiled_api.js (compiled_api.js:1)    at __webpack_require__ (bootstrap:76)```and this is what I see in the console![xor test](https://user-images.githubusercontent.com/20750445/42343328-6627ed14-80a1-11e8-8caf-03817d414dbd.PNG)","[""I have tried it to install crypto but again is not working, all versions from 10.0 and above is not working on angular projects I am not sure if they don't work on any typescript version `npm i crypto-js `====="", 'Can you please provide us a link to the source that we can fork/download? This will help with investigation.=====', '@nkreeger I have created a repo you can run inside the root folder : `npm install`after the installation run `ng serve`this should create a server on `http://localhost:4200/xor`check the console both on vscode or google chromeyou can clone the ptoject https://github.com/George35mk/example-tf-error.git=====', '@nkreeger thank you for looking on this=====', 'The same happens with stackblitz.com @nkreeger https://stackblitz.com/edit/angular-eu4cjy=====', ""What I have found so far is that:**crypto** is now part of the node.js coreAnd I think the tensorflow.js is not working on any newest angular application.I can't do anything about this.====="", ""Hi @George35mk -I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947dBut instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:```js// old:node: false,// new:node: { crypto: true, stream: true },```I found an issue that you should chime-in on to help fix this down the road: https://github.com/angular/angular-cli/issues/10954Hope this helps!====="", '@nkreeger Thank you very much.I am surprised how you find the solution.=====', 'not fixed Yet!=====', 'This issue is related to another one that is 2 years old ...guys common ... =====', '@George35mk  please let us know if this is still an issue ?=====', ""> Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:> > ```js> // old:> node: false,> // new:> node: { crypto: true, stream: true },> ```> > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!I love you====="", 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====', ""While the original issue was an error, it is now manifested as a warning on my system.    WARNING in ./node_modules/@tensorflow/tfjs-core/dist/tf-core.esm.js    Module not found: Error: Can't resolve 'crypto' in 'C:\\Users\\user\\github\\repo.github.io\\source\\node_modules\\@tensorflow\\tfjs-core\\dist'My Angular app works as expected in the browser.## Environment```     _                      _                 ____ _     ___    / \\   _ __   __ _ _   _| | __ _ _ __     / ___| |   |_ _|   / △ \\ | '_ \\ / _` | | | | |/ _` | '__|   | |   | |    | |  / ___ \\| | | | (_| | |_| | | (_| | |      | |___| |___ | | /_/   \\_\\_| |_|\\__, |\\__,_|_|\\__,_|_|       \\____|_____|___|                |___/    Angular CLI: 6.2.5Node: 8.11.1OS: win32 x64Angular: 6.1.10... animations, common, compiler, compiler-cli, core, forms... http, language-service, platform-browser... platform-browser-dynamic, router, service-workerPackage                           Version-----------------------------------------------------------@angular-devkit/architect         0.8.5@angular-devkit/build-angular     0.8.5@angular-devkit/build-optimizer   0.8.5@angular-devkit/build-webpack     0.8.5@angular-devkit/core              0.8.5@angular-devkit/schematics        0.8.5@angular/cdk                      6.4.7@angular/cli                      6.2.5@angular/material                 6.4.7@angular/pwa                      0.7.5@ngtools/webpack                  6.2.5@schematics/angular               0.8.5@schematics/update                0.8.5rxjs                              6.3.3typescript                        2.7.2webpack                           4.20.2```====="", ""> Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:> > ```js> // old:> node: false,> // new:> node: { crypto: true, stream: true },> ```> > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!thank you. It helped me!!!!====="", 'Because I needed to work with some 3D tensors I installed again the tensorflow.js package in my angular application, and again I get the same error:> Can\'t resolve \'crypto\' in \'C:\\Users\\user\\....what I found to solve this warnings without modifing any of the angular core files but only your **package.json**try this on your **package.json**```json{  ""scripts"": { },  ""dependencies"": { },  ""devDependencies"": { },  ""browser"": {    ""crypto"": false  }}```=====', '> Because I needed to work with some 3D tensors I installed again the tensorflow.js package in my angular application, and again I get the same error:> > > Can\'t resolve \'crypto\' in \'C:\\Users\\user....> > what I found to solve this warnings without modifing any of the angular core files but only your **package.json**> > try this on your **package.json**> > ```json> {>   ""scripts"": { },>   ""dependencies"": { },>   ""devDependencies"": { },> >   ""browser"": {>     ""crypto"": false>   }> > }> ```Worked for me! Thank you!=====', ""> Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:> > ```js> // old:> node: false,> // new:> node: { crypto: true, stream: true },> ```> > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!I love you. You save my time.====="", ""> Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:> > ```js> // old:> node: false,> // new:> node: { crypto: true, stream: true },> ```> > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!感謝您 這很有幫助====="", '> Because I needed to work with some 3D tensors I installed again the tensorflow.js package in my angular application, and again I get the same error:> > > Can\'t resolve \'crypto\' in \'C:\\Users\\user....> > what I found to solve this warnings without modifing any of the angular core files but only your **package.json**> > try this on your **package.json**> > ```json> {>   ""scripts"": { },>   ""dependencies"": { },>   ""devDependencies"": { },> >   ""browser"": {>     ""crypto"": false>   }> > }> ```Thank you providing such a wonderful solution=====', ""> Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js' and changed the lines in that regex:> > ```js> // old:> node: false,> // new:> node: { crypto: true, stream: true },> ```> > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!I cannot find the file.====="", '> > Hi @George35mk -> > I found a couple of cycles to try this out. It looks like this is a problem with the core-parts of the angular webpack bundler. I followed the steps here: https://gist.github.com/niespodd/1fa82da6f8c901d1c33d2fcbb762947d> > But instead of using a pre-install, I just hand edited `node_modules/@angular-devkit/build-angular/src/angular-cli-files/models/webpack-configs/browser.js\' and changed the lines in that regex:> > ```js> > // old:> > node: false,> > // new:> > node: { crypto: true, stream: true },> > ```> > > > > > I found an issue that you should chime-in on to help fix this down the road: [angular/angular-cli#10954](https://github.com/angular/angular-cli/issues/10954)> > Hope this helps!> > I cannot find the file.In angular 11, the path of the file changed. So the new file looks like this:```const fs = require(\'fs\')const f = \'node_modules/@angular-devkit/build-angular/src/webpack/configs/browser.js\'fs.readFile(f, \'utf8\', function(err, data) {  if (err) {    return console.log(err)  }  var result = data.replace(/node: false/g, \'node: {crypto: true, stream: true, fs: ""empty""}\')  fs.writeFile(f, result, \'utf8\', function(err) {    if (err) return console.log(err)  })})```=====', 'same issue 2021=====', 'Same issue 2022=====']",0
https://github.com/tensorflow/tfjs/issues/1733,Converter: Place the selected signature_def in the model.json,1,closed,2019-07-10T16:51:19Z,2019-11-07T21:25:52Z,"Instead of relying on heuristics for detecting input and output nodes, we should place the selected SignatureDef in the `model.json` when converting a SavedModel to TF.js format.",['cc @nsthorat (since we talked about this couple of times before)====='],0
https://github.com/tensorflow/tfjs/issues/5153,How to build tf.min.js from source code?,1,open,2021-06-01T01:16:30Z,2021-06-01T01:38:32Z,"**System information**- OS Platform and Distribution: Linux Ubuntu 18.04 (x86_64) on dual Intel Xeon 5600, node 11.1.0, Linux 5.4 kernel, gcc 7.5.0- TensorFlow.js installed from (npm or script link): github source code- TensorFlow.js version: 3.6.1- CUDA/cuDNN version: Cuda 10.0, Nvidia Geforce GTX 1060**Describe the problem**Sorry if I'm missing something trivial. I need to build dist/tf.min.js from source code since my CPU does not support AVX instruction set. I don't know if should use bazel, or where are the detailed build instructions. When I runnpm run releasethe script gets blocked asking for a github user~~~ Creating new release branch tfjs_3.6.1 ~~~Username for 'https://github.com':**Provide the exact sequence of commands / steps that you executed before running into the problem**npm run releaseon options, Release unit 0, Phase 0, New version 3.6.1 and when trying to give a Username for github process is not responding anymore to keyboard input","['I have the same issue as https://github.com/tensorflow/tfjs/issues/2631 - changing tfjs-node to ""@tensorflow/tfjs"": ""1.2.2"" on package.json makes simple examples to work, but it is possible to build newer versions with support for not AVX-enabled CPUs?=====']",1
https://github.com/tensorflow/tfjs/issues/5123,[Codelab]: Making Predictions from 2D Data,1,closed,2021-05-25T09:40:22Z,2021-05-25T19:14:44Z,"The section [Define the model architecture](https://codelabs.developers.google.com/codelabs/tfjs-training-regression/index.html#3) is not able to complete due to code error.In the HTML code example `<script src=""script.js""></script>` appears before `<body>`. Since the JavaScript code is executed before the HTML body tag has loaded, document.body is null.A possible solution is to move the script at the end of the body tag, once it’s been loaded:```<body><script src=""script.js""></script></body>```","['This is fixed , please verify.=====']",1
https://github.com/tensorflow/tfjs/issues/4764,model converted with tensorflowjs_convert fails with webgl backend due to use of Infinity constant,3,closed,2021-02-28T13:49:59Z,2021-03-05T19:15:31Z,"I've converted Places365 from Caffe model (GoogleNet variation) to TF saved model using MMDNN and then to TFJS graph model using tensorflowjs_convert  (if needed, I can post actual conversion code or the model itself)  Resulting graph model works fine with tfjs-node (fully tested) and `wasm`, but fails in browser using `webgl` backend:error occurs during call to `predict()`:```logwebgl_util.js:82 Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.js:82)    at GPGPUContext.createProgram (gpgpu_context.js:199)    at compileProgram (gpgpu_math.js:44)    at backend_webgl.js:694    at MathBackendWebGL.getAndSaveBinary (backend_webgl.js:731)    at MathBackendWebGL.runWebGLProgram (backend_webgl.js:693)    at Object.padV2$1 [as kernelFunc] (PadV2.js:27)    at kernelFunc (engine.js:463)    at engine.js:524    at Engine.scopedRun (engine.js:337)```with actual error being:```logERROR: 0:178: 'Infinity' : undeclared identifier```and generated GLSL code in question is:```glslvoid main() {  ivec4 outputLoc = getOutputCoords(),  vec4 result = vec4(0.),  ivec4 rc = outputLoc,  if (any(lessThan(rc, start)) || any(greaterThanEqual(rc, end))) {    result[0] = float(-Infinity), // <-- ERROR 'Infinity' : undeclared identifier  } else {    ivec4 source = rc - start,    result[0] = getChannel(getX(source.x,source.y,source.z,source.w), vec2(source.z,source.w)),  }  ...```Which comes from `tfjs-backend-webgl/mirror_pad_packed_gpu.ts` `class PadPackedProgram`:```glsl  result[${i}] = float(${constantValue}),```Where `constantValue` is set in attributes of PadV2 in the model itself.Per OpenGL specs, there are no pre-defined constants, so this is bound to fail unless constant is either converted to value or set by TFJS itself.Or another way would be to address this in the converter itself so `Infinity` is translated to numeric value during conversion?Environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10```tensorflowjs_converter --versiontensorflowjs 3.2.0Dependency versions:  keras 2.4.0  tensorflow 2.4.1```","[""Update:I've worked around the issue by modifying original Python model code from  ```pythonpool2_3x3_s2_pad = tf.pad(conv2_norm2, paddings = [[0, 0], [0, 1], [0, 1], [0, 0]], constant_values=float('-Inf'))```to:```pythonpool2_3x3_s2_pad = tf.pad(conv2_norm2, paddings = [[0, 0], [0, 1], [0, 1], [0, 0]], constant_values=float('-3.4028235e+38'))````where `-3.4028235e+38` is value of `-tf.float32.max`Now converted model works with with TFJS WebGL backendBut this should really be handled by TFJS itself as in many cases model code is not available====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4764"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4764"">No</a>=====', 'just saw the linked PR, thanks!=====']",1
https://github.com/tensorflow/tfjs/issues/5490,Blazeface result is not correct on WASM backend,6,open,2021-08-14T12:05:53Z,2021-10-05T23:03:34Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  Yes, I am using the estimateFace function of blazeface to detect the actual face in the image uploaded. Here is my code for reference:```  const detectFace = async () => {    const p = document.getElementById('result'),    p.innerHTML = ""Detecting ..."",    console.log('Loading model...')    const model = await blazeface.load({scoreThreshold: 0.9}),    console.log('Loaded model...')    var predictions,    var t0, t1,    for (j = 0, j < 5, j++) {        var t0 = performance.now()        // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an        // array of detected faces from the MediaPipe graph. If passing in a video        // stream, a single prediction per frame will be returned.        predictions = await model.estimateFaces(canvas, false),        console.log(predictions),        var t1 = performance.now()    }    if (predictions.length > 0) {        var prediction = predictions[0]        var text = ""The face detection completed in "" + (t1 - t0) + "" ms.<br>The face matched with predictions of "" + predictions[0].probability[0],        text += ""<br>""        text += `<strong>Right Eye:</strong> <br> x = ${prediction.landmarks[0][0]}, y = ${prediction.landmarks[0][1]}<br>`        text += `<strong>Left Eye:</strong> <br> x = ${prediction.landmarks[1][0]}, y = ${prediction.landmarks[1][1]}<br>`        text += `<strong>Nose:</strong> <br> x = ${prediction.landmarks[2][0]}, y = ${prediction.landmarks[2][1]}<br>`        text += `<strong>Mouth:</strong> <br> x = ${prediction.landmarks[3][0]}, y = ${prediction.landmarks[3][1]}<br>`        text += `<strong>Right Ear:</strong> <br> x = ${prediction.landmarks[4][0]}, y = ${prediction.landmarks[4][1]}<br>`        text += `<strong>Left Ear:</strong> <br> x = ${prediction.landmarks[5][0]}, y = ${prediction.landmarks[5][1]}<br>`        renderPrediction(predictions),    } else {        var text = ""No face found in the image."",    }    p.innerHTML = text,}```- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  This behaviour is reproducible across multiple OS of both desktop & mobile devices.- TensorFlow.js installed from (npm or script link):  https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0- TensorFlow.js version (use command below):  3.1.0  - Browser version:  this behavior is reproducible across multiple Browser versions including latest Chrome 92**Describe the current behavior**I am using the estimateFace function of blazeface to detect the actual face in the image uploaded. It is giving `0.9804430603981018` probability when I placed the mobile device image to detect. If the face is present in the frame, then it detects properly the face and results are as expected. But if we don't have any face in the frame, in that case the blazeface detection results shows that face exists while it is Mobile device with `0.9804430603981018` probability with 0.9 scoreThreshold. (Attaching the image for reference)**Describe the expected behavior**It should show result of face detected only if any face is present in frame. If there is no face in frame, then there is no point to show face detected with such high probability i.e. of 98% for mobile device on frame.**Standalone code to reproduce the issue**You can use the following codepen link to reproduce: https://codepen.io/deepanshusharma012/pen/MWmRaLWAttaching the mobile device pic on which the issue is reproducible, Also the blazeface models that I'm using for face detection.`Issue Reproducible Snapshot:` ![Screenshot from 2021-08-14 17-17-12](https://user-images.githubusercontent.com/41855523/129445669-3d1142af-9f78-49ad-b085-b82153f84f39.png)`Image to reproduce the issue`![Proctoring Snapshot - 2021-08-12T11_03_56 388Z](https://user-images.githubusercontent.com/41855523/129445676-30295d7a-04b3-4b12-9da5-3056877a7174.jpeg)`Blazeface models used in face detection:`[blazeface models.zip](https://github.com/tensorflow/tfjs/files/6986431/blazeface.models.zip)","[""@lina128 @rthadur Hope you are doing great!Is there any update on this issue or any ETA to resolve it? Actually this issue is a blocker for our application and detecting the false events.It would be great if you respond on it and try to roll it's fix asap.====="", ""Hi @deepanshusharma012 , thank you for reporting this. I tested it with our own demo, same problem. Actually we're working on a new BlazeFace model, which is going to be more accurate. It will be released this quarter.====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', ""The work is still going on this issue. I hope we'll be getting activity on this issue, so no need to mark it stalled.====="", 'Hi @lina128, as you said the new BlazeFace model (more accurate one) will be released by Q3 quarter. So, are we still working on it, or it has released? or any ETA for the same.=====', 'Sorry, I actually meant Q4. I replied in September, and we are already thinking for Q4 :) Sorry for the confusion.=====']",1
https://github.com/tensorflow/tfjs/issues/124,Add unit tests for tensor naming,4,closed,2018-04-06T15:41:27Z,2018-06-11T16:21:07Z,"common.ts includes functionality for managing tensor names, such as `getUniqueTensorName(prefix)`,This functionality should have unit tests.","['@bileschi Is this issue still pending?=====', 'Looks like `getUniqueTensorName` is covered, but `isValidTensorName` is not=====', 'Hi @bileschi : I see the test cases available for both `getUniqueTensorName` and `isValidTensorName`. Hope we can close this issue. =====', 'You are right!  Closing.=====']",0
https://github.com/tensorflow/tfjs/issues/1449,errors executing async models loaded via tf.loadGraphModel.,10,closed,2019-03-26T21:01:36Z,2019-08-21T20:04:25Z,"#### TensorFlow.js version```""@tensorflow-models/coco-ssd"": ""^1.0.0"",""@tensorflow-models/mobilenet"": ""^1.0.0"",""@tensorflow-models/universal-sentence-encoder"": ""1.0.1"",""@tensorflow/tfjs"": ""1.0.0"",""@tensorflow/tfjs-node"": ""1.0.1"",```Error also occurs on tfjs 1.0.2#### Node versionNode 11.11#### Describe the problem or feature requestThe following error occurs when making a prediction with either coco-ssd or universal-sentence-encoder (which are both models with control flow ops). But doesn't happen with mobilenet.```node:59415) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'id' of undefined    at /Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:295:99    at Array.map (<anonymous>)    at GraphExecutor.<anonymous> (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:295:58)    at step (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:56:23)    at Object.next (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:37:53)    at fulfilled (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:28:58)    at processTicksAndRejections (internal/process/next_tick.js:81:5)```From doing some console logging it looks like the tensors in the graph are undefined during executeAsync.Note that I only get this error when using the node backend (tfjs-node), if i use the vanilla cpu backend (tfjs) it works.#### Code to reproduce the bug / link to feature request```// const tf = require('@tensorflow/tfjs'), // using this import worksconst tf = require('@tensorflow/tfjs-node'),const cocossd = require('@tensorflow-models/coco-ssd'),const mob = require('@tensorflow-models/mobilenet'),global.fetch = require('node-fetch'),const useLoader = require('@tensorflow-models/universal-sentence-encoder'),// This worksconst model = await mob.load(),const res = await model.classify(tf.randomNormal([224, 224, 3])),console.log(res),// This fails.const cocomodel = await cocossd.load(),const cocoa = await cocomodel.detect(tf.randomNormal([224, 224, 3])),console.log(cocoa)  // this failsconsole.time('Loading Universal Sentence Encoder'),const use = await useLoader.load(),console.timeEnd('Loading Universal Sentence Encoder'),```","[""Further info on this. This bug doesn't occer when using the following import pattern (where both tfjs and tfjs-node are required).```const tf = require('@tensorflow/tfjs'),require('@tensorflow/tfjs-node'),```Note this was on a different code sample than listed in the issue, but hopefully provides a clue as to what the underlying issue might be.====="", '@tafsiri can you share your package.json file? I suspect that when you including multiple models, they installed different versions of tfjs.=====', 'Here are my dependencies. I only have one model loaded in my actual project, the multiple models in the example above was just to confirm that it wasn\'t specific to universal-sentence-encoder```""dependencies"": {    ""@tensorflow-models/universal-sentence-encoder"": ""1.0.1"",    ""@tensorflow/tfjs"": ""1.0.1"",    ""@tensorflow/tfjs-node"": ""1.0.1"",    ""d3-dsv"": ""^1.1.1"",    ""lodash"": ""^4.17.11"",    ""node-fetch"": ""^2.3.0""  },  ""devDependencies"": {    ""babel-core"": ""^6.26.3"",    ""babel-plugin-transform-runtime"": ""^6.23.0"",    ""babel-preset-env"": ""^1.7.0"",    ""clang-format"": ""^1.2.4"",    ""cross-env"": ""^5.2.0"",    ""http-server"": ""^0.11.1"",    ""parcel-bundler"": ""^1.12.3""  }```=====', ""@tafsiri I believe tfjs-node has tfjs as one of the dependencies, you don't need to install tfjs separately. Can you try to remove tfjs from your package.json file?====="", ""@pyu10055 If i do that I get the following error```(node:56574) UnhandledPromiseRejectionWarning: Error: Found more than one (2) load handlers for URL 'https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder/model.json'    at GraphModel.findIOHandler (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:147:23)    at GraphModel.<anonymous> (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:163:30)    at step (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:48:23)    at Object.next (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:29:53)    at /Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:23:71    at new Promise (<anonymous>)    at __awaiter (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:19:12)    at GraphModel.load (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:158:16)    at Object.<anonymous> (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:377:48)    at step (/Users/yassogba/projects/intent-classifier/node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_model.js:48:23)```====="", '@tafsiri This is IOHandler error, probably https are register with both browser and node handlers.Do you have repo or codepen for me to reproduce?=====', ""@pyu10055 This repo code demonstrates it https://github.com/tafsiri/use-text-classifier/blob/master/training/train.js, update the package.json to remove the other models. However on your point about that I thought all the models specify tfjs as a peerDependency, so they won't install their own copy.Also see https://github.com/tensorflow/tfjs/issues/1454====="", '@tafsiri @caisq The multiple io handler issue seems to be root in this codehttps://github.com/tensorflow/tfjs-node/blob/3f8f64a420711b3cf542bc97ce97cf3d78c471e1/src/io/node_http.tswhich register a handler for http url as the same time as core browser_http class.If we allow multiple handlers, we need to update the model loaders to use a particularhandler (the last one)?=====', '@tafsiri is this issue resolved?=====', 'No longer an issue on latest tfjs.=====']",0
https://github.com/tensorflow/tfjs/issues/5111,[Codelab]: TensorFlowJS Comment Spam Detection,2,closed,2021-05-23T05:58:32Z,2021-06-03T21:27:12Z,"[Found a typo in Section 8 Serve the machine learning model](https://codelabs.developers.google.com/codelabs/tensorflowjs-comment-spam-detection?continue=https%3A%2F%2Fdevelopers.google.com%2Flearn%2Fpathways%2Fget-started-text-classification-web%3Fhl%3Den%23codelab-https%3A%2F%2Fcodelabs.developers.google.com%2Fcodelabs%2Ftensorflowjs-comment-spam-detection#7)The current sentence is like this. **First, if you have not done so already, unzip the files you downloaded for the model downloaded at the start of this codelab.** The word downloaded is repeating and it bit confusing. It will be better if we remove the second occurrence of word downloaded so the sentence will be more understandable like this.**First, if you have not done so already, unzip the files you downloaded for the model  at the start of this codelab**","['Thanks for the feedback on this one. I shall update that and push it out shortly. Apologies for the delay I was out of office.=====', 'Changes made they should be pushed live shortly (next day or two).=====']",1
https://github.com/tensorflow/tfjs/issues/5612,Installing qna model using npm doesn't NOT install the last version of the sources but older version with outdated version of tensorflow,5,open,2021-09-13T13:05:24Z,2021-09-13T21:14:11Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): tfjs-node- TensorFlow.js version: 3.9.0- CUDA/cuDNN version: NAWhen installing qna model using npm install @tensorflow-models/qna, the package installed is referencing tensor v1.3.3 and NOT last major release v3.0.0 as indicated inside the last version of the source package.json. I assume NPM registry is NOT referencing the last commit of your sources.Thanks","[""Thanks for the issue report, @fbx31. Can you please check that the version of `@tensorflow-models/qna` you've installed is `1.0.0`? `1.0.0` [lists `@tensorflow/tfjs-core` and `@tensorflow/tfjs-converter` as peer dependencies](https://github.com/tensorflow/tfjs-models/blob/master/qna/package.json#L15-L18), so you should be able to (and will need to) install a version of your choice of those packages to use automl.It's also possible you're using another package that lists core and cpu as dependencies, and that may be why you're seeing them installed at an earlier version than expected.====="", 'Hello,Starting from fresh install, installing @tensorflow/tfjs-node (v3.9.0 OK), then installing qna model doing npm i @tensorflow-models/qna.I have the version 1.0.0 of the package installed with following peer dependencies:     ""peerDependencies"": {    ""@tensorflow/tfjs-core"": ""^1.5.2"",    ""@tensorflow/tfjs-converter"": ""^1.5.2""  },BUT in github, when i look the package.json in sources of qna, i have the following dependencies:  ""peerDependencies"": {    ""@tensorflow/tfjs-converter"": ""^3.0.0"",    ""@tensorflow/tfjs-core"": ""^3.0.0""  }That\'s why I said that npm package downloaded of qna seems to not be the last commit in github.Thanks=====', '@fbx31 thats correct , we need a fresh build to npm , @mattsoulanille can you please help push a fresh build to npm ?=====', 'My bad! I should have checked the NPM package, not GitHub. We can publish an updated build to NPM.=====', 'Thanks a lot, I,ll wait for the update instead of trying to rebuild it by myself :-)Have a nice day.=====']",1
https://github.com/tensorflow/tfjs/issues/1365,tensorflow custom model : “Giving wrong prediction with bulk data”,1,closed,2019-03-12T17:09:06Z,2019-03-12T19:12:41Z,"We are facing an issue here, while training with the bulk dataset and use that custom model for prediction it giving the wrong prediction. Most of the time it gives the second index value.Setting Label:    let label = [],    for(var k = 0, k< 21, k++){        label.push('Store Shelf'),    }    for(var l = 0, l< 21, l++){        label.push('Rejected'),    }    console.log('label:::'+label),    const setLabel = Array.from(new Set(label)),    console.log('model1',model),    let prediction =  await model.predict(tensor),    console.log('prediction:::'+ prediction),    pred = prediction.argMax(1).dataSync(),     const labelsPred = Array.from(pred).map(e => setLabel[e]),    console.log('labelsPred:::'+ labelsPred),    let top5 = Array.from(prediction)    .map(function(p,i){        return {            probability: p,            className: prediction[i]        },    }).sort(function(a,b){        return b.probability-a.probability,    }).slice(0,1),    $(""#prediction-list"").empty(),    //top5.forEach(function(p){    $(""#prediction-list"").append(labelsPred),    //}),Here we have two labels 'Store Shelf' and 'Reject' but it gives us 'Rejected' for all kind of images that we pass with bulk data(trained with 40 images).Please guide what I'm doing wrong here, or how can I achieve correct prediction with bulk dataset after loading saved model.","['This seems like an issue with your model setup rather than an issue with the library. You will probably get more luck solving this kind of issue on stackoverflow.com with the ""tensorflow.js"" tag.=====']",0
https://github.com/tensorflow/tfjs/issues/4603,API Version 3.0.0 is not working with script link,4,closed,2021-01-26T22:59:59Z,2021-02-10T17:47:27Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**I use this <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0/dist/tf.min.js""></script>to implement CNN. There is  a error poped up. When the version downgrades to 2.8.5, the error has been disappeared. Are there any solutions for this issue?**Provide the exact sequence of commands / steps that you executed before running into the problem**Node Version v12**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.","['@jacobchen1 Thanks for reporting, Can you share a codepen for reproducing this error? =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4603"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4603"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5624,multiple and various tfjs build issues,1,open,2021-09-14T15:14:00Z,2021-10-17T12:47:04Z,"can someone take a look at the **tfjs build issues**? none are fatal,  but sheer number and variety is *not a good indicator*using `tfjs` from main branch...- obsolete packages:```logwarning @babel/polyfill@7.12.1: 🚨 This package has been deprecated in favor of separate inclusion of a polyfill and regenerator-runtime (when needed). See the @babel/polyfill docs (https://babeljs.io/docs/en/babel-polyfill) for more information.warning @babel/polyfill > core-js@2.6.12: core-js@<3.3 is no longer maintained and not recommended for usage due to the number of issues. Because of the V8 engine whims, feature detection in old core-js versions could cause a slowdown up to 100x even if nothing is polyfilled. Please, upgrade your dependencies to the actual version of core-js.warning rollup > fsevents@2.1.3: ""Please update to latest v2.3 or v2.2""warning rollup-plugin-babel@4.4.0: This package has been deprecated and is no longer maintained. Please use @rollup/plugin-babel.```- incorrectly marked external packages:    (this one is related to #4745 which did not resolve it fully)```logsrc/index.ts → dist/tf-backend-wasm.node.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js, but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external, but could not be resolved – treating it as an external dependencycreated dist/tf-backend-wasm.node.js in 9.9s```- obsolete api:```logwarning karma-typescript > url > querystring@0.2.0: The querystring API is considered Legacy. new code should use the URLSearchParams API instead.(node:3683) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.```- incorrect dependencies:```logwarning "" > @rollup/plugin-typescript@3.1.1"" has incorrect peer dependency ""rollup@^1.20.0"".```- missing dependencies:```logwarning "" > @rollup/plugin-typescript@3.1.1"" has unmet peer dependency ""tslib@*"".```- incorrect exports:```logWARNING:  ../link-package-core/node_modules/@tensorflow/tfjs-core/dist/hash_util.js (23:12) 'default' is not exported by '../link-package-core/node_modules/long/src/long.js'```- incorrect imports:```logWARNING:  'setBackend' and 'pow' are imported from external module '@tensorflow/tfjs-core' but never used```- bad code:```log > tfjs-layers/src/layers/normalization.ts:586:22: warning: Comparison using the ""!=="" operator here is always true    586 │             this.axis !== [nDims - 1]) {```- incorrect tree shaking:this was mentioned numerous times and never properly resolved. latest (still open) is #5182.and as a final note, build process works only with `yarn` - but then scripts sometimes call `npm instead`!?","[""FYI, I've created a custom TFJS build process <https://github.com/vladmandic/tfjs>:- runs on unmodified `tfjs` sources,- builds 10x faster- produces bundle which is 5x smaller: The main difference is that it always resolves to actual `.ts` sources using custom resolver in the build process- it doesnt allow any imports from `/dist`- it doesnt allow any imports from symlinked or virtual packagesFor example, original `tf.es2017.js` is **3.9MB** without `wasm` and `webgpu` backends which are another **2MB** on top  This new `tfjs.esm.js` is `2.5MB` *(1.3MB minified)* with `wasm` and `webgpu` included  =====""]",1
https://github.com/tensorflow/tfjs/issues/1625,Transpose kernel should use shared memory.,1,closed,2019-05-30T11:06:32Z,2019-12-13T16:13:30Z,,"['Hi, I may help on this issue.=====']",0
https://github.com/tensorflow/tfjs/issues/3444,"Uncaught (in promise) Error: Unrecognized transform type: ""filter""",4,closed,2020-06-12T21:06:03Z,2020-06-15T23:09:51Z,"Hello,Running tfjs-vis from CDN link within HTML file, and it's not rendering properly. **Screenshot**:<img width=""1116"" alt=""Screen Shot 2020-06-12 at 4 59 21 PM"" src=""https://user-images.githubusercontent.com/8492625/84546128-95854100-acce-11ea-871d-f2c1288914fa.png"">**CDN tag**:  ``` <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis"">      </script>```**Code**: ``` async function plot(pointsArray, featureName) {              tfvis.render.scatterplot({                  name: `${featureName} vs House Price`              }, {                  values: [pointsArray],                  series: [""original""]              }, {                  xLabel: featureName,                  yLabel: ""Price"",              })          }          async function run() { // reads csv file              const houseSalesDataSet = tf.data.csv('http://127.0.0.1:8080/kc_house_data.csv'),              const sampleDataSet = houseSalesDataSet.take(10),              const dataArray = await sampleDataSet.toArray(),              console.log(dataArray),              const points = houseSalesDataSet.map((record) => ({                  x: record.sqft_living,                  y: record.price,              }))              plot(await points.toArray(), ""Square feet""),          }          run(),```","['Also experiencing this using the latest version on the CDN, which is **1.4.2**The workaround for now is to pin to **1.4.1**:     <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.4.1""></script>=====', 'That fixed the issue - thanks!=====', 'Should be fixed in https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.4.3 give that a try and let us know if that works.=====', 'Yes, version 1.4.3 works. Thanks for the update.=====']",0
https://github.com/tensorflow/tfjs/issues/1114,More clearly distinguish between ticklables and axis labels,0,closed,2019-01-22T16:47:44Z,2019-03-01T18:25:23Z,Following on from the discussion on https://github.com/tensorflow/tfjs-vis/pull/43. Use the [x/y]Label convention for axis labels/titles and [x/y]TickLabels convention for tick mark labels. This aligns nicely with ggplot & matplotlib.,[],0
https://github.com/tensorflow/tfjs/issues/5292,"[BUG] With require.js inside electron app , does not work",5,open,2021-07-05T07:21:41Z,2021-07-29T01:55:55Z,"Here is my scenerio:My app is using require.js and is running inside electron.I kept getting error and then i fugred it out.```          ENV.registerFlag('IS_NODE', function () {            // ignore NODE if AMD is found            if (typeof define === 'function' && define.amd) {              return false,            }            return typeof process !== 'undefined' && typeof process.versions !== 'undefined' && typeof process.versions.node !== 'undefined',          }),```Please Update function ```ENV.registerFlag('IS_NODE', function ```so that it ignores NODE if AMD is found","['@gbaned can you provide a full running example to expedite the reproduction of the issue. thanks=====', '@mafar In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!=====', '> @mafar In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!example will require an electron mock up with require.js inside it. Too much for me :)But let me explain plzWhen require.js is running inside electorn as in my case, - ```tfjs``` is loaded via require.js (AMD), i dont load it via node.js (commonJs)- but when code reaches at point as shown above , ```IS_NODE``` becomes true since we are inside electron - - but from here dependencies start to fail loading becuse initialy it was loaded via ```AMD``` and now it switched to ```CommonJS ```- - Simply put, if  ```tfjs``` is loaded via ```AMD``` , it should not consider ```CommonJs``` at all, to avoid dependency resolution conflicts=====', ""We may need to add an `IS_COMMONJS` flag to decide if we can `require()` dependencies. There are some cases where knowing we're running in node is important apart from knowing whether we can `require()` dependencies (e.g. [WASM multithread support](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-backend-wasm/src/flags_wasm.ts#L39-L44)).The only place I see where we're checking `IS_NODE` and then calling `require()` is in [file_data_source.ts](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-data/src/sources/file_data_source.ts#L46-L50). Alternatively, we could just check for commonjs here and avoid adding the new flag.What do you think, @pyu10055?====="", ""> We may need to add an `IS_COMMONJS` flag to decide if we can `require()` dependencies. There are some cases where knowing we're running in node is important apart from knowing whether we can `require()` dependencies (e.g. [WASM multithread support](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-backend-wasm/src/flags_wasm.ts#L39-L44)).> > The only place I see where we're checking `IS_NODE` and then calling `require()` is in [file_data_source.ts](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-data/src/sources/file_data_source.ts#L46-L50). Alternatively, we could just check for commonjs here and avoid adding the new flag.> > What do you think, @pyu10055?I agree. This wIll be a nice solution. If you provide me with a PR, I can test it using my project and report back quick if this is working=====""]",1
https://github.com/tensorflow/tfjs/issues/1384,Model.Classify() gives GL_INVALID_OPERATION: Object cannot be used because it has not been generated.,10,closed,2019-03-14T18:16:54Z,2019-03-22T22:26:50Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.0.0#### Browser versionGoogle Chrome:  Version 73.0.3683.75 (Official Build) (64-bit)### Describe the problem or feature requestI followed the steps from Google Codelab: https://codelabs.developers.google.com/codelabs/tensorflowjs-teachablemachine-codelab/index.html#0The given example works fine, but when I made some changes to the example for my Blog with multiple images, it gives this error:<img width=""508"" alt=""error"" src=""https://user-images.githubusercontent.com/11924142/54380667-695db900-4662-11e9-8f94-dc0afd338649.png"">As you can see, when I click on the first image, it works fine. Loads the model, does classification, displays the results. As I click on a second image, it gives these errors. This seems to be happening on this Windows Laptop [Dell  Inspiron, Windows 10] with this Version of Chrome only as I am also testing the same thing on a MacBook Pro with the exact same version of Chrome without any errors.#### Code to reproduce the bug/link to feature requestYou can check my Blog here [https://anujdutt9.github.io/TFjs-ImageClassifier.html] and look for errors in the console. Also, you can find the complete code for this here [https://github.com/anujdutt9/anujdutt9.github.io/blob/master/TFjs-ImageClassifier.html].","['I came to this repo to open up an issue with exactly the same error message. In my project I am using tfjs 1.0.0, mobilenet 1.0.0 and knn 1.0.0 to classify images from a webcam stream. I can train for about 30sec before exactly the error from @anujdutt9 happens. Not sure if related but maybe it helps narrowing down the issue.=====', '@dsmilkov Possibly related to #1381 Like that issue, this one is also Windows.=====', '@zugende Are you also on Windows? Can you share basic info about the computer (model, GPU card, windows version). Thanks!=====', ""Can you run `tf.ENV.set('WEBGL_PACK', false)` right after you load the TF.js library and let me know if it helps so we can narrow down the problem. Thanks!====="", 'Hi @dsmilkov. ""tf.ENV.set(\'WEBGL_PACK\', false)"" This somehow seems to solve the problem. I don\'t see that error anymore on the Windows machine. Maybe I\'ll wait for others to confirm the same as well.@zugende Did you get a chance to try @dsmilkov \'s solution ?=====', ""Thanks! That's helpful to us. We are waiting for a windows machine and will look into this asap.====="", 'Hi @dsmilkov, I had another demo that was working perfectly till yesterday and suddenly today it broke. I have fixed it but thought it might be good to share as it might help someone else.This is the link to the demo for doing Linear Regression in the browser:https://anujdutt9.github.io/TFjs-LinearRegression.htmlEarlier when I was importing tfjs 0.15.3, it was working fine till yesterday but suddenly today it started giving an error saying:<img width=""746"" alt=""Screen Shot 2019-03-21 at 2 39 20 PM"" src=""https://user-images.githubusercontent.com/11924142/54776761-2746f100-4be7-11e9-9c1e-751bde71f99d.png"">I got this error when I am importing TensorFlow.js using the following line of code:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.15.3/dist/tf.min.js""></script>```Today, I changed it to the following after getting the error above:```<script src=""https://unpkg.com/@tensorflow/tfjs""></script>```and it works fine now. So, I don\'t know what changed in between these two imports within last couple of days, but the previous import broke the working demo yesterday while the latter import fixed it.=====', ""Hi @anujdutt9. I checked out your demo in Chrome on a Lenovo Yoga (Intel Graphics 520) running Windows 10 and was unable to reproduce the error, even with `WEBGL_PACK` set to `true`. Would you mind trying your demo again and seeing whether the tfjs version bump (we're now at `1.0.2`) somehow resolved your issue? Could you also make sure that you're only loading one instance of the library (make sure you don't a warning in the console with 'webgl backend was already registered')?Let me know - thank you!====="", 'Hi @annxingyuan Thanks for your response. Yes, I changed the import link for the library and the demo works now. I think the tfjs version bump was the trick maybe. If I go back to 0.15.3, then some issues happen. But the new one looks good. Thanks 👍 =====', 'Glad to hear it :)=====']",0
https://github.com/tensorflow/tfjs/issues/2590,[WASM] Downcasting broken in wasm.,0,open,2019-12-19T19:44:02Z,2021-11-05T18:34:06Z,These unit tests fail:`tensor test-wasm {} Tensor2D float32 -> bool`and `tensor test-wasm {} Tensor2D int32 -> bool`,[],0
https://github.com/tensorflow/tfjs/issues/4539,Regresssion: WASM fails to load due to _scriptDir not defined,4,closed,2021-01-15T13:10:08Z,2021-01-20T13:21:12Z,This is a regression in `tfjs` 2.8.4 compared to 2.8.3  Setting backend to `wasm` on the main thread results in error:```worker.js onmessage() captured an uncaught exception: ReferenceError: _scriptDir is not defined  threadPrintErr @ e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1  onmessage @ e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1ReferenceError: _scriptDir is not defined  WasmBackendModuleThreadedSimd (a0196e5e-0cae-4d78-90e3-7f0714096dfc:1)  onmessage (e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1)```No issues running TFJS 2.8.3 or older.This is likely related to pull request #4528,"['Hi Vladimir, Thanks for reporting! This is a little bit strange because we did test it in 2.8.4. See this codepin: https://codepen.io/jing-jin/pen/BaLqJoE. If you click ""Settings"", you can see that the program is loading tfjs core and wasm backend @ 2.8.4.Could you please provide more context/sample code about this issue? Thanks!=====', ""Ok, this is related to both #4392 and #4528.  Originally `tfjs-backend-wasm` was not fully minification safe when running through a bundler - minify whitespace and minify syntax were ok, but minify identifiers was not. Then #4392 fixed it and everything was fine for a while.Now #4528 (i assume its this one since i don't see other differences) broke it once more and it's even worse since not even minify syntax works anymore.  If I disable minification completely, then yes `wasm` works in 2.8.4 - but that is not realistic for production code.So yes, it's a regression.  If you want to see a simple project, it's at <https://github.com/vladmandic/stocks> and bundler runs via `server/build.js`====="", 'closing as fixed in tfjs 2.8.5.  thank you!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4539"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4539"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5498,Obtain a different model by converting the tensorflow JS (tfjs) to python (keras),3,closed,2021-08-16T01:56:54Z,2021-08-16T19:34:50Z,"**System information**- Have I written custom code : [download](https://drive.google.com/file/d/10alQ8HIwix_EcWYLO6X8Jt5-rw1wA6PF/view?usp=drivesdk)- OS Platform and Distribution : MacOS v11.0.1- Mobile device : None- TensorFlow.js installed from (npm or script link): script link https://unpkg.com/ml5@0.6.1/dist/ml5.min.js- TensorFlow.js version (use command below): P5 v1.4.0, ML5 v0.6.1- Browser version: Chrome v92.0.4515.107 - Tensorflow.js Converter Version: v3.7.0**Describe the current behavior**The goal is to convert the tensorflow JS (tfjs) to python (keras). However, input the same parameters to both tfjs and keras will get the different results.**Describe the expected behavior**If we input the same parameters to both tfjs and keras, the result should be the same.**Standalone code to reproduce the issue**Step1. Construct a simple model to learn the summation of two units digits by using tfjs. Then, train the model.Step2. Save the trained model, which automatically generates three files, such as “model_meta.json”, “model.json”, “model.weights.bin”.Step3. (js2h5.py)Convert three files from step2 to python, and obtained a file called “model.h5”Step4. Using python (keras) to read “model.h5”Step5. Input the same arguments to predict the result. However, the result is different between tfjs and keras. For example, input 8+4, tfjs gets 12.5162(sum.html), but keras gets 1(predict.py).To clarify this problem, my approach is to use the official released tool “tensorflowjs_converter” to convert “model.h5” to tfjs. However, it only outputs two files “group1-shard1of1.bin” and “model.json”.By using the binary compared tool, we can find that:“model.json” is different from the file “model.json” obtained from step2.“group1-shard1of1.bin” is same as the file “model.json” obtained from step2.Furthermore, if I use tfjs to read both “group1-shard1of1.bin” and “model.json”, it shows an error message:Uncaught (in promise) Error: Weight file with basename 'group1-shard1of1.bin' is not provided.","['Hi @wirlsawyer , we will not be supporting https://unpkg.com/ml5@0.6.1/dist/ml5.min.js as this is thrid party library , please reach out to ml5. You can follow these below using latest tfjs version 1) train the model and save using [model.save](https://js.tensorflow.org/api/latest/#tf.LayersModel.save).2) then convert the model using tfjs-converter3) Load the model in Keras using tf.keras.load_model(...)=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5498"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5498"">No</a>=====', 'Closing this as this is not a bug or feature request for tfjs library.=====']",1
https://github.com/tensorflow/tfjs/issues/1547,how to convert python tf.get_variable to js,8,closed,2019-04-27T04:26:37Z,2019-04-29T02:46:53Z,"python  embedding_table = tf.get_variable(      name=word_embedding_name,      shape=[vocab_size, embedding_size],      initializer=create_initializer(initializer_range))how can i convert it to js?Thanks.george wu","['In Python `tf.get_variable()` is used to create variables. In TensorFlow.js, you can create variables by using `tf.variable()`. See documentation at: https://js.tensorflow.org/api/latest/#variable Let me know if that answers your question.=====', 'However, be aware that `tf.variable()` in TensorFlow.js does not support initializers. You have to provide an explicit initial value.=====', 'Thank you.Still some question on this:1) If I use the same variable name, and call it twice, I should get the same variable. Is this true for js version?2) how can I use the equivalent js version for the python version:with tf.variable_scope(""encoder"")George Wu=====', ""@aihuawu 1: No. tf.variable() in TensorFlow.js doesn't support this reusing pattern. If you want to use a variable twice, just use the same variable object.2. There is no `tf.variable_scope()` in TensorFlow.js. `tf.variable_scope()` has been deprecated in TensorFlow 2.0 also. Users are now recommended to achieve Layer/Variable reusing through keras layers. TF.js has an equivalent of keras API that helps you do that. If you can be more specific on what you are trying to achieve, I may be able to help further.====="", 'Thank you.So you recommend me to just use Layer which has member of Variable. And then I just keep the Layer reference and reuse this layer in other location.  I will try that.=====', '@caisq For python:  tvars = tf.trainable_variables()  grads = tf.gradients(loss, tvars)I have found js:tf.variableGrads (f, varList?) Is it possible to get all trainable_variables in js? Or should I keep reference of all the trainable_variables myself?Thanks.George wu=====', ""`tf.variableGrads()` calculates gradients for all trainable variables by default, if you don't specify the 2nd argument.====="", ""Great, Thanks.That's all I need so far.George Wu=====""]",0
https://github.com/tensorflow/tfjs/issues/5453,[wasm] failed to asynchronously prepare wasm,5,closed,2021-08-10T18:08:32Z,2021-08-12T19:10:23Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 WSL2- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: v3.8.0- CUDA/cuDNN version: N/A**Describe the problem**An error occurred in the build process in CI when sending a PR. The error appears at the wasm backend testing stage. Log says the error is `failed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory`. This problem was not encountered when testing on the local machine. I don't know what causes this. Could someone explain this?**Provide the exact sequence of commands / steps that you executed before running into the problem**Sumbit a PR.**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.Log:```shellfailed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:54                throw ex,                ^RuntimeError: abort(RangeError: WebAssembly Instantiation: Out of memory: wasm memory). Build with -s ASSERTIONS=1 for more info.    at abort (/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:9596)    at /workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:11594error Command failed with exit code 7.```Code:[[wasm] Add bincount kernel #5448](https://github.com/tensorflow/tfjs/pull/5448)","[""@raffizulvian can you please check this related PR where '`sinh`' kernal was added https://github.com/tensorflow/tfjs/pull/4845  and https://github.com/tensorflow/tfjs/pull/4486. Thank you ====="", 'Also please check you chrome version , I see there was an issue with chrome browser here earlier https://stackoverflow.com/questions/55039923/why-does-chrome-eventually-throw-out-of-memory-wasm-memory-after-repeatedly-r =====', 'Hi @rthadur, the error did not appear in my chrome in my local machine (build, lint, and test passed). But it made my PR #5448 check fail as shown in [https://console.cloud.google.com/cloud-build/builds/7dd9824e-1f5a-48a6-849c-a84cf29fe5ad?project=learnjs-174218](https://console.cloud.google.com/cloud-build/builds/7dd9824e-1f5a-48a6-849c-a84cf29fe5ad?project=learnjs-174218).=====', 'I assigned the PR to @lina128 , you can discuss at one place regarding the error . I will close this issue. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5453"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5453"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5495,tfjs-backend-webgpu bad handling of non-webgpu enabled browser,2,closed,2021-08-14T17:36:43Z,2021-08-14T21:53:36Z,"attempting to run `tfjs-backend-webgpu` on a non-webgpu enabled browser results  in a cryptic error instead of stating that `webgpu` is not available:environment: chrome/92 (which is not webgpu enabled), tfjs 3.8.0, tfjs-backend-webgpu 0.0.1-alpha.7```logbackend_webgpu.ts:79 Uncaught ReferenceError: GPUBufferUsage is not defined  at backend_webgpu.ts:79  at tf-backend-webgpu.es2017.js:20  at tf-backend-webgpu.es2017.js:21```same issue previously reported under #3003 which was closed without resolution due to ""lack of activity"" - what kind of resolution is that if there is nothing needed from submitter and it's up to devs to come up with a solution??cc @qjia7","['closing as resolved when building `tfjs-backend-webgpu` from main branch,  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5495"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5495"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5692,Feature: Support vscode-languagedetection on the wasm backend,8,open,2021-10-04T16:25:52Z,2021-10-11T18:02:29Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0 - Browser version: node 14.17.5- Tensorflow.js Converter Version: 3.9.0**Describe the current behavior**```Error: Kernel 'StringSplit' not registered for backend 'wasm'      at Engine.runKernel (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4466:19)      at stringSplit_ (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:24073:25)      at Object.stringSplit__op [as stringSplit] (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:5406:29)      at executeOp$h (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6506:35)      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6642:56      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4394:22      at Engine.scopedRun (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4404:23)      at Engine.tidy (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4393:21)      at Object.tidy (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:10072:19)      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6642:30      at executeOp$j (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6660:7)      at _loop_1 (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7418:31)      at GraphExecutor.processStack (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7444:13)      at GraphExecutor.<anonymous> (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7370:41)      at step (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:81:23)      at Object.next (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:62:53)      at fulfilled (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:52:58)```**Describe the expected behavior**Not throw error.**Standalone code to reproduce the issue**So I tried switching my library to using the wasm backend by changing this line:https://github.com/microsoft/vscode-languagedetection/blob/main/lib/index.ts#L163(and importing the right package of course)but it doesn't seem like my model is playing nice with the wasm backend.Steps are pretty simple:* clone vscode-languagedetection* `yarn add -D @tensorflow/tfjs-backend-wasm`* change the line above and also the import to use `wasm` instead of `cpu`* run `npm test` in the repoProvide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""Hi @TylerLeonhardt. You're seeing this error because the `StringSplit` kernel is not implemented in the wasm backend yet. You can see a list of what ops each backend supports in the [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0) or each backend's `register_all_kernels.ts` file ([here's the one for wasm](https://github.com/mattsoulanille/tfjs/blob/fix_ios_32_bit/tfjs-backend-wasm/src/register_all_kernels.ts#L107)).I'll change this issue to a feature request. It's possible there are other ops in your model that are also not yet supported by wasm. If you add them here, we can better track and implement them.====="", ""@mattsoulanille unfortunately I don't own the model, it's @yoeo over in https://github.com/yoeo/guesslang/tree/master/guesslang/data/modeland I convert it into a tfjs model.maybe @yoeo could comment on that.====="", 'I took a look at the [converted model.json](https://github.com/microsoft/vscode-languagedetection/blob/main/model/model.json), and it looks like the following ops are used. Ones with a checkmark are supported by the wasm backend, and the ones with a question mark did not appear in the ops matrix (I think they might correspond to another op) :* ? AddV2 (Add and AddN are supported)* ? BiasAdd* [x] Cast* ? ConcatV2 (Concat is supported)* [x] ExpandDims* [x] GatherNd* [x] GatherV2* [x] GreaterEqual* [x] NotEqual* [x] Pack* [x] Prod* [x] Reshape* [x] Select* ? Shape* [x] Slice* [x] Softmax* [ ] SparseFillEmptyRows* [ ] SparseReshape* [ ] SparseSegmentMean* [ ] SparseSegmentSum* ? StatelessWhile* [x] StridedSlice* [ ] StringToHashBucketFast* ? TensorListFromTensor* ? TensorListReserve* ? TensorListStack* [x] Tile* ? Where* [x] ZerosLike* [x] _FusedMatMulIt looks like sparse ops will also need to be implemented, along with StringToHashBucketFast.=====', 'THank you Matt for generating this list, all ops should be supported.ConcatV2 is same as Concat, AddV2, BiasAdd are same as Add.We might need to register Sparse and string ops, which are CPU ops, they can be supported in wasm by forwarding to CPU impl or implement these ops in c++ @ahmedsabie .The rest or control flow ops are supported since they exist in the model executors.=====', ""Just FYI, I started doing model validation for all models I'm loading  it compares ops used by model to ops registered by currently selected backend: <https://github.com/vladmandic/human/blob/980285d36367d3ae903455a4884ff5d8ff9d0ea1/src/models.ts#L90>====="", ""@vladmandic, that validation script looks very useful. @pyu10055, it sounds like we could be doing this sort of validation in tfjs to give better error messages about missing ops (i.e. a complete list of missing ops instead of just the first one encountered). To avoid introducing a breaking change, we could run the validation the first time the model is run on a new backend instead of when it's loaded. This avoids the case where a user loads a model that doesn't work on the current backend but then switches to a backend that supports the model. Alternatively, or in addition, we could make it a warning instead of an error just in case the model is somehow able to run anyway ~(although this should never be the case)~.Edit: Thanks @vladmandic for pointing out cases where you may have ops in the model that are missing from tfjs but never run  during execution.====="", ""> Alternatively, or in addition, we could make it a warning instead of an error just in case the model is somehow able to run anyway (although this should never be the case).@mattsoulanille actually, this does happen sometimes - model may have an execution path that simply does not get triggered.for example, one model I'm using frequently has a missing op `floormod`, but never had an execution error.====="", '@mattsoulanille @vladmandic backend compatibility validation would be great feature, but it might need to be a runtime validation, as some of the control flow functions or others nodes might not be executed at runtime.  I think it might be good feature for our in-development debugger app @jinjingforever =====']",1
https://github.com/tensorflow/tfjs/issues/2553,[webgpu] Devise heuristic for running im2col as separate shader.,2,closed,2019-12-13T13:42:59Z,2021-11-08T01:27:25Z,"As a follow up to https://github.com/tensorflow/tfjs/pull/2208, let's figure out a heuristic for running im2col as a separate shader and then get rid of the environment flag. @qjia7 - maybe you have some initial insights here?","['@qjia7 Can you verify if this issue can be closed? thanks=====', ""After we supported the conv2d vec4 version, I think im2col is not that necessary for us now. Let's close it.=====""]",0
https://github.com/tensorflow/tfjs/issues/5454,TFJS WebGL on WebWorker still blocks GUI,13,open,2021-08-10T20:44:16Z,2021-10-26T01:13:22Z,"After investigating a [report by a user](https://discuss.tensorflow.org/t/how-to-get-the-duration-of-predict/3311/18) who was attempting to use web workers + tfjs to reduce lagging of GUI when model performs inference I have noticed that when using backend ""webgl"" web worker makes no effect on executing code in a separate context as this does not apply to GPU.I have confirmed that by setting backend to ""cpu"" there is no performance issue, as the TFJS execution correctly is executed on a new thread, such that the browser DOM updates are not interfered with. I have also confirmed that browser relies on GPU to update DOM for anything visual - not CPU.Thus the request here is how to limit ""webgl"" execution to leave enough processing for DOM updates for other user tasks to prevent this ""jankiness"" from occuring.Demo of issue:https://codepen.io/jasonmayes/project/editor/DBYaRjSimply change first line of tfWorker.js to import one of:`importScripts('dist/cpu.js'),`or for WebGL backend change to:`importScripts('dist/main.js'), `Confirmed this issue across devices including Windows 10, Desktop, Chrome and also Xiaomi 8 (Android phone with Snapdragon 845 processor)Example output from Chrome Dev tools shows GPU block when using WebGL backend and also that all execution comes from WebWorker.js.![unnamed](https://user-images.githubusercontent.com/4972997/128932303-e422ed1d-ef87-46dc-bc36-ee2aac0fe260.png)","['Will the next version solve this problem???I also encountered this problem=====', 'Team is currently discussing issue to see what may be best path of action for this as WebGL seems to be shared resource unlike like CPU on WebWorker which separates execution so that 2 processes can not eat the resources of the other. Please check back on this bug for updates. In the meantime if you are able to execute on ""WASM"" for your TFJS model or even ""CPU"" I have confirmed these work as CPU based and when in web worker execute on a different thread as intended which does not block the GUI. It is only WebGL backend that is effected by this for now which is often the default form of execution for models if no backend is specified.=====', 'I don\'t understand. Which part of WebGL is not threaded? Can you provide alink to a description of the underlying WebGL issue?You can certainly render to offscreen canvases using a second GL contextwithout blocking the main thread. Sure the GPU is a shared resource, but isit really true that the browser doesn\'t use a separate GL context andoff-screen rendering in the web worker threads? Or is it something aboutTFJS that is improperly using WebGL?On Wed, Aug 11, 2021, 11:59 AM Jason Mayes ***@***.***> wrote:> Team is currently discussing issue to see what may be best path of action> for this as WebGL is not ""threaded"" like you can do with CPU on WebWorker.> Please check back on this bug for updates. In the meantime if you are able> to execute on ""WASM"" for your TFJS model or even ""CPU"" I have confirmed> these work as CPU based and when in web worker execute on a different> thread as intended which does not block the GUI. It is only WebGL backend> that is effected by this for now which is often the default form of> execution for models if no backend is specified.>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/5454#issuecomment-897073644>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAEQE2KWVKC5KWWWYYCBHA3T4LCCZANCNFSM5B43IDPA>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>> .>=====', 'I made a slightly simplified version of the code to the demo the original reporter created here: https://codepen.io/jasonmayes/project/editor/DBYaRj Executing this - click the big button at the bottom - and you will notice the webcam feed freezes as do the red numbers rendered to the left of the button.If I change the TFJS backend to CPU it does not have this ""lag"".I am unsure of the exact reason that is causing this, if you have suggestions of what it could be please do feel free to suggest, but the only difference here is the backend being used indicating something is not sharing the GPU as intended when using WebGL backend.I will follow up with the team as to how WebGL execution works in TFJS when called from a WebWorker context in case something needs to be changed there. =====', 'Seems like there\'s at least an equal chance the bugs are in TFJS as inWebGL. I\'d also suggest testing in other browsers, e.g. Firefox. Some linksdiscussing non-blocking WebGL rendering in web workers:Chrome: https://developers.google.com/web/updates/2018/08/offscreen-canvasFirefox (see WebGL Worker <https://github.com/kripken/webgl-worker> repo):https://research.mozilla.org/2014/07/22/webgl-in-web-workers-today-and-faster-than-expected/TFJS: ***@***.***/webworker-in-tensorflowjs-49a306ed60aaMany more on D3 and other tools with simple web searches.This is an issue that would definitely benefit from some investigation bythe TFJS dev team as performance regressions in WebGL are easy to miss.Updates can break the fast paths, and careless data copies and transferscan easily overwhelm performance gains.On Thu, Aug 12, 2021, 9:53 AM Jason Mayes ***@***.***> wrote:> I made a slightly simplified version of the code to the demo the original> reporter created here: https://codepen.io/jasonmayes/project/editor/DBYaRj>> Executing this - click the big button at the bottom - and you will notice> the webcam feed freezes as do the red numbers rendered to the left of the> button.>> If I change the TFJS backend to CPU it does not have this ""lag"".>> I am unsure of the exact reason that is causing this, if you have> suggestions of what it could be please do feel free to suggest, but the> only difference here is the backend being used indicating something is not> sharing the GPU as intended when using WebGL backend.>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/5454#issuecomment-897802279>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAEQE2IILXYAP4OHDPMVBN3T4P4A3ANCNFSM5B43IDPA>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>> .>=====', ""In my first glance at your example, you are likely to be bound by data transfer rather than compute. You are copying image buffers from the video to the webworker using `postMessage({img: bufferT, taskType: 'tf_model'})`, which copies the bufferT. Instead you should be using the second argument for postMessage to send the buffer as a Transferable. You should also test it out with much larger images, to ensure you are compute bound. I'm not exactly sure what your model is doing, but also make sure that you do enough computation entirely down on the GPU to minimize the texture download costs.I'm not totally sure, but you are also not using any of the resulting computed results, which is a bit artificial. However, that may be fine for this testing, if you want to ignore transfer costs from the GPU back to the DOM.====="", 'The original user who reported the issue can not share their model / all code so instead for now has shared the demo code you see above and put some placeholder code to execute as detailed here in place of the real model that replicates the issue they had: https://discuss.tensorflow.org/t/how-to-get-the-duration-of-predict/3311/18 which is what is being executed when `tfWrapper.processImg(inputFloat), `  is called.```  let testRand = tf.randomNormal([1024 * 1024 * 4]),  let {values, indices} = tf.topk(testRand, 1024 * 1024),  let valueArray = values.arraySync(),  let indArray = indices.arraySync(),```=====', 'I think the next steps are:1. Check the `tf.randomNormal` implementation. I\'d be surprised if that runs on the GPU at all (WebGL does not have random number support). Often this is simulated in a number of ways if you do want it to run within a shader, e.g. using a texture filled with `drand48()` values from the CPU. Regardless, it is likely doing some CPU work and we want to know why it isn\'t doing that work within the Worker thread.2. Check the `tf.topk` implementation to see if that uses a GPU-side reduce or, instead, if it reads back to the CPU and does the work on the CPU, and, again, why this might be blocking the main thread.3. Good to see that we\'re doing readback of the value and indices array, which is a bit better, but I\'d again be very concerned that this test is doing very little GPU work compared to a normal TF network. In order for the GPU to be faster, the amount of work done on the GPU must be much larger than the ""work"" required to transfer the data to the GPU and back again. It is *very* common for simple tests to fail at this and send people off to lala optimization land.One difficult situation will be dealing with the WebGL GPU drivers. If we run a software path for WebGL that runs in the driver, it will most likely run in the main thread.=====', 'As another point of reference: I write a Firefox addon that spawns a ""content page"" for performing model processing, then have the background page interact with it over messages. The content page is more-or-less a normal web page and it blocks for several seconds during the model load and first inference with the WebGL backend as the shaders compile. Specifically, it blocks the entire browser: other tabs cannot be switched to for part of the load time, etc. I always assumed this had to do with some type of resource contention around the main render thread, but do not know the root cause.Unfortunately there is not a Chrome port due to missing WebExtensions API\'s, but if you are curious to see code that exhibits the issue, the code and current model can be [found here](https://github.com/wingman-jr-addon/wingman_jr). I had even just created [a minimal reproduction](https://github.com/wingman-jr-addon/wingman_jr/tree/load-perf-regression-tfjs-3.4.0-minimal-repro-webpage) for the TF.js for a different issue that might be helpful for anyone looking too.=====', 'Any update on this issue? I use tf.signal.stft in a web worker with webgl backend and UI freezes until processing is finished.=====', 'So the team have been researching into how to reduce GPU load when loading models / inference to give time back to browser render to reduce freezing of GUI etc. It seems to be more about how the browser schedules tasks to GPU. From what I understand WebWorker is aimed at CPU multithreading, and does not account for GPU tasks that are spawned from it which are still shared with the main browser level GPU render thread it seems. However focusing on GPU sharing instead we have had some promising results but still working on refining publishable solution to prod. This is actively under investigation though. =====', '@jasonmayes I had quick followup. Some background text:""https://www.tensorflow.org/js/guide/platform_environment""TensorFlow.js executes operations on the GPU by running WebGL shader programs. These shaders are assembled and compiled lazily when the user asks to execute an operation. The compilation of a shader happens on the CPU on the main thread and can be slow. TensorFlow.js will cache the compiled shaders automatically, making the second call to the same operation with input and output tensors of the same shape much faster. Typically, TensorFlow.js applications will use the same operations multiple times in the lifetime of the application, so the second pass through a machine learning model is much faster.TensorFlow.js also stores tf.Tensor data as WebGLTextures. When a tf.Tensor is created, we do not immediately upload data to the GPU, rather we keep the data on the CPU until the tf.Tensor is used in an operation. If the tf.Tensor is used a second time, the data is already on the GPU so there is no upload cost. In a typical machine learning model, this means weights are uploaded during the first prediction through the model and the second pass through the model will be much faster.""1. How do these parts relate to using webworkers for webgl and the above thread? What I noticed is the first time inference is usually five times slower so if these parts such as the compilation can be done without freezing the ui that would be amazing. 2.  I know offscreen canvas is is not available in browsers such as safari/firefox. I think a lot of production use cases will need to support these especially Safari? Can a solution be made with that in mind? IE maybe the compilation part which might not require the canvas reference be made using the webworker so all browsers can see that speed up?Thanks,Rohan=====', 'I shall let @lina128 reply as she is currently working on the final solution to this which looks very promising in terms of that initial load blocking issue you mentioned by allowing time back to the browser render thread to do what it needs to do for updating GUI, and maybe she has some thoughts on q2 too.  Essentially though:In the WebGL API, some methods are blocking, while others are not. These are different from the sync/async concept in JS. The blocking methods in WebGL mean that, certain WebGL entry points cause synchronous stalls on the calling thread (possibly the same calling thread Chrome uses for GUI rendering I believe as everything is rendered via GPU now on Chrome if I remember correctly). Even basic requests can take as long as 1ms, but they can take even longer if they need to wait for all graphics work to be completed.In the new version Na is working on, we first compile and link everything without waiting because they are async, and then check and wait everything at the end, instead of individually, which gives time back to the browser to do other things it needs to perform in that time frame too.=====']",1
https://github.com/tensorflow/tfjs/issues/5449, ReferenceError: FormData is not defined ' when attempting to call a POST endpoint to save a model (tfjs-node -> tfjs-core)),7,open,2021-08-10T10:50:09Z,2021-09-17T18:27:18Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04.2- TensorFlow.js installed from (npm or script link): npm install @tensorflow/tfjs-node- TensorFlow.js version (use command below):  @tensorflow/tfjs-node@3.8.0- Browser version: No browser involved- Tensorflow.js Converter Version: @tensorflow/tfjs-converter@3.8.0**Describe the current behavior**When trying to call an http POST endpoint to save a model, a ReferenceError is thrown at @tensorflow/tfjs-core/dist/tf-core.node.js:8270:    ReferenceError: FormData is not defined (see log at end of post for full stacktrace) **Describe the expected behavior**The code should execute without throwing a ReferenceError**Standalone code to reproduce the issue**```/*Dependencies: npm install @tensorflow/tfjs-node*/tf = require(""@tensorflow/tfjs-node"") function create_and_compile_model(){    let res_model = tf.sequential({        layers: [            tf.layers.dense({inputShape: [2], units: 1, activation: 'sigmoid'})        ]    }    )    res_model.compile({optimizer: tf.train.adam(learningRate=0.05), loss:tf.losses.sigmoidCrossEntropy}) // bce=Binary cross entropy    return res_model  }    async function main(){    model = create_and_compile_model()    await model.save(tf.io.http('http://localhost:3000/save_model'), {method: 'POST'})    } main()```Note: I did not use the standard saving code from the docs, which would be`await model.save('http://localhost:3000/save_model') ` because then I got errors concerning not being able to find save handlers for the URL, although the endpoints were working. I figured this was related to https://github.com/tensorflow/tfjs/issues/723, but could not get it to work with any combination of imports, so opted for this approach. If the explicit usage of tf.io.http makes no sense, feel free to let me know and I'll further debug my initial approach with the standard model.save(url) instead.Note: this is the maximally reduced version of the actual code. The actual code creates an express server with get_model and save_model endpoints. Obviously this minimal example is not working code as the endpoint does not exist, but it already generated the error. I can provide working stand-alone code (up to this bug) with the express server if relevant.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Full traceback:```/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8270                        init.body = new FormData(),                                        ^ReferenceError: FormData is not defined    at HTTPRequest.<anonymous> (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8270:41)    at step (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:82:23)    at Object.next (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:63:53)    at /nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:56:71    at new Promise (<anonymous>)    at __awaiter (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:52:12)    at HTTPRequest.save (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8260:16)    at Sequential.<anonymous> (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:19791:60)    at step (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:9723:23)    at Object.next (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:9704:53)```I made a first attempt at debugging by adding the FormData dependency to tf-core.node.js, but then another similar error occurred a few lines later. I think the code is not executing some require statements, but I don't know where it's going wrong.","['The above method should work , I believe this is a bug ![image](https://user-images.githubusercontent.com/43972606/128973948-401fba79-72bd-47e5-a65c-6e518ee1bf4d.png)=====', 'I have the same issue with the model.save() method which is giving ReferenceError: FormData is not defined.  In the node modules (node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js) form-data is required.=====', ""If anyone has an intuition as to what the underlying problem could be and could point me in the right direction, I'd be more than happy to try my hand at fixing this====="", 'https://www.npmjs.com/package/form-data says from version 3.x FormData has dropped support for node@4.x. Not sure, but this might be the reason for the reference error.=====', ""I did some more research, and I think I've got the problem resolved, albeit in an ad-hoc manner. If one of the devs confirms this makes sense, I'll be happy to make a pull request so you can check if it breaks anything else.The issue is that Node.js does not natively have FormData, while browserside Javascript does. The same applies for Blob. Two changes are required to fix the issue, both to tf-core.node.js:1. Add the following line at the start of the file:`const FormData = require('form-data'),`(note: this might break browerside Javascript code, in case this code is supposed to run browserside as well)2. There is already some logic present for detecting when node is being used rather than browserside Javascript:```// Use Buffer on Node.js instead of Blob/atob/btoavar useNodeBuffer = typeof Buffer !== 'undefined' &&    (typeof Blob === 'undefined' || typeof atob === 'undefined' ||        typeof btoa === 'undefined'),```After these lines I added the following, to explicitly replace all usage of Blob by Buffer:```if(useNodeBuffer){    Blob = Buffer }```Note: at first sight the variable useNodeBuffer was introduced to solve the issue of Blob not being present in Node. However, it seems to be implemented in a way which is prone to issues. I think the original idea was to include an if-statement whenever Blob or something similar was used, and to implement different logic for Node than for browserside Javascript. But obviously it's easy to forget this, which I think is what happened when implementing the logic for creating that http request. Sources:https://stackoverflow.com/questions/63576988/how-to-use-formdata-in-node-js-without-browserhttps://stackoverflow.com/questions/14653349/node-js-cant-create-blobs====="", '@HDegroote thank you for details , will you be interested in submitting a PR ?=====', '@rthadur I hadn\'t realised the original source was in typescript: my debugging was on the generated Javascript file, and my proposed solution for Blob doesn\'t seem to work with typescript due to different types.I\'ve never worked with typescript, and I\'m struggling because whatever I do I keep getting type errors. Could anyone who knows typescript have a quick look? I think it\'s a fairly easy fix to make (although probably indicative of a larger issue in the project, because this will happen wherever Blob or FormData is used on Node). The relevant file is ""tfjs-core/src/io/http.ts"". The changes to make are:1. add `const FormData = require(\'form-data\'),` at the top (and ensure this doesn\'t break browser-based javascript)2. Either monkey-patch Blob or introduce an explicit ""if"" for the two usages around line 100, in function ` async save(modelArtifacts: ModelArtifacts): Promise<SaveResult>`=> For the monkeypatch solution see https://stackoverflow.com/questions/14653349/node-js-cant-create-blobs,  or simply replace by Buffer as I did in my previous comment=> For the explicit \'if\', the code should be something like the following (with useNodeBuffer defined as in my previous comment):```    if(useNodeBuffer){      init.body.append(\'model.json\', new Buffer.from([JSON.stringify(modelTopologyAndWeightManifest)], { type: JSON_TYPE }), \'model.json\'),    }    else{        init.body.append(\'model.json\', new Blob([JSON.stringify(modelTopologyAndWeightManifest)], { type: JSON_TYPE }), \'model.json\'),    }```A cleaner solution would probably use a makeBlob function, and put the \'if\' there, to reduce code duplication.In case it\'s relevant, I\'m using the following code to create a server to handle the save_model requests: https://gist.github.com/HDegroote/44bf3bfe919b0a473f4a52ca4ea5b4d6 =====']",1
https://github.com/tensorflow/tfjs/issues/5937,[tjfs-node] Unsupported system: cpu-linux-arm64 in raspberry pi 4,7,open,2021-12-13T11:06:42Z,2021-12-17T07:42:49Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster) (raspberry pi 4B)- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.12.0 Installing tfjs-node, raspberry pi gave `Unsupported system: cpu-linux-arm64`.I used node.js v17.2.0I ran `npm install @tensorflow/tfjs-node` and this gave the error.This is the full error: ```npm ERR! code 1npm ERR! path /home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-nodenpm ERR! command failednpm ERR! command sh -c node scripts/install.jsnpm ERR! CPU-linux-3.12.0.tar.gznpm ERR! * Downloading libtensorflownpm ERR! /home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:106npm ERR!     throw new Error(`Unsupported system: ${libType}-${platform}-${os.arch()}`),npm ERR!           ^npm ERR! npm ERR! Error: Unsupported system: cpu-linux-arm64npm ERR!     at getPlatformLibtensorflowUri (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:106:11)npm ERR!     at downloadLibtensorflow (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:139:15)npm ERR!     at async run (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:208:5)npm ERR! npm ERR! Node.js v17.2.0```","['Please check this issue #5173 , this has been resolved with latest versions.=====', 'It was the latest version of tensorflow.=====', ""I tried with node.js v16 and v17, and they didn't work.====="", '@Ddongddi can you please share your package.json ?=====', '```{  ""name"": ""coronapredict"",  ""version"": ""1.0.0"",  ""description"": """",  ""main"": ""index.js"",  ""scripts"": {    ""test"": ""echo \\""Error: no test specified\\"" && exit 1""  },  ""author"": """",  ""license"": ""ISC""}```My package.json file=====', '@Ddongddi we have support for arm v7 32 bit linux, is your RPI running on arm 64 linux?=====', '> @Ddongddi we have support for arm v7 32 bit linux, is your RPI running on arm 64 linux?Yes. I am running arm 64 debian=====']",1
https://github.com/tensorflow/tfjs/issues/4325,missing ops in kernel cause misleading error message,10,closed,2020-12-01T12:20:13Z,2021-01-20T13:11:13Z,"If an op is missing in kernel, error messages printed are not helpful at all.I have a simple home-made `tf.sequential` model that works with `webgl` and `cpu` backends, but fails with `wasm` backend:```logbackend.ts:665 Uncaught (in promise) Error: 'step' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.ts:665)    at BackendWasm.step (backend.ts:418)    at step.ts:48    at engine.ts:625    at engine.ts:433    at Engine.scopedRun (engine.ts:444)    at Engine.tidy (engine.ts:431)    at kernelFunc3 (engine.ts:625)    at engine.ts:639    at Engine.scopedRun (engine.ts:444)```Finally I've traced it to `recurrentActivation` set to `relu` inside a layer instead of default `hardSigmoid` - you'd never guess that based on the message!Also, I understand that some ops are not implemented in all kernels, but is there a full matrix of ops per kernel?Environment: TFJS 2.7.0 in Chrome 87","['cc @pyu10055 @lina128 =====', 'linking to #4500=====', 'update: tested with **tfjs 2.8.3**, error message has changed but result is still a failure:```engine.js:487 Uncaught (in promise) Error: Error running Step: Neither modular kernel nor forward func passed  runKernelFunc\t@\tengine.js:487  runKernel\t@\tengine.js:391  step_\t@\tstep.ts:47  step__op\t@\toperation.ts:51  x\t@\tRelu_grad.ts:29  (anonymous)\t@\ttape.js:128  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  (anonymous)\t@\tengine.js:843  backpropagateGradients\t@\ttape.js:128  (anonymous)\t@\tengine.js:841  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  gradients\t@\tengine.js:837  variableGrads\t@\tgradients.js:252  computeGradients\t@\toptimizer.js:82  minimize\t@\toptimizer.js:38  (anonymous)\t@\ttraining.js:1118  (anonymous)\t@\ttraining_tensors.js:197  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  tidy\t@\tglobals.js:175  fitLoop\t@\ttraining_tensors.js:188```@pyu10055 this looks related to your work in #4511 ?=====', '@vladmandic Step op was added to WASM in the latest (2.8.3), can you give it another try, see the error message makes more sense now? thanks=====', ""@pyu10055 i've noticed it was added and i've tried, but i can't make sense of the new error message:```engine.js:487 Uncaught (in promise) Error: Error running Step: Neither modular kernel nor forward func passed```note that same code works just fine with webgl backend.====="", ""Hi @vladmandic, the missing op is implemented and merged into our codebase, but hasn't been released yet. I will be in 2.8.4, likely happen later this week.====="", ""@lina128 thanks - i thought it was released in tfjs 2.8.3, my bad. i'll wait for 2.8.4====="", 'No problem, @vladmandic. You can track what is released using our release notes: https://github.com/tensorflow/tfjs/releases/tag/tfjs-v2.8.3=====', 'closing as fixed in tfjs 2.8.5=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4325"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4325"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2844,Errors when loading models from storage.googleapis.com,3,closed,2020-03-07T06:27:27Z,2020-03-10T15:33:16Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.6.0#### Browser versionFirefox 73.0.1(64bit)#### Describe the problem or feature requestWhen loading the bodypix model MobileNetV1 with stride 32, it always returns HTTP ERROR 404, regardless of quantification and multiplier.One of the error example is the URL below:`https://storage.googleapis.com/tfjs-models/savedmodel/bodypix/mobilenet/float/075/model-stride32.json`#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['I think the docs for this are wrong and stride 32 is _not_ support with Mobilenet. cc @tylerzhu-github =====', 'Thx Yannick for answering this. The MobileNet backbones have been changed to have stride 16 and 8 only.=====', '@tylerzhu-github thx for responding (to this and the PR)=====']",0
https://github.com/tensorflow/tfjs/issues/132,[Community Idea]  Kaggle alternative for Deeplearn/JavaScript,1,closed,2018-04-07T14:46:24Z,2018-10-24T19:59:52Z,"_From @QuantumInformation on February 23, 2018 14:19_The python folks are pretty lucky in that they have Kaggle. For example there are cool tutorials like sohttps://www.kaggle.com/sohier/how-to-integrate-bigquery-pandas where they query data. In that example they query a REST service and perform a whole bunch of cool stuff. It would be awesome if the js community had such a tool._Copied from original issue: tensorflow/tfjs-core#773_","['Closing this, out of scope of this library.=====']",0
https://github.com/tensorflow/tfjs/issues/4397,JS Error in demo,1,closed,2020-12-13T14:46:21Z,2020-12-14T21:08:02Z,**System information**Demo URL mentioned in the link (https://github.com/tensorflow/tfjs-models/tree/master/qna)https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html is not working!**Describe the expected behavior**I didn't get answers to any question. Attaching the error here.![demoerror](https://user-images.githubusercontent.com/2010921/102015121-af880500-3d7f-11eb-9f09-eef093a6cbaa.JPG),"['There is a similar issue [here](https://github.com/tensorflow/tfjs/issues/3861) for tracking , will close this and track it at one place.=====']",1
https://github.com/tensorflow/tfjs/issues/4208,WASM heap overflow,5,open,2020-11-10T13:54:12Z,2021-02-16T21:26:23Z,"tfjs-backend-wasm@2.7.0tf-backend-wasm.node.jsI'm trying to use backend-wasm with bodypix. It's hard for me to provide a simple test case.At some point BackendWasm.prototype.makeOutput = function (shape, dtype, memoryOffset) {        var dataId,        if (memoryOffset == null) {            dataId = this.write(null /* values */, shape, dtype),        callsBackendWasm.prototype.write = function (values, shape, dtype) {        var dataId = {},        this.move(dataId, values, shape, dtype), which overwrites the memory holding the displayed html page and hangs.It does not extend the heap or return a ""heap full"" error. (I don't know what it's supposed to do.)","['cc @annxingyuan =====', 'not sure it helps, but i\'ve noticed that `wasm` backend requires some time to *""warmup""* to avoid internal timeouts and if you try with lower resolution input on first inference, it will run.  and if you then increase it to desired resolution, it will still work.  but if you start with high resolution, it will fail exactly like you wrote.  much less issues with `simd` variation - it\'s much faster, so far less chance of this happening.=====', 'Thanks, @vladmandic, I will experiment with this idea. Seems to confirm some problem with heap management.=====', ""@dennis508 fyi, this is my investigation so far (only reported yesterday, so it's early) <https://github.com/vladmandic/human/issues/26>====="", 'This code insertion eliminates the crash:tf-backend-wasm.node.jsBackendWasm.prototype.move = function (dataId, values, shape, dtype) {...var memoryOffset = this.wasm._malloc(numBytes),if (memoryOffset == 0) throw new Error(""Heap overflow""), // Insert this line....If something is leaking memory or allocating excessively or if 2GB heap is just too small I cannot say.=====']",0
https://github.com/tensorflow/tfjs/issues/4481,Use tfjs 2.8.2 executeAsync show 'length' undefined error,4,closed,2021-01-04T12:35:17Z,2021-01-06T00:31:41Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.8.2- Browser version: Google Chrome. Version 87.0.4280.88**Describe the current behavior**I use a train model by myself. When I use Tensorflow.js 2.7.0, it was ok. But, when I use the latest Version Tensorflow.js 2.8.2(script link: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs), use code **model.executeAsync**, it show an error: **Uncaught (in promise) TypeError: Cannot read property 'length' of undefined****Describe the expected behavior**I just want to use as Tensorflow.js 2.7.0 as well**Standalone code to reproduce the issue**Here is the code:```const MODEL_URL = ""./niu_web_model/model.json"",const model = await tf.loadGraphModel(MODEL_URL),const cat = document.getElementById(""cat""),model   .executeAsync(tf.browser.fromPixels(cat).expandDims(0))   // error when execute to here   .then((modelres) => {                             console.log(modelres[6].dataSync(),modelres[7].dataSync())    }),```error show:```Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at Vz (tfjs:17)    at Object.kernelFunc (tfjs:17)    at p (tfjs:17)    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.runKernelFunc (tfjs:17)    at e.t.runKernel (tfjs:17)    at topk_ (tfjs:17)    at topk__op (tfjs:17)    at tfjs:17```It 's ok when use in Tensorflow.js 2.7.0.","[""I got a similar error, but when running tf.topk() on my prediction results instead.What seemed to fix it was running .print() on the inference output tensor.This works:```const prediction = model.predict(x),prediction.print(),const topPreds = tf.topk(prediction, 3, true),````But this throws an error```const prediction = model.predict(x),const topPreds = tf.topk(prediction, 3, true),Unhandled Promise rejection: Cannot read property 'length' of undefined , Zone: <root> , Task: Promise.then , Value: TypeError: Cannot read property 'length' of undefined    at topKImpl (TopK_impl.js:22)    at Object.topK [as kernelFunc] (TopK.js:24)    at kernelFunc (engine.js:440)    at engine.js:504    at Engine.scopedRun (engine.js:333)    at Engine.runKernelFunc (engine.js:502)    at Engine.runKernel (engine.js:391)    at topk_ (topk.js:57)    at Module.topk__op (operation.js:44)```No clue why the error is being thrown without the print though.====="", '@zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?=====', '> @zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?See here:https://codepen.io/miksvasiljevs/pen/VwKxpGKLet me know if there are any issues with the pen or anything.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1415,Can't use tfjs-vis in a React app due to Preact typings collision,3,closed,2019-03-19T14:26:22Z,2019-08-12T21:23:31Z,"Depending on tfjs-vis from our React app causes trouble.  We use @types/react, but tfjs-vis uses preact, which provides colliding global JSX definitions.The problem started with tfjs-vis 0.5.1 (and persists in 1.0.x).  I don't understand why things worked with tfjs-vis 0.5.0, still trying to pin down any relevant difference.Some background on the issue:https://github.com/developit/preact/issues/1036https://github.com/Microsoft/TypeScript/issues/18588 (maybe?)","['Updating issue after convo with David. Workaround is to delete preact.d.ts in node_modules/preact/dist and node_modules/preact/srcI suspect the preact 8.* typings are just incompatible with react 16s latest typings. But can dig deeper when I get a chance. Either way preact typings should not be relevant for consumers of tfjs-vis.=====', 'Yep, thanks.  An effective workaround is to add this to package.json:    ""scripts"": {        ""postinstall"": ""rimraf node_modules/preact/*/preact.d.ts""    }That way the irrelevant preact types are automatically deleted after every ```yarn```, and everything works fine.  I think this is sufficient to close the bug?=====', ""Thanks David! I might keep it open for a bit just to remind me to investigate typings drift between the projects (though I do think your solution is the right one, react users would always want to use react types). So I might close it soon (maybe i'll remove the bug tag since it has a solution). =====""]",0
https://github.com/tensorflow/tfjs/issues/5173,"Issue Raspberry install tfjs-node 3.7.0 , wrong ELF class: ELFCLASS64",18,closed,2021-06-04T13:46:57Z,2021-08-12T14:57:49Z,"When I install tfjs-node, and run it```bashWelcome to Node.js v14.16.0.Type "".help"" for more information.> const tf = require('@tensorflow/tfjs'),undefined> require('@tensorflow/tfjs-node')node-pre-gyp info This Node instance does not support builds for Node-API version 8node-pre-gyp info This Node instance does not support builds for Node-API version 8Uncaught:Error: /home/pi/program/nanocptscenariorunner/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1122:18)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)>```When doing```bashnpm rebuild @tensorflow/tfjs-node ....gyp verb node dev dir /home/pi/.cache/node-gyp/14.16.0gyp verb `which` succeeded for `make` /usr/bin/makegyp info spawn makegyp info spawn args [ 'V=1', 'BUILDTYPE=Release', '-C', 'build' ]../binding/tfjs_backend.cc: In function ‘TFE_TensorHandle* tfnodejs::CreateTFE_TensorHandleFromStringArray(napi_env, int64_t*, uint32_t, TF_DataType, napi_value)’:../binding/tfjs_backend.cc:196:64: error: ‘TF_TString’ was not declared in this scope                                          array_length * sizeof(TF_TString))),                                                                ^~~~~~~~~~../binding/tfjs_backend.cc:198:15: error: ‘t’ was not declared in this scope....```Nodejs: 14.13Raspbian lite strechRaspberry 3 armv7l ==> 32 bits","['@franckOL there is a mis-match between the binding (TF 2.4.1) and RPI TF binary (v1.4.0)We will try to build a new RPI TF binary (2.4.1). TF have dropped official support for RPI (no new releases), If you have manually built before, please share your steps. thanks=====', 'In fact I understand than `npm rebuild @tensorflow/tfjs-node` is the rebuilding process, but as you see I have another issue on that (`TF_TString` unknown symbol)=====', ""@pyu10055 @franckOL I'm running into the same issue in Raspberry p3I'm running tfjs-node v3.3.0 due to a regression bug with v3.7 as suggested by @pyu10055 https://github.com/tensorflow/tfjs/issues/5161#issuecomment-854972551,has this been resolved for v3.7?What to do to get it to run in RPi```> node index.jsnode:internal/modules/cjs/loader:1167  return process.dlopen(module, path.toNamespacedPath(filename)),                 ^Error: /server/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (node:internal/modules/cjs/loader:1167:18)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at Module.require (node:internal/modules/cjs/loader:997:19)    at require (node:internal/modules/cjs/helpers:92:18)    at Object.<anonymous> (/server/node_modules/@tensorflow/tfjs-node/dist/index.js:60:16)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14) {  code: 'ERR_DLOPEN_FAILED'}```====="", '@playground As you see a PR is done to fix that, already merged, just waiting for release 3.7.x=====', 'Thanks @franckOL, I was also checking with @pyu10055 if the other regression bug has been addressed so I can run v3.7.x.=====', 'Any updates on this?=====', 'cc @mattsoulanille =====', 'Test today version 3.8.0 : always wrong ELF class: ELFCLASS64=====', 'V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.=====', '> V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.@playground dans rpi4 is 64 bits platform ?=====', '> > V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.> > @playground dans rpi4 is 64 bits platform ?oh yes ==> Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHzhttps://www.raspberrypi.org/products/raspberry-pi-4-model-b/specifications/but rpi3 64 bit tooQuad Core 1.2GHz Broadcom BCM2837 64bit CPUTry to clean everything and see.I work on rapsbian lite.=====', ""@franckOL rpi4 running ubuntu 32bit and  NX 64bit, I'm using SavedModel.====="", 'Just try again, a clean env on raspberry 3 , raspbian : ```Welcome to Node.js v14.16.0.Type "".help"" for more information.> const tf = require(\'@tensorflow/tfjs\')undefined> require(\'@tensorflow/tfjs-node\')node-pre-gyp info This Node instance does not support builds for Node-API version 8node-pre-gyp info This Node instance does not support builds for Node-API version 8Uncaught:Error: /tmp/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1122:18)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)```=====', '@franckOL I\'m running my app with  node v14.17.3 and in a docker container with node v16.3.0 and both loads up fine```platform  linuxStarted on 30002021-08-02 13:09:39.283490: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./model2021-08-02 13:09:40.358427: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }2021-08-02 13:09:40.358568: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./model2021-08-02 13:09:44.264451: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.2021-08-02 13:09:44.837456: W tensorflow/core/platform/profile_utils/cpu_utils.cc:118] Failed to find bogomips or clock in /proc/cpuinfo, cannot determine CPU frequency2021-08-02 13:09:50.320029: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.321330: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.322138: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.330052: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.335342: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.622648: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./model2021-08-02 13:09:53.412700: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }, Status: success: OK. Took 14129215 microseconds.loading time:  ./model, 25695.750468969345version:  { name: \'Demo model\', version: \'1.0.0\' }./public/images/image.pngpredictions: 1 {  detectedBox: [ \'0.026\', \'0.328\', \'0.215\', \'0.538\' ],  detectedClass: \'hard-hat\',  detectedScore: \'1.000\'}time took:  19462.35897707939``````FROM arm32v7/node:16.3.0RUN apt-get update -y && apt-get install -y apt-utils && apt-get install fswebcam -y && apt-get install mpg123 -y && apt-get install vim -yWORKDIR /serverCOPY . /serverRUN npm install -g npmRUN npm installVOLUME /demo_model_mms_helper_shared_volumeEXPOSE 3000CMD [ ""npm"", ""start"" ]```=====', '@franckOL could you please update if you are still facing same issue ?=====', '> @franckOL could you please update if you are still facing same issue ?I update my version nodejs form 14.17.1 to 14.17.3. And it is **OK**=====', 'Thank you , closing this issue .=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5173"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5173"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/109,Allow multiple instances of tf,1,closed,2018-04-05T18:30:18Z,2018-06-29T12:37:06Z,"_From @HalfdanJ on January 18, 2018 18:17_If there are two instances of deeplearnjs present (eg. in two different modules node_modules) they can easily both be loaded, and they both try to register backend. Currently that fails with `Uncaught Error: cpu backend was already registered`, could it instead just re-use the already registered backend?_Copied from original issue: tensorflow/tfjs-core#562_","['This also occurs if the user imports `tf` more than once in a webpack project (for example, in multiple components using angular-cli).  =====']",0
https://github.com/tensorflow/tfjs/issues/1310,MobileNet models: why are only v1_${alpha}_224 are being stored on public storage bucket?,1,closed,2019-03-02T14:41:28Z,2019-03-14T20:21:51Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser version#### Describe the problem or feature requestfrom what i understand, you can in retrieve mobilenet models urls that are:https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v${version}_${alpha}_${size}/However, only those which size is 224 are being stored there.I would be very interested in being able to access smaller size (96 or 124) models, as i constantly run into problems trying to access/load/use them otherwise. #### Code to reproduce the bug / link to feature request","[""We only host the 224 version since that was the only version that we ported using the TF.js Converter tool at that time (this was before TF Hub).If you find the version you want on TF Hub, (e.g. [v2_050_160](https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/classification/2)), you can load it like this:```tsconst modelUrl =    'https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/classification/2',const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true}),const zeros = tf.zeros([1, 224, 224, 3]),model.predict(zeros).print(),```=====""]",0
https://github.com/tensorflow/tfjs/issues/5847,Unsupported dtype in weight 'unknown_147': float16,1,closed,2021-11-15T00:00:56Z,2021-11-16T20:40:03Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.js- TensorFlow.js version (use command below): 3.11.0- Browser version: Chrome 95- Tensorflow.js Converter Version: tensorflowjs 3.11.0, Dependency versions: keras 2.7.0,  tensorflow 2.7.0**Describe the current behavior**I'm trying to port OpenAI's CLIP model to tfjs, and I've used the following conversion command:```!tensorflowjs_converter --input_format tf_saved_model ./clip-text-vit-32-tf ./clip-text-vit-32-tfjs```[Here's the TF Saved Model](https://drive.google.com/drive/folders/1-7VMCP6OSpCLqBRu0qL7DW66Lk95yX8I?usp=sharing) that I'm trying to convert. When running the model with tfjs I get this error:![image](https://user-images.githubusercontent.com/1167575/141703862-615bb3dd-8266-4a9a-9281-60adc849b038.png)**Describe the expected behavior**If it's due to float16 not being supported, then I'd have expected `tensorflowjs_converter` to either automatically convert the float16 values to float32 like it seems to do with other unsupported datatypes, or throw an error telling me that this model cannot be converted into a tfjs model.**Standalone code to reproduce the issue**1. Download this HTML file: https://github.com/josephrocca/openai-clip-js/blob/main/tfjs-text-demo.html2. Download the tfjs model files from [this Google Drive link](https://drive.google.com/drive/folders/1-GI6-OTDiJcjYKTavoobbubc9BYjQDzW?usp=sharing) (or generate them using the above-linked Saved Model + the `tensorflowjs_converter` command). Ensure the folder is named `clip-text-vit-32-tfjs`.3. Put the tfjs model folder in the same directory as the HTML file and run a static file server (e.g. `deno run --allow-net --allow-read=. https://raw.githubusercontent.com/josephrocca/denoSimpleStatic/master/main.ts --port=3001`)","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5847"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5847"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1462,Running posenet on video input,4,closed,2019-03-28T10:20:19Z,2020-09-02T19:28:01Z,"I was trying to implement 2D posenet implementation using ml5.js wrapper for machine learning. I wanted to do pose-estimation on a video stored locally on my PC . However , the output skeleton seems to be quite off and the predictions seem to be wrong.I am using p5js webeditor . [https://i.stack.imgur.com/fP5Yi.jpg](url)I have the following questions1. What parameters can be modified ?2. Limitations of ml5.js wrapper ?3. Is there limitations on posenet if the subject is not facing the camera ? In the test video, there are different camera angles.4. Is posenet suitable for video input stream from a webcamera ?The offset is as follows : ```var video,var videofile = 'test.mp4',let poseNet,let poses = [],let skeletons = [],var imageScaleFactor= 0.3,var outputStride=16 ,var minConfidence = 0.5,var maxPoseDetections = 1,var scoreThreshold = 0.5 ,var multiplier = 0.75,function setup() {  createCanvas(400, 400),  video = createVideo(videofile,onLoad),  //Posenet  poseNet = ml5.poseNet(video,imageScaleFactor,outputStride,minConfidence, maxPoseDetections,scoreThreshold,multiplier),  console.log(ml5),  //poseNet.on('pose', gotPoses),  poseNet.on('pose', function (results) {    poses = results,    }),    video.hide(),}function onLoad() {     // This function is called when the video loads//  print(""start auto play after load""),//  video.play(),  print(""mouse click to start""),}function modelReady() {  console.log('model ready'),}//Pose Net function gotPoses(poses) {   console.log(poses),}function draw() {  image(video, 0, 0, width, height),  // We can call both functions to draw all keypoints and the skeletons drawKeypoints(),  drawSkeleton(),}//=======Draw skeleton and keypoints ===/// A function to draw ellipses over the detected keypointsfunction drawKeypoints()  {  // Loop through all the poses detected  for (let i = 0, i < poses.length, i++) {    // For each pose detected, loop through all the keypoints    for (let j = 0, j < poses[i].pose.keypoints.length, j++) {      // A keypoint is an object describing a body part (like rightArm or leftShoulder)      let keypoint = poses[i].pose.keypoints[j],      // Only draw an ellipse is the pose probability is bigger than 0.2      if (keypoint.score > 0.1) {        fill(255, 0, 0),        noStroke(),        ellipse(keypoint.position.x, keypoint.position.y, 10, 10),      }    }  }}// A function to draw the skeletonsfunction drawSkeleton() {  // Loop through all the skeletons detected  for (let i = 0, i < poses.length, i++) {    // For every skeleton, loop through all body connections    for (let j = 0, j < poses[i].skeleton.length, j++) {      let partA = poses[i].skeleton[j][0],      let partB = poses[i].skeleton[j][1],      stroke(255, 0, 0),      line(partA.position.x, partA.position.y, partB.position.x, partB.position.y),    }  }}//======End skeleton ============/function mousePressed() {  video.loop(), // set the video to loop mode ( and start )  print(""set loop mode""),}```","['Hi @basicvisual. I think your question may be better answered by the ml5 folks over at https://github.com/ml5js/ml5-library/issues, could you try posting your question over there?=====', 'Thanks for the pointers, I am now trying to run a sample program , if possible within posenet. I was wondering if it was possible to run a video locally instead of running it on the web camera =====', '@basicvisual no problem. To answer your question about running on posenet on a video file rather than locally yes that is possible. Looking at your [stack overflow](https://stackoverflow.com/questions/55381969/ambigous-results-using-ml5-js-wrapper-for-implementation-of-posenet-example) question it looks like you have already gotten it working on video.Also have you read this blog post https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5, it has a few other pointers on using PoseNet. To your first question, i think this page https://ml5js.org/docs/PoseNet should answer what params can be changed and what they do.=====', ""Hi @tafsiri ! The links you posted here are not available any more. I've met the same problem with running PoseNet on the video file but from the Android Device (tensor flow lite). Can you provide some additional info to solve this issue? =====""]",0
https://github.com/tensorflow/tfjs/issues/4846,[rn-tfjs] TensorCamera memory leak on iOS,1,open,2021-03-21T05:35:14Z,2021-07-08T23:12:38Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- I used the code from the camera demo to extract the pose : https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsx- - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iOS- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 2.7.0**Describe the current behavior**When I compile my app with expo, and display the performance monitor, I observe that every-time the app is fast refreshed, the RAM increases of 20MB.  So I start around 300 MB and after several fast refresh I reach +600 MBThe memory leak comes from the TensorCamera, since I don't observe any RAM increase when I comment it out. RAM increases steadily even when TensorCamera doesn't receive any prop.**Describe the expected behavior**No increase of the RAM between two fast refresh. **Standalone code to reproduce the issue**https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsxthen```expo start```and then fast refresh by saving several times the same file.",['@valentinchelle  I was just curious if you had faced any issue on running the realtime_demo.tsx file. I have some error executing it https://github.com/tensorflow/tfjs/issues/5304====='],1
https://github.com/tensorflow/tfjs/issues/5276,Build tfjs-converter with Bazel,1,closed,2021-07-01T20:30:16Z,2021-09-08T20:42:33Z,"This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package, take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275",['converter has been migrated to Bazel.====='],1
https://github.com/tensorflow/tfjs/issues/5866,tf.ready() throwing JSON Parse error with react-native 0.66.3,6,open,2021-11-18T03:35:56Z,2021-12-11T02:48:02Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.6- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iOS Simulator- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.11.0- CUDA/cuDNN version:**Describe the problem**After spinning up a fresh react-native app and installing tfjs [per the instructions](https://www.npmjs.com/package/@tensorflow/tfjs-react-native) I'm getting the following error:<img width=""300"" alt=""CleanShot 2021-11-17 at 20 24 13@2x"" src=""https://user-images.githubusercontent.com/53795920/142347037-29340711-81c6-4afd-b182-e2d2fdb2f893.png"">**Provide the exact sequence of commands / steps that you executed before running into the problem**1. Spin up a brand new react native app2.  Follow tfjs react native instructions**Any other info / logs**Minimal reproduction: https://github.com/kateinkim/tfjs-rn-example","['Sometimes we experience the same issue when just calling tf.ready() after starting react native app (iOS 15 device).@tensorflow/tfjs: 3.11.0react-native: 0.65.1=====', 'same for me - both android and iOS (real devices):```    ""@react-native-async-storage/async-storage"": ""^1.15.14"",    ""@tensorflow/tfjs"": ""^3.11.0"",    ""@tensorflow/tfjs-react-native"": ""^0.8.0"",    ""expo"": ""^43.0.3"",    ""expo-camera"": ""~12.0.3"",    ""expo-gl"": ""~11.0.3"",    ""expo-gl-cpp"": ""~11.0.1"",    ""react"": ""17.0.2"",    ""react-native"": ""0.66.3"",    ""react-native-fs"": ""^2.18.0"",```**solution**:downgrade react-native version to the [latest officially supported by expo](https://docs.expo.dev/versions/latest/#each-expo-sdk-version-depends-on-a) ( **0.64.3** for expo 43.0.3 )**extra steps due to react-native downgrade:****_iOS_**: Podfile -> comment this line if exists:``` # __apply_Xcode_12_5_M1_post_install_workaround(installer) ```**_Android_**: android/build.gradle``` allprojects {    repositories {        ...        // add next line. WARNING! jcenter will shutdown on February 2022        jcenter(),    }}```[Example](https://github.com/raienko/holmes/pull/1/files)=====', '@tafsiri  or @lina128   any insight here?  This seems to be a blocker for our dev to make progress on a React Native component library.=====', 'Sorry for the delay.. I plan to take a look at this issue today or early next week. Thanks for the patience!=====', ""Hi folks,I took a closer look and it looks like the error happens right after the tfjs-react-native backend creates the GLView context ([here](https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/src/platform_react_native.ts#L171)). We actually found this problem before, and that's why we mentioned the working version combination [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native). I will move this paragraph to the main tfjs repo so people can easily see it. I also found a [similar issue](https://github.com/expo/expo/issues/15390) reported in the expo repo.  As @raienko mentioned, the solution is to downgrade. Sorry I don't have any better solutions for now. I will keep an eye on the expo repo and test the newer versions again when the problems on their side are fixed.@kateinkim just FYI that due to the usage of GLView (expo-gl), tfjs-react-native can only be tested on real devices, not simulators. Thanks!====="", 'Thanks @jinjingforever! I tested it on a real device before but didn’t work so downgrade it is!=====']",1
https://github.com/tensorflow/tfjs/issues/4793,Error: Cannot find name 'EmscriptenModule'. when using wasm as a backend in Angular,1,closed,2021-03-09T10:26:28Z,2021-03-09T18:19:54Z,"### Error: node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.d.ts:18:44 - error TS2304: Cannot find name 'EmscriptenModule'.I got this error while executing this line in Angular.`import '@tensorflow/tfjs-backend-wasm',`Also, I installed and activated Emscripten SDK (version 2.0.14), EMSDK path also declared.`./emsdk activate 2.0.14&& source ./emsdk_env.sh&& echo $EMSDK`>   '/home/emsdk'","['@sivarajakani i see you already commented in a related issue https://github.com/tensorflow/tfjs/issues/3866#issuecomment-793419787 , will wait for @jinjingforever response. Thank you for your patience.=====']",1
https://github.com/tensorflow/tfjs/issues/900,Feature Request: Mirror padding op,2,closed,2018-11-11T15:58:28Z,2020-06-05T05:27:42Z,"Currently trying to port Magenta's arbitrary style transfer to the browser and I'm just missing reflection padding. Just wondering if this is on the roadmap or something? If it's not a priority, I'll probably take a stab at it when I have time. Zero padding works fine anyway, minus a few weird border artifacts..","['Hi @reiinakano , it looks like the padding strategy should be added to to op, to match the API in python tensorflow.   If someone wants to take a crack at this, the relevant code is here:https://github.com/tensorflow/tfjs-core/blob/e3b9a344a216fb556a9b6a31e3f926163f6b7a6f/src/ops/array_ops.ts#L617=====', 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/4995,navigator.hardwareConcurrency and native pthread gets different thread pool size,1,closed,2021-04-27T05:19:32Z,2021-04-28T18:00:39Z,"(On Intel i7-9700)By navigator.hardwareConcurrency: https://github.com/tensorflow/tfjs/pull/4994, the WASM_THREAD_POOL_SIZE is 4. WASM_THREAD_POOL_SIZE is got by:```Math.min(4, Math.max(1, (navigator.hardwareConcurrency || 1) / 2)),```By native pthread:  https://github.com/tensorflow/tfjs/pull/4942, the WASM_THREAD_POOL_SIZE is 1. WASM_THREAD_POOL_SIZE is got by:```// emscripten_num_logical_cores corresponds to navigator.hardwareConcurrency.// Many x86-64 processors have 2 threads per core, so we are dividing by 2.#ifdef __EMSCRIPTEN_PTHREADS__int num_cores = emscripten_num_logical_cores() / 2,#elseint num_cores = 1,#endifint min_num_threads = 1,int max_num_threads = 4,int thread_pool_size =  // thread_pool_size is WASM_THREAD_POOL_SIZE    std::min(std::max(num_cores, min_num_threads), max_num_threads),```Does this possible due to #ifdef __EMSCRIPTEN_PTHREADS__?BTW, code has rebased, and https://github.com/tensorflow/tfjs/pull/4957 is included.Reproduce steps:1.  Use local wasm:```+++ b/e2e/benchmarks/local-benchmark/index.html@@ -91,7 +91,7 @@ limitations under the License.   <script src=""https://unpkg.com/@tensorflow/tfjs-backend-webgl@latest/dist/tf-backend-webgl.js""></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-layers@latest/dist/tf-layers.js""></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-converter@latest/dist/tf-converter.js""></script>-  <script src=""https://unpkg.com/@tensorflow/tfjs-backend-wasm@latest/dist/tf-backend-wasm.js""></script>+  <script src=""../../../tfjs-backend-wasm/dist/tf-backend-wasm.js""></script>```2.  Build:cd tfjs-backend-wasmtfjs-backend-wasm$ yarn & yarn build-npmcd ../e2ee2e$ yarn build-deps3. Serup servercd ../../npx http-server4.  Open http://127.0.0.1:8080/tfjs/e2e/benchmarks/local-benchmark/","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4995"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4995"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3282,NCHW Support for conv2d on loaded Frozen Model  while computing bias,7,closed,2020-05-17T14:07:58Z,2020-07-30T05:45:57Z,"#### TensorFlow.js version""@tensorflow/tfjs"": ""^1.7.4""#### Browser versionChrome 81.0.4044.138#### Describe the problem or feature requestIt is said that tfjs has ability to support NCHW for  conv2d on loaded Frozen Model. Now I load a frozen  model in  NCHW, it throws out an error when i execuate my model:**Error: Operands could not be broadcast together with shapes 1,32,220,220 and 32.**![image](https://user-images.githubusercontent.com/9960612/82149825-e188c580-9889-11ea-8612-1fab51593b4f.png)I debug the code and find that the output shape of conv2d is [1, 32, 220, 220] and the dataformat is indeed NCHW. **The error occurs in node BiasAdd with shape [32].**![image](https://user-images.githubusercontent.com/9960612/82149750-cfa72280-9889-11ea-8a30-54f03868aaa9.png)![image](https://user-images.githubusercontent.com/9960612/82149942-07ae6580-988a-11ea-9e5e-d4c6d4033fb1.png)I run the same code with model in NHWC format and it went well.**Does the BaisAdd node in conv2d really support NCHW?**","['@vonlyinno Thank you for reporting, unfortunately our biasAdd does not support NCHW data format right now. What is the version of Tensorflow.js you are using?The is a chance that your conv2d + biasAdd could be fused into FusedConv2d Op by the latest converter >= 1.7.4, in which NCHW should be supported.=====', '@pyu10055 Thanks for your reply.However I do use the latest version 1.7.4 of @tensorflow/tfjs.=====', '@vonlyinno What is the version of converter you used for converting your model? can you also share your conversion command line? thanks=====', ""@pyu10055 I use `pip install tensorflowjs` as instruction and the version is `1.7.4r1`.convert command line:`tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='output' /xxx/SR_x2_RGB.pb  /xxx/rgb_model`====="", 'does this frozen model target CPU? Is it possible for you to generate a CPU version of this model? I believe tensorflow CPU does not support NCHW conv2d.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/138,Add tf.strided_slice op,2,closed,2018-04-07T14:58:18Z,2018-07-12T17:44:27Z,"_From @pyu10055 on March 13, 2018 22:43_I have noticed many estimators are generating this op.https://www.tensorflow.org/versions/master/api_docs/python/tf/strided_slice_Copied from original issue: tensorflow/tfjs-core#851_","[""This is the most common missing op I've seen, let's prioritize this.====="", 'the op has been added https://github.com/tensorflow/tfjs-converter/pull/135=====']",0
https://github.com/tensorflow/tfjs/issues/5110,Backend WASM does not implement kernel op `mod`,3,open,2021-05-22T15:45:41Z,2021-11-05T17:26:30Z,"As subject line says, trivial math operation `tf.mod` is missing from `tfjs-backend-wasm`...```logindex.js:646 Error: Kernel 'Mod' not registered for backend 'wasm'    at Engine.runKernel (engine.ts:540)    at mod_ (mod.ts:63)    at mod__op (operation.ts:51)    at executeOp (arithmetic_executor.ts:45)    at operation_executor.ts:61    at engine.ts:467    at Engine.scopedRun (engine.ts:478)    at Engine.tidy (engine.ts:465)    at tidy (globals.ts:192)    at operation_executor.ts:60```Issue found during porting and optimizing MobileNet-v3 with CenterNet object detection model: <https://github.com/vladmandic/mb3-centernet>Environment: TFJS 3.6.1 with WASM backend with Edge/Chromium 90 on Windows 10","['I can work on this =====', 'cc: @rthadur @pyu10055 =====', ""workaround until #5317 is resolved:```js    const kernelMod = {      kernelName: 'Mod',      backendName: 'wasm',      kernelFunc: (op) => tf.tidy(() => tf.sub(op.inputs.a, tf.mul(tf.div(op.inputs.a, op.inputs.b), op.inputs.b))),    },    tf.registerKernel(kernelMod),```=====""]",1
https://github.com/tensorflow/tfjs/issues/2128,Red Error when import tfjs-react-native,3,closed,2019-10-02T11:50:20Z,2020-02-22T06:51:52Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versiontfjs-react-native#### Browser versionIn my packages.json file```  ""dependencies"": {    ""@react-native-community/async-storage"": ""^1.6.2"",    ""@tensorflow/tfjs"": ""^1.2.9"",    ""@tensorflow/tfjs-react-native"": ""0.1.0-alpha.2"",    ""expo-gl"": ""^7.0.0"",    ""expo-gl-cpp"": ""^7.0.0"",    ""react"": ""16.9.0"",    ""react-native"": ""0.61.1"",    ""react-native-unimodules"": ""^0.7.0-rc.1""  },```#### Describe the problem or feature requestWhen I am trying to use the demo code in STEP 6 of [tfjs-react-native documentation ](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native), this error shows:![image](https://user-images.githubusercontent.com/22137687/66041631-34d42080-e512-11e9-94ed-99413c044f12.png)Then I found out, this probelm is caused by this import statement:```import '@tensorflow/tfjs-react-native',```By removing this statement, the build process run through, but I think this miss the point to use the tfjs-react-native adopter, so what is going wrong here?#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['Hi, this looks like an issue in react-native-unimodules. Did you follow all the setup and configuration instructions, in particular for android there are some steps outlined here https://github.com/unimodules/react-native-unimodules#-configure-androidWere any issues reported when setting up react-native-unimodules and expo-gl? =====', 'Hi, @tafsiri Yes, setup the project with react-native-unimodules solved the import problem. Now the app is built without any error. Many thanks.=====', 'No problem. Feel free to let us know how using the package goes.=====']",0
https://github.com/tensorflow/tfjs/issues/5876,Processing locally stored image for custom object detection in tensorflow/tfjs-react-native,12,closed,2021-11-22T15:37:33Z,2021-12-16T16:46:24Z,"I am building custom object detection on locally stored images into a React Native app using Tensorflow.js and would be **grateful for help processing the image to a tensor before I call model.predict()**. My code (shown below) to access a local image, process it to a tensor, and perform inference has worked nicely for a custom _classification_ model (MobileNetV2). However, I am now getting a dtype error when I apply the same code to my custom _object detection_ model (SSD MobileNet V2 FPNLite 640x640).> Error: The dtype of dict['input_tensor'] provided in model.execute(dict) must be int32, but was float32```useEffect(() => {    (async () => {      await tf.ready(),      const modelJson = require(""../assets/VisModels/model_m.json"")      const modelWeight = require(""../assets/VisModels/model_m_weights.bin"")      const model = await tf.loadLayersModel(bundleResourceIO(modelJson,modelWeight)) //for classification      //const model = await tf.loadGraphModel(bundleResourceIO(modelJson,modelWeight)) //for object detection      setModel(model)    })(),}, []),const fileUri = **uri of locally stored jpeg image**       const imgB64 = await FileSystem.readAsStringAsync(fileUri, { encoding: FileSystem.EncodingType.Base64, }),const imgBuffer = tf.util.encodeString(imgB64, 'base64').buffer,const imageData = new Uint8Array(imgBuffer),const IMGSIZE = 640,const imageTensor = decodeJpeg(imageData).expandDims().resizeBilinear([IMGSIZE,IMGSIZE]).div(tf.scalar(255)).reshape([1,IMGSIZE,IMGSIZE,3])const prediction = await model.predict(imageTensor).data()```Changing the tensor to int32 produces this error: > Error: This execution contains the node 'StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow/Where', which has the dynamic op 'Where'. Please use model.executeAsync() instead. Alternatively, to avoid the dynamic ops, specify the inputs [StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow/Reshape]]Using model.executeAsync() produces this error:> TypeError: model.executeAsync(imageTensor).data is not a function.This is a managed expo (v43.02) workflow with react-native v0.64.3, @tensorflow/tfjs v3.11, and @tensorflow/tfjs-react-native v0.8.0. **Any help in properly setting up the imageTensor would be greatly appreciated!**","['Is there a more appropriate place to ask this question? Thanks!=====', 'Hi @brianspiesman, sorry for the delay.. I have been busy with other projects. `executeAsync` returns a `Promise` of `Tensor` (if input is a single tensor), so you probably need to do the following:```jsconst result = await model.executeAsync(imageTensor),const prediction = result.dataSync(),```Please give it a try. Thanks! =====', ""@jinjingforever thank you so much for your suggestion. However, I am back to the initial error: > Unhandled promise rejection: Error: The dtype of dict['input_tensor'] provided in model.execute(dict) must be int32, but was float32I am processing the raw image for the object detection model in the same way as for my classification model. Should the images be processed differently depending on whether they will be used for object detection or classification? Thank you for any further suggestions.====="", 'Good question. Is it possible to share your object detection model? I can take a closer look. Thanks!=====', 'Sure, what is the best way to get it to you? The model is about 450kb and the weights are about 12mb.=====', 'Maybe a github repo (along with your code would be great)? Thank you=====', '@jinjingforever: Ah yes, of course. [Here is a link to a repository with code for a test version of the app.](https://github.com/brianspiesman/objectDetection) The model is in the assets/VisModels directory and the code for importing the OD model and running inference is in components/ImagePicker.js. Thank you for your patience as I am new to using github.=====', 'Hi @brianspiesman,I took a look at the model and it is indeed has [int input](https://user-images.githubusercontent.com/8752427/144651811-b8c099cb-d077-4332-89db-73380086c04b.png). (I am using [netron](https://github.com/lutzroeder/netron) to visualize and inspect the model). I am not super familiar with the training and model conversion process, but our own [coco-ssd model](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd) also has int input.So to make your code work, you can simply add ""toInt"" to convert the input tensor to int type:```jsconst imageTensor = decodeJpeg(imageData).expandDims().resizeBilinear([IMGSIZE,IMGSIZE])    .div(tf.scalar(255)).reshape([1,IMGSIZE,IMGSIZE,3])    .toInt(), // <----```Also, the return value of `model.executeAsync` in this case will be an array of `tf.Tensor` because the model has multiple output tensors. You will need to process each one individually. Something like: ```jsconst result = await model.executeAsync(imageTensor),console.log(result[0].dataSync(), result[1].dataSync()),```You can take a look at how our model processes the result [here](https://github.com/tensorflow/tfjs-models/blob/master/coco-ssd/src/index.ts#L125-L133). Our model probably has different outputs from yours, due to some [extra optimization steps](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd#technical-details-for-advanced-users).Thank you! Let me know if you have questions.=====', '@jinjingforever I think this is working. Thank you! However, as you say, I receive an output of 8 tensors and I am not sure which ones to use. I would like extract the prediction value, the box coordinates, and the class (however, there is only 1 class in this OD model). Here is an example of the output:[Output array of 8 tensors](https://github.com/brianspiesman/objectDetection/blob/main/Output%20array)[tensor0](https://github.com/brianspiesman/objectDetection/blob/main/Tensor0) shape: [1, 100][tensor1](https://github.com/brianspiesman/objectDetection/blob/main/Tensor1) shape: [1, 100, 4][tensor2](https://github.com/brianspiesman/objectDetection/blob/main/Tensor2) shape: [1, 100][tensor3](https://github.com/brianspiesman/objectDetection/blob/main/Tensor3) shape: [1, 100, 2][tensor4](https://github.com/brianspiesman/objectDetection/blob/main/Tensor4) shape: [1, 100][tensor5](https://github.com/brianspiesman/objectDetection/blob/main/Tensor5) shape: [1][tensor6](https://github.com/brianspiesman/objectDetection/blob/main/Tensor6) shape: [1, 51150, 4][tensor7](https://github.com/brianspiesman/objectDetection/blob/main/Tensor7) shape: [1, 51150, 2]In your link above, I see your model processing results in 2 tensors: [0] the scores and [1] the boxes. Can you tell which of the tensors in my output correspond to scores and boxes? Thank you for any further guidance.=====', ""Hi @brianspiesman:I think one way to do it is to look at the doc of the original model you converted this tfjs model from. For example, this TF object detection [model](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1). If you scroll to the bottom, you can see what different output tensors mean. If you don't know the original model, I think the model I linked might give you some ideas. Here is a [full list of object detection models](https://tfhub.dev/tensorflow/collections/object_detection/1) I found on tfhub.In this case, I think tensor1 is probably the bounding box. One of the tensor0, tensor2, and tensor4 is probably the class index. Please give them a try.Thanks!   ====="", '@jinjingforever this is perfect! Thank you for all your help on this. It is very much appreciated.=====', 'Closing this , please @mention to reopen if issue still persists. Thank you =====']",1
https://github.com/tensorflow/tfjs/issues/5689,WebGPU Performance Issues,14,closed,2021-10-03T12:03:12Z,2021-12-13T11:52:19Z,"i just tried new `tfjs-backend-webgpu` *0.0.1-alpha.8* on `tfjs` *3.9.0*   *environment: chrome 96 canary on windows 11*first, great job on adding tons of new ops - from perspective of supported kernel ops, `webgpu` is becoming usable!however, switch to WGSL is anything but useful so far - it comes as a **major performance degredation**  overall, `webgpu` has  gotten slower than `webgl`  (and `webgl` itself has become significantly slower since `tfjs` 3.4.0 - this is discussed separately in several open issues)  not to mention that new work that has gone into `webgl` to make it manageable (enable uniforms) has no effect on `webgpu`comparing warmup times  (fyi, my app by default uses 8 simple models running in parallel - total models size is actually tiny, below 30mb):- `webgl` *(default settings)*  > **14 sec** (double the value with uniforms enabled)- `webgl` with `WEBGL_PACK_DEPTHWISECONV=false` and `WEBGL_USE_SHAPES_UNIFORMS=true`  > **7 sec** (pretty good)- `webgpu` *(default settings)*  > **25 sec** (this is incredibily slow)- `webgpu` with `WEBGPU_USE_GLSL=true`  > **15 sec** (already slower than webgl)- `wasm` (no real warmup, included for refrerence only)  > **2 sec***imo, when developing new backend, goal should be that its better than the previous one - not just that it passes unit testsif `webgpu` is not significantly improved, it will be a d.o.a. once released*cc @qjia7 and @xhcao due to work on webgpu  cc @pyu10055 as assignee on webgl performance degradation issue","[""@vladmandic Thanks for the good comments and data, as always! Chrome 94 was released on Sep 21, with WebGPU Origin Trial support. This means in addition to Chrome Canary, we may use Chrome Stable (still need option --enable-unsafe-webgpu) for WebGPU experiment now. Unfortunately,  Chrome decided not to support GLSL anymore for WebGPU  (changes happened in master so all the release channels would be impacted, including Canary and Stable), so WGSL is the only one that can be consumed now. We always align well with WebGPU development (My team also heavily contributes to WebGPU spec, CTS and Chrome impl) and started the TFJS GLSL to WGSL transition in June. After fixing many critical perf issues in Chrome (e.g., workgroup memory init perf regression) together with Google and working around perf issues in TFJS (e.g., hardware limits), we finished the transition after 3+ months of work.Internally we have daily track of performance against almost all the workloads defined in TFJS e2e benchmarks. Before switching to WGSL, we double-checked there was no performance regression regarding to warmup time and run time. For sure, due to resources, we could only cover very limited platforms (Actually only Intel Coffee Lake and Tiger Lake are under daily test), and very limited workloads. We'd like to hear more details from your side (e.g., hardware configuration) to understand the regression. We'll investigate right after our holidays (We are off from Oct 1 to 7 for National Day Holidays). BTW,1. The uniform idea was already implemented in WebGPU backend. Google thought it great, so we're bringing it to WebGL backend.2. Comparing with WebGL, compiled shaders couldn't be cached in Chrome. We already raised this implementation issue to Chrome and it's going to take a while for its implementation (not easy). Thanks again for your valuable feedback, hopes to hear more details from your side about warmup regression (e.g., hardware configuration), and looks forward to more collaborations in the future!====="", ""Thank you for the notes, here are full details  I've created an automated test so its easy to check all scenarios...  # Performance TestingEnvironment: `tfsj` **3.9.0** and `tfjs-backend-webgpu` **0.0.1-alpha.8**Hardware: Notebook with Intel Coffee Lake i7-8750 and nVidia GTX 1050Ti## Notes- `WebGPU` `GSLS` code has been recently removed and cannot be compared with new `WGSL`- `WebGL` warmup has massive benefit of ~80% of browser shader caching- `WebGPU` warmup has little benefit of ~12% of browser shader caching- `WebGPU` is much faster on inference compared to `WebGL`- `WebGPU` is faster to warmup than `WebGL` in most cases     Except when `WebGL` shaders are cached in browser cross-session and uniforms are enabled   `WebGL` is 2x faster than `WebGPU` in that scenario showing necessity of caching support- `WebGL` performance benefits of uniforms is massive at 2x and I dont see any side-effects     Will this be enabled by default in the future?- `WebGL` packing caused massive performance regression in TFJS in 3.4.0 (3.3.0 is last unaffected version)    There are several open issues, but no progress?- Using `tf.zeros` as input is convinient, but does not produce realistic results    Test using real input image to excercise real-world model execution path## Test Results```js{ message: 'initial', warmup: 3134, inference: 2638, tfjs: '3.9.0', backend: 'wasm', tensors: 304, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 3119, inference: 2618, tfjs: '3.9.0', backend: 'wasm', tensors: 304, agent: 'Chrome/94', env: [] }{ message: 'initial', warmup: 11836, inference: 61, tfjs: '3.9.0', backend: 'webgl', tensors: 304, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 2665, inference: 60, tfjs: '3.9.0', backend: 'webgl', tensors: 304, agent: 'Chrome/94', env: [] }{ message: 'initial', warmup: 6128, inference: 54, tfjs: '3.9.0', backend: 'webgl', tensors: 304, agent: 'Chrome/94', env: [ { WEBGL_PACK_DEPTHWISECONV: false }, { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'cached', warmup: 1202, inference: 67, tfjs: '3.9.0', backend: 'webgl', tensors: 304, agent: 'Chrome/94', env: [ { WEBGL_PACK_DEPTHWISECONV: false }, { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'initial', warmup: 5018, inference: 23, tfjs: '3.9.0', backend: 'webgpu', tensors: 304, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 4454, inference: 22, tfjs: '3.9.0', backend: 'webgpu', tensors: 304, agent: 'Chrome/94', env: [] }```## IssuesUsing `WebGPU` backend is causing a lot of warnings although execution seems to work:```text> warning Binding size bigger than maximum uniform buffer binding size: binding 0 given 146313216 bytes, maximum is 16384 bytes    at ValidateBufferBinding (../../third_party/dawn/src/dawn_native/BindGroup.cpp:114)    at ValidateBindGroupDescriptor (../../third_party/dawn/src/dawn_native/BindGroup.cpp:290)    at CreateBindGroup (../../third_party/dawn/src/dawn_native/Device.cpp:1043)```## ReproductionFully automated test in `NodeJS` using `puppeteer` and reproducible anytime  Code available at <https://gist.github.com/vladmandic/fbdcaf7fe2e2add5c33b98936d4d5740>====="", ""Above post is using single model (can be re-tested using any model, I've used **Inception v4** trained on **ImageNet 1k**)However, when I try `WebGPU` backend on my demo app, it runs at **~3 FPS** average while `WebGL` runs at **~9 FPS**  **that is 300% negative difference** in inference performance!  My best guess is that some ops get executed on CPU thus causing major slowdown  You can try using following URLs:- <https://vladmandic.github.io/human/demo/index.html?backend=webgl>- <https://vladmandic.github.io/human/demo/index.html?backend=webgpu>====="", '@vladmandic Can you put the Inception v4 model somewhere that I can access? It seems that `http://wyse:10010/models/imagenet/inception-v4/model.json` is in your local server. The webgpu warning seems like a bug in our implementation.And for your demo app, I can reproduce the bad performance for webgpu. Thanks for the reporting. I will take a look. =====', ""> Can you put the Inception v4 model somewhere that I can access? To keep it reproducible with a readily available public model, you can use any mid-complexity model,  here's an example with **EfficientNet-B5** from **TFhub**: <https://tfhub.dev/google/efficientnet/b5/classification/1>   (just convert from *TFSavedModel* to *TFJSGraphModel* using `tensorflowjs_converter`)  ```js{ message: 'initial', warmup: 2645, inference: 1908, tfjs: '3.9.0', backend: 'wasm', tensors: 394, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 2330, inference: 1808, tfjs: '3.9.0', backend: 'wasm', tensors: 394, agent: 'Chrome/94', env: [] }{ message: 'initial', warmup: 20148, inference: 107, tfjs: '3.9.0', backend: 'webgl', tensors: 394, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 5374, inference: 105, tfjs: '3.9.0', backend: 'webgl', tensors: 394, agent: 'Chrome/94', env: [] }{ message: 'initial', warmup: 7428, inference: 119, tfjs: '3.9.0', backend: 'webgl', tensors: 394, agent: 'Chrome/94', env: [ { WEBGL_PACK_DEPTHWISECONV: false }, { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'cached', warmup: 2053, inference: 103, tfjs: '3.9.0', backend: 'webgl', tensors: 394, agent: 'Chrome/94', env: [ { WEBGL_PACK_DEPTHWISECONV: false }, { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'initial', warmup: 5087, inference: 64, tfjs: '3.9.0', backend: 'webgpu', tensors: 394, agent: 'Chrome/94', env: [] }{ message: 'cached', warmup: 4427, inference: 70, tfjs: '3.9.0', backend: 'webgpu', tensors: 394, agent: 'Chrome/94', env: [] }```As you can see, data is pretty much the same as with **Inception v4** model  (even bigger impact of WEBGL packing and uniforms, but numbers tell the same story)  > And for your demo app, I can reproduce the bad performance for webgpuI've traced it down - there are couple of places where WebGPU is a touch slower than WebGL,  but by far the biggest issue is `tf.image.nonMaxSuppressionAsync`  `WebGL` runs **NMS** in **~25 ms** and `WebGPU` runs **NMS** in **~135 ms** (over **5x** slower)  FYI NMS function params are:```jsboxes.shape = [896, 4]scores.shape = [896]maxOutputSize = 1iouThreshold = 0.1scoreThreshold = 0.2```Also, it seems like `WebGPU` has some additional execution latency?  In more complex models, that is not visible since overall execution time is faster than `WebGL`  But with very simple models that execute in near-real-time `WebGPU` is slower than `WebGL`For example, running a `requestAnimationFrame` loop on [**BlazeFace**](https://tfhub.dev/tensorflow/tfjs-model/blazeface/1/default/1) `model.execute()`:- `WebGL`: 12 ms / frame- `WebGPU`: 20 ms / frame====="", ""@vladmandic Thanks for the detailed information. I can run your benchmarks using `EfficientNet` now. Some comments below:1. I didn't meet the warning `warning Binding size bigger than maximum uniform buffer binding size: binding 0 given 146313216 bytes, maximum is 16384 bytes` using `EfficientNet`. Maybe it's `Inception v4` specific. If you can further narrow down which op with what kind of input shapes introduce this warning, it will be helpful for us.2. The `cached` is total unimplemented in chrome browser. See [gpuweb:2111](https://github.com/gpuweb/gpuweb/issues/2111), [dawn:549](https://bugs.chromium.org/p/dawn/issues/detail?id=549). So we have to wait the browser to support it. In TFJS level, we will see if we can further reduce the shader variance to reduce the warmup time.3. For `tf.image.nonMaxSuppressionAsync`, it may be not the culprit. Current, `nonMaxSuppressionAsync` only runs on cpu. There is no gpu kernel for it. So I guess the slowness is caused by the previous ops before `nonMaxSuppressionAsync`. `nonMaxSuppressionAsync` triggers all ops in gpu must finish execution. What kind model are you executing before `nonMaxSuppressionAsync`?4. For small size model, webgpu does have performance issue. We notice that the current conv2d/matmul is not efficient for irregular inputs, like M,N is small, K is very large. Or inputs height/width is smaller than filter height/width. We are working on this kind of shapes optimization. Will update here once we have progress. Thanks. ====="", ""> The cached is total unimplemented in chrome browser. See gpuweb:2111, dawn:549. So we have to wait the browser to support itThanks  I took a look and current approach by Chrome team doesnt seem very encouraging and the thread on the spec itself is idle for 8 months :(> In TFJS level, we will see if we can further reduce the shader variance to reduce the warmup timeMuch appreciated!> For small size model, webgpu does have performance issue. We notice that the current conv2d/matmul is not efficient for irregular inputs, like M,N is small, K is very large. Or inputs height/width is smaller than filter height/width. We are working on this kind of shapes optimization. Will update here once we have progress. Thanks.Thanks for confirming  > For tf.image.nonMaxSuppressionAsync, it may be not the culprit. Current, nonMaxSuppressionAsync only runs on cpu. There is no gpu kernel for it. So I guess the slowness is caused by the previous ops before nonMaxSuppressionAsync. nonMaxSuppressionAsync triggers all ops in gpu must finish execution. What kind model are you executing before nonMaxSuppressionAsync?You're right, perf problem is basically **ANY** first TF opereration executed in JS code (outside of the model) - there is a massive latency penalty  In my previous post, `nonMaxSuppressionAsync` just happened to be the one  Simple reproduction:```js  const numIterations = 50,  const arr = new Uint8Array(imageData?.data.buffer), // input data in my case is 4k imageData, but can be any dataset  const t0 = performance.now(),  for (let i = 0, i < numIterations, i++) {    const rgba = tf.tensor(arr, [imageData.width, imageData.height, 4], 'int32'), // create rgba tensor    const rgb = tf.slice3d(rgba, [0, 0, 0], [-1, -1, 3]), // strip alpha channel    const tensor = tf.expandDims(rgb, 0), // create standard image tensor [1, height, width, 3]    // const data = await tensor.array(), // download data from gpu    tf.dispose([rgba, rgb, tensor]), // just dispose everything  }  const t1 = performance.now(),  const avgTime = Math.round((t1 - t0) / numIterations),  console.log({ backend: tf.getBackend(), average: avgTime }),```This loop in `WebGPU` is about **3x** slower than in `WebGL`  Setting `tf.ENV.set('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE', 0)` reduces latency by 50%, but its **still slower** than `WebGL`Note that when disabled line that downloads data back from GPU is enabled, both `WebGL` and `WebGPU` slow down a lot since downloading data is slow (expected)  BUT - overal execution becomes faster for `WebGPU` than `WebGL` - WebGPU is fast, its just initial latency thats a killer  So running models in WebGPU is faster than WebGL, but preparing inputs and processing outputs adds huge penalty at the moment> I didn't meet the warning ...> Maybe it's Inception v4 specific.> If you can further narrow down which op with what kind of input shapes introduce this warning, it will be helpful for us.I'm getting warning even running this simple code from above, no model execution at all  Message is slightly different in Chrome 97 vs 94 and maximum binding size is much bigger, but error is pretty much the same:```logwarning Binding size (146313216) is larger than the maximum binding size (134217728). - While validating entries[0] as a Buffer - While validating [BindGroupDescriptor] against [BindGroupLayout] - While calling CreateBindGroup([BindGroupDescriptor]).DATA:  warning [BindGroup] is an error.    at ValidateObject (../../third_party/dawn/src/dawn_native/Device.cpp:473)    at ValidateSetBindGroup (../../third_party/dawn/src/dawn_native/ProgrammablePassEncoder.cpp:116)    at operator() (../../third_party/dawn/src/dawn_native/ComputePassEncoder.cpp:184)    at FinishInternal (../../third_party/dawn/src/dawn_native/CommandEncoder.cpp:1035)```There are no errors logged in `about://gpu` in Chrome====="", ""@qjia7 did you have a chance to look at the `webgpu` latency issues i've mentioned? tests above is from 22 days ago.====="", ""@vladmandic Sorry, I can't reproduce the latency issue you mentioned. From the code snippet you paste, it basically does nothing and is executed instantaneously in my side.```  const numIterations = 50,  const arr = new Uint8Array(imageData?.data.buffer), // input data in my case is 4k imageData, but can be any dataset  const t0 = performance.now(),  for (let i = 0, i < numIterations, i++) {    const rgba = tf.tensor(arr, [imageData.width, imageData.height, 4], 'int32'), // create rgba tensor    const rgb = tf.slice3d(rgba, [0, 0, 0], [-1, -1, 3]), // strip alpha channel    const tensor = tf.expandDims(rgb, 0), // create standard image tensor [1, height, width, 3]    // const data = await tensor.array(), // download data from gpu    tf.dispose([rgba, rgb, tensor]), // just dispose everything  }  const t1 = performance.now(),  const avgTime = Math.round((t1 - t0) / numIterations),  console.log({ backend: tf.getBackend(), average: avgTime }),```Some update for the warmup time:- In tfjs level, we are reducing some shader variants firstly from binary ops #5791. Will start others when we get each operator's exact shader compilation time. Some challenges for us are that 1) it's hard to measure each operator's shader compilation time. Details can be found [here](https://bugs.chromium.org/p/chromium/issues/detail?id=1265616), [here](https://github.com/gpuweb/gpuweb/issues/2236). 2) browser devtools also [lack this support](https://bugs.chromium.org/p/dawn/issues/detail?id=1167). Fortunately, chrome just supported it in chrome://tracing. We can continue this work once it's ready in canary.- In webgpu level, google developer has started the work to [cache pipeline in memory](https://bugs.chromium.org/p/dawn/issues/detail?id=549). ====="", '@qjia7 thanks for the update!  and the description of the chromium queue handling is sounds like it could be the same root cause for what i\'m seeing as extreme latency issues  for reproduction, im guessing your test failed since `imageData` was empty, but you can even use `tf.zeros` to reproduce  no model needed, nothing - just a trivial loop  here\'s a live link: <https://vladmandic.github.io/tfjs-utils/src/latency-issue.html>  and output on my notebook:```textuser agent: Mozilla/5.0 (Windows NT 10.0, Win64, x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4689.0 Safari/537.36tf version: 3.11.0-20211102backend: webgl | total time: 4556 ms | average time: 91 msbackend: webgpu | total time: 13058 ms | average time: 261 ms```basically, for any ""real"" work, `webgpu` is really fast - but for simple stuff that is done in js code outside of the model, latency is a killer - its 300% slower than `webgl`  unfortunately, in real-world, any model inference is followed by some post-processing in js and that is where this impact becomes as showstopper  setting `tf.ENV.set(\'WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE\', 0)` reduces the latency by 50%,  but its still nowhere as fast as `webgl`=====', '@vladmandic Thanks for your live case. I can reproduce it now. After debugging, I find the time mainly costs on `queue.writeBuffer`. It seems that this API needs to be optimized in browser for big data uploading. I reported a bug to chromium https://bugs.chromium.org/p/chromium/issues/detail?id=1266727.=====', ""@vladmandic Jiawei in our team has fixed the `queue.writeBuffer` issue in chromium. You can retest your example https://vladmandic.github.io/tfjs-utils/src/latency-issue.html with latest chrome canary (--enable-unsafe-webgpu). The webgpu backend becomes much faster than before. In my machine, the latency is not that obvious now. We are also trying to use `mapAsync` instead of `writeBuffer`, which shows better perf than webgl with your example. But it brings another [issue](https://github.com/tensorflow/tfjs/pull/5928#discussion_r765587353). It's still in discussion, but hope we can provide the most performant solution soon.And for the long warmup time, we drafted the prototype for the parallel compilation, showing almost 4x speedup for the warmup time. Currently, we are discussing how to expose this capability uniformly between webgl and webgpu.  Will keep you updated.====="", ""Thanks @qjia7!I've tested with Chrome 99 and latency is gone - **WebGPU** now performs on-par with **WebGL**I'm looking forward to other proposed changes (once issue is resolved) for `mapAsync` in #5928And I'm guessing you're still discussing how to align changes from #5826 with changes from #5815 ?Anyhow, I'm closing this issue as resolved...====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5689"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5689"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5468,webgpu backend does not support sync calls,7,closed,2021-08-11T16:44:22Z,2021-10-13T12:20:10Z,"since this is documented in code as by-design, i'm flagging it as a feature request instead of a bug  but in reality its a blocker for future `webgpu` adoption  ```jsconst data = tensor.dataSync(),```results in error:```logindex.js:883 Error: WebGPU readSync is only available for CPU-resident tensors.    at WebGPUBackend.readSync (backend_webgpu.ts:338)    at Engine.readSync (engine.ts:1206)    at Tensor.dataSync (tensor.ts:364)```i know same functionality can be achieved by using```jsconst data = await tensor.data(),```but that requires quite a lot of changes to existing apps - so unless even when `webgpu` is widely available, it will not be able to use it as a plug-and-play instead of existing `webgl` backend  even worse, `tf.tidy()` does not support async calls. which means that memory management became soo much more complicated.and there are other JS functions that don't support async/await calls, for example `array.map()`, so any iterator would have to be reworked to use for loops.also note that usage of `dataSync()` is not accidental to start with since its faster for very small tensors and where blocking main thread is irrelevantall-in-all, support for sync calls is a must or webgpu is a non-starter.environment: chrome/94 canary, tfjs 3.8.0, tfjs-backend-webgpu 0.0.1-alpha.7","[""@vladmandic It is a known issue for us that webgpu does not support sync read. Another similar issue is #5092. Currently WebGPU spec only provides the async one `mapAsync`. There are some discussions whether to support a `readBuffer` API in WebGPU https://github.com/gpuweb/gpuweb/issues/1972. But it properly continues after the WebGPU MVP. We will follow up this issue and provide some inputs from TFJS side that we need the sync read API badly. For now, I'd suggest you try to use `data()` instead `dataSync` in your application. It's glad to know that you are trying the webgpu backend. Do you see any other problems that webgpu may be missing?====="", ""thanks for the reference linki'll play with `webgpu` on a side, but i can't test it is any of my existing code due to extensive use of `tf.tidy` which doesn't support async calls.and rewritting code to use `await tensor.data()` would not be trivial as main reason for `tf.tidy` is so i can use chained calls.```jsconst scores = tf.tidy(() => predictions.slice([0, 0], [-1, 1]).sigmoid().squeeze().dataSync()),```or```jsconst data = tf.tidy(() => input.cast('float32').div(255).sub(0.5).expandDims(0).arraySync()),```without `tf.tidy()` this would be 4 temp variables and later 4 calls to `tf.dispose()` - going from one-liner to 9 lines plus refactoring the caller functions to deal with async results  without `webgpu` supporting sync reads, only option i see is enhancing `tf.tidy` so it works with async calls - not sure how feasible is that?  and given the reluctance of `webgpu` team to implement sync reads, that may be what's needed...`array.map` not supporting async calls (ok, it can be wrapped into `await promise.all`, but that is really not efficient) is a different story and not `tfjs` specific, so lets stick to `tfjs` specific issues.====="", ""@vladmandic We have some discussion with `webgpu` team to implement sync reads. Our feedback is `sync reads` is not a good solution, which will block the main thread and hurt the performance. You can find the detailed explanation here https://github.com/gpuweb/gpuweb/issues/1972#issuecomment-900117505. Even they provide one finally, it will be available only in workers https://github.com/tensorflow/tfjs/pull/5505#issuecomment-904917818, which require putting the whole webgpu backend to worker. It will produce new issues, for example, how to support Canvas/HTMLVideoElement in workers.As for the `tf.tidy`, I think you can still put the sync part in the tidy and move the async API out of tidy(). ```const scores = tf.tidy(() => predictions.slice([0, 0], [-1, 1]).sigmoid().squeeze().dataSync()),```=>```const result = tf.tidy(() => predictions.slice([0, 0], [-1, 1]).sigmoid().squeeze()),const scores = await result.data(),result.dispose()``` I ever asked @lina128 about the tf.tidy() issue. She said `tidy doesn't support async calls, because it is not designed for async methods, and async memory management is pretty manageable in our experience.`. @lina128 @pyu10055 Can we provide a `tf.tidyAsync()` API for developers since we are exposing the `tf.data()`?====="", '@qjia7 I know how to make it work (and my example was a trivial one) - the problem is when a method wrapped in `tf.tidy` is a long one and uses interim results inside the method (e.g.,  checking scores before proceeding to process strides). It can all be made to work, question is of effort  Moving forward, I will not rely on sync calls or tf.tidy at all for new code, but the problem is existing code base  And just to illustrate, just take a look at your own official pre-built TFJS modules at <https://github.com/tensorflow/tfjs-models> - From the first 10, **NONE** would work with `tfjs-backend-webgpu` without major refactoring  =====', "">And just to illustrate, just take a look at your own official pre-built TFJS modules at https://github.com/tensorflow/tfjs-models - From the first 10, NONE would work with tfjs-backend-webgpu without major refactoringI haven't looked at the latest status of all of those models. But we have some local test for the posenet/pose-detection demos with webgpu backend. The changes are very easy, just like webgl backend. For example, https://github.com/tensorflow/tfjs-models/pull/824. Apply that PR, and use a local build webgpu npm package to replace the release one (since it doesn't work now.). Then you can test the webgpu backend for that demo. For some demos, if you meet `WebGPU readSync is only available for CPU-resident tensors.` error by some ops implementation which use `dataSync` rather than the user directly calling `tensor.dataSync()`, it should be a bug in webgpu that we need to fix.Can you point me the code snippets that you have seen that need major refactoring in https://github.com/tensorflow/tfjs-models ? I may have misunderstandings.====="", ""> The changes are very easy, just like webgl backendi've finished refactoring my code and it works  (except for missing kernel ops required for some models as i've documented in #5496)  i have no problem with `data` vs `dataSync`, my comments are more general how it's a breaking change that will cause some solution incompatibilities with the other backends  off-topic, i think moving forward i'll just avoid `tf.tidy` completly and use silly anti-pattern like this to make sure all tensors are deallocated at the end of the method: `for (const tensor of Object.keys(t)) tf.dispose(t[tensor])` where `t` is an object containing all tensor variables instead of creating each as a separate variable====="", 'closing this issue as its per-design  i hope this is well documented since a lot of ppl are going to run into this once webgpu is released  =====']",1
https://github.com/tensorflow/tfjs/issues/423,Unknow error when import SavedModel. NotFoundError: ./tensorflowjs_model.pb.frozen,6,closed,2018-06-13T11:28:57Z,2018-10-26T18:19:27Z,"I try to convert SavedModel to JS Model and got this error. Please help me to figure out. Thanks> Traceback (most recent call last):  File ""/Users/mac/anaconda2/bin/tensorflowjs_converter"", line 11, in <module>    sys.exit(main())  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflowjs/converters/converter.py"", line 230, in main    quantization_dtype=quantization_dtype)  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 253, in convert_tf_saved_model    graph = load_graph(output_graph + '.frozen', output_node_names)  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 59, in load_graph    graph_def.ParseFromString(f.read())  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 120, in read    self._preread_check()  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 80, in _preread_check    compat.as_bytes(self.__name), 1024 * 512, status)  File ""/Users/mac/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__    c_api.TF_GetCode(self.status.status))tensorflow.python.framework.errors_impl.NotFoundError: ./tensorflowjs_model.pb.frozen, No such file or directoryMacBook-Pro-cua-Mac:saved_model_2 mac$ ","['@bangoc123 It looks like the converter failed to freeze the graph, can you share the command line that you used? thanks.=====', 'Having a similar issue, I used       tensorflowjs_converter --input_format=tf_saved_model \\MODEL/1529446682 \\WEB_MODEL/as my command to run it.=====', 'I was running into the same problem.Manually creating an empty `tensorflowjs_model.pb.frozen` file did the trick.=====', '@BayanBennett I tried that too, but I got this instead> Traceback (most recent call last):>   File ""/usr/local/bin/tensorflowjs_converter"", line 11, in <module>>     sys.exit(main())>   File ""/usr/local/lib/python3.5/dist-packages/tensorflowjs/converters/converter.py"", line 230, in main>     quantization_dtype=quantization_dtype)>   File ""/usr/local/lib/python3.5/dist-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 253, in convert_tf_saved_model>     graph = load_graph(output_graph + \'.frozen\', output_node_names)>   File ""/usr/local/lib/python3.5/dist-packages/tensorflowjs/converters/tf_saved_model_conversion.py"", line 65, in load_graph>     for node in output_node_names.split(\',\'):> AttributeError: \'NoneType\' object has no attribute \'split\'=====', '@Ladoli The `--output_node_names` flag is missing in your command. [Docs](https://github.com/tensorflow/tfjs-converter)=====', 'Looks like this is fixed in the docs!=====']",0
https://github.com/tensorflow/tfjs/issues/559,Silence CPU feature warnings?,2,closed,2018-07-30T00:29:01Z,2018-09-12T14:52:34Z,"#### TensorFlow.js versiontfjs 0.12.3tfjs-node 0.1.9#### Browser versionNode 10.6.0#### Describe the problem or feature requestWhen I run some code using tfjs+tfjs-node, I get warnings like```nsorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this Tenso```(sorry, for some reason my code is truncating the warnings).  I guess the point is that the tf build included with tfjs/tfjs-node isn't taking advantage of all the CPU features available on my machine.  Is there any way or option to (a) silence these kinds of warnings, or if not, (b) build and use my own local copy of tf (presumably with optimal compiler settings) and use that with tfjs/tfjs-node?  (I'd strongly prefer option (a)!)  Thanks for any ideas/suggestions!","[""This notice comes from the TensorFlow C library - it is an expected message. The Python version of TF also has this notice. We'll be providing an option to roll-your-own TF build in the future: https://github.com/tensorflow/tfjs/issues/571====="", ""you can set the env value before running the script. Disables TF's C++ logs: `TF_CPP_MIN_LOG_LEVEL=2`If you're running node, then you might have another warning for n-api. To disable that, `node --no-warnings scriptname.js`=====""]",0
https://github.com/tensorflow/tfjs/issues/887,tf.fromPixels() and canvas,4,closed,2018-11-07T19:18:13Z,2019-08-12T20:01:28Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version""@tensorflow/tfjs-node"": ""^0.1.19""#### Browser versionNode 8.11.1#### Describe the problem or feature requestA way to work out with tf.fromPixels() function.I tried:```const imageToBuffer = (path) => {        const img = fs.readFileSync(""./img/cachorro2.jpg""),        const canvImg = new Image,        canvImg.src = img,        return canvImg,     }       let imgPixel = tf.fromPixels(imageToBuffer('a'))            .resizeNearestNeighbor([224, 224])            .toFloat()            .sub(meanImageNetRGB)            .reverse(2)            .expandDims()            .print(),  // and loadImage(""./img/cachorro2.jpg"").then((image) => {            let imgPixel = tf.fromPixels(image)            .resizeNearestNeighbor([224, 224])            .toFloat()            .sub(meanImageNetRGB)            .reverse(2)            .expandDims()            .print(),           console.log(imgPixel.src)    })```But nothing happen.","['In order to expedite the trouble-shooting process, please provide few more details example: browser,use case scenario, type of system.Thanks!=====', 'Im using in node js, my Idea is build a model to predict images.My system Windows 10.Node version 8.11.1tensorflow-node"": ""^0.1.19=====', '```loadImage(""./img/cachorro2.jpg"").then((image) => {    const canvas = createCanvas(image.width, image.height)    const ctx = canvas.getContext(\'2d\')      ctx.drawImage(image, 0, 0)    console.log(ctx)          let imgPixel = tf.fromPixels(canvas)              .resizeNearestNeighbor([224, 224])              .toFloat()              .sub(meanImageNetRGB)              .reverse(2)              .expandDims()              .dataSync()      console.log(imgPixel)  })```I tried this also=====', 'we have tf.node.decodeImage now: https://js.tensorflow.org/api_node/1.2.6/#node.decodeImage=====']",0
https://github.com/tensorflow/tfjs/issues/4442,[wasm] realDivide not yet implemented,3,closed,2020-12-23T17:01:19Z,2021-01-05T09:58:27Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 2.7.0- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**When using *BodyPix 2.0* with the WASM backend this error appears:```Uncaught (in promise) Error: 'realDivide' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen```The `realDivide` operation is used by BodyPix `preprocessInput` function but is not implemented in the wasm backend.**Who will benefit with this feature?**While BodyPix with the WebGL backend is fine on PC it is way too slow for real-time applications on mobile devices.**Any Other info.**I'll happily implement the feature myself if there's no plan for adding it.","['cc @annxingyuan =====', 'yeah I had this same exact problem. What should I do to fix it?=====', 'This seems to be working now with tf version 2.8.2Closing=====']",1
https://github.com/tensorflow/tfjs/issues/3484,node-pre-gyp info This Node instance does not support builds for N-API v6 using Electron and tfjs-node,9,closed,2020-06-20T15:37:00Z,2021-10-02T05:01:20Z,"#### TensorFlow.js version   @tensorflow/tfjs 2.0.2   @tensorflow/tfjs-node 2.0.1Windows 10 x64Node 12.18.1-x64Electron 9.0.4#### Browser versionChrome Version 83.0.4103.106 (Official Build) (64-bit)#### Describe the problem or feature requestI have seen a few issues discussing this error with N-API v4 but I am using node version node-v12.18.1-x64 which is the current lts and electron version 9.0.4 and get the above error#### Code to reproduce the bug / link to feature requestWith the above system configuration and this package configuration  ""dependencies"": {    ""@tensorflow-models/facemesh"": ""0.0.3"",    ""@tensorflow/tfjs"": ""^2.0.1"",    ""@tensorflow/tfjs-node"": ""^2.0.1"",    ""electron"": ""^9.0.4"",    ""stats.js"": ""^0.17.0""  },  ""devDependencies"": {},  ""scripts"": {    ""start"": ""electron main.js""  },run npm installthen npm startthis is assuming you have a render.js file with const tf = require('@tensorflow/tfjs'),require('@tensorflow/tfjs-node')My code repository for testinghttps://github.com/delebash/facemesh_node","['> node-pre-gyp info This Node instance does not support builds for N-API version 6Have this exact same issue on `@tensorflow/tfjs-node 2.0.1`=====', 'Also having this issues, is there any solution yet?=====', 'Same here=====', 'Same issue on OSX Catalina=====', 'Same here with Windows 10 x64Node 12.18.1-x64=====', 'Could this be related to the issue described here https://github.com/tensorflow/tfjs/issues/3651#issuecomment-686701119, seems like there are some electron specific elements to consider in terms of version compatibility. Another electron related issue is this one https://github.com/tensorflow/tfjs/issues/2558=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'having this issue for the last 24 hours now=====']",0
https://github.com/tensorflow/tfjs/issues/1323,Install tfjs failed win10,1,closed,2019-03-05T01:54:54Z,2019-03-05T03:09:55Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versioninstall failed, no idea..#### Browser versionchrome 72.0.3626.119 64bit#### Describe the problem or feature requestI try to install tfjs by yarn and get the error as the images:https://upload.cc/i1/2019/03/05/dLtSon.jpg#### Code to reproduce the bug / link to feature requestinstall instruction:yarn add @tensorflow/tfjsHow to fix the error?Os system:win10 home version 1803 build 17134.590yarn version:v1.13.0node version:v11.10.1","[""Sorry about my fault,I realize i didn't install tensorflow, when i installed it and this is working now.=====""]",0
https://github.com/tensorflow/tfjs/issues/3762,Cannot obtain camera on iOS Chrome - handpose demo doesn't work,7,closed,2020-08-10T09:23:51Z,2021-04-08T17:55:30Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versionWhichever is used for https://storage.googleapis.com/tfjs-models/demos/handpose/index.html#### Browser versionChrome 84#### Describe the problem or feature requestThe handpose demo doesnt start on iOS Chrome with the error saying that it was unable to get user media. Does this mean, these libraries wont work there? I want to use Chrome as it has better WebGL support needed for this model.#### Code to reproduce the bug / link to feature requestLaunch https://storage.googleapis.com/tfjs-models/demos/handpose/index.html on Chrome on an iOS device","['@Nithanaroy which IOS device you are using , according to this [documentation](https://github.com/tensorflow/tfjs-models/tree/master/handpose#performance) , it is well suited for iPhone 11, thank you Note : I was facing same issue on iPhone X cc @annxingyuan =====', ""Hi Rajesh,It works on Safari on my iPhone 10. But it doesn't work on Chrome iOS v84.I think either the demo is using an inaccurate JavaScript API to fetchcamera stream on Chrome iOS or some other major problem exists.On Mon, Aug 10, 2020 at 7:01 PM Rajeshwar Reddy T <notifications@github.com>wrote:> @Nithanaroy <https://github.com/Nithanaroy> which IOS device you are> using , according to this documentation> <https://github.com/tensorflow/tfjs-models/tree/master/handpose#performance>> , it is well suited for iPhone 11, thank you> cc @annxingyuan <https://github.com/annxingyuan>>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/tensorflow/tfjs/issues/3762#issuecomment-671679891>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAFDWXFQECITLPPEVLKYBTDSACQ7DANCNFSM4PZYMBIA>> .>-- Regards Nitin Pasumarthy Sent from a mobile====="", 'Would be worth trying to reproduce this issue on the other model demos as well, since all of them use the same camera initialization code.=====', 'for most of the models it is not loading well on mobile chrome browser , the above demo link is not working.cc @pyu10055 @lina128 =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3762"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3762"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/434,Different results on linux and windows when running non-Keras model.,6,closed,2018-06-15T15:45:18Z,2019-03-26T17:03:16Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.11.6#### Browser versionChrome 67.0.3396.79 (Official Build) (64-bit)#### Describe the problem or feature requestHi everyone,we are trying to run a custom yolo model. the model is trained with tensorflow-1.6 and we convert it into a frozen model. this is model is loaded using loadFrozenModel as in the examples. we think that the model runs fine on windows10 pcs and also Mac, however it doesn't detect anything on ubuntu 16/17 (tried several different setups). we made sure that the input data is constant and cross checked the output of each layer. we saw that the outputs after the first few cnn layers is identical, however after the fifth layer we get completely different results. it seems that this happens after applying tf.conv2d operator, however only in this particular layer. i am not sure if this the same problem as in #345. in our case i think the differences are much bigger (which might be also due to different setups of course).windows10:![image](https://user-images.githubusercontent.com/32954413/41473238-fbad20b4-70b8-11e8-8d23-2b897ab6eacd.png)ubuntu16:![image](https://user-images.githubusercontent.com/32954413/41473300-27c1e0f4-70b9-11e8-8d30-ceb335b1aee1.png)for completeness: we also loaded another yolo model, however trained with keras which worked fine everywhere. however this model also had a different topology (yolo tiny).#### Code to reproduce the bug / link to feature requestjust checkout:https://github.com/thies1006/yolo-js","['Apparently the behavior remains the same in 0.13.1. is there any explanation / solutions for this?thanks=====', ""Not sure yet but we'll prioritize this.====="", 'Like 0.1 + 0.2 = 0.300000000000004?=====', 'Marking as duplicate of https://github.com/tensorflow/tfjs/issues/345=====', ""checked with 1.0.2. after setting the tensors explicitly to dtype float32 (original code had thrown an error now) i get now identical results on windows and linux ubuntu 16 (chrome browser). the numbers are 99.999% identical to the 'windows10' numbers i posted above, however not identical.![image](https://user-images.githubusercontent.com/32954413/54987675-43ff7200-4fb5-11e9-924c-32bb3bf4b971.png)====="", ""Great! This is what I've been seeing on the Lenovo Yoga X1 machine too. Thanks for re-testing!=====""]",0
https://github.com/tensorflow/tfjs/issues/4841,Cannot read property 'backend' of undefined when running the face-landmarks-detection in a comlink web worker,7,closed,2021-03-19T17:01:59Z,2021-04-04T20:46:28Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): 3.3.0- Browser version: Chrome 89.0.4389.90**Describe the current behavior**I ran into an issue building a snapchat filter clone app thing. I've setup an example project of the same issue I had. I'd like to run the model in a web worker. When I move my landmark face detection code to a comlink web worker, I get following issue:![image](https://user-images.githubusercontent.com/27679518/111815707-482ca480-88dc-11eb-9ded-3b53617a5659.png)**Describe the expected behavior**It should work as if it ran in the main thread.**Standalone code to reproduce the issue**codesandbox: https://codesandbox.io/s/github/driescroons/tensorflow-comlink-issue (gives an error on the loader, so I suggest you use the gh link)github link: https://github.com/driescroons/tensorflow-comlink-issueIn following diff you can see I just move the code from the already working app to the comlink webworker:https://github.com/driescroons/tensorflow-comlink-issue/commit/1c239d4a14406c825e90e17e4e3f39a2a3c316ecLet me know if you need anything else!","['Thanks for the reproduction code , I could not run the code as I was getting below error ,![image](https://user-images.githubusercontent.com/43972606/112201899-fd59a800-8bcd-11eb-99ad-b6bb3870c351.png)=====', 'as mentioned, you need to check the gh repo. The comlink-loader does not seem to work within csb.=====', 'any update on this? Let me know if you need any more examples / help.=====', 'Sorry Dries, I have been busy with other projects, but I will take a look at this issue some time this week. Thanks for providing the sample project.=====', 'I think the problem is the usage of [`Comlink.proxy(video)`](https://github.com/driescroons/tensorflow-comlink-issue/blob/main/src/Landmark.js#L34). It is not possible to pass the video stream like this from the main thread to webworker. The stream data needs to be serialized in some form which can then be passed to webworker. You can try drawing the stream to a canvas, get the ImageData, and pass the ImageData to webworker (face landmarks detection supports ImageData as input). Check out [this demo](https://codepen.io/oceangermanique/pen/LqaPgO) for some ideas (give it camera permission). Also check out [this post](https://stackoverflow.com/questions/12746053/pass-large-amounts-of-data-between-web-worker-and-main-thread/27242554#27242554) for some performance advice.(I tested using face landmark detection in a webworker with ImageData as input, and it seems to work fine)Hope it helps!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4841"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4841"">No</a>=====', ""Thanks for the reply @jinjingforever. I've tried what you suggested and got it to work. Seems like rendering the video to an offscreen canvas and passing the imageData to my webworker is a bit slower than running everything in the main thread though. I should do some benchmarking.For anyone else, here are the changes I've made:https://github.com/driescroons/tensorflow-comlink-issue/compare/feat/imagedata =====""]",1
https://github.com/tensorflow/tfjs/issues/4808,BodyPix - drawBokehEffect stretched background on Safari Browsers (MacOS - iOS),4,open,2021-03-12T03:48:44Z,2021-03-16T16:23:46Z,"**System information**- Wrote custom code and tried stock examples.- macOS Big Sur 11.1- Safari iOS browser- TensorFlow.js installed from (npm and script link): 1.2 for script link & tfjs-core 3.1.0 npm version- Browser version: Safari Version 14.0.2 (16610.3.7.1.9)- Tensorflow.js Converter Version: 3.1.0**Describe the current behavior**On Safari browsers when blurring the background of video tracks coming from a `navigator.mediaDevices.getUserMedia` stream, the blurred background seems stretched. Assuming this is a Safari only issue, I think an aspect ratio fix on the [cpuBlur](https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/blur.ts) function should do it. 😉**Describe the expected behavior**drawBokehEffect expected behavior **Standalone code to reproduce the issue**Opening the codepen below in Safari should immediately reproduce the bug.https://codepen.io/guillaume250/pen/ZEBVPaG- Cheers","[""Hi @guillaume250. Thanks for the codepen reproduction! `drawBokehEffect` [bases its output dimensions on the video element's `width` and `height` properties](https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/output_rendering_util.ts#L503-L504) (although perhaps it shouldn't). If you set the video element's width and height to match the camera's aspect ratio, the output should have the correct aspect ratio. See [my fork of your codepen](https://codepen.io/mattsoulanille/pen/eYBxvqM?editors=1111) as an example. Feel free to re-open if this doesn't solve the issue.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4808"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4808"">No</a>=====', 'Hi @mattsoulanille, thanks for the quick response, and the fork.I just opened your fork in a safari browser and still see the same issue.<img width=""913"" alt=""Screen Shot 2021-03-15 at 1 36 47 PM"" src=""https://user-images.githubusercontent.com/26730301/111196976-4a1cfd80-8594-11eb-9bab-1e707f1d37ad.png""> <img width=""912"" alt=""Screen Shot 2021-03-15 at 1 37 31 PM"" src=""https://user-images.githubusercontent.com/26730301/111197107-73d62480-8594-11eb-9774-2f8a433b56f7.png"">=====', ""I initially thought your report was about the incorrect aspect ratio of the output image, but I've been able to reproduce the issue you're seeing on Safari. I'll take a look. Thanks!=====""]",1
https://github.com/tensorflow/tfjs/issues/5743,[tf-core] Cannot pass ImageBitmap to model in worker,8,closed,2021-10-19T10:54:10Z,2021-10-28T18:20:13Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7- TensorFlow.js installed from (npm or script link): https://www.jsdelivr.com/package/npm/@tensorflow/tfjs-core- TensorFlow.js version (use command below): 3.9.0- Browser version: Chrome  94.0.4606.81- Tensorflow.js Converter Version: 3.9.0Model: face-landmarks-detection v. 0.0.4, with wasm-only backend.**Preconditions**1. Put all face-landmarks-detection code into web-worker2. Pass current ImageBitmap**Describe the current behavior**`ReferenceError: document is not defined`.**Describe the expected behavior**ImageBitmap gets processed.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.https://github.com/AlexShafir/Sensoria/blob/tfjs5743Message passed [here](https://github.com/AlexShafir/Sensoria/blob/tfjs5743/src/index.js#L176), received [here](https://github.com/AlexShafir/Sensoria/blob/tfjs5743/src/worker.js#L25)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```Uncaught (in promise) ReferenceError: document is not defined    at fromPixels_ (:8887/fld/tf-core.js:8911)    at Object.fromPixels__op [as fromPixels] (:8887/fld/tf-core.js:5406)    at :8887/fld/face-landmarks-detection.js:1350    at :8887/fld/tf-core.js:4394    at Engine.scopedRun (:8887/fld/tf-core.js:4404)    at Engine.tidy (:8887/fld/tf-core.js:4393)    at Object.tidy (:8887/fld/tf-core.js:10072)    at FaceMesh.<anonymous> (:8887/fld/face-landmarks-detection.js:1348)    at step (:8887/fld/face-landmarks-detection.js:81)    at Object.next (:8887/fld/face-landmarks-detection.js:62)```Probably related to #4218","[""This is not a bug in TFJS, this is how web workers work in general - there is no DOM in a worker, so you cannot rely on any DOM objects. TFJS could print more informative error message, but root cause is the same.What you can do is pass pixel data (as value or actually transfer ownership of an array to worker) such as `ImageData.data` (which is just an array so that can be used by web worker) that you've already prepared in the main thread.====="", '@vladmandic - Sure, web workers do not have access to `document`. - Thanks for tip with `ImageData.data`! I ended up using `ImageData.data.buffer` as ArrayBuffer is transferable.=====', '@AlexShafir yup, thats the best approach for web workers...and inside web worker you can either reconstruct `ImageData` and then use `fromPixels` or just create tensor directly (but then you need to slice off alpha channel manually since `ImageData` is in RGBA format)=====', 'Thank you @vladmandic @AlexShafir closing this issue as this is not a bug and more support issue. Feel free to @mention to reopen.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>=====', '@rthadur if API says that one can enter ImageBitmap, you pass it (in worker context) and it crashes, I expect this behavior is called a bug?In any case, what is the process for this kind of problems?=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1521,Error: Unknown layer: Lambda.,2,closed,2019-04-17T07:58:57Z,2020-02-22T00:24:17Z,"Error: Unknown layer: Lambda. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().","['Hello @chenqing It sounds like you are using a Lambda layer.  Lambda layers run native python code, and are thus not currently supported in tfjsPlease see this StackOverflow question & answer:https://stackoverflow.com/questions/50878885/unknown-layer-lambda-in-tensorflowjs-on-browser/50893452#50893452=====', 'https://twitter.com/BenjaminWegener/status/1231010778100178945=====']",0
https://github.com/tensorflow/tfjs/issues/4131,facelandmarks-detection wrong when image dimensions change,6,closed,2020-10-26T11:55:46Z,2021-01-07T13:49:27Z,"I am using @tensorflow-models/face-landmarks-detection: 0.0.2 with wasm backend. I am drawing an image onto a canvas and then using the canvas the classify the landmarks.The first image is classified fine. But when I classify a second image with different dimensions, the classification is wrong. Example:![grafik](https://user-images.githubusercontent.com/3040570/97169181-ea12ef80-1789-11eb-8cd7-332ca767b45a.png)When I reload the facedetection model after classifying the first image, the prediction on the second image is correct. So I am not sure if the model is caching something","['You have to remove previous calculated regionsOfInterest between classification of two differents images to prevent this issue:`model.pipeline.regionsOfInterest  = []`=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', '@yann86 Thanks for the response. So is this expected behavior and not a bug? Is this documented somewhere?=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4131"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4131"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/970,support dtype int64 in the savedmodel python converter,2,closed,2018-12-06T16:58:37Z,2021-09-03T17:41:49Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.14.0#### Browser version#### Describe the problem or feature requestWhen supporting https://tfhub.dev/google/universal-sentence-encoder-lite/2, there are constants of type int64, currently write_weight method will error out by saying 'int64 are not supported'#### Code to reproduce the bug / link to feature request","['We have a project that would strongly prefer to use the sentence encoder in TFJS and so we would love an estimate as to when this (and any other problem) is likely to be resolved. Since our project must be web-based the alternative we hope to avoid is needing a server to run the sentence encoding model.=====', 'related changes have been merged , closing this issue.=====']",0
https://github.com/tensorflow/tfjs/issues/5283,Build tfjs-backend-webgpu with Bazel,0,open,2021-07-01T20:41:57Z,2021-07-01T20:41:57Z,"This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package, take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275, #5276, #5277",[],0
https://github.com/tensorflow/tfjs/issues/1427,fetch issue running pre-trained USE model in Node.js,7,closed,2019-03-20T11:55:04Z,2020-04-12T04:04:57Z,"Had an issue with `fetch` when loading a pre-trained TensorFlow.js model in a Node.js app.### System information- Mac OS Mojave - 10.14.1- Node.js - 10.15.1- relevant npm packages:    - @tensorflow-models/universal-sentence-encoder@1.0.1    - @tensorflow/tfjs@1.0.0    - @tensorflow/tfjs-node@1.0.1    - express@4.16.4    - node-fetch@2.3.0### IssueI'm using the [pre-trained USE model](https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder) in a simple Express server endpoint—it basically just returns the text embedding for an input string.The relevant source (which lives in a separate file called by the endpoint):```// use tensorflow js node backendrequire('@tensorflow/tfjs-node'),const universalSentenceEncoder = require('@tensorflow-models/universal-sentence-encoder'),/** * Encode a variable length string as a sentence embedding via the Universal Sentence Encoder. * * @param {*} text - the input string */exports.embed = async function embed(text) {  const result = universalSentenceEncoder.load().then(model => model.embed(text)),  return result,},```This appears to be related to #489, but my issue remained.Calling the `universalSentenceEncoder.load()` function causes a `ReferenceError: fetch is not defined` error.This appears to be the offending line: https://github.com/tensorflow/tfjs-models/blob/master/universal-sentence-encoder/src/index.ts#L49It seems odd to me that `tfjs-node` requires the `node-fetch` package and yet still required a separate package to fix this.The workaround I am using is overriding the global fetch with a line like this at the top of the file:```global.fetch = require('node-fetch'),```","[""I don't seem to be able to get the toxicity model working either. I commented here with the error: https://github.com/tensorflow/tfjs/issues/1162#issuecomment-474525325====="", ""It'd be ideal if all examples worked on Node as well as in the browser. ====="", ""@alexellis yes good point - thanks for pointing this out! We'll patch this soon. Meanwhile you can override global fetch as a bandaid, but hopefully not for too much longer :)====="", ""I don't know if this is related, or a new issue—with the same system configuration as above (fresh copies of the packages, but same versions), I'm now seeing the following error:```TypeError: Cannot read property 'id' of undefined    at .../node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:295:99    at Array.map (<anonymous>)    at GraphExecutor.<anonymous> (.../node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:295:58)    at step (.../node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:56:23)    at Object.next (.../node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:37:53)    at fulfilled (.../node_modules/@tensorflow/tfjs-converter/dist/src/executor/graph_executor.js:28:58)    at tryCatcher (.../node_modules/bluebird/js/release/util.js:16:23)    at Promise._settlePromiseFromHandler (.../node_modules/bluebird/js/release/promise.js:512:31)    at Promise._settlePromise (.../node_modules/bluebird/js/release/promise.js:569:18)    at Promise._settlePromise0 (.../node_modules/bluebird/js/release/promise.js:614:10)    at Promise._settlePromises (.../node_modules/bluebird/js/release/promise.js:694:18)    at _drainQueueStep (.../node_modules/bluebird/js/release/async.js:138:12)    at _drainQueue (.../node_modules/bluebird/js/release/async.js:131:9)    at Async._drainQueues (.../node_modules/bluebird/js/release/async.js:147:5)    at Immediate.Async.drainQueues [as _onImmediate] (.../node_modules/bluebird/js/release/async.js:17:14)    at runCallback (timers.js:705:18)```====="", 'related to #1449 =====', 'et {PythonShell} = require(\'python-shell\')var myPythonScriptPath = \'tacotron-master/eval.py\',var options = {  mode: \'text\',  args: [\' --checkpoint ~/tacotron/logs-tacotron/model.ckpt\', \'-185000\']},PythonShell.run(\'tacotron-master/eval.py\', options, function (err, results) {  if (err) throw err,  // results is an array consisting of messages collected during execution  console.log(\'results: %j\', results),}), i wrote this code to run this command ""python3 eval.py --checkpoint ~/tacotron/logs-tacotron/model.ckpt-185000""and gives me error No module named \'tensorflow\' wich is in another script.py that eval call import  tensor flow what should i do to run this pre-trained model=====', 'This has been resolve [here](https://github.com/tensorflow/tfjs-models/pull/196) , @Mariaa98 please open a new issue using template , thank you=====']",0
https://github.com/tensorflow/tfjs/issues/651,Provide esm file via jsdelivr,5,closed,2018-08-29T00:55:42Z,2019-03-20T22:28:22Z,"Now that browsers are capable of importing es-modules natively, I write a lot of my hobby projects without a bundler. This should be easy (I think) since you're already generating an esm dist file","['Does this not work?https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.6/dist/tf.esm.js=====', ""Sorry, thought I checked to see if there already were any already on jsdelivr.That would work for a bundler, but not for a browser, since the browser can't resolve npm package names (e.g. `@tensorflow/tfjs-core`).Looks like [the way Ractive.js handles it](https://cdn.jsdelivr.net/npm/ractive@0.10.9/ractive.mjs) is by bundling everything into a single es-module. So maybe this _would_ require some extra work to generate====="", ""Ah yeah, we do this for core but haven't yet done it for the union package:https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@0.12.6/dist/tf-core.esm.jsWe can keep this issue around to track that.====="", 'Sweet, thanks 👍 =====', 'Union does this now: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.1/dist/tf.esm.js=====']",0
https://github.com/tensorflow/tfjs/issues/4953,Supporting more NLP-centric ops,4,open,2021-04-19T02:41:21Z,2021-06-03T16:15:39Z,"**System information**- TensorFlow.js version 2.6.0:- Are you willing to contribute it No**Describe the feature and the current behavior/state.**ReadVariableOp, DatasetToSingleElement, BatchDatasetV2, ParallelMapDatasetV2, SentencepieceOp, TensorSliceDataset, VarHandleOp, SentencepieceDetokenizeOpAre all currently not supported by tensorflow.js, but some of them have similar operations that already exist in tensorflow.js**Will this change the current api? How?**Yes, It would add new operations to tensorflow.js**Who will benefit with this feature?**Many people working in the NLP space would be able to use these two operations to bundle tokenizers into their models on tensorflow.js with these operations**Any Other info.**I have not been able to sucessfully test the functionality of SentencePieceOp (it may already work) but TensorSliceDataset definitely does not exist, the closest existing function in tfjs being tf.data.CSVDataset.","['cc @pyu10055=====', 'will you be interested in contributing? =====', ""More support for NLP ops would be great! I just got:> tensorflow.python.framework.errors_impl.NotFoundError: Graph ops missing from the python registry ({'SentencepieceTokenizeOp', 'SentencepieceOp', 'SentencepieceDetokenizeOp'}) are also absent from the c++ registry.while trying to run:```bashtensorflowjs_converter --input_format tf_hub https://tfhub.dev/google/bertseq2seq/roberta24_cnndm/1 ./roberta24_cnndm```@rthadur JEF1056 answered that in their original post:> Are you willing to contribute it NoIt would be useful to know what specifically needs to be done to contribute these ops. I think a lot of people posting feature requests in this repo are end-users of tfjs who only have (or mostly have) experience with JS, and don't really know much about the internals (including writing webgl and compiling c/c++ wasm, let alone the internals of `tensorflowjs_converter`). A guide on how to add ops might help people to contribute?====="", '@josephrocca here is the guide https://github.com/tensorflow/tfjs/blob/master/CONTRIBUTING_MISSING_OP.md , please check. Thank you =====']",0
https://github.com/tensorflow/tfjs/issues/1605,op_list missing when running with bazel,3,closed,2019-05-23T09:24:21Z,2019-05-24T17:52:59Z,#### TensorFlow.js versionRunning at head#### Describe the problem or feature requestWhen running with bazel:bazel build tensorflowjs/converters:converter./bazel-bin/tensorflowjs/converters/converter --input_format=tf_saved_model --  --output_format=tfjs_graph_model /tmp/my_saved_model /tmp/converted_modelI get an error:OSError: [Errno 2] No such file or directory: '~/tfjs-converter/python/bazel-bin/tensorflowjs/converters/converter.runfiles/org_tensorflow_js/tensorflowjs/converters/../op_list/',"['This question is better asked on [StackOverflow](https://stackoverflow.com/search?q=tensorflowjs) since it is not a bug or feature request. There is also a larger community that reads questions there.=====', 'Ok, found a solution. For future reference and for those who are trying to build the package themselves with bazel. To solve the ""missing op_list"" error, before `bazel build`, run:```yarn gen-json```=====', 'Thank you =====']",0
https://github.com/tensorflow/tfjs/issues/4965,React Native: Crash on tf.ready() when running on Android's JS Engine,8,closed,2021-04-21T22:50:00Z,2021-09-27T21:08:02Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur 11.2.3- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Any Android device. I have tested on OnePlus 5 and Samsung Galaxy S10- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs-react-native - TensorFlow.js version (use command below): 3.4.0- Browser version: The problem happens when JS is run on the phone- Tensorflow.js Converter Version: The problem happens on initialization of tensor flow**Describe the current behavior**We have integrated TFJS into a react native app. It works fine with remote debugging as the JS is running on the computer in chrome. However, when we try to run on the phone itself it crashes the app as soon as tf.ready() is called. Following is the log from logcatA/libc: Fatal signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xfe8aa0003f3 in tid 25679 (mqt_js)The above only happens for Android. We have also tried with another JS engine https://www.npmjs.com/package/react-native-v8 but that also fails with a crashIt works fine on iPhone both in remote debugging and running on the phone itself.**Describe the expected behavior**TensorflowJS should initialize**Standalone code to reproduce the issue**Add tensorflowjs imports to App.js in a react native app and call await tf.ready()**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.A/libc: Fatal signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xfe8aa0003f3 in tid 25679 (mqt_js)","[""Sorry Matt, re-assigning this for you now since I don't have an Android device:) Thanks!====="", 'Here is the package.json. This may provide an insight of the environment tfjs is running in[package.json.zip](https://github.com/tensorflow/tfjs/files/6368794/package.json.zip)=====', ""I'm also having this issue right now, any solution?I'm using tfjs 3.7.0 and react native 0.62.3====="", 'i am facing same issue. @jinjingforever  @mattsoulanille my app crash when await tf.ready() called.i am using react native cli.""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.15.7"",    ""@tensorflow/tfjs"": ""^3.8.0"",    ""@tensorflow/tfjs-react-native"": ""^0.6.0"",    ""expo-camera"": ""^11.2.2"",    ""expo-constants"": ""^11.0.1"",    ""expo-gl"": ""^10.4.2"",    ""expo-gl-cpp"": ""^10.4.1"",    ""react"": ""17.0.2"",    ""react-native"": ""0.65.1"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-unimodules"": ""^0.14.6""  },=====', 'I am also facing the same issue```    ""@react-native-async-storage/async-storage"": ""^1.15.8"",    ""@tensorflow/tfjs"": ""^3.9.0"",    ""@tensorflow/tfjs-react-native"": ""^0.7.0"",    ""expo-camera"": ""^11.2.2"",    ""expo-gl"": ""^10.4.2"",    ""expo-gl-cpp"": ""^10.4.1"",    ""react"": ""17.0.2"",    ""react-native"": ""0.65.1"",    ""react-native-ble-manager"": ""^7.6.1"",    ""react-native-fs"": ""^2.18.0"",    ""react-native-svg"": ""^12.1.1"",    ""react-native-unimodules"": ""^0.14.9"",    ""react-native-vector-icons"": ""^8.1.0"", ```A screenshot of my device logs![Screenshot 2021-09-26 at 14 51 19](https://user-images.githubusercontent.com/2452503/134808787-ca121a0f-fa1a-4ded-991a-6014258b2ae8.png)=====', 'Hi @wvanooijen92 @AhmedRazaVerge, I recently checked in some tfjs react native example apps [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native). Please take a look. One thing to note is that react native 0.65.1 does not work well with expo-gl 10.4.2 (would crash Android as you guys mentioned). Please use react native 0.63.2. Thank you!=====', '@jinjingforever thanks for the update, I was just about to leave you a message. That is exactly what I did today, I was able to get it running. Fyi I am on 0.63.4 now and it seems to be working. Thank you!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4965"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4965"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5513,[tfjs-react-native] react-native-unimodules is deprecated,2,closed,2021-08-20T07:12:07Z,2021-08-21T02:40:06Z,"Here: https://github.com/expo/expo/tree/master/packages/react-native-unimodulesAnd actually when I installed `react-native-unimodules: 0.14.6` with `react-native: 0.64.2` on Android, I got this:``` WARN  The ""UMNativeModulesProxy"" native module is not exported through NativeModules, verify that @unimodules/react-native-adapter's native code is linked properly ERROR  TypeError: null is not an object (evaluating 'NativeUnimoduleProxy.viewManagersNames') ERROR  Invariant Violation: Module AppRegistry is not a registered callable module (calling runApplication). A frequent cause of the error is that the application entry file path is incorrect.       This can also happen when the JS bundle is corrupt or there is an early initialization error when loading React Native. ERROR  Invariant Violation: Module AppRegistry is not a registered callable module (calling runApplication). A frequent cause of the error is that the application entry file path is incorrect.       This can also happen when the JS bundle is corrupt or there is an early initialization error when loading React Native.```","[""Fixed.It seems like I need to config react-native-unimodules manually:https://docs.expo.dev/bare/installing-unimodules/Because I built my project with bare rn, instead of expo.And don't forget to install/config android ndk in the right place.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5513"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5513"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4270,"Unable to resolve ""@react-native-community/async-storage"" from ""node_modules\@tensorflow\tfjs-react-native\dist\async_storage_io.js""",8,open,2020-11-19T09:33:23Z,2021-06-16T09:06:43Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- Mobile device : Honor7x- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 2.7.0Description:Importing fetch from @tensorflow/tfjs-react-native resulting in above issue.I am getting this issue when I import only. I did follow the exact steps as in docs.Please see the screenshot below:![WhatsApp Image 2020-11-19 at 2 57 16 PM](https://user-images.githubusercontent.com/49977739/99647365-af922f00-2a77-11eb-9683-8cebf1c2b4cd.jpeg)Package  Dependencies : ```javascript ""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.13.2"",    ""@tensorflow-models/mobilenet"": ""^2.0.4"",    ""@tensorflow/tfjs"": ""^2.7.0"",    ""@tensorflow/tfjs-react-native"": ""^0.5.0"",    ""expo"": ""~39.0.2"",    ""expo-camera"": ""^9.0.0"",    ""expo-constants"": ""^9.2.0"",    ""expo-gl"": ""^9.1.1"",    ""expo-gl-cpp"": ""^9.1.2"",    ""expo-image-picker"": ""^9.1.1"",    ""expo-permissions"": ""^9.3.0"",    ""expo-status-bar"": ""~1.0.2"",    ""jpeg-js"": ""^0.4.2"",    ""react"": ""16.13.1"",    ""react-dom"": ""16.13.1"",    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-39.0.4.tar.gz"",    ""react-native-fs"": ""^2.16.6"",    ""react-native-web"": ""~0.13.12""  },``` Please help me how to resolve this issue.","['Your dependencies have ""@react-native-async-storage/async-storage"": ""^1.13.2"". But you need to have ""@react-native-community/async-storage"" installed. It looks like the async-storage packaged moved to a new org recently so we would need to update our dependency in a future release, but at the moment you should be able to load it with the older package name. Let me know if you run into problems with that.I\'ll leave this open to track updating the dependency on our end.=====', ""I'm sorry ... but I need the current package(https://github.com/react-native-async-storage/async-storage), and installing both creates conflict. Can it be updated for the package considered indicated?====="", 'Is there any update regarding this? It is difficult to start a new project because of how far back in versioning for several libraries you have to go back to support this particular piece.=====', 'Since Expo SDK 41 dropped the support of old package name, we cannot upgrade to SDK 41 as long as we use tfjs on an expo project.=====', ""@tafsiri This issue has been open for over 6 months. Would love it if you can make this your priority. Can't use BundleResourceIO and load models at all because this dependency hasn't been updated yet====="", ""Same issue here, app doesn't even build. ====="", '> Same issue here, app doesn\'t even build.If you want to build the app. Go to the file the error takes you and replace the dependency manually to the `""@react-native-async-storage/async-storage""`. I\'m not sure if the async storage functionalities still work though. I worked around them. =====', '> > Same issue here, app doesn\'t even build.> > If you want to build the app. Go to the file the error takes you and replace the dependency manually to the `""@react-native-async-storage/async-storage""`. I\'m not sure if the async storage functionalities still work though. I worked around them.Yes that works, but this issue is already open for 6+ months. I think monkey patching is not the solution.=====']",1
https://github.com/tensorflow/tfjs/issues/5197,[Perf] The time of Conv2DBackpropInput is very long in BlazePose/hand_detector models in WebGL,3,open,2021-06-09T02:39:37Z,2021-11-04T17:35:22Z,"hand_detector<!--StartFragment-->Kernel | Time(ms) | Inputs | Output | GPUPrograms-- | -- | -- | -- | --Conv2DBackpropInput | 9.73 | input0: 4D[1,16,16,256]input1: 4D[2,2,128,256] | 1,32,32,128 | UnpackProgram: 0.026083, Conv2DDerInputProgram: 9.706294Conv2DBackpropInput | 4.93 | input0: 4D[1,8,8,256]input1: 4D[2,2,256,256] | 1,16,16,256 | UnpackProgram: 0.011333, Conv2DDerInputProgram: 4.91648<!--EndFragment-->BlazePose with inputSize 256, inputType Tensor<!--StartFragment-->Kernel | Time(ms) | Inputs | Output | GPUPrograms-- | -- | -- | -- | --Conv2DBackpropInput | 20.20 | input0: 4D[1,7,7,1152]input1: 4D[2,2,192,1152] | 1,14,14,192 | UnpackProgram: 0.042083, Conv2DDerInputProgram: 20.159669<!--EndFragment-->","['fyi @pyu10055 @lina128 @jinjingforever =====', 'This is very informative, thank you @qjia7 ! In the GPUPrograms column, is Conv2DDerInputProgram the packed version and UnpackProgram the unpacked version, both from WebGL backend?=====', '@lina128 the UnpackProgram is the shader to unpack outputs from the previous kernel, the Conv2DDerInputProgram is the shader for Conv2DBackpropInput, which one supports unpacked texture.=====']",1
https://github.com/tensorflow/tfjs/issues/2679,Prediction from tf.data.Dataset,6,closed,2020-01-15T10:01:03Z,2021-09-09T20:51:11Z,"#### TensorFlow.js versionlatest version#### Node version```$ node --versionv12.13.1```#### Describe the problem or feature requestIn my python code I have a `tf.data.Dataset` where a file list is mapped to a `tf.py_function`:```python dataset = tf.data.Dataset.from_tensor_slices({        'audio_id': list(filenames),        'start': list(starts),        'end': list(ends)    })    dataset = dataset.map(        lambda sample: dict(            sample,            **audio_adapter.load_tf_waveform(                sample['audio_id'],                session=session,                sample_rate=sample_rate,                offset=sample['start'],                duration=sample['end'] - sample['start'])),        num_parallel_calls=2)```When I load the model into `tfjs` with the new `tf.node.loadSavedModel` I get the model with the following signature:```TFSavedModel {  sessionId: 0,  jsid: 0,  inputNodeNames: {    audio_id: 'Placeholder_1:0',    mix_spectrogram: 'strided_slice_3:0',    mix_stft: 'transpose_1:0',    waveform: 'Placeholder:0'  },  outputNodeNames: {    accompaniment: 'strided_slice_23:0',    audio_id: 'Placeholder_1:0',    vocals: 'strided_slice_13:0'  },  backend: NodeJSKernelBackend {    binding: {},    isGPUPackage: false,    isUsingGpuDevice: false,    tensorMap: DataStorage {      backend: [Circular],      dataMover: [Engine],      data: [WeakMap],      dataIdsCount: 0    }  },  disposed: false}```When in python the predict takes the dataset as input:```pythonfn = lambda: get_dataset(                    audio_adapter,                    filenames_and_crops,                    sample_rate,                    n_channels, session)with session.as_default():    with session.graph.as_default():        prediction = estimator.predict(fn,yield_single_examples=False)```How load a this dataset format into tfjs `model.predict`?#### Code to reproduce the bug / link to feature request-","[""Currently model.predict does not take a [dataset](https://js.tensorflow.org/api/latest/#class:data.Dataset) but only takes tensors. You could however use functions like [map](https://js.tensorflow.org/api/1.5.1/#tf.data.Dataset.map) custom loop to call the predict function of your loaded model with a realized tensor. You can use the other dataset methods to set up a pipeline to stream the data from disk. The [generator](https://js.tensorflow.org/api/latest/#data.generator) function is probably the most flexible way to create a dataset from file streams (we don't have a built in utility to stream data from files in node).cc @kangyizhang in case i missed anything/got anything wrong.====="", 'To clarify my message a bit the function passed to tf.data.generator would need to do the work that `**audio_adapter.load_tf_waveform` does after opening the files that you want (you can chain generators if you want).=====', 'Like Yannick said, the current `model.predict` function in tfjs does not take a dataset as input. One option is to modify the model in python and change the input format as a tensor, then export the model.=====', 'closing this issue , feel free to reopen if you need more info.=====', '@rthadur @kangyizhang @tafsiri thank you very much for your help, I get the big picture, but it is still pretty hard to figure out how to actually implement it. An example related to `tf.Dataset` and audio files would be worth, thanks a lot.=====', 'Is there a feature request to add `tf.data.Dataset` as a supported input type for `model.predict`? In the meantime, I guess we use` tf.stack` and handle pre-loading ourselves? Is it worth looking at the Keras `model.predict `as an example of how to implement `tf.data.Dataset` support in` model.predict`?=====']",0
https://github.com/tensorflow/tfjs/issues/4736,speech-command playRawAudio plays blank audio,8,closed,2021-02-24T17:56:13Z,2021-05-08T23:15:50Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution: Windows10- TensorFlow.js installed from: npm- TensorFlow.js version: 3.2.0- speech-command version: 0.5.2- Browser version: Version 88.0.4324.192 (Official Build) (x86_64)**Describe the current behavior**I'm trying to use [playRawAudio](https://github.com/tensorflow/tfjs-models/blob/59c6ecb1d6bddfe287d4a6559ae1a70f5b423a8d/speech-commands/src/index.ts#L84) method by providing `RawAudioData` object. `Float32Array` as [data](https://github.com/tensorflow/tfjs-models/blob/59c6ecb1d6bddfe287d4a6559ae1a70f5b423a8d/speech-commands/src/types.ts#L727) and `sampleRateHz:44100`. There's an audio playing with the same duration of `data`, but the audio is blank (no audio)I provided `Float32Array` array from `SpeechCommandRecognizerResult.spectrogram.data` in `listen` function, then concatenated the array using `concatenateFloat32Arrays` util function**Describe the expected behavior**Expected to play the correct audio, not a blank audio.**Standalone code to reproduce the issue**[Example in Codesandbox](https://codesandbox.io/s/tfjs-listen-recognize-scores-iyrjd)","['@adotnusiyan did you find a solution? i also need this and got same issue=====', 'Not yet.Hopefully waiting @lina128 =====', ""Hi @adotnusiyan and @zipweight , I'm investigating this, I'm seeing a sampleRate mismatch error in my browser, not sure if it is related, just fyi: https://github.com/tensorflow/tfjs-models/pull/612====="", 'Hi @adotnusiyan, I looked at the code you provided, it seems you provide the spectrum data to the playRawAudio function. You need to provide raw audio sound, not the spectrum data.=====', '@lina128 How to play the spectrum data as audio?Or how can I get raw audio data from listen function?Thanks =====', 'If you have a transferRecognizer with collectExample({includeRawAudio = true}). And then get the rawAudio from tranferRecognizer.getExamples()[0].example.rawAudio.If you just want a generic solution, you need to implement your own.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4736"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4736"">No</a>=====', ""@lina128 I tried your suggestion with basic code from [the example in README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) and the audio doesn't sound right. You can hear some distortion/glitching in the audio[You can try it yourself in this example](https://codesandbox.io/s/tfjs-play-raw-audio-v0y12)=====""]",1
https://github.com/tensorflow/tfjs/issues/5609,How do I make a build that uses glibc 2.31,6,closed,2021-09-11T20:47:22Z,2021-10-04T19:56:24Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): I built tensorflow from source using instructions from https://github.com/tensorflow/tfjs/tree/master/tfjs-node#optional-build-optimal-tensorflow-from-source- TensorFlow.js version: N/A- CUDA/cuDNN version: N/A**Describe the problem**I am using a library in Node called face-api and this library is using @tensorflow/tfjs-node. On my local machine (Windows) the app is working fine but after I pushed the code to a VPS with the below information I get `Illegal instruction` when the app requires @tensorflow/tfjs-node. After some research I found that I need to build tensorflow from source. I followed instructions from https://github.com/tensorflow/tfjs/tree/master/tfjs-node#optional-build-optimal-tensorflow-from-source and after dealing with tons of issues I finally managed to make a build. I had to make the build on a VM because the VPS has low RAM 2GB and the build always fails there. My VPS information:Processor information	QEMU Virtual CPU version 2.5+, 2 coresMemory			        2GBOperating system	        Ubuntu Linux 20.04Kernel and CPU		Linux 5.4.0-84-generic on x86_64After I replaced tensorflow in node_modules with the build I made I no longer get `Illegal instruction` but now I get:```Error: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/dan/backend/passport-ai-backend/node_modules/@tensorflow/tfjs-node/lib/napi-v8/../../deps/lib/libtensorflow.so.2)    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1131:18)    at Module.load (internal/modules/cjs/loader.js:937:32)    at Function.Module._load (internal/modules/cjs/loader.js:778:12)    at Module.require (internal/modules/cjs/loader.js:961:19)    at require (internal/modules/cjs/helpers.js:92:18)    at Object.<anonymous> (/home/dan/backend/passport-ai-backend/node_modules/@tensorflow/tfjs-node/dist/index.js:60:16)    at Module._compile (internal/modules/cjs/loader.js:1072:14)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1101:10)    at Module.load (internal/modules/cjs/loader.js:937:32)    at Function.Module._load (internal/modules/cjs/loader.js:778:12)```My VPS has glibc v2.31 and I read that it's not safe to just update it since a lot of packages depend on it and the safest way to solve this is to make a new build that uses glibc v2.31.My problem is that I'm lost and don't know how to make bazel make a build that uses glibc v2.31**Provide the exact sequence of commands / steps that you executed before running into the problem**For `./configure` I chose no for all questions and for optimization flags I used `-mtune=generic -mno-avx` because I am building on a VM Virtualbox and I think -mtune=generic is the right flag (unless I'm wrong).I am using bazel version 4.2.1**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.","['@JosephBotros can you please check if this related isssue https://github.com/tensorflow/tfjs/issues/2872 ?=====', ""@rthadur thank you, but I don't see how is the issue you tagged is related to my issue.====="", ""Hi @JosephBotros. This question seems more related to TensorFlow's build rather than TF.js. I think you'll get better answers from the [TensorFlow forum](https://discuss.tensorflow.org/) or the [TensorFlow GitHub](https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+is%3Aopen+glibc). I took a look at the TensorFlow docs, source code, and a few issues, and I wasn't able to find anything about changing the glibc version. The first thing I would probably try is to build TensorFlow using the same type of image you plan on deploying to, since that should make sure the glibc versions (and others) match.====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5609"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5609"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/266,tf.loadModel not working in react native,15,closed,2018-05-05T00:09:46Z,2019-09-04T18:59:24Z,"#### TensorFlow.js version0.10.3#### Browser versionreact-native 0.55.3#### Problem:tf.loadModel not working for local folder and remo url in react native, load local model not working for React#### Feature request:  tf.loadModel react-native support#### Code to reproduce the bug / link to feature request```const path = 'https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_0.25_224/model.json',const localPath = './tfjsmodel/model.json',  async loadmodel(p) {    const model = await this.loadHostedPretrainedModel(p),    console.log(model),  }  async loadHostedPretrainedModel(p) {    try {      const model = await tf.loadModel(p),      return model,    } catch (err) {      console.error(err),    }  }```this is the demo app i created for running tfjs in react-native, it can train and predict model locally,if you are interested this is the repo:  [https://github.com/tangtai/tfjsdemo](url)<img src=""https://user-images.githubusercontent.com/29759753/39657143-01f30aa8-4fba-11e8-80fa-143aa2d230d9.PNG"" width=""250"" >error i get when try to load model from url, i tried the code on react as well it returns the model successfully:<img src=""https://user-images.githubusercontent.com/29759753/39657258-1bbc6b68-4fbb-11e8-89da-13d5110dfa85.PNG"" width=""250"" >error i get when try to load model from local folder, for React it returns pending Promise : <img src=""https://user-images.githubusercontent.com/29759753/39657212-bac063be-4fba-11e8-99bd-2b18ee8a870d.PNG"" width=""250"" >the model i got from tensorflowjs file convertor:![screen shot 2018-05-04 at 4 59 55 pm](https://user-images.githubusercontent.com/29759753/39657439-b1d1af86-4fbc-11e8-9898-fd8b9a3e8ab0.png)","[""Hi @tangtai, I'm not really familiar with React but I think you need to find a way to serve your model with a http server, moreover, it must allow CORS requests. Is there a way to do this within the React ecosystem? Otherwise, I described a solution in a similar issue: #257.====="", '@timotheebernard  thanks for your response, i tried to host the model on AWS s3 bucket and turn on CORS request it worked for React(web app), but didn\'t work for react native(mobile app), apparently according to https://facebook.github.io/react-native/docs/network.html native apps have not concept of CORS. how ever i can only fetch the json file from AWS and it returns the model configuration for me:<img width=""658"" alt=""screen shot 2018-05-07 at 12 24 40 pm"" src=""https://user-images.githubusercontent.com/29759753/39720577-ab30b4ba-51f1-11e8-8603-669a52abd685.png"">I\'m wondering if we can somehow bypass the CORS issue and turn the model.json file into a tf.Sequential model we can use in our native app that would be great! as i was trying to figure out what tf.loadModel does, i think here is what it uses the model.json file to configure the model https://github.com/tensorflow/tfjs-layers/blob/72d82c90619c449548c5e881c64fcc5a299c5cfc/src/models.ts#L200 but im not 100% sure.and i also tried tfjs examples on chrome on my phone, the model still works offline, once the model is loaded.=====', '@tangtai does React native support loading file locally? If so, this would be similar to node.js env, we can provide API to load the model and weights from local file system instead of urls.=====', ""hi @pyu10055 , react native can definitely load local json files, as for other files that created by the tensorflowjs converter i am not so sure, since they don't have a file format.  I'm wondering how exactly does the loadModel function turns the json file into a tf.Sequential(). Even if the model is loaded from url other than CORS, if tf.Sequential() is a javascript object, i think react native will be able to store the object locally in async Storage, and use the model offline.====="", 'Adding @caisq to this conversation. That could be a better route, we should move to a more generic loading API, sitting on top of the new serialization mechanism, which allows you to create io handler to store and load models for different storages. =====', ""`tf.loadModel()` works by1. making a request to the specified `model.json` path. In the JSON response, there is a field called `weightsManifest`, containing a number of relative paths.2. requesting binary weight values from the relative paths.So he server that serves `model.json` must be able to work with relative path. This is the case for Google Cloud Storage, as many of our example and demo models are hosted there. Isn't this also the case for AWS S3 buckets work with relative paths too? It has been a while since I last used S3.====="", ""@caisq for AWS s3 buckets you can enable CORS sharing, but it is a web thing, react native can't do it, while React can because it is a web framework. i think i understand the other files are storing the weights of the neural net, while the json file is the configuration of the neural net. it is just strange to me that you have to host your json model to access it, even-though the model is stored locally, is there a way around without hosting it for CORS sharing?====="", '@tangtai Looking at your original error message, it seems to fail at the step where `FileReader.readAsArrayBuffer` is called.  This is the 2nd step, i.e., the step where the binary weights are loaded. It says the method is not implemented. So it seems to be not directly related to CORS, but a limitation of your browser. What browser and platform is this?=====', '@caisq I’m using react-native which is a JavaScript platform for mobile development, it says on its documents it does not support CORS, can android use tfjs?=====', ""@tangtai Actually,  tf.loadModel in version 0.10.3 of tfjs doesn't actually call `FileReader.readAsArrayBuffer` directory. It should call only `fetch`. Is the call from your own code? ====="", 'the version of tfjs Im using is @ version 0.10.3.=====', 'you can try webpack to import the cdn of tensorflowjs in Reactnative=====', 'Managed to make it work by using a custom loader:```const loader = {  load: async () => {    return {      modelTopology: topology,      weightSpecs: specs      weightData: data,    },  }}const model = await tf.loadLayersModel(loader)```=====', '@tangtai could you please share the code you used to load a tf.loadLayersModel from AWS s3 bucket on React webapp? Thanks!=====', 'We just released a package for this! https://www.npmjs.com/package/@tensorflow/tfjs-react-native try it out and feel free to re-open this issues if you have trouble with it.=====']",0
https://github.com/tensorflow/tfjs/issues/5162,Tensorflow Js predict output zero for all classes,11,closed,2021-06-03T00:48:30Z,2021-06-28T19:26:15Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): built using expo SDK 39 on Ubuntu 20.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Any Android Phone- TensorFlow.js installed from (npm or script link): npm - TensorFlow.js version: @tensorflow/tfjs"": ""^2.6.0"",""@tensorflow/tfjs-react-native"": ""^0.4.0"",- CUDA/cuDNN version: Keras Model built on CUDA 11.2 in a GPU environment. The model was then converted to layers as well  graph models**Describe the problem**Tensorflow Predict out put always zero, irrespective whatever model we choose for any image**Provide the exact sequence of commands / steps that you executed before running into the problem**The following function was used to load the modeluseEffect(() => { async function loadModel(){  const tfReady = await tf.ready(),  const modelJson = await require(""./assets/model/model.json""),  const modelWeight = await require(""./assets/model/group1-shard.bin""),   const InsectDetector = await       tf.loadLayersModel(bundleResourceIO(modelJson,modelWeight)),    setInsectDetector(InsectDetector)  } } loadModel()}, []), The following code was used to load an image for classificaionconst imgBuffer = tf.util.encodeString(Imgb64, 'base64').buffer,  const raw = new Uint8Array(imgBuffer),  imageTensor = decodeJpeg(raw),  const imageTensoresized =  tf.image.resizeBilinear(imageTensor, [224, 224]),  const offset = tf.scalar(255.0),  const normalized = (imageTensoresized).div(offset),  let Result = InsectDetector.predict(normalized.reshape([1,224,224,3])),  console.log(""[+] Insect Class Prediction Result"",Result)  let PrData = Result.dataSync()  let InsectClass = Result.argMax().dataSync()[0]  console.log(""[+] Insect Class Prediction done Insect class "",PrData,InsectClass)**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.The out put of predict from console.log{ kept: false,      isDisposedInternal: false,      shape: [ 102 ],      dtype: 'float32',      size: 102,      strides: [],      dataId: {},      id: 1162,      rankType: '1',      scopeId: 848 },The ouput of .dataSync() fo 102 classes '0': 0,      '1': 0,      '2': 0,      '3': 0,      '4': 0,      '5': 0,      '6': 0,      '7': 0,      '8': 0,      '9': 0,      '10': 0,      '11': 0,      '12': 0,      '13': 0,      '14': 0,      '15': 0,      '16': 0,      '17': 0,      '18': 0,      '19': 0,      '20': 0,      '21': 0,      '22': 0,      '23': 0,      '24': 0,      '25': 0,      '26': 0,      '27': 0,      '28': 0,      '29': 0,      '30': 0,      '31': 0,      '32': 0,      '33': 0,      '34': 0,      '35': 0,      '36': 0,      '37': 0,      '38': 0,      '39': 0,      '40': 0,      '41': 0,      '42': 0,      '43': 0,      '44': 0,      '45': 0,      '46': 0,      '47': 0,      '48': 0,      '49': 0,      '50': 0,      '51': 0,      '52': 0,      '53': 0,      '54': 0,      '55': 0,      '56': 0,      '57': 0,      '58': 0,      '59': 0,      '60': 0,      '61': 0,      '62': 0,      '63': 0,      '64': 0,      '65': 0,      '66': 0,      '67': 0,      '68': 0,      '69': 0,      '70': 0,      '71': 0,      '72': 0,      '73': 0,      '74': 0,      '75': 0,      '76': 0,      '77': 0,      '78': 0,      '79': 0,      '80': 0,      '81': 0,      '82': 0,      '83': 0,      '84': 0,      '85': 0,      '86': 0,      '87': 0,      '88': 0,      '89': 0,      '90': 0,      '91': 0,      '92': 0,      '93': 0,      '94': 0,      '95': 0,      '96': 0,      '97': 0,      '98': 0,      '99': 0,      '100': 0,      '101': 0 },","['@z2043947 can you verify if what are the values for the input of the model looks fine:`normalized .print()`And ensure the model requires normalization result between [0.0, 1.0]=====', 'Thanks PingSo the value has to be between 0.0 and 1.0? Cannot be 0 or greater than 1?The attached is the converted tensor from the image.Tensor        [[[218, 214, 233],          [153, 149, 168],          [154, 153, 171],          ...,          [209, 214, 234],          [204, 207, 228],          [208, 211, 232]],             [[230, 226, 245],          [162, 158, 177],          [157, 156, 174],          ...,          [145, 150, 170],          [142, 145, 166],          [147, 150, 171]],             [[221, 216, 234],          [152, 147, 165],          [149, 145, 164],          ...,          [135, 140, 160],          [131, 134, 155],          [137, 140, 161]],             ...         [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]],             [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]],             [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]]]After dividing by scalar 255 it is all zeros. That does not look right?Tensor        [[[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],             [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],             [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],             ...         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],             [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],             [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]]]regards,SankaranSankaran Iyer+61 431 499 867Sent from Mail for Windows 10From: Ping YuSent: Friday, 4 June 2021 3:33 AMTo: tensorflow/tfjsCc: z2043947, MentionSubject: Re: [tensorflow/tfjs] Tensorflow Js predict output zero for allclasses (#5162)@z2043947 can you verify if what are the values for the input of the model looks fine:normalized .print()And ensure the model requires normalization result between [0.0, 1.0]—You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or unsubscribe.=====', ""> After dividing by scalar 255 it is all zeros. That does not look right?```jsconst normalized = imageTensoresized.cast('float32').div(255),```====="", ""> > After dividing by scalar 255 it is all zeros. That does not look right?> > ```js> const normalized = imageTensoresized.cast('float32').div(255),> ```It is not dividing by a scalar that results in zeros. It is the following statementconst imageTensoresized = tf.image.resizeBilinear(imageTensor, [224, 224]), Same problem with resizeNearestNeighbor as well.  I am now exploring alternate ways of resizing the image.====="", ""> It is not dividing by a scalar that results in zeros. It is the following statement> `const imageTensoresized = tf.image.resizeBilinear(imageTensor, [224, 224]),`You're using `imageTensor = decodeJpeg(raw),` - what is `decodeJpeg`?  And more importantly, what is the shape of the resulting `imageTensor`? It should be `[1, height, width, 3]`====="", '> > It is not dividing by a scalar that results in zeros. It is the following statement> > `const imageTensoresized = tf.image.resizeBilinear(imageTensor, [224, 224]),`> > You\'re using `imageTensor = decodeJpeg(raw),` - what is `decodeJpeg`?> And more importantly, what is the shape of the resulting `imageTensor`? It should be `[1, height, width, 3]`import {fetch, bundleResourceIO,decodeJpeg} from ""@tensorflow/tfjs-react-native"",decodeJpeg is provided by tensorflow/tfjs-react-native. It returns  tensor3d(buffer, [height, width, channels]) . The tensor seems to be OK. The following is imageTensor,print() added after decodeJpegTensor        [[[218, 214, 233],          [153, 149, 168],          [154, 153, 171],          ...,          [209, 214, 234],          [204, 207, 228],          [208, 211, 232]],         [[230, 226, 245],          [162, 158, 177],          [157, 156, 174],          ...,          [145, 150, 170],          [142, 145, 166],          [147, 150, 171]],         [[221, 216, 234],          [152, 147, 165],          [149, 145, 164],          ...,          [135, 140, 160],          [131, 134, 155],          [137, 140, 161]],         ...         [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]],         [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]],         [[243, 243, 243],          [243, 243, 243],          [243, 243, 243],          ...,          [255, 255, 255],          [255, 255, 255],          [255, 255, 255]]]Tensor becomes zero after executing this statement const imageTensoresized = tf.image.resizeBilinear(imageTensor, [224, 224]),imageTensoresized.print()  added after the statement results inTensor        [[[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],         ...         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]],         [[0, 0, 0],          [0, 0, 0],          [0, 0, 0],          ...,          [0, 0, 0],          [0, 0, 0],          [0, 0, 0]]]=====', 'The shape of imageTensoresized is [224,224,3] as required even though it is all zero =====', 'I have now ensured that a feed an input tensor between 0 and 1 of shape 1,224,224,3 to the model without using resizeBilinear.  The tensor fed to the mode is as follows:Tensor    [[[[0.654902 , 0.8039216, 0.5176471],       [0.6470588, 0.7960784, 0.509804 ],       [0.6431373, 0.7921569, 0.5058824],       ...,       [0.5568628, 0.7568628, 0.2588235],       [0.5568628, 0.7568628, 0.2588235],       [0.4901961, 0.6901961, 0.1921569]],      [[0.6588235, 0.8078431, 0.5215687],       [0.6470588, 0.7960784, 0.509804 ],       [0.6352941, 0.7843137, 0.4980392],       ...,       [0.5647059, 0.7647059, 0.2666667],       [0.5529412, 0.7529412, 0.254902 ],       [0.4823529, 0.682353 , 0.1843137]],      [[0.6627451, 0.8117647, 0.5254902],       [0.6392157, 0.7882353, 0.5019608],       [0.6196079, 0.7686275, 0.4823529],       ...,       [0.5568628, 0.7647059, 0.254902 ],       [0.5333334, 0.7411765, 0.2313726],       [0.454902 , 0.6627451, 0.1529412]],      ...      [[0.3764706, 0.572549 , 0.145098 ],       [0.3882353, 0.5843138, 0.1568628],       [0.3921569, 0.5882353, 0.1529412],       ...,       [0.5450981, 0.7372549, 0.3333333],       [0.5450981, 0.7372549, 0.3333333],       [0.5450981, 0.7372549, 0.3333333]],      [[0.3764706, 0.572549 , 0.145098 ],       [0.3882353, 0.5843138, 0.1568628],       [0.3921569, 0.5882353, 0.1529412],       ...,       [0.5450981, 0.7333333, 0.3411765],       [0.5450981, 0.7333333, 0.3411765],       [0.5450981, 0.7333333, 0.3411765]],      [[0.3803922, 0.5764706, 0.1490196],       [0.3882353, 0.5843138, 0.1568628],       [0.3921569, 0.5882353, 0.1529412],       ...,       [0.5450981, 0.7333333, 0.3411765],       [0.5450981, 0.7333333, 0.3411765],       [0.5450981, 0.7333333, 0.3411765]]]]But I still get predictions of zero.  I have converted the .h5 file and tried with both layer and graph models. I don know what the issue is now.Insect Class Prediction Result Tensor {  ""dataId"": Object {},  ""dtype"": ""float32"",  ""id"": 297,  ""isDisposedInternal"": false,  ""kept"": false,  ""rankType"": ""2"",  ""scopeId"": 1,  ""shape"": Array [    1,    102,  ],  ""size"": 102,  ""strides"": Array [    102,  ],} 0.78978 0.14285714285714285INFO13:30[+] Insect Class Prediction done Insect class  Float32Array [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,]=====', ""I now verified that I get correct predictions when I turn on remote debugging using the chrome debugger tools. The issue is only in the app's JS on the phone!====="", 'Thank for you for confirming , please mention@ to reopen if you still see issue =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5162"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5162"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4927,Using WASM with NodeJS but LoadGraphModel does not accept local file as input,1,open,2021-04-14T01:01:24Z,2021-08-25T12:08:16Z,"`WASM` backend is supported in `NodeJS` environments as per  <https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html>> In Node.js, the WASM backend is a great solution for devices that don’t support the TensorFlow binary or you don’t want to build it from source.And yes, loading `tfjs` and `tfjs-backend-wasm` and setting backend to `wasm` works fine - but then how to use it?```jstf.loadGraphModel('file://models/model.json'),``````errorRuntimeError: abort(TypeError: Only HTTP(S) protocols are supported).```So I've started a dev web server just to load models using *http://...* - and it works - but that really goes against idea of running NodeJS with WASM.I understand the scope of this issue is more of an enhancement, but since `WASM` is supported in `NodeJS`, expectation is that it should work natively.Also, note that missing local file I/O handler is just for model loader as loading *.wasm files from local filesystem works just fine.Just as a test, I've tried loading `tfjs-node` with `tfjs-backend-wasm` as `tfjs-node` provides local file I/O handler, but that causes hard conflict.Environment: TFJS 3.3.0 on Ubuntu 20.10 with NodeJS 15.14.0",['Just ran into the same problem!====='],1
https://github.com/tensorflow/tfjs/issues/3867,"Error trying to install DanfoJs, The Node.js native addon module (tfjs_binding.node) can not be found at path",8,closed,2020-08-31T20:14:29Z,2020-09-04T00:50:39Z,I am trying to install DanfoJS on **windows 10** using below command: ```npm install danfojs-node```but I get following error when I run ```npm start```> Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: I tried to ran ```npm rebuild @tensorflow/tfjs-node build-addon-from-source``` but still having problem.,"[""@samiragol we don't actually have windows machine to try this , but can you please list down steps what you are doing ?Also please specify tfjs versions. ====="", 'the steps are exactly as I mentioned above. here are the versions:>  @tensorflow/tfjs@2.3.0> @tensorflow/tfjs-node-gpu@2.3.0> node 12.18.1=====', 'when I ran ``` node-gyp configure --verbose``` the process throw this log:```λ node-gyp configure --verbosegyp info it worked if it ends with okgyp verb cli [gyp verb cli   \'C:\\\\Program Files\\\\nodejs\\\\node.exe\',gyp verb cli   \'C:\\\\Users\\\\p2761065\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js\',gyp verb cli   \'configure\',gyp verb cli   \'--verbose\'gyp verb cli ]gyp info using node-gyp@7.1.0gyp info using node@12.18.1 | win32 | x64gyp verb command configure []gyp verb find Python Python is not set from command line or npm configurationgyp verb find Python Python is not set from environment variable PYTHONgyp verb find Python checking if ""python3"" can be usedgyp verb find Python - executing ""python3"" to get executable pathgyp verb find Python - executable path is ""C:\\Python38\\python3.exe""gyp verb find Python - executing ""C:\\Python38\\python3.exe"" to get versiongyp verb find Python - version is ""3.8.3""gyp info find Python using Python version 3.8.3 found at ""C:\\Python38\\python3.exe""gyp verb get node dir no --target version specified, falling back to host node version: 12.18.1gyp verb command install [ \'12.18.1\' ]gyp verb install input version string ""12.18.1""gyp verb install installing version: 12.18.1gyp verb install --ensure was passed, so won\'t reinstall if already installedgyp verb install version is already installed, need to check ""installVersion""gyp verb got ""installVersion"" 9gyp verb needs ""installVersion"" 9gyp verb install version is goodgyp verb get node dir target node version installed: 12.18.1gyp verb build dir attempting to create ""build"" dir: C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator\\buildgyp verb build dir ""build"" dir needed to be created? C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator\\buildgyp verb find VS msvs_version not set from command line or npm configgyp verb find VS VCINSTALLDIR not set, not running in VS Command Promptgyp verb find VS checking VS2017 (15.9.28307.1177) found at:gyp verb find VS ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools""gyp verb find VS - found ""Visual Studio C++ core features""gyp verb find VS - found VC++ toolset: v141gyp verb find VS - found Windows SDK: 10.0.17763.0gyp info find VS using VS2017 (15.9.28307.1177) found at:gyp info find VS ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools""gyp info find VS run with --verbose for detailed informationgyp verb build/config.gypi creating config filegyp verb build/config.gypi writing out config file: C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator\\build\\config.gypigyp verb config.gypi checking for gypi file: C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator\\config.gypigyp verb common.gypi checking for gypi file: C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator\\common.gypigyp verb gyp gyp format was not specified, forcing ""msvs""gyp info spawn C:\\Python38\\python3.exegyp info spawn args [gyp info spawn args   \'C:\\\\Users\\\\p2761065\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\node-gyp\\\\gyp\\\\gyp_main.py\',gyp info spawn args   \'binding.gyp\',gyp info spawn args   \'-f\',gyp info spawn args   \'msvs\',gyp info spawn args   \'-I\',gyp info spawn args   \'C:\\\\Users\\\\p2761065\\\\Projects\\\\Charter_Gateway4\\\\screenpop-emulator\\\\build\\\\config.gypi\',gyp info spawn args   \'-I\',gyp info spawn args   \'C:\\\\Users\\\\p2761065\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\node-gyp\\\\addon.gypi\',gyp info spawn args   \'-I\',gyp info spawn args   \'C:\\\\Users\\\\p2761065\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\12.18.1\\\\include\\\\node\\\\common.gypi\',gyp info spawn args   \'-Dlibrary=shared_library\',gyp info spawn args   \'-Dvisibility=default\',gyp info spawn args   \'-Dnode_root_dir=C:\\\\Users\\\\p2761065\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\12.18.1\',gyp info spawn args   \'-Dnode_gyp_dir=C:\\\\Users\\\\p2761065\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\node-gyp\',gyp info spawn args   \'-Dnode_lib_file=C:\\\\\\\\Users\\\\\\\\p2761065\\\\\\\\AppData\\\\\\\\Local\\\\\\\\node-gyp\\\\\\\\Cache\\\\\\\\12.18.1\\\\\\\\<(target_arch)\\\\\\\\node.lib\',gyp info spawn args   \'-Dmodule_root_dir=C:\\\\Users\\\\p2761065\\\\Projects\\\\Charter_Gateway4\\\\screenpop-emulator\',gyp info spawn args   \'-Dnode_engine=v8\',gyp info spawn args   \'--depth=.\',gyp info spawn args   \'--no-parallel\',gyp info spawn args   \'--generator-output\',gyp info spawn args   \'C:\\\\Users\\\\p2761065\\\\Projects\\\\Charter_Gateway4\\\\screenpop-emulator\\\\build\',gyp info spawn args   \'-Goutput_dir=.\'gyp info spawn args ]gyp: binding.gyp not found (cwd: C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulator) while trying to load binding.gypgyp ERR! configure errorgyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (C:\\Users\\p2761065\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\lib\\configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:315:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12)gyp ERR! System Windows_NT 10.0.18363gyp ERR! command ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Users\\\\p2761065\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js"" ""configure"" ""--verbose""gyp ERR! cwd C:\\Users\\p2761065\\Projects\\Charter_Gateway4\\screenpop-emulatorgyp ERR! node -v v12.18.1gyp ERR! node-gyp -v v7.1.0gyp ERR! not ok```=====', '@samiragol can please take a look at similar issue [here](https://github.com/tensorflow/tfjs/issues/3577)..!=====', '@> @samiragol can please take a look at similar issue [here](https://github.com/tensorflow/tfjs/issues/3577)..! the link you referred is an issue with installing tensorflow but I am be able to install it, I get error when I try to run node after installing DanfoJS package. and from the error looks like its something relate to tensorflow =====', 'I was using a different node.js version that Electron was. so I installed node.js version is the same as the version of Electron is using (12.16.3) and that **solved the problem**=====', 'Thank you , glad it worked for you !=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3867"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3867"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5475,[tfjs-react-native] tf.loadGraphModel with bundled files slow,6,open,2021-08-12T14:25:49Z,2021-11-07T23:46:19Z,"**Packages installed**- ""react-native"": ""0.63.4"",- ""@tensorflow/tfjs"": ""^3.8.0"",- ""@tensorflow/tfjs-automl"": ""^1.2.0"",- ""@tensorflow/tfjs-react-native"": ""^0.6.0"",- ""expo-gl"": ""^10.4.2"",- ""expo-gl-cpp"": ""^10.4.1"",- Tensorflow.js Converter Version: v3.6.0 (as reported by the model file)**Describe the current behavior**Loading a graph model (13MB) bundled in a react-native application (bare) takes an excessive amount of time on Android devices, taking upwards of 60s.**Model**We have trained an image classification model using Google Vision and exported it as a Tensorflow.js package using their dashboard. After downloading it from the Google Vision dashboard we haven't modified the model in any way.The model.json file weighs 167KB. The weights are sharded into 4 files (each named group1-shardxof4.bin), three of which weigh 4.2MB and the last 400KB, totalling to 13MB.The three top lines of the model.json file read:```""format"": ""graph-model"",""generatedBy"": ""2.7.0"",""convertedBy"": ""TensorFlow.js Converter v3.6.0"",```**Init code**As for the model initialization in the application, we make sure that tensorflow is ready by running and awaiting tf.ready() early on in the application's lifecycle and making sure it resolves successfully before loading the model.In the file where classification happens, we then import the necessary libraries, require the bundled model files and loadGraphModel, such as:```jsimport * as tf from '@tensorflow/tfjs',import { bundleResourceIO } from '@tensorflow/tfjs-react-native',const modelJson = require('../../../assets/model/model.json'),const modelWeights1 = require('../../../assets/model/group1-shard1of4.bin'),const modelWeights2 = require('../../../assets/model/group1-shard2of4.bin'),const modelWeights3 = require('../../../assets/model/group1-shard3of4.bin'),const modelWeights4 = require('../../../assets/model/group1-shard4of4.bin'),// Basic model setup, code wrapped in a functional component which I'm not including hereconst setupModel = async () => {  try {    // Create GraphModel    const ioHandler = bundleResourceIO(modelJson, [ modelWeights1, modelWeights2, modelWeights3, modelWeights4 ]),    const graphModel = await tf.loadGraphModel(ioHandler),    // Save model for further use    // ...  }  catch (modelSetupError) {    // ...  }},useEffect(() => {  setupModel(),}, []),```graphModel is later used to create and store an automl.ImageClassificationModel, which isn't relevant here.**The issue**The issue is the **tf.loadGraphModel** method, which takes **upwards of 60s** to resolve on slightly older Android devices - such as Nexus 5x - while making the app interface completely unresponsive in the meantime.Running the application as a built release apk resulted in:Xiaomi Redmi 7: taking ~24s to load the model,Nexus 5x: taking **~60s to load the model**,Samsung A10: taking ~78s to load the model.For comparison:when ran locally on an iPhone 7 or 8 plus: ~3s to load the model,when archived, downloaded from Testflight and ran on iPhone7 or 8 plus: ~9s,when serving the model files from a locally ran node server and loading the model through http using automl.loadImageClassification(modelUrl): **17s to load the model on Nexus 5x**.**Describe the expected behavior**I would expect the loadGraphModel method to resolve faster than the reported times. Given that it's pretty simple, with only ~3k images used to train, but it's hard for me to judge whether the observed load times are reasonable and would love for someone to let me know what kind of load performance could be expected from a 13MB model. Also, if anyone could point me to how the model load time could be optimized in react-native, any steps that could've been missed and would affect the load time or a better approach to using the Google Vision-trained model (automl) in react-native, I'd greatly appreciate that 🙂 Additionally, is there any proven way of loading the model without completely blocking the js thread while it happens?Let me know if any additional information would be helpful to resolving the issue 🙂 ","['Any chance of a follow-up?=====', '@Caundy thank you for the detail report , is the slowness happening for first load or every time , if it happens for first time it is expected and you may need to write a warm up code to optimize it before loading ?=====', ""@rthadur Thanks for responding :)To answer your question, I built the apk, ran it on the Nexus and ran a couple of consecutive model loads using the tf.loadGraphModel method to test the times. The resulting load times are inconsistent, with the first load **not** taking significantly longer than the later ones. While most of the time it takes ~61 seconds to load the model - regardless of whether it's the first time or the fifth - I've also seen it being loaded in 25s, only to be reloaded in ~61s right after. The loads also only appear to either take ~61s or ~25s, with no in-between.In general I would be aiming to load the model just once per application session and store it in some service property to be used when needed without the necessity of reloading it for each use. That - along with the fact that loading the model blocks users from interacting with the app - is why the initial load time is important to me :) I am aware that the first **inference** might be much slower than the consecutive ones due to caching that happens on that first run, but does that also apply to **loading the model itself**? How would I go about writing the code to optimize it before loading it?====="", '@Caundy did you get chance to look at this older issue https://github.com/tensorflow/tfjs/issues/2177 which was dealing with similar bundle issue ?=====', ""@rthadur I believe the similarities end with both issues using the tfjs-react-native package and bundled model files. I'm not experiencing any issues using the bundleResourceIO method, as that handler resolves successfully and quickly. I am experiencing extremely long model load times using the loadGraphModel method, which are either 61s or 25s to load the model, seemingly without any rule to it.====="", 'very same issue (the difference is that the graph model NEVER loads), on ios in release mode. in debug mode everything works like a charm. i am using expo. =====']",1
https://github.com/tensorflow/tfjs/issues/142,numBytes is negative,4,closed,2018-04-07T15:01:21Z,2018-10-24T20:01:46Z,"_From @riatzukiza on March 18, 2018 22:28_The value of `numBytes` as recieved from `dl.memory()` is a negative value and appears to constantly be going down.```dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -549791510 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -576580910 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -661819910 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -736099610 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -907186460 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -2191251110 }dl.memory()Object { unreliable: false, numTensors: 8, numDataBuffers: 5, numBytes: -5602028810 }```_Copied from original issue: tensorflow/tfjs-core#867_","['_From @tafsiri on March 19, 2018 22:43_@riatzukiza Would you be able to post any sample code that resulted in this behavior?cc @dsmilkov =====', '_From @riatzukiza on March 20, 2018 20:41_# main.jsSans a bit of heading code.```jsvar kernel = dl.reshape(dl.tensor2d([    [1, 1, 1],    [1, 0, 1],    [1, 1, 1]]), [3, 3, 1, 1]),var state0Tensor = dl.randomUniform([H, W]).greater(dl.scalar(0.5, ""float32"")),var state = dl.variable(dl.cast(dl.reshape(state0Tensor, [1, H, W, 1]), ""float32"")),var nextGeneration = (function nextGeneration$() {    /* next-generation inc/dl.sibilant:2:8 */    return dl.tidy((() => {        var neighbors = dl.conv2d(state, kernel, [1, 1, 1, 1], ""same""),        var survive = dl.logicalAnd(dl.equal(state, dl.scalar(1, ""float32"")), dl.equal(neighbors, dl.scalar(2, ""float32""))),            born = dl.equal(neighbors, dl.scalar(3, ""float32"")),        return dl.cast(dl.logicalOr(survive, born), ""float32""),    })),}),var step = (function step$() {    /* step inc/dl.sibilant:2:8 */    return dl.tidy((() => {        return state.assign(nextGeneration()),    })),}),var black = rgb(0, 0, 0),var red = rgb(255, 0, 0),window.onload = (function window$onload$() {    /* window.onload eval.sibilant:50:0 */    var canvas = document.createElement(""canvas""),    document.body.appendChild(canvas),    canvas.height = H,    canvas.width = W,    var gameField = colored(canvas, red, [H, W], state),    async function start() {        await dl.nextFrame(),        console.log(""tick""),        if (!(running__QUERY)) {            return false,        },        gameField.render(canvas, step()),        return start(),    },    return start(),}),```# field.jsThe only fragment of this file to access `dl````js    render(canvas = this.canvas, state = this.state, shape = this.shape, imageData = this.imageData, ctx = this.ctx) {        if (!(running__QUERY)) {            return false,        },        var height = shape[0],            width = shape[1],        return state.data().then(((d) => {            var j = 0,                k = 0,            for (var i = 0, i < (width * height), ++(i)) {                j = (i * 4),,                this._renderCell(d[i], j, imageData)            },            return ctx.putImageData(imageData, 0, 0),        })),    }```=====', ""_From @tafsiri on March 21, 2018 16:52_Thanks @riatzukiza we'll take a look. ====="", 'Fixed.=====']",0
https://github.com/tensorflow/tfjs/issues/825,Create fused matMul + relu shader for frozen model inference,5,closed,2018-10-24T11:21:22Z,2019-03-04T22:02:10Z,"When matMul and an activation e.g. relu are adjacent in a frozen model, encode them as a single new op: `matMulWithActivation`. This should happen only for inference so don't need to worry about gradients yet.","['> [Fuse matMul with activation and bias for performance. (#1475)](https://github.com/tensorflow/tfjs-core/commit/5a95a6407959c3b49675c9de8f013f1976df49ce) @annxingyuan, very nice. Thanks. This could be used to fold ""conv2D -> BatchNorm -> Relu6"" from MobileNetV2 to single operation: folding conv2D+BatchNorm to conv2D+BiasAdd (e.g. with [graph_transform](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_batch_norms)) and then we execute this conv2D as MatMul...Has the work on modifying tfjs-converter conversion to export fused nodes already started for this, or the plan is different?  =====', ""@astojilj We have no immediate plans for modifying tfjs-converter to export fused nodes as that would require graph rewriting. However we hope to release a patch for tfjs-layers that would take advantage of fused nodes - in this case no graph rewriting is needed because the layers API allows specifying `activation` / `bias` for different layer types (dense, convolutional, etc.). We definitely would like to invest more into fusing though - we're realizing more and more how underutilized our shaders are...====="", ""@annxingyuan What's your opinion about approach where fusing would happen as part of nodes compiling in tfjs-converter - like in the [prototype here](https://github.com/tensorflow/tfjs-converter/compare/master...astojilj:fusedconvaddact#diff-91817df5c918eb884dea14ca23240852)?There, I have started prototyping fused conv2d+biasAdd+relu6, this is [corresponding tfjs-core code](https://github.com/tensorflow/tfjs-core/compare/master...astojilj:fusedconvaddact?expand=1). DeepLabV3 MobileNetV2+ inference is showing very minor performance improvement although, from original 187 operations in the model, there are 15 fused poinwise conv2d+biasAdd+relu6. I plan to do benchmarking and try to fuse more conv2d+biasAdd pairs.====="", ""@astojilj Very interesting! I'm impressed that you went down that rabbit hole with the converter :) However I think we should ideally avoid getting into the business of graph rewriting in the converter. I think the more sustainable approach is to work with the [Grappler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler) team to do this sort of work upstream. That being said, if there's a very compelling use case - i.e. a popular model being run through the converter shows *significant* improvements with fusing - we might need to reevaluate...====="", ""@annxingyuan Thanks. I could [see that Grappler](https://github.com/tensorflow/tensorflow/commit/bd2610f638bd35dca30bb00a6c16da775b3bfcdf) is already working on it.The approach I propose here is similar to what, my understanding is like that, Grappler is doing for tensorflow: in run time (during compilation) optimize graph for inference. In a way, I wouldn't use term rewrite.Alternatively, this sort of approach could be offered through custom ops to developers.I feel I'm hijacking this issue for something else. Let me branch to new issue and continue there...=====""]",0
https://github.com/tensorflow/tfjs/issues/5158,"Do not get model inferences, only thing that comes on the screen is laggy camera screen when using tensorcamera that also crushes after 1-2 minutes",6,closed,2021-06-02T15:00:51Z,2021-07-29T06:20:00Z,"I do not get any warnings or errors - only that async storage has been renamed in new expo, so I have older one installed not to cause an issue (as new one crushed the code)all code I use is here https://stackoverflow.com/questions/67790431/tfjs-react-native-for-real-time-object-detection-expo-managed-workflow","['can you please check this issue https://github.com/tensorflow/tfjs/issues/3808 if it is related ? Thank you =====', '@rthadur thanks a lot for your answer! I tried that, but the same happens + after one minute expo go gets closed by itself. Now I tried this example code: https://github.com/tafsiri/tfjs-expo-managed-example and again the same happened, even after reinstalling Expo Go on my android device. Otherwise my expo works just fine. Is it possible, that all of this is because I try to run the code on my android 10 ?=====', 'tried on Android 11, same problem. only difference is that expo does not crush after 1-2 minutes=====', 'please check a related issue here https://github.com/tensorflow/tfjs/issues/4902=====', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5158"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5158"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1602,"Strange behaviour using quantize uint8 , uint16 works fine.",0,closed,2019-05-23T02:25:45Z,2019-05-23T02:33:01Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.1.2#### Describe the problem or feature requestI'm training a model with Tensorflow TF2 Keras 2.0.0-dev20190522 , if I quantize it to uint16 for TensorflowJS via the command:tfjs.converters.save_keras_model(model, ""models_tfjs/q16"",quantization_dtype=np.uint16)it works fine,but If I change it to np.unint8 , the predictions don't work correctly anymore.command line conversion to 1 byte has similar behaviour as above. (2 bytes works)tensorflowjs_converter \--input_format=keras \--output_format=tfjs_layers_model \./models/model.h5 \./models_tfjs/q8 \--quantization_bytes 1",[],0
https://github.com/tensorflow/tfjs/issues/5751,console.warn Platform browser has already been set. Overwriting the platform with [object Object].,10,closed,2021-10-20T08:13:39Z,2021-10-21T10:59:08Z,"I am receiving this warning multiple times during my tests. I am using React `17.0.2` with jest and react-testing-library `11.2.5`. I was using `@tensorflow/tfjs` version `3.2.0` and I upgraded to `3.9.0` because I saw there is a way to suppress this error but still I get this in the console multiple times![Screenshot from 2021-10-20 11-10-54](https://user-images.githubusercontent.com/14216599/138054116-47c2bd9b-8443-46b7-9368-c350709bc7d2.png)I tried using this command `tf.env().set('PROD', true),` as described here but there are no instructions where to place it or use it. Should I use it inside `beforeEach`, `afterEach`. It would be nice to have this in the documentation.I tried using it on `setupTests.js` and at each test file individually but with no luck. Could you please advice where to use the command to suppress this error message?Thanks in advance.","[""> I was using @tensorflow/tfjs version 3.2.0 and I upgraded to 3.9.0 because I saw there is a way to suppress this error but still I get this in the console multiple timesYou don't want to suppress the warning, you want to fix it - and the core issue is that you're importing `tfjs` multiple times.TFJS should only be imported once and on import it registers its backends and runtime environment. So if you import `tfjs` again, it will throw a warning that it's already been registered.====="", 'Thanks a lot for the answer. The project I am working has some legacy code which I have not developed and I am not familiar with this package so I did not know that detail. Right now the import takes place in multiple React components. If I have to import the library only once how can I use it in multiple places (as this is the case right now) ? Or this is something that is not recommended?=====', ""there are many ways to achieve that - simplest is to place tfjs on a global object after initial import:```jsimport * as tf from ...window['tf'] = tf,```i don't like using global namespace, but it is simplest - so any component can now access tf as a global namespace.====="", 'I agree that is the reason I asked. I think ""polluting"" the window object is not the way to go but if there is no other way I will have to follow it as currently my testing console is full of error messages and I am not able to work comfortably while testing. =====', 'you can create a component that loads tfjs and then everywhere else reference that component - definitely cleaner.=====', 'I get the exact same error but tf is not  imported more than once?  The warning does not happen in the browser only in tests?=====', ""yes, it is - review the tests - and check for all variations of duplicates  for example: `tfjs` already includes `tfjs-core` so that if both are included that's a duplicate. or `tfjs-node` already includes `tfjs` and its a common mistake to require them both for node projects. or how `tfjs` bundles `tfjs-backend-webgl`  need to know which module already includes which one so not to include it twice  ====="", 'I have reviewed the tests - the only line that imports tensor flow in the whole of my application is the component under test and it has one line:import * as tf from ""@tensorflow/tfjs"",=====', "" I created a TensorFlow component as you advised ```import { useEffect } from 'react',import * as tf from '@tensorflow/tfjs',export default function TensorFlow() {  useEffect(() => {    window.tf = tf,  }, []),  return null,}```and tried importing it both only on `App.js` and on each comp that uses `tf` and then accessing it via `window.tf` but `window.tf `is always `undefined`. The issue is that there are two lets say utils files that use `tf` and then what is exported from them is used inside 4 or 5 components. Therefore, inside utils it seems that `tf `is not defined====="", ' I managed to resolve it finally after many efforts. The issue eventually was that a function inside the utils that were importing `tf` was used to initialize a variable in a redux reducer and that seems to cause importing multiples times the `tf` object and was generating the error in the testing console. When I moved that function away from that file the error stopped being generated. I would like to thank you @vladmandic for your useful observations and suggestions.=====']",1
https://github.com/tensorflow/tfjs/issues/4746,tfjs-vis not compatible with tfjs 3.1.0 and higher,8,closed,2021-02-25T14:28:30Z,2021-04-23T01:02:50Z,"regression in tfjs 3.1.0 cannot create sequential modelwhen importing tfjs using```jsimport * as tf from '@tensorflow/tfjs',```and running through a bundler (`esbuild`)  it fails to add layer to sequential model:```jsconst model = tf.sequential({ name: 'testModel' }),model.add(tf.layers.dense(layerDense),```error backtrace using tfjs 3.2.0 (error is same in tfjs 3.1.0 but line numbers may change):```logengine.js:699 Uncaught (in promise) TypeError: e.backend.decComplexRef is not a function    at b.disposeTensor (engine.js:699)    at x.dispose (tensor.js:283)    at Dense.addWeight (topology.ts:1298)    at Dense.build (core.ts:244)    at topology.ts:993    at nameScope (common.ts:48)    at Dense.apply (topology.ts:979)    at Sequential.add (models.ts:478)    at createRNN (model.js:47) <-- this is where model.add is called```environment: tfjs 3.1.0 or 3.2.0 on chrome 88reproduction code: <https://github.com/vladmandic/stocks>error is same regardless of backend used (cpu, webgl, wasm)now, i've tried tfjs 3.0.0 and it WORKS without issues  upgrading to tfjs 3.1.0 causes breaking issue which stays the same in tfjs 3.2.0  however, when using tfjs via script tag in html instead of import in js, everything works (even with tfjs 3.2.0):```html<script src=""../node_modules/@tensorflow/tfjs/dist/tf.es2017.js""></script>```all this likely indicates that some dropped import between tfjs 3.0.0 and tfjs 3.1.0 causes incorrect tree shaking.only hit is <https://github.com/tensorflow/tfjs/issues/4609>, but that was closed as stale and never resolved.","[""Hi @vladmandic , decComplexRef is removed in 3.2.0. A quick search of the repo shows no occurence of it in our codebase. Can you rm -rf dist node_modules yarn.lock and yarn install everything again? It's most likely cached install.====="", 'ah, found it: latest `tfjs-vis` is still built against older `tfjs`,  so if its loaded it causes tfjs version conflict and results in this error  changing title to `tfjs-vis not compatible with tfjs 3.1.0 and higher````logrm -rf node_modules/ package-lock.json ~/.npm/_cacache/npm installgrep @tensorflow package.json    ""@tensorflow/tfjs-vis"": ""^1.5.0"",    ""@tensorflow/tfjs"": ""=3.2.0"",    ""@tensorflow/tfjs-backend-wasm"": ""=3.2.0"",    ""@tensorflow/tfjs-backend-webgl"": ""=3.2.0"",grep -Rl decComplexRef node_modules/@tensorflow/  node_modules/@tensorflow/tfjs-vis/dist/tfjs-vis.umd.min.js  node_modules/@tensorflow/tfjs-vis/dist/tfjs-vis.umd.min.js.mapgrep @tensorflow node_modules/@tensorflow/tfjs-vis/package.json    ""@tensorflow/tfjs-backend-webgl"": ""3.0.0"",    ""@tensorflow/tfjs-core"": ""3.0.0"",    ""@tensorflow/tfjs-layers"": ""3.0.0"",```and `tfjs-vis` cannot be loaded as import, only as umd.i went ahead and did a custom build of tfjs-vis using tfjs 3.2.0 and targeting esm output and it works like a charm.(looks like tfjs-vis build procedure has not been modernized like the rest of tfjs was.)=====', '@rthadur @lina128 just to confirm, tfjs-vis 3.3.0 is still BROKEN as it\'s still built against tfjs 3.0.0 which doesn\'t work:this should be a trivial fix - it\'s just that your build platform does not update dependencies for tfjs-vis, why no progress?```log grep @tensorflow package.json    ""@tensorflow/tfjs-vis"": ""^1.5.0"",    ""@tensorflow/tfjs"": ""^3.3.0"",    ""@tensorflow/tfjs-core"": ""^3.3.0"",    ""@tensorflow/tfjs-backend-wasm"": ""^3.3.0"",    ""@tensorflow/tfjs-backend-webgl"": ""^3.3.0"",grep @tensorflow node_modules/@tensorflow/tfjs-vis/package.json  ""name"": ""@tensorflow/tfjs-vis"",    ""@tensorflow/tfjs-core"": ""3.0.0"",    ""@tensorflow/tfjs-layers"": ""3.0.0"",    ""@tensorflow/tfjs-backend-webgl"": ""3.0.0"",    ""@tensorflow/tfjs-core"": "">= 1.0.0""```=====', ""@rthadur @lina128 @pyu10055 I just checked and this is STILL broken in 3.4.0. And it's a regression since `tfjs` 3.1.0This is a completely TRIVIAL fix and it's still lingering - `tfjs-vis` simply has not updated it's dependencies since tfjs 3.0.0====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">No</a>=====', 'Will publish a new version of the tfjs-vis now=====', 'Published tfjs-vis 1.5.1 with tfjs 3.5.0 dependencies. Thanks!=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5931,Bad advice in README for the TFJS version of the Universal Sentence Encoder,2,closed,2021-12-09T09:24:41Z,2021-12-11T02:34:00Z,"Reading https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder the sample code needs to compute the dot product of two tensors and it uses arraySync on the tensors, nested for loops, and JavaScript defined vector operations. Wouldn’t it be better practice to use tf.layers.dot? The code would be much more concise and presumably much faster. (If it is the case the sample code predated tf.layers.dot then shouldn’t it be updated?)","['@pyu10055 Would you be able to confirm why it was done this way? I believe you were involved in this model?=====', ""@ToonTalk Thanks for reporting this, this should be computed on the gpu for efficiency.I used tf.matMul instead:```  const scores = tf.matMul(embeddings['queryEmbedding'],      embeddings['responseEmbedding'], false, true).dataSync(),```=====""]",1
https://github.com/tensorflow/tfjs/issues/1089,Tensor flow js model upload to Google Cloud Storage,1,closed,2019-01-12T21:59:32Z,2019-03-05T04:02:39Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js versionLatest version#### Browser versionChrome#### Describe the problem or feature requestCan't upload my saved model to Google cloud storage keep getting error 408 bad request while using upload type multipart#### Code to reproduce the bug / link to feature request",['@Comestibles this is probably a better question for [StackOverflow](https://stackoverflow.com/). We are directing people there for support questions and this one may be more related to GCS than tensorflow.js.====='],0
https://github.com/tensorflow/tfjs/issues/2358,[webgpu] Add batchNorm kernel.,0,closed,2019-11-08T19:39:46Z,2020-05-13T15:26:52Z,Used in handtracking.,[],0
https://github.com/tensorflow/tfjs/issues/1857,mobilenet.load documents an alpha config option with default 1.0 but error if not specified,5,closed,2019-08-19T15:02:42Z,2019-08-26T21:23:46Z,"Error: MobileNet constructed with invalid alpha undefined. Valid multipliers for this version are: 0.50,0.75,1.00.from `mobilenet.load({version: 2})`Despite https://github.com/tensorflow/tfjs-models/tree/master/mobilenet stating> alpha: Controls the width of the network, trading accuracy for performance. A smaller alpha decreases accuracy and increases performance. Defaults to 1.0.Also error message doesn't mention of 0.25 (and lacks space after the commas)> alpha?: 0.25 | .50 | .75 | 1.0,`mobilenet.load({version: 2, alpha: 1})`works fine.","['Version from loadinghttps://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet=====', 'Mobilenet version 2.00 does not support alpha 0.250.25 is only for version 1.00=====', 'Thank you @bileschi , @ToonTalk hope this answers your question. =====', 'Thanks. But then the documentation at https://github.com/tensorflow/tfjs-models/tree/master/mobilenet should be updated.=====', 'Thank you , updated documentation here in the [PR](https://github.com/tensorflow/tfjs-models/pull/284) =====']",0
https://github.com/tensorflow/tfjs/issues/5912,tfjs-react-native expo ERESOLVE unable to resolve dependency tree,2,closed,2021-12-02T02:23:23Z,2021-12-03T00:29:47Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):Only installing right now and following this guide [Here](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native) - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur- TensorFlow.js installed from (npm or script link): none yet- TensorFlow.js version (use command below): none yet- Browser version: chrome- Tensorflow.js Converter Version: none yetI have another issue [here](https://github.com/tensorflow/tfjs/issues/5897) and noticed it is probably my dependencies being the issue for the first one listed so I decided to start fresh. I reset everything and started installing all of the packages in order [here](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native). When I get to `Install and configure react-native-fs` I get the following errors:![Screen Shot 2021-12-01 at 6 20 49 PM](https://user-images.githubusercontent.com/47552416/144345644-8c862ad3-2b56-4f19-84d6-e7c06cf98954.png)package.json:![Screen Shot 2021-12-01 at 6 22 09 PM](https://user-images.githubusercontent.com/47552416/144345798-d1f61d9e-2c1e-4017-8327-5aa12a79528a.png)","['Downgrading to npm 6 worked=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5912"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5912"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2998,[WASM] The value of tensor created from TypedArray is incorrect,4,closed,2020-03-31T04:39:59Z,2020-05-27T18:30:49Z,"#### TensorFlow.js version- @tensorflow/tfjs-core@""1.5.2""- @tensorflow/tfjs-backend-wasm@1.5.1-alpha5#### Browser version- 80.0.3987.132 (Official Build) (64-bit)#### Describe the problem or feature requestThe issue is shown as the following picture. The data of bias is from a [squeezeNet](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz) tflite model and its value has changed after an add operation. I'm still trying to find a more simple way to reproduce this issue. ![issue](https://user-images.githubusercontent.com/44738552/77986731-6fe0d080-734a-11ea-8ee8-10d2871eedb7.png)Do you have any idea of the potential cause, may it cause by the type or the precision of the variable?Looking forward to you reply.","[""#### Code to reproduce the bugFound a common way to reproduce this issue just now. I reuse the code of the sample of [webpack](https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm/starter/webpack), just need to rewrite the function [run()](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/starter/webpack/src/index.js#L21-L25) as follows:`async function run() {``  // Create a tensor from arrayBuffer`` ``  var data = [-1132269917, 1028860590, 991861027, 1038002411, -1070382690, 1012508611, 1038155669, 1020400317, 1021993200, 1003353622, -1071122272, 1026564351, 1032339440, 1032425711, 1034563524, 1024892554, 1027299904, 1033349386, 1041155330, 1034166925, 1041501890, -1093047882, -1099473869, -1096050077, -1098527033, -1091974403, -1096862997, 1036731708, 1049547796, 1019814215, 1031452522, 1029429360, -1094537812, 1042169757, 1017258414, 1007595120, -1143345557, 1050487005, -1097259576, -1108488449, -1073552025, -1130910040, 1047248971, 1001644069, 1025110182, 1042517571, 1002539717, 1041357420, 1023918362, -1091949005, 1038927025, -1105753473, -1086086134, -1090195871, 1032900068, 1038858460, 1036881066, -1125031069, 1035177010, 1028151202, 1041087741, 1027650220, 1036019015, -1115176233, 1038089265, -1090045943, 1027957262, 1017695537, 1048773706, -1130922018, 1021999085, 1041367801, 1049640461, -1077502711, -1073198811, 1041359913, 1041133318, 1033359553, 1039185066, -1114848176, 1027660658, -1106981919, 1028790231, 1021494398, 1052030906, 1035564792, 1034262398, -1123659351, 1030045095, 1033315188, 1014507782, -1087299909, 1015958672, -1092315154, -1090311538, 1019947795]`` ``  var buffer = new ArrayBuffer(5006664),``  var int32 = new Int32Array(buffer, 4987712, 96),``  for (let i=0, i<96, ++i){``    int32[i] = data[i],``  }``  var b = new Float32Array(buffer, 4987712, 96),``  var bias = tf.tensor(b, [96], 'float32'),``  bias.print(),`` ``  setWasmPath(wasmPath),``  await tf.setBackend('wasm'),`` ``  var a = new Float32Array(96),``  var input = tf.tensor(a, [96]),``  var output = input.add(bias),``  bias.print(),``}`All of the params I used above are from the squeezeNet tflite model. In this example, bias is a tensor create from a Float32Array which is part of a arrayBuffer and the value of bias will change after the add op. And I find that it will use the value of the arrayBuffer without the specified parameter 'byteOffset'. You can test by assign value to the first few bytes of the arrayBuffer.It seems caused by the use of function `tf.setBackend('wasm')`, bacause the tensor's value will be incorrect from the beginning if you setBackend before create the tensor. And this issue won't appear if use backend 'webgl'.====="", ""I'm also experiencing this bug, it makes it difficult to conditionally switch backends as this code works with the webgl backend and not the wasm backend.====="", ""> I'm also experiencing this bug, it makes it difficult to conditionally switch backends as this code works with the webgl backend and not the wasm backend.So that seems to be a commont bug when switch the backend from webgl to wasm. I used to investigate that and suspect this issue may caused by [code here](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/src/backend_wasm.ts#L87) since the default byteOffset is 0. But I also found that the typedArray has been processed, so I'm not sure this is the root cause. @annxingyuan  Do you have time to take a look at this issue, thanks a lot!====="", ""A simpler way to recreate the issue is:```await tf.setBackend('wasm'),const data = [-0.1, 0.2, 0.3],const buffer = new ArrayBuffer(32),const view = new Float32Array(buffer, 8, data.length),for (let i=0, i < data.length, ++i) {  view[i] = data[i],}const t = tf.tensor(view),t.print(),```This results in:`Tensor    [0, 0, -0.1]`From what I see from the link you posted, seems the fix should be as simple as using the TypedArray byteOffset property and not 0.=====""]",0
https://github.com/tensorflow/tfjs/issues/2014,EnumTypeWrapper' object has no attribute 'DT_FLOAT' while importing in Python 3,8,closed,2019-09-11T08:27:24Z,2019-09-19T23:37:15Z,"## TensorFlow.js version1.2.9#### Browser versionDoes not matter#### Describe the problem or feature requestWhile importing tensorflowjs module in python3 (using Google Colab environment), I get the following error:`---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)<ipython-input-6-4aa5f30b4c2f> in <module>()      1 from google.colab import drive      2 from google.colab import files----> 3 import tensorflowjs as tfjs      4       5 drive.mount('/content/gdrive')4 frames/usr/local/lib/python3.6/dist-packages/tensorflowjs/__init__.py in <module>()     19      20 # pylint: disable=unused-imports---> 21 from tensorflowjs import converters     22 from tensorflowjs import quantization     23 from tensorflowjs import version/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/__init__.py in <module>()     22 from tensorflowjs.converters.keras_tfjs_loader import deserialize_keras_model     23 from tensorflowjs.converters.keras_tfjs_loader import load_keras_model---> 24 from tensorflowjs.converters.tf_saved_model_conversion_v2 import convert_tf_saved_model/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py in <module>()     39 from tensorflowjs.converters import common     40 from tensorflowjs.converters import fold_batch_norms---> 41 from tensorflowjs.converters import fuse_prelu     42      43 # enable eager execution for v2 APIs/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/fuse_prelu.py in <module>()    198     prelu_fn(tf.constant(1.0), tf.constant(1.0))    199 --> 200 register_prelu_op()/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/fuse_prelu.py in register_prelu_op()     40      41   value = attr_value_pb2.AttrValue()---> 42   value.list.type.extend([types_pb2.DataType.DT_FLOAT])     43   attr = op_def_pb2.OpDef.AttrDef()     44   attr.name = 'T'AttributeError: 'EnumTypeWrapper' object has no attribute 'DT_FLOAT'`#### Code to reproduce the bug / link to feature requestYou can try it on Google Colab.https://colab.research.google.com/drive/1gftM84djEwrxNfgqbriIzxatI5S_p9aX","['seems to be a bug , @pyu10055 can you please take a look ?=====', ""AttributeError: 'EnumTypeWrapper' object has no attribute 'DT_FLOAT'Same problem====="", '+1=====', 'I tried it with version **1.2.6** and it does not have this error. **1.2.9** has=====', '`pip install tensorflowjs==1.2.6` seems to fix this issue.=====', 'I believe this is fixed by this PR https://github.com/tensorflow/tfjs/commit/dd5d9ed60e9cadf0426304d5a470504fd9b6d44ayou should see the changes in next release.=====', 'on version 1.2.6 I get this error:> ValueError: tf.enable_eager_execution must be called at program startup.=====', '""pip install tensorflowjs==1.2.6"" worked for me, thanks=====']",0
https://github.com/tensorflow/tfjs/issues/5278,Build tfjs-layers with Bazel,1,closed,2021-07-01T20:32:34Z,2021-10-22T01:57:55Z,"This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package, take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275.Tests depend on #5277.",['Closed by #5672 ====='],1
https://github.com/tensorflow/tfjs/issues/738,Unknown layer: Dot,1,closed,2018-09-27T08:16:26Z,2018-09-27T20:59:12Z,"Hi there, I've seen an issue reported a while ago, https://github.com/tensorflow/tfjs/issues/218, where this ""Unknown layer: Dot"" was mentioned, but there was no a clear solution. Besides the error message seems to be different in my case:""Uncaught (in promise) Error: Unknown layer: Dot. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().""I'm using **TensorFlow.js 0.13.1**, in **Chrome 68**The keras model trained can be found here: https://github.com/jscriptcoder/date-translator/tree/master/jupyter, `dates_model.h5` file, and the converted model is here: https://github.com/jscriptcoder/date-translator/tree/master/tfjsmodel. The Jupyter notebook with the training is here: https://bit.ly/2Oc14Ce.I'm using **Tensorflow 1.10.0** as backend and **Keras 2.2.2**. To convert the model I used **Tensorflowjs 0.6.0**.Now, the error message seems to be quite clear when it says: ""_The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code_"", in that case the question is, how can I do so?Thanks a lot,Fran",['This is a duplicate of #218====='],0
https://github.com/tensorflow/tfjs/issues/176,"Where is ""/homepage/assets/support.js"" in  demo of tfjs-core?",1,closed,2018-04-11T06:52:07Z,2018-04-13T16:40:19Z,"In every demo's file ""index.html"", there's a line of code: ` <script src=""../homepage/assets/support.js""></script>`But I can't find the path “homepage”",['@wdl000001 The demos in tfjs-core are deprecated and we are in the process of deleting/migrating them which is probably why that file is missing. Our new examples are at https://github.com/tensorflow/tfjs-examples. Sorry for the confusion we will be cleaning that up soon.====='],0
https://github.com/tensorflow/tfjs/issues/4681,Unknown op 'SparseTensorDenseMatMul' and 'SparseMatMul',1,open,2021-02-15T05:03:41Z,2021-06-03T16:15:37Z,"**System information**- TensorFlow.js version (you are using): 3.0.0- Are you willing to contribute it (Yes/No):**Describe the feature and the current behavior/state.**I am trying to convert a TF2 saved model using [sparse_dense_matmul](https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul) (or alternatively [matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) with `b_is_sparse=True`, but can't as tfjs does not support that yet.**Will this change the current api? How?** The only change will be a new transformation for people to use.**Who will benefit with this feature?**I am using this function in my TF model to multiply a tensor from my training batch against a transformation matrix (not dissimilar from a [DFT matrix](https://en.wikipedia.org/wiki/DFT_matrix)). My transformation matrix happens to be very sparse (full of 0s in my case) and is suitable to be converted to a sparse tensor. However, I would not get the speed boost (I would get the memory boost) that comes with a sparse tensor unless I use a function like sparse_dense_matmul or sparse matmul.Functionality like this is very useful in signal processing, particularly audio processing (my use case) where transformation matrices like this exist.**Any Other info.**",['cc @ping @mattsoulanille ====='],1
https://github.com/tensorflow/tfjs/issues/4838,Can not convert TensorFlow model to TensorFlowJS: Unsupported Ops in the model before optimization,29,closed,2021-03-19T15:14:11Z,2021-06-24T15:42:55Z,"Hi, I work on VS Code and we are trying to use TensorFlow for automatic programming language classification based on file content.To be concise, we need this working in a browser, thus we would like to use TensorFlowJS, and we would like to reuse this TensorFlow model https://github.com/yoeo/guesslang/tree/master/guesslang/data/modelWe are trying to convert this Model to a TensorFlowJs model using your conversion scripts and we are hitting the following issue:```$ tensorflowjs_wizard2021-03-17 23:39:53.238316: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0', dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-03-17 23:39:53.238359: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Welcome to TensorFlow.js Converter.? Please provide the path of model file or the directory that contains model files.If you are converting TFHub module please provide the URL. /workspaces/tfjs-notebooks/master/guesslang-master/guesslang/data/model/saved_model.pb? What is your input model format? (auto-detected format is marked with *) Tensorflow Saved Model *? What is tags for the saved model? serve? What is signature name of the model? signature name: predict? Do you want to compress the model? (this will decrease the model precision.) No compression (Higher accuracy)? Please enter shard size (in bytes) of the weight files? 4194304? Do you want to skip op validation?This will allow conversion of unsupported ops,you can implement them as custom ops in tfjs-converter. No? Do you want to strip debug ops?This will improve model execution performance. Yes? Do you want to enable Control Flow V2 ops?This will improve branch and loop execution performance. Yes? Do you want to provide metadata?Provide your own metadata in the form:metadata_key:path/metadata.jsonSeparate multiple metadata by comma.? Which directory do you want to save the converted model in? /workspaces/tfjs-notebooks/src/new_model? The output already directory exists, do you want to overwrite it? Yesconverter command generated:tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=predict --strip_debug_ops=True --weight_shard_size_bytes=4194304 /workspaces/tfjs-notebooks/master/guesslang-master/guesslang/data/model /workspaces/tfjs-notebooks/src/new_model15:34but alas...ValueError: Unsupported Ops in the model before optimizationSparseFillEmptyRows, StringToHashBucketFast, SparseReshape, SparseSegmentSum, SparseSegmentMean```1. Is it even possible to convert this model to TensorFlowJS?2. Do we have something misconfiguredI am a TensorFlow beginner so I apologise for not the nicest issue. Any help is appreciated. Thank youfyi @tylerLeonhardt","['Hi @isidorn, based on the log, this model uses SparseTensor, currently it is not supported by TensorFlow.js. We will investigate how much effort is needed to add that support, and get back to you. It would also be good to understand your timeline for this feature? Thanks=====', '@pyu10055 thanks for your reply! We do not have a strict timeline, we just think it would be a very cool feature to have. Especially for new users who do not know how to set a language mode, so automatic classification would help a lot.Our plan was to experiment with this in April, but we can push it out to May, June or whenever this is possible.In case you are more interested about this feature on the VS Code you can take a look at this issue https://github.com/microsoft/vscode/issues/118455 it has a proof of concept demo and some discussion and a linked PR=====', '@isidorn Nice, we are heavy users of VSCode, thank you for the great tool. We have done some research on the SparseTensor ops, I think it will be fairly straightforward for us to add those missing ops. Initial estimation would be around May timeframe.=====', '@pyu10055 fantastic news, thanks a lot 👏 I am really glad you like VS Code 😊 =====', ""@isidorn My understanding is that VSCode is an electron app, we have node package tfjs-node that is built on top of tensorflow c++ runtime. And we have the [saved model direct execution API](https://blog.tensorflow.org/2020/01/run-tensorflow-savedmodel-in-nodejs-directly-without-conversion.html). With which you don't need to do the model conversion, and any saved model can be executed directly. I am wondering if that can get you started?====="", 'VS Code is also used for GitHub Codespaces which is ""vscode in your browser"":![image](https://user-images.githubusercontent.com/2644648/112207981-0bf78d80-8bd5-11eb-853a-eaf44e63b018.png)So we can\'t assume we\'ll always have a node runtime.=====', ""@pyu10055 thanks and yes you are correct we are an Electron app, however as @TylerLeonhardt pointed out we also have to run in the Browser for various scenarios. Apart from that the new Electron that is coming out is changing it's architecture so that all renderer processes are node free, and only one shared process can have a node dependency. We would prefer to do the classification in the renderer where it will be used.So due to the above we would like to drop the node dependency.But you are also right, that was enough to get us started, so @TylerLeonhardt published a VS Code extension that uses the node dependency, so you can try it out in case you are interested https://marketplace.visualstudio.com/items?itemName=TylerLeonhardt.auto-languageHowever our final goal is to have this in VS Code core====="", 'Just a heads up, my extension only works on linux as I never took the time to figure out how to properly distribute the native bits for each platform within the extension.=====', '@isidorn @TylerLeonhardt The extension looks very nice, and now we understood more about your use case, we will add those missing ops and keep you posted.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Pls no bot 🙏=====', 'Just to check if there are any updates regarding this issue?Also adding @yoeo to the discussion, the original author of the Model we want to convert.Thanks!=====', '@isidorn All ops are supported, we are investigating some issues with the model execution, will try to update here as it progresses.=====', '@isidorn @TylerLeonhardt The execution issue has been fixed, and the model is compatible with TFJS now.you can try out the demo here https://storage.googleapis.com/tfjs-testing/guesslang-demo/index.htmlSince the model is fairly small and involves dynamic shaped string tensors, it is actually faster on CPU backend.Currently the demo is compiled with the latest code from the master branch of TFJS, we will release a new version next week, after that you should be able to use the npm package directly.=====', '@pyu10055 this is amazing, I just tried out the demo! We will look into this on the VS Code side next milestone, for more details on our side you can follow this issue https://github.com/microsoft/vscode/issues/118455Thanks a lot 👏 👏 =====', '@pyu10055 do you think you can let us know when the npm package goes out?=====', '@TylerLeonhardt should be this week.=====', '@TylerLeonhardt @isidorn we have just released 3.7.0, please take a look at the updated [demo](https://storage.googleapis.com/tfjs-testing/guesslang-demo/index.html)=====', 'Super cool ✨ Next week I will try to look into integrating this into vscode, the week after I am on vacation, so I might look into it once I get back. Though maybe @TylerLeonhardt jumps on this in the meantimeShould we close this issue and continue the discussion in our repo? https://github.com/microsoft/vscode/issues/118455Thanks a lot 👏 =====', 'Closing this issue since the model has been fully supported with 3.7.0, please keep us posted on your progresses. cheers, happy Friday.=====', ""@pyu10055 I'm curious, what did you use to convert the model into the tfjs model? tensorflow_wizard?====="", '@TylerLeonhardt `tensorflow_wizard` should work, also there is `tensorflow_converter`, if you familiar with the params.=====', '@pyu10055 I\'m very curious what your `tensorflow_converter` command looked like while converting the model. I gave it a go just now and didn\'t have the same luck:```root@a4861c6ad41f:/guesslang/guesslang/data/model# tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=classification --strip_debug_ops=True --weight_shard_size_bytes=4194304 /guesslang/guesslang/data/model ./foo2021-06-11 04:33:01.965439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\', dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-06-11 04:33:01.965500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-06-11 04:33:03.004337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcuda.so.1\', dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2021-06-11 04:33:03.004400: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)2021-06-11 04:33:03.004426: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a4861c6ad41f): /proc/driver/nvidia/version does not exist2021-06-11 04:33:03.004720: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-06-11 04:33:03.202627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)2021-06-11 04:33:03.203916: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299965000 HzWARNING:tensorflow:Issue encountered when serializing global_step.Type is unsupported, or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing variables.Type is unsupported, or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing trainable_variables.Type is unsupported, or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.2021-06-11 04:33:03.576848: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 02021-06-11 04:33:03.577123: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session2021-06-11 04:33:03.584601: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn\'t exist in graphWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:394: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.Instructions for updating:This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:399: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py:857: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 813, in pip_main    main([\' \'.join(sys.argv[1:])])  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 817, in main    convert(argv[0].split(\' \'))  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 804, in convert    weight_shard_size_bytes, metadata_map)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 533, in _dispatch_converter    metadata=metadata_map)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 691, in convert_tf_saved_model    metadata=metadata)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 154, in optimize_graph    \', \'.join(unsupported))ValueError: Unsupported Ops in the model before optimizationReadVariableOp, OptionalFromValue, OptionalNone```I ran this in a fresh python:3.6-buster docker container.=====', 'I just published a new version of my extension:https://marketplace.visualstudio.com/items?itemName=TylerLeonhardt.auto-languageJust to ""get it working"" I pull in the model.json you have hosted:https://github.com/TylerLeonhardt/vscode-autountitledlanguage/blob/main/src/extension.ts#L97but interestingly, I did originally try downloading the `model.json` but it looks like `tf.loadGraphModel(` does not allow me to pass it a file path or a `file://` uri. It says the Uri is not absolute. Any ideas around this? Any other way to provide the model.json to tfjs?=====', '@tyler I just tried out your extension and this works great! Awesome job 👏 @pyu10055 thanks again for making this possible!I have commented with some ideas on what we should do in order to ship this to all our users ✨ https://github.com/microsoft/vscode/issues/118455=====', ""Also, @pyu10055 if you'd like me to open new issues on any of the above feel free to let me know. I'm happy to :)====="", '@TylerLeonhardt you can use following command:```tensorflowjs_converter --input_format=tf_saved_model --skip_op_check model web_model```=====', '@TylerLeonhardt TFJS model loading API aims for browser usage, which uses fetch API under the hood. It does have a file loader, but it is only for node.js. For loading from local file system within vscode, you might need to implement the custom loader API.You can take a look at this doc https://www.tensorflow.org/js/guide/save_load=====', '@pyu10055 did you mean this section:![image](https://user-images.githubusercontent.com/2644648/123292423-e1fd8200-d4c7-11eb-93d2-0ac7c4d992e2.png)Looks like all of the links 404. Example: https://github.com/tensorflow/tfjs-core/blob/master/src/io/types.ts#L165~Have they moved somewhere else?~ahh... found ithttps://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/io/types.ts=====']",1
https://github.com/tensorflow/tfjs/issues/3140,tfnode.node.encodeJpeg is returning a black image,3,closed,2020-04-22T22:11:20Z,2021-01-10T02:30:25Z,"#### TensorFlow.js version""^1.7.2""#### node-js version12.16.2#### Describe the problem or feature requesti try to convert a Tensor to a image JPEG using`tfjs-node.node.encodeJpeg` but when i save the imagen its save as a black image, but if i try the same code but in the browser using `browser.toPixels` the image is fine[code reference ](https://github.com/reiinakano/arbitrary-image-stylization-tfjs)[chicago image](https://github.com/reiinakano/arbitrary-image-stylization-tfjs/blob/master/images/chicago.jpg)[seaport image](https://github.com/reiinakano/arbitrary-image-stylization-tfjs/blob/master/images/seaport.jpg)#### Code to reproduce the bug / link to feature request``` javascript const tf = require(""@tensorflow/tfjs""),const tfnode = require(""@tensorflow/tfjs-node""),const fs = require(""fs""),let img1 = fs.readFileSync(""chicago.jpg""),let img2 = fs.readFileSync(""seaport.jpg""),const loadModel = async (path) => {  return await tf.loadGraphModel(path),},const loadMobileNetStyleModel = async () => {  const path = `https://reiinakano.com/arbitrary-image-stylization-tfjs/saved_model_style_js/model.json`,  return await loadModel(path),},const loadTranformModel = async () => {  const path = `https://reiinakano.com/arbitrary-image-stylization-tfjs/saved_model_transformer_separable_js/model.json`,  return await loadModel(path),},const start = async (image1, image2) => {  const net = await loadMobileNetStyleModel(),  const transformNet = await loadTranformModel(),  const imageToPixel = (img) =>    tfnode.node.decodeJpeg(img).toFloat().div(tf.scalar(255)).expandDims(),  const pixelImage1 = await imageToPixel(image1),  const pixelImage2 = await imageToPixel(image2),  let bottleneck = net.predict(pixelImage2),  const stylized = await transformNet    .predict([pixelImage1, bottleneck])    .squeeze(),  //in the browser works  //   tf.browser.toPixels(stylized, document.getElementById(""canvas"")),  //in nodejs  const newImg = await tfnode.node.encodeJpeg(stylized),  fs.writeFileSync(""out.jpg"", newImg),  return stylized,},start(img1, img2),```","[""I have no problem generating an output jpeg.Here my code:It assumes your input matrix is a float32 matrix with values between 0 and 1. If you have float values higher than 1 or below 0, they will be black.```javascriptfunction tensor2DToImage(matrix, filename){    const file = './output/'+filename,    tf.node.encodeJpeg(matrix.mul(255).cast('int32').expandDims(2), 'grayscale', 100).then((data) => {        /*fs.access(file, fs.constants.W_OK, function(err){            if(!err){*/                fs.writeFile(file, Buffer.from(data), function (err) { }),            /*}        }),*/    }),}```====="", 'Closing this due to lack of activity, feel to reopen. Thank you=====', 'Still obsoleted, but for reference.Stylized output range is [0-1], but encodeJpeg looks like require [0-255] image output range.So you have to apply mul(255) for stylized output.from```js  const stylized = await transformNet    .predict([pixelImage1, bottleneck])    .squeeze(),```to```js  const stylized = await transformNet    .predict([pixelImage1, bottleneck])    .mul(255)    .squeeze(),```and it works.![out](https://user-images.githubusercontent.com/31843888/104112860-21c81700-5337-11eb-894b-6027fd5fe791.jpg)=====']",0
https://github.com/tensorflow/tfjs/issues/5950,Universal Sentence Encoder support for wasm backend,2,closed,2021-12-16T00:30:10Z,2021-12-16T18:38:33Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 3.11.0- Are you willing to contribute it (Yes/No): yes, but I don't want to commit to doing it bc I don't yet understand how much work it requires.**Describe the feature and the current behavior/state.**I want to use the Universal Sentence Encoder model with a Wasm backend, because I don't have access to webgl and cpu is really slow. According to the [list of supported wasm models](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md), USE is not yet supported.**Will this change the current api? How?**I assume not but can't say for sure.**Who will benefit with this feature?**Anyone looking to use the USE model with a wasm backend.**Any Other info.**","['@pyu10055 @jasonmayes =====', '@marcgreen USE is supported by wasm, we will update the readme to reflect that. thanks!=====']",1
https://github.com/tensorflow/tfjs/issues/822,Is it useful if I add a simple example on tfjs-examples posenet with video feed and not webcam?,9,open,2018-10-23T11:55:23Z,2021-08-29T13:39:32Z,Is it useful if I add a simple example on tfjs-examples posenet with video feed and not webcam?It will enable anyone to get posenet estimation from a downloaded video easily by replacing a placeholder video.,"['I think it\'ll be useful! ""help wanted"" tag added. :)=====', 'Quick question, would the placeholder video be in the repository or hosted somewhere? Is this something that could potentially be added to the existing posenet demo?cc @nsthorat @oveddan=====', '@tafsiri it could be both for the video, hosted outside or a really lightweight one in the folder in the repository. What would be better?=====', ""I have a preference towards something that can fit in the existing posenet demo folder (e.g. maybe a video.js to complement the camera.js), just so that it is easy to find and we can point people to one thing (as opposed to a separate folder in a different repo). As for video location having it hosted means that the demo can just work being served off a static host like we do now so that is also good, if there is something suitable we can find that is a few hundred kilobytes or something that would be good.However I'll let others chime in who have done more work on that model.====="", 'cc @oveddan=====', '@Giorat @tafsiri I think a demo like this would be great.  Hosting a couple videos on a cdn would work.It would also be great if the image demo would support drag & drop of an image (but thats for a separate issue).=====', ""I could really use a demo with a video file. I'm working on a project using tfjs posenet and am struggling with how to process a video, image is easy but a full video is a challenge. ====="", 'I want to solve this issue but I am a begineer in open source so pls anyone guide me in this=====', '@tafsiri is this issue still open?=====']",0
https://github.com/tensorflow/tfjs/issues/1802,[Codelab]: Making Predictions from 2D Data,0,closed,2019-08-09T12:58:40Z,2019-08-09T14:42:46Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### Describe the problem or feature request### Syntactic error: First we need to make some predictions. Here we will **make take** 500 images and predict what digit is in them (you can increase this number later to test on a larger set of images).### Suggestion: First we need to make some predictions. Here we will **make** 500 images and predict what digit is in them (you can increase this number later to test on a larger set of images).",[],0
https://github.com/tensorflow/tfjs/issues/4679,`@tensorflow/tfjs` is missing dependency on `seedrandom`,5,closed,2021-02-14T02:19:03Z,2021-04-09T16:09:41Z,"**System information**- TensorFlow.js installed from (npm or script link): `3.0.0`**Describe the current behavior**`@tensorflow/tfjs-data` has a peer dependency on `seedrandom`:https://github.com/tensorflow/tfjs/blob/1612c9a316dd4edeea9c9ef9cd6ab029f43741b5/tfjs-data/package.json#L71`@tensorflow/tfjs` depends on `@tensorflow/tfjs-data`, but does not provide the `seedrandom` peer dependency:https://github.com/tensorflow/tfjs/blob/1612c9a316dd4edeea9c9ef9cd6ab029f43741b5/tfjs/package.json#L92**Describe the expected behavior**`@tensorflow/tfjs` should provide the `seedrandom` peer dependency for `@tensorflow/tfjs-data`.","['This is a duplicate , here is the similar issue opened to track the same https://github.com/tensorflow/tfjs/issues/3875 , Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4679"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4679"">No</a>=====', '@rthadur Are you sure this is the same issue? This is a dependency issue - not a bundling issue.=====', 'Any solution for this issue? Thx=====', '> Any solution for this issue? ThxWhat package manager are you using?=====']",1
https://github.com/tensorflow/tfjs/issues/1277,Multiple GPUs with tfjs-node-gpu,10,closed,2019-02-23T11:57:59Z,2020-09-02T06:30:15Z,#### TensorFlow.js version tfjs-node-gpu#### Describe the problem or feature requestAre multiple GPUs supported for the training of a single model?Something like [multi_gpu_model](https://keras.io/utils/#multi_gpu_model) in keras.Is it possible to take advantage of Nvidia tensor cores with FP16 precision?Are there plans to expose [gpu_options](https://github.com/tensorflow/tfjs/issues/671)?,"['+1=====', ""I've made a test with two GPUs and they are being used. ====="", 'With multiple GPUs, memory is allocated but there is no performance benefit.It is the other way around, performance degrades.Simple benchmark with oversized model:```Total params: 11233402 / Batch size: 5121 GPU - 72295.879ms/epoch1 GPU - 74323.471ms/epoch2 GPUs - 97455.554ms/epoch2 GPUs - 98588.145ms/epochTotal params: 44843194 / Batch size: 2561 GPU - 121129.245ms/epoch1 GPU - 121760.142ms/epoch2 GPUs - 158337.767ms/epoch2 GPUs - 157352.398ms/epoch_________________________________________________________________Layer (type)                 Output shape              Param #=================================================================conv2d_Conv2D1 (Conv2D)      [null,26,26,4]            40_________________________________________________________________....._________________________________________________________________conv2d_Conv2D2 (Conv2D)      [null,24,24,8]            296_________________________________________________________________....._________________________________________________________________conv2d_Conv2D5 (Conv2D)      [null,18,18,64]           18496_________________________________________________________________....._________________________________________________________________conv2d_Conv2D8 (Conv2D)      [null,6,6,512]            524800_________________________________________________________________....._________________________________________________________________dense_Dense1 (Dense)         [null,4096]               33558528_________________________________________________________________...._________________________________________________________________dense_Dense2 (Dense)         [null,10]                 40970=================================================================Total params: 44843194Trainable params: 44822594Non-trainable params: 20600_________________________________________________________________```Looks like there is no benefit of having multiple GPUs....=====', '```Sun Mar  3 22:36:45 2019       +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce GTX 108...  Off  | 00000000:04:00.0  On |                  N/A || 32%   58C    P2   130W / 250W |  10801MiB / 11176MiB |     30%      Default |+-------------------------------+----------------------+----------------------+|   1  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A || 26%   50C    P2    60W / 250W |  10631MiB / 11178MiB |      2%      Default |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================||    0     36327      C   node                                       10227MiB ||    1     36327      C   node                                       10619MiB |+-----------------------------------------------------------------------------+```when I use tfjs-node-gpu, in default this looks like occupy both of the GPU (as shown in FIG).  and actually this use one or two?=====', ""Thanks for sharing @chenqing ! I see the same with two cards.Memory is allocated and both are used but without actual improvement of training speed.Probably this depends on training data. I've used MNIST for testing. Maybe distributing the load between the cards is not effective with small images.@annxingyuan , please can you share an insight into how two cards are used in parallel?====="", ""I will share my findings and close this.I've tested with RTX 2080 Ti cards. They are not supported by CUDA 9 and must have version >= 10.Tensorflow(python) on its turn does not have a release that supports CUDA 10. The user could compile it by himself. I did it with TF 1.13.1 and CUDA 10. Python works fine.However, TFJS doesn't work with CUDA 10 and TF 1.13 and node binary is not compiled properly (without any error message, unfortunately).This means:> Nvidia tensor cores with FP16 precision are not supported> RTX cards are not supported at all, very sad :(====="", ""Sorry for reopening, but the question is still same and, I guess, not answered. I have my NN training on single 1080Ti. I'm just about to buy a second 1080Ti to speed up things twice. Will this work? General idea is to get 6xGTX1080Ti setup later. If not, is there any option to configure tfjs-node-gpu to use a single specified gpu, so it will be possible for 2 GPUs system to run 2 separate processes where each one will train its own different NN?According to nvidia-smi logs above it looks like tfjs-node-gpu gets all GPUs prepared for training, but only one really works. Isnt that a bug? ====="", '+1=====', '@nsthorat =====', '+1There is no distributed training support like the tf.distribute.MirroredStrategy in tfjs at present.=====']",0
https://github.com/tensorflow/tfjs/issues/1426,Feature request : `tf.image.transform`,4,closed,2019-03-20T10:25:49Z,2020-06-05T05:36:17Z,"#### TensorFlow.js version1.0.0#### Browser versionAll#### Describe the problem or feature requestI'm working on a js implementation of image augmenter (work in progress in https://github.com/piercus/image-augment).In this context, i would like to access a geometric transform operator in tensorflow.js that would be equivalent to https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform","['[`ImageProjectiveTransform`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/kernels/image_ops.cc#L127) is not available in the [ops list](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/ops.pbtxt)Does anyone know if this might be added in the future ?Is there any other way to achieve this in tfjs-node ?=====', 'Thanks! Are you using node or the browser?=====', ""I'm using node====="", 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/5034,TensorFlow.LayersModel.dispose() leaves tensors behind,4,closed,2021-05-04T21:48:24Z,2021-05-19T23:31:56Z,"Not quite sure this should be reported, since I found a workaround and I'm not sure it applies to **all** models, but here it goes:### System information- Have I written custom code: no- OS Platform and Distribution: Windows 10 Pro 20H2 (build 19042.928)- TensorFlow.js installed from: npm - TensorFlow.js version: ^3.3.0- Browser version: Microsoft Edge Version 90.0.818.51 (Official build) (64-bit)### Code```jsimport * as TensorFlowLayers from '@tensorflow/tfjs-layers',console.log('numTensors (before model): ' + TensorFlow.memory().numTensors),const modelUrl = '[link to a model JSON]', // this is the only line you must changeconst model = await TensorFlowLayers.loadLayersModel(modelUrl),console.log('numTensors (before dispose): ' + TensorFlow.memory().numTensors),model.dispose(),console.error('numTensors (after dispose): ' + TensorFlow.memory().numTensors),```### Describe the current behaviorNot all tensors are disposed. See output:>numTensors (before model): 0numTensors (before dispose): 264numTensors (after dispose): 260### Describe the expected behaviorI expected to be no tensors after the call to `.dispose()`.### AlternativeI thought about using `.tidy`, but `loadLayersModel` is async so I couldn't.I managed to free all tensors using the bellow code (although TypeScripts gives me an error, requiring me to cast `model` to `any`):```jsTensorFlow.dispose(model as any),```","['@schdck import is not correct @tensorflow/tfjs-layers has been moved to mono repo tfjs , please check the documentation here as how to use tfjs layer https://github.com/tensorflow/tfjs/tree/master/tfjs-layers#tensorflowjs-layers-high-level-machine-learning-model-api=====', ""Hey @rthadur! Issue remains with the following code:```jsimport * as TensorFlow from '@tensorflow/tfjs',console.log('numTensors (before model): ' + TensorFlow.memory().numTensors),const model = await TensorFlow.loadLayersModel(modelUrl),console.log('numTensors (before dispose): ' + TensorFlow.memory().numTensors),model.dispose(),console.error('numTensors (after dispose): ' + TensorFlow.memory().numTensors),```Output:>numTensors (before model): 0numTensors (before dispose): 264numTensors (after dispose): 260====="", 'please check this document as how the memory management happens in tfjs https://www.tensorflow.org/js/guide/tensors_operations#memory . cc @mattsoulanille @lina128 =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5034"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5034"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/3184,tfjs-react-native API Docs Error,0,closed,2020-04-30T18:19:48Z,2020-11-20T21:40:57Z,"#### TensorFlow.js versionTF.js : 0.2.3#### Browser versionReact Native (Expo Managed App)#### Describe the problem or feature requesttfjs-react-native API Docs Error #### Code to reproduce the bug / link to feature request[Docs](https://js.tensorflow.org/api_react_native/0.2.1/#cameraWithTensors)have an error instead of requestAnimation(loop) it should be requestAnimationFrame(loop)```function handleCameraStream(images, updatePreview, gl) {    const loop = async () => {      const nextImageTensor = images.next().value,      // do something with tensor here      // if autorender is false you need the following two lines.      // updatePreview(),      // gl.endFrameEXP()      requestAnimationFrame(loop),    },    loop(),  }```![Screenshot from 2020-04-30 23-30-18](https://user-images.githubusercontent.com/47809786/80743989-978aaa80-8b3b-11ea-85cc-fd64c3b6264f.png)",[],0
https://github.com/tensorflow/tfjs/issues/3247,tf.layer.rnn with stackedRNNCells doesn't work with bidirectional RNN,3,closed,2020-05-11T22:28:35Z,2020-11-20T21:40:31Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versionlatest#### Browser versionChrome Version 81.0.4044.138 (Official Build) (64-bit)#### Describe the problem or feature requestI am developing one sequence model with bidirectional RNN structures. It will throw the error that the class name and config must be set when I construct the model#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.```        const input = tf.input({ shape: [100, 512] }),        const lstmCell = tf.layers.lstmCell({ units: 512 }),        const cellArray = [lstmCell, lstmCell],        const stackedCells = tf.layers.stackedRNNCells({ cells: cellArray }),        const rnn = tf.layers.rnn({ cell: stackedCells, returnSequences: true, returnState: false }),        const birRnn = tf.layers.bidirectional({ layer: rnn as tf.RNN }).apply(input),        const model = tf.model({ inputs: input, outputs: birRnn as tf.SymbolicTensor }),        model.summary(),```The error is:```tf-layers.esm.js:17 Uncaught Error: layer: Improper config format: {""_callHook"":null,""_addedWeightNames"":[],""_stateful"":false,""id"":5,""activityRegularizer"":null,""inputSpec"":null,""supportsMasking"":false,""_trainableWeights"":[],""_nonTrainableWeights"":[],""_losses"":[],""_updates"":[],""_built"":false,""inboundNodes"":[],""outboundNodes"":[],""name"":""lstm_cell_LSTMCell1"",""trainable_"":true,""initialWeights"":null,""_refCount"":null,""fastWeightInitDuringBuild"":false,""DEFAULT_ACTIVATION"":""tanh"",""DEFAULT_RECURRENT_ACTIVATION"":""hardSigmoid"",""DEFAULT_KERNEL_INITIALIZER"":""glorotNormal"",""DEFAULT_RECURRENT_INITIALIZER"":""orthogonal"",""DEFAULT_BIAS_INITIALIZER"":""zeros"",""units"":512,""activation"":{},""recurrentActivation"":{},""useBias"":true,""kernelInitializer"":{""scale"":1,""mode"":""fanAvg"",""distribution"":""normal"",""seed"":null},""recurrentInitializer"":{""DEFAULT_GAIN"":1,""gain"":1},""biasInitializer"":{},""kernelRegularizer"":null,""recurrentRegularizer"":null,""biasRegularizer"":null,""kernelConstraint"":null,""recurrentConstraint"":null,""biasConstraint"":null,""dropout"":0,""recurrentDropout"":0,""stateSize"":[512,512],""dropoutMask"":null,""recurrentDropoutMask"":null}.'className' and 'config' must set.```GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['The problem should be that tf.layer.bidirectional function do not support tf.layers.rnn() layer as the input args=====', 'Hi @buaazhangfan, it has to be an instance of rnn, such as lstm or gru. @shanqing-cai may have more details to share.=====', 'Closing this due to lack of activity, feel to reopen. Thank you=====']",0
https://github.com/tensorflow/tfjs/issues/9,Investigate calling tf.nextFrame() in model.fit() between batches.,5,closed,2018-03-19T20:19:35Z,2018-10-23T17:47:06Z,"Calling tf.nextFrame() in a browser context will release the UI thread so the page can be responsive. This should have no impact on performance for medium-large sized models, though will negatively impact small models (where forward / backprop is < 16ms).Some benchmarks from @cais ## No tf.nextFrame() (today)![image](https://user-images.githubusercontent.com/1100749/37559540-5b20a0c2-29fe-11e8-99ee-4934eaa7d4a6.png)## tf.nextFrame() between batches![image](https://user-images.githubusercontent.com/1100749/37559548-8658d4a8-29fe-11e8-9361-42e43913bf41.png)Notice that there is no performance impact for rows 2+ (the first row slows down by about a frame, which is expected since the forward / backwards pass is sub-frame).After launch, we can see what type of models people typically use and come back to this discussion.","['From Daniel (other issue):""Nice tables. Model.fit() should be smart and keep a record of the ms that the last step took, and decide based on that how often to call nextFrame()""=====', ""I still think we should do this, even at the cost of smaller models. I've seen people have to add this in several places:https://github.com/shiffman/Tensorflow-JS-Examples/blob/master/02_Color_Classifier/sketch.js#L83Most models are larger than the tiny models we're talking about here, so let's go forward and do this automatically.@caisq @dsmilkov @tafsiri thoughts?====="", '+1. No reservations.=====', 'One more person talking about this: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/tfjs/754OcyLgfjg/X9rXT8IfBwAJ@caisq would you prioritize this?=====', 'This is now fixed.=====']",0
https://github.com/tensorflow/tfjs/issues/5293,[WebGL] Failed to compile fragment shader,8,closed,2021-07-06T02:32:01Z,2021-07-26T00:26:55Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All platforms- TensorFlow.js version (use command below):  tf.js 3.7.0 webgl backend.- Browser version: Chrome 93.0.4545.0**Describe the current behavior**WebGL backend throws `Failed to compile fragment shader` error while using `depthwiseConv2d` op with specific options. While WebGPU backend doesn't reproduce.**Describe the expected behavior**No error.**Standalone code to reproduce the issue**CodePen: https://codepen.io/honry/pen/wvdMmva```    const input = tf.fill([1, 65, 65, 960], 0),    const weights = tf.fill([3, 3, 960, 1], 1),    const result = tf.depthwiseConv2d(input, weights, 1, 'same', 'NHWC', 4),```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.Please check error log from https://codepen.io/honry/pen/wvdMmva","['Thanks @Honry , how about the cpu backend and wasm backend?=====', '@huningxin, this issue only reproduce on webgl backend.=====', 'A similiar situation could be occurred by the following codes (i.e. depthwiseCon2d with ( strides == 2 ) ):```//await tf.setBackend( ""cpu"" ), // Error not happend in CPU.await tf.setBackend( ""webgl"" ), // Error happend in WebGL.let a = [  111, 112, 113, 114, 121, 122, 123, 124, 131, 132, 133, 134,  141, 142, 143, 144, 151, 152, 153, 154, 211, 212, 213, 214,  221, 222, 223, 224, 231, 232, 233, 234, 241, 242, 243, 244,  251, 252, 253, 254, 311, 312, 313, 314, 321, 322, 323, 324,  331, 332, 333, 334, 341, 342, 343, 344, 351, 352, 353, 354],let shape = [ 3, 5, 4 ],let x = tf.tensor( a, shape ),let filters = tf.tensor( [ 11, 22, 33, 44 ], [ 1, 1, 4, 1 ] ),// When ( strides == 1 ), No problem.let c = tf.depthwiseConv2d( x, filters, 1, ""valid"" ),c.print(),let d = tf.depthwiseConv2d( x, filters, 1, ""same"" ),d.print(),// When ( strides == 2 ), error: ""Failed to compile fragment shader.""let e = tf.depthwiseConv2d( x, filters, 2, ""valid"" ),e.print(),// When ( strides == 2 ), error: ""Failed to compile fragment shader.""let f = tf.depthwiseConv2d( x, filters, 2, ""same"" ),f.print(),```=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5293"">No</a>=====', ""@pyu10055, many thanks for fixing this issue! The `compile error` had been fixed, but the computed result looks wrong.Here's a simple test, I just compared the first value of the output buffer, which is obviously different from result from `wasm` and `cpu` backends.https://codepen.io/honry/pen/wvdMmva====="", '@pyu10055, friendly ping. :) Should I create a separate issue for incorrect computational result?=====', '@Honry please open a new issue. Thank you =====', ""Thanks @rthadur, I've created a new issue at https://github.com/tensorflow/tfjs/issues/5378.=====""]",1
https://github.com/tensorflow/tfjs/issues/4434,[wasm] Add Round kernel. Unblocks efficientdet models.,2,closed,2020-12-21T13:13:33Z,2021-01-05T16:37:12Z,"**System information**- TensorFlow.js version (you are using): 2.7.0**Describe the feature and the current behavior/state.**There are plans to add an implementation for efficientdet models in the wasm backend ? It is related to the issue https://github.com/tensorflow/tfjs/issues/4408.When i use webgl backend i  have a problem with precision (my device doesn't support float32) I took the advice from this issue. I tried to use the wasm backend but i have exception ""Error: 'round' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen""","['cc @annxingyuan =====', 'XNNPACK implementation: https://github.com/google/XNNPACK/tree/master/src/f32-vrnd=====']",1
https://github.com/tensorflow/tfjs/issues/3656,Error during conversion of .h5 model with h5py._hl.dataset.Dataset (MobileBERT),7,open,2020-07-25T16:07:45Z,2020-08-13T19:05:03Z,"## TensorFlow.js version2.2.0#### Describe the problem or feature requestConversion of MobileBERT from huggingface`s transformers fails with the following error. It seems that it's due to lack of support of h5py._hl.dataset.Dataset in .h5 file. ```python3Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 735, in pip_main    main([' '.join(sys.argv[1:])])  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 739, in main    convert(argv[0].split(' '))  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 654, in convert    weight_shard_size_bytes=weight_shard_size_bytes)  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 84, in dispatch_keras_h5_to_tfjs_layers_model_conversion    h5_file, split_by_layer=split_weights_by_layer)  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 241, in h5_weights_to_tfjs_format    group = _convert_h5_group(layer)  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 85, in _convert_h5_group    group_out += _convert_h5_group(group[key])  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 85, in _convert_h5_group    group_out += _convert_h5_group(group[key])  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 85, in _convert_h5_group    group_out += _convert_h5_group(group[key])  [Previous line repeated 2 more times]  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py"", line 83, in _convert_h5_group    for key in group.keys():AttributeError: 'Dataset' object has no attribute 'keys'```#### Code to reproduce the bug / link to feature requestMake model using huggingface```python3from transformers import TFAutoModelmodel = TFAutoModel.from_pretrained('google/mobilebert-uncased')print(model.summary())model.save_pretrained('model')``````bashtensorflowjs_converter --input_format keras \                       model/tf_model.h5 \                       tfjs_target_dir```Excuse me, if it's not a bug, but my fault in incorrect usage.","['@AIshutin did you try this existing model for MobileBERT [here](https://www.npmjs.com/package/@tensorflow-models/qna) =====', ""Yes, but I need to modify the model a lot. It's easier for me to do this in PyTorch using transformers. But first, I need to convert the original model successfully. Thanks for your fast response.====="", ""@AIshutin we don't support the dataset layer in HDF5, thats the reason you are seeing above error.There is no plan in near future to support this.cc @pyu10055 ====="", 'I am also interested to convert MobileBERT to tfjs. Is there an easy way to do it ? How was the question answering model converted ?[EDIT] I was able to convert the mobilbert-qa. However , still trying to figure out how to convert the base model. =====', 'I tried to convert the tf hub version ``` consoletensorflowjs_converter \\    --input_format=tf_hub \\    \'https://tfhub.dev/tensorflow/tfjs-model/mobilebert/1\' \\    web_model/```but I get the following error ```Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 735, in pip_main    main([\' \'.join(sys.argv[1:])])  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 739, in main    convert(argv[0].split(\' \'))  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py"", line 692, in convert    control_flow_v2=args.control_flow_v2)  File ""/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 639, in convert_tf_hub_module    module_path = hub.resolve(module_handle)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py"", line 52, in resolve    return registry.resolver(handle)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/registry.py"", line 42, in __call__    return impl(*args, **kwargs)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py"", line 88, in __call__    self._lock_file_timeout_sec())  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py"", line 402, in atomic_download    download_fn(handle, tmp_dir)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py"", line 83, in download    response = self._call_urlopen(request)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py"", line 96, in _call_urlopen    return url.urlopen(request)  File ""/usr/lib/python3.6/urllib/request.py"", line 223, in urlopen    return opener.open(url, data, timeout)  File ""/usr/lib/python3.6/urllib/request.py"", line 532, in open    response = meth(req, response)  File ""/usr/lib/python3.6/urllib/request.py"", line 642, in http_response    \'http\', request, response, code, msg, hdrs)  File ""/usr/lib/python3.6/urllib/request.py"", line 564, in error    result = self._call_chain(*args)  File ""/usr/lib/python3.6/urllib/request.py"", line 504, in _call_chain    result = func(*args)  File ""/usr/lib/python3.6/urllib/request.py"", line 756, in http_error_302    return self.parent.open(new, timeout=req.timeout)  File ""/usr/lib/python3.6/urllib/request.py"", line 532, in open    response = meth(req, response)  File ""/usr/lib/python3.6/urllib/request.py"", line 642, in http_response    \'http\', request, response, code, msg, hdrs)  File ""/usr/lib/python3.6/urllib/request.py"", line 570, in error    return self._call_chain(*args)  File ""/usr/lib/python3.6/urllib/request.py"", line 504, in _call_chain    result = func(*args)  File ""/usr/lib/python3.6/urllib/request.py"", line 650, in http_error_default    raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 404: Not Found```=====', 'Can you help with this @annxingyuan ? What is the convert script you used and do we need to make any changes for the conversion. =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====']",0
https://github.com/tensorflow/tfjs/issues/4809,[tfjs-react-native] tf.sparseToDense is broken,3,closed,2021-03-12T15:41:21Z,2021-03-12T17:43:12Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 7 Emulator- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs-react-native- TensorFlow.js version (use command below): (core@3.2.0, react-native@0.5.0)- Browser version: n/a- Tensorflow.js Converter Version: n/a**Describe the current behavior**```jsconst indices = tf.tensor1d([4, 5, 6, 1, 2, 3], 'int32'),const values = tf.tensor1d([10, 11, 12, 13, 14, 15], 'float32'),const shape = [8],tf.sparseToDense(indices, values, shape).print(),// outputs: [0, 1, 0, 0, 0, 0, 0, 0]```**Describe the expected behavior**Expected output: `[0, 13, 14, 15, 10, 11, 12, 0]`It works flawlessly with the CPU backend.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""I've just found that my emulator doesn't support webgl2. This can be the cause of the issue. I'll reopen it once I'll confirm that it doesn't work on a device.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4809"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4809"">No</a>=====', 'Resolved.For if anyone will stumble upon this. `tfjs-react-native` requires GLES 3 / WebGL 2. You can enable it in Android SDK Emulator via:```echo ""GLESDynamicVersion = on"" >> ~/.android/advancedFeatures.ini```I tested this only under Linux. Your host GPU should support at least OpenGL ES 3.2, you can check if it does via:```$ sudo apt install mesa-utils$ glxinfo | grep \'version.*OpenGL\'```=====']",1
https://github.com/tensorflow/tfjs/issues/2100,tensor shape issues,1,closed,2019-09-27T13:33:02Z,2019-10-01T05:26:01Z,,"[""Hi @thirukumars, i noticed that you already [opened an issue](https://github.com/ml5js/ml5-library/issues/586) on the ml5 repo. I think that is the best place for this question since you are using the ML5 wrapper. So I'll close this issue to allow focus on that one. All the best!=====""]",0
https://github.com/tensorflow/tfjs/issues/4419,`npm install` succeeds despite errors due to not having Python (in a container),3,closed,2020-12-16T20:56:51Z,2021-02-17T16:27:00Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No.- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux container (docker) node:14- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ^2.8.0- Tensorflow.js Converter Version: N/A**Describe the current behavior**1. Running `npm install` in a `docker build step`, with Dockerfile:	  ```	  FROM node:14-slim	  WORKDIR /usr/src/app	  COPY package*.json ./	  RUN npm install --only=production	  COPY . ./	  CMD [ ""node"", ""index.js"" ]	  ```2. It succeeds and exits with 0.3. However, the build log shows errors saying it cannot find Python.**Describe the expected behavior**If it can't find python, `npm install` **should fail** and fail the whole build process in turn.**Standalone code to reproduce the issue****Other info / logs** Logs from docker build:```docker build -t gcr.io/ahmetb-demo/tfjs-cloudrun .Sending build context to Docker daemon  55.85MBStep 1/6 : FROM node:14-slim ---> 30b595bd6403Step 2/6 : WORKDIR /usr/src/app ---> Using cache ---> aa273d65905cStep 3/6 : COPY package*.json ./ ---> 95c5ad63c38fStep 4/6 : RUN npm install --only=production ---> Running in ba6ed059f4a1npm WARN read-shrinkwrap This version of npm is compatible with lockfileVersion@1, but package-lock.json was generated for lockfileVersion@2. I'll try to do my best with it!> @tensorflow/tfjs-node@2.8.0 install /usr/src/app/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-linux-2.8.0.tar.gz* Downloading libtensorflow* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https downloadnode-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/2.8.0/CPU-linux-2.8.0.tar.gznode-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@2.8.0 and node@14.15.1 (node-v83 ABI, glibc) (falling back to source compile with node-gyp)gyp ERR! find Pythongyp ERR! find Python Python is not set from command line or npm configurationgyp ERR! find Python Python is not set from environment variable PYTHONgyp ERR! find Python checking if ""python"" can be usedgyp ERR! find Python - ""python"" is not in PATH or produced an errorgyp ERR! find Python checking if ""python2"" can be usedgyp ERR! find Python - ""python2"" is not in PATH or produced an errorgyp ERR! find Python checking if ""python3"" can be usedgyp ERR! find Python - ""python3"" is not in PATH or produced an errorgyp ERR! find Pythongyp ERR! find Python **********************************************************gyp ERR! find Python You need to install the latest version of Python.gyp ERR! find Python Node-gyp should be able to find and use Python. If not,gyp ERR! find Python you can try one of the following options:gyp ERR! find Python - Use the switch --python=""/path/to/pythonexecutable""gyp ERR! find Python   (accepted by both node-gyp and npm)gyp ERR! find Python - Set the environment variable PYTHONgyp ERR! find Python - Set the npm configuration variable python:gyp ERR! find Python   npm config set python ""/path/to/pythonexecutable""gyp ERR! find Python For more information consult the documentation at:gyp ERR! find Python https://github.com/nodejs/node-gyp#installationgyp ERR! find Python **********************************************************gyp ERR! find Pythongyp ERR! configure errorgyp ERR! stack Error: Could not find any Python installation to usegyp ERR! stack     at PythonFinder.fail (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:307:47)gyp ERR! stack     at PythonFinder.runChecks (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:136:21)gyp ERR! stack     at PythonFinder.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:179:16)gyp ERR! stack     at PythonFinder.execFileCallback (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:271:16)gyp ERR! stack     at exithandler (child_process.js:315:5)gyp ERR! stack     at ChildProcess.errorhandler (child_process.js:327:5)gyp ERR! stack     at ChildProcess.emit (events.js:315:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12)gyp ERR! stack     at onErrorNT (internal/child_process.js:465:16)gyp ERR! stack     at processTicksAndRejections (internal/process/task_queues.js:80:21)gyp ERR! System Linux 4.19.121-linuxkitgyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7"" ""--napi_version=7"" ""--node_abi_napi=napi"" ""--napi_build_version=7"" ""--node_napi_label=napi-v7""gyp ERR! cwd /usr/src/app/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v14.15.1gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute '/usr/local/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --fallback-to-build --module=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node --module_name=tfjs_binding --module_path=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7 --napi_version=7 --node_abi_napi=napi --napi_build_version=7 --node_napi_label=napi-v7' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/usr/src/app/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:315:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1048:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:288:5)node-pre-gyp ERR! System Linux 4.19.121-linuxkitnode-pre-gyp ERR! command ""/usr/local/bin/node"" ""/usr/src/app/node_modules/.bin/node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd /usr/src/app/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v14.15.1node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok> core-js@3.8.1 postinstall /usr/src/app/node_modules/core-js> node -e ""try{require('./postinstall')}catch(e){}""Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon:> https://opencollective.com/core-js> https://www.patreon.com/zloirockAlso, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)npm WARN object-detection@ No descriptionnpm WARN object-detection@ No repository field.npm WARN object-detection@ No license field.added 182 packages from 140 contributors and audited 182 packages in 12.534s5 packages are looking for funding  run `npm fund` for detailsfound 0 vulnerabilitiesRemoving intermediate container ba6ed059f4a1 ---> d20320f6d3f5Step 5/6 : COPY . ./ ---> c79f02866ef4Step 6/6 : CMD [ ""node"", ""index.js"" ] ---> Running in fa4cbd173895Removing intermediate container fa4cbd173895 ---> dab5876e0308Successfully built dab5876e0308Successfully tagged gcr.io/ahmetb-demo/tfjs-cloudrun:latest```It shouldn't succed. It clearly says `node-pre-gyp ERR! not ok` but `npm install` succeeds.","['We had issues with 2.8.0 , which was rolled back , can you please try with 2.8.1 ?=====', ""The issue was present even on 1.4.0, I don't think it's a recently occurred issue. ====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4419"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4419"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/903,Broken link for list of supported TensorFlow Ops in docs,1,closed,2018-11-13T01:44:17Z,2018-12-18T22:09:45Z,On this page: https://js.tensorflow.org/tutorials/import-saved-model.htmlCurrently TensorFlow.js only supports a limited set of TensorFlow Ops. See the full list. The full list link is broken.Observed: https://github.com/tensorflow/tfjs-converter/docs/supported_ops.mdExpected: https://github.com/tensorflow/tfjs-converter/blob/master/docs/supported_ops.mdThe content comes from the README at : https://github.com/tensorflow/tfjs-converter/blob/master/README.mdwhich is using a relative URL.Whatever is used to generate the Tensorflow website should probably be use https://github.com/tensorflow/tfjs-converter/blob/master/ as link prefix,"[""Thanks for reporting @steren. I've made a PR to fix it. =====""]",0
https://github.com/tensorflow/tfjs/issues/5517,"[tensorflowjs_converter] InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0,0] (0 elements)",4,closed,2021-08-20T14:30:54Z,2021-08-24T18:54:50Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04- TensorFlow.js installed from (npm or script link): `pip install tensorflowjs`- Tensorflow.js Converter Version:```sh$ tensorflowjs_converter  --versiontensorflowjs 3.8.0Dependency versions:  keras 2.6.0  tensorflow 2.6.0```**Describe the current behavior**```sh$ tensorflowjs_converter \>     --input_format=tf_frozen_model \>     --output_node_names final_result \>     ./tensorflowjs_model.pb \>     ./webTraceback (most recent call last):  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 496, in _import_graph_def_internal    results = c_api.TF_GraphImportGraphDefWithResults(tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0,0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze, MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1,1001], [2] and with input tensors computed as partial shapes: input[1] = [0,0].```**Describe the expected behavior**Convert the frozen model to web format**Standalone code to reproduce the issue**```sh$ git clone git@github.com:google/emoji-scavenger-hunt.git$ cd emoji-scavenger-hunt/dist/model$ ls -ltotal 3568-rw-rw-r-- 1 huan huan 3580716 Aug 18 00:37 group1-shard1of1-rw-rw-r-- 1 huan huan   55746 Aug 18 00:37 tensorflowjs_model.pb-rw-rw-r-- 1 huan huan    9489 Aug 18 00:37 weights_manifest.json$ tensorflowjs_converter \>     --input_format=tf_frozen_model \>     --output_node_names final_result \>     ./tensorflowjs_model.pb \>     ./web```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.```sh$ tensorflowjs_converter     --input_format=tf_frozen_model     --output_node_names final_result     ./tensorflowjs_model.pb     ./web2021-08-20 22:28:16.527933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0', dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-08-20 22:28:16.527994: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 496, in _import_graph_def_internal    results = c_api.TF_GraphImportGraphDefWithResults(tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0,0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze, MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1,1001], [2] and with input tensors computed as partial shapes: input[1] = [0,0].During handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/home/huan/.local/bin/tensorflowjs_converter"", line 8, in <module>    sys.exit(pip_main())  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 813, in pip_main    main([' '.join(sys.argv[1:])])  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 817, in main    convert(argv[0].split(' '))  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 803, in convert    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py"", line 574, in _dispatch_converter    tf_saved_model_conversion_v2.convert_tf_frozen_model(  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 518, in convert_tf_frozen_model    graph = load_graph(frozen_model_path)  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 68, in load_graph    tf.import_graph_def(graph_def, name='')  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 549, in new_func    return func(*args, **kwargs)  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 400, in import_graph_def    return _import_graph_def_internal(  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal    raise ValueError(str(e))ValueError: Cannot reshape a tensor with 1001 elements to shape [0,0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze, MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1,1001], [2] and with input tensors computed as partial shapes: input[1] = [0,0].```Link to https://github.com/huan/emoji-net/issues/3","['Frozen models are not supported in latest versions. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5517"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5517"">No</a>=====', 'Hi @rthadur ,Thanks for the reply.I think the `frozen models` are still supported according to the help message of tensorflowjs_converter 3.8:```sh$ tensorflowjs_converter --help  --input_format {tf_hub,tf_saved_model,tfjs_layers_model,keras,tf_frozen_model,keras_saved_model}$ tensorflowjs_converter  --vesiontensorflowjs 3.8.0Dependency versions:  keras 2.6.0  tensorflow 2.6.0```We can see **tf_frozen_model** is listed in the `input_format`.And also, I can reproduce this problem in the older version of tensorflowjs_converter with version 0.8.5, and the following is the reproduce steps that have the exact same error message outputted as the latest version.## Reproduce this error with tensorflowjs_converter v0.8.5### 1. Get a Python 2.7 environment```sh$ docker run --rm -ti -v $(pwd):/model ubuntu:18.04 bash$ apt update && apt install -y python python-pip```### 2. Install```sh$ pip install tensorflowjs==0.8.5$ tensorflowjs_converter --versionUsing TensorFlow backend.tensorflowjs 0.8.5Dependency versions:  keras 2.2.2  tensorflow 1.13.1```### 3. Convert```sh# tensorflowjs_converter \\>     --input_format=tf_frozen_model \\>     --output_node_names final_result \\>     ./tensorflowjs_model.pb \\>     ./webUsing TensorFlow backend.Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter"", line 11, in <module>    sys.exit(main())  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/converter.py"", line 352, in main    strip_debug_ops=FLAGS.strip_debug_ops)  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py"", line 329, in convert_tf_frozen_model    graph = load_graph(frozen_model_path, output_node_names)  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py"", line 67, in load_graph    tf.import_graph_def(graph_def, name=\'\')  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func    return func(*args, **kwargs)  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def    raise ValueError(str(e))ValueError: Cannot reshape a tensor with 1001 elements to shape [0,0] (0 elements) for \'MobilenetV1/Predictions/Reshape\' (op: \'Reshape\') with input shapes: [1,1001], [2] and with input tensors computed as partial shapes: input[1] = [0,0].```=====', ""As mentioned [here](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter#conversion-flags) it is supported for backward compatibility and we don't think we will be supporting older tfjs(0.86) and tenforflow(1.13) anymore. cc @pyu10055 @lina128 =====""]",1
https://github.com/tensorflow/tfjs/issues/2765,Uncaught (in promise) TypeError: Cannot assign to read only property 'message' of object '',7,closed,2020-02-18T03:13:06Z,2020-03-20T14:58:35Z,"#### TensorFlow.js version1.5.2#### Browser versionChrome  80.0.3987.106#### Describe the problem or feature requesttf.data.webcam(webcamElement) causes error on ChromeI did not get the error with Safari, Firefox.#### Code to reproduce the bug / link to feature request```<html>  <head>    <!-- Load the latest version of TensorFlow.js -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet""></script>  </head>  <body>    <div id=""console""></div>    <!-- Add an image that we will use to test -->    <img id=""img"" crossorigin src=""https://i.imgur.com/JlUvsxa.jpg"" width=""227"" height=""227""/>    <!-- Load index.js after the content of the page -->    <script src=""index.js""></script>  </body></html>```script.js```const webcamElement = document.getElementById('webcam')let netasync function app() {  console.log('Loading mobilenet..'),  // Load the model.  net = await mobilenet.load(),  console.log('Successfully loaded model'),    // Create an object from Tensorflow.js data API which could capture image   // from the web camera as Tensor.  const webcam = await tf.data.webcam(webcamElement),  while (true) {    const img = await webcam.capture(),    const result = await net.classify(img),    document.getElementById('console').innerText = `      prediction: ${result[0].className}\n      probability: ${result[0].probability}    `,    // Dispose the tensor to release the memory.    img.dispose(),    // Give some breathing room by waiting for the next animation frame to    // fire.    await tf.nextFrame(),  }}app()```https://codelabs.developers.google.com/codelabs/tensorflowjs-teachablemachine-codelab/index.html#5","['@tamatsu is this still an issue ? I could not reproduce the error in my local , mine is a Mac OS and latest chrome.=====', 'Have the same problem=====', '@Filipp585 can you please give your system configuration and tfjs version =====', 'Hey Reddy T,here is a minimal HTML/JS example as a zip, just open the html with your chrome browser and click on Run, in some cases it runs the coco_ssd inference perfectly - in some not. The only reason for that can be the function tf.data.webcam(webcamElement)It tries to fetch the webcam, and in some environments it is not successful. In my it is successful, but in my mates browser not – we all have the same Chrome-Version Version 80.0.3987.132 (Offizieller Build) (32-Bit).[tf_bug.zip](https://github.com/tensorflow/tfjs/files/4342411/tf_bug.zip)=====', 'It runs successfully in my browser.Not sure where the problem is. Can you please give your system configuration.=====', 'Ok, i figured out my solution. You have to delete the cookies and the cache. Then it should run without problems. @tamatsu did it helped ?=====', 'It runs successfully in my local macOS and latest Chrome. thank you both.=====']",0
https://github.com/tensorflow/tfjs/issues/4695,Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node.,3,closed,2021-02-17T15:01:19Z,2021-02-18T02:08:20Z,"**System information**- OS Platform and Distribution : macOS 11.2.1 Big Sur 20D74- TensorFlow.js installed from: `yarn add @tensorflow/tfjs-node`- TensorFlow.js version: `3.1.0`**Describe the problem**1. create a new node project2. install tensorflow with `yarn add @tensorflow/tfjs-node`3. add import or require statement to index.js file4. receive the error**Any other info / logs**rebuilding from source did work but didn't solve the problem```bashnpm rebuild @tensorflow/tfjs-node --build-from-source``````jsimport tf from '@tensorflow/tfjs-node'const model = tf.sequential(),``````js{  ""type"": ""module"",  ""version"": ""1.0.0"",  ""main"": ""index.js"",  ""scripts"": {    ""start"": ""node index.js""  },  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^3.1.0""  }}``````bash> sw_versProductName:    macOSProductVersion: 11.2.1BuildVersion:   20D74``````bash> sysctl -n machdep.cpu.brand_string                                                                                                                                                                                                                                                        Apple M1``````bashnode --version        v15.5.1``````bash> node index.js/node_modules/@tensorflow/tfjs-node/dist/index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +          ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node. Please run command 'npm rebuild @tensorflow/tfjs-node --build-addon-from-source' to rebuild the native addon module. If you have problem with building the addon module, please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (node_modules/@tensorflow/tfjs-node/dist/index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:199:29)    at ModuleJob.run (node:internal/modules/esm/module_job:152:23)    at async Loader.import (node:internal/modules/esm/loader:166:24)    at async Object.loadESM (node:internal/process/esm_loader:68:5)error Command failed with exit code 1.```","['i placed the project into my documents folder, where there is no space in the path. That is a workaround for now.the next issue is```bash> node index.js[1]    59351 illegal hardware instruction  node index.js```=====', 'This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4695"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4695"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4800,After setting TensorflowJS model's backend as WASM is too slow,10,closed,2021-03-10T11:23:27Z,2021-04-12T04:52:49Z,"It's an **LSTM model.** The model total size is **23.2 MB**(model.json, group1.bin, ...).### Before setting a backend as wasm, the prediction runtime is <1500ms, after setting wasm >5000ms.I use this set of code to change the backend as wasm.```setWasmPaths({	'tfjs-backend-wasm.wasm': 'assets/wasm/tfjs-backend-wasm.wasm',	'tfjs-backend-wasm-simd.wasm': 'assets/wasm/tfjs-backend-wasm-simd.wasm',	'tfjs-backend-wasm-threaded-simd.wasm': 'assets/wasm/tfjs-backend-wasm-threaded-simd.wasm'}),setBackend('wasm').then(() => {...}```**Is it possible to fix this issue?**TensorflowJS - 3.2.0Angular - 11.2.3Brave - 1.21.73 Chromium - 89.0.4389.72OS - ubuntu 20.04.2","['In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks! =====', '### I used this code snippet for wasm.```import { loadLayersModel, LayersModel, Tensor, squeeze, setBackend } from \'@tensorflow/tfjs\',import { setWasmPaths } from ""@tensorflow/tfjs-backend-wasm"",private model: LayersModel,private topValues: number = 5,private minPredictionValue = 0.7,constructor() {    setWasmPaths({        \'tfjs-backend-wasm.wasm\': \'assets/wasm/tfjs-backend-wasm.wasm\',        \'tfjs-backend-wasm-simd.wasm\': \'assets/wasm/tfjs-backend-wasm-simd.wasm\',        \'tfjs-backend-wasm-threaded-simd.wasm\': \'assets/wasm/tfjs-backend-wasm-threaded-simd.wasm\'    }),        setBackend(\'wasm\').then(() => {        this.loadModel(),    }}private loadModel(): void {    const modelURL = ""assets/model/model.json"",        loadLayersModel(modelURL).then(        model => {            this.model = model,        },        error => alert(error)    ),}private predict(input: string): number[] {    const predictedValue = this.model.predict(input),    const predictedIndex = this.decodePrediction(predictedValue),    return predictedIndex,}private decodePrediction(predictedValue: Tensor): number[] {    const {values, indices} = squeeze(predictedValue, [0]).topk(this.topValues),        const valueArray = values.dataSync(),    const indexArray = indices.dataSync(),        let output: number[] = [],        for(let i = 0, i < this.topValues, i++) {            if(valueArray[i] > this.minPredictionValue) {            output[i] = indexArray[i],        } else {            break,        }    }        return output, }```### Before using wasm, I used this code snippet.```...constructor() {    this.loadModel(),}...```=====', 'is it possible to share the model ? or similar model which we can use for reproduction ?=====', '[I modified the model from this](https://keras.io/examples/generative/lstm_character_level_text_generation/), other processes are the same as this. [JS model is here](https://drive.google.com/drive/folders/1kvcKukV6fEjgB-J_HsOCwnXI8N-YXHIW?usp=sharing)=====', 'Any updates? @rthadur=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Any updates? @rthadur =====', '@sivarajakani Quick question: is the default backend (before you set it) WebGL? If so, you are probably just seeing the difference in performance between the two backends on your machine. As an example: on my machine, WASM is often about 10x slower than WebGL for a MobileNetV2 prediction, but for someone who uses my project, WASM performs faster than WebGL. So it can vary a lot based on the setup.=====', ""**is the default backend (before you set it) WebGL?**I think CPU is the default backend in TensorFlowJS, but I'm not sure about that. So I tried CPU and WebGL. WebGL runtime same as the default runtime, and the CPU runtime is greater than the default runtime. From these results, I confirmed WebGL is the default backend in my project.Thank you, @wingman-jr-addon.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4800"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4800"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5017,node-pre-gyp info This Node instance does not support builds for N-API version 8,20,open,2021-04-30T13:26:52Z,2021-12-16T16:34:40Z,"I am trying to use TensorFlow with Firebase Cloud Functions. I'm on Node.js v14.16.0 which should be Node-API v7.And I get this when starting the server:```console$ firebase serve!  D:\folder_name\functions\node_modules\@tensorflow\tfjs-automl\dist\index.js:18export { ImageClassificationModel, loadImageClassification } from './img_classification',^^^^^^SyntaxError: Unexpected token 'export'    at wrapSafe (internal/modules/cjs/loader.js:979:16)    at Module._compile (internal/modules/cjs/loader.js:1027:27)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)    at Object.<anonymous> (D:\folder_name\functions\index.js:14:11)    at Module._compile (internal/modules/cjs/loader.js:1063:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)!  We were unable to load your functions code. (see above)$ head -n 14 index.jsconst functions = require(""firebase-functions""),        express = require(""express""),        app = express(),        admin = require(""firebase-admin""),        cookieParser = require(""cookie-parser""),        Busboy = require(""busboy""),        path = require(""path""),        os = require(""os""),        fs = require(""fs""),        vader = require(""vader-sentiment""),        morgan = require(""morgan""),        axios = require(""axios""),        tfnode = require(""@tensorflow/tfjs-node""),        automl = require(""@tensorflow/tfjs-automl""),$ nodeWelcome to Node.js v14.16.0.Type "".help"" for more information.> console.log(process.versions.napi)7undefined> .exit$ cat package.json""dependencies"": {                ""@tensorflow/tfjs-automl"": ""^1.1.0"",                ""@tensorflow/tfjs-converter"": ""^3.3.0"",                ""@tensorflow/tfjs-core"": ""^3.3.0"",                ""@tensorflow/tfjs-node"": ""^3.6.1"",        },```MCVE available at: https://github.com/aravindvnair99/TFJS-Issue-5017I've set up a GitHub Actions workflow to reproduce the issue on multiple OS and Node.js versions. Results are available here: https://github.com/aravindvnair99/TFJS-Issue-5017/actionsIt's failing in all cases. Workflow configuration: https://github.com/aravindvnair99/TFJS-Issue-5017/blob/main/.github/workflows/node.js.yml","['Latest version has support for napi version 8 https://github.com/tensorflow/tfjs/pull/4991/files , please try latest version. Thank you =====', ""> Latest version has support for napi version 8 https://github.com/tensorflow/tfjs/pull/4991/files , please try latest version. Thank you@rthadur I'm sorry, I didn't understand. I'm on Node.js 14 which is 7 and not 8.====="", '@aravindvnair99 Thank you for reporting, can you share the full error message? =====', ""> @aravindvnair99 Thank you for reporting, can you share the full error message?@pyu10055 It's what I put in my first message itself. Everything else wasn't an error message. Just the usual Firebase loading.====="", ""@pyu10055 @rthadur @gbaned  I've put together an MCVE with instructions to reproduce: https://github.com/aravindvnair99/TFJS-Issue-5017Update: I've added 4 different test cases.====="", ""I've set up a GitHub Actions workflow to reproduce the issue on multiple OS and Node.js versions. Results are available here: https://github.com/aravindvnair99/TFJS-Issue-5017/actionsIt's failing in all cases. Workflow configuration: https://github.com/aravindvnair99/TFJS-Issue-5017/blob/main/.github/workflows/node.js.yml====="", '@aravindvnair99  Looks like the failure is not on the tfjs-node library but the tfjs-autoML, you can use the latest version 1.2.0 of tfjs-autml which have solve the node.js compatibility issue.=====', '@pyu10055 Can confirm updating to @tensorflow/tfjs-automl v1.2.0 has resolved the issue.Fixed in https://github.com/tensorflow/tfjs/pull/5024=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5017"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5017"">No</a>=====', 'I am still getting `node-pre-gyp info This Node instance does not support builds for N-API version 8` and using node v14.16.0. Also only trying to use `""@tensorflow/tfjs-node"": ""3.6.1""`. No automl.Any ideas?=====', '> I am still getting `node-pre-gyp info This Node instance does not support builds for N-API version 8` and using node v14.16.0. Also only trying to use `""@tensorflow/tfjs-node"": ""3.6.1""`. No automl.> Any ideas?@Fabiansson I too still get it. But I had closed this issue as the other bug I\'ve mentioned had got fixed. That bug prevented me from using the module itself. I guess I\'ll re-open this issue.@pyu10055 Could you confirm which Node.js versions are supported? It would be good to have this mentioned in the documentation as well as I found certain files aren\'t available in the bucket from where it\'s fetching files to build.=====', ""To be clear I did not manage to install the newest tfjs-node on any Node Version. I tested on Node: 10x, 12x, 14x and even 16x. Would have to look up which version exactly. I tried on Windows, Ubuntu and WSL(Ubuntu). Some combinations will not install at all and also manual building does not work reliably. I'm genuinly surprised not more people are having this problem. I really don't know what else I could try. Maybe someone can specify a exact working combination of Node-Version, OS and tfjs-node version so I could try this.====="", 'Make sure you have the necessary **build tools** installed in your system. ( For ubuntu or debian based use `sudo apt install build-essential` , for windows `npm i windows-build-tools`).Now try these versions :Node 16.x@tensorflow/tfjs-node 3.7.0Tested today on Ubuntu 20.04.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Not stale=====', '@pyu10055 @rthadur Could you confirm which Node.js versions are supported? It would be good to have this mentioned in the documentation as well as certain files are not available in the bucket from where it is fetching files to build.=====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Not stale=====', ""@aravindvnair99 Not sure if you're still running into this, but I just solved a similar issue in one of my projects. In my case, I was using electron, which bundles its own version of Node (separate from the version I had installed). In my code I ran `console.log('Version: ' + process.version),` and it spat out v14.6.0, which was Napi 7. Updating electron (and therefore the version of node bundled with it) fixed the issue. Firebase might similarly bundle an old version of node with their CLI.====="", '@aravindvnair99 did you get chance to check above solution ?=====']",1
https://github.com/tensorflow/tfjs/issues/5452,(ranged) Sigmoid layer in advanced activation function,7,open,2021-08-10T17:38:59Z,2021-08-29T04:10:53Z,"- Are you willing to contribute it (Yes I am willing to):In tfjs, there is a lot of activation functions as a layer, but what is weirdly missing is layers.sigmoid.This day I was making a dot product matrix factorization recommender system and the output of the dot layer should be in between 0 and 1, for this layers.sigmoid is needed.FastAI also offer ranged sigmoid which is very interesting, it squeeze the value into a range ]a,b[ instead of ]1,0[, this can be also used in my case when i need to predict ratings in a certain range (let's say ratings from 1 star to 5 stars).I can contribute to add this feature.","['cc @pyu10055 =====', '@rthadur Can I work on this feature (If no one else is working on this currently) ??=====', '@sourabh112 please go ahead , thank you =====', '@rthadur Thanks for the opportunity and sorry for the late reply. I was making myself comfortable with the code of the advanced activation layer. In order to accommodate the sigmoid layer in advanced activation layers. I just need to make some changes to the` exports_layers.ts` and `advanced_activations.ts` files. I will make a PR hopefully by tomorrow. with these changes with a working example. After that, I will add test in `advanced_activations.ts` and make any changes if needed.=====', ""@rthadur As for the ranged sigmoid function suggested by @bibs2091 might be a difficult task because (As per my knowledge) there is no file related to the ranged sigmoid activation function and I have to implement that from scratch. In doing so, I would need some (Actually a lot in the tfjs-core part where actually these functions start) help. If you allow doing that, then I will do that in the same PR after implementing stuff included in the above comment. If you think I'm wrong somewhere please correct me.====="", '@rthadur the implementation of SigmoidRange is in this pull request https://github.com/tensorflow/tfjs/pull/5548 , kindly review it it and tell me what do you think.@sourabh112  I am sorry I did not notice that you have been assigned, I already started working on the issue days ago and I wrote in my issue that I am ready to contribute to the idea. =====', '@bibs2091 No problem, I was just starting to work on it. And after going through your PR. I got to know that I was on quite a bit wrong path. And sorry for not reading the last line of the issue.=====']",1
https://github.com/tensorflow/tfjs/issues/1202,How to build docs?,6,closed,2019-02-04T22:47:57Z,2019-02-07T16:02:06Z,"I would like to build the TensorFlow.js docs from their source. I could not find any relevant information in the [documentation section](https://www.tensorflow.org/community/documentation) of the community pages. I was able to locate [this script](https://github.com/tensorflow/tfjs-website/blob/docs/build-scripts/build-api), which appears to be for building the docs. However, I was unable to figure out what command line parameters should be provided and in what format. The relevant section of the code seems to be [lines 32 to 40](https://github.com/tensorflow/tfjs-website/blob/docs/build-scripts/build-api.js#L32-L40).```jscommander.option('--in [path]', 'main source entry')    .option('--package [path]', 'Package.json path')    .option('--src [path]', 'Path to src folder of repo')    .option('--repo [path]', 'Path to repo')    .option('--bundle  [path]', 'JS Bundle Path')    .option('--github [url]', 'Github repository URL')    .option('--out [path]', 'Output Path')    .option('--branch [branch name]', 'branch to build from')    .parse(process.argv),```I would appreciate any and all help that is provided. Thanks in advance!","[""I have found instructions [here](https://github.com/generic-github-user/tfjs-website/blob/master/README.md) on how to use Yarn to build the docs, but I am now receiving this error:```Please make sure you have the correct access rightsand the repository exists.fatal: clone of 'git@github.com:tensorflow/tfjs-converter' into submodule path 'C:/Path/tfjs-website/libs/tfjs-converter' failedFailed to clone 'libs/tfjs-converter' a second time, abortingerror Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```I have attempted to fork the repository and make a fresh clone and this has not resolved the issue. I also could not find any relevant issues existing within this repository.====="", ""I've also tried creating a new SSH key and adding it to my GitHub account and adding GitHub to `.ssh/known_hosts`.====="", ""I resolved the above error using [this solution](https://stackoverflow.com/a/23634016/10940584). I now receive this error:```fatal: Needed a single revisionUnable to find current revision in submodule path 'libs/tfjs-core'error Command failed with exit code 1.```I will work on this more tomorrow and update accordingly.====="", 'Is `yarn prep` working for you? If not you can try running the individual git commands (separated by `&&`) specified in https://github.com/tensorflow/tfjs-website/blob/master/package.json#L12. It may be easier to see if there are any git configuration issues that way.=====', ""`yarn prep` was giving an access rights error. I fixed that by changing `git@github.com:tensorflow/tfjs-layers.git` to `github.com/tensorflow/tfjs-layers.git` for all the repositories listed in `.gitmodules` and `config`. When I run each git command individually `git submodule update` produces an error:```fatal: Needed a single revisionUnable to find current revision in submodule path 'libs/tfjs-core'```====="", 'I was able to locate the fully built [JSON documentation tree](https://github.com/tensorflow/tfjs-website/blob/master/source/_data/api/0.14.2/docs.json) in the tfjs-website repository after some more searching. I no longer need to build the documentation for myself, but someone else may find this thread useful. Feel free to close the issue.=====']",0
https://github.com/tensorflow/tfjs/issues/4508,Support Reciprocal in wasm backend,1,open,2021-01-08T14:54:32Z,2021-06-03T16:16:31Z,"**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link):  2.8.3- TensorFlow.js version (use command below): 2.8.3- Browser version:  Chrome 87- Tensorflow.js Converter Version: 2.7**Describe the current behavior**When i run my efficientdet model in the WASM backend i have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. Possibly related to https://github.com/tensorflow/tfjs/issues/4480i initialize model as follows```const init = async () => {  await tf.setBackend('wasm'),  await tf.ready(),  prepareEfficentDetModel(),},init(),```","[""@rafallukasik123 thanks for adding the extra info about the backend you are using. The WASM backend does not yet implement Reciprocal. You could try this with the webgl backend and see if it works.I'm going to update this bug to be a feature request for adding reciprocal to the wasm backend.=====""]",1
https://github.com/tensorflow/tfjs/issues/2365,"[webgpu] Add sub, abs, min, max kernels.",1,closed,2019-11-08T19:43:52Z,2021-10-20T23:22:05Z,Used in handtracking.,"['These changes are implemented here https://github.com/tensorflow/tfjs/pull/5222 , closing this request.=====']",0
https://github.com/tensorflow/tfjs/issues/5093,why is @tensorflow/tfjs-backend-wasm still in alpha？,1,closed,2021-05-19T07:18:06Z,2021-05-24T18:41:38Z,why is @tensorflow/tfjs-backend-wasm still in alpha？what problems is facing？Can @tensorflow/tfjs-backend-wasm be used in project online？Thank you,['Thank you for the report! Looks like the README file is outdated. It is no longer in alpha and it is definitely can be used in online projects. I will update it.The only thing to note is that not all our pre-trained models support the WASM backend. I will update the list in the README file too. Thanks!====='],1
https://github.com/tensorflow/tfjs/issues/428,expandDims not aligned with TF for negative axis,0,closed,2018-06-14T01:11:19Z,2018-07-10T15:40:35Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.11.2#### Browser versionChrome 66.0.3359.181#### Describe the problem or feature requestThe output of the `expandDims` function differs from the output of Tensorflow's `expand_dims` when a negative axis is passed.#### Code to reproduce the bug / link to feature request```const x = tf.tensor2d([15, 16, 17, 18], [2, 2]).expandDims(-1)console.log(x.shape) // will be [2, 1, 2] but the Tensorflow version will be [2, 2, 1]",[],0
https://github.com/tensorflow/tfjs/issues/2150,tfjs-node-gpu can not be installed with `preinstall:prep-gpu.sh`,3,closed,2019-10-04T21:00:23Z,2019-10-07T16:07:09Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version#### Browser version#### Describe the problem or feature request#### Code to reproduce the bug / link to feature requestIf you would like to get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.GitHub issues for this repository are tracked in the [tfjs union repository](https://github.com/tensorflow/tfjs/issues).Please file your issue there, following the guidance in [that issue template](https://github.com/tensorflow/tfjs/blob/master/ISSUE_TEMPLATE.md).","['cc: @nsthorat =====', '@kangyizhang Thanks for filing this bug and investigating it. It is not specific to windows, is that correct? If so, the title of this bug should be updated accordingly.=====', '@caisq yes you are correct, thanks for catching it.=====']",0
https://github.com/tensorflow/tfjs/issues/4768,Inconsistent behaviour across macOS when using WebGL backend,1,closed,2021-03-02T09:28:29Z,2021-03-02T17:09:55Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Most were derived from the example.- OS Platform: macOS Catalina, Macbook Pro 2018- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):```    ""@tensorflow-models/posenet"": ""^2.2.1"",    ""@tensorflow/tfjs-backend-webgl"": ""^2.8.6"",    ""@tensorflow/tfjs-core"": ""^2.8.6"",```- Browser version: Chrome Version 88.0.4324.192 (Official Build) (x86_64)- Tensorflow.js Converter Version: ` ""@tensorflow/tfjs-converter"": ""^3.2.0"",`**Describe the current behavior**I'm running a simple posenet detection using WebWorker, it runs fine on my Macbook Pro 2017, but when I tried it on Macbook Pro 2018 it throws me the following error, which is extremely odd considering it worked on an older Macbook.**Describe the expected behavior**Posenet should be able to detect the user's pose.**Standalone code to reproduce the issue**https://labs.bebensiganteng.com/dubai-proj/**Other info / logs** Include any logs or source code that would be helpful toCode,```const tf = require('@tensorflow/tfjs-core'),const posenet = require('@tensorflow-models/posenet'),require('@tensorflow/tfjs-backend-webgl'),const RES_WIDTH = 257,const RES_HEIGHT = 200,let model,const getPosenet = async () => {  await tf.setBackend(""webgl""),  model = await posenet.load({    architecture: 'ResNet50',    outputStride: 32,    inputResolution: { width: RES_WIDTH, height: RES_HEIGHT },    quantBytes: 2  }),}getPosenet(),export const calculatePose = async (props)=> {  if(model) {    const input = tf.browser.fromPixels(props.data),    const predictions = await model.estimateSinglePose(input, {      flipHorizontal: false    }),    return predictions,  }  return null}```Error Logs (some of the lines were omitted),```canvas_util.ts:45 Could not get context for WebGL version 2webgl_util.ts:537 Error when getting WebGL context:  Error: Cannot create a canvas in this context    at canvas_util.ts:74    at canvas_util.ts:82    at j (canvas_util.ts:41)    at mt (webgl_util.ts:532)    at Object.evaluationFn (flags_webgl.ts:37)    at t.value (environment.ts:138)    at t.value (environment.ts:94)    at t.value (environment.ts:107)    at Object.evaluationFn (flags_webgl.ts:31)    at t.value (environment.ts:138)engine.ts:340 Error: WebGL is not supported on this device    at new n (backend_webgl.ts:142)    at Object.factory (base.ts:25)    at t.value (engine.ts:301)    at t.<anonymous> (engine.ts:254)    at l (runtime.js:63)    at Generator._invoke (runtime.js:293)    at Generator.next (runtime.js:118)    at r (asyncToGenerator.js:3)    at c (asyncToGenerator.js:25)    at asyncToGenerator.js:32engine.ts:393 Uncaught (in promise) Error: Could not initialize any backends, all backend initializations failed.    at t.value (engine.ts:393)    at t.get (engine.ts:192)    at t.value (engine.ts:764)    at i (tensor_ops_util.ts:75)    at o (tensor.ts:56)    at Module.p (io_utils.ts:225)    at t.value (graph_model.ts:160)    at t.<anonymous> (graph_model.ts:134)    at l (runtime.js:63)    at Generator._invoke (runtime.js:293)```","['@bebensiganteng thank you for reporting , same issue has been tracked here , please check https://github.com/tensorflow/tfjs/issues/4284 .=====']",1
https://github.com/tensorflow/tfjs/issues/5686,Different result on converting keras to graph model,11,closed,2021-10-01T02:38:16Z,2021-10-28T06:16:51Z,"HiI converted a tensorflow keras model from Python to JavaScript graph model and the program works fine, but the model is much less accurate than the Python model.I checked all the inputs and the program, all the values ​​were correct.I tried all the conversion parameters but it did not work.","['any warnings about unsupported ops during conversion?  which backend? if its `wasm`, not surprised, see #5641  did you quantize weights? if yes, you might be running into values clipping  is your model using very large values? tensorflow/python natively uses f64, tfjs uses f32=====', ""@vladmandic thanks for your replying.> any warnings about unsupported ops during conversion?No, but there is a warning. you can see in Conversion output.> which backend?I have  the same problems with any backends, **WebGL, WASM or CPU**> did you quantize weights? I tried this switches quantize_float16, quantize_uint8, quantize_uint16, old way quantization_bytes{1,2}  **and no quantize** > is your model using very large values?My model use float32 ### and Conversion output  is:2021-10-02 07:56:55.577985: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.2021-10-02 07:56:56.330169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2153 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5**WARNING**:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.2021-10-02 07:57:15.603301: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.C:\\Users\\admin\\.conda\\envs\\tfgpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: **CustomMaskWarning**: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.  **warnings**.warn('Custom mask layers require a config and must override '2021-10-02 07:57:49.538891: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 12021-10-02 07:57:49.539553: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session2021-10-02 07:57:49.545380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2153 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.52021-10-02 07:57:49.679314: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize  function_optimizer: Graph size after: 2969 nodes (2202), 4589 edges (3821), time = 78.26ms.  function_optimizer: function_optimizer did nothing. time = 1.917ms.2021-10-02 07:57:52.382050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize  model_pruner: Graph size after: 2201 nodes (-765), 3821 edges (-765), time = 41.385ms.  constant_folding: Graph size after: 1441 nodes (-760), 3061 edges (-760), time = 96.044ms.  arithmetic_optimizer: Graph size after: 1459 nodes (18), 3120 edges (59), time = 24.767ms.  dependency_optimizer: Graph size after: 1407 nodes (-52), 1505 edges (-1615), time = 128.634ms.  model_pruner: Graph size after: 1407 nodes (0), 1505 edges (0), time = 9.362ms.  constant_folding: Graph size after: 1407 nodes (0), 1505 edges (0), time = 28.538ms.  arithmetic_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 21.171ms.  dependency_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 12.314ms.  debug_stripper: debug_stripper did nothing. time = 1.452ms.  model_pruner: Graph size after: 1407 nodes (0), 1505 edges (0), time = 7.574ms.  constant_folding: Graph size after: 1407 nodes (0), 1505 edges (0), time = 28.541ms.  arithmetic_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 22.465ms.  dependency_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 12.542ms.  model_pruner: Graph size after: 1407 nodes (0), 1505 edges (0), time = 10.61ms.  constant_folding: Graph size after: 1407 nodes (0), 1505 edges (0), time = 27.608ms.  arithmetic_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 22.503ms.  dependency_optimizer: Graph size after: 1407 nodes (0), 1505 edges (0), time = 12.364ms.  debug_stripper: debug_stripper did nothing. time = 1.486ms.2021-10-02 07:58:01.840274: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize  remapper: Graph size after: 1334 nodes (-221), 830 edges (-221), time = 33.389ms.  constant_folding: Graph size after: 732 nodes (-602), 830 edges (0), time = 38.634ms.  arithmetic_optimizer: Graph size after: 732 nodes (0), 830 edges (0), time = 15.25ms.  dependency_optimizer: Graph size after: 732 nodes (0), 830 edges (0), time = 8.385ms.  remapper: Graph size after: 732 nodes (0), 830 edges (0), time = 5.251ms.  constant_folding: Graph size after: 732 nodes (0), 830 edges (0), time = 18.008ms.  arithmetic_optimizer: Graph size after: 732 nodes (0), 830 edges (0), time = 14.307ms.  dependency_optimizer: Graph size after: 732 nodes (0), 830 edges (0), time = 10.187ms.Writing weight file ./model.json...====="", '> Compiled the loaded model, but the compiled metrics have yet to be builthow was this keras model created?  id try exporting kernas model as a tf frozen model and converting that to make sure everything is there> CustomMaskWarning: Custom mask layers require a config and must override get_confighas this been done?=====', ""@vladmandic > how was this keras model created?It's a keras model with tensorflow backend.> id try exporting kernas model as a tf frozen model and converting that to make sure everything is thereI try tf Frozenmodel but it makes too much less accurate.>> **CustomMaskWarning**: Custom mask layers require a config and must override get_config>has this been done?I don't have this warning in python, and I couldn't solve this warning in converting.- how can I solve **CustomMaskWarning** ?- how important this warning?### Have you another idea to try?====="", '> I try tf Frozenmodel but it makes too much less accurate.If your model when frozen produces less accurate results in python, then that is the issue - if it cannot be properly frozen, you\'ll have the same issues when converted to TFJS graph model. So its not a TFJS issue, but a *""how to freeze a Keras model""* issue.> Compiled the loaded model, but the compiled metrics have yet to be builtTo me this looks like a big issue.=====', ""> If your model when frozen produces less accurate results in pythonin python everything is ok. but frozen model that converted to TFJS  produces less accurate results.I think it's a converting issue.  ====="", '@mmmahdiii Can you share how are you freezing the keras model? We recommend to convert TF keras model directly to TFJS graph model from TF saved model or to TFJS layers model from the H5 file. Frozen model support has been deprecated in TF and it is not recommended as you have other more up-to-date options. =====', ""@pyu10055 My model is H5, I used Frozen for testing graph. The main problem isn't Frozen model. [https://github.com/tensorflow/tfjs/issues/5686#issue-1012816518](url)For now, I refused to do this by converting to Graph. I'll continue with Layer till find a solution. ====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5686"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5686"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/4905,"After tf.Tensor creation, its value can be changed by input TypedArray (not immutable)",10,closed,2021-04-06T04:57:52Z,2021-04-09T23:55:51Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.2.0""> </script>- TensorFlow.js version (use command below): 3.2.0- Browser version: Google Chrome | 89.0.4389.114- Tensorflow.js Converter Version: N/A**Describe the current behavior**```jsconst array = new Float32Array([1, 2, 3, 4]),const tensor = tf.tensor(array, [2, 2]),tensor.print(),// Tensor//    [[1, 2],//     [3, 4]]array[0] = 2,tensor.print(),// Tensor//    [[2, 2],//     [3, 4]]```**Describe the expected behavior**As the [doc](https://js.tensorflow.org/api/latest/#class:Tensor) says, `tf.Tensor` should be immutable. So change `array` should not change the value of `tensor`. It is expected that:```jsconst array = new Float32Array([1, 2, 3, 4]),const tensor = tf.tensor(array, [2, 2]),tensor.print(),// Tensor//    [[1, 2],//     [3, 4]]array[0] = 2,tensor.print(),// Tensor//    [[1, 2],//     [3, 4]]```**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks, please include the fulltraceback. Large logs and files should be attached.","[""@huningxin  Tensor's are immutable and Variable is a fairly thin wrapper around that to allow reassignment.Tensor are results of computation so you are able to change the value of tensor output.====="", '> Tensor are results of computation so you are able to change the value of tensor output.In the test code above, the tensor output is changed without any computations. Is that the expected behavior?=====', 'but the the variable `array` is mutable , so the value is changed=====', ""tensor data is just a reference *until* it's first used - then it becomes internal to tensor and immutable  for example, if you perform *any* operation on a tensor (even a no-op such as `tensor = tensor.add(0),`), the result of that op becomes internal to tensor and no longer associated to the original variable====="", '> but the the variable `array` is mutable , so the value is changedAccording to the doc, ""A tf.Tensor object represents an immutable, multidimensional array of numbers that has a shape and a data type."", I suppose it should be immutable after creation. The change of `array` should not impact `tf.Tensor`. > tensor = tensor.add(0),`tensor.add` just returns another tensor. My issue is about the user created tensor, it should be immutable after creation.=====', 'In theory, completely agree  In practice, it would just cause another data copyIf you really need it, trigger that copy manually using tf.clone or by using spread operator on the array when creating tensor=====', 'If the `array` is a JS array, there is no issue. So I suppose it is an optimization for typed array. My point it would be helpful if this typed array optimization could be documented.Does that make sense?=====', 'It does=====', '@huningxin Thank you for raising this issue. I agree, we should document this limitation properly.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4905"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4905"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/5013,TFJS converter mismanages EfficientDet models 2,6,closed,2021-04-29T14:16:17Z,2021-06-08T19:05:56Z,"Efficientdet  can't be used , it relates to #3903, but seems to be a completely different oneFor this  [model](https://tfhub.dev/tensorflow/efficientdet/d0/1)## Converted the following way```tensorflowjs_converter \    --input_format=tf_hub  \    --output_format=tfjs_graph_model \    --signature_name=serving_default \    --saved_model_tags=serve \    ./efficientdet_d0_1 ./web_model_tfhub```## Loaded and executed as follows in a browser```const model = await tf.loadGraphModel(""web_model_tfhub/model.json""),t = tf.browser.fromPixels(img).expandDims(0)let output = await model.executeAsync(t)```## Results in```Uncaught Error: FusedConv2d and DepthwiseConv2d with BiasAdd must have one extra argument: bias.    at UO (tfjs@2.7.0:17)    at tfjs@2.7.0:17    at tfjs@2.7.0:17    at tfjs@2.7.0:17    at e.t.scopedRun (tfjs@2.7.0:17)    at e.t.tidy (tfjs@2.7.0:17)    at lx (tfjs@2.7.0:17)    at tfjs@2.7.0:17    at KO (tfjs@2.7.0:17)    at p (tfjs@2.7.0:17)```Same when the model is taken from the official repo.Tried both tfjs version 2.7.0 , which was released just after #3903 was resolved , and 3.6.0, the result is the same ","['@partus I was able to convert and run the TFJS model without errors. Please make sure your tensorflowjs pip is the same version (3.6.0) as the tfjs library. =====', 'What versions of other packages are relevant? : I am running it on Arch linux, hence the most recent tensorflow + python as well, e.g. tensorflow with python 3.9... is not officially supported. Which are the important dependencies so that I will I will put it into a cantainer, Hopefully  veryify that it’s fine. Then with the next step we might verify the issue in a container with upgraded versions =====', ""@partus Have you resolved this issue, I've countered the same one. Change tfjs version and tensorflowjs won't work```Uncaught Error: FusedConv2d and DepthwiseConv2d with BiasAdd must have one extra argument: bias.```====="", '@Linh0704 , @pyu10055  I was able to convert and run the tfhub version with python 3.8 instead of 3.9, and tfjs 3.6.0 , but I was unable to run the model from the official repo. Did not dive into it further as decided to run the model on a server.=====', 'Closing the issue , please mention@ if you need further help to reopen. Thank you =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5013"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5013"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/284,Indices must be of dtype `int32` tf.gather,2,closed,2018-05-10T11:44:55Z,2018-05-10T15:26:08Z,"To get help from the community, check out our [Google group](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs).#### TensorFlow.js version0.10.0#### Browser versionhttps://js.tensorflow.org/api/0.10.0/#gather#### Describe the problem or feature requestWhen I testing tf.gather on the docs pageError is thrown: Indices must be of dtype int32#### Code to reproduce the bug / link to feature requestconst x = tf.tensor1d([1, 2, 3, 4]),const indices = tf.tensor1d([1, 3, 3]),x.gather(indices).print(),","['Hi @cclcalder, thanks for reporting. I feel this has been fixed in https://github.com/tensorflow/tfjs-core/pull/989 and release for this will be made soon.=====', 'Yep, thanks @ManrajGrover for the tip!=====']",0
https://github.com/tensorflow/tfjs/issues/1806,Update documentation,1,closed,2019-08-11T14:22:48Z,2019-08-12T15:26:40Z,"https://www.tensorflow.org/js/tutorials/conversion/import_saved_modelHas a code snippet for conversion from tf_frozen_model which is not supported anymore. Also, why is it not supported anymore?","['Hi Can you please open the issue using [template](https://github.com/tensorflow/tfjs/issues/new) so that we can better assist you  , thank you so much.=====']",0
https://github.com/tensorflow/tfjs/issues/2101,feature request: tf.node - support saving to / loading from numpy array,0,open,2019-09-27T17:21:05Z,2019-09-27T17:49:18Z,"It would be great if from `tf.node` you can save tensors in a format that can be loaded in numpy with [numpy.load](https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html), such as in the `.npy` format.  And conversely, it would be great if you can load numpy array files saved as .npy into tensorflow.js with `tf.node`.",[],0
https://github.com/tensorflow/tfjs/issues/1396,converting pb models for tfjs1.0.0,6,closed,2019-03-17T20:53:49Z,2019-03-19T21:37:37Z,"I've tried to convert pretrained model from here - http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gzusing both approaches as described in #1379 , however in both cases when i try to load the model in the browser i,m getting the error below. Uncaught (in promise) Error: Based on the provided shape, [1,1,320,256], the tensor should have 81920 values but has 2295    at d (tfjs@1.0.0:2)    at Te (tfjs@1.0.0:2)    at i (tfjs@1.0.0:2)    at Object.Pl [as decodeWeights] (tfjs@1.0.0:2)    at t.<anonymous> (tfjs@1.0.0:2)    at tfjs@1.0.0:2    at Object.next (tfjs@1.0.0:2)    at a (tfjs@1.0.0:2)here's the parameters used for tensorflowjs_convertertensorflowjs_converter \     --input_format=tf_frozen_model \     --output_node_names='SemanticPredictions' \     --saved_model_tags=serve \     --output_json=true \     deeplabv3_mnv2_pascal_trainval/frozen_inference_graph.pb \     deeplabv3_mnv2_pascal_trainval/webtf","['@alex77t I have tried the converted model, it runs fine on my desktop.Can you share your code that makes the prediction?My input tensor is an image tensor of shape [1, 224, 224, 3].=====', '` <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0""></script>  <script type=""module"">     const MOBILENET_MODEL_PATH =\'models/webtf/model.json\',     const runIt = async () =>{        await tf.loadGraphModel(MOBILENET_MODEL_PATH),       }    runIt(),</script>`in my model.json i\'m indeed seeing references to 1,1,320,256  - i tried with version 0.8.0 on both ubuntu and windows 10 machines. does your converter parameters exactly like mine? {""modelTopology"": {""node"": [{""name"": ""image_pooling/Conv2D/merged_input"", ""op"": ""Const"", ""attr"": {""value"": {""tensor"": {""dtype"": ""DT_FLOAT"", ""tensorShape"": {""dim"": [{""size"": ""1""}, {""size"": ""1""}, {""size"": ""320""}, {""size"": ""256""}]}}}, ""dtype"": {""type"": ""DT_FLOAT""}}}, {""name"": ""image_pooling/BatchNorm/FusedBatchNorm/Offset"", ""op"": ""Const"", ""attr"": {""value"": {""tensor"": {""dtype"": ""DT_FLOAT"", ""tensorShape"": {""dim"": [{""size"": ""256""}]}}}, ""dtype"": {""type"": ""DT_FLOAT""}}}, =====', 'What is inside you runIt() method?The first node of the graph is a constant node, which is not the input node.The input node should be the node with Placeholder op:{""attr"": {""dtype"": {""type"": ""DT_UINT8""}, ""shape"": {""shape"": {""dim"": [{""size"": ""1""}, {""size"": ""-1""}, {""size"": ""-1""}, {""size"": ""3""}    ]}}}, ""name"": ""ImageTensor"", ""op"": ""Placeholder""}=====', 'const runIt = async () =>{ await tf.loadGraphModel(MOBILENET_MODEL_PATH), } only the call to the tf.loadGraphModel method. >The input node should be the node with Placeholder op:Is it possible to post model.json you have got? I:m attaching mine... [model.zip](https://github.com/tensorflow/tfjs/files/2981015/model.zip)=====', '@alex77t If the failure happens during model loading, not at the inference time, I suspect that your server is not serving the model files properly.Can you check the network tab of your browser dev tool, see if there are any errors there. If you can post a screen capture would be great. Thanks =====', ""Hi @pyu10055 , you're right - indeed the shard files were not loading correctly because of my local webserver configuration...  apologies for causing confusion and wasting your time on this. This ticket can be closed. =====""]",0
https://github.com/tensorflow/tfjs/issues/2453,window not defined reference error,2,closed,2019-11-30T09:45:36Z,2020-04-17T06:56:18Z,"I got this type of error after installation:/home/debo/node_modules/vega-embed/build/vega-embed.js:3874  const w = window,            ^ReferenceError: window is not defined    at /home/debo/node_modules/vega-embed/build/vega-embed.js:3874:13    at xhtml (/home/debo/node_modules/vega-embed/build/vega-embed.js:2:83)    at Object.<anonymous> (/home/debo/node_modules/vega-embed/build/vega-embed.js:5:2)    at Module._compile (internal/modules/cjs/loader.js:654:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:665:10)    at Module.load (internal/modules/cjs/loader.js:566:32)    at tryModuleLoad (internal/modules/cjs/loader.js:506:12)    at Function.Module._load (internal/modules/cjs/loader.js:498:3)    at Module.require (internal/modules/cjs/loader.js:598:17)    at require (internal/modules/cjs/helpers.js:11:18)","[""Hello, this sort of question is better suited for StackOverflow. This issues list is for bugs and feature requests. Thanks and please use the tag 'tensorflow.js' on your StackOverflow question to makes sure it's discoverable.====="", 'I solved this by just commenting out that line.`// const w = window,`=====']",0
https://github.com/tensorflow/tfjs/issues/5224,how to assign values to a portion of a large tensor,2,closed,2021-06-16T07:19:09Z,2021-07-01T17:10:41Z,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>**Describe the feature and the current behavior/state.**suppose that we have a 3*3 tensor```Tensor    [[0, 0, 0],     [0, 0, 0],     [0, 0, 0]]```Is there any way we can change it into ```Tensor    [[1, 1, 0],     [1, 1, 0],     [0, 0, 0]]```?The application may be editing some masks of an image","['@JoeyChuuichi If you want to manipulate values of the tensor, you can download them to CPU by using the data() or dataSync() method.Manipulate the data as you need, and create a new tensor.=====', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!=====']",0
https://github.com/tensorflow/tfjs/issues/5691,Tensorflow JS Converter Can't Find Expected FusedBatchNorm Input,16,closed,2021-10-04T16:03:37Z,2021-11-05T20:25:23Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes. I have used the following script to convert a graph model to a .pb format as I could not find official documentation on how to do this:  ```import tensorflow as tfmeta_path = 'test.meta' # Your .meta fileoutput_node_names = ['pose/locref_pred/block4/BiasAdd', 'pose/part_pred/block4/BiasAdd']    # Output nodeswith tf.compat.v1.Session() as sess:    # Restore the graph    saver = tf.compat.v1.train.import_meta_graph(meta_path)    # Load weights    saver.restore(sess,tf.compat.v1.train.latest_checkpoint('checkpoint_folder'))    # Freeze the graph    frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(        sess,        sess.graph_def,        output_node_names, variable_names_blacklist=[n.name for n in sess.graph_def.node if ""FusedBatchNorm"" in n.name])    # Save the frozen graph    with open('_graph.pb', 'wb') as f:      f.write(frozen_graph_def.SerializeToString())```- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Kubuntu 21.04- TensorFlow.js installed from (npm or script link):  Installed from pip - Tensorflow.js Converter Version:  3.9.0**Describe the current behavior**When using this command :```tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='pose/locref_pred/block4/BiasAdd','pose/part_pred/block4/BiasAdd' output_graph.pb output --skip_op_check```I get```2021-10-04 11:07:07.150459: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0', dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-10-04 11:07:07.150484: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-10-04 11:07:13.137106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize  debug_stripper: debug_stripper did nothing. time = 0.369ms.  model_pruner: Graph size after: 1130 nodes (-530), 1201 edges (-530), time = 29.391ms.  constant_folding: Graph size after: 1088 nodes (-42), 1147 edges (-54), time = 105.765ms.  arithmetic_optimizer: Graph size after: 1088 nodes (0), 1147 edges (0), time = 52.367ms.  dependency_optimizer: Graph size after: 1064 nodes (-24), 1123 edges (-24), time = 23.379ms.  model_pruner: Graph size after: 1064 nodes (0), 1123 edges (0), time = 10.789ms.  constant_folding: Graph size after: 1064 nodes (0), 1123 edges (0), time = 42.342ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 41.601ms.  dependency_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 26.46ms.  debug_stripper: debug_stripper did nothing. time = 2.391ms.  model_pruner: Graph size after: 1064 nodes (0), 1123 edges (0), time = 15.009ms.  constant_folding: Graph size after: 1064 nodes (0), 1123 edges (0), time = 40.475ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 33.552ms.  dependency_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 20.708ms.  model_pruner: Graph size after: 1064 nodes (0), 1123 edges (0), time = 12.907ms.  constant_folding: Graph size after: 1064 nodes (0), 1123 edges (0), time = 44.195ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 37.797ms.  dependency_optimizer: Graph size after: 1064 nodes (0), 1123 edges (0), time = 25.408ms.WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'```The warnings here appear to be problematic. When loading the files into a browser, I get ``tf.loadGraphModel('http://127.0.0.1:5000/download/model.json')Promise {<pending>}util.ts:105 Uncaught (in promise) Error: Based on the provided shape, [3,3,512,512], the tensor should have 2359296 values but has 2300067``**Describe the expected behavior**Either the warnings should not exist (i.e. the FusedBatchNorm is converted properly), or the model should be able to load in the browser without the batch norm layers**Standalone code to reproduce the issue**https://github.com/ByrdOfAFeather/tfjs_bug",,1
https://github.com/tensorflow/tfjs/issues/5941,Multi output model history validation accuracy problem.,3,closed,2021-12-14T14:37:45Z,2021-12-16T22:31:42Z,"**System information**- Have I written custom code: Yes- OS Platform and Distribution: Windows 10,  and whatever codesandox.io runs on- TensorFlow.js installed from (npm or script link): npm @tensorflow/tfjs-node@3.12.0 and @tensorflow/tfjs-node-gpu@3.12.0**Describe the current behavior**`fitDataset` of `tf.model` with multiple (3) outputs show invalid history entry for validation accuracy of 3rd output, on both `tfjs-node` and `tfjs-node-gpu`.Also there's an type error when trying to pass output layers into array of model outputs.**Describe the expected behavior**Show proper metrics for outputs **Standalone code to reproduce the issue**code to reproduce ```import {  layers,  train,  model,  SymbolicTensor,  tensor2d,  tensor1d,  data} from ""@tensorflow/tfjs-node"",const { flatten, dense, input, conv1d, maxPooling1d } = layers,(async () => {  const inputs = input({    shape: [8, 4],    name: ""input331""  }),  const conv1D0 = conv1d({ filters: 32, kernelSize: 4 }).apply(inputs),  const maxPooling1D0 = maxPooling1d({ poolSize: 4, padding: ""valid"" }).apply(    conv1D0  ),  const flatten0 = flatten().apply(maxPooling1D0),  const dense00 = dense({ units: 512, activation: ""relu"" }).apply(flatten0),  const dense10 = dense({ units: 512, activation: ""relu"" }).apply(flatten0),  const dense20 = dense({ units: 512, activation: ""relu"" }).apply(flatten0),  const dense01 = dense({ units: 256, activation: ""relu"" }).apply(dense00),  const dense11 = dense({ units: 256, activation: ""relu"" }).apply(dense10),  const dense21 = dense({ units: 256, activation: ""relu"" }).apply(dense20),  const y1 = dense({ units: 1, name: ""y1"" }).apply(dense01),  const y2 = dense({ units: 1, name: ""y2"" }).apply(dense11),  const y3 = dense({ units: 1, name: ""y3"" }).apply(dense21),  let tfModel = model({    inputs: inputs,    // @ts-ignore here is also a bug with types...    outputs: [y1, y2, y3],    name: ""1""  }),  tfModel.compile({    optimizer: train.adam(0.0001),    loss: {      y1: ""meanSquaredError"",      y2: ""meanSquaredError"",      y3: ""meanSquaredError""    },    metrics: {      y1: ""accuracy"",      y2: ""accuracy"",      y3: ""accuracy""    }  }),  tfModel.summary(),  const array = [    [2, 3, 1, 2],    [2, 4, 2, 3],    [3, 5, 2, 4],    [4, 5, 3, 3],    [3, 6, 3, 5],    [5, 7, 4, 5],    [5, 7, 4, 6],    [6, 7, 3, 4]  ],  const tensor0 = tensor2d(array).expandDims(0),  const tensor1 = tensor2d(array.reverse()).expandDims(0),  const labels0 = [tensor1d([0.01]), tensor1d([0.02]), tensor1d([0.03])],  const labels1 = [tensor1d([0.1]), tensor1d([0.09]), tensor1d([0.08])],  const ds1 = data.array([tensor0, tensor1]),  const ds2 = data.array([labels0, labels1]),  const ds3 = data.zip([ds1, ds2]),  const dataSet = ds3.map((x) => {    return { xs: x[0], ys: x[1] },  }),  const history = await tfModel.fitDataset(dataSet, {    epochs: 20,    verbose: 0,    validationData: dataSet  }),  console.log(history),  const predictions = await tfModel.predict(tensor1),  // @ts-ignore  predictions.forEach((tensor) => {    tensor.print(),  }),})(),```https://codesandbox.io/s/typescript-node-tfjs-dunsw?file=/src/index.ts:2214-2224**Other info / logs** Example output from codesanbox.io, see `val_y3_acc` as it somehow doesn't show correct value, but the actual predictions are good.```History {  validationData: null,  params: {    epochs: 15,    initialEpoch: null,    samples: null,    steps: 2,    batchSize: null,    verbose: 0,    doValidation: true,    metrics: [      'loss',        'y1_loss',      'y2_loss',     'y3_loss',      'y1_acc',      'y2_acc',      'y3_acc',      'val_loss',      'val_y1_loss', 'val_y2_loss',      'val_y3_loss', 'val_y1_acc',      'val_y2_acc',  'val_y3_acc'    ]  },  epoch: [     0,  1,  2, 3,  4,  5,     6,  7,  8, 9, 10, 11,    12, 13, 14  ],  history: {    val_loss: [      0.06740279495716095,[... omitted for readability],      0.00021780715906061232    ],    val_y1_loss: [      0.08847129344940186,[... omitted for readability],      0.0015886977780610323    ],    val_y2_loss: [      0.10580939054489136,[... omitted for readability],      0.003424264956265688    ],    val_y3_loss: [      0.06740279495716095,[... omitted for readability],      0.00021780715906061232    ],    val_y1_acc: [      0.021068500354886055,      0.010667561553418636,      0.02188786491751671,      0.022847648710012436,      0.013655520044267178,      0.004707253538072109,      0.0018774932250380516,      0.0036717751063406467,      0.005106504075229168,      0.0037899017333984375,      0.0013052645372226834,      0.000026645591788110323,      0.0004705819010268897,      0.001332651823759079,      0.0013708906481042504    ],    val_y2_acc: [      0.01733810268342495,      0.0026979746762663126,      0.016361301764845848,      0.020113449543714523,      0.01138104498386383,      0.002396896481513977,      0.0001758798462105915,      0.003242085687816143,      0.005758211947977543,      0.004670063033699989,      0.0017483194824308157,      0.00013128435239195824,      0.0006708669243380427,      0.001789348665624857,      0.0018355674110352993    ],    val_y3_acc: [      0, 0, 0, 0, 0, 0,      0, 0, 0, 0, 0, 0,      0, 0, 0    ],    loss: [      0.34463465213775635,[... omitted for readability],      0.003640228882431984    ],    y1_loss: [      0.1969234198331833,[... omitted for readability],      0.00024854816729202867    ],    y2_loss: [      0.07084406167268753,[... omitted for readability],,      0.0014955892693251371    ],    y3_loss: [      0.07686717063188553,[... omitted for readability],      0.0018960912711918354    ],    y1_acc: [      0, 0, 0, 0, 0, 0,      0, 0, 0, 0, 0, 0,      0, 0, 0    ],    y2_acc: [      0, 0, 0, 0, 0, 0,      0, 0, 0, 0, 0, 0,      0, 0, 0    ],    y3_acc: [      0, 0, 0, 0, 0, 0,      0, 0, 0, 0, 0, 0,      0, 0, 0    ]  }}```","['@tuturis The `acc` metric counts how many time the output is the same as label throughout the training, it targets classification (with fixed label). In your case your output value is float,  the training results will never be exactly the same as the labels. So you will see `0` values in the metric.You can try other metrics listed [here](https://js.tensorflow.org/api/latest/#Metrics) =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5941"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5941"">No</a>=====', ""> @tuturis The `acc` metric counts how many time the output is the same as label throughout the training, it targets classification (with fixed label). In your case your output value is float,  the training results will never be exactly the same as the labels. > So you will see `0` values in the metric.> You can try other metrics listed [here](https://js.tensorflow.org/api/latest/#Metrics) Thanks for the reply, noticed it was an regression problem and accuracy only partially makes sense for it. But I think there's still a bug because some of the accuracy metrics still report some values and only the third one is faulty. I would still consider this a bug.=====""]",1
https://github.com/tensorflow/tfjs/issues/4433,div error when tfjs-backend-wasm=2.8.1,1,open,2020-12-21T08:34:55Z,2021-02-16T21:10:32Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),we only address code/doc bugs, performance issues, feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code : yes - OS Platform and Distribution : macOS 10.14.6- TensorFlow.js installed from : npm - TensorFlow.js version : tfjs-backend-wasm 2.8.1- Browser version: Chrome 87.0.4280.88- Tensorflow.js Converter Version:**Describe the current behavior**use div in wasm backend,it didn't work.**Describe the expected behavior**it will work**Standalone code to reproduce the issue**open `chrome://flags/#enable-webassembly-simd` in chrome,and enable `WebAssembly SIMD support.` and `WebAssembly threads support`  tfjs-backend-wasm 2.6.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.7.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.7.0 with `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.0 with `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.1 without `WebAssembly SIMD support.` and `WebAssembly threads support`  work error tfjs-backend-wasm 2.8.1 with `WebAssembly SIMD support.` and `WebAssembly threads support`  work error```javascriptvar point1= tf.tensor2d([[-0.6070004, 0.881     , -0.0700001],     [-0.5930004, 0.8440001 , -0.069    ],     [-0.5609999, 0.8100002 , -0.064    ]])var N=tf.tensor1d([0.015242, -0.054074, -0.3919158])point1.dot(N).abs().div(N.norm())```Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible, please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful to```logUncaught RuntimeError: abort(Assertion failed: Cannot call unknown function RealDiv, make sure it is exported). Build with -s ASSERTIONS=1 for more info.    at abort (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:13285)    at assert (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6125)    at getCFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6211)    at ccall (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6765)    at eval (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:7337)    at kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/dist/kernels/binary_kernel.js:52:34)    at Object.kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/dist/kernels/binary_kernel.js:55:13)    at kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:455:30)    at eval (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:519:27)    at Engine.scopedRun (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:348:25)    at Engine.runKernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:517:14)    at Engine.runKernel (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:406:21)    at div_ (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/ops/div.js:65:59)    at div__op (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:49:28)    at Tensor._tensor__WEBPACK_IMPORTED_MODULE_1__.Tensor.div (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/public/chained_ops/div.js:24:64)    at eval (eval at ransacRodeSegment (webpack-internal:///./src/views/tool/editor/ai/pointcloud/rodeSegment.js), <anonymous>:1:23)```","[""Could you please provide the version numbers of other tfjs packages you're using on this page (e.g. tfjs-core, tfjs)? I suspect this has to do with a version mismatch (earlier versions of tfjs-core do not export RealDiv).=====""]",1
https://github.com/tensorflow/tfjs/issues/1337,D3D shader compliation failed with skip optimization flags,10,closed,2019-03-07T00:20:46Z,2019-04-13T22:10:08Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js version1.0.0#### Browser versionGoogle ChromeVersion 72.0.3626.121 (Official Build) (64-bit)#### Describe the problem or feature requestHi I'm using Posenet for my project. After the latest update of Posenet version (from 0.2.3 to 1.0.0), I got an error message like below. Right now I'm using posenet version 0.2.3 and tensorflow 0.15.3 for my project as it works well with them. So fixing this is not urgent for me, but I'm reporting it here anyway. I hope to use the latest version later. Thx!**1. Message on console**C:\fakepath(77,32-58): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(78,32-80): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): warning X3556: integer divides may be much slower, try using uints if possible.Warning: D3D shader compilation failed with default flags. (ps_5_0) Retrying with skip validationC:\fakepath(77,32-58): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(78,32-80): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): warning X3556: integer divides may be much slower, try using uints if possible.Warning: D3D shader compilation failed with skip validation flags. (ps_5_0) Retrying with skip optimizationC:\fakepath(77,32-58): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(78,32-80): warning X3556: integer divides may be much slower, try using uints if possible.C:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): error X4010: Unsigned integer divide by zeroC:\fakepath(116,33-81): warning X3556: integer divides may be much slower, try using uints if possible.Warning: D3D shader compilation failed with skip optimization flags. (ps_5_0)Failed to create D3D ShadersError: Failed to link vertex and fragment shaders#### Code to reproduce the bug / link to feature request**1. Part of my code:** const pose = await net.estimateSinglePose($video, imageScaleFactor, flipHorizontal, outputStride),","['Before getting this error I getTypeError: t.fromPixels is not a functionThis is from this sample page: https://github.com/tensorflow/tfjs-models/tree/master/posenet#via-script-tagChanging it to t.browser.fromPixels then results in the error reported in this issue.```<html>  <head>    <!-- Load TensorFlow.js -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script>    <!-- Load Posenet -->    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet""></script> </head>  <body>    <img id=\'cat\' src=\'cat.jpg \'/>  </body>  <!-- Place your code in the script tag below. You can also use an external .js file -->  <script>    var imageScaleFactor = 0.5,    var outputStride = 16,    var flipHorizontal = false,    var imageElement = document.getElementById(\'cat\'),    posenet.load().then(function(net){      return net.estimateSinglePose(imageElement, imageScaleFactor, flipHorizontal, outputStride)    }).then(function(pose){      console.log(pose),    })  </script></html>```=====', '@Cha-Dahee  are you not getting a tf.fromPixels method changed warning with that version?? what is your work around? after 0.2.3, posenet has 1.0.0 directly =====', 'I wonder if this is related to the various Windows WebGL issues: https://github.com/tensorflow/tfjs/issues/1400Here the same error appears but specifically in MS Edge: https://stackoverflow.com/questions/55188619/ms-edge-script5022-failed-to-link-vertex-and-fragment-shaders=====', ""Can you run `tf.ENV.set('WEBGL_PACK', false)` right after you load the TF.js library and let me know if it helps so we can narrow down the problem. Thanks! ====="", 'Has this fix propagated tohttps://cdn.jsdelivr.net/npm/@tensorflow/tfjsandhttps://cdn.jsdelivr.net/npm/@tensorflow-models/posenetI still get the same errors when running the sample code at https://github.com/tensorflow/tfjs-models/tree/master/posenet#via-script-tag=====', ""Not yet, this fix will go in our next release `tfjs@1.0.4` which will come out next week. If you are using posenet, you will have to wait for posenet's next release too, which we plan to do next week as well====="", ""> Not yet, this fix will go in our next release tfjs@1.0.4 which will come out next week. If you are using posenet, you will have to wait for posenet's next release too, which we plan to do next week as wellAlso waiting for this fix. Brings up the question. Does Posenet have some script tag version history? The change to 1.0.0 broke a few of my old examples that use the live link. Would appreciate being able to download an old version while I look into how to make updates. I can probably find an old version using github desktop, but other people might be in the same boat as me.https://hpssjellis.github.io/tensorflowjs-bvh/cam/index.htmlhttps://hpssjellis.github.io/tensorflowjs-bvh/save-video/index.htmlhttps://hpssjellis.github.io/beginner-tensorflowjs-examples-in-javascript/tf-examples/posenet/camera/index.htmlMost of my examples were done around TFJS version 0.11.4. perhaps @oveddan has an old version for us to download.====="", 'Well that was strangely easy. I was going to fork tfjs-models for posenet , but it said I already had a version, then I was going to put it on cloud9 at http://c9.io to compile it, but found an old version, looked in the ""dist"" folder and found ""posenet.min.js"". Renamed it and put it on my fork at:https://github.com/hpssjellis/tfjs-models/blob/master/posenet.v011.min.jsand my old examples work again :)=====', ""Also looking forward to this fix. The real-time speed of 1.0.0 isn't comparable to 0.2.3. I was planning to launch a product tomorrow using posenet-related work. @dsmilkov do you happen to know when exactly 1.0.4 will be released? Thanks!====="", 'So I got my Posenet working with tfjs version 1.0.4, which now does not give the shader error that version 1.0.3 was showing for single-pose, but strangely single-pose seems to not be refreshing the screen. mult-ipose works fine.![image](https://user-images.githubusercontent.com/5605614/56085758-976b2000-5dfd-11e9-9dd0-c6c465a3591c.png)I think the issue is probably in the function estimateSinglePose, wondering if this is only for my implementation or if others are seeing this. I also probably have the [CPU fall back](https://github.com/tensorflow/tfjs/issues/1472) as my version seems kind of slow.=====']",0
https://github.com/tensorflow/tfjs/issues/4340,UpSampling2D(interpolation=bilinear) gives significant difference when converted using tensorflowjs_converter,1,open,2020-12-02T23:40:40Z,2021-06-03T16:16:30Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): close to stock example- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04- TensorFlow.js version (use command below): ""@tensorflow/tfjs-node"": ""^2.1.0""- Tensorflow.js Converter Version: tensorflowjs 2.7.0**Describe the current behavior**1. Added a layer`UpSampling2D` with interpolation as `bilinear` (tensorflow python model).`model.add(tf.keras.layers.UpSampling2D(size=(2,2), interpolation='bilinear'))`2. Converted the model (to be used in js) using tensorflowjs_converter`tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --saved_model_tags=serve --signature_name=serving_default --skip_op_check --strip_debug_ops=True --weight_shard_size_bytes=4194304 model_path output_path`3. Now, the output in tensorflow js (converted model) is significantly different than tensorflow python.**Note:** The outputs are same when we use `interpolation=nearest` (js vs python)**Describe the expected behavior**There should not be significant difference when a model with a particular layer is converted from python to js.","[""I am having the same issue.I am using an `UpSampling2D` layer to generate images using a Generative Adversarial Network. In Python I set `tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')`, but the resulting generated image in TensorFlow.js looks as though it was switched to `interpolation='nearest'`.=====""]",1
https://github.com/tensorflow/tfjs/issues/546,reshape operation will lose shape information after using tensorflowjs_converter,2,closed,2018-07-25T09:42:32Z,2018-08-06T22:23:11Z,"When I tried to use tensorflowjs_converter to convert some models to tensorflowjs models. I found that when I load the .pb file with python code to check the shape in the reshape operation, I got 0s, for example, [0, 0].  Any ideas why it happened?","['Hi WinterTaoTao,Thanks for the report.  Can you share a minimal reproduction of your problem?  Thanks.=====', '@WinterTaoTao when tfjs converts TF pb file, it stripes out all the constant tensors and puts them into a separate binary file. This way, we can optimize the weight files for browser storage and keep the pb file minimal.In the case of the shape tensor for shape op, it is removed from the converted pb file. You can take a look at the weight_manifest.json file, it should have an entry for that tensor.=====']",0
https://github.com/tensorflow/tfjs/issues/5551,"In compiling `tfjs-backend-wasm`, emscripten will always use a default Python 3.5, which will cause a syntax error in `emcc.py`",1,closed,2021-08-29T11:57:56Z,2021-09-24T02:42:06Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): cloned from github repository- TensorFlow.js version: v3.8.0- CUDA/cuDNN version: N/A**Describe the problem**Building `tfjs-backend-wasm` package requires emscripten to compile XNNPack into wasm. Emscripten will use Python to execute its `emcc.py` file.The default Python on my machine is a Python 3.5 at `/usr/bin/python3`. When executing `emcc.py`, it will raise a syntax error on the line 1625:> `exit_with_error(f'{setting} must be a multiple of WebAssembly page size (64KiB), was {settings[setting]}')`I find out that the 'f-string' is added to Python after Python 3.6, so I should use a newer version of Python 3 in compiling.There is a Python 3.7 installed on my machine (located at /usr/local/python3.7/bin/python3). I added the Python 3.7 to my `$PATH` variable and `$PYTHON` variable, and I have checked that now the commands `python`, `python3`, `/usr/bin/env python3`, `$PYTHON` all refer to the Python 3.7 executable. However, emscripten still uses the default Python 3.5 to run the `emcc.py` file, ignoring these variables and producing the same error. I even try to modify the first line in `emcc.py` from `#!/usr/bin/env python3` into `#!/usr/local/python3.7/bin/python3`, but it does not work at all.I do not have the root access on the machine, so I cannot remove the `/usr/bin/python3` executable, nor can I update or replace it. How could I let emscripten to use a Python 3.7 installed in a custom location?**Provide the exact sequence of commands / steps that you executed before running into the problem**```cd tfjs-backend-wasmyarn test # just for building the package```**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5551"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5551"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/5674,"Cannot run on github actions ""ubuntu-20.04""",4,closed,2021-09-28T19:18:27Z,2021-10-12T21:44:30Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution: ubuntu-20.04 (github actions)- TensorFlow.js installed from (npm or script link): npm- ""@tensorflow/tfjs-node"": ""^3.9.0""- ""@tensorflow/tfjs"": ""^3.9.0""- ""@tensorflow-models/mobilenet"": ""^2.1.0""**Describe the problem**Error loading shared library ld-linux-x86-64.so.2: No such file or directory (needed by /github/workspace/packages/app/node_modules/@tensorflow/tfjs-node/lib/napi-v8/../../deps/lib/libtensorflow.so.2)at Runtime._loadModule (node_modules/jest-runtime/build/index.js:893:29) at Object.<anonymous> (node_modules/@tensorflow/tfjs-node/dist/index.js:60:16)**Provide the exact sequence of commands / steps that you executed before running into the problem**npm installnpm run testsI'm running tests with jest and mobilenet**Any other info / logs**![Screenshot 2021-09-28 at 21 12 00](https://user-images.githubusercontent.com/18119833/135150848-a2ec8341-78e3-48cf-b333-7b27e35ddd70.png)","['You might probably missing libc, you might to run `apt install libc-dev or build-essential` , please check a related [so](https://stackoverflow.com/questions/50288034/unsatisfiedlinkerror-tmp-snappy-1-1-4-libsnappyjava-so-error-loading-shared-li/51655643#51655643) question. Thank you =====', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5674"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5674"">No</a>=====']",0
https://github.com/tensorflow/tfjs/issues/3401,Missed data normalization in the predictSample,4,closed,2020-06-06T10:42:47Z,2020-08-07T18:42:45Z,"#### TensorFlow.js version ""@tensorflow/tfjs@1.7.4""""@tensorflow/tfjs-node"" : 1.3.2#### Browser version-#### Node versionv12.16.1####  npm version 6.14.5#### Describe the problem or feature requestThen running example by [Codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#4) sources - sample data back  pitch type - `Fastball (4-seam)`But in example comment and by data in the csv must be - `Curveball`#### Code to reproduce the bug / link to feature requestBefore sample data normalization return ```bashaccuracyPerClass {  'Fastball (2-seam)': { training: 0.7185505301601661, validation: 0.6986126235348639 },  'Fastball (4-seam)': { training: 0.8041197856226936, validation: 0.6679492705687881 },  'Fastball (sinker)': { training: 0.5543037599927575, validation: 0.40438333789759784 },  'Fastball (cutter)': { training: 0.7544288540147245, validation: 0.6399490784760564 },  Slider: { training: 0.7030178446555511, validation: 0.6925535777537152 },  Changeup: { training: 0.7323414023183287, validation: 0.6433946020342409 },  Curveball: { training: 0.7786480566757964, validation: 0.6720112682203763 }}predictSample [2.668,-114.333,-1.908,4.786,25.707,-45.21,78,0]result [2.802596928649634e-44,1,0,0,0,0,0] predictedPitch 1 'Fastball (4-seam)'```After problem solution - normalize sample data in the [predictSample](https://github.com/wirtaw/baseball-tensorflow-pitch-type/blob/a2401d416d5b8b4e6b103222a02aba38f4636896/src/pitch_type.js#L112) function```bash1999ms 28558us/step - acc=0.814 loss=0.498 accuracyPerClass {  'Fastball (2-seam)': { training: 0.5205437056370938, validation: 0.5150882147124503 },  'Fastball (4-seam)': { training: 0.7875944544905796, validation: 0.6238932343688793 },  'Fastball (sinker)': { training: 0.6789201184713064, validation: 0.4676991201750934 },  'Fastball (cutter)': { training: 0.7440306098423898, validation: 0.63376548955217 },  Slider: { training: 0.602961182523286, validation: 0.6999326177127659 },  Changeup: { training: 0.7483404694814235, validation: 0.6366032464616 },  Curveball: { training: 0.8942686270378762, validation: 0.7976557667413726 }}predictSample [2.668,-114.333,-1.908,4.786,25.707,-45.21,78,0][0.5833017591339648,0.5769492653845572,0.533830954048916,0.6717661028237695,0.40997410954427776,0.07895708794832494,0.41850220264317173,0]result [2.1684355699935765e-10,3.1887741145514292e-12,3.5616327664911296e-8,6.949693442948046e-7,0.022125843912363052,0.00005654788401443511,0.9778168797492981] predictedPitch 6 'Curveball'```[`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js)[`tfjs-node`](https://stackoverflow.com/questions/tagged/tfjs-node)","['@wirtaw please let us know if this is still an issue ,if yes can you please provide steps to reproduce the error ?=====', ""@rthadur yes, this issue is actualsteps for reproduce- Clone and install [baseball-node](https://github.com/tensorflow/tfjs-examples/tree/master/baseball-node) exaple- In the [server.js](https://github.com/tensorflow/tfjs-examples/blob/master/baseball-node/server.js) after `console.log('training complete'),` add line to emit predictSample `io.emit('predictResult', await pitch_type.predictSample([ 2.668, -114.333, -1.908, 4.786, 25.707, -45.21, 78, 0])),`- Run `npm run start-server`- After training complete `[console.log(result)](https://github.com/tensorflow/tfjs-examples/blob/046a2a18f00bf5c24d8f5fc766127d2b217a1a45/baseball-node/pitch_type.js#L121)` return `[ [    1, 0, 0, 0,    0, 0, 0  ] ] `this falsy resut====="", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 dyas if no further activity occurs. Thank you.=====', 'Closing as stale. Please @mention us if this needs more attention.=====']",0
https://github.com/tensorflow/tfjs/issues/4930,"tfjs 3.4.0 has been released on cdnjs, but it's not on github releases page and there are no release notes",3,closed,2021-04-14T11:34:13Z,2021-04-14T18:43:47Z,"`tfjs` **3.4.0** has been released on <https://www.npmjs.com/package/@tensorflow/tfjs>,  but it's not on <https://github.com/tensorflow/tfjs/releases> releases page and there are no release notes  environment: tfjs 3.4.0 on ubuntu 20.10","['@vladmandic release notes has been updated and i see it here https://github.com/tensorflow/tfjs/releases, thanks for noticing this.cc @lina128 =====', ""yup, it's online now, closing the issue.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4930"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4930"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/1631,Support batched matMul.,0,open,2019-05-30T21:11:44Z,2019-05-30T21:15:04Z,,[],0
https://github.com/tensorflow/tfjs/issues/5611,"Same image after first prediction returns different [x,y,z] values for face-landmarks-detection model",3,open,2021-09-13T10:53:42Z,2021-09-21T13:59:24Z,"**System information**- Custom code- OS Platform and Distribution (MacOs BigSur):- TensorFlow.js installed from npm (latest)- Browser version: Chrome 93 StableFrom my package.json:``` ""dependencies"": {    ""@material-ui/core"": ""^4.12.3"",    ""@material-ui/icons"": ""^4.11.2"",    ""@tensorflow-models/face-landmarks-detection"": ""0.0.1"",    ""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"",    ""@tensorflow/tfjs-backend-wasm"": ""^3.8.0"",    ""@tensorflow/tfjs-backend-webgl"": ""^2.4.0"",    ""@tensorflow/tfjs-converter"": ""^2.4.0"",    ""@tensorflow/tfjs-core"": ""^2.4.0"",    ""axios"": ""^0.21.1"",    ""react"": ""^17.0.2"",    ""react-dom"": ""^17.0.2"",    ""react-redux"": ""^7.2.4"",    ""react-router-dom"": ""^5.2.0"",    ""react-scripts"": ""^4.0.3""  },```I have an image uploader using reactJs. All I'm doing is selecting an image file, creating an image element from with the img.src as the base64 encoded data of the image and passing it through the model to detect the landmark coordinates for that image.```async function predictImage(file) {      return new Promise((rs, rj) => {        try {          const imageElement = document.createElement(""img""),          imageElement.src = file,          imageElement.onload = async () => {           const preds = await model.estimateFaces({             input: imageElement,           }),          return rs(),        },        } catch (err) {          return rj(err),        }      }),    }```I then store a selected set from the scaled mesh obj in a database for later comparison.Given image A and image B,Image A: ![jackie1](https://user-images.githubusercontent.com/5845311/133072304-a54731e8-5463-4cf6-a2c8-d5ef5dd1774a.jpeg)Image B: ![salma1](https://user-images.githubusercontent.com/5845311/133072332-22fead35-b7e5-4d14-b845-b59c79947d0a.jpg)If I run image A for the first time after loading the model I get a set of values, say from key point 6 from the [face mesh](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/mesh_map.jpg) provided, `[674.8491821289062,452.3935241699219,-8.618978500366211]`. I then upload image B and run it through the model without restarting or refreshing the page. If I then upload image A again, the values for the same key point 6 are now different from the first prediction of image A, `[336.6038818359375,387.3382263183594,-14.38083553314209]`.More extensive list of the key point values from initial upload and secondary upload:![Screenshot 2021-09-13 at 12 03 11](https://user-images.githubusercontent.com/5845311/133072756-7c681f50-dfd1-4828-b9e8-8e37cd05e66a.png)If I then refresh my page and upload image A, the values will match the first upload from the previous attempt.I have checked my upload logic to make sure I am not overlapping my images in any way and that I am passing a clean image to the model.Is this the expected behaviour? What would explain this behaviour? Has anyone else encountered this issue?","[""Thanks for the question, @hyprstack. face-landmarks-detection is actually stateful, i.e. past inputs to the model can affect future predictions. This is because the model pipeline [caches regions of interest](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/pipeline.ts#L430-L460) and then [runs landmark detection on just those regions](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/pipeline.ts#L300-L304). I think this is done for performance reasons when running on a video stream, and it allows the pipeline to avoid running face detection if faces have not moved, although I'm not 100% sure (@lina128 would know more). @lina128 do you know of a way to avoid this caching in case a user wants to run the model on several separate static images?====="", ""We'll add a staticImage option to the API soon.====="", 'Thanks @mattsoulanille & @lina128 . Makes sense to optimise it like that. Will wait for the update for now :)=====']",1
https://github.com/tensorflow/tfjs/issues/583,tensorflow-posenet installation fails,3,closed,2018-08-06T21:42:41Z,2018-11-05T18:21:50Z,"#### TensorFlow.js versionv0.12.0#### Browser versionChrome v68#### Describe the problem or feature requestTried installing posenet via npm for use in my project. When installing the latest version (0.2.0) using npm install @tensorflow-models/posenet the postinstall script fails on running the command `yarn update --pattern @tensorflow`. This gives the error:```error No lockfile in this directory. Run `yarn install` to generate one.```After running yarn install the same error occurs when a lockfile is present.Attempted deleting and reinstalling node_modules and installing posenet in a new clean project, both with the lockfile present. These both gave the same error.**The install worked fine when specifically installing posenet version 0.1.3**#### Code to reproduce the bug / link to feature request`npm install @tensorflow-models/posenet`","[""We don't have a post-install anymore, can you try this again?====="", 'Just tried a fresh install. Seems to be working.=====', 'Closing the issue as this has been resolved.Thank you =====']",0
https://github.com/tensorflow/tfjs/issues/5702,Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT,4,closed,2021-10-08T15:28:31Z,2021-10-08T18:59:50Z,"I'm having this same issue since v3.8.0, running node v15.7.0, tfjs-node v3.8.0 or v3.9.0 on Mac Big Sur.But no issue running on Raspberry Pi 3 & 4.  I'm attaching the model I'm using.  const image = fs.readFileSync(imagePath),  const decodedImage = tfnode.node.decodeImage(new Uint8Array(image), 3),  const model = await tfnode.node.loadSavedModel(modelPath),  const predictions = model.predict(decodedImage),2021-05-31 18:58:58.942711: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: saved_model2021-05-31 18:58:59.291324: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }2021-05-31 18:58:59.291368: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: saved_model2021-05-31 18:59:00.692648: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.2021-05-31 18:59:03.160455: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: saved_model2021-05-31 18:59:04.360189: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }, Status: success: OK. Took 5417477 microseconds.2021-05-31 18:59:12.511652: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?2021-05-31 18:59:12.511706: W tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at constant_op.cc:79 : Invalid argument: Cannot parse tensor from tensor_proto.2021-05-31 18:59:12.546008: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?2021-05-31 18:59:12.546526: W tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at constant_op.cc:79 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANTtensor_shape {}variant_val {  type_name: ""tensorflow::TensorList""  metadata: ""\001\000\001\377\377\377\377\377\377\377\377\377\001\030\001""}/test-saved-model/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:449        var outputMetadata = this.binding.runSavedModel(id, this.getMappedInputTensorIds(inputs, inputTensorInfos), inputTensorInfos.map(function (info) { return info.name, }).join(','), outputOpNames.join(',')),                                          ^Error: Session fail to run with error: Cannot parse tensor from proto: dtype: DT_VARIANTtensor_shape {}variant_val {  type_name: ""tensorflow::TensorList""  metadata: ""\001\000\001\377\377\377\377\377\377\377\377\377\001\030\001""}[model.zip](https://github.com/tensorflow/tfjs/files/7312434/model.zip)","['@playground This bug is introduced in TF 2.x and has been fixed in TF 2.6.0https://github.com/tensorflow/tensorflow/issues/44428I have verified your model works with tfjs-node using TF 2.6.0 binary on mac, and I will upgrade the tfjs-node to use TF 2.6.0 binary.=====', ""Hi @pyu10055  that's great, thanks for the update.  Will this be in the next release?====="", 'Yes it will be part of next release. =====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5702"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5702"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4480,Error running LinSpace,10,closed,2021-01-04T08:14:59Z,2021-04-26T00:34:09Z,"When I run **[tf.linspace](https://js.tensorflow.org/api/2.8.2/#linspace)** in the 2.8.2  API documentation.I got this ""An error occured on line: 1 Error running LinSpace: Neither modular kernel nor forward func passed"".![image](https://user-images.githubusercontent.com/422362/103514521-23658b00-4eb0-11eb-9284-8197ab47d166.png)The 2.7.0 vsersion of tf.linspace works but 2.8.0, 2.8.1, 2.8.2  are not working for now.**System information**- TensorFlow.js version: 2.8.0, 2.8.1, 2.8.2- OS Platform and Distribution (MacOS 10.15, Windows 10):","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>=====', 'Re-opening as this will need a new release to publish the fix.For context this will only happen if linspace is the first op called. As a workaround you call `tf.ready()` or `await tf.ready()` before calling any ops.=====', '@tafsiri  your workground doesn\'t work for me. When i call tf.ready() before calling any ops i have error in one of kernel func. More specifically in fromPixels.js and the error content is ""backend.makeTensorInfo is not a function""=====', '@rafallukasik123 could you post a reprodcution (maybe in codepen form) so that we can take a look.=====', 'Unfortunately I can\'t share all the code. Here is a small snipe of code https://codepen.io/rafallukasik123/pen/LYRBpVj When i remove tf.ready() from the beginning i have error similar to you ""Error running Round: Neither modular kernel nor forward func passed"".If my code contain tf.ready() i have mentioned error ""backend.makeTensorInfo is not a function"" in @tensorflow/tfjs-backend-webgl/dist/kernels/FromPixels.js=====', 'in the latest version 2.8.3 I have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. should I create a new issue?=====', ""@rafallukasik123 Yes I think a new issue would be best. In terms of reproduction, you don't have to post your whole code, ideally it would be the smallest snippet of code that shows the error. It can even just load the library and directly call tf.reciprocal or tf.browser.fromPixels. In the codepen you posted I get a syntax error so the code doesn't run.====="", 'Fixed in 2.8.3 https://js.tensorflow.org/api/2.8.3/#linspace=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>=====', ""In the latest version (3.5.0) I've seen:```Error: Kernel 'LinSpace' not registered for backend 'wasm'```Is this a related issue?=====""]",1
https://github.com/tensorflow/tfjs/issues/5378,[WebGL] incorrect computational result when use depthwise conv2d with specific options,1,closed,2021-07-26T00:26:12Z,2021-08-17T01:17:06Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All platforms- TensorFlow.js installed from (npm or script link): tf.js 3.8.0 webgl backend.- TensorFlow.js version (use command below):- Browser version: Chrome 92.0.4515.107 **Describe the current behavior**This issue is captured when running DeepLab V3 MobileNet V2 model, test depthwise conv2d with following options/input/weights, test result from webgl backend is incorrect.```  const input =tf.randomNormal([1, 65, 65, 960]),  const weights = tf.randomNormal([3, 3, 960, 1]),  const conv = tf.depthwiseConv2d(input, weights, 1, 'same', 'NHWC', 4),```**Describe the expected behavior**The computational result is same with wasm and cpu backend.**Standalone code to reproduce the issue**Here's a simple test, I just compared the first value of the output buffer, which is obviously different from result from wasm and cpu backends.CodePen: https://codepen.io/honry/pen/wvdMmva**Other info / logs**This issue is derived from https://github.com/tensorflow/tfjs/issues/5293","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5378"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5378"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/4271,[tfjs-core] Error: The output # of rows (11.4) must be an integer when running pool test,5,closed,2020-11-19T12:42:07Z,2020-11-26T07:17:40Z,"**System information**- TensorFlow.js version (use command below):""@tensorflow-core"": ""^2.7.0"" **Describe the current behavior**When running [this pool test](https://brucedai.github.io/report-bug/unset_roundingMode_param.html), error happened **Describe the expected behavior**None error, and the output # of rows is an integer (11) computed with below Python code of [AVERAGE_POOL_2D test](https://android.googlesource.com/platform/frameworks/ml/+/refs/tags/android-cts-11.0_r1/nn/runtime/test/specs/V1_0/avg_pool_float_2.mod.py) from NNAPI  CTS.```pythonrow = 52std = 5flt = 100pad = 50output_row = (row + 2 * pad - flt + std) // std #L35```**Standalone code to reproduce the issue**Here's a [test case](https://brucedai.github.io/report-bug/unset_roundingMode_param.html).**Other info / logs** Include any logs or source code that would be helpful toWhen calling function `conditionalRound`, none value (undefined) was passed to the second **roundingMode** paramter, so as the first **value** paramter being not an integer (11.4), the value wasn't rounded. ```ts/** * Rounds a value depending on the rounding mode * @param value * @param roundingMode */function conditionalRound(    value: number, roundingMode?: 'floor'|'round'|'ceil') {  if (!roundingMode) {    return value,  }  switch (roundingMode) {    case 'round':      // used for Caffe Conv      return Math.round(value),    case 'ceil':      // used for Caffe Pool      return Math.ceil(value),    case 'floor':      return Math.floor(value),    default:      throw new Error(`Unknown roundingMode ${roundingMode}`),  }}```Traced that when calling function **conv_util.computePool2DInfo** of [tfjs-core/src/ops/pool.ts#L85](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/ops/pool.ts#L85), none value (undefined) was passed to the sixth **roundingMode** parameter .```ts  const convInfo = conv_util.computePool2DInfo(      x4D.shape, windowShape, strides, dilations, pad),```According to [**computePool2DInfo** definitions](https://github.com/tensorflow/tfjs/blob/4d81b29a13cb67a0fae67833890cac326ca4d720/tfjs-core/src/ops/conv_util.ts#L119):```tsexport function computePool2DInfo(    inShape: [number, number, number, number],    filterSize: [number, number]|number, strides: number|[number, number],    dilations: number|[number, number], pad: 'same'|'valid'|number,    roundingMode?: 'floor'|'round'|'ceil',    dataFormat: 'channelsFirst'|'channelsLast' = 'channelsLast')```","['might be related issue [here](https://github.com/tensorflow/tfjs/issues/1055) =====', 'Hi @BruceDai, thank you for the bug report! Do you want to contribute a fix PR? =====', ""@lina128 OK, I'll submit a PR for it.====="", '@lina128 Please take a look this PR #4282, thanks.=====', 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4271"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4271"">No</a>=====']",1
https://github.com/tensorflow/tfjs/issues/2212,Convert Yolo9000 to Saved model then to GraphModel (Web model),3,closed,2019-10-15T16:56:10Z,2019-11-26T11:03:20Z,"To get help from the community, we encourage using Stack Overflow and the [`tensorflow.js`](https://stackoverflow.com/questions/tagged/tensorflow.js) tag.#### TensorFlow.js versionTFJS 1.2.9#### Describe the problem or feature requestI try to use the object recognition model YOLO9000 into my node.js program. For that, I use the original files for the model YOLO9000 (https://pjreddie.com/media/files/yolo9000.weights and https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolo9000.cfg) and use the following command using darkflow([https://github.com/thtrieu/darkflow](https://github.com/thtrieu/darkflow)):`flow --model ./yolo9000.cfg --load ./yolo9000.weights --savepb `It converts successfully the model to the saved model format.Now I have a saved model format, I want to convert it from `tf_saved_model` to`tfjs_graph_model` like it's written on [https://github.com/tensorflow/tfjs/tree/master/tfjs-converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter)I have the following issue when doing the convertion:```RuntimeError: MetaGraphDef associated with tags 'serve' could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`available_tags: []```The thing is that a tag and a signature seem necessary to use the converter, Is there a way to accept no tags and no signature ?#### Code to reproduce the bug / link to feature request`sudo tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model ""./YOLO9000/built_graph/""  ""./YOLO9000/jsonModel/""  `The `built_graph` folder contains th following files:```saved_model.pbyolo9000.meta```(I renamed the file `yolo9000.pb` produced by darkflow, to `saved_model.pb`)The json `jsonModel` folder is empty","[""@CharlesCousyn the files generated by darkflow are TensorFlow frozen graph format, not the saved model format.We have added frozen model support recently, you will need to wait for the next release (1.3.1) of the pip package, but the usage is as following:```$ tensorflowjs_converter \\    --input_format=tf_frozen_model \\    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\    /mobilenet/frozen_model.pb \\    /mobilenet/web_model```====="", 'Thanks!=====', 'That would be VERY useful. When will it be available?=====']",0
https://github.com/tensorflow/tfjs/issues/5634,Enhance benchmark result,0,open,2021-09-16T22:55:50Z,2021-11-10T22:57:47Z,Refactor the database schema to include failure cases as well. Revise the addResultToFirestore function to also store failure results to Firebase. https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/browserstack-benchmark/firestore.js#L73-L81,[],0
https://github.com/tensorflow/tfjs/issues/5440,[wasm]Error: Kernel 'Complex' not registered for backend 'wasm',0,open,2021-08-06T00:56:45Z,2021-08-12T16:52:55Z,"Case: ``` it('complex', async () => {    const real = tf.tensor1d([3, 30]),    const imag = tf.tensor1d([4, 40]),    const complex = tf.complex(real, imag),    const res = tf.mul(complex, complex),    expectArraysClose(await res.data(), [-7, 24, -700, 2400]),  }),```Error:```Step #23 - ""test-wasm-tfjs-backend-wasm"":   Message:Step #23 - ""test-wasm-tfjs-backend-wasm"":     Error: Kernel 'Complex' not registered for backend 'wasm'Step #23 - ""test-wasm-tfjs-backend-wasm"":   Stack:Step #23 - ""test-wasm-tfjs-backend-wasm"":     Error: Kernel 'Complex' not registered for backend 'wasm'Step #23 - ""test-wasm-tfjs-backend-wasm"":         at Engine.runKernel (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/engine.js:521:23)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at complex_ (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/complex.js:59:32)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at Object.complex__op [as complex] (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:79:33)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at /workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:125:38Step #23 - ""test-wasm-tfjs-backend-wasm"":         at step (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:47:23)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at Object.next (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:28:53)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at /workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:22:71Step #23 - ""test-wasm-tfjs-backend-wasm"":         at <Jasmine>Step #23 - ""test-wasm-tfjs-backend-wasm"":         at __awaiter (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:18:12)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at UserContext.<anonymous> (/workspace/link-package-core/node_modules/@tensorflow/tfjs-core/dist/ops/binary_ops_test.js:118:44)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at <Jasmine>Step #23 - ""test-wasm-tfjs-backend-wasm"":         at runCallback (timers.js:705:18)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at tryOnImmediate (timers.js:676:5)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at processImmediate (timers.js:658:5)Step #23 - ""test-wasm-tfjs-backend-wasm"":         at process.topLevelDomainCallback (domain.js:126:23)Step #23 - ""test-wasm-tfjs-backend-wasm"": Pending:```",[],0
https://github.com/tensorflow/tfjs/issues/4759,arraySync gets tripped up by complex numbers,5,closed,2021-02-27T09:51:20Z,2021-03-17T23:41:01Z,"**System information**Using Tensorflow JS 3.2.0 in NodeJS v15.5.0, MacOS 10.15.7 (19H512)**Describe the current behavior**Running the below code produces the error:```Error: [32,1025] does not match the input size 65600.```**Describe the expected behavior**The code should return a 2-dimensional array of size 32800.**Standalone code to reproduce the issue**```javascriptlet tf = require('@tensorflow/tfjs')let samples = new Float32Array(18048),let res = tf.signal.stft(tf.tensor(samples), 2048, 512, 2048),console.log(res.arraySync()),```**Other info / logs**Tensor info after running stft:```Tensor {  kept: false,  isDisposedInternal: false,  shape: [ 32, 1025 ],  dtype: 'complex64',  size: 32800,  strides: [ 1025 ],  dataId: { id: 3014054 },  id: 453,  rankType: '2',  scopeId: 0}```","[""Ok I think I see what's going on, it's having trouble with the real and imaginary parts of the complex number right?====="", 'Well, in my case I can solve this because I need to run `abs()` on my Tensor anyways which gets rid of the imaginary part.=====', ""Ok so, after reading a bit more I'll summarize:When complex64 was added (https://github.com/tensorflow/tfjs/issues/565), it was decided that dataSync should return both parts of the complex number. That's why the array returned by dataSync is twice as big. However, when arraySync was added this wasn't taken into account, which is why it prints an error. It's trying to fit the bigger array into the same shape.So dataSync is working correctly, but arraySync is not. A potential fix for arraySync would be to change it's shape to return an array where each element is a pair of real/imaginary parts of the complex number, to mirror the behavior of dataSync.I assume the above also applies to the non-sync'd version of arraySync.====="", 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4759"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4759"">No</a>=====', 'Thanks for the report Joonatan! =====']",1
