issue;title;comments;state;created_at;updated_at;body;comments_content
https://github.com/justadudewhohacks/face-api.js/issues/838;Unhandled error in blazor has occured;1;open;2021-12-16T09:23:29Z;2021-12-16T09:25:10Z;I managed to get real time face recognition working in visual studio code however when I moved my codes over to blazor web assembly on visual studio and after setting up JsRuntime on a razor page. When I tried to run the project. I would either get something likeUncaught (in promise) Error: Based on the provided shape, [1,1,16,32], the tensor should have 512 values but has 417orUncaught (in promise) Error: Based on the provided shape, [1,1,512,9], the tensor should have 4608 values but has 2018After troubleshooting I still have no clue why this has happened. ;['Uncaught (in promise) Error: Based on the provided shape, [1,1,16,32], the tensor should have 512 values but has 417    at P (face-api.min.js:1)    at Rn (face-api.min.js:1)    at In (face-api.min.js:1)    at e (face-api.min.js:1)    at Vh (face-api.min.js:1)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1=====']
https://github.com/justadudewhohacks/face-api.js/issues/837;vladmandic / face-api;2;open;2021-12-16T04:39:56Z;2021-12-17T02:38:03Z;Hi, I have a question! This repository https://github.com/vladmandic/face-apiand this repository https://github.com/justadudewhohacks/face-api.js/are **Same** ?? Which of them do I have to install in the project?;['Second one is original，but first one has a good compatibility for the new version`tfjs`.=====', 'More info in this issue [782](https://github.com/justadudewhohacks/face-api.js/issues/782)=====']
https://github.com/justadudewhohacks/face-api.js/issues/834;edits;3;open;2021-12-07T07:39:33Z;2021-12-07T10:59:14Z;"can we add new densblock and aslo can do edits in models like if we want to add like this { ""name"": ""dense0/conv4/pointwise_filter"", ""shape"": [1, 1, 32, 32], ""dtype"": ""float32"", ""quantization"": { ""dtype"": ""uint8"", ""scale"": 0.010322785377502442, ""min"": -1.4658355236053466 } },or this { ""name"": ""dense4/conv1/pointwise_filter"", ""shape"": [1, 1, 512, 512], ""dtype"": ""float32"", ""quantization"": { ""dtype"": ""uint8"", ""scale"": 0.010322785377502442, ""min"": -1.4658355236053466 } }, here i want to add denseblock and conv when i add its shows me error...";['Hi, Can you write the error context?=====', '> Hi, Can you write the error context?![image (1)](https://user-images.githubusercontent.com/58817326/145013932-47192053-fd0e-4527-90fc-8eda7ca25c61.png)![image](https://user-images.githubusercontent.com/58817326/145013937-a15f936b-f334-40b9-a2fb-95008d318a6c.png)=====', 'Oh, I **understood finally**. this library is using tenserflow.js and TensorFlow needs parameters for that.You are not passing enough parameters to TensorFlow. Visit this link ...https://stackoverflow.com/questions/61381112/uncaught-error-based-on-the-provided-shape-1024-3-the-tensor-should-have-30=====']
https://github.com/justadudewhohacks/face-api.js/issues/833;Best option for detecting faces;2;open;2021-12-06T10:52:23Z;2021-12-07T10:34:20Z;Hi, I want to know which option is best for detecting faces? Mtccn?Yolyo?tiny Face Detector?;['`mtcnn` is obsolete and not really working. `tiny` is well, tiny - good for extremely low powered devices. best is `SsdMobilenet`=====', 'Thank you!=====']
https://github.com/justadudewhohacks/face-api.js/issues/831;Matching faces between iOS (swift) and javascript;1;open;2021-12-04T23:43:22Z;2021-12-05T22:13:26Z;Hi, great work and thanks, this library has been an awesome easy inclusion to our solution.however, now we need to implement the same tensor models to find faces and the 68 landmarks on iOS devices using swift. To do so, we would prefer to convert the appropriate models to coreml using apples python converter.I’ve been trying to use the true converter to convert the js models back to the original models to load into python, but having a few issues. Could you provide any pointers? Do you have the original tf saved models or know where they are so we aren’t converting backwards?thanks so much again and keep up the great work!;['To be clear, we are intending to try and match people identified in a photo on one platform against people in different photos on the other platform, so we need to use the same models in both environments.Thanks again!=====']
https://github.com/justadudewhohacks/face-api.js/issues/828;Use with CPU without AVX support;3;open;2021-11-10T12:26:34Z;2021-11-11T12:20:29Z;Hello guys!How to use face-api.js with ubuntu 20.04 and CPU without AVX support for node.js? I found * .whl packages, but how to use them with face-api.js? ;['you found `whl` of what?  and this is not question for `face-api.js`, its for `tfjs`  if you get `@tensorflow/tfjs-node` working, then `face-api.js` will *just work*  however, there is no recent prepackaged version `@tensorflow/tfjs-node` available anymore that doesnt require AVX  you can try installing some ancient version (dont remember what is the latest version without AVX) or build your own (painful)=====', '@tensorflow/tfjs-node lib compiled for CPU with support AVX instructions. I have server with CPU without AVX. recommend to update tensor compiled without avx instructions. there are ready-made whl packages on the web with the assembled tensor, but for python for example, https://github.com/furas/tensorflow-no-avx etc=====', 'now that you have `tensorflow` without AVX, you need to build `@tensorflow/tfjs-node` package (it links to `tensorflow`).but that is anything but a clean process. search on tfjs git issues.=====']
https://github.com/justadudewhohacks/face-api.js/issues/826;Hangs forever and unresponsive after sometime in desktop browser;3;open;2021-10-11T06:07:25Z;2021-10-21T17:10:59Z;In laptop browser the screen hangs and doesn't go forward with face detection. I identified its getting hanged in the below line. const detections = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceExpressions(),Note: Works fine in mobile browsers;"['In mobile browsers too, when upgrade it. I think something changed in browsers in the last month.I try to find out what caused this, and found an error message in the console: ""Pass at least one tensor to tf.stack""With more debugging I found the first occurrence of this error at the ""FaceLandmark68NetBase.prototype.postProcess"" close to the line ""var landmarkTensors = output"" but I haven\'t been able to figure out how can be fix it yet.=====', 'well, this original version of `face-api.js` hasnt been updated since 04/2020 and uses quite old version of tfjs 1.7.6 internally which is based on es5 standard - chances something will break in latest browsers are not small.you can always try a newer port of `face-api`.=====', 'Thanks @vladmandic for the suggestion and awesome project.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/816;Minimizing memory and CPU consumption with Face API;1;open;2021-09-12T15:16:51Z;2021-09-13T02:32:37Z;I am using Face API to detect faces on a webcam. I call the detectSingleFace() function every 500ms. Everything works fine, but I can see that RAM and CPU usage is increasing. I realize that this is normal, finally face detection needs some resources, but I am asking if there is a way to optimize this. I am using the tiny models version. Maybe the ram consumption is related to storing the model in the browser memory, because it weighs a bit (if so, will cdn help something)?;"[""assuming you have no memory leaks in your code (check for `tf.engine().memory.numTensors` if it's growing after each frame), it's really not up to `face-api`, its up to browser to perform garbage collection  and modern browsers are lazy doing so as they value responsiveness more than anything  if youre using `webgl` backend, you can force gpu memory deallocation (which does slow things down) with `tf.ENV.set('WEBGL_DELETE_TEXTURE_THRESHOLD', 0)`  another thing is that this `face-api` is using very old `tfjs` v1.x while latest is v3.9.0 - but if memory is your biggest concern, newer tfjs definitely doesn't use less memory  =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/807;In SSD Mobilenet V1;1;open;2021-07-22T08:39:38Z;2021-07-29T13:14:01Z;how much size of images face detected & what is minimum and maximum size of images supported ( like KB, MB ) and also supported for which resolution area ( Width x Hight);['all models take image of any size and convert it internally to a square image with per-model fixed size  (if input image is not square, it will be extended with borders to achieve perfect square)  - face detection: 512px- then crop each detected face and resize it to 112px to run landmarks, expressions and descriptors- age and gender model takes same cropped face, but resizes to 150px=====']
https://github.com/justadudewhohacks/face-api.js/issues/803;FaceExpressionNet tflite;1;open;2021-06-22T06:11:16Z;2021-06-30T05:40:54Z;"HiI want to convert an emotion recognition model to tflite. But after conversion, the model works much worse. She almost always predicts ""neutral"", ""angry"" or ""happy"". The model from face api demo works much better. I created a model architecture using tensorflow.keras (tf==2.3.1). Then I loaded the weights using [decode_weights](https://github.com/tensorflow/tfjs/blob/77c4ef10fd3d8458f6958a52bac92fdaae09b885/tfjs-converter/python/tensorflowjs/read_weights.py#L126). Then saved as tf SavedModel. Then I converted to tfliteCode for creating architecture ```from tensorflow.keras import layers, Modeldef get_face_exp_net():    inputs = layers.Input((112,112,3))    out1 = layers.ReLU()(layers.Conv2D(32, 3, strides=[2,2], padding='same')(inputs))    out2 = layers.SeparableConv2D(32, 3, padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1, out2]))    out3 = layers.SeparableConv2D(32, 3, padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1, out2, out3]))    out4 = layers.SeparableConv2D(32, 3, padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1, out2, out3, out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(64, 3, strides=[2,2], padding='same')(end))    out2 = layers.SeparableConv2D(64, 3, padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1, out2]))    out3 = layers.SeparableConv2D(64, 3, padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1, out2, out3]))    out4 = layers.SeparableConv2D(64, 3, padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1, out2, out3, out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(128, 3, strides=[2,2], padding='same')(end))    out2 = layers.SeparableConv2D(128, 3, padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1, out2]))    out3 = layers.SeparableConv2D(128, 3, padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1, out2, out3]))    out4 = layers.SeparableConv2D(128, 3, padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1, out2, out3, out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(256, 3, strides=[2,2], padding='same')(end))    out2 = layers.SeparableConv2D(256, 3, padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1, out2]))    out3 = layers.SeparableConv2D(256, 3, padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1, out2, out3]))    out4 = layers.SeparableConv2D(256, 3, padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1, out2, out3, out4]))    end = layers.AvgPool2D((7,7), strides=(2,2), padding='valid')(end)    end = layers.Flatten()(end)    # end = layers.Dropout(0.5)(end)    end = layers.Dense(7)(end)    end = layers.Softmax()(end)    return Model(inputs=inputs, outputs=end)```Code for loading of weights and converting to tflite```    model_config = {'func': get_face_exp_net(),                     'name': 'face expression net',                     'manifest': 'models/tfjs_models/face_expression_model-weights_manifest.json',                     'weights': 'models/tfjs_models/face_expression_model-shard1'                     }    print('Converting of ', model_config['name'])    with open(model_config['manifest']) as json_file:        manifest = json.load(json_file)    weights_bytes = read_weights(model_config['weights'])    # [{'name':'layer_name1', 'data':np.array with weights},     # {'name':'layer_name2', 'data':np.array with weights},    # ...]    weights = tfjs.read_weights.decode_weights([manifest], [weights_bytes])[0]        model = model_config['func']    target_layers_ids = []    for i, l in enumerate(model.layers):        if 'conv' in l.name or 'dense' in l.name:            # print(i, l.name, [w.shape for w in l.get_weights()])            target_layers_ids.append(i)    # set weights to model    w_counter = 0    for layer_id in target_layers_ids:        num_w = len(model.layers[layer_id].get_weights())        w_list = [w['data'] for w in weights[w_counter:w_counter+num_w]]        model.layers[layer_id].set_weights(w_list)        w_counter+=num_w    model_name = ""_"".join(model_config['name'].split())    tf_model_path = 'models/tf_models/{}'.format(model_name)    model.save(tf_model_path)    print('TF model is saved to ', tf_model_path)    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)    tflite_model = converter.convert()    tflite_model_path = 'models/tflite_models/{}.tflite'.format(model_name)    open(tflite_model_path, ""wb"").write(tflite_model)    print('TFLite model is saved to ', tflite_model_path)```My code for inference```class FaceExpressionModel:    def __init__(self, tflite_path=os.getcwd()+'/tflite_face_api/models/tflite_models/face_expression_net.tflite',                ):        self.model = tf.lite.Interpreter(model_path=tflite_path)        self.model.allocate_tensors()        self.input_details = self.model.get_input_details()        self.output_details = self.model.get_output_details()        self.image_size = self.input_details[0]['shape'][1:3] # [112,112]    def normalize(self, img):        img[..., 0] -= 122.782        img[..., 1] -= 117.001        img[..., 2] -= 104.298        return img / 255.0    def pad_to_square(self, img):        h, w, ch = img.shape        if h==w:            return img        max_dim = max(h, w)        dim_diff = int(np.abs(h - w) * 0.5)        pad_array = np.zeros((max_dim, max_dim, ch))        if h > w:            pad_array[:, dim_diff:dim_diff+w, :] = img        else:            pad_array[dim_diff:dim_diff+h, :, :] = img        return pad_array    def preprocessing(self, img):        img = cv2.resize(img, (self.image_size)).astype(np.float32)        img = self.pad_to_square(img)        img = self.normalize(img)        return np.expand_dims(img, axis = 0)    def predict(self, img):        """"""        Does inference, preprocessing and returns probabilities for emotions        Arguments:        ----------        img : 3d np.ndarray            Single rgb image        Return:        ---------        1d np.ndarray            array of class probabilites, values from 0 to 1        """"""        img = self.preprocessing(img)        self.model.set_tensor(self.input_details[0]['index'], img)          self.model.invoke()        preds = self.model.get_tensor(self.output_details[0]['index'])[0]        return preds```";['assuming everything is correct, likely issue is loss of resolution or clipping of values - `tflite` conversion quantizes models to `uint8` which may not be enough for all interim processing results.=====']
https://github.com/justadudewhohacks/face-api.js/issues/802;Distance between landmark positions;1;open;2021-06-09T09:06:39Z;2021-06-11T16:43:02Z;Hi, I'm trying to detect the movements of a face using face-api.js, in order to detect approximately if the user is looking on the screen or not ( right, left, up, down).. And for that, i want to calculate some distances between some points and see if it increases or decreases. My probleme is that i don't really understand how the landmark work. For exemple, when i retrieve the position of a point, i get x and y coordinates. But when i display this coordinates every interval of time ( every seconde for example ) and i move left and right, i see that : 1- x coordinate doesnt change that much2- if im in the middle of the screen and i have x = 100, i move left i get x= 180, i move right i get x= 150, which is not logical in my opinion.My questions is :  How can i use the positions because i think that i'm not understanding how it works. If anyone can explain to me or give me more informations because i didn't find a lot in the documentation, it would be really helpful. ;"[""the fact that x-axis is mirrored should not be an issue - it's simply mirrored. but `face-api` 68-point 2d model is just not good enough for this application.much better choice would be more precise `face-mesh` 3d model augmented with `iris` model for detailed eye positions.take a look at this, as implemented in <https://github.com/vladmandic/human>![image](https://user-images.githubusercontent.com/57876960/121721677-83410d00-cab2-11eb-9c60-eab032144519.png)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/796;Use flv.js to load the live stream. In the video, the live broadcast was delayed due to the loading time of 3 or 4 seconds, so I added a live broadcast catch-up mechanism, but the canvas did not load after catching up;2;open;2021-05-20T08:19:37Z;2021-05-20T08:34:39Z;Use flv.js to load the live stream. In the video, the live broadcast was delayed due to the loading time of 3 or 4 seconds, so I added a live broadcast catch-up mechanism, but the canvas did not load after catching up.;"['```js <script>    let forwardTimes = []    let withFaceLandmarks = false    let withBoxes = true    function onChangeWithFaceLandmarks(e) {      withFaceLandmarks = $(e.target).prop(\'checked\')    }    function onChangeHideBoundingBoxes(e) {      withBoxes = !$(e.target).prop(\'checked\')    }    function updateTimeStats(timeInMs) {      forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)      const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length      $(\'#time\').val(`${Math.round(avgTimeInMs)} ms`)      $(\'#fps\').val(`${faceapi.utils.round(1000 / avgTimeInMs)}`)    }    async function onPlay(videoEl) {      if(!videoEl.currentTime || videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())        return setTimeout(() => onPlay(videoEl))      const options = getFaceDetectorOptions()      const ts = Date.now()      const drawBoxes = withBoxes      const drawLandmarks = withFaceLandmarks      let task = faceapi.detectAllFaces(videoEl, options)      task = withFaceLandmarks ? task.withFaceLandmarks() : task      const results = await task      updateTimeStats(Date.now() - ts)      const canvas = $(\'#overlay\').get(0)      const dims = faceapi.matchDimensions(canvas, videoEl, true)      const resizedResults = faceapi.resizeResults(results, dims)      if (drawBoxes) {        faceapi.draw.drawDetections(canvas, resizedResults)      }      if (drawLandmarks) {        faceapi.draw.drawFaceLandmarks(canvas, resizedResults)      }      setTimeout(() => onPlay(videoEl))    }    async function run() {      // load face detection and face landmark models      await changeFaceDetector(TINY_FACE_DETECTOR)      await faceapi.loadFaceLandmarkModel(\'/\')      changeInputSize(416)      // start processing frames      onPlay($(\'#inputVideo\').get(0))    }    function updateResults() {}    $(document).ready(function() {      startVideo()      renderNavBar(\'#navbar\', \'video_face_tracking\')      initFaceDetectionControls()      run()    })    function startVideo() {            var videoElement = document.getElementById(\'inputVideo\'),            var flvPlayer = flvjs.createPlayer({                type: \'flv\',                enableWorker: true,     //浏览器端开启flv.js的worker,多进程运行flv.js                isLive: true,           //直播模式                hasAudio: false,        //关闭音频                             hasVideo: true,                stashInitialSize: 128,                enableStashBuffer: false, //播放flv时，设置是否启用播放缓存，只在直播起作用。                url: \'xxx\'            }),            flvPlayer.attachMediaElement(videoElement),            flvPlayer.load(),            flvPlayer.play(),            setInterval(function () {                console.log(""时延校正判断""),                if (!flvPlayer.buffered.length) {                    return,                }                var end = flvPlayer.buffered.end(0),                var diff = end - flvPlayer.currentTime,                console.log(diff),                if (diff >= 4) {                    console.log(""进行时延校正""),                    flvPlayer.currentTime = end - 0.1,                    //alert(""时延过长，请点击时延校准""),                }            }, 3000)        }  </script>```=====', 'flv.js address  https://github.com/Bilibili/flv.js/=====']"
https://github.com/justadudewhohacks/face-api.js/issues/793;Unhandled Rejection (Error): Kernel 'Identity' not registered for backend 'webgl';1;open;2021-05-10T18:48:59Z;2021-06-17T18:42:44Z;![image](https://user-images.githubusercontent.com/42316496/117709333-6cc33f80-b1ee-11eb-967c-7139bf02a17e.png);['Same for me : (=====']
https://github.com/justadudewhohacks/face-api.js/issues/791;How to use face-api.js in Nodejs?;2;open;2021-05-09T16:26:44Z;2021-12-06T10:47:08Z;I have the following code in the browser:```js  async function compareFaces(face1, face2) {    const detection1 = await faceapi.detectSingleFace(face1).withFaceLandmarks().withFaceDescriptor()    const detection2 = await faceapi.detectSingleFace(face2).withFaceLandmarks().withFaceDescriptor()    return 1 - faceapi.euclideanDistance(detection1.descriptor, detection2.descriptor)  }  async function main() {    console.log('loading models...')    await faceapi.nets.ssdMobilenetv1.loadFromUri('/models')    await faceapi.nets.faceLandmark68Net.loadFromUri('/models')    await faceapi.nets.faceRecognitionNet.loadFromUri('/models')    console.log('loaded models')    const input1 = document.getElementById('myImg1') // get image from disk instead    const input2 = document.getElementById('myImg2') // get image from disk instead    console.log(await compareFaces(input1, input2))  }  main().catch(e => console.log(e))```How would I use this code in NodeJs and read the images from the disk instead of DOM elements?;['example using canvas: <https://github.com/vladmandic/face-api/blob/master/demo/node-canvas.js>  example using tfjs native functions: <https://github.com/vladmandic/face-api/blob/master/demo/node-image.js>=====', 'Download the face-api.js master and see node.js Documentation.=====']
https://github.com/justadudewhohacks/face-api.js/issues/789;create NetInput from base64 string;1;open;2021-05-04T07:57:45Z;2021-05-04T13:58:41Z;"Hi, I'm trying do create a webservice to do face matching, so I'm writting code with textscript and express, I can get the picture in base64 but I'm trying to do a face matching but in the middle I don't know how to create a NetInput from base 64 string.I tryied to create an Image but when I run I get this error: (node:10663) UnhandledPromiseRejectionWarning: Error: Image given has not completed loadingthis is my code:       `const imgRef = faceapi.env.getEnv().createImageElement(),         imgRef.src = 'image/jpeg,base64,'+rostros.referencia,         imgRef.width = 393,         imgRef.height = 574,         console.log(""imgRef complete""),         console.log(imgRef.complete),         console.log(""about detectAllFaces""),        const results = await faceapi.detectAllFaces(imgRef)            .withFaceLandmarks().withFaceDescriptors(),`";"[""it's exactly what it says 'image has not completed loading' - assigning `imgRef.src` is ok, but then execute actual detection inside event that triggers when image is loaded, something like `imgRef.onload = () => faceapi.detectAllFaces(...)`=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/787;Can I use tensor as input in my browser?;1;open;2021-04-27T09:27:19Z;2021-04-28T13:18:12Z;Can I use tensor as input in my browser? I'm using offScreenCanvas as the input to detect singleface in another thread. But the browser I need to use doesn't support offScreenCanvas. So I want to use typearray that passes in the image directly;"[""you can't pass a tensor as reference to a web worker as it's not a static type, but you can with `ImageData` and then construct a tensor inside worker from that. In your case, if you already have a typed array, pass that and again just construct tensor inside worker.be careful regarding RGBA vs RGB as ImageData is typically RGBA and TFJS works with RGB inputs, so you might need to strip alpha channel unless you've already done that in your typed array.Just a quick example:```js  const tensor = tf.tidy(() => { // create tensor from image data    const data = tf.tensor(Array.from(imageData.data), [imageData.height, imageData.width, 4], 'int32'), // create rgba image tensor from flat array and flip to height x width    const channels = tf.split(data, 4, 2), // split rgba to channels    const rgb = tf.stack([channels[0], channels[1], channels[2]], 2), // stack channels back to rgb    const reshape = tf.reshape(rgb, [1, imageData.height, imageData.width, 3]), // move extra dim from the end of tensor and use it as batch number instead    return reshape,  }),```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/786;Face api is incompatible with new versions of tensorflow.js 2.x, 3.x;9;open;2021-04-26T10:23:10Z;2021-04-27T14:03:22Z;face-api.js requires 1.x version and it fails with newer versions like 2.x, 3.x and gives type errors because of the compatibility issues;"[""`faceapi` has not been updated for a long time, so that's main reason why this port exists:  <https://github.com/vladmandic/face-api>====="", ""Recent posts on TensorFlow's blog make me think maybe there is another model that could be loaded up and used for fast results and more detailed info. https://blog.tensorflow.org/2020/11/iris-landmark-tracking-in-browser-with-MediaPipe-and-TensorFlowJS.html====="", ""Oh, also, this is happening https://www.chromestatus.com/feature/5678216012365824And this https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.htmlAnd most importantly, this happened https://blog.tensorflow.org/2020/04/tensorflow-lite-core-ml-delegate-faster-inference-iphones-ipads.htmlwhich will eventually manifest as this https://webmachinelearning.github.io/webnn/and then we shouldn't have to worry about performance anymore. ====="", ""@jeffreytgilbert > Recent posts on TensorFlow's blog make me think maybe there is another model that could be loaded up and used for fast results and more detailed info.I've been working with those models (and few others) for a while now, check out: <https://github.com/vladmandic/human>> Oh, also, this is happening https://www.chromestatus.com/feature/5678216012365824Is great if all you need is to be able to authenticate user - that's what it's for, nothing else.> And this https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.htmlNot sure how that relates? It's a great high-level API for learning, but any SOTA object detection model doesn't rely on OD API.> And most importantly, this happened https://blog.tensorflow.org/2020/04/tensorflow-lite-core-ml-delegate-faster-inference-iphones-ipads.htmlA way to natively use Apple's proprietary NPU - great performance speedup on Apple's new silicone.> which will eventually manifest as this https://webmachinelearning.github.io/webnn/One more GPU/TPU/NPU accelerated backend? Just because it's committee driven, doesn't make it better than existing ones. E.g, `tfjs-node-gpu` + `cuda` already do that job if you have nVidia GPU.I'm far more interested in development and adoption of WebGPU (to be used as a next-gen WebGL).====="", ""WebGPU is great, but that shouldn't be the only way Apple and everyone access those NPUs. My focus has been on the performance improvements using efficient non-blocking APIs. Face/object detection in the browser APIs is not feature rich, however with web standards, they start small and get better from there. That's why I'm following that spec and WebNN/OpenVino/NNAPI/etc. They aren't drop in replacements for anything here, but might be used for simple tasks in a non-blocking way. You might be thinking, but currently this code isn't blocking! Fair, but 2 years ago when i was using it i hit a GPU bottleneck that blocked main ui rendering because the integrated graphics chip couldn't do both and it's stuck with me ever since. ====="", 'Agree, `WebGPU` use-case is in-browser, not catch-all.But regarding WebNN, etc. - I don\'t see how a committee driven backend designed for high-level usage can accomplish much other than increase entry-level adoption of ML (great for quick learning). All really good models require a lot more low-level ops and that is not the goal of WebNN. E.g., I don\'t care about pre-packaged ""person detection"", I want to be able to use best available model.Also, most money in ML research is coming from corp sponsors that develop their own backends (e.g., FB has PyTorch, Google has Tensorflow, Alibaba has MNN, Apple has CoreML, Tenecent has NCNN, nVidia has TensorRT, etc.) - and I need to be able to run models developed in their native frameworks.Notice that Apache foundation and OpenAI are pushing for MXNet adoption, but it really hasn\'t picked up so far.Now, if I can easily convert those models to ONNX (as ""universal"" model format) and then run them using WebNN - great. But I see that as always being couple of steps behind cutting edge as new ops need to be implemented downstream. =====', ""You're more versed in this than I am. My last touchpoint was 2019. That's not how I understood WebNN's approach as documented in their workflow hierarchy graphics and spec documentation, but I'm fully ok with being way off in my interpretation. Here, it's mentioned what the roles are:https://webmachinelearning.github.io/webnn/#programming-modelSo my understanding is that WebNN would be a common interface for loading, queuing and executing work against low level system APIs available to the native code in the browser and that the common interface would not look to be a replacement for WebGPU, WebGL, etc, but instead a standard way to access them and get async results back from work that may block a thread. There would still be a javascript framework (Tensorflow for example) which decided which models to load and which of the available APIs to choose, similar to how Canvas allows you to choose a 2D or 3D context for instance, but the dev/framework is still responsible for implementing compatible instructions. I'm not deep enough into it to have any insight into what you mentioned regarding apache and ONNX, but that sounds like it might be cool. I wonder if Kronos or OASIS standards workgroups will pick something like that up so it ends up working like Vulcan does in replacing OpenGL with a universal API for common low level GPU tasks. I'll have to read up on it more when I'm done with my backlog of other reading materials. ,)====="", ""PS, happy you're involved and posting on these tickets! Can't make magic happen without magicians! ====="", 'I definitely don\'t see a scenario where TFJS loads a model and then passes it to WebNN for execution - why would devs maintain TFJS then? Just as a loader?TFJS would not run on top of WebNN - both of them are frameworks that implement low level functions using some CPU/GPU/TPU/NPU acceleration. And in case of browser, how do you get to underlying hardware? Via WebGL or (in the future) WebGPU (which then internally use something like OpenGL or Vulcan or D3D - that is up to browser implementation).But...WebNN cannot have a parser and loader for every model format that comes from different framework, they have to pick one. I hope they pick ONNX as that is supposed to be ""universal"" format.I can see how someone uses Tensorflow to develop a model, then converts it to ONNX and runs it using WebNN - so role of Tensorflow to develop models stays, but role of TFJS to run models at the edge diminishes *if* (and that\'s a big if) WebNN picks up.But given it\'s several steps from original model, I see it as always being several steps behind.Still, PyTorch (for example) doesn\'t have browser-native solution. Actually, none of the other frameworks do, only TF does in form of TFJS (that\'s the reason why I choose to start with TF/TFJS when I started with ML 6 months ago). So WebNN may pick up given the need to run all those other models in browser.Anyhow, I get the goals of WebNN, but if underlying interface to HW is flaky as WebGL is, they can talk about non-blocking as much as they want, but they need better way to interface with HW. And *if* WebGPU solves that (and uses Vulcan in the backend), it will solve it for TFJS as well.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/780;i want to load the mtcnn json and shard file from cdn but its not working!! Need help;3;open;2021-04-15T06:34:28Z;2021-04-16T11:23:57Z;;['`mtcnn` model has been removed a looong time ago, any reference to it is pretty much invalid  but if you really want it, you can download it from `https://github.com/justadudewhohacks/face-api.js-models`  (its not on any *cdn*)  =====', 'No actually What i wanted is, we are dockerising eveything and we dont want any static files there so we moved all the static files to cdn but my question is can put mtcnn model on our own cdn and serve the model or not to js?=====', 'of course - no issues.=====']
https://github.com/justadudewhohacks/face-api.js/issues/776;error api face in Motorola E5;8;open;2021-04-07T21:34:09Z;2021-04-13T02:51:07Z;![MicrosoftTeams-image (1)](https://user-images.githubusercontent.com/24438895/113937590-65f87580-97c7-11eb-9c2a-20360c0a5c74.png);"[""Insufficient GPU memory to compile GL shaders.  In general, if you don't have a dedicated GPU, use of `WebGL` backend is not recommended in any machine learning project - Use `WASM` backend instead.====="", 'how can I use backend wasm in this case?=====', ""after `tfjs` has been loaded, but before `faceapi` has been used:```jsawait tf.setWasmPaths('./path-to-wasm-files/'),await tf.setBackend('wasm'),```and you do need to provide *.wasm files (part of `tfjs` package itself), typically in `node_modules/@tensorflow/tfjs-backend-wasm/dist` or download from any CDN  however, in this case quite old as `faceapi` uses `tfjs` 1.7.0 and `wasm` version must match  or use a newer port of `faceapi`: <https://github.com/vladmandic/face-api> that's compatible with tfjs 2.x and 3.x  also, recommended to enable `SIMD` as performance is higher by order of magnitude  for example, go to <chrome://flags> and enable `WebAssembly SIMD support`====="", 'I\'m usingimport * as faceapi from ""components / lib / face-api.esm"",await faceapi.tf.setWasmPaths (""../ statics /""),      await faceapi.tf.setBackend (""wasm""),but the compilation is very slow, because the file weighs 3MB.I am using this face-api.esm because it was the one that solved the problem of Insufficient GPU memory to compile GL shaders.In general, if you don\'t have a dedicated GPU, use of WebGL backend is not recommended in any machine learning project - Use WASM backend instead.What other option do I have?=====', ""@deylyn if you're using `https://github.com/vladmandic/face-api`, please post issues there  also, size of js file has little to do with execution speed - but yes, it can be minimized to around 1.3mb if that means anything    since your device is really low on memory, make sure you're using `tinyFaceDetector` model, not `ssdMobilenetv1` - it's slightly less accurate, but far less memory demanding. notes on how to use it are in the docs.====="", 'if i am using tinyFaceDetectorimport * as faceapi from ""components/lib/face-api.esm"",  var backendWasm = await this.backendWasm(),    Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri(""/statics/models"")]),this.video.addEventListener(        ""play"",        function() {          self.canvasFace = document.getElementById(""c1""),          self.displaySize = {            width: self.videoWidth,            height: self.videoHeight          },          faceapi.matchDimensions(self.canvasFace, self.displaySize),          self.timerCallback(),        },        false      ), timerCallback: function() {      if (this.video.paused || this.video.ended) {        return,      }      const box = { x: 75, y: 37.5, width: 150, height: 225 },      var lenghtx = box.x + box.width,      var lenghty = box.y + box.height,      // see DrawBoxOptions below      const drawOptions = {        lineWidth: 2,        boxColor: ""#23b2be""      },      const drawBox = new faceapi.draw.DrawBox(box, drawOptions),      drawBox.draw(this.canvasFace),      let inputSize = 128,      let scoreThresholds = 0.1,      this.id = setInterval(async () => {        let self = this,        const useTinyModel = true,        const detections = await faceapi.detectAllFaces(          self.video,          new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThresholds })        ),        // resize the detected boxes in case your displayed image has a different size than the original        const resizedDetections = faceapi.resizeResults(          detections,          self.displaySize        ),        let boxArea = 35000,        if (self.$q.screen.xs) {          boxArea = 35000,        } else {          boxArea = 21000,        }        if (resizedDetections[0]) {          let anchBoundBox = resizedDetections[0].box[""width""] - 15,          let xBox = box.x - 35,          let altBoundBox = resizedDetections[0].box[""height""] - 15,          let yBox = box.y + 45,          if (            anchBoundBox + resizedDetections[0].box[""x""] > lenghtx ||            resizedDetections[0].box[""x""] < xBox ||            resizedDetections[0].box[""y""] < yBox ||            altBoundBox + resizedDetections[0].box[""y""] > lenghty          ) {            self.validCentered = false,            self.textAlert = ""Centre su rostro"",            self.paintBoxAlert(self.textAlert),            self.$emit(""detect-centered"", false),          } else {            self.validCentered = true,            self.$emit(""detect-centered"", true),          }          if (resizedDetections[0].box.area > boxArea) {            self.validProximity = true,            self.$emit(""detect-proximity"", true),          } else {            self.textAlert = ""Acerque su rostro"",            self.paintBoxAlert(self.textAlert),            self.validProximity = false,            self.$emit(""detect-proximity"", false),          }        } else {          self.validProximity = false,          self.validCentered = false,        }        if (self.validCentered && self.validProximity) {          self.canvasFace            .getContext(""2d"")            .clearRect(0, 0, self.canvasFace.width, 36),        }           }, 250),    }=====', ' async backendWasm() {      await faceapi.tf.setWasmPaths(""../statics/""),      await faceapi.tf.setBackend(""wasm""),   },=====', ""@deylyn like I said - if you're using https://github.com/vladmandic/face-api, please post issues therebut there are several major issues here:- you're creating new instance of model on each frame: `new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThresholds })`    create it outside of the loop as a variable- your loop is a 250ms fixed loop which is a great way to create race conditions    don't use `setInterval`, refactor the code to call itself upon completion using `requestAnimationFrame`- reduce `scoreThreshold` to a reasonable value  low values mean that library has to do far more processing for all possible false-positives=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/775;# dynamically changing faceMatcher for bulk photo loading;3;open;2021-04-05T20:36:40Z;2021-04-06T15:44:32Z;I'm creating a web(react) app which **bulk loads a few tens of pictures, and then identifies and recognizes all faces together** - and sort them to albums by label\person name.because of the obvious performance problem with bulk face recognition, I'm struggling to find the most efficient way for that -* should I just compare the current image descriptor to all other (saved) labeled descriptors?* or maybe is it possible to dynamically add descriptors to a big faceMatcher which will just work on every incoming picture and classify it? will it even be worth the trouble(performance-wise)? * any other offers will be appreciated.  ;"[""if you want to be as efficient as possible, run math as fast as possible, don't look how to wrap it in nice functions that do the same thing but with additional overhead.```jsfunction distance(embedding1, embedding2, order = 2) {  if (!embedding1 || !embedding2) return 1,  if (embedding1?.length === 0 || embedding2?.length === 0) return 1,  if (embedding1?.length !== embedding2?.length) return 1,  // general minkowski distance, euclidean distance is limited case (default) where order is 2  return embedding1    .map((val, i) => (Math.abs(embedding1[i] - embedding2[i]) ** order)) // distance squared    .reduce((sum, now) => (sum + now), 0) // sum all distances    ** (1 / order), // get root of}```====="", ""@vladmandic true. the next level is probably to understand how the faceMatcher work from the inside, I'm just not there yet, do to my lack of experiance\\knowledge. from the little I can understand of your function, is the faceMatcher probability calculated by the average of euclidean distances between 68 face points and their counterparts from theother descriptor? could you please reference me to a some specific topics, so I can understand it better?thank you for your time!====="", ""face descriptor or otherwise called face embedding is basically a tensor feature vector  calculated on a cropped image of a face with weights trained on several versions of faces at various angles, etc.  it's just a numerical array that describes entire face image, not related to detected face points at all  and the length of the array is how precise it is - here it's a relatively short vector- <https://www.tensorflow.org/hub/common_signatures/images>- <https://medium.com/looka-engineering/how-to-visualize-feature-vectors-with-sprites-and-tensorflows-tensorboard-3950ca1fb2c7>=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/774;can we use faceapi js using cdn?;1;open;2021-04-05T11:24:55Z;2021-04-06T01:00:03Z;;"['```html<script src=""https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/dist/face-api.js""></script>```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/771;Difference between running the algorithm on browser and nodeJS;6;open;2021-03-18T10:57:24Z;2021-04-26T18:38:53Z;"Node Version: 14.16.0Face-api.js : master clonedChrome version :  89.0.4389.90 (Official Build) (64-bit)I was running tests to see the difference between how the algorithm performs on the two platforms. For some reason, nodeJS is able to find images in the same image in which browser fails to find. An example of this would be : `George_W_Bush_0137.jpg` from the Labelled faces in the wild dataset. Shouldn't the encoding algorithm be platform agnostic. Why exactly does this happen?Standalone code to encode on browser:```<!DOCTYPE html><html><head>  <script src=""../../../dist/face-api.js""></script>  <script src=""../public/js/commons.js""></script>  <script src=""../public/js/faceDetectionControls.js""></script>  <!-- <link rel=""stylesheet"" href=""styles.css""> -->  <!-- <link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css""> -->  <script type=""text/javascript"" src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>  <!-- <script src=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js""></script> --></head><body>  <input type=""file"" id=""file"" name=""file""/><br><br>    <button id = ""run"" onclick=""run()"">Run</button><br><br>  <script>async function run(){    await faceapi.loadSsdMobilenetv1Model('../../../weights')    await faceapi.loadFaceLandmarkModel('../../../weights')    await faceapi.loadFaceRecognitionModel('../../../weights')    const inputElement = document.getElementById(""file""),    console.log(inputElement.files[0]),    const img = await faceapi.bufferToImage(inputElement.files[0]),    const encoding = await calculateEncoding(img),    console.log(encoding),}async function calculateEncoding(img){      //We want to detect the best face in each file      const result = await faceapi.detectSingleFace(img, getFaceDetectorOptions())      .withFaceLandmarks()      .withFaceDescriptor()      //only if face found      if(result != undefined)          return result.descriptor,      else         return undefined,}  </script></body></html>```NodeJS:```//encodes one image, path of which is suppliedconst fsPromises = require('fs/promises'),const path = require('path'),const process = require('process'),const faceapi = require('face-api.js')const { canvas, faceDetectionNet, faceDetectionOptions } = require('./commons'),async function run(){    //load models    await faceDetectionNet.loadFromDisk('../../weights')    await faceapi.nets.faceLandmark68Net.loadFromDisk('../../weights')    await faceapi.nets.faceRecognitionNet.loadFromDisk('../../weights')    const encoding = await calculateEncoding(process.argv[2]),    console.log(encoding),}async function calculateEncoding(imagePath){    const img = await canvas.loadImage(imagePath),        //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img, faceDetectionOptions)    .withFaceLandmarks()    .withFaceDescriptor()    //only if face found    if(result != undefined)        return result.descriptor,    else         return undefined,}run(),        ```";"[""what are actual detect options? in browser case, you're calling undocumented function `getFaceDetectorOptions` and in node case, you're importing from `commons` - neither is known.also, why the difference in loading (plus mix&match methods)? use `faceapi.nets.ssdMobilenetv1.load` and `faceapi.nets.ssdMobilenetv1.loadFromDisk`====="", ""I've been using the examples provided. They point to the same code, but to make sure that wasn't the reason, here are the updated partsnodeJS:```    await faceapi.nets.ssdMobilenetv1.loadFromDisk('../../weights')    await faceapi.nets.faceLandmark68Net.loadFromDisk('../../weights')    await faceapi.nets.faceRecognitionNet.loadFromDisk('../../weights')-----    //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img, new faceapi.SsdMobilenetv1Options({ minConfidence }))    .withFaceLandmarks()    .withFaceDescriptor()```Browser:```    await faceapi.nets.ssdMobilenetv1.load('../../../weights')    await faceapi.nets.faceLandmark68Net.load('../../../weights')    await faceapi.nets.faceRecognitionNet.load('../../../weights')        //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img, new faceapi.SsdMobilenetv1Options({ minConfidence }))    .withFaceLandmarks()    .withFaceDescriptor()```The results are still the same. ====="", 'This made me really curious...I wrote a quick test and moved image loading and conversion to tensor out of FaceAPI to eliminate that part  And yes, `Browser` and `NodeJS` return different values  But in `Browser`, both `WebGL` and `WASM` return same values  So it\'s not a image loading and not backend specific  And it\'s not a different model as FaceAPI makes no distinction when running inference on browser vs node  So what\'s different? This is just a guess, but `tf.image.resizeBilinear` has different defaults in TFJS and TF  For example, property `alignCorners` is not specified by FaceAPI, so it\'s interpreted as `false` in TFJS and `true` in TF  (and there are probably more instances of similar cases)  It makes sense as when you look at result outputs, face bounding box is slightly shifted (by about half a pixel)And that\'s enough to create a small cascading differences in descriptor and elsewhereNow, if there is an actual reason to ""fix"" this,  I can do that in my newer fork <https://github.com/vladmandic/face-api>  Otherwise, this will stay as curiosity  =====', 'Yes, has me baffled for a couple of days. I tried that. The flow doesn\'t even go in the conditional loop for the example image mentioned.```          if (imgTensor.shape[1] !== inputSize || imgTensor.shape[2] !== inputSize) {            imgTensor = tf.image.resizeBilinear(imgTensor, [inputSize, inputSize], true),            console.log(""resizeBilinear alignCorners arg changed to true""),          } ```  But do check it on your end as I\'m not familiar with the codebase and there might be something that I\'m missing.   Don\'t you think fixing this or at least finding the root cause would be beneficial as it looks like the browser is underperforming. However, I\'ve only found two instances (yet) so concluding that browser always underperforms might be naive. Yes, I did look at your fork. Kudos on the work!. Should I open an issue in your fork, so the problem can be resolved there.    =====', ""yes, i was wrong. but at least i managed to find it at the end :)seems its an actual bug in implementation of `tf.conv2d`  actual function in FaceAPI is `pointwiseConvLayer` which is literary first operation performed in `mobileNetV1` model implementation - so this tiny change cascades down to eventually create a difference you're seeing.see <https://github.com/tensorflow/tfjs/issues/4843> for details  there is one more difference between browser and nodejs and that's the initial jpg decoding which shifts pixel values by one (not coordinate offset, but rgb values itself). so simply adding `tf.add(input, 1)` to nodejs decode workflow solves it.====="", '@mayankagarwal-cf just closing the loop...it seems that issue was `conv2d` implementation in old **TF1** binaries which were used by `tfjs-node`  `tfjs-node` **3.5.0** finally ships with TF2 binaries and this issue is resolved  =====']"
https://github.com/justadudewhohacks/face-api.js/issues/766;What's the accuracy of detecting the facial expression(emotion);3;open;2021-03-11T10:59:52Z;2021-12-12T09:03:34Z;"I had explored so many where and I still can't find a word about the accuracy of the Face Expression Recognition Model.It only told me that ""The face expression recognition model is lightweight, fast and provides reasonable accuracy.""I want to know and use it for my thesis writing because I used it in my school project.Thanks";"['Anybody can help=====', ""Also interested in! I didn't find any information neither====="", 'is there any update about accuracy information? =====']"
https://github.com/justadudewhohacks/face-api.js/issues/764;Low Performance on Safari on iOS;3;open;2021-03-04T17:26:37Z;2021-03-05T15:21:42Z;Hello!My web page is using the face-api.js api to find the face expressions of human faces on mobile. On android all works fine, the video of the webcam is smooth, on Google Chrome and Firefox. On iOS using Chrome the video is smooth but on Safari is very laggy: it shows 2-3 frames and then stops and then other 2-3 frames and then stops and so on. Since we cannot use Chrome on iOS due to other problems, I wanna know if someone else has encountered the same problem and how did he solve it.Thank you all for the support.;"[""Safari is lacking full support for GL version 2, so if you're using `webgl` backend, it will be lacking.  Check if `wasm` backend is an option for you, although I don't know what's the state of SIMD support in Safari.====="", ""I've managed to solve the issue reducing the inputSize from 512x512 to 128x128. Anyway, I will go to inform about it!Thanks @vladmandic ====="", '@lucaTriboli I have the same issue, face-api use GPU hardware, so I try to install a graphic card, and work fine. Check the system resource monitor when the app is running.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/757;how to detected the actural physical dimensions ( or the approximate value) of face;1;open;2021-02-03T01:07:04Z;2021-02-04T18:37:23Z;eg. get the width of face in cm;"[""just with this model, i don't think anything would make sense as you don't have anything to compare with and have no idea how far is the face from the camera or whats the field of view.what comes to mind is to use object with known size as anchor and then do simple math to determine size of the face.for example, human iris is quite stable and `iris` model (which augments `facemesh` model, not `face-api`) is not bad.so running `facemesh` model augmented with `iris` model would give size of iris which can be used to infer distance which then can be inferred to calculate scale of the face itself.if you want to check it out, i've done some work on that to combine different models in <https://github.com/vladmandic/human>, i might add this as it seems like an interesting problem to solve.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/756; ts-node faceDetection.js ------- Error: Cannot find module;8;open;2021-01-28T11:45:21Z;2021-02-03T14:26:13Z;"HellloI am following the instructions in the link :""https://github.com/justadudewhohacks/face-api.js"".When I run the command  ""ts-node faceDetection.js"", it throws the following error:Error: Cannot find module 'D:\TECHEXP\face\facerec-js-git\face-api.js\examples\examples-nodejs\node_modules\face-api.js\build\commonjs\index.js'. Please verify that the package.json has a valid ""main"" entryI can't seem to find out the root cause. Any directions?Thank YouDeepak";"['HelloWhen I run the command ""tsc faceDetection.ts""I get the following error:tsc faceDetection.tscommons/env.ts:5:26 - error TS2307: Cannot find module \'face-api.js\' or its corresponding type declarations.5 import * as faceapi from \'face-api.js\',                           ~~~~~~~~~~~~~commons/faceDetection.ts:1:26 - error TS2307: Cannot find module \'face-api.js\' or its corresponding type declarations.=====', 'i also have the same error=====', 'You can find the solution to that problem here: https://github.com/justadudewhohacks/face-api.js/issues/491#issuecomment-569703137. Happy hacking :)=====', ""can't build getting  face-api.js@0.22.2 build rm -rf ./build && rm -rf ./dist && npm run rollup && npm run rollup-min && npm run tsc && npm run tsc-es6'rm' is not recognized as an internal or external command,operable program or batch file.npm ERR! code 1npm ERR! path D:\\Programming\\face-api.js-2ndnpm ERR! command failednpm ERR! command C:\\WINDOWS\\system32\\cmd.exe /d /s /c rm -rf ./build && rm -rf ./dist && npm run rollup && npm run rollup-min && npm run tsc && npm run tsc-es6====="", 'Everything mostly worked for me (I do get a simple error in the command ""tsc FaceDetection.ts"", but that doesn\'t stop me from running the next command: ""node FaceDetection.js""). @marksamfd  : Looks like you are running on Windows. ""rm"" is typically an Unix command=====', 'yes what can I do??=====', '> yes what can I do??Can you ignore this error and run the next commands. Does that work?=====', '> > yes what can I do??> > Can you ignore this error and run the next commands. Does that work?yah worked thanks a lot=====']"
https://github.com/justadudewhohacks/face-api.js/issues/755;How many faces FaceMatcher supports with good performance;3;open;2021-01-26T15:15:38Z;2021-02-15T01:59:45Z;Hello @justadudewhohacksHow many faces can I put on FaceMatcher and still have good performance and accuracy greater than 90%?;"[""I'm going to redo the question in a different way: Does FaceDetector have a limit on the number of faces?====="", 'with images or with video, multiple or singular sources? I have two video feeds at a time and it does pretty well for the first 10 minutes=====', 'Tanks cindyloo.Our application runs with images frames not videos. Our question is: how many images the FaceMatcher suports? There are a limit, for good performance and acurracy?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/754;Using Google Coral to speed things up?;1;open;2021-01-24T09:12:02Z;2021-01-25T00:31:03Z;Hello,Just got Google Coral USB dongle. It detects persons but not individual faces that can be connected with name.Anyone here tried using face-api.js with Google Coral and have succeeded?Thank you.Best regards,Andrej;['Google Coral supports only TensorFlow Lite models that are fully 8-bit quantized and then compiled specifically for the Edge TPU.  AFAIK, there is no support for saved (used by standard TF) or graph/layers models (used by TFJS) - as such, any project (including `face-api`) that uses TFJS will not work on Google Coral.=====']
https://github.com/justadudewhohacks/face-api.js/issues/748;How do you use just tiny model?;1;open;2021-01-15T20:20:01Z;2021-01-17T13:01:46Z;"I have a Vue.js app and I'd like to use just the tiny model. I have this code:```<template>    <div>        <img id=""my-img"" src=""img/some-image.jpg"" />    </div></template><script>    import * as faceapi from 'face-api.js',    export default {        async mounted() {            await faceapi.nets.tinyFaceDetector.loadFromUri('/models'),            const input = document.getElementById('my-img'),            const detections = await faceapi.detectAllFaces(input),            console.log(detections),        }        }</script>```but I'm getting this error:```SsdMobilenetv1.js?fbbc:22 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (SsdMobilenetv1.js?fbbc:22)    at SsdMobilenetv1.eval (SsdMobilenetv1.js?fbbc:56)    at step (tslib.es6.js?9ab4:100)    at Object.eval [as next] (tslib.es6.js?9ab4:81)    at fulfilled (tslib.es6.js?9ab4:71)```which indicates that it's trying to use the `ssdMobilenetv1` when calling `detectAllFaces()`. If I change the code to ```await faceapi.ssdMobilenetv1('/models')const input = document.getElementById('my-img'),const detections = await faceapi.detectAllFaces(input),```it works but that's not what I want. How do I load just the tiny face detection model?";"[""you need to specify you want to use tiny, not just load it (if you don't pass options to detect method, it defaults to ssd):```jsconst options = new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold }),const data = await faceapi.detectAllFaces(input, options)```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/745;Unable to run nodejs example (module not found);1;open;2021-01-08T21:09:41Z;2021-04-10T02:48:57Z;Hi, I tried to build and run the nodejs example following the instructions in Readme.MD, specifically:```cd face-api.js/examples/examples-nodejsnpm itsc faceDetection.tsnode faceDetection.js```It builds successfully (the 'tsc' command has been executed) but it doesn't run and returns an error on 'node' command.At first it also gave me some errors on build (related to some dependencies' prerequisites) and I somehow fixed them,what am I still missing?The error message is:```C:\Users\User\Desktop\demo1\face-api.js\examples\examples-nodejs>node faceDetection.jsinternal/modules/cjs/loader.js:1122  return process.dlopen(module, path.toNamespacedPath(filename)),                 ^Error: Cannot find the specified module.\\?\C:\Users\User\Desktop\demo1\face-api.js\node_modules\@tensorflow\tfjs-node\lib\napi-v5\tfjs_binding.node    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1122:18)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)    at Object.<anonymous> (C:\Users\User\Desktop\demo1\face-api.js\node_modules\@tensorflow\tfjs-node\dist\index.js:46:16)    at Module._compile (internal/modules/cjs/loader.js:1063:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)```Thanks to everyone who can help me fix this;['Solution: move D:\\TFJS\\node_modules\\@tensorflow\\tfjs-node\\deps\\lib\\tensorflow.dll to D:\\TFJS\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v6\\*ps napi-v6 could be v5 or else depending on version=====']
https://github.com/justadudewhohacks/face-api.js/issues/743;MTCNN Face Detector demo won't work;1;open;2021-01-04T07:58:40Z;2021-01-05T21:59:20Z;The link address: https://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740The moment the MTCNN model is loaded, the console prints: 'mtcnn is deprecated and will be removed soon', May I ask why it is overdue?The following two lines of code run error: faceapi.drawDetection('overlay', mtcnnResults.map(res => res.faceDetection), { withScore: false })faceapi.drawLandmarks('overlay', mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: 'red' })Can examples in the repository provide clear examples of MTCNN?Looking forward to reply~;"[""`mtcnn` and `tinyYolov2` models are quite old and not included in latest `face-api.js` for a while now (it's possible to make them work, but why bother?).  models that are included are `mobileNetv1` and `tinyFaceDetector`.  =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/742;Why is my app so fast with iPhone and slow with MacOS?;1;open;2020-12-30T15:40:04Z;2021-01-05T22:06:32Z;Hias many other users, I see that the very first recognition is successfully done after around ten seconds from the models loading. I have the most common scenario: a canvas where images are saved from the webcam stream. This actually happens with Chrome 87 on a MacBook Air 2019 with Big Sur installed.If I try the same webpage with my iPhone SE 2020 the first recognition happens in less than one second probably, with Safari.I would have expected the opposite behavior. Is there any reason why iPhone is so faster than Mac? I don't think it is a GPU nor a WebGL stuff... Thanks!;"[""you haven't said which backend is used in both cases (webgl, wasm, cpu)? (and if `wasm`, if `simd` is enabled or not)  but in general, MacBook Air 2019 does not have a decidated GPU - it has integrated Intel UHD which is pretty bad.   on the other hand, iPhone SE 2020 has an actual decent GPU. so no surprise your phone is faster than notebook - if you're using `webgl` backend.also note that fast vs slow is different than initialization time.   e.g, `webgl` shader initialization and upload can take quite a lot of time, but then run smooth after that - that is quite common for `webgl` backend with more complex models.  on the other hand, `wasm` backend has almost instant initialization but doesn't run anywhere as fast when compared to `webgl` on a good GPU.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/739;I don't know how to implement this in Angular;2;open;2020-12-15T09:30:26Z;2021-02-16T05:22:03Z;I have working Javascript code but I don't know how to convert it to angular. Can someone help me please?This is the link to my github implementation of the javascript,https://github.com/thelionleo/JStoAngular.git;"[""You dont need anything special. Just install the library and start using it in your angular project:`npm i face-api.js`And then in your code import the library: `import * as faceapi from 'face-api.js'`====="", '@thelionleo Here is my [repo](https://github.com/imrushi/face-api-js-angular) I have implemented the Face-api.js in angular. I hope this will help you.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/738;Syntax for changing landmark line and point draw color with DrawFaceLandmarksOptions();1;open;2020-12-15T04:55:39Z;2020-12-15T14:36:39Z;"Looking for clear documentation on how to change the point and line color for landmarks being draw onto a live webcam feed, using the following is unsuccessful:const options = { lineColor: ""#FFFFFF"" }const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()const resizedDetections = faceapi.resizeResults(detections, displaySize)canvas.getContext(""2d"").clearRect(0, 0, canvas.width, canvas.height)faceapi.draw.drawFaceLandmarks(canvas, resizedDetections, options)faceapi.draw.drawFaceExpressions(canvas, resizedDetections)";"['you\'ll need to pass in options. Here\'s a start:```function getFaceDetectorOptions() {  if (selected_face_detector == ""tiny_face_detector"") {    return new self.faceapi.TinyFaceDetectorOptions({      inputSize,      scoreThreshold,      drawLines: true,      drawPoints: false    }),  } }```and then ```result = await self.faceapi      .detectSingleFace(imgCanvas, detector_options)      .withFaceLandmarks(true),```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/736;Bad performance for browsers;12;open;2020-12-07T18:07:42Z;2021-02-14T23:00:25Z;I have found that performance is terribly slow for browsers. But that's the case at least if you use windows.Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces.https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gifOn linux it works quite alright, taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:https://bugzilla.mozilla.org/show_bug.cgi?id=1678652https://bugzilla.mozilla.org/show_bug.cgi?id=1679671In short, for firefox v83, there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU, which is rather slow.However, that's true for linux only for now. On windows it still uses webgl, and still rather slow (20-30 seconds) for any browser.I am not sure if that's the issue of face-api.js or tensorflow itself;"[""I had a similar issue (on a mac)are you using tfjs2.0? that helps. I also am using a [webworker](https://github.com/justadudewhohacks/face-api.js/issues/47)have a look here also-  it's pretty fast[webcam example w react](https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking/)====="", 'I am using face-api.js of 0.22.2 which in turn uses tfjs-core"": ""1.7.0"".  I guess I can fork and upgrade tfjs to see if the issue is fixed there=====', '@cindyloo  @JeviScript  upgrade is not that trivial as there are breaking changes between tfjs 1.x and 2.x, including ops used inside models.  take a look at <https://github.com/vladmandic/face-api>=====', '@vladmandic I did read through your code but I was able to remove /nodemodules/faceapi tensorflow 1.7 so it would default to tfjs2.0 and it worked fine...=====', '@cindyloo I\'ve heard that being done before, but not sure what is it supposed to accomplish since TFJS 1.7 is bundled inside face-`api.js` during build process. And if you try to rebuild `face-api.js` using TFJS 2.x, it fails.  (`face-api.js` incorrectly lists `@tensorflow/tfjs-core"": ""1.7.0` as dependency when in reality its a devDepencency)=====', ""hmm - if I look inside the plainscript face-api.js, I don't see any specific bundling. Wouldn't that mean it would look to the package settings and node_modules to determine the library?====="", ""face-api.js includes bundled tfjs-core.If you don't even look at the code, look at sizes - face-api on its own is ~200kb, remaining ~600kb is tfjs-core.====="", ""thanks for the clarification @vladmandic. I can see it invoking the tflib nowso, having revisited the codebase, I am getting reasonably good performance. The fix for me was the throttling of the face detection and using an async call for the analysis'analyzeVideo' is essentially the onPlay method in the exampleeg:```const pullAndPostFrameTimer = (videoEl, mode) => {  const faceDetectorOptions = getFaceDetectorOptions(),  if (!videoEl || videoEl.ended || !faceDetectorOptions || faceDetectorOptions == undefined) {    setTimeout(function() {      pullAndPostFrameTimer(videoEl, mode),    }, THROTTLETIMER),    return,  }  let canvasVideoBuff = document.querySelector(    `#videoBuff${mode}`  ) as HTMLCanvasElement,  analyzeVideo(videoEl, mode, faceDetectorOptions),   setTimeout(function() {    pullAndPostFrameTimer(videoEl, mode),  }, THROTTLETIMER),},```====="", ""i'm not sure i understand as this example is not complete.is `analyzeVideo` async function? if yes, then you have race scenario that is reduced a bit by having a delay between frames.why not put a promise in `analyzeVideo` and then do something like:```jsanalyzeVideo(videoEl, mode, faceDetectorOptions).then(() => requestAnimationFrame(() => pullAndPostFrameTimer(videoEl, mode)),```or easier to read with async/await```jsawait analyzeVideo(videoEl, mode, faceDetectorOptions),requestAnimationFrame(() => pullAndPostFrameTimer(videoEl, mode)),```that way loop will run as fast as possible and there is no race.====="", ""@vladmandic so appreciate the note to race conditions. I had to review those within javascript ([this is a good review](https://medium.com/@slavik57/async-race-conditions-in-javascript-526f6ed80665)) as well as  `requestAnimationFrame`so here's the thing- performance seems to suffer a little with using `requestAnimationFrame` as there is no delay imposed and it attempts w every result to detect a face...I guess I could do a sleep or something?====="", ""When it comes to video, I don't like adding sleep to solve problems.  Ideally, you could make it a two separate loops, both using requestAnimationFrame:  - One loop that handles rendering and uses last known results  - Second loop that handles processing    Ideally get ImageData on main thread and pass it as a reference to web worker thread     to handle detection and send results back to main thread.  That way main UI thread gets minimal blocking and first loop will run at full refresh rate of the screen.    (that is one benefit of requestAnimationFrame - it will run up to refresh rate and not faster if not needed)====="", ""I took @vladmandic's advice to separate the rendering logic from the facial analysis (see the analyzeVideo vs. drawFaces routines). I alos tried to make my code work with `requestAnimationFrame` as well as with `requestVideoFrameCallback` but it ended up being really laggy (not sure what I was doing incorreclty.  The best solution I have found is to use the setTimeout/setInterval pattern eg:```const pullAndPostFrameTimer = (mode, anonymousParticipants) => {  const faceDetectorOptions = getFaceDetectorOptions(),  // for each element tag passed in, add a timer participants.map((p, index) => {    const videoEl = whichVideo(`${mode}Video${p}`) as HTMLVideoElement,    if (      !videoEl ||      videoEl.ended ||      videoEl.readyState < 3 ||      !faceDetectorOptions ||      faceDetectorOptions == undefined    ) {      return false,    }    analyzeVideo(videoEl, faceDetectorOptions), // async    drawFaces(mode, p),  //async    return true,  }),  setTimeout(function () {    pullAndPostFrameTimer(mode, participants),  }, THROTTLETIMER), // 8000 while waiting for faces, then speed it up to 2000ms (my best reaction time is between 1000-2000ms)},const FaceVisualiser = (params) => {  const { mode, participants } = params,  useEffect(() => {    if (participants) {      if (pullAndPostFrameTimer(mode, participants)) {        THROTTLETIMER = FACE_DETECTING, // 2000 ms        return,      }    }    THROTTLETIMER = NO_FACE_YET,    return function cleanup() {      THROTTLETIMER = NO_FACE_YET,    },  }),    [participants],  return <div />,},```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/734;Slow performance & Errors with 2 1080s;4;open;2020-12-04T20:04:59Z;2020-12-07T17:30:38Z;I am batch processing a few hundred images. Running detectAllFaces().withFaceLandmarks().withFaceDescriptors()Running with tfjs-node It takes about 200 secondsRunning with TFJS-node GPU it takes about 100 seconds...About 80 seconds in (image 1682/1745) I start to get buffer allocation errors. `2020-12-04 15:03:04.531673: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocating tensor with shape[1,64,64,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`Was really hoping for more than double CPU performance. GPUs only reach about 20% utilization. Situation does not change with only one 1080 present in the system.Each image is called with:```images.forEach(image => {    const tensor = decodeImage(image)    const faces = await faceapi.detectAllFaces(tensor, detectFacesOptions).withFaceLandmarks().withFaceDescriptors()})```Any ideas?;"[""Can confirm that the performance is also terribly slow for browser as well. But that's the case at least if you use windows. Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces. ![fr-slow](https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gif)On linux it works quite alright, taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:https://bugzilla.mozilla.org/show_bug.cgi?id=1678652https://bugzilla.mozilla.org/show_bug.cgi?id=1679671In short, for firefox v83, there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU, which is rather slow. However, that's true for linux only for now. On windows it still uses webgl, and still rather slow (20-30 seconds) for any browser.I am not sure if that's the issue of face-api.js or tensorflow itself====="", ""> Can confirm that the performance is also terribly slow for browser as well. But that's the case at least if you use windows.> Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces.> ![fr-slow](https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gif)> > On linux it works quite alright, taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:> https://bugzilla.mozilla.org/show_bug.cgi?id=1678652> https://bugzilla.mozilla.org/show_bug.cgi?id=1679671> > In short, for firefox v83, there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU, which is rather slow.> However, that's true for linux only for now. On windows it still uses webgl, and still rather slow (20-30 seconds) for any browser.> > I am not sure if that's the issue of face-api.js or tensorflow itselfFaces are recognized almost instantly. I mean milliseconds, its incredibly fast. I'm watching very subtle spikes in GPU 3D usage, and vRam is sitting at 1.5 GB after models are loaded.It seems to be an issue specifically with tfjs-node-gpu.Would be great if I could get GL to work with node out of the browser.**Also* I wrote a program that dispatches instances of faceAPI with their own tfjs-node backend. Running multiple threads quadrupled my performance when running solely on the CPU. However the moment I switch to tfjs-node-gpu I get the issue described here**https://github.com/tensorflow/tfjs/issues/4362====="", ""To clarify, I'm getting significantly more performance (with multithreading) out of tfjs-node than tfjs-node-gpu. This certainly does not hold true in the browser.FWI I have 2 GTX 1080s, Windows 10====="", ""On another note, to troubleshoot the issue, I found a fork of FaceAPI that runs the latest versions of TensorFlow. The exact same issues persist, Here is the issue I posted there:https://github.com/vladmandic/face-api/issues/20I also tried multithreading with the fork as described above. Again, significant performance boost from multithreading. Multithreading doesn't work on the GPU because of the issues with tjfs-node-gpu vram managment.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/732;detectAllFaces() Leaks Memory in WebWorker;3;open;2020-11-25T18:18:05Z;2021-04-30T16:26:54Z;I can confirm that `detectAllFaces` is leaking memory. I pass frames to a WebWorker with postMessage and transfer lists, then create an ImageData in the WebWorker as shown below.If I run the code as shown, the Chrome process memory balloons and grows DURING the frame processing. (E.g. while the webworker is inside `detectAllFaces`. Within ~3 frames, Chrome is using 25% of the RAM and growing. After ~30 seconds, Chrome has consumed nearly all of my 16 GB of RAM.As soon as I comment out the line that has `detectAllFaces` - memory usage flatlines. Does not grow or change. And that is the ONLY code change I make - with detectAllFaces, memory leaks. Without that call, no memory leak. Therefore, something is going wrong with detectAllFaces - because there are no other references to the given imgData other than detectAllFaces.Any ideas on how to diagnose further or, hopefully, fix?```// Earlier in the code ...faceapi.nets.ssdMobilenetv1.loadFromUri('/models')this.faceDetectorOptions = new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }),// For every message from the parent thread, we do...const imgData = new ImageData(	new Uint8ClampedArray(message.data),	message.width,	message.height),const img = faceapi.createCanvasFromMedia(imgData),const results = await faceapi.detectAllFaces(img, this.faceDetectorOptions)```_Originally posted by @josiahbryan in https://github.com/justadudewhohacks/face-api.js/issues/345#issuecomment-733872505_;"[""I think this might be specific to using the `cpu` tensorflow backend...why?I'm running my app over VNC on a laptop in latest chrome - in the devtools of that chrome, I see warnings about `webgl` not supported. Tried using `wasm` - that failed completely. So logic is that it falls back to the `cpu` backend.Running the same code - EXACT same code - in a browser locally on my Mac (same Chrome version, just local - not over VNC) - and no `webgl` warnings, and I can even use `wasm` - and no memory leaks. (Using Chrome's Task Manager, the RAM usage for that tab + worker stays flat)So, something about the CPU backend...? What's up with that...?====="", 'Can confirm the memory leak. @josiahbryan  could you solve that in the meanwhile?=====', 'Never tried to solve CPU memory leak - my use case did not depend on CPU backend because I could reply on using webgl in production.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/731;Verification of Face live(like face lock);1;open;2020-11-20T22:09:25Z;2020-12-01T05:26:56Z;can we just store details of one person details and then verify if the person is same or other? just like face lock work?;"['Yes. Have you looked at the `FaceMatcher` class in this project...? Check the docs for that class. I wrote my own for finding ""known"" faces in video feeds for the purposes of switching cameras for streaming, but the idea is the same - get a face descriptor, store it, label it, then later match new descriptors from found faces against the existing descriptors you have stored. If under some threshold of Euclidean distance, then voila, you have a match.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/730;BBT face recognition face page not recognizing my face;4;open;2020-11-20T08:03:10Z;2021-01-15T09:46:42Z;Hello,**[I am also looking for paid support if someone can help.]**I have added 5 colored pics of mine in a folder. kept name of folder 'saurabh' and name of pics as follows'saurabh1' , 'saurabh2' , 'saurabh3' , 'saurabh4' , 'saurabh5'.I have added 'saurabh' in the required array , code below for reference -`const classes = ['amy', 'bernadette', 'howard', 'leonard', 'penny', 'raj', 'sheldon', 'stuart', 'saurabh']`Now when i do `npm start` and run this page  - `http://localhost:3000/bbt_face_recognition.html`I choose my image from choose file option, once image is loaded it still says `unknown`.  Unable to understand the reason.![image](https://user-images.githubusercontent.com/50316624/99775129-c9de1280-2b34-11eb-9d7b-d6538d09c738.png) ;"[""Hey @ersaurabh101  happy to help - I've done a ton of face matching in my own projects. If you'd consulting / support, you can ping me directly at josiahbryan@gmail.com or happy to try to help answer questions here, of course.I've wrote my own face matcher code, I don't use the internal class, but the principals are the same:- Find a face- Get descriptors- Store descriptors with labels for later use- Later, with new face, match against stored descriptors - if less then some threshold of Euclidian distance, then you have a matchLike I said, built-in matcher should work, but I use my own classes to do the storing/matching of descriptors just so I have full control of the process and can tune the matching better.====="", ""I will show you what i have done till now but i am not happy with theresults of face matching.Let me know when you are available, i can show you on anydesk or googlemeet.My requirement: I want it to match faces accurately.On Tue, Dec 1, 2020 at 11:00 AM Josiah Bryan <notifications@github.com>wrote:> Hey @ersaurabh101 <https://github.com/ersaurabh101> happy to help - I've> done a ton of face matching in my own projects. If you'd consulting /> support, you can ping me directly at josiahbryan@gmail.com or happy to> try to help answer questions here, of course.>> I've wrote my own face matcher code, I don't use the internal class, but> the principals are the same:>>    - Find a face>    - Get descriptors>    - Store descriptors with labels for later use>    - Later, with new face, match against stored descriptors - if less>    then some threshold of Euclidian distance, then you have a match>> Like I said, built-in matcher should work, but I use my own classes to do> the storing/matching of descriptors just so I have full control of the> process and can tune the matching better.>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/730#issuecomment-736229488>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AL74KUGO2XXHXDCRKDEJF4TSSR5N3ANCNFSM4T4OBK4Q>> .>====="", 'Yeah ""accurately"" is all based on how good the sample / source data you have. No guarantees we can get it perfect, but we can get it to match ""close to accurately"". I would say I personally get about 80-90% perfection, and it improves as I label the faces and merge duplicates. DM me / email me for personal support if you want :)=====', ""> I will show you what i have done till now but i am not happy with the results of face matching. Let me know when you are available, i can show you on anydesk or google meet. My requirement: I want it to match faces accurately.> […](#)> On Tue, Dec 1, 2020 at 11:00 AM Josiah Bryan ***@***.***> wrote: Hey @ersaurabh101 <https://github.com/ersaurabh101> happy to help - I've done a ton of face matching in my own projects. If you'd consulting / support, you can ping me directly at ***@***.*** or happy to try to help answer questions here, of course. I've wrote my own face matcher code, I don't use the internal class, but the principals are the same: - Find a face - Get descriptors - Store descriptors with labels for later use - Later, with new face, match against stored descriptors - if less then some threshold of Euclidian distance, then you have a match Like I said, built-in matcher should work, but I use my own classes to do the storing/matching of descriptors just so I have full control of the process and can tune the matching better. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#730 (comment)](https://github.com/justadudewhohacks/face-api.js/issues/730#issuecomment-736229488)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AL74KUGO2XXHXDCRKDEJF4TSSR5N3ANCNFSM4T4OBK4Q> .HI.! sir I am also facing same problem..=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/728;It is allowing two similar face structures for validation.;1;open;2020-11-17T16:54:55Z;2020-12-01T05:30:48Z;When I tried to validate a face it is allowing similar faces which I got from online.;['You can set the threshold - see https://github.com/justadudewhohacks/face-api.js/blob/master/src/globalApi/FaceMatcher.ts - constructor shows a threshold option. Try .4 or .3 for tighter constraints.=====']
https://github.com/justadudewhohacks/face-api.js/issues/727;Live detection;7;open;2020-11-10T20:10:58Z;2020-12-01T05:33:05Z;Is that possible to have a live detection function in it? If the function is existing, where can I find it? Thanks. I mean the code should detect a real human face, should not detect a face from a picture.;"[""You should check online first. Here's a tutorial which exactly does that :) [LINK](https://youtu.be/CVClHLwv-4I)====="", ""> You should check online first. Here's a tutorial which exactly does that :) [LINK](https://youtu.be/CVClHLwv-4I)Thanks for the link. I have watched it and made the demo code. My project needs to detect a real man' face, not from a picture.Maybe could face-api.js have a feature of detecting a  face from human or from a picture?====="", ""> detect a real man' faceI am not sure what you mean by this because the videos shows the implementation in real time====="", ""> > detect a real man' face> > I am not sure what you mean by this because the videos shows the implementation in real-timeThanks for taking the time with me. For example: If I need to make a door lock/unlock system by using the Face-api.js. The system should  **not** unlock a door when a person using a (face) picture in front of the system's camera. ====="", 'i got your point titan im having the same project live verification via webcam. if you find solution anywhere please lemme know=====', ""> using a (face) pictureI got you now. I'd want that feature too :))====="", ""Technically what would distinguish a live face from a photo-face? Just curious what you mean exactly. How do you propose distinguishing the two, purely visually? :) Curious, interesting idea!You might have to branch out to use OpenCV or a different tensorflow model - I know TF does have some models that track ~400 points on a face. Then you could do FFT on those points and if they change by some small frequency greater than some threshold, then you have a live face. If the points all stay static to each other, then it's a photo maybe?If you want me to write code to do that for you, happy to accept paid consulting gigs lol!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/726;faceapi.computeFaceDescriptor(alignedFaceImage);1;open;2020-11-05T11:06:53Z;2020-12-01T05:34:18Z;In  gitHub repo(https://github.com/justadudewhohacks/face-api.js)there is a below code to detect or compute face descriptor:**const descriptor = await faceapi.computeFaceDescriptor(alignedFaceImage)**My question is HOW to get **alignedFaceImage**??Please provide any resources for same to compute face descriptors of two images and match images.;['Did you check the demos? Literally just do face detection, get the face rectangle, and then grab the face (.getImageData or whatever from a canvas using that rectangle) and pass it to the compute function...very straightforward in the demo code=====']
https://github.com/justadudewhohacks/face-api.js/issues/725;BBT Face similarity matches for all the images;1;open;2020-11-05T11:01:36Z;2020-12-01T05:39:05Z;Im trying to find similarity value between two faces in two different photographs,but i always get faces matched(ie value less than 0.6) eventhough the images are different and different gender.;"[""Make threshold smaller?Seriously, https://github.com/justadudewhohacks/face-api.js/blob/master/src/globalApi/FaceMatcher.ts has a threshold option in constructorAlso, I wrote my own matcher that factors in gender (using the gender detection net, of course) - and invalidates a match if genders don't match. So I don't use the internal matcher, I do some other funky things with the tensor/vectors to massage matching, but same basic thresholding idea, with gender thrown in, etc.HTH, YMMV=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/720;Device Requirement for optimal performance?;1;open;2020-11-02T11:20:51Z;2020-12-01T05:37:29Z;Hi @justadudewhohacks Many many thanks for this awesome project <3 i just wanted to ask what are the best specs for using this project? While running this on my Mac Air, 8 GB Ram it started taking some loaded and my laptop fan started. I just want to understand what requirements could be the best fit for this project? (I just want to tell me, users, to meet a minimum of these requirements. before using this)**I want to run this project in these conditions.**1. This project will run continuously for 2 to 3 hours2. User will use only and only this program on a single chrome instance and nothing running in any tab or any software in the background;"[""Fan running ...yeah, that's tough to avoid that with this project lol. Is that a problem for you if the fan kicks in?Not sure of minimums, but I've let it run for ~9 hrs overnight, getting ~20fps on a MacBook Pro (2.4 Ghz 8-core i9 with 32 GB RAM) - and yeah, fans kicked in - but that's just because I'm limited to two cores on the MBP because the Ui takes one core and web worker takes one core - no real easy way to split work across more than 2 CPUs here in this case.So yeah.. multiple cores? RAM is not as important as CPU ..still need healthy ram, but ymmv ... @rohitcoder - just my thoughts, hope it helps. Happy to answer questions, lmk=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/719;'multiply' not yet implemented or not found in the registry;8;open;2020-10-31T09:09:37Z;2020-12-02T22:19:53Z;"Following are the versions of node-packages to reproduce the error""face-api.js"": ""^0.22.2""""@tensorflow/tfjs-backend-cpu"": ""^2.7.0""""@tensorflow/tfjs-backend-webgl"": ""^2.7.0""""@tensorflow/tfjs-converter"": ""^2.7.0""""@tensorflow/tfjs-core"": ""^2.7.0""I tried loading the tinyFaceDetectorModel, here is the code snippet`await faceapi.nets.tinyFaceDetector.loadFromUri('/models'),``const input = document.querySelector(""img""),``const result = await faceapi.detectSingleFace(input,new faceapi.TinyFaceDetectorOptions()),`It leads to this error:**backend.js?8b87:504 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen**";"['This looks like a regression in tensorflowjs, downgrading to 2.6.0 fixes the issue=====', ""> This looks like a regression in tensorflowjs, downgrading to 2.6.0 fixes the issueI downgraded to TF 2.6.0 and now it is giving some new error here it is:`Uncaught (in promise) TypeError: Cannot read property 'length' of undefined`   ` at parseTupleParam (conv_util.js?b818:239)`    `at tupleValuesAreOne (conv_util.js?b818:386)`    `at Module.eitherStridesOrDilationsAreOne (conv_util.js?b818:390)`    `at Object.maxPool [as kernelFunc] (MaxPool.js?898d:27)`    `at kernelFunc (engine.js?6ae2:425)`    `at eval (engine.js?6ae2:477)`    `at Engine.scopedRun (engine.js?6ae2:318)`    `at Engine.runKernelFunc (engine.js?6ae2:475)`    `at cl (tf-core.esm.js?953f:17)`    `at maxPool_ (tf-core.esm.js?953f:17)`@framp I think that downgrading didn't work====="", 'This works ok on my machine: https://gist.github.com/framp/ac13e105f0d1a4209b12047781321a7a=====', 'I am getting the excat same issue @archeelp Did you find any solution ?=====', ""for what it's worth, I also get this error when using TFJS 2.7.0```backend.js:665 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.js:665)    at MathBackendWebGL.multiply (backend.js:176)    at Kt.runKernelFunc.a (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)```But trying other 2.x releases (I tried TFJS 2.2.0, 2.3.0, 2.4.0, 2.5.0, 2.6.0) all threw this error```face-api.min.js:1 Uncaught (in promise) TypeError: t.batchNormalization is not a function    at Kt.runKernelFunc.x (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)    at Engine.runKernelFunc (engine.js:636)    at yu (face-api.min.js:1)```  **_Update_**: I've just seen that [this was already reported/known](https://github.com/justadudewhohacks/face-api.js/issues/644#issuecomment-647049700)====="", '@dalelane I used TFJS 1.9 and it solved that error.=====', '@shailjaa downgrading to TFJS 1.* will be a workaround. But if you have other packages which have dependency requirements of TFJS version >= V2.0 it will again cause a series of errors.=====', ""it's well known that `face-api` is not compatible with tfjs 2.0+ - you just run into few issues (such as `batchNormalization` being deprecated so models need updating, not just JS code, etc.)  i've spend a lot of time to rewrite parts, if you wish you can check out <https://github.com/vladmandic/face-api>=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/718;Detect uneven lighting on face;3;open;2020-10-30T13:42:32Z;2020-12-21T05:16:58Z;"Hi,I want to detect uneven lighting on face. Is there any way to do so. Please help.I want to use this lib for some face validations like ""Face is not fronting to camera"","" Uneven lit on face"" and ""Face is too far from camera"". Please help me with how I can detect uneven lit face with this lib.";"['@justadudewhohacks please help me with this. Can we get blur and exposure value of face using this lib?=====', 'Probably not blur/exposure, noYou could do a histo of the image using opencv.js for lighting. Exposure is tough. I found this project: https://github.com/idealo/image-quality-assessment - but haven\'t made it work in javascript yet - just an idea of other work in the field.For your ""front facing face"" problem, my best suggestion is ... you could also use opencv.js to detect only front-facing faces (one of their haarcascades ONLY works for front-faces) then use this lib to do the face matching for descriptors...just an idea@Namrataijare HTH, lmk if you have more questions=====', '@josiahbryan Thanks for your suggestions. I will try out this. Thank you so much.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/717;how to distinguish between photo and real person through webcam;1;open;2020-10-30T06:11:44Z;2020-12-01T05:44:11Z;Thank you very much for making this amazing project, I have noticed that this face-api.js can't distinguish between photo and real person. so is there a way to accurately distinguish, if not then I'm just suggesting to community for thinking about this thing.;"[""@pratik9722 looks like a discussion going on in issue #727  - here's some thoughts I had on that: https://github.com/justadudewhohacks/face-api.js/issues/727#issuecomment-736230897=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/713;node-fetch - npm install vunerability;4;open;2020-10-19T13:50:52Z;2020-10-20T20:44:22Z;After installing face-api.js npm returns low vulnerability that can't be fixed. Output of npm audit:```Low           Denial of Service                               Package       node-fetch                                      Patched in    >=2.6.1 <3.0.0-beta.1|| >= 3.0.0-beta.9         Dependency of face-api.js                                     Path          face-api.js > @tensorflow/tfjs-core > node-fetch```Running npm audit fix fails.;"['Have you tried `npm audit fix --force`?=====', '> Have you tried `npm audit fix --force`?No, mostly because I saw node-fetch causing issues for other people with other packages when I was looking into it.=====', 'This issue is related to packages depending on `node-fetch`, to not update their modules and end up with vulnerabilities. But it makes sense because they have to read the new updates of `node-fetch` in order to maintain a stable package.I believe that using `--force` flag will get rid of the vulnerability but will create other issues with your `face-api` package.You have two options:- Wait `face-api` contributors to fix it- Fix it and open a PR (win-win situation)=====', ""I've updated all packages and switched to TFJS 2.0 branch in my fork if you want to try   <https://github.com/vladmandic/face-api>=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/712;Face descriptors question;1;open;2020-10-16T09:17:47Z;2020-10-29T15:22:50Z;Hi there, sorry about my question but im kinda new in that area yet.Im trying figure how works and what represents each point of face descriptors, i mean, i know the  landmarks are just [x,y] positions of the face points that we detected using the framework, but when it is transformed to Face descriptors i just see an array of decimals that not sure if they are represnetable in any graffic? or how i can know what face descriptor correspond to what face landmark point?I would like to can know what face descriptor corresponds with a concrete point of the face, and i what format? If anyone can assist me would be great.Thx you all in advance and again sorry about my english.;"[""it's a calculated value in `Float32Array` format.    Computes a 128 entry vector (face descriptor / face embeddings) from the face shown in an image,    which uniquely represents the features of that persons face. The computed face descriptor can    be used to measure the similarity between faces, by computing the euclidean distance of two    face descriptors.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/707;examples-nodejs not compiling;2;open;2020-10-13T11:03:46Z;2020-10-20T20:48:12Z;# Steps taken:1. Clone the repo2. cd face-api.js/examples/examples-nodejs3. npm i4. tsc faceDetection.ts# Expected Result:ts should compile into js, and then I can move forward and run faceDetection.js, as the README states.# Actual Result:Error:```../../node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 556 more ..., readonly WAIT_FAILED: number, }', but here has type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 557 more ..., readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number, }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../../../usr/local/lib/node_modules/typescript/lib/lib.dom.d.ts:16354:13    16354 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```# Environment:**node version**: v10.22.0**os**: Mac OS Catalina (10.15.7);"['Fix is to make sure that you:```npm i --save @types/webgl2```Pre-requisites should definitely go into the documentation.=====', ""also, there are quite a few type mismatched is you happen to to upgrade your typescript/tslib as newer version have more in-depth type checking. so if you want to use original face-api, you're stuck with old tslib 1.1. or pick a newer port.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/688;posenet and face-api in the same project;4;open;2020-09-01T05:36:04Z;2021-04-19T12:23:29Z;Hi, I'm not exactly sure what's happening, but I hope someone can save me some sleuthing.I'm using both TFJS Posenet and Face-Api on different sections of my project (not at the same time).I'm finding that I'm getting errors running Face-Api:```Uncaught (in promise) TypeError: t is not a function    at engine.ts:583    at engine.ts:423```But this only occurs after I import Posenet elsewhere in my code (without even running it):`import * as posenet from '@tensorflow-models/posenet',`Is there anything I can do to mitigate this error? Ideally, I need to go back and forth between Face-Api and Posenet in the same browser based single page application.thanks;"[""Both PoseNet and Face-API import `tfjs-core` and it registers some globals, so you end up with multiple version conflict.One of the reasons why I've forked face-api, updated it and modified it so you can import `tfjs` independently and use it across different models.This is not a self-promotion, just hope it helps.  <https://www.npmjs.com/package/@vladmandic/face-api>====="", ""@vladmandic Thanks, I'm sure it would help if I still needed it, but I just switched over to FaceMesh which I don't think was available when I started this project.====="", '@vladmandic Have the same problem, it helps a lot!=====', ""i use facemesh (heavily modified) in my own project that i've started after `face-api` as well: <https://github.com/vladmandic/human>=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/687;Is there any callback for detectAllFaces(), so that result can be processed asynchronously;5;open;2020-08-31T14:50:11Z;2020-12-01T05:49:07Z;"I want to call `faceapi.detectAllFaces()` asynchronously, I means I don't want to wait for its result, so I can't call await faceapi.detectAllFaces(). I tried below code:```async function detectFace(){    document.getElementById('camPic').src = getCurrentImageAsBase64(), //getCurrentImageAsBase64() gets the image in base64 format from canvas    return faceapi.detectAllFaces(document.getElementById('camPic'), new faceapi.SsdMobilenetv1Options()),  }```In index.jsp```$('#camPrcd').click(function(e){    e.preventDefault(),     detectFace().then((arr) => console.log(arr)),    console.log('detection called'),}),```After button click, on console I can see message ""detection called"" and after sometime detection result gets logged from `then()` block.But my observation is when `detectFace()` is called the html page feels like hanged (not able to click on any button) and when `arr` gets printed on console then I can able to click on page.Looks like even though ""detection called"" message gets printed immediately before detection happens but `faceapi.detectAllFaces()` is not doing work asynchronously._Note: This happens only for first call of `faceapi.detectAllFaces()` as per author of face-api.js during first call model for face detection get compiled and so detection time is more at first call as compare to subsequent call to function._So is there any way that, I can call for detection and still web page is accessible and when it's finished a callback function will handle the detection result.";"['As far as a I know, detectAllFaces returns a promise, so the process is asynchronous. I also noticed that, if the process is being executed on cpu instead of gpu, the main thread will freeze. Take a look at faceapi.tf.getBackend().=====', 'what about this?  ---> https://www.tensorflow.org/js/guide/platform_environment#avoid_blocking_the_ui_thread=====', '> > > As far as a I know, detectAllFaces returns a promise, so the process is asynchronous. I also noticed that, if the process is being executed on cpu instead of gpu, the main thread will freeze. Take a look at faceapi.tf.getBackend().`faceapi.tf.getBackend()` returns ""webgl"". I also checked with:```faceapi.detectAllFaces(img, new faceapi.TinyFaceDetectorOptions()).then((detections) =>{    //further logic}```Even `tinyFaceDetector` also fezzes the web page on first call of  `detectAllFaces()`. Sometimes browser (Mozilla) complaints about ""Script is taking too long to execute"". There is no way to avoid this delay of first time compilation of models?=====', '> > > what about this? ---> https://www.tensorflow.org/js/guide/platform_environment#avoid_blocking_the_ui_thread@justadudewhohacks have you checked this? Will it solve #87 , #160  and mine problem also.=====', '@rover886 I do all my face detection in a webworker. DEFINITELY helps to not block UI thread - UI thread is buttery smooth, and I get 12fps - 20fps in my WebWorker for face detection. #47 is the issue where all that is discussed=====']"
https://github.com/justadudewhohacks/face-api.js/issues/685;Web Worker on Android / IOS WebView;2;open;2020-08-25T06:53:31Z;2020-12-01T05:51:20Z;Is there any way to make it work in Web Worker on Android / IOS WebView ? I able to implement it in browser using offscreencanvas and it worked perfectly. When I build the app using capacitor I'm getting undefined offscreencanvas function error. After further research, webview doesn't support offscreencanvas and capacitor doesn't allow changing of default webview.Thanks;"[""I'm trying to do the same using WebView Component, but I'm stuck at getting camera permission with WebView (I'm experimenting to use face-api.js within a WebView by setting WebView source prop as my webapp but I can't get permission to use camera). Have you managed to get camera permission from WebView? Thanks in advance. ====="", ""#47 documents successful use of WebViews - however, here's how I do it:- Capture camera in UI thread, show in a `<video>` element- Set a timeout every X ms to render the `<video>` into a `<canvas>` that is hidden- Do `.getImageData` on canvas, post that via transfer lists to the web worker- WebWorker does all the face api detection- WebWorker posts a message back to UI thread with the detected results#47 documents an overlay canvas for rendering the views too - definitely works great. I get about 12-20fps detection, while the UI thread stays buttery smooth rendering live `<video>`=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/682;Not working in Firefox;1;open;2020-08-16T11:28:03Z;2020-08-20T13:35:49Z;Hello, unfortunally this library doesn't work in Firefox.Neither the demos nor my own implementation work. The console always outputs a warning and doesn't detect anything.``WebGL warning: readPixels: Buffer for `target` is null``In Chrome it works fine.;"[""This is confirmed. It's also happening in Chrome/Edge with a slightly different error in some cases (we haven't been able to determine which cases yet, but are testing further). I'm suspecting an update with browsers or maybe Windows is causing the problem, as it's happening in multiple browsers, but not every user is experiencing it.@justadudewhohacks are you still maintaining this library?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/680;I'm having an issue with fetching the api can anyone help please. ;4;open;2020-08-13T15:43:00Z;2020-09-10T14:00:46Z;"<img width=""732"" alt=""Screen Shot 2020-08-13 at 10 41 20 AM"" src=""https://user-images.githubusercontent.com/65366881/90156070-aa736480-dd51-11ea-9f70-8378f77aa7c6.png"">";"[""I fixed this issue about 2 weeks ago by putting all my models in another folder and called that folder but now it's not working and I have no idea why. ====="", 'It looks that the browser does not allow accessing weight data due to CORS. Try to access the data using http:// instead of file://=====', 'pscardoso is correct. You need to be running this behind an http or https server. =====', 'If you\'re running **face-api** from browser, you must use http or https a fetch does not provide capability to load from file.And if you\'re running **face-api** from nodejs, it can load from file, but it doesn\'t have built-in fetch method at all, so you should provide custom fetch method, something like:```jsconst fetch = require(""node-fetch""),faceapi.env.monkeyPatch({ fetch: fetch }),```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/679;Question: Is face-api.js based on OpenCV?;1;open;2020-08-12T05:38:29Z;2020-08-24T07:28:37Z;Is face-api.js based on OpenCV?Also, can we do face-recognition based on Local binary Patterns Histogram? And how we can train a specific Neural Network model?Thanks.;['No, It is base on TensorFlow.Good luck.=====']
https://github.com/justadudewhohacks/face-api.js/issues/677;I´m getting the following error, I´ll leave it in the comment;4;open;2020-08-07T01:25:10Z;2021-04-12T08:49:51Z;"613face-api.min.js:1 Uncaught (in promise) Error: Box.constructor - expected box to be IBoundingBox | IRect, instead have {""left"":0.15384615384615385,""top"":null,""right"":0.15384615384615385,""bottom"":null}    at Vp.Wp (face-api.min.js:1)    at new Vp (face-api.min.js:1)    at Gg.<anonymous> (face-api.min.js:1)    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)";"[""Perhaps it's because your gpu is not compatible with the current tensorflow version. I've got the same problem and solved it changing the backend to cpu.====="", ""> is not compatible with the current tensorflow version. I've got the same problem and solved it changing the backend to cpu.how did you change it?====="", 'I was having this error when running the html served in a node environment. Running it from traditional apache works fine.=====', 'Got similar error on Ionic platform using Angular 10 and Capacitor.js```Line 314410 - Msg: ERROR Error: Uncaught (in promise): Error: Box.constructor - expected box to be IBoundingBox | IRect, instead have {""left"":null,""top"":null,""right"":null,""bottom"":null}    Error: Box.constructor - expected box to be IBoundingBox | IRect, instead have {""left"":null,""top"":null,""right"":null,""bottom"":null}```It is normal when serving in Angular. When serving in a smartphone using Capacitor, it happens. Any clue? or is it just capacitor webview\'s fault?Thank you.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/675;sending faceMatcher as the response from node API;4;open;2020-08-03T20:36:34Z;2020-08-12T17:28:02Z;"Very Urgent: Please help!Node:require(""@tensorflow/tfjs-node""),const faceapi = require(""face-api.js""),const canvas = require(""canvas""),const path = require(""path""),const { promisify } = require(""util""),const readdir = promisify(require(""fs"").readdir),const stat = promisify(require(""fs"").stat),const router = require(""express"").Router(),const loadLabelledFaceImages = async () => {  await faceapi.nets.tinyFaceDetector.loadFromDisk(    `${__dirname}/../face-models/`  ),  await faceapi.nets.faceLandmark68Net.loadFromDisk(    `${__dirname}/../face-models/`  ),  await faceapi.nets.faceRecognitionNet.loadFromDisk(    `${__dirname}/../face-models/`  ),  await faceapi.nets.ssdMobilenetv1.loadFromDisk(    `${__dirname}/../face-models/`  ),  await faceapi.nets.faceExpressionNet.loadFromDisk(    `${__dirname}/../face-models/`  ),  await faceapi.nets.ageGenderNet.loadFromDisk(`${__dirname}/../face-models/`),  //joining path of directory  let directoryPath = path.join(__dirname, ""../labelled-images""),  //passsing directoryPath  const files = await readdir(directoryPath),  return Promise.all(    files.map(async (file) => {      const descriptions = [],      for (let i = 0, i <= 39, i++) {        const imageLink = `${directoryPath}/${file}/${i}.jpg`,        const image = await canvas.loadImage(imageLink),        const detections = await faceapi          .detectSingleFace(image)          .withFaceLandmarks()          .withFaceDescriptor(),        descriptions.push(detections.descriptor),      }      return new faceapi.LabeledFaceDescriptors(file, descriptions),    })  ),},router.get(""/getimagesdata"", async (req, res) => {  const labeledFaceDescriptors = await loadLabelledFaceImages(),  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6),  res.status(200).send({ faceMatcher }),}),module.exports = router,Frontend Angular:faceRecognition = async () => {    this.userManagementPermissionService.getImagesData().subscribe((res) => {      const faceMatcher = res.faceMatcher,      const faceCanvas = faceapi.createCanvasFromMedia(        this.video.nativeElement      ),      this.webcam.nativeElement.append(faceCanvas),      faceCanvas.classList.add('face-canvas'),      const displaySize = {        width: this.video.nativeElement.width,        height: this.video.nativeElement.height,      },      faceapi.matchDimensions(faceCanvas, displaySize),      let x: any = [],      let y: any = [],      let width: any = [],      let height: any = [],      let resizedDetections: any = [],      setInterval(async () => {        const detection = await faceapi          .detectAllFaces(            this.video.nativeElement,            new faceapi.TinyFaceDetectorOptions()          )          .withFaceLandmarks()          .withFaceDescriptors(),        resizedDetections = faceapi.resizeResults(detection, displaySize),        faceCanvas          .getContext('2d')          .clearRect(0, 0, faceCanvas.width, faceCanvas.height),        const results = resizedDetections.map((d: any) => {          return faceMatcher.findBestMatch(d.descriptor),        }),        results.forEach((result: any, i: any) => {          const box = resizedDetections[i].detection.box,          const drawBox = new faceapi.draw.DrawBox(box, {            label: result.toString(),          }),          drawBox.draw(faceCanvas),        }),      }, 100),I am getting error : TypeError: faceMatcher.findBestMatch is not a functionTypeError: faceMatcher.findBestMatch is not a functionPlease do help!! Thanks in advance![error](https://user-images.githubusercontent.com/25694814/89224897-e5112a80-d5f6-11ea-9a09-041d3867a6e4.png)";"['Please guys, If anyone can provide the solution=====', ""my best guess is that you don't have faceapi 'loaded' or in state somewhere in the sense it is instantiated and can access it's methods. I'm in the react world and had to re-purpose react-use-faceapi to access the methods without error====="", 'actually this is a MEAN stack project and just to load the require(""@tensorflow/tfjs-node""), i had to install node v10.16.3 as it  was not compatible and throwing error. I used   ""@tensorflow/tfjs-node"": ""^1.2.11"", to make it compatible, I have no idea what is the issue, working great with just used on angular but photos been saved to server that\'s why wanna run it on server.=====', ""if I am not wrong, you can't load the Facematcher on the server and then pass it onto the client. I don't think that will work. What you can do, is create the descriptors and save that as a json. And send the json file to the client and then load it on the client side. You would still need to initiate the faceMatcher on the client side except you don't have to create the descriptors all over again. Just load them. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/671;"Question : Is it possible to modify this to be a ""vehicle-api.js""?";1;open;2020-07-29T18:50:06Z;2020-12-01T05:55:55Z;I want to start off by saying I am a real fan of your (justadudewhohacks) work here and if you find the time to answer I thank you but I am just as happy with community input.On to the details : I want to use the premise that face-api.js follows where it finds faces then you can save those faces somewhere and even label them with names of your own choosing. Quite quickly in fact.I am very much willing to do all the work necessary to make it happen. In essence I would like to know if this is a reasonable place to begin.1. Fork face-api.js2. Read `https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceRecognitionNet/FaceRecognitionNet.ts`3. Make a basic model set? (this I don't know how to do, do I just make a vehicle detector model? like the kind used for object detection?)4. replace model loaded in file mentioned in .2I am seeing coco stuff in the src folder as well. Does that mean I can sort of just swap it? Or is that an over-simplification?Thank you to anyone who answers.And Thanks again to justadudewhohacks.;"[""@moeiscool I know opencv.js has some haarcascades for licence plate detection, and some great object tracking features. Also, there are tons of other tensorflow models, I would google for TF models specific to vehicle tracking...ymmv, but yeah, I'm with you on how awesome this project is!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/667;[Question] Results of faceapi.detectSingleFace equivalent in PHP;1;open;2020-07-22T07:37:34Z;2020-07-29T18:59:49Z;Is there a way to make equivalent of this JS code `const face = await faceapi.detectSingleFace(myImage, getFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor(),` into PHP?;"[""I'm no expert on the stuff, just a little experience in face-api and PHP.I think you should be able to run an exec command upon a node script within PHP. Essentially like this1. Make a node script that has this command setup.2. Make it so when you run the script with `node scriptName.js` you can add an argument like `node scriptName.js imagePath`3. Then in the script make it read the image path when it is run and get the buffer data to push inward.4. Make it print the data to the scripts `process.stdout`5. Add an exec to the PHP script that executes `node scriptName.js imagePath`I assume you already considered this if you are looking to use a Buffer however I don't see any other way with tfjs. You can possibly consider finding a PHP-driven module for tensorflow, however it make not be up to the same quality as face-api.js's implementation of tfjs.Also, just because I can't see a way doesn't mean there isn't one. I haven't used PHP in a very long time so take my recommendation with salt.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/660;Detecting closed eye from webcam feed;3;open;2020-07-13T21:27:51Z;2020-08-26T08:11:56Z;Thanks for this awesome library.I want to detect when user closes the eyes. I highly appreciate if I get code example. ;"['I am not sure there is an API in face-api for this. All the relevant examples possible are given in the examples folder. Not seen one for eye lid detection. This is more aimed at the face itself. =====', 'You can use face landmark for eyes, and then find the vertical distance between the eyelids to recognize blink or closed eyes.=====', ""> You can use face landmark for eyes, and then find the vertical distance between the eyelids to recognize blink or closed eyes.No the model doesn't recognise whether eye is closed. The landmarks just show the position of eye.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/659;Save detected face into jpg;3;open;2020-07-13T11:29:11Z;2020-08-08T04:42:24Z;I am trying to capture the detected face into a jpg, is there a way for me to save detected face into jpg extension?;"['I want the same, any solution to capture face detected on camera=====', ""share a little function to do that:```jsconst getFaceAndDescriptor = (canvas, type = 'base64') => {  if (!canvas) return Promise.reject(new Error('Foto no válida')),  return new Promise(async (resolve, reject) => {    try {      const photo = {        thumbnail: {          b64: null,          blob: null,        },        faces: [],      },      resizeToMax(canvas), //<- reduce canvas height and width if it necesary      const result = await faceapi.detectAllFaces(canvas, faceDetectionOptionsFiles),      if (!result) return reject(new Error('Cara no encontrada')),      if (result.length > 1) {        return reject(new Error(`Foto no válida, se encontraron ${result.length} rostros`)),      }      if (result.length === 0) {        return reject(new Error('La imagen no contiene un rostro o se encuentra muy lejos')),      }      let { box } = result[0],      let region = new faceapi.Rect(box._x, box._y, box._width, box._height),      const boxFace = box,      let face = await faceapi.extractFaces(canvas, [region]),      const landmarks = await faceapi.detectFaceLandmarksTiny(face[0]),      box = landmarks.align(),      region = new faceapi.Rect(box._x, box._y, box._width, box._height),      face = await faceapi.extractFaces(face[0], [region]),      canvas2gray(face[0]), // Convierte el canvas a escala de grises      const blobFace = await canvas2blob(face[0]),      let descriptor = await faceapi.computeFaceDescriptor(face[0]), // get Float32Array      // Encode descriptor to send a server      descriptor =  btoa(String.fromCharCode.apply(null, new Uint8Array(descriptor.buffer)))      // Push blob of face and descriptor      photo.faces.push({        blob: blobFace,        descriptor,      }),      box = boxFace,      if (canvas.height > 240 && canvas.width > 320) {        let x,        let y,        let h = canvas.height * 0.9,        let w = box.width,        const centerX = box.width / 2,        const centerY = box.height / 2,        x = box.x - centerX,        if (x < 0) x = box.x,        else w = box.width * 2,        y = box.y - centerY,        if (y < 0) y = 0,        if (h - y < canvas.height) h -= y,        else h = box.height,        // Get thumbnail        region = new faceapi.Rect(x, y, w, h), // region center imagen      } else {        region = new faceapi.Rect(box.x, box.y, box.width, box.height),      }      const thumbnail = await faceapi.extractFaces(canvas, [region]),      const blobThumbnail = await canvas2blob(thumbnail[0]),      photo.thumbnail = {        blob: blobThumbnail,      },      if (type === 'base64') photo.thumbnail.b64 = thumbnail[0].toDataURL('image/jpeg'),      resolve(photo),    } catch (error) {      console.error(error),      reject(error),    }  }),},function canvas2blob(can, type = 'image/jpeg', quality = 0.97) {  return Promise.try(() => {    if (!can) throw new Error('Empty canvas'),    return new Promise((resolve, reject) => {      can.toBlob(resolve, type, quality),    }),  }),}function resizeToMax(can, max_size = 640) {  if (!can || !can.width || !can.height) return,  let { width } = can,  let { height } = can,  if (width <= max_size && height <= max_size) return,  if (width > height) {    if (width > max_size) {      height *= max_size / width,      width = max_size,    }  } else if (height > max_size) {    width *= max_size / height,    height = max_size,  }  can.width = width,  can.height = height,}function canvas2gray(c) {  if (!c || !c.width || !c.height) return,  const ctx = c.getContext('2d'),  const idataSrc = ctx.getImageData(0, 0, c.width, c.height), // original  const idataTrg = ctx.createImageData(c.width, c.height), // empty data  const dataSrc = idataSrc.data, // reference the data itself  const dataTrg = idataTrg.data,  const len = dataSrc.length,  let i = 0,  let luma,  // convert by iterating over each pixel each representing RGBA  for (, i < len, i += 4) {    // calculate luma, here using Rec 709    luma = dataSrc[i] * 0.2126 + dataSrc[i + 1] * 0.7152 + dataSrc[i + 2] * 0.0722,    // update target's RGB using the same luma value for all channels    dataTrg[i] = dataTrg[i + 1] = dataTrg[i + 2] = luma,    dataTrg[i + 3] = dataSrc[i + 3], // copy alpha  }  // put back luma data so we can save it as image  ctx.putImageData(idataTrg, 0, 0),}```You can pass a canvas element reference```jscanvasEl = .... // React.useRef o vainilla javascriptgetFaceAndDescriptor(canvasEl).then((result)=>console.log(result))```====="", 'Thanks @punisher97 =====']"
https://github.com/justadudewhohacks/face-api.js/issues/657;Training faces descriptors of one person vs query face descriptor;3;open;2020-07-11T11:40:19Z;2020-07-11T17:04:39Z;Hello, I'm wondering  for one person with many training descriptors(training images) ,when we want to recognize a new query face image ,will the recognize method take the average of the training descriptors of every single person and compare it with the query face descriptor ? Thanks in advance.;"[""I don't believe it does a 'weighted average'. I believe it goes through each face given in the descriptors. You add more faces to ensure that you have better references for each person. What is your specific use case, if you can elaborate the same? ====="", ""> I don't believe it does a 'weighted average'. I believe it goes through each face given in the descriptors. You add more faces to ensure that you have better references for each person. What is your specific use case, if you can elaborate the same?I'm trying to understand how the model works to ensure I have the best performance.So I want to know if adding more faces to a person increases the recognition performance of that person or the model just compares the query face descriptor with all the face descriptors of that person (which means a bad picture might mislead the model as well ) ? ====="", ""When you add a reference image to the descriptor, and run an image through the api, the api does not know which person it is, and hence it will run through all the persons defined in the descriptor. Is parallel processing possible? If you have thousands of reference images, it might make sense to parallel process the same. I don't know, as I have not tried it. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/652;Error: FaceRecognizer.constructor - expected inputs to be of type LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array | Array<LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array>;6;open;2020-06-26T03:21:13Z;2020-07-09T09:51:14Z;The [documentation for FaceMatcher](https://github.com/justadudewhohacks/face-api.js#face-recognition-by-matching-descriptors) throws an exception in NodeJS. Although `faceapi.detectAllFaces` finds faces, `new faceapi.FaceMatcher(faceResults)` throws an exception. Can you please let us know what we should be doing here?Code:```  await faceapi.nets.ssdMobilenetv1.loadFromDisk(modelUrl)  await faceapi.nets.faceLandmark68Net.loadFromDisk(modelUrl)  await faceapi.nets.faceRecognitionNet.loadFromDisk(modelUrl)  const faceBase64 = 'data:image/jpg,base64,' + fs.readFileSync(`./tests/data/face1.jpg`, {encoding: 'base64'})  const faceImage = new Image()  faceImage.src = faceBase64  const faceResults = await faceapi.detectAllFaces(faceImage)      .withFaceLandmarks()      .withFaceDescriptors()  if (!faceResults.length) { // length is 1    return  }  const faceMatcher = new faceapi.FaceMatcher(faceResults) // --> Exception``````Error: FaceRecognizer.constructor - expected inputs to be of type LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array | Array<LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array>    at /Users/.../node_modules/face-api.js/src/globalApi/FaceMatcher.ts:40:13    at Array.map (<anonymous>)    at new FaceMatcher (/Users/.../node_modules/face-api.js/src/globalApi/FaceMatcher.ts:27:43)    at ...```;"[""I executed same code in my MacBook and it was working fine.```await faceapi.nets.ssdMobilenetv1.loadFromDisk('./weights')await faceapi.nets.faceRecognitionNet.loadFromDisk('./weights')await faceapi.nets.faceLandmark68Net.loadFromDisk('./weights')const faceBase64 = 'data:image/jpg,base64,' + fs.readFileSync('faces94.jpg', {encoding: 'base64'})const faceImage = new Image()faceImage.src = faceBase64const res = await faceapi.detectAllFaces(faceImage).withFaceLandmarks().withFaceDescriptors()const faceMatcher = new faceapi.FaceMatcher(res)console.log('faceMatcher:',faceMatcher)```**Output:**```faceMatcher: FaceMatcher {  _distanceThreshold: 0.6,  _labeledDescriptors: [    LabeledFaceDescriptors {      _label: 'person 1',      _descriptors: [Array]    }  ]}```**Node version:** v12.14.1**nmpm version:** 6.13.4====="", ""I don't know how it's working in Mac, but you are missing a step after:const faceResults = await faceapi.detectAllFaces(faceImage)      .withFaceLandmarks()      .withFaceDescriptors()You have to get the labelled descriptors. It could look something like this:             const faceResults = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors()            if(faceResults ) {                               descriptions = [faceResults .descriptor]            }            const singlelabeledDescriptor = new faceapi.LabeledFaceDescriptors(label,descriptions )Let me know if this helps. ====="", 'Hi @Karthik-tr , thank you for pointing out. I also faced same error after adding your code. Actually `faceResults` is a list with one item and not a json object. So to access `descriptor`, you need to specify index like `faceResults[0].descriptor`Try below code, it should work.```const faceResults = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors()if(faceResults ) {   descriptions = [faceResults[0].descriptor]}const singlelabeledDescriptor = new faceapi.LabeledFaceDescriptors(label,descriptions )```=====', 'Ah my use case was for single face detection and hence it was not an array. Missed that. Thanks for pointing it out. Glad it worked. =====', 'If single face, api should be `detectSingleFace` instead of `detectAllFaces`. In that case, it may not return array and your suggested code may work for that. =====', 'Yes i use a single face detector. Hence I made the mistake of assuming that the descriptor only has one object. Mine works well. Thanks for the inputs =====']"
https://github.com/justadudewhohacks/face-api.js/issues/650;Retrain the facelandmark68net Model;1;open;2020-06-25T11:15:16Z;2021-06-10T19:47:02Z;First of all, thanks for this fantastic api to build browser based apps. I just want to know how I can retrain facelandmark68net model? I have a new dataset and have annotated different points. is it even possible to retrain the model?;['have you reached a solution regarding the retraining part ? I am having the same issue.=====']
https://github.com/justadudewhohacks/face-api.js/issues/647;Error when detecting face;3;open;2020-06-22T16:00:26Z;2021-08-23T14:54:38Z;"Occassionally, we receive an error when attempting to detect a face in a still image, we receive the following error:Uncaught (in promise) Error: Box.constructor - expected box to be IBoundingBox | IRect, instead have {""left"":211.82223243423,""top"":188.34445656565,""right"":299.35465223232,""bottom"":null}at BoundingBox.Boxat new BoundingBoxat minBboxat FaceLandmarks68.FaceLandmarks.alignMinBbox...It appears that it's missing a value for the bottom of a bounding box, but I'm not sure why. Any suggestions?";"['I was having this error when running the html served in a node environment. Running it from traditional apache works fine.=====', 'Hi @ScottDellinger Did you ever manage to find a solution for this issue?=====', ""> Hi @ScottDellinger Did you ever manage to find a solution for this issue?No... I stopped using this library because it's been abandoned and we found the models it was trained with did not perform well on Asian or Middle Eastern faces. There is a more active fork of it, though.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/646;Detect(single|all)Face intermittently only returns undefined on certain browsers;3;open;2020-06-22T10:26:45Z;2020-06-29T21:04:18Z;I've been experiencing this for a while, in my case I can replicate on Chrome on Ubuntu.. I can load the page 10 times, and maybe 4 out of those 10 times, Detecting faces doesn't work and returns undefined. I can lean into the camera, etc but it never detects a face (When its running in a loop at 1 second intervals) However, I can refresh, and then it works fine, same code.. I can keep refreshing and it stops again. So far I have only ever seen it happen in chrome but due to its intermittent nature its hard to replicate. On the same PC I switch to FireFox and cannot replicate. There are no errors in the console, and literally no clues! I'm posting in the hope someone else may have experienced, or any pointers of what to check / where to debug. I'm using ReactJS;"[""Perhaps you're facing the some problem I had. Depending on your face position inside the picture, the library will not return a valid result. Try to duplicate the image width, leaving the original picture at left.====="", ""I am not seeing this as ur use case is something that we do as well. Capture face every 1 second. We have done this for upto 5 mins and have not seen a loss of data as mentioned. Maybe it is Ubuntu + Chrome + camera combination. I don't use Ubuntu but have tested it on Mac and windows. No issues as you mentioned. ====="", ""I'm thinking something in React is causing this. I think I'll pull down the dev version of faceAPI and put debug statements everywhere! =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/644;Unhandled Rejection (TypeError): t.batchNormalization is not a function;4;open;2020-06-19T17:52:42Z;2020-12-07T17:31:12Z;"I am getting the following error when trying to recreate the example code for bbt. Unhandled Rejection (TypeError): t.batchNormalization is not a functionMy packages are:   ""dependencies"": {    ""@tensorflow-models/coco-ssd"": ""^0.1.1"",    ""@tensorflow/tfjs"": ""^2.0.1"",    ""@tensorflow/tfjs-core"": ""^2.0.1"",    ""@tensorflow/tfjs-data"": ""^2.0.1"",    ""@testing-library/jest-dom"": ""^4.2.4"",    ""@testing-library/react"": ""^9.5.0"",    ""@testing-library/user-event"": ""^7.2.1"",    ""bootstrap"": ""^4.5.0"",    ""capture-video-frame"": ""^0.1.3"",    ""face-api.js"": ""^0.22.2"",    ""react"": ""^16.13.1"",    ""react-bootstrap"": ""^1.0.1"",    ""react-bootstrap-range-slider"": ""^1.0.0"",    ""react-dom"": ""^16.13.1"",    ""react-player"": ""^2.3.0"",    ""react-redux"": ""^7.2.0"",    ""react-scripts"": ""3.4.1"",    ""react-thunk"": ""^1.0.0"",    ""react-webcam"": ""^5.1.0"",    ""redux"": ""^4.0.5"",    ""tslib"": ""^2.0.0"",    ""typescript"": ""^3.9.5""  },";"['face-api is not yet compatible with tfjs 2.0, you have to use older 1.7.x branch.=====', '> face-api is not yet compatible with tfjs 2.0, you have to use older 1.7.x branch.It worked for me with these versions.```""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.7.0"",    ""aws-sdk"": ""2.785.0"",    ""canvas"": ""^2.6.1"",    ""face-api.js"": ""^0.22.2"",    ""gm"": ""1.23.1"",    ""sharp"": ""0.26.3"",    ""smartcrop-sharp"": ""2.0.3""  }```=====', 'since this thread is still alive, I might mention that I did a full port of `face-api` to tfjs 2.x (including updated models as well as entire toolchain and typescript and ecma standards).  if you want, check out <https://github.com/vladmandic/face-api>=====', '> since this thread is still alive, I might mention that I did a full port of `face-api` to tfjs 2.x (including updated models as well as entire toolchain and typescript and ecma standards).> > if you want, check out https://github.com/vladmandic/face-apiAwesome!! thanks=====']"
https://github.com/justadudewhohacks/face-api.js/issues/642;Uncaught (in promise) Error: Based on the provided shape, [25], the tensor should have 25 values but has 21;3;open;2020-06-18T22:30:11Z;2021-03-27T10:31:30Z;"I have a problem starting the online application in that part:```Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri('./assets/lib/face-api/models'),    faceapi.nets.faceLandmark68Net.loadFromUri('./assets/lib/face-api/models'),    faceapi.nets.faceRecognitionNet.loadFromUri('./assets/lib/face-api/models'),    faceapi.nets.faceExpressionNet.loadFromUri('./assets/lib/face-api/models'),    faceapi.nets.ageGenderNet.loadFromUri('./assets/lib/face-api/models'),    faceapi.nets.ssdMobilenetv1.loadFromUri('./assets/lib/face-api/models')	]).then(startVideo)```In the console returns the following error:```face-api.js:23 Uncaught (in promise) Error: Based on the provided shape, [25], the tensor should have 25 values but has 21    at C (face-api.js:23)    at _n (face-api.js:23)    at Fn (face-api.js:23)    at o (face-api.js:23)    at cf (face-api.js:23)    at face-api.js:23    at Array.forEach (<anonymous>)    at face-api.js:23    at Array.forEach (<anonymous>)    at face-api.js:23```If I put the command `console.log (faceapi.nets)` on the console the following appears:```{ssdMobilenetv1: SsdMobilenetv1, tinyFaceDetector: TinyFaceDetector, tinyYolov2: TinyYolov2, mtcnn: Mtcnn, faceLandmark68Net: FaceLandmark68Net, …}ageGenderNet: AgeGenderNet {_name: ""AgeGenderNet"", _params: undefined, _paramMappings: Array(0), _faceFeatureExtractor: TinyXception}faceExpressionNet: FaceExpressionNet {_name: ""FaceExpressionNet"", _params: undefined, _paramMappings: Array(0), _faceFeatureExtractor: FaceFeatureExtractor}faceLandmark68Net: FaceLandmark68Net {_name: ""FaceLandmark68Net"", _params: undefined, _paramMappings: Array(0), _faceFeatureExtractor: FaceFeatureExtractor}faceLandmark68TinyNet: FaceLandmark68TinyNet {_name: ""FaceLandmark68TinyNet"", _params: undefined, _paramMappings: Array(0), _faceFeatureExtractor: TinyFaceFeatureExtractor}faceRecognitionNet: FaceRecognitionNet {_name: ""FaceRecognitionNet"", _params: undefined, _paramMappings: Array(0)}mtcnn: Mtcnn {_name: ""Mtcnn"", _params: undefined, _paramMappings: Array(0)}ssdMobilenetv1: SsdMobilenetv1 {_name: ""SsdMobilenetv1"", _params: undefined, _paramMappings: Array(0)}tinyFaceDetector: TinyFaceDetector {_name: ""TinyYolov2"", _params: undefined, _paramMappings: Array(0), _config: {…}}tinyYolov2: TinyYolov2 {_name: ""TinyYolov2"", _params: undefined, _paramMappings: Array(0), _config: {…}}__proto__: Object```I don't know where it can be wrong or what is wrong, at localhost it worked, I put it online to test it and it just returns me this error. Can you help me? Thank you";"['I get exactly the same error only online not localhost=====', ""@nicolau-arbex @XavierLGLS Try this: https://github.com/justadudewhohacks/face-api.js/issues/131#issuecomment-561738466It worked for me.Also, when using `loadFromUri` to load the nets, don't use a file spec, use a uri spec, ie:`faceapi.nets.tinyFaceDetector.loadFromUri('assets/lib/face-api/models')`or`faceapi.nets.tinyFaceDetector.loadFromUri('/assets/lib/face-api/models')`If you want load the nets from files, you must use `LoadFromDisk`, check the docs for more info:https://justadudewhohacks.github.io/face-api.js/docs/index.html====="", 'Indeed, like @nosuko said, the solution is in this comment : https://github.com/justadudewhohacks/face-api.js/issues/131#issuecomment-561738466Your shard file needs an extension (and change the name with the extension in the json file too)Worked for me !=====']"
https://github.com/justadudewhohacks/face-api.js/issues/636;how to detect fake face?;2;open;2020-06-09T16:52:14Z;2020-06-14T17:03:07Z;awesome work !!!1. but how to do some stuff only on detections,  as it was defined on setInterval, like showing capture button etc., may cause loop effect and 2. how to check the realness of video ? is it possible to do eye blink detection and mobile screen detectionplease assist :);"['Is there any solution for this ?=====', 'display button part is easy, instead of creating we can pre-create before interval and display is using if condition like this``        let video = $(this)[0],        const canvas = faceapi.createCanvasFromMedia(video)        const displaySize = {width:video.videoWidth,height:video.videoHeight}        faceapi.matchDimensions(canvas,displaySize)        $(canvas).appendTo(""#camera .row:first-child"")        let detections,        let loop = setInterval(async()=>{            detections = await faceapi.detectAllFaces(video,new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks()            const resizedDetections = faceapi.resizeResults(detections,displaySize)            canvas.getContext(\'2d\').drawImage(video,0,0)            //faceapi.draw.drawDetections(canvas,resizedDetections)            faceapi.draw.drawFaceLandmarks(canvas,resizedDetections)            if(detections.length>0){                $(""#capt"").show(),                console.log(""face detected"")            }else{                $(""#capt"").hide(),            }        },108)``but don\'t know how to identify fake face,can we detect eye blink with facelandmark 68 model ? if so we can win over the **2d face faking** found this article is it possible to implement in face-api.jshttps://medium.com/swlh/anti-spoofing-mechanisms-in-face-recognition-based-on-dnn-586011ccc416=====']"
https://github.com/justadudewhohacks/face-api.js/issues/633;solved: face-api is not compatible with tfjs 2.x or tfjs 3.x;21;open;2020-06-08T14:38:39Z;2021-12-12T13:09:22Z;Now that TFJS 2.0 has been released, are there any plans to update face-api models to be compatible with it?Currently it fails due to batchNormalization being obsoleted in tfjs 2.0.I'm using tfjs with multiple models on the same image to classify/detect all different types of objects (not just faces) and since concurrently loading multiple different versions of tfjs is not possible thus I cannot upgrade entire project because of a face-api dependency on tfjs 1.x.;"['True that!```Unhandled Rejection at: TypeError: backend.batchNormalization is not a function    at engine_1.ENGINE.runKernelFunc.x (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm.js:280:27)    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3229:55    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at kernelFunc (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3229:29)    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3240:27    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.runKernelFunc (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3238:14)    at batchNorm_ (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm.js:279:31)    at Object.batchNorm (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:46:29)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:9:18    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at depthwiseConvLayer (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:7:15)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:38:19    at Array.forEach (<anonymous>)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:35:24    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at Object.mobileNetV1 (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:17:15)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:29:42    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at SsdMobilenetv1.forwardInput (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:26:19)    at SsdMobilenetv1.<anonymous> (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:58:35)    at step (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:139:27)    at Object.next (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:120:57)    at fulfilled (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:110:62) {}```=====', 'Is there any solution/work-around for this?=====', 'same here, unable to upgrade to 2.0.1 version=====', 'I have same issues like @igorescobar=====', 'Check to see if you have a `node_modules/` inside the node_modules/face-api.js folder -- this can have the older version of the tensor library and then they will conflict, because the face-api will initially load this version which then conflicts with the version in the main node_modules folder...   Once I nuked this extra node_modules folder, everything worked fine w/ v2.x=====', ""> Check to see if you have a `node_modules/` inside the node_modules/face-api.js folder -- this can have the older version of the tensor library and then they will conflict, because the face-api will initially load this version which then conflicts with the version in the main node_modules folder... Once I nuked this extra node_modules folder, everything worked fine w/ v2.xCan you elaborate on that? IMO, loading any specific version of TFJS is not a problem, but if TFJS 2.0 is loaded Face-API will not work since TFJS 2.0 obsoleted some functions - primarily batchNormalization(). And that is a dependency inside models, so models need to be recompiled, it's not just a library code issue.====="", ""Sometimes NPM is stupid, so what happens is you have this:[YourProject]-> [node_modules]---> [...Your other package.json modules...]---> [@tensorflow/ ] (All v2.x)---> [faceapis.js]---------> [node_modules]-----------------[@tensorflow/] (v1.x)So when faceapi loads tensorflow it loads the v1.0 library from its node_modules, which then when it attempts to load tsflow-node, it gets the v2.0 from the  root @tensorflow 2.0 version.   Of course v1 doesn't work with v2 and you get that nice cryptic error message.   If you NUKE/DELETE the [node_modules] under the faceapi.js folder, then when faceapi.js goes to load tensorflow it will load the 2.0 version from the root node_modules, and that 2.0 will load the tsflow-node which is also 2.0 and everything will work.  ====="", 'As I said, the issue is not forcing load of TFJS 2.0 - even if you do that, the models used by face-api.js rely on batchNormalization() function that was deprecated and removed in TFJS 2.0 - see: <https://github.com/tensorflow/tfjs/pull/3238> and <https://github.com/tensorflow/tfjs/issues/3232>Anyhow, I\'ve tried your method (and few others)...1. Nuke private tfjs within face-api.js so it tries to use tfjs 2.0 from node_modules/@tensorflow```rm -rf node_modules/face-api.js/node_modules/@tensorflow```As expected, it doesn\'t load TFJS at all, it just fails on first usage of any method since instance is undefined.```""Cannot read property \'fetch\' of undefined""```2. Update face-api.js dependencies in-place```cd node_modules/face-api.js../.bin/ncu -u  @tensorflow/tfjs-core        1.7.0  →     2.3.0  tslib                      ^1.11.1  →    ^2.0.1  @tensorflow/tfjs-node        1.7.0  →     2.3.0  @types/jasmine              ^3.5.9  →   ^3.5.12  @types/node                ^13.9.2  →  ^14.0.27  jasmine                     ^3.5.0  →    ^3.6.1  jasmine-core                ^3.5.0  →    ^3.6.0  karma                       ^4.4.1  →    ^5.1.1  karma-jasmine               ^3.1.1  →    ^4.0.1  karma-typescript            ^5.0.1  →    ^5.1.0  rollup                      ^2.1.0  →   ^2.26.2  rollup-plugin-typescript2  ^0.26.0  →   ^0.27.2  ts-node                     ^8.7.0  →   ^8.10.2  typescript                  ^3.8.3  →    ^3.9.7npm i```Result is the same as in first case.3. Download and rebuild face-api.js from sources, not npm package```git clone https://github.com/justadudewhohacks/face-api.jscd face-api.js/npm i ```This will do a recompile which may fail due to missing system dependencies, in which case install them manually. E.g.:```sudo apt install libjpeg-dev libgif-dev```Then force package update and do a full rebuild```npm i npm-check-updatesnode_modules/.bin/ncu -u  Upgrading face-api.js/package.json  @tensorflow/tfjs-core        1.7.0  →     2.3.0  tslib                      ^1.11.1  →    ^2.0.1  @tensorflow/tfjs-node        1.7.0  →     2.3.0  @types/jasmine              ^3.5.9  →   ^3.5.12  @types/node                ^13.9.2  →  ^14.0.27  jasmine                     ^3.5.0  →    ^3.6.1  jasmine-core                ^3.5.0  →    ^3.6.0  karma                       ^4.4.1  →    ^5.1.1  karma-jasmine               ^3.1.1  →    ^4.0.1  karma-typescript            ^5.0.1  →    ^5.1.0  rollup                      ^2.1.0  →   ^2.26.2  rollup-plugin-typescript2  ^0.26.0  →   ^0.27.2  ts-node                     ^8.7.0  →   ^8.10.2  typescript                  ^3.8.3  →    ^3.9.7npm inpm run build```Fails, so it\'s not just models that need work```  Error: /home/vlado/dev/face-api.js/src/dom/NetInput.ts(131,72): semantic error TS2344: Type \'Rank.R4\' does not satisfy the constraint \'Tensor<Rank>\'.```=====', 'Hi,I had initially followed the sites instructions and did this for my project:`npm i face-api.js canvas @tensorflow/tfjs-node`Here is what I\'m running:node: v12.16.3tsc: V3.8.3All I can report is what worked for me.   I got the exact same message as `Unhandled Rejection at: TypeError: backend.batchNormalization is not a function` the very first time I ran the examples, and so that is how I found this issue.  :grinning:The `npm i` Unfortunately,  creates the nested node_modules because the face-api.js `package.json` file asks for `@tensorflow/tfjs-core"": ""1.7.0""` however the npm i @tensorflow/tfjs-node part get whatever is the latest TF (which in my case was 2.1).My projects node_modules contains these versions of libraries- @tensorflow/tfjs@2.1.0- @tensorflow/tfjs-backend-cpu@2.1.0- @tensorflow/tfjs-backend-webgl@2.1.0- @tensorflow/tfjs-converter@2.1.0- @tensorflow/tfjs-core@2.1.0- @tensorflow/tfjs-data@2.1.0- @tensorflow/tfjs-layers@2.1.0- @tensorflow/tfjs-node@2.1.0- face-api.js@0.22.2- tslib@1.13.0Originally under the `node_modules/face-api.js` their was another `node_modules` with the v1.7 tensorflow module, this is what I nuked.If I take any of the node-js examples that are on this repo, build them with TS, and then run them with node they ALL work.![image](https://user-images.githubusercontent.com/850871/90366068-80a98e80-e02c-11ea-9662-26f1f8f08cf1.png)(This is my out directory)If I add code to any of the demos to output the TensorFlow version, this is what I get:![image](https://user-images.githubusercontent.com/850871/90366363-d3834600-e02c-11ea-9eaf-412b73b71b4c.png)The code I\'m using to get the version is I just add this to the very top of any of the examples to output TensorFlow version...```const tf = require(\'@tensorflow/tfjs\'),console.log(`Initializing TensorFlow/JS version ${tf.version_core}`),```As you can see from the output, IT is using TensorFlow 2.1, Node is loading TF 2.1 with face-api, and face-api is using it.    Now to triple check, If I manually edit the node_modules/face-api.js/build/commonjs/index.js and add the exact same console.log right under where the face-api loads tensorflow like so:![image](https://user-images.githubusercontent.com/850871/90368687-14308e80-e030-11ea-9d3e-f85502e4d25d.png)I also get the exact same output:![image](https://user-images.githubusercontent.com/850871/90368782-3e824c00-e030-11ea-866f-e0a62d5b69cc.png)Once from the added code in the example, and once from when face-api loads tensorflow.=====', ""Ok, I dug a bit deeper...  face-api.js includes multiple models:  - **tinyFaceDetector**: This one WORKS with TFJS 2.0+. This is the model that is used in most of examples anyhow.- **ssdMobilenetv1**: This one depends on obsolete function batchNorm() so no chance to make it work with TFJS 2.0. I used this model since in my testing it seemed to have better precision than tiny (although a touch slower). - **mtcnn**:  Mostly obsolete, but still present- **tinyYolov2**: Code is there, but weights are missing since forever, so I guess this is more of an abandonware.  So if you're ok using tinyFaceDetector, TFJS 2.0+ works just fine.  You could nuke local node_modules like NathanaelA stated or you can upgrade face-api.js packages.json and make it use newer tfjs or anything else.  ====="", ""FYI, I've forked face-api.js, updated it for tfjs 2.0, cleaned up obsolete code and modernized build process.  And it's about 2.5x smaller than original.See <https://www.npmjs.com/package/@vladmandic/face-api> if you want to use it instead of original face-api.js.  There are no changes except what's noted, so original docs still apply.  ====="", '@vladmandic - You should drop a PR so that this can get fixed...  :D =====', ""@NathanaelA I'd be happy to write a PR for minor code changes and update dependencies, but it's really up to the author to clean up models - as it is, there is code for 4 different models and only 1 is working - removing that much code is not something that should be done in a PR.Also, build process itself is quite old and targeting old standards which did cause some issues for me when loading face-api concurrently with tfjs itself in a different model.That's why I did a fork, not a PR.====="", 'I find the accuracy of the `tinyFaceDetector` model not good for my use-case compared to the `ssdMobilenetv1` model. For those who want still to use the `ssdMobilenetv1` model, the workaround is downgrading your `tfjs` to version `1.7.0`.I hope this gets fixed soon.cc. @justadudewhohacks =====', ""@focux I agree that ssdMobilenetv1 is generally a bit better than tinyFaceDetector and I wish author retrains model to be compatible with tfjs@2.0+ although not sure how much can be done since mobilenet v1 is a really old model. Ideally, it should be trained with either ssd & mobilenet v2 (that would be simplest as it's very close to existing model) or a newer & better alternative.i've noticed some work has been done in a separate branch <https://github.com/justadudewhohacks/face-api.js/tree/face_detection_save>, but there are no weights and it hasn't been updated in 3 months.btw, if you really need ssdMobilenetv1, use latest tfjs@1.7.4, not 1.7.0 as there are more than few bugfixes and v1.7.4 is final version in 1.x branch.i'll reopen this issue so it can be monitored...====="", 'One more note, ssd_mobilenetv1 model actually comes from a diferent project, <https://github.com/yeephycho/tensorflow-face-detection>.  And that project has actually been updated for tfjs@2.0+ compatibility.However, latests weighs released there <https://drive.google.com/open?id=0B5ttP5kO_loUdWZWZVVrN2VmWFk> are in TF Frozen format.  They do convert nicely to TF Graph model (and optionally quantize for reduced size), ```shell> summarize_graph \\  --in_graph=""./frozen_inference_graph_face.pb"" \\  --print_structure=falseFound 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity)> tensorflowjs_converter \\  --input_format tf_frozen_model \\  --output_format tfjs_graph_model \\  --skip_op_check \\  --strip_debug_ops=True \\  --quantize_uint16 \\  --weight_shard_size_bytes 4194304 \\  --output_node_names detection_boxes,detection_scores,num_detections \\  ./frozen_inference_graph_face.pb \\  ./converted/```but...face-api.js model-weights list to iterate and load weights, not full model.json.If someone wants to dig deeper, I\'m sure this can be used, I just don\'t have any more time for this.As it is, tinyFaceDetector works as-is and ssdMobilenetv1 can probably be made to work. Other models are obsolete.=====', 'Done!I\'ve just pushed updated package to <https://www.npmjs.com/package/@vladmandic/face-api>Both tinyFaceDetector and ssdMobileNetv1 work:```jsModel: {name: ""FaceAPI SSD/MobileNet v1"", modelPath: ""models/faceapi/"", exec: ""ssd"", score: 0.3, topK: 1, size: 416 },Result: {  age: 21.586017608642578  expressions: FaceExpressions {...}  gender: ""female""  genderProbability: 0.960713230073452  alignedRect: FaceDetection {...}  descriptor: Float32Array(128) [...]  detection: FaceDetection {...}  landmarks: FaceLandmarks68 {...}}{ name: \'FaceAPI TinyYoloDetector\', modelPath: \'models/faceapi/\', exec: \'yolo\', score: 0.3, topK: 1, size: 416 },Result: {  age: 24.492658615112305  expressions: FaceExpressions {...}  gender: ""female""  genderProbability: 0.964848417788744  alignedRect: FaceDetection {...}  descriptor: Float32Array(128) [...]  detection: FaceDetection {...}  landmarks: FaceLandmarks68 {...}}```=====', ""Wow! That's awesome, great work @vladmandic, I will definitely use your package. Thank you.====="", ""Awesome work @vladmandic thank you for sharing! Can I buy you a beer/do you have a tip jar anywhere? :D(and @justadudewhohacks too if you're still around 😃 )====="", 'no tip jar, just having fun - enjoy!=====', 'Thank youuuuuuuuu very much 💕💕💕💕💕=====']"
https://github.com/justadudewhohacks/face-api.js/issues/627;tsc can't compile examples;1;open;2020-05-31T04:40:22Z;2021-03-04T22:43:46Z;When running the node examples with ts-node, all is good, but if I want to compile with tsc I'm getting the following error:tsc faceDetection.ts```../../node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 556 more ..., readonly WAIT_FAILED: number, }', but here has type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 557 more ..., readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number, }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../../../usr/local/lib/node_modules/typescript/lib/lib.dom.d.ts:16316:13    16316 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```So WebGL2RenderingContext is being declared in two separate parts...Thanks;['for those who come here in the future:https://github.com/tensorflow/tfjs/issues/2007=====']
https://github.com/justadudewhohacks/face-api.js/issues/624;How to use it with grayscale images?;1;open;2020-05-26T19:04:40Z;2021-06-05T17:15:10Z;Now I have an error:>UnhandledPromiseRejectionWarning: Error: Size(262144) must match the product of shape 512,512,3>console.log(img.shape)[ 209, 140, 1 ]There is [grayscale_to_rgb](https://www.tensorflow.org/api_docs/python/tf/image/grayscale_to_rgb) In python version of tensorflow but in js version isn't.;['Is this issue solved? I am facing the same problem.=====']
https://github.com/justadudewhohacks/face-api.js/issues/621;How to load model in Ionic react;1;open;2020-05-20T19:29:02Z;2021-02-06T17:09:29Z;"First of all, thank you for this awesome plugin. I am using face-api.js in my Ionic react project to detect face from image clicked by user. however the detection canvas only works in browser ionic serve and not working after i compiled it into Android apk and IOS app. Please help me on this.export const loadModels = () => {  const MODEL_URL = `${process.env.PUBLIC_URL}/models`,  return Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),    faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),  ]),},I also, try using **loadModel**(see below) and using **loadFromDisk**, but so far no luck    const MODEL_URL = process.env.PUBLIC_URL + ""/models"",  await faceapi.loadTinyFaceDetectorModel(MODEL_URL),  await faceapi.loadFaceLandmarkTinyModel(MODEL_URL),  await faceapi.loadFaceRecognitionModel(MODEL_URL),If anybody with similar issue and have idea how to resolve this. Can you please let me know where I am doing wrong? I appreciate your help Thank you";['Have you tried changing the filename extension of the model file and the paths value of model-weights_manifest.json to .bin?=====']
https://github.com/justadudewhohacks/face-api.js/issues/620;face-mask implementation;7;open;2020-05-19T20:38:14Z;2021-04-04T08:58:08Z;Hi,first of all, thank you for this awesome plugin...cheers!!!I just want to know, how I can detect face with a mask or not? do i need to create new model for this. I am trying to understand the manifest JSON files and shard files. can you give me some tips!!!;"[""i'm in the same wave... I suppose that it is necessary to train a net similar to face expression or perhaps swap this net with a facemask net....How to train a new net?====="", '+1 to this issue. Did you guys were able to figure that out?Thanks!=====', '+1. Also looking for a face-with-mask detection solution with face-api.js.   Would be great.=====', 'Hi there i have a facemask model trained, now just trying figure how i setup that into faceapi for concat the results. Any help will be welcome guys.Great proyect thank you for the efford=====', '@raulocho Did you already managed to setup your trained model into faceapi?=====', '@aveloso4 Yes exactly, sorry for delay answering, had a bad time that weeks with work. I have the model ready just need take the face from face api and concat the result in the analysis Json.=====', '@raulocho  Can u show some code about how to use your own mask model with face-api.js ?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/618;TinyYolov2 - load model before inference;6;open;2020-05-08T10:01:11Z;2021-07-30T16:34:12Z;I am getting this error in production build in React Js and working fine when use npm start.```javascriptimport * as faceapi from 'face-api.js'export async function loadModels() {    const MODEL_URL = process.env.PUBLIC_URL + '/models'    await faceapi.loadTinyFaceDetectorModel(MODEL_URL)    await faceapi.loadFaceLandmarkTinyModel(MODEL_URL)    await faceapi.loadFaceRecognitionModel(MODEL_URL)}export async function getFullFaceDescription(blob, inputSize = 512) {    const scoreThreshold = 0.5    const OPTION = new faceapi.TinyFaceDetectorOptions({        inputSize,        scoreThreshold    })    const useTinyModel = true    const img = await faceapi.fetchImage(blob)    const fullDesc = await faceapi        .detectAllFaces(img, OPTION)        .withFaceLandmarks(useTinyModel)        .withFaceDescriptors()}```Please help me on this.;"[""I saw the same error in a situation where I wasn't even using the Yolo model, which was confusing.In the end it was because I had made a coding error, and the models I did want to use weren't actually loaded. Perhaps there is something in the FaceApiJs code where it throws a wrong error for a missing model, and ends up outputting the last name in the list of nets.====="", 'Hi @swapanil  ,I am faced with same issue. We have same code block. Have you solve your problem? =====', ""Hey @swapanil ,FYI, I solved my problem. I couldn't load models correctly. Can u check your models location? It should be under the public folder as follows, public/models====="", 'I made it sure my all models are loading correctly infact my TinyYolov2 json and shard file loading correctly but still it showing me same issue, I replace it with SsdMobilenetv1Options it is working for now. I think this should be a bug.=====', 'I added this and issue resolved`faceapi.nets.tinyFaceDetector.loadFromUri(this.MODEL_URL),`=====', ""This is probably due to failure to load the models you can try loading them like this insteadload from url `export async function loadModels() {    const MODEL_URL = process.env.PUBLIC_URL + '/models'    await faceapi.nets.loadTinyFaceDetectorModel.loadFromUri(MODEL_URL)    await faceapi.nets.loadFaceLandmarkTinyModel.loadFromUri(MODEL_URL)    await faceapi.nets.loadFaceRecognitionModel.loadFromUri(MODEL_URL)}`you can also load them from the disk`await faceapi.nets.loadTinyFaceDetectorModel.loadFromDisk('./models')`  =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/614;The distance between the eyes;2;open;2020-05-04T18:02:55Z;2021-06-09T19:30:42Z;Hello, thank you to the authors for this nice library.Is it possible to calculate the distance between the eyes (pupils) in centimeters?We are trying to use this library in healthcare to detect squinting.Thank you very much;"[""this would be quite complex with a regular webcam. The web browser will have great difficulty figuring out the camera properties like focal length, sensor size, etc, as well as accurately estimating how far the user's face is from the camera. It has no information to detect the difference between a small head and a far away head accurately.To achieve something like this, I think you would need to have the user wear something that provides a reference scale, like a set of eyeglasses that have tracking dots a known distance apart. Then you could measure the distance between the trackers, and scale the distance between the eye pupils to a relative amount within that scale.If you will only be running the software in a situation where the user's face is a known distance from the camera, using a known camera (i.e. a workstation PC with special equipment, or a standard issue smartphone) you might be able to achieve something decent without a reference scale, but I think it will be difficult to do well.If it was easy to measure IPD in real measurements using a regular webcam, the online eyeglasses companies that have AR eyeglasses try on features would surely be doing it.====="", '@Bluebie  ok, thank you :)=====']"
https://github.com/justadudewhohacks/face-api.js/issues/608;Angular Universal - Platform browser has already been set. Overwriting the platform with [object Object].;4;open;2020-04-28T21:10:01Z;2020-11-27T07:37:14Z;```Platform browser has already been set. Overwriting the platform with [object Object].```This is happening to me lately in Angular Universal, the difference with angular that this executes processes at the server level, using Node.js, so how do I make it work, since I am overwriting the platform with this function:```e.setPlatform | @ | face-api.min.js:1-- | -- | --  | ./node_modules/@tensorflow/tfjs-core/dist/tf-core.esm.js | @ | tf-core.esm.js:17  | __webpack_require__ | @ | bootstrap:79  | ./node_modules/face-api.js/build/es6/index.js | @ | index.js:1  | __webpack_require__ | @ | bootstrap:79  | ./src/app/components/verificacion/verificacion-facial/verificacion-facial.component.ts | @ | verificacion-dactilar.component.ts:31  | __webpack_require__ | @ | bootstrap:79  | ./src/app/app-routing.module.ts | @ | app-config.ts:60  | __webpack_require__ | @ | bootstrap:79  | ./src/app/app.module.ts | @ | app.component.ts:10  | __webpack_require__ | @ | bootstrap:79  | ./src/main.ts | @ | environment.ts:18  | __webpack_require__ | @ | bootstrap:79  | 0 | @ | main.ts:14  | __webpack_require__ | @ | bootstrap:79  | checkDeferredModules | @ | bootstrap:45  | webpackJsonpCallback | @ | bootstrap:32  | (anonymous) | @ | main.js:1```;"['I had same trouble when i used tfjs-react-native package on my device. My app just had closes when i tried use something from this package. In console was only warning ""Platform browser has already been set. Overwriting the platform with [object Object]"".This was solved by setting backend before used tfjs-react-native. Maybe it will help you too.``` javascriptawait tf.setBackend(\'cpu\'),```=====', '@fomin-max does face-api can be used in react-native ?=====', '@gravestar i think yes, you cani suggest you read tutorials from https://github.com/justadudewhohacks/face-api.js/and i found already ready solution on android with react-native https://github.com/aboozaid/react-native-facerecognitioni have similar working project with blazeface and posenet tensorflow-models https://github.com/fomin-max/react-native-tfjsin RealTimeDetection.tsx component you can see how models are appliedhope it will help for you!=====', '@fomin-max well, my goal is to get face recognition from this, from what i read, face-api store data from descriptors, while what im getting from blazeface / facedetector is X,Y,and im still confused about it, how to store the data and predict itim trying using KNN to store X,Y data, and predict but the result not makes me happy xD=====']"
https://github.com/justadudewhohacks/face-api.js/issues/605;poor accuracy on macbook with master, but online demo looks good;7;open;2020-04-27T00:20:52Z;2020-05-20T08:07:38Z;"On a macbook running the examples from the master branch, I get poor accuracy on face detection (see screenshots attached). Is the online demo at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection running current code? Also would note that I get good accuracy on my linux computer working against the master branch. My linux laptop has an intel video card, and my macbook has both an nvidia and intel gpu.<img width=""877"" alt=""master_branch_example"" src=""https://user-images.githubusercontent.com/1912197/80323686-eb735780-87fa-11ea-841d-1d97657ceb59.png""><img width=""811"" alt=""online_example"" src=""https://user-images.githubusercontent.com/1912197/80323687-ec0bee00-87fa-11ea-8bd8-7386cbef2fc2.png"">";"[""The big question I have is should I be able to achieve the results I see on the online demo myself? Things work great for me on that site, but I'm not able to reproduce those results locally, including the examples in the repo. Is the source code available for the server running at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/ ?====="", ""> The big question I have is should I be able to achieve the results I see on the online demo myself? Things work great for me on that site, but I'm not able to reproduce those results locally, including the examples in the repo. Is the source code available for the server running at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/ ?Agree, I have the same issues and same question. Trying the example/demo site there is no indication of faces being detected at all. ![2020-04-27_17-55-27](https://user-images.githubusercontent.com/11222718/80347985-75c9c300-88b0-11ea-8039-f074adae3374.jpg)====="", 'I did some more research on this. For me, things work well on version 0.21.0. They start to break on 0.22.0. Is that the same for you?=====', 'Can you guys guide, i need api, to compare 2 images from url and send the result back instead of using it from form.=====', 'Hi, have anyone found the issue. I also facing same with multiple records in DB, when the pic only contains clear face image and not any background/shape, the accurate results coming, else wrong matches. I am using 0.22.2 in Nodejs app=====', ""I've switched to using 0.21.0.  Far from an ideal solution. The commit to upgrade to 0.22.0 is quite large, and I haven't had the time to dig through there to look for the issue.I would be curious if the downgrade fixes things for someone else as well. If there are multiple people for whom the downgrade works, it would make me more inspired to dig into the codebase myself.====="", 'Thanks for the suggestion, Kevin. I will try downgrading to that version and let you know.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/604;Node js real time;5;open;2020-04-24T20:04:04Z;2020-07-01T05:39:50Z;"Hello,thank you very much for this library!I'm trying to run a small face detection program in node js. It takes as input a frame read from the webcam and output the detection object.I see that the browser version does realtime detection while with the node version it takes around 350ms per frame which is very low compared.I took some measures of just the prediction step```console.time(""detect"")const detections = await faceapi.detectAllFaces(frame)console.timeEnd(""detect"")```detect: 442.732msdetect: 363.256msdetect: 365.847msdetect: 338.513msdetect: 334.374msdetect: 340.412msdetect: 324.549msdetect: 333.849msdetect: 327.226msdetect: 325.315msdetect: 322.517msdetect: 322.398msI am already importing the tfjs-node library and I am running the program on a 2,6 GHz Intel Core i7 quad-core 16gb ram MacBook.I just want to know if that frame rate is common or I am facing slow performances.Thank you";"['@tonxxd - I\'m at pretty much the same point as you and coming up against this issue running tfjs in Node on a Rasp Pi 4. The best performance I can get at the moment is around 550-600ms....Detecting Faces: 595.007msDetecting Faces: 588.997msDetecting Faces: 549.516msDetecting Faces: 616.711msDetecting Faces: 580.152msDetecting Faces: 562.429msDetecting Faces: 540.243msDetecting Faces: 551.98msDetecting Faces: 532.101msDetecting Faces: 578.383msFrom reading a number of other issues, here are some things to check. * Do you have the same version of `@tensorflow/tfjs-node` as is in the current version of face-api.js\'s `package.json`? Currently this is... ```    ""@tensorflow/tfjs-core"": ""1.7.0"",    ""@tensorflow/tfjs-node"": ""1.7.0"",    ""@tensorflow/tfjs-node-gpu"": ""1.7.0"",    ""face-api.js"": ""0.22.2"",```* Have you tried using `tfjs-node-gpu` over `tfjs-node`, this made little difference for me on Raspberry Pi but may help on a Macbook. * What nets are you using, apparently some have different performance impacts?=====', ""Hi @ChrisDalley thanks for the quick response!I tried the tiny face detector and it gives much higher performances (~ 60ms per frame). Changing to node-gpu didn't increase performances (strange(?)).Anyway, what I don't understand is why the node version is slower than the browser one when tensorflow has access to the Native C modules in the node env. I have the exact same app in python (with tensorflow for python and opencv-python) and I get real-time detections without issues. ====="", ""I'm honestly not too sure on the difference between Python and the node implementation, it's my first time touching this library today. I'm going to do some more digging over the next few days and see what I can figure out - even getting down to 60ms for me would be better than where it's at right now! Maybe @justadudewhohacks knows more about the difference between Python + Node / where we are going wrong performance wise?====="", '@ChrisDalley yes but that was on my MacBook, since I plan to run the app on fewer resources I am looking for a better strategy as you =====', '@ChrisDalley could you able to get any improvement? I am also facing same issue in my MacBook. It is taking ~7.5 sec for each image which is quite a lot in these days. I used GPU as well, but not much difference.Please help if you got any breakthrough.Thanks in advance.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/602;face-api.js ERROR: Based on the provided shape, [256,128], the tensor should have 32768 values but has 29076;10;open;2020-04-21T19:02:47Z;2021-10-17T02:57:44Z;Hello.i have a problem with this API. I dont know why, but localhost work very well, but in production not.I uploaded the files directly to the serverThe error says:**tf-core.esm.js:17 Uncaught (in promise) Error: Based on the provided shape, [256,128], the tensor should have 32768 values but has 29076**    at g (tf-core.esm.js:17)    at kn (tf-core.esm.js:17)    at In (tf-core.esm.js:17)    at o (tf-core.esm.js:17)    at Ph (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17i know this error is on side face-api.js, but i don't know how to fix.i use hostgator to serverBest Regards;"['Yes, I have the same issue. Works perfectly on local, but not on production.For some reason, I think the model did not load all of the content. It should load 2MB instead of bytes.<img width=""420"" alt=""Screen Shot 2020-06-23 at 19 02 59"" src=""https://user-images.githubusercontent.com/15343619/85401511-55527800-b584-11ea-8292-f09850f42b9f.png"">=====', ""i fixed. i changed the save file. I moved Model file to the root folder. Sorry, i don't know if you understand me, because i'm Brazilian. LOL====="", ""I understand you just fine. I fixed it too, the model wasn't loading correctly because I misconfigure URL settings so it doesn't load the model. It's a bad error message making us think the error comes from the javascript program.====="", ""Hello, if someone got here with the same problem, in my case, it was working on local but not on hosted. So, after a long time, I've found out the problem was on the ftp transfer. We must pay atention to transfer the *shard* files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.====="", ""> Hello,> if someone got here with the same problem, in my case, it was working on local but not on hosted. So, after a long time, I've found out the problem was on the ftp transfer.> We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.You are right!====="", ""> Hello,> if someone got here with the same problem, in my case, it was working on local but not on hosted. So, after a long time, I've found out the problem was on the ftp transfer.> We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.How to do that?====="", ""> > Hello,> > if someone got here with the same problem, in my case, it was working on local but not on hosted. So, after a long time, I've found out the problem was on the ftp transfer.> > We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.> > How to do that?Hello, using filezilla client, transfer> transfer type > binary====="", 'I faced same issue when I loaded models from git repository. To solve this I made archive of models and committed to git. Once I setup I extract the zip to appropriate directory.=====', ""I have the same problem with a dotnet SPA React app, deployed in AWS Elastic Beanstalk the json files are there but I get the error ' Error: Based on the provided shape, [1,1,256,24], the tensor should have 6144 values but has 4841' ====="", ""> Hello, if someone got here with the same problem, in my case, it was working on local but not on hosted. So, after a long time, I've found out the problem was on the ftp transfer. We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.thanks, it's worked=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/598;Model Loading Issue: Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0;12;open;2020-04-16T16:36:43Z;2021-06-06T18:34:35Z;"In loading the model    async faceDetectRecognize() {      await faceapi.nets.ssdMobilenetv1.loadFromUri(""@/services/face-api/weights""),    }I get this error:    asyncToGenerator.js?1da1:6 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at     position 0How to solve the problem?Looking forward to your kind help.Marco";"[""Hello,I am using react and I was receiving this error when the models folder was in the src folder.A quick fix for me was putting the models folder in the public folder.This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')Maybe this helps you:)====="", 'Hi @jhony123 thank you for your kind suggestion.Putting the weights (models) folder in the same folder of the .vue file:    /src/components/auth    -rw-r--r-- 1 marco marco  14K apr 28 18:49 Peerjs.vue    drwxr-xr-x 2 marco marco 4,0K apr  7 12:57 weightsand then updating the path:     await faceapi.nets.ssdMobilenetv1.loadFromUri(""/weights""),the problem persists.I tried also to move the the weights folder within the public folder:    public$ ls -lah    total 24K    drwxrwxr-x 3 marco marco 4,0K apr 28 18:56 .    drwxrwxr-x 6 marco marco 4,0K apr 28 17:01 ..    -rw-r--r-- 1 marco marco 4,2K feb  6 17:45 favicon.ico    -rw-r--r-- 1 marco marco 1003 apr 23 15:16 index.html    drwxr-xr-x 2 marco marco 4,0K apr  7 12:57 weightsand then updated the path:    await faceapi.nets.ssdMobilenetv1.loadFromUri(""../../../public/weights""),but the problem persistsI also tried to change the loader from .loadFromUri to .load :     await faceapi.nets.ssdMobilenetv1.load(""/weights""),    await faceapi.nets.ssdMobilenetv1.load(""../../../public/weights""),but still the problem persists=====', ""I'd recommend trying to manually go to the URL in your web browser that your JS is trying to load.When I got errors like that, it meant that either the file was missing or in the wrong place, or my local web server wasn't properly configured to serve up those static files.====="", '@lazerwalker I solved all the issues related to the NGINX web server configuration for static files serving. Now I\'m able to access through URI all the files in the weights folder /home/marco/www/weights:![staticfile-003](https://user-images.githubusercontent.com/3073209/81922922-20deba00-95dd-11ea-9bb1-c8b864e1f374.jpg)![staticfile-004](https://user-images.githubusercontent.com/3073209/81923299-bbd79400-95dd-11ea-9b65-3196ea0aff26.jpg)In .vue file I put these lines:    async faceDetectRecognize() {        console.log(""faceDetectRecognize() called""),         await faceapi.nets.ssdMobilenetv1.load(""/home/marco/www/weights/ssd_mobilenetv1_model-    shard1""),    }and in console.log I get this error message:    faceDetectRecognize() called    asyncToGenerator.js?1da1:6 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at     position 0What do you think could be the cause of the problem?=====', ""You don't want the URL to be your local filesystem filepath, it needs to be the URL hosted by your local web server so your web browser is capable of requesting  downloading it.====="", '@lazerwalker probably due to my lack of knowledge, I\'m not following you.What do you mean as ""URL hosted by my local web server, so my web browser is capable of requesting downloading it""? The files have to be located somewhere in the local filesystem.  So I need to indicate somehow the location of those files to the web browser.As far as I understand, this location needs to be an URL, to be defined in my local web server.Is this what I have already done as follows?    location /weights {      root /home/marco/www,      try_files $uri $uri/ =404,I tried to search within the NGINX \'s documentation but I didn\'t find, probably to my lack of knowledge, any reference.Would you be so kind in pointing me in the right direction? http://nginx.org/en/docs/=====', 'You have copies of the models/weights hosted somewhere they\'re accessible from a web browser (looks like ggc.world/weights from your screenshot). Your current JS code tries to load the weights from your local hard disk by accessing a file path on your local file system (/home/marco/www/weights). However, that JS code running your web browser doesn\'t have access to your local file system! It needs to be able to download them from a URL/web server that your web browser can resolve.Fortunately, you have one of those!Instead of `await faceapi.nets.ssdMobilenetv1.load(""/home/marco/www/weights/ssd_mobilenetv1_model-shard1""),`, you likely want something like `await faceapi.nets.ssdMobilenetv1.load(""ggc.world/weights/ssd_mobilenetv1_model-shard1""),`=====', 'Thank you @lazerwalker.Putting this :     await faceapi.nets.ssdMobilenetv1.load(""https://ggc.world/weights/"")I get this message:![staticfile-005](https://user-images.githubusercontent.com/3073209/81975994-2c0b0780-9628-11ea-8405-1fe94d36742e.jpg)But I guess, this is another story=====', ""> Hello,> I am using react and I was receiving this error when the models folder was in the src folder.> A quick fix for me was putting the models folder in the public folder.> This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')> Maybe this helps you:)Wow, thanks bro====="", ""> Hello,> I am using react and I was receiving this error when the models folder was in the src folder.> A quick fix for me was putting the models folder in the public folder.> This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')> Maybe this helps you:)That also did the trick for me. I am using this lib in react with Typescript project====="", 'I am facing same issue. my model files are hosted in web app service and from local i am trying to access. but everytime I am getting this error.error : Uncaught (in promise) SyntaxError: Unexpected token T in JSON at position 0.trying to access with below url:  https://cors-anywhere.herokuapp.com/azureURL/SampleModels/=====', 'same here @Namrataijare did you fixed?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/596;Face Recognition on Webcam Implementation: Cannot read property 'descriptor' of undefined;7;open;2020-04-15T17:35:39Z;2020-06-13T10:53:56Z;"Urgent: Facing this issue in the implementation of face recognition on the webcam camera for the browser. This error only comes when I add more than 1 name in my labels in loadLabeledImages() function. If I have one name, it works perfectly fine. Any help is appreciated. This is a part of a project that has to be completed in a week!Thank you so much!Getting ""Uncaught (in promise) TypeError: Cannot read property 'descriptor' of undefined""![image](https://user-images.githubusercontent.com/35863027/79368026-d3c8e300-7f5f-11ea-9901-5a209eadf3e0.png)This is the code for your reference:```` const video = document.getElementById('video')  Promise.all([   faceapi.nets.tinyFaceDetector.loadFromUri('/models'),   faceapi.nets.faceLandmark68Net.loadFromUri('/models'),   faceapi.nets.faceRecognitionNet.loadFromUri('/models'),   faceapi.nets.ssdMobilenetv1.loadFromUri('/models'), ]).then(startVideo)``````function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}this.video.addEventListener('play',() => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width, height: video.height }  faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors()    const resizedDetections = faceapi.resizeResults(detections, displaySize)    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    this.labeledFaceDescriptors = await this.loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)    const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))    results.forEach((result, i) => {      const box = resizedDetections[i].detection.box      const drawBox = new faceapi.draw.DrawBox(box, {label: result.toString()})      drawBox.draw(canvas)     })  }, 100) })function loadLabeledImages() {  try{    const labels = ['Shriya', 'judhi']    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 1, i <= 3, i++) {          const img = await faceapi.fetchImage(`public/img/${label}/${i}.jpg`)          const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()          descriptions.push(detections.descriptor)        }          return new faceapi.LabeledFaceDescriptors(label, this.descriptions)      })    )  }  catch(err){    console.log(err)  }}````";"[""It is because your label image doesn't detect any face. You need to ensure the images are person face====="", '@chenchiuchi Thank you so much, this fixed my problem. The issue was that my image was of bad quality. Thank you=====', 'Sorry, I have another question. What do I do to record the timestamp of whenever the face is recognised? I am making an attendance system so I need to record the check-in, check-out time. Thank you.=====', '@idkidk-idk leave the timestamp to your backend (or DB level). I mean, once you have a successful match, you make a request to your backend to record the clock in/out.=====', 'Hi @idkidk-idk,I have a problem as above, so as your comment ""The issue was that my image was of bad quality."" I have updated my image so it still displays the error. Could you please help me share your opinion?=====', ""Heyy @ThaiNhung I was stuck with this issue for almost half a month. My problem was that I had taken a screenshot of a pretty small picture and saved it and never looked at it again. So I thought my code was wrong and I kept making changes to the code. I opened the picture and noticed its too small and hence the quality was bad and I replaced it and everything worked.So basically if your error points out to the descriptor in the loadLabledImages() function then it's because it can't find a face in the image you have stored and nothing is wrong with your code. If its not your quality, then there should be something wrong with perhaps the color or some other element that is making it difficult to recognise the face in the stored image. So I guess try changing it to a clear picture of just the face.I hope it helped! My code remained the same. Let me know if you need any help from my side! Good luck <3====="", 'Hi @idkidk-idk , Thank you for your quick response. As your comment, I try to change to a clear picture just the face so so it still displays this error. Besides, I take care of some elements such as color, Brightness but it not feasible. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/595;Around 40-50% match rate when comparing two single faces in two images;2;open;2020-04-13T14:53:32Z;2020-10-05T00:02:44Z;```exports.verifyProfile = functions.runWith({  memory: '1GB'}).https.onCall(async (data) => {  try {    console.log('====================== Cloud function called ==================')    console.log(data),    require('@tensorflow/tfjs-node'),    const faceApi = require('face-api.js'),    console.log('Loaded successfully face api'),    const canvas = require('canvas'),    const { Canvas, Image, ImageData } = canvas,    console.log('Loaded successfully canvas'),    faceApi.env.monkeyPatch({ Canvas, Image, ImageData }),    console.log('Loaded successfully modules'),    let imgUrl = 'IMAGE_URL_1',    let imgUrl2 = 'IMAGE_URL_2',    console.log('Loading new models'),    await faceApi.nets.faceRecognitionNet.loadFromDisk('./models'),    await faceApi.nets.faceLandmark68Net.loadFromDisk('./models'),    await faceApi.nets.ssdMobilenetv1.loadFromDisk('./models'),    console.log('loaded new models'),    const img = await canvas.loadImage(imgUrl),    console.log('Image converted to buffer'),    const faceDescriptionOne = await faceApi      .detectSingleFace(img).withFaceLandmarks().withFaceDescriptor(),    const img2 = await canvas.loadImage(imgUrl2),    console.log('Image 2 converted to buffer'),    const faceDescriptionTwo = await faceApi      .detectSingleFace(img2).withFaceLandmarks().withFaceDescriptor(),    console.log('All detections called successfully'),    // Similarity match    const similarityMatch = faceApi.euclideanDistance(faceDescriptionOne.descriptor, faceDescriptionTwo.descriptor),    console.log(similarityMatch),    let matched = false,    if (similarityMatch > 0.6) {      console.log('Images matched'),      matched = true,    } else {      console.log('NO MATCH'),    }    // Write to the database    const hashedValue = stringHash(data['uid']),    const documentReference = admin.firestore().collection('profiles').doc(hashedValue),    return documentReference.set({      'verified': matched    }, { merge: true }),  }     catch (error) {    console.log('==========================Error Occured============================='),    console.log(error),    const hashedValue = stringHash(data['uid']),    const documentReference = admin.firestore().collection('profiles').doc(hashedValue),    return documentReference.set({      'verified': 'error'    }, { merge: true }),  }}),```I am using firebase cloud functions running with Node.js which basically have nothing related to the low similarity error between two images but please suggest if I am doing something wrong here or what can I do to improve the results because I have seen some tutorials where the match rate is around 60%  like https://www.youtube.com/watch?v=AZ4PdALMqx0&t=1156s&pbjreload=10I understand that I am passing just two images but they are of the same person. Is using euclidian distance correct or I have to use `faceapi.FaceMatcher`? Any code snippets would be really helpful.Also, what can be a safe percentage to say the two images belong to same user?  If the image quality is less the results are lower like around 30%, so what would be safe?These are two images in place of URL1 AND URL2 which gave 57% ![1](https://user-images.githubusercontent.com/34508540/79130214-34231d80-7dc4-11ea-983f-ab2f9cb3890e.jpg)![2](https://user-images.githubusercontent.com/34508540/79130385-83694e00-7dc4-11ea-8162-edfad51271ca.jpg)These are two images in place of URL1 AND URL2 which gave 45% even when they are of different gender. Am I missing something here lol?![1](https://user-images.githubusercontent.com/34508540/79133452-ad713f00-7dc9-11ea-80f0-8c95a0e9b33a.jpg)![2](https://user-images.githubusercontent.com/34508540/79133456-aea26c00-7dc9-11ea-8705-97916519f3dc.jpg) ;['A Euclidean distance of 0 would occur for a perfect match (e.g., same picture).I believe the distance is based on the color values of the image. In the first example, although the two pictures are of the same person (as we know) they are of different lighting and the person has aged making for different hair color. In contrast, the second example has two faces with similar complexion skin and similar hair color. As a result, from a color value and distance point of view, the second example would reasonably have a smaller distance (0.45 as you say) than the first example (0.57).=====', 'I have same issue, I compare my pictures but the distance I get is between 0.5 to 0.6 , which means 40 to 50 % match=====']
https://github.com/justadudewhohacks/face-api.js/issues/594;Tiny face detection not working in example or locally;3;open;2020-04-12T21:06:14Z;2020-07-14T10:10:31Z;I was struggling to get the tiny face detector model to work locally on the same images that are working in the [live demo](https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection) for face and landmark detection—it would always return an empty array for detections. So I followed the README to clone and run the examples locally and I noticed they aren't working there either.Is it possible the live-hosted version is behind the latest version in this repository and the latest version introduced a breaking change to the tiny face detector model? I noticed the demo web app looks significantly different with new examples as well.![cap](https://user-images.githubusercontent.com/555859/79079913-db7a5480-7cdf-11ea-9437-fa13578b2283.gif);"[""FWIW It looks like it was a change after 0.21.0 that broke this code...````javascriptawait faceapi.nets.tinyFaceDetector.loadFromUri('/static')const input = document.getElementById('myImg')const detection = await faceapi.detectSingleFace(  input,  new faceapi.TinyFaceDetectorOptions())console.log(detection) // undefined in version 0.22.0 and above ````====="", 'Thank you @craigspaeth for the bug hunt! I was having the same issue and checking out version 0.21 did the trick.=====', 'Version 0.21 still undefined in ios 12.4 with ionic. Working Fine in Android.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/585;out of memory issue with face reacognition.js;1;open;2020-03-23T08:16:51Z;2020-11-25T18:21:37Z;Hii,The error is: out of memory issue. Is there any memory dispose method in face-api.js and i also tried with tensorflow memory leak issue still am getting the same error. what best could be done to resolve this??2020-03-23 13:41:31.839784: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocating tensor with shape[1,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpuException code=0xc0000005 flags=0x0 at 0x00007FF95CFFA277. Access violation - attempting to read data at address 0x0000000000000010;"[""Did you ever find a solution to this? I'm running into leaks in WebWorkers myself and can't find a way to get memory be freed. (Noted in another issue: https://github.com/justadudewhohacks/face-api.js/issues/732)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/583;Detecting hand touches (covid19);2;open;2020-03-20T14:49:54Z;2020-03-29T01:43:55Z;Hi Due to current coronavirus circulation I want to avoid face touching (which seems not possible when your brain is focused on some task)So I want to feed my webcam to face-api to detect face and then on top of that to detect if anything(like hand :) ) is hovering it (and if it does play some alarm sound)Do you have any example on how to detect if face is blocked by some object ?Thanks;['Hello @vitalik Try posenet ![camera](https://user-images.githubusercontent.com/11027129/77837843-56ddf100-718b-11ea-9e07-440c74da6222.gif)Repo: https://github.com/tensorflow/tfjs-models/blob/master/posenet/README.mdDemo:https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html=====', 'Even better there is this handpose ![IMG_20200329_071334](https://user-images.githubusercontent.com/11027129/77837961-d4eec780-718c-11ea-9e79-b1571f22c7bb.jpg)https://github.com/tensorflow/tfjs-models/blob/master/handpose/README.mdDemo:https://storage.googleapis.com/tfjs-models/demos/handpose/index.html=====']
https://github.com/justadudewhohacks/face-api.js/issues/572;react.js app integration, detectAllFaces() returns empty array;1;open;2020-03-06T21:09:40Z;2020-06-04T17:44:19Z;"`  const loadModels = function(){    return Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri('/models'),      faceapi.nets.faceLandmark68Net.loadFromUri('/models'),      faceapi.nets.faceRecognitionNet.loadFromUri('/models')    ]).then(console.log(""Loaded models""))  }  useEffect(() => {    const video = document.getElementById('video')        video.addEventListener('playing', () => {      const canvas = faceapi.createCanvasFromMedia(video)      document.body.append(canvas)      const displaySize = { width: video.width, height: video.height }      faceapi.matchDimensions(canvas, displaySize)      setInterval(async () => {        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())        console.log(""detections"", detections)        const resizedDetections = faceapi.resizeResults(detections, displaySize)        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)        faceapi.draw.drawDetections(canvas, resizedDetections)        faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)      }, 200)    })  }, ['face-api.js'])`Console output:detections: []models load just fine, i've checked on the network tab.this is a pwa implementation, wich will run on many diferent types of devices.Currently i'm running it on a macbook pro 2014.the video element exists and is reaching the function.would anyone have any pointers?";"['Try adding \'faceapi.nets.ssdMobilenetv1.loadFromUri(""/models"")\' too.However, I\'m not able to get these things. My app doesn\'t load the json for these models properly and I get ""Unexpected Token < at position 0 of JSON"". How did you overcome that?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/566;Cannot read property 'monkeyPatch' of undefined;2;open;2020-02-29T09:05:23Z;2020-08-10T15:59:44Z;"I tried following the docs for nodejs and it's giving me errorCode```js// import nodejs bindings to native tensorflow,// not required, but will speed up things drastically (python required)// import ""@tensorflow/tfjs-node"",// implements nodejs wrappers for HTMLCanvasElement, HTMLImageElement, ImageDataimport * as canvas from ""canvas"",import * as faceapi from ""face-api.js"",const app = async () => {  // patch nodejs environment, we need to provide an implementation of  // HTMLCanvasElement and HTMLImageElement  const { Canvas, Image, ImageData } = canvas,  faceapi.env.monkeyPatch({ Canvas, Image, ImageData }),  // await faceapi.loadSsdMobilenetv1Model('/models')  await faceapi.loadTinyFaceDetectorModel(""/models""),  const input = await canvas.loadImage(""./faces.jpeg""),  const detections = await faceapi.detectAllFaces(input),  console.log(detections),},app(),````node version v13.8.0``macOS latest catalina````❯ node index.js                                          (node:37624) ExperimentalWarning: The ESM module loader is experimental.(node:37624) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'monkeyPatch' of undefined    at app (file:///Users/shivam/Documents/web/face-api-js/index.js:15:15)    at file:///Users/shivam/Documents/web/face-api-js/index.js:24:1    at ModuleJob.run (internal/modules/esm/module_job.js:110:37)    at async Loader.import (internal/modules/esm/loader.js:164:24)(node:37624) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:37624) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```";"['Write the import as following:`import faceapi from ""face-api.js"",`=====', 'I have same issues.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/561;How to train the model with own data set and export it to model.json;1;open;2020-02-26T07:30:58Z;2020-02-28T09:39:10Z;Hi,I am confused. Is it possible i can export a model.json by training with my image dataset in node js environment;"[""Use tfjs for that, that's not the purpose of face-api.js.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/559;Not able to load models properly in ionic;4;open;2020-02-23T15:49:06Z;2021-11-30T13:00:30Z;"hi, I am not able to load the models successfully in ionic. However, there was no problem running it in broswer and serving in localhost. The errror is:""Based on the provided shape, [3,3,32,1], the tensor should have 288 values but has 177""";"['Inspect the network tab and make sure, that the shards are all loaded correctly and that they are not corrupted, e.g. the size of the fetched shards is unchanged.=====', 'Thanks man. Solved it by file accessing process recommended in ionic, as it can not access local file. Thanks again.=====', ""> accessingHi @juny58 ,I'm also facing a similar problem in ionic. Can you just brief me on how do you solve this?====="", ""> Thanks man. Solved it by file accessing process recommended in ionic, as it can not access local file. Thanks again.How did you fix this issue on ionic? I'm facing same problem. Thanks!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/552;Conditionally use High-Level API to detect face landmarks only when 1 face detected?;4;open;2020-02-14T22:26:16Z;2020-02-29T07:19:31Z;With my use-case, I am attempting to only detect the facial landmarks **if and only if** one face was detected. I am well aware `detectSingleFace` returns the face with the highest score, but in my situation, I am attempting to scrape and build a database, so I am not sure if the highest score will be of the right subject. In fact, I have already came across an example where that is not the case.I could execute `faceapi.detectAllFaces(input).withFaceLandmarks()`, but to me this doesn't seem efficient. I would be detecting facial landmarks of the `input` even when there are multiple faces, which is what I am trying to avoid for the sake of efficiency.Is it possible for me to conditionally use the high-level api, perhaps using promises or such? Below is a **non-working** snippet I was messing about, hopefully this helps portray my idea better.```jsawait new Promise((resolve, error) => {	faceapi.detectAllFaces(img).then(detectedFaces => {		console.log(`Detected ${detectedFaces.length} faces.`),		if (detectedFaces.length === 1) {			resolve(detectedFaces)		} else {			error('Found more than one face'),		}	}).withFaceLandmarks(landmarks => {		console.log(landmarks),	}),}),```;"['You can use `faceapi.detectAllFaces` to get the bounding boxes, and then use the low level API to detect landmarks if detections === 1:``` javascript     const results = await faceapi.detectAllFaces(img)    if (results.length === 1) {      const faces = await faceapi.extractFaces(img, results.map(res => res.detection))      const landmarks = await faceapi.nets.faceLandmark68Net.detectLandmarks(faces[0])   }```You can also have a look at the implementation of the high level API, to get a better idea of how the low level API works: [DetectFaceLandmarksTasks.ts](https://github.com/justadudewhohacks/face-api.js/blob/master/src/globalApi/DetectFaceLandmarksTasks.ts#L43-L54)=====', 'Thank you for the snippet and the additional link to an implementation.I do have a few follow up questions if you don\'t mind clearing up: - why is it that you\'re directly referencing `faceLandmark68Net`, couldn\'t we simply do `faceapi.detectLandmarks`?- Since we know there is only one face, can\'t we skip mapping the results to the detection, seems like an unnecessary execution? **Though I tried this and received` Cannot read property \'map\' of undefined` resulting from `node_modules\\face-api.js\\build\\commonjs\\dom\\extractFaces.js:45:40)`, so I suppose I have to**   - What exactly does `extractFaces` achieve? README says ""to extract face regions from bounding boxes"", but I thought `detectAllFaces` essentially does the same thing?- So if I remember correctly, when we call `withFaceLandmarks` using the high-level API, which I believe aligns the face(?). My use case is creating the facial descriptor for the image, so am I correct in saying that by calling `detectLandmarks(faces[0])` with the low-level API, it will still align `faces[0]`?Here is my current implementation with your help:```jstry {\tconst image = await canvas.loadImage(await googleUtils.getImageAsBuffer(file.id)),\t//console.log(`[${i++}] Detecting faces in ${file.id}`),\tconst detections = await faceapi.detectAllFaces(image, faceapiOptions),\tif (detections.length == 1) {\t\t//console.log(`[${i}] Detecting single face in ${file.id} | Total found: ${facecount++}`),\t\tconst faces = await faceapi.extractFaces(image, detections[0].detection), // doesnt work unless second argument is detections#map\t\tconst landmarks = await faceapi.detectLandmarks(faces[0]),\t\tconst descriptors = await faceapi.computeFaceDescriptor(faces[0]),\t\treturn descriptors,\t}} catch (err) {\tconsole.warn(`Hit an error trying to detect faces in file \'${file.name}\' (${file.id})`),\tconsole.error(err),}```=====', ""~~Scratch that, I forgot to remove my previous code I had where I was using the high-level api, and the code was executing with no problem: `await faceapi.detectAllFaces(image, faceapiOptions).withFaceLandmarks().withFaceDescriptors(),`~~~~But when I got rid of the other methods after `detectAllFaces`, to `await faceapi.detectAllFaces(image, faceapiOptions)`, I am getting the following error:~~> ~~TypeError: Cannot read property 'clipAtImageBorders' of undefined     at \\node_modules\\face-api.js\\build\\commonjs\\dom\\extractFaces.js:48:58~~~~I assume this is because of the image I am passing in: `const image = await canvas.loadImage(await googleUtils.getImageAsBuffer(file.id)),`~~~~Going to look into this a bit more~~~~**EDIT: Isn't `faceapi.detectAllFaces` supposed to return a `FaceDetection` that has a `detection` property? Because if so, it's not doing so in the snippet you provided**~~After looking at [DetectFacesTasks](https://github.com/justadudewhohacks/face-api.js/blob/cb52dab46ecfd7585e17fb5a671eddcb2f7cd0ee/src/globalApi/DetectFacesTasks.ts) I see that you should be passing in just the results array, and not `results.map...`, at least that's what fixed the error I was having above, because it was trying to reference `res.detection`, **but results return a `FaceDetection`,** but with no `description` property.====="", ""**Please ignore the previous two comments**. So I attempted the solution you've provided, passing in my img, `const img = await canvas.loadImage(await googleUtils.getImageAsBuffer(<fileId>)),`, and I received the following error:> UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'clipAtImageBorders' of undefined    at ...\\node_modules\\face-api.js\\build\\commonjs\\dom\\extractFaces.js:48:58---However, I noticed that if I **change your snippet to `detectAllFaces` followed by `withLandmarks`, it works fine**, for example:```jsconst img = await canvas.loadImage(await googleUtils.getImageAsBuffer(<fileId>)),const results = await faceapi.detectAllFaces(img).withFaceLandmarks(),if (results.length === 1) {\tconst faces = await faceapi.extractFaces(img, results.map(res => res.detection)),\tconst landmarks = await faceapi.detectLandmarks(faces[0]),}```**Once again, if I remove `withFaceLandmarks`, it results in an error.** However, I forgot to mention in the main post that **I want the facial descriptors**. So, when I attempt to compute the descriptors:```jsconst img = await canvas.loadImage(await googleUtils.getImageAsBuffer(<fileId>)),const img2 = await canvas.loadImage(await googleUtils.getImageAsBuffer(<fileId>)),const results = await faceapi.detectAllFaces(img).withFaceLandmarks(),if (results.length === 1) {\tconst faces = await faceapi.extractFaces(img, results.map(res => res.detection)),\tconst landmarks = await faceapi.detectLandmarks(faces[0]),\tconsole.log(JSON.stringify(await faceapi.computeFaceDescriptor(faces[0]))), // Returns descriptor A\tconsole.log(JSON.stringify(await faceapi.computeFaceDescriptor(img))), // Returns descriptor B}console.log(JSON.stringify((await faceapi.detectAllFaces(img2).withFaceLandmarks().withFaceDescriptors())[0].descriptor)), // Returns descriptor C}```All three of those values return different results. I feel like I am not understanding something properly here. Because firstly, I thought that `detectLandmarks` manipulated the input and aligned it as described in the README for the high-level API. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/551;Storing face data in Mongodb and querying it;6;open;2020-02-12T20:57:21Z;2020-10-10T09:59:10Z;I'm looking to work with a large dataset of images. I would like to process faces in one periodic script and load the descriptors/distances into a Mongo collection for querying at a later time since I can imagine there will be a massive performance hit for storing 100,000s of instances of facial data in memory.Any advice on how to achieve this?;"['You would simply fetch batches of your stored descriptors, compare your query data against the batch, keep only the matches and then proceed I would guess.=====', ""> You would simply fetch batches of your stored descriptors, compare your query data against the batch, keep only the matches and then proceed I would guess.That's what I was thinking. Wasn't sure if there was a way to create a query to reduce the results returned.Thanks====="", ""I'm thinking about that too cause I'm wondering why there are no projects/examples of smth like a photo gallery which labels images with recognized peoples names.====="", 'If you have an input vector and  want to query the most similar vector  form lots of vectors in DB, then you need to calculate the similarity between input and each vector in DB every time. This is basically a map-reduce task,  so [Mongo Map-Reduce](https://docs.mongodb.com/manual/core/map-reduce/) will help you do this, the logic will run in mongoDB so if you build a  mongoDB cluster, it could process lots of data.There is a better solution  [faiss](https://github.com/facebookresearch/faiss)，a vector query engine with high performance.=====', ""> Você simplesmente buscava lotes de seus descritores armazenados, comparava os dados da consulta com o lote, mantinha apenas as correspondências e prosseguia, imagino.I tried to store LabeledFaceDescriptors in mongodb and it didn't work. I read somewhere that mongodb returns arraybuffer instead of float32array.Some extra work is needed that I'm not getting.====="", '@FrancinaldoCabral have you finished your project? How many images are you storing? How precise recognition algorithm works?I also working on something similar - going to store over 100k images as LabeledFaceDescriptors -> convert them to plain array and store in firebase database. Then on client side convert face to LabeledFaceDescriptor then search in database for similar images.@justadudewhohacks @RocMax any pitfalls in my idea?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/550;it is possible to use backend tfjs-backend-wasm or webgl?;2;open;2020-02-12T16:36:38Z;2020-04-07T23:23:13Z;it is possible to use backend tfjs-backend-wasm or webgl?performance is much betterhttps://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm![Captura de pantalla 2020-02-12 a la(s) 13 34 59](https://user-images.githubusercontent.com/731936/74356052-7caa6480-4d9c-11ea-93d5-02d685f32fce.png);"[""The WebGL backend is utilized by default, I didn't try tfjs-backend-wasm yet, but if all the ops are supported by the backend yet then yes you should be able to use it as well.====="", ""It looks like the `fill` op is a dependency for `tinyFaceDetector` and `tfjs-backend-wasm` doesn't yet support it:```Uncaught (in promise) Error: 'fill' not yet implemented or not found in the registry. Did you forget to import the kernel?```(I didn't forget to import the kernel 😄)List of available kernels: https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm/src/kernels=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/546;Typescript compiler errors;3;open;2020-02-09T02:15:30Z;2021-03-13T20:23:52Z;"Am I dumb or am I missing something? I followed the documentation for usage with node but this produces many typescript compiler errors even tho the code functions.The following produces the errors below```const { Canvas, Image, ImageData } = canvasfaceapi.env.monkeyPatch({  Canvas,  Image,  ImageData})``````Type 'typeof Canvas' is not assignable to type '{ new (): HTMLCanvasElement, prototype: HTMLCanvasElement, }'.Types of property 'prototype' are incompatible.Type 'Canvas' is missing the following properties from type 'HTMLCanvasElement': toBlob, transferControlToOffscreen, addEventListener, removeEventListener, and 239 more.ts(2322)types.d.ts(6, 5): The expected type comes from property 'Canvas' which is declared here on type 'Partial<Environment>'```Also```const img = await canvas.loadImage('./temp/Thumbnail1.png')const fullFaceDescriptions = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors()``````Argument of type 'Image' is not assignable to parameter of type 'TNetInput'.Type 'Image' is not assignable to type 'HTMLImageElement'.ts(2345)```Why use typescript at all if I have to use // @ts-ignore everywhere to remove compiler errors? Or am I missing something? Thanks.""typescript"": ""3.7.5""";"['The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs, so you might have to cast to ""any"" here and there:```faceapi.env.monkeyPatch({  Canvas,  Image,  ImageData} as any)````faceapi.detectAllFaces(img)`By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.=====', '> > > The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs, so you might have to cast to ""any"" here and there:> > ```> faceapi.env.monkeyPatch({>   Canvas,>   Image,>   ImageData> } as any)> ```> > `faceapi.detectAllFaces(img)`> > By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.Hi! I\'m trying to read/write images with the tjfs-node utility but I got an error when I run it: ![image](https://user-images.githubusercontent.com/56895143/111039780-36dc2780-8406-11eb-8a8d-07c665d7a065.png)The error occurs between lines 90-91:![image](https://user-images.githubusercontent.com/56895143/111039811-6e4ad400-8406-11eb-85d6-4bd63e584728.png)Amazing library, if you have any idea of that error and you can help me I would be grateful=====', '> > > > The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs, so you might have to cast to ""any"" here and there:> > ```> > faceapi.env.monkeyPatch({> >   Canvas,> >   Image,> >   ImageData> > } as any)> > ```> > > > > > `faceapi.detectAllFaces(img)`> > By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.> > Hi! I\'m trying to read/write images with the tjfs-node utility but I got an error when I run it:> > ![image](https://user-images.githubusercontent.com/56895143/111039780-36dc2780-8406-11eb-8a8d-07c665d7a065.png)> > The error occurs between lines 90-91:> > ![image](https://user-images.githubusercontent.com/56895143/111039811-6e4ad400-8406-11eb-85d6-4bd63e584728.png)> > Amazing library, if you have any idea of that error and you can help me I would be gratefulSolved. I just update the face api version ^0.22.2=====']"
https://github.com/justadudewhohacks/face-api.js/issues/544;face api js with cuda;2;open;2020-02-07T02:00:07Z;2020-08-03T14:27:31Z;hi anybody know how to maximize performance recognize with cuda running in browser with api js , i've get slow recognize detection  when run this code faceMatcher.findBestMatch?;"['You can only utilize the cuda backend with nodejs installing the tfjs-node-gpu package, but not in the browser.`faceMatcher.findBestMatch` should not be the bottleneck of performance though, unless you are comparing with a ton of descriptors.=====', 'require(""@tensorflow/tfjs-node""),const faceapi = require(""face-api.js""),const canvas = require(""canvas""),const path = require(""path""),const { promisify } = require(""util""),const readdir = promisify(require(""fs"").readdir),const stat = promisify(require(""fs"").stat),const router = require(""express"").Router(),const loadLabelledFaceImages = async () => {  //joining path of directory  let directoryPath = path.join(__dirname, ""../labelled-images""),  //passsing directoryPath  const files = await readdir(directoryPath),  return Promise.all(    files.map(async (file) => {      const descriptions = [],      for (let i = 0, i <= 39, i++) {        const imageLink = `${directoryPath}/${file}/${i}.png`,        const img = await canvas.loadImage(imageLink),        const detections = await faceapi          .detectSingleFace(img)          .withFaceLandmarks()          .withFaceDescriptor(),        descriptions.push(detections.descriptor),      }      return new faceapi.LabeledFaceDescriptors(file, descriptions),    })  ),},router.get(""/getimagesdata"", async (req, res) => {  const labeledFaceDescriptors = await loadLabelledFaceImages(),  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6),  res.status(200).send({    status: ""OK"",    data: {      faceMatcher,    },    error: {},  }),}),module.exports = router,on the frontend I am getting the error is that faceMatcher.findBestMatch() is not a function.I have no idea how to resolve this!! help needed=====']"
https://github.com/justadudewhohacks/face-api.js/issues/543;Library Collision between face-api and posenet?;1;open;2020-02-03T05:14:58Z;2020-02-28T08:43:09Z;My posenet code works, but when I add `import * as faceapi from 'face-api.js',` I start to get errors like:> Error: Argument 'x' passed to 'pad' must be a Tensor or TensorLike, but got 'Tensor'> convertToTensor — modules.js:108247> pad_ — modules.js:95597> f2 — modules.js:96538> f2 — modules.js:96538> (anonymous function) — modules.js:75986> (anonymous function) — modules.js:10545> scopedRun — modules.js:10555> padAndResizeTo — modules.js:75984> (anonymous function) — modules.js:75561> step — modules.js:75414> (anonymous function) — modules.js:75389> initializePromise> Promise> (anonymous function) — modules.js:75385Is there a way to have both of these libraries?Thanks for any thoughts or help! I am using Meteor and this is running on my iphone. In a regular laptop browser it works fine. Very confused! Any advice to track this down?;['You have to make sure that you are using a posenet version, that utilizes the same tfjs-core version as face-api.js, or atleast make sure, that you are not ending up with 2 different versions of tfjs-core in your bundle.=====']
https://github.com/justadudewhohacks/face-api.js/issues/542;Error with Euclides number comparing 2 faces;1;open;2020-01-31T19:40:22Z;2020-02-28T08:41:40Z;"Hi there, I'm using the library from node js and it works quite good, but I have some errors comparing 2 faces. Usually it works good, but sometimes it does not recognise the same person, and sometimes returns that 2 different faces are the same...My code looks like this:```jsimport { Request, Response } from 'express',import '@tensorflow/tfjs-node',import * as faceapi from 'face-api.js',import { loadImage, Canvas, Image, ImageData } from 'canvas',import fetch from 'node-fetch',import env from '../config/environment',// @ts-ignorefaceapi.env.monkeyPatch({ Canvas, Image, ImageData, fetch }),const maxDescriptorDistance = Number(env.faceMaxdescriptor),interface ResultType {  message: string,  data?: any,  success: boolean,  error?: string,},let loadedModels: boolean = false,const loadFaceApiModels = async () => {  return new Promise(async (resolve) => {    if (!loadedModels) {      await Promise.all([        faceapi.nets.ssdMobilenetv1.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`),        faceapi.nets.faceLandmark68Net.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`),        faceapi.nets.faceRecognitionNet.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`),      ]),      loadedModels = true,      resolve(),    } else {      resolve(),    }  }),},const detectFace = (image: any): any => faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor(),export const checkFace = async (req: Request, res: Response): Promise<void> => {  const {    body: { image1, image2 },  } = req,  let result: ResultType = { success: false, message: req.t('faceRecognition.notMatch'), error: 'FACE_NOT_MATCH' },  await loadFaceApiModels(),  try {    const [canvasImage1, canvasImage2] = await Promise.all([ loadImage(image1), loadImage(image2) ]),    if (canvasImage1 && canvasImage2) {      const [detection1, detection2] = await Promise.all([ detectFace(canvasImage1), detectFace(canvasImage2) ]),      if (detection1 && detection2) {        const distance = faceapi.euclideanDistance(detection1.descriptor, detection2.descriptor),        result.data = distance,        if (distance <= maxDescriptorDistance) {          result = {            success: true,            message: req.t('faceRecognition.success'),            data: distance,          }        } else {          result = { success: false, message: req.t('faceRecognition.notMatch'), error: 'FACE_NOT_MATCH', data: distance },        }      } else {        result = { success: false, message: req.t('faceRecognition.cannotDetectFace', { image: !detection1 ? image1 : image2 }), error: 'FACE_NOT_DETECTED' },      }    } else {      result = { success: false, message: req.t('faceRecognition.noImageError', { image: !canvasImage1 ? image1 : image2 }), error: 'NO_IMAGE' },    }  } catch (error) {    result = { success: false, message: req.t('faceRecognition.error'), error: 'INTERNAL_ERROR', data: error },  },  res.status(200).send(result),  return,},```So this endpoint basically receive 2 images urls which are saved in a bucket in s3 (they are public)Something like:**POST** to **endpoint/check**> body: {>	""image1"": ""S3_PATH_IMAGE_1"",>	""image2"": ""S3_PATH_IMAGE_1""> }I'm using euclides number with value **0.6** as I saw in the documentation...But for example with an image, if the right person is compared, it returns an euclides number 0.35541702332548575, and a different person is compared and it returns 0.5926215395658295 (also we are testing with male and female image)Can I improve in someway the validations? I don't want to reduce euclides numbers, because in some cases the same person fails in the comparison with 0.62 or something similar, so if I reduce the number, it will fail multiple times.Thanks in advance, and thanks because the library is really useful!";"[""A threshold of 0.6 is optimal yes, I won't change it. The quality of the image and the size of the face in the image after cropping also affects the accuracy. `Can I improve in someway the validations?`One thing that can be done is to match a query image to 2 or more images of reference person X and then take the mean of the distances.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/541;Firefox 72.0.1 WebGL Errors and failed face detection;1;open;2020-01-31T02:03:30Z;2020-02-28T08:35:41Z;"## SummaryRunning Firefox 72.0.1, experiencing several WebGL errors in the console and face detection is failing. Same code ran in Chromium works great with no errors. I'm running Fedora 30 if it matters.## Additional Information**Screenshot**![image](https://user-images.githubusercontent.com/10566658/73506336-3f5cd480-43a3-11ea-886f-9d9a7121a14a.png)**Errors as Text**```Error: WebGL warning: readPixels: PIXEL_PACK_BUFFER must be null.Error: WebGL warning: readPixels: Buffer for `target` is null.```**OS Info** ```5.2.11-200.fc30.x86_64 #1 SMP Thu Aug 29 12:43:20 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux```**My Code**```  const foo = async () => {    const MODEL_URL = '/models'        console.log(""here"")    var result = faceapi.loadFaceLandmarkTinyModel(MODEL_URL)    await faceapi.loadTinyFaceDetectorModel('/models')    await faceapi.loadFaceLandmarkTinyModel('/models')    await result    console.log(result)    console.log(""i loaded it?"")    const input = document.getElementById('image')    const detections2 = await faceapi.detectAllFaces(input, new faceapi.TinyFaceDetectorOptions())    console.log(detections2)  }  foo()```Thank you for your hard work on this project!";"[""Hmm, it's better to ask the tfjs team about issues with the WebGL backend, since I am not familar with the internals.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/538;Weights not loading correctly.(Asp .net mvc);2;open;2020-01-29T02:11:03Z;2020-02-28T12:18:54Z;"__Uncaught (in promise) Error: Based on the provided shape, [3,3,32,32], the tensor should have 9216 values but has 1842    at D (face-api.min.js:1)    at Ge (face-api.min.js:1)    at e (face-api.min.js:1)    at jc (face-api.min.js:1)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)-_This is my Error in Console. I and trying to use face-api with asp.net._Promise.all([	faceapi.nets.faceRecognitionNet.loadFromUri(""/recognition_models""),	faceapi.nets.faceLandmark68Net.loadFromUri(""/recognition_models""),	faceapi.nets.ssdMobilenetv1.loadFromUri(""/recognition_models"")]).then(start),_I am trying to load the weights this way but it doesn't works properly.The same code runs correctly outside the asp.net project and weights load properly.I don't know what the issue is.  ";"['Check the network tab and make sure, that all the shards are fetched correctly from your backend, e.g that the uris are correct and the files are not corrupted.=====', 'I just ran into the same problem today. IIS will not serve these files by default. Try adding a web.config to the folder from which these files are served containing this:```<?xml version=""1.0"" encoding=""UTF-8""?><configuration>  <system.webServer>    <staticContent>      <mimeMap fileExtension=""."" mimeType=""application/octet-stream"" />    </staticContent>  </system.webServer></configuration>=====']"
https://github.com/justadudewhohacks/face-api.js/issues/532;trained models in other formats?;3;open;2020-01-23T01:07:32Z;2021-06-21T13:20:27Z;Hi @justadudewhohacks Really like Face API, I'm currently working on an app feature using the landmark points to infer head orientation and descriptor comparison to ensure to liveness of the user. We're building our app on web and native iOS/Android. On the web we're using tf.js, we started with your API for this feature and are working with TFjs and our own models for other app features. We'd like to standardise our models across platforms, which is easy enough for our own models, but we can't convert your TF.js models to .tflite. From the output of tfjs-converter, I presume that the models in this repo are in the tf_graph_model format. Would you be able to provide access to your models in .tflite, tfjs_layers_model or keras formats?;"[""> From the output of tfjs-converter, I presume that the models in this repo are in the tf_graph_model format.They are somewhat in a similar format, but you are right, the only way to convert these models back to a standard tensorflow format is by rebuilding the model architecture in tf and extracting the weights manually.> Would you be able to provide access to your models in .tflite, tfjs_layers_model or keras formats?With upcoming models, I will definitely provide a .pb file, since I am training new models with keras anyways. > Would you be able to provide access to your models in .tflite, tfjs_layers_model or keras formats?I am most likely not putting work and time into converting them into a standard format if that's your question, sorry.====="", '@justadudewhohacks @blakjakau any way to convert these models to .tflite ?=====', 'Hi @blakjakau have you managed to convert it eventually?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/531;Face matcher is not giving the correct output while matched;1;open;2020-01-21T10:44:31Z;2020-08-03T14:33:08Z;"@justadudewhohacks thanks for the beautiful library.but this is not helpful in face matching.please check the below code for recognition as it gives wrong descriptors.**CODE**    const labels = [      {name:'Raj',img:      [        {personimg:'assets/imgs/raj1.jpg'},        {personimg:'assets/imgs/raj2.jpg'},        {personimg:'assets/imgs/raj3.jpg'}      ]     }    ]    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 0, i <= 2, i++) {          console.log('.img[i].personimg',label.img[i].personimg)          const img = await faceapi.fetchImage(label.img[i].personimg)          const detections = await faceapi.detectSingleFace(img,new faceapi.SsdMobilenetv1Options({ minConfidence: 0.9 })).withFaceLandmarks().withFaceDescriptor()          if (!detections) {            throw new Error(`no faces detected for ${label.name}`)          }          descriptions.push(detections.descriptor)        }          return new faceapi.LabeledFaceDescriptors(label.name, descriptions)      })    )}**matcher code** const labeledFaceDescriptors = await this.loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)    // console.log(labeledFaceDescriptors,faceMatcher)    const displaySize = { width: myImg.offsetWidth, height: myImg.offsetHeight }    //Detect Faces from input IMAGE     faceapi.matchDimensions(canvas, displaySize)    const detections = await faceapi.detectAllFaces(myImg).withFaceLandmarks().withFaceExpressions().withFaceDescriptors()    console.log('detections',detections)    const resizedDetections = faceapi.resizeResults(detections, displaySize)    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    // const results = faceMatcher.findBestMatch(detections.descriptor) // for detectSingleFace    const results = resizedDetections.map(d =>      faceMatcher.findBestMatch(d.descriptor))    console.log('results',results)Gives wrong matchRef Img: ""https://i.ibb.co/DfP6VMf/raj2.jpg""Query Img: ""https://i.ibb.co/ZN47xBY/test1.jpg""";"[' Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri(\'../../../../assets/models/\'),      faceapi.nets.faceLandmark68Net.loadFromUri(\'../../../../assets/models/\'),      faceapi.nets.faceRecognitionNet.loadFromUri(\'../../../../assets/models/\'),      faceapi.nets.faceExpressionNet.loadFromUri(\'../../../../assets/models/\'),      faceapi.nets.ageGenderNet.loadFromUri(\'../../../../assets/models/\'),      faceapi.nets.ssdMobilenetv1.loadFromUri(\'../../../../assets/models/\'),    ]).then((value: any) => {}),loadLabelledImages = () => {    const labels = [""labels""],    return Promise.all(      labels.map(async (label) => {        const descriptions = [],        for (let i = 1, i <= 5, i++) {          const image = await faceapi.fetchImage(            `../../../../assets/labelled_images/${label}/${i}.jpg`          ),          const detections = await faceapi            .detectSingleFace(image)            .withFaceLandmarks()            .withFaceDescriptor(),          descriptions.push(detections.descriptor),        }        return new faceapi.LabeledFaceDescriptors(label, descriptions),      })    ),  },=====']"
https://github.com/justadudewhohacks/face-api.js/issues/524;getting error in node js while rendering the locally saved user images;2;open;2020-01-16T07:54:21Z;2020-01-28T10:40:45Z;UnhandledPromiseRejectionWarning: TypeError: Only absolute URLs are supported const img = await faceapi.fetchImage(`./labeled_images/${label}/${i}.jpg`);['faceapi.fetchImage uses the fetch API under the hood, which does not allow you to read images from disk / access file system directly, it requests an image over the network.To read images from disk in nodejs use the canvas package, as shown in the examples or use the filesystem API of tfjs-node.=====', 'Thanks  for the response @justadudewhohacks .Could u pls tell me how to stream a rtsp video for face recognition.=====']
https://github.com/justadudewhohacks/face-api.js/issues/522;Face Detection custom dataset;1;open;2020-01-10T20:12:38Z;2020-01-28T10:20:32Z;Hello,Your projects are really awesome! I read Readme file and it says you used a custom dataset of ~14K images labeled with bounding boxes to train tiny yolo model. Would you mind sharing the dataset? So I can train my own model on it. Thanks!Felipe;['> Would you mind sharing the dataset? So I can train my own model on it.Unfortunately I am not able to share this dataset sorry, but if you want to train your own face detector, get the WIDER face dataset, this one is publicly available and the standard dataset for training face detectors.=====']
https://github.com/justadudewhohacks/face-api.js/issues/520;[QUESTION]: Login form HTML;3;open;2020-01-09T13:30:39Z;2020-02-03T03:45:07Z;It's possible to make a login form using face detection?;"[""yes possible. I've working on a asp.net project in which i used login with face id. ====="", '@abdulazizsapra Any tutorial?=====', 'In case you want to create a basic implementation for a web app that uses face recognition follow this tutorial: https://youtu.be/AZ4PdALMqx0 (also linked on this repo) and start by creating a classifier that is able to recognize your face. Once you figured that out use a webcam as input instead of an image that you uploaded and link the result from the recognition to your existing login system for your app. Good luck!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/519;Angular New Project, including faceapi.js gives Built error;7;open;2020-01-09T11:50:59Z;2021-02-02T16:30:15Z;"===========================ERROR in node_modules/face-api.js/build/commonjs/NeuralNetwork.d.ts:8:9 - error TS1086: An accessor cannot be declared in an ambient context.8     get params(): TNetParams | undefined,==========================I have installed face-api as **npm i face-api.js**Version of Typescript- ""typescript"": ""~3.5.3"" for Angular/cli latest version  is **""@angular/cli"": ""~8.3.22"",**I have added one line in the component.tsimport * as faceapi from 'face-api.js',Thanks a lot and best wishes..";"['As per the latest youtube video from web dev simplified and your tutorial we need to copy and paste the js from dist folder into angular assets folder and then run add npm i node-fetch..Also in index.html ->   <script type=""module"" src=""./assets/face-api.min.js""></script>Also in the component.ts folder -> import * as faceapi from \'../assets/face-api\',console.log(faceapi), confirms it has loaded. So please close this issue.=====', ""Same issue here. Can someone give a full example with Angular? I'm really lost====="", 'Add face-api.js to the scripts array (angular.json)""scripts"": [ ""./node_modules/face-api.js/dist/face-api.min.js"" ]Do not import faceapi into your component or service.Just declare it after your imports like so:declare var faceapi: any,=====', ""@somogyi-develappers Thanks!, it worked, I was able to access all the face-api functionality from the given changes. But I wonder why the import thing didn't worked, because for tensorflow.js it works.Hint for others: After making given changes try restarting the server [LiveDemo](https://imhimanshoe.github.io/Angular-faceApiDemo/)  and used [AngularCode](https://github.com/ImHimanShoe/Angular-face-Api.jsDemo)====="", 'I was getting same error. Fixed it by setting ""skipLibCheck"" : true  in tsconfig.json under the ""compilerOptions""=====', ""> @somogyi-develappers Thanks!, it worked, I was able to access all the face-api functionality from the given changes. But I wonder why the import thing didn't worked, because for tensorflow.js it works.> Hint for others: After making given changes try restarting the server [LiveDemo](https://imhimanshoe.github.io/Angular-faceApiDemo/) and used [AngularCode](https://github.com/ImHimanShoe/Angular-face-Api.jsDemo)@Im-Himanshu Thanks for your reference. Can you pls share how shall i upload a image and detect face or match it with other images====="", ""> @somogyi-develappers Thanks!, it worked, I was able to access all the face-api functionality from the given changes. But I wonder why the import thing didn't worked, because for tensorflow.js it works.> Hint for others: After making given changes try restarting the server [LiveDemo](https://imhimanshoe.github.io/Angular-faceApiDemo/) and used [AngularCode](https://github.com/ImHimanShoe/Angular-face-Api.jsDemo)live demo and code was deleted... can you please repost?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/517;Documentation confusion: It's there a way to load the models from a variable instead that from the files and use it later;3;open;2020-01-03T19:31:09Z;2021-05-05T16:23:50Z;In the documentation looks like the only way to load the models and pass it to the instance is from the files on the static folder:`await faceapi.nets.ssdMobilenetv1.loadFromUri('/models')`But i have observed that we can use a Float32Array but i can't find a way to pass it to the faceapi instance, this is what i have tried:`let net = new faceapi.TinyFaceDetector()``let weights = await faceapi.fetchNetWeights('/models/tiny_face_detector_model-shard1.weights')``console.log('weights')``console.log(weights)``net.load(weights)``console.log('net')``console.log(net)``await faceapi.nets.tinyFaceDetector.loadFromWeightMap(weights)` -> In this line fails`const detection = await faceapi.detectSingleFace(contextForFaceDetection.canvas, new faceapi.TinyFaceDetectorOptions({ scoreThreshold: configuration.FACE_SCORE_ADMISSION }))`But i get:![image](https://user-images.githubusercontent.com/16965508/71744463-b1420c80-2e3d-11ea-8850-0d64e3bb7265.png)Can someone help me to clarify this, in the documentation is not so clear how i can load the model without using the methods to access to the files?The model weights a lot and i wuold like so save it into a variable to reuse it using the localStorage or the IndexedDB;"['> await faceapi.nets.tinyFaceDetector.loadFromWeightMap(weights)Why are you calling `loadFromWeightMap` after loading the weights into your local net instance?`await faceapi.nets.tinyFaceDetector.load(weights)` should do the job for you.=====', '@lagcamgc has your problem been solved? I found `tiny_face_detector_model.bin` and `ssd_mobilenetv1_model.bin` seems to have some problems:```jsimport * as faceapi from ""face-api.js"",const float32Array_1 = await faceapi.fetchNetWeights(  ""/weights/face_landmark_68_tiny_model.bin""),console.log(float32Array_1), // [6.35521664824198e+30, -1.8255705378378978e-13, ...]const float32Array_2 = await faceapi.fetchNetWeights(  ""/weights/tiny_face_detector_model.bin""),console.log(float32Array_2), // Error: byte length of Float32Array should be a multiple of 4```![1](https://user-images.githubusercontent.com/47501692/117122964-02169c00-adc9-11eb-9fad-1578c2922c45.png)and all models will fail to load:```jsconst net = new faceapi.FaceLandmark68TinyNet(),const float32Array_1 = await faceapi.fetchNetWeights(  ""/weights/face_landmark_68_tiny_model.bin""),await net.load(float32Array_1), // Error: Based on the provided shape, [1,1,32,32], the tensor should have 1024 values but has 578```![2](https://user-images.githubusercontent.com/47501692/117122980-093daa00-adc9-11eb-8930-fe959e047b6b.png)I am using the module provided by [@vladmandic/face-api ](https://github.com/vladmandic/face-api). Even though I am using the [face-api.js](https://github.com/justadudewhohacks/face-api.js/tree/master/weights) module, I still make the above error. Am I missing something?=====', 'Hello! @awdr74100 at the end of the day i end up using a library called picojs, so short answer... no, i was not able not make it work even with the help of the previous comment=====']"
https://github.com/justadudewhohacks/face-api.js/issues/515;Can this work in react native webview;2;open;2019-12-27T15:00:43Z;2021-02-10T05:46:52Z;Hello, I saw through discussion there is no fully native support for face.api js through tensorflow, however would there be functionality through react webviewhttps://facebook.github.io/react-native/docs/webview.htmlThanks;"[""I do not see any reason, why this wouldn't work in a webview, I haven't tried it out yet though.====="", ""I don't think `react-native-webview` supports webgl yet so this might be slow. Although my react native knowledge is limited, so it'd be great if someone can reply with a workaround =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/514;Help combining Age / Gender / Mood / Face Detection through webcam;3;open;2019-12-27T11:31:56Z;2020-06-10T06:56:10Z;I don't know what I'm doing wrong.  Any help?  I know its just a matter of decoupling the codes into one script but i seem to be getting something wrong.  Help please or code?Thank you!;"['![face](https://user-images.githubusercontent.com/59276010/71523320-730b9100-2903-11ea-8315-ff89c577eb8e.png)This is what I need to do but my <script> tag becomes so messed up when i merge all of them.  I was hoping if you guys already have a page built that incorporates all three (3) functionalities in one page already (Age / Gender / Mood)  .  Thanks in advance for any help.=====', ""Use a [bundler](https://medium.com/@gimenete/how-javascript-bundlers-work-1fc0d0caf2da). Check out [Parcel](https://parceljs.org/), it's simple and I like it a lot.[You can check a project of mine that uses it with face-api.js if that helps.](https://github.com/lucasavila00/filtroume/blob/master/viewer/src/ts/main.ts)I'm sorry but there is no easy way for you. You will have to learn some stuff about javascript ecosystem in order to do what you want.====="", '```  setInterval(async () => {    const detections = await faceapi      .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())      .withFaceLandmarks()      .withFaceExpressions()      .withAgeAndGender(),    const resizedDetections = faceapi.resizeResults(detections, displaySize),    canvas.getContext(""2d"").clearRect(0, 0, canvas.width, canvas.height),    faceapi.draw.drawDetections(canvas, resizedDetections),    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),    faceapi.draw.drawFaceExpressions(canvas, resizedDetections),```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/512;Error using GPU;1;open;2019-12-26T03:49:09Z;2020-01-28T10:58:37Z;Hi!When I using GPU I get this error:E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERRORI can found a solution, but don't know how to implement:https://github.com/tensorflow/tensorflow/issues/24496`I did try compiling from source, but ran into the same issue. I was finally able to fix my problem was setting config.gpu_options.allow_growth = True.`;['This error does not come from face-api.js, probably from tfjs-node-gpu?=====']
https://github.com/justadudewhohacks/face-api.js/issues/510;Face Extraction ?;2;open;2019-12-21T06:47:01Z;2020-01-20T22:24:58Z;Having images saved in a folder we can easily detect faces and do whatever functionality we want but I want to know how to extract faces and save it to the image folder or any database. And when we encounter the same images it will be detected.???;"[""We could extract the face descriptor of the reference image(s) and save it into the database. When we have a query image, we extract the query image's face descriptor and compare against the reference image descriptor stored in the database.Heres an example:**faceRecognitionService.js**```const faceapi = require('face-api.js'),const path = require('path'),const fs = require('fs'),const {createCanvas, Image, Canvas} = require('canvas'),faceapi.env.monkeyPatch({Canvas, Image})class faceRecognitionService {    //asynchronous constructor    constructor() {        return (async () => {           await this.loadModels(),            return this,         })(),    }        //a function to load models    async loadModels(){        const modelURL = path.join(__dirname, '../', './models'),        await faceapi.nets.tinyFaceDetector.loadFromDisk(modelURL),        await faceapi.nets.faceLandmark68TinyNet.loadFromDisk(modelURL),        await faceapi.nets.faceRecognitionNet.loadFromDisk(modelURL),    }        //a funtion that returns Float32Array of face descriptor of an image    //arguments - imagePath: String    async getFaceDescriptor(imagePath){            const imageFile = await fs.readFileSync(imagePath),                //create canvas        const canvas = createCanvas(900, 900),        const ctx = canvas.getContext('2d'),        const img = new Image()        img.onload = async() => ctx.drawImage(img, 0, 0)        img.onerror = err => { throw err }        img.src = imageFile                    //face detection        const option = new faceapi.TinyFaceDetectorOptions({            inputSize: 512,            scoreThreshold: 0.6        }),        const useTinyModel = true,        const faceDescriptor = await faceapi        .detectSingleFace(canvas, option)        .withFaceLandmarks(useTinyModel)        .withFaceDescriptor(),        return faceDescriptor.descriptor,    }   //a function that returns true if referenceFace and queryFace matches   //arguments: queryImageFaceDescriptor : Float32Array of query face descriptor   //referenceImageFaceDescriptor: Array of Float32Arrays of reference face descriptors   //referenceImageFaceName: String    async isFaceMatch(queryImageFaceDescriptor, referenceImageFaceDescriptors, referenceImageFaceName){                const labeledDescriptors = await new faceapi.LabeledFaceDescriptors(referenceImageFaceName, referenceImageFaceDescriptors),        //console.log(labeledDescriptors),        const faceMatcher = await new faceapi.FaceMatcher(labeledDescriptors, 0.5),        const result = await faceMatcher.findBestMatch(queryImageFaceDescriptor),        if(result._label == referenceImageFaceName && result._distance < 0.5){            return true        }        else return false,   }}module.exports = {faceRecognitionService}```**test.js**```const {faceRecognitionService} = require('./faceRecognitionService'),const path = require('path'),//multiple reference image allows better face recognitionconst referenceImagePath1 = path.join(__dirname, '../', './testImage.jpg'),const referenceImagePath2 = path.join(__dirname, '../', './testImage2.jpg'),const queryImagePath = path.join(__dirname, '../', './testImage4.jpg'),const test = async()=>{    const test = await new faceRecognitionService(),       //this array could be saved to database and retrieved on demand    const refImageDescArray = []    await refImageDescArray.push(await test.getFaceDescriptor(referenceImagePath1)),    await refImageDescArray.push(await test.getFaceDescriptor(referenceImagePath2)),            const queryImageDesc =  await test.getFaceDescriptor(queryImagePath),    const result = await test.isFaceMatch(queryImageDesc, refImageDescArray, 'John Doe'),    console.log(result), //returns true if query face and reference face matches    }test(),```Hope this helps.====="", 'You can use the [sharp](https://github.com/lovell/sharp) module (`npm install sharp`) and once you have the detections you can do something like this:```jsconst sharp = require(\'sharp\'),...const detections = await faceapi.detectAllFaces(img),crop(detections),function crop(matches) {  console.log(\'found\', matches.length, \'faces\'),  matches.forEach((match, index) => {    sharp(imgPath)    .extract({      left: Math.round(match._box._x),      top: Math.round(match._box._y),      width: Math.round(match._box._width),      height: Math.round(match._box._height)    })    .toFile(""out_"" + index + "".jpg""),  }),}```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/509;[Feature Request] Support WebGLFramebuffer as input;2;open;2019-12-19T00:08:06Z;2020-01-28T11:06:04Z;I'm using a webcam video but I'm showing it on three.js.I would like to be able to save processing time by being able to pass the texture I already extracted from the stream as input to the neural net.Right now I'm using the HTMLVideoElement as input and so the frame is being extracted twice.By doing this I can also make sure the frame I'm showing was the same one that was processed.Right now it errors here: https://github.com/justadudewhohacks/face-api.js/blob/cb52dab46ecfd7585e17fb5a671eddcb2f7cd0ee/src/dom/toNetInput.ts#L39But if the FrameBuffer is accepted then data could be retrieved with [WebGLRenderingContext.readPixels()](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/readPixels) into an [ArrayBufferView](https://developer.mozilla.org/en-US/docs/Web/API/ArrayBufferView) which could be used to create the [tensor](https://js.tensorflow.org/api/latest/#tensor).@justadudewhohacks are you okay with it? Should I submit a PR?;"[""I'm trying to pass a custom tensor to face api but it wont understand the data.Should I resize data or something before passing it down?```tsconst extractTensorFromFrameBuffer = (): tf.Tensor3D => {  const x = 0,  const y = 0,  const width = _videoEl?.videoWidth!,  const height = _videoEl?.videoHeight!,  const format = _gl!.RGB,  const type = _gl!.UNSIGNED_BYTE,  if (pixels == null) {    pixels = new Uint8Array(width * height * 3),  }  _gl!.bindFramebuffer(_gl!.FRAMEBUFFER, _frameBuffer),  _gl!.readPixels(    x,    y,    width,    height,    format,    type,    pixels,  ),  console.log({ pixels }),  return tf.tensor(pixels, [height, width, 3]),},====="", 'Wait I do not entirely get it. > But if the FrameBuffer is accepted then data could be retrieved with WebGLRenderingContext.readPixels() into an ArrayBufferView which could be used to create the tensor.Why are you not simply doing that in your code and pass the constructed tensor to face-api?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/501;Save models into db;1;open;2019-12-10T06:18:10Z;2019-12-12T01:24:27Z;"Hi,I would like to save detected faces into a database for later repeat recognition. I keep failing in doing this because of not understanding the data structure. (can be any db but preferable mysql)Can you provide some clue how to store a detected face landmarks and gender into a db and how to get the data into face-api again after a reboot ? this way the system can be ""self learning"" recognising  recurrent faces";"['Hi, You can create a varchar field and save the descriptors.In this code I parse all the float values in a string and encoding in a json, later you can decode it.```\t\t\tcase ""/GetTemplate"":\t\t\t\tresponse.writeHead(200, {\t\t\t\t\t""Content-Type"": ""text\\plain""\t\t\t\t}),\t\t\t\tvar form = new formidable.IncomingForm(),\t\t\t\tform.parse(request, async function (err, fields, files) {\t\t\t\t\tlet buff = new Buffer(fields.picture, \'base64\'),\t\t\t\t\tconst img = await commons.canvas.loadImage(buff),\t\t\t\t\tlet detections = await faceapi.detectSingleFace(img, faceDetectionOptions).withFaceLandmarks().withFaceDescriptor(),\t\t\t\t\tif (detections) {\t\t\t\t\t\tlet encoding = {},\t\t\t\t\t\tlet vecEnco = [],\t\t\t\t\t\tfor (var i = 0, i < detections.descriptor.length, i++)\t\t\t\t\t\t\tvecEnco.push(detections.descriptor[i]),\t\t\t\t\t\tencoding.Encoding = ""["" + vecEnco.toString() + ""]"",\t\t\t\t\t\tconsole.log(""GetTemplate: template encontrado""),\t\t\t\t\t\tresponse.write(JSON.stringify(encoding)),\t\t\t\t\t}\t\t\t\t\telse { \t\t\t\t\t\tconsole.log(""GetTemplate: template NO encontrado""),\t\t\t\t\t\tresponse.write(null),\t\t\t\t\t}\t\t\t\t\tresponse.end(),\t\t\t\t}),\t\t\t\tbreak,```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/495;SSD Mobilenet v1: not detecting faces on some systems;3;open;2019-12-06T16:17:26Z;2019-12-16T15:40:08Z;I developed a demo code for face detection using face-api js library, below are the details of model I have used - **Model** : SSD Mobilenet v1**Function** : faceapi.detectAllFaces(canvasElement)Demo code is able to detect faces on other system, but it is not working on the systems mentioned below**System Hardware Info**:Laptop model : Lenovo Thinkpad L440CPU : Intel i5 - 4210MRAM : 8 GBGraphics : Intel HD Graphics 4600**System Software Info**:OS : Windows 7 ultimate 64-bitChrome version : 78.0.3904.108 (Official Build)(64-bit)Observations : - faceapi.detectAllFaces function returns no face but faces are present in the image coming from webcam- Intel HD Graphics driver stopped working and restarted in between few times.- Live Demo site with webcam face tracking is also not working properly with ssd mobilenet v1 model on this system;"['Hi @justadudewhohacks ,There seems an issue with the tfjs webgl backend, Face-api is detecting faces when we use CPU as tfjs backend.It is being fixed by tfjs team. But the current version of face-api is not supporting latest tfjs version.When are you planning to upgrade it?=====', 'This weekend.=====', ""Hi @justadudewhohacks,This Issue still persist with the latest version of face-api and tfjs.Code works properly with 'cpu' backend on tfjs but fails to give proper result on 'webgl' backend on tfjs on the system mentioned above.Any suggestions/solution?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/494;False positive examples. How to improve matching accuracy?;2;open;2019-12-05T08:20:07Z;2019-12-06T23:33:03Z;This is a great library to detect faces, landmarks and expressions. Is it possible to improve the accuracy of face matching? I found that Margot Robbie and Jaime Pressly often produce false positive matches. I tested the same pictures using Face++ and it also produced false positive matches for them (distance is 0.42 below).![face-api-js-jaime-and-margo](https://user-images.githubusercontent.com/7564876/70215614-78f6c380-1703-11ea-88d9-c1643c153e9b.png)Elijah Woods and Daniel Radcliffe also produced false positives matches (distance is 0.38 below).![face-api-js-elijah-and-daniel](https://user-images.githubusercontent.com/7564876/70215708-b3f8f700-1703-11ea-910d-56b588cd0d96.png)They didn't score high using Face++ and all pictures of Elijah and Daniel did not match.Is it possible to use these examples to improve the CNN model(s) or the euclidean distance algorithm? What tools were used to create the TensorFlow models?Thank you.;"['The problem is not the models, you are comparing two unaligned faces with each other, which is invalid input for the face recognition model. The BBT Face Similarity example works on aligned and cropped face images as you can see from the initial images provided in the example.You first have to perform face detection + alignment, which is done by the example shown in the ""Face Recognition"" tab of the web GUI.=====', '@justadudewhohacks, I will try that out. Thanks so much.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/493;How to control the accuracy of detectFaceLandmarks?;1;open;2019-12-04T17:57:58Z;2019-12-06T09:08:14Z;I tried to set TinyFaceDetectorOptions to control the accuracy of the result from detectFaceLandmarks, however it seems not work, even I set the scoreThreshold=0.9, which is expected that it's hard to find the result, however the detectFaceLandmarks still returns a non-null result.Is there a way to control the accuracy of detectFaceLandmarks?;['No there is no such option. The scoreThreshold parameter determines the minimum confidence of the face detection algorithm. It has nothing to do with the landmark regression model.=====']
https://github.com/justadudewhohacks/face-api.js/issues/492;Question: What would it take to detect hair color?;1;open;2019-12-04T09:45:58Z;2019-12-06T09:28:24Z;Just wondering maybe you can help me, is there a way to detect hair color does the face have?I mean I can get the rectangle top part and sample some pixels for RGB 😄 , but it should be a little bit more sophisticated.In my case, besides gender and age, I also need to know hair color.;['You could train your own small CNN to classify the hair color. All you would need is some training data. Maybe you can create your own dataset by labeling images yourself, you might not even need that much samples to get good results.=====']
https://github.com/justadudewhohacks/face-api.js/issues/491;node-js examples not compiling;17;open;2019-12-02T23:44:17Z;2021-02-26T14:46:21Z;To try and run the examples I do the following with a freshly cloned repo:```cd examples/examples-nodejsnpm itsc faceDetection.ts```I get the following error:```node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 556 more ..., readonly WAIT_FAILED: number, }', but here has type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 557 more ..., readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number, }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../usr/lib/node_modules/typescript/lib/lib.dom.d.ts:16485:13    16485 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```I have the following:tsc --version: 3.7.2node --version: 12.13.0I have tried adding skipLibCheck: true to tsconfig.json but it did not do any difference.Do you have any idea why it is not working for node?;"[""This command compile.```tsc --skipLibCheck faceDetection.ts```But then it doesn't workerror:```UnhandledPromiseRejectionWarning: TypeError: this.backend.register is not a function    at Engine.registerTensor (/home/ff/nodejs/face-api.js/node_modules/@tensorflow/tfjs-core/dist/engine.js:468:30)```====="", ""Please check out @russellwmy's answer in this issue #376. Make sure you are using the same typescript (take a look at the package.json of the examples folder) and tfjs-core version as face-api.js.====="", 'same error, I tried #376, seems different errors. not work. I just used the default package.json.tsc --version  Version 3.7.3node --version v12.13.1tsc --skipLibCheck faceDetection.tsthere is no error.but when I run node faceDetection.js(node:25928) UnhandledPromiseRejectionWarning: TypeError: this.backend.register is not a function    at Engine.registerTensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\engine.js:468:30)    at new Tensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\tensor.js:246:21)    at Function.Tensor.make (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\tensor.js:261:16)    at makeTensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\ops\\tensor_ops.js:98:28)    at Object.tensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\ops\\tensor_ops.js:55:12)    at _loop_2 (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\io_utils.js:219:36)    at Object.decodeWeights (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\io_utils.js:223:9)    at E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\weights_loader.js:255:66    at Array.forEach (<anonymous>)    at E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\weights_loader.js:253:44(node:25928) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:25928) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.=====', 'got the reason. it seems the ""npm i"" in examples will get the tfjs 1.4.0 version, which is not compatible with face-api.js. =====', 'Thanks! Just to make this clear for anyone else, you can fix by removng the caret (^) in the package.json for  @tensorflow/tfjs-node and getting version 1.2.3 specifically. For example in the examples folder changing package.json to:```{  ""author"": ""justadudewhohacks"",  ""license"": ""MIT"",  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.2.3"",    ""canvas"": ""^2.5.0"",    ""face-api.js"": ""../../""  }}```=====', ""just cloned the repository, with tsc --version: 3.7.3node --version: v13.3.0```cd face-api.js/examples/examples-nodejsnpm its-node  faceDetection.ts```gives:```faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js'.1 import * as faceapi from 'face-api.js',```====="", 'Same scenario, and same results as @sbocconi.node --version: 12.4.0tsc --version: 3.7.4=====', '@sbocconi @imgh1010 What version of tfjs-node are you using? Did you change the version in package.json before installing with `npm install` as per my previous comment?=====', 'My package.json is as  currently available in the repository:```{  ""author"": ""justadudewhohacks"",  ""license"": ""MIT"",  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.4.0"",    ""canvas"": ""^2.6.0"",    ""face-api.js"": ""../../""  }}```and the error does not depend on the version of tensorflow, but on the fact that face-api.js cannot be found.I also tried with:```{  ""author"": ""justadudewhohacks"",  ""license"": ""MIT"",  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.4.0"",    ""canvas"": ""^2.6.0"",    ""face-api.js"": ""git+https://github.com/justadudewhohacks/face-api.js.git#master""  }}```but I get the same error, even after a `npm install face-api.js`=====', 'The solution was to first run```npm i && npm run-script build```in the root directory before trying to run the examples.=====', 'however,I found :examples/examples-nodejs/package.json {  ""author"": ""justadudewhohacks"",  ""license"": ""MIT"",  ""dependencies"": {    ""face-api.js"": ""../../""  #   this will happen circulate create  }}will circulate create face-api.js fold, is usually or not? @justadudewhohacks =====', 'when I do ""npm i"" in console.=====', '> The solution was to first run> > ```> npm i && npm run-script build> ```> > in the root directory before trying to run the examples.when I do “npm i && npm run-script build”,and I get:> @tensorflow/tfjs-node@1.7.0 install /home/dzy/Project/face-api.js/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-linux-1.7.0.tar.gz* Downloading libtensorflow[==============================] 15944787/bps 100% 0.0s* Building TensorFlow Node.js bindings> canvas@2.6.1 install /home/dzy/Project/face-api.js/node_modules/canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download [canvas] Success: ""/home/dzy/Project/face-api.js/node_modules/canvas/build/Release/canvas.node"" is installed via remotenpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})added 442 packages from 783 contributors and audited 1741 packages in 170.919s10 packages are looking for funding  run `npm fund` for detailsfound 8 low severity vulnerabilities  run `npm audit fix` to fix them, or `npm audit` for details> face-api.js@0.22.2 build /home/dzy/Project/face-api.js> rm -rf ./build && rm -rf ./dist && npm run rollup && npm run rollup-min && npm run tsc && npm run tsc-es6> face-api.js@0.22.2 rollup /home/dzy/Project/face-api.js> rollup -c rollup.config.jsloaded rollup.config.js with warnings(!) Unused external importsdefault imported from external module \'path\' but never usedsrc/index.ts → dist/face-api.js...created dist/face-api.js in 10.3s> face-api.js@0.22.2 rollup-min /home/dzy/Project/face-api.js> rollup -c rollup.config.js --environment minify:trueloaded rollup.config.js with warnings(!) Unused external importsdefault imported from external module \'path\' but never usedsrc/index.ts → dist/face-api.min.js...created dist/face-api.min.js in 14.5s> face-api.js@0.22.2 tsc /home/dzy/Project/face-api.js> tsc> face-api.js@0.22.2 tsc-es6 /home/dzy/Project/face-api.js> tsc --p tsconfig.es6.jsonAnd tsc faceDetection.ts,  get same error.=====', 'Worked fine for me:- npm install & npm run-script build (the remove commands for the directories fail on windows, remove them from package.json in the face-api.js\\package.json file)- run npm install in examples-nodejs- running tsc faceDetection.ts fails with an error (duplicate librarydefinition WebGL2 or something)- instead tsc --skipLibCheck faceDetection.ts and things run fine.=====', ""@ronaldkuip I am also that add '--skipLibCheck' , but is fine?====="", ""ts-node faceDetection.ts should run in examples-nodejs directory.Tsc -skipLibCheck.ts creates faceDetection.jsnode faceDetection.js then runs fine too.Van: homedawn <notifications@github.com>Verzonden: woensdag 6 mei 2020 02:52Aan: justadudewhohacks/face-api.js <face-api.js@noreply.github.com>CC: ronaldkuip <ronald.kuip@Live.nl>, Mention <mention@noreply.github.com>Onderwerp: Re: [justadudewhohacks/face-api.js] node-js examples not compiling (#491)@ronaldkuip<https://github.com/ronaldkuip> I am also that add '--skipLibCheck' , but is fine?—You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/491#issuecomment-624383634>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACV4DRCPYR5R362D6IYE3JDRQCYDJANCNFSM4JUN6MOA>.====="", ""**Hi everyone getting this error while running examples-nodejs using command => tsc faceDetection.ts**commons/env.ts:5:26** - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.5 import * as faceapi from 'face-api.js',                           ~~~~~~~~~~~~~commons/faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.1 import * as faceapi from 'face-api.js',                           ~~~~~~~~~~~~~faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.1 import * as faceapi from 'face-api.js',                           ~~~~~~~~~~~~~node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.d.ts:61:5 - error TS2416: Property 'stridedSlice' in type 'NodeJSKernelBackend' is not assignable to the same property in base type 'KernelBackend'.  Type '<T extends Tensor<Rank>>(x: T, begin: number[], end: number[], strides: number[], beginMask: number, endMask: number, ellipsisMask: number, newAxisMask: number, shrinkAxisMask: number) => T' is not assignable to type '<T extends Tensor<Rank>>(x: T, begin: number[], end: number[], strides: number[]) => T'.61     stridedSlice<T extends Tensor>(x: T, begin: number[], end: number[], strides: number[], beginMask: number, endMask: number, ellipsisMask: number, newAxisMask: number, shrinkAxisMask: number): T,       ~~~~~~~~~~~~node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.d.ts:64:5 - error TS2416: Property 'fusedBatchMatMul' in type 'NodeJSKernelBackend' is not assignable to the same property in base type 'KernelBackend'.  Type '(a: Tensor3D, b: Tensor3D, transposeA: boolean, transposeB: boolean, bias?: Tensor<Rank>, activation?: Activation) => Tensor3D' is not assignable to type '({ a, b, transposeA, transposeB, bias, activation, preluActivationWeights }: FusedBatchMatMulConfig) => Tensor3D'.64     fusedBatchMatMul(a: Tensor3D, b: Tensor3D, transposeA: boolean, transposeB: boolean, bias?: Tensor, activation?: Activation): Tensor3D,       ~~~~~~~~~~~~~~~~node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 556 more ..., readonly WAIT_FAILED: number, }', but here has type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, ... 557 more ..., readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number, }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../AppData/Roaming/npm/node_modules/typescript/lib/lib.dom.d.ts:16394:13    16394 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 6 errors.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/489;Compare and match same face in an image;6;open;2019-11-30T11:11:13Z;2019-12-06T09:59:03Z;Hello,I am trying to compare two faces in the same image and find the match.   Could you please let me know how do I achieve this by using detectAllFaces method.  ;"[""The Key to recognize Faces lays in comparing their `Descriptors`. These 128 Values represent the, as far as I can tell, the Landmarks of a Face - so the Postion of your Nose, your Eyes and so on.Read this [section](https://github.com/justadudewhohacks/face-api.js/#face-recognition-by-matching-descriptors) to know what it is about and how to use it.Using `FaceMatcher` is basically comparing two Descriptors which also happen to have a Label.There's also the [Euclidean Distance](https://github.com/justadudewhohacks/face-api.js/#euclidean-distance), which tells you - more or less the overall Distance of Points/Landmarks in an Image. So the farther away from the Value 0 the less likely it is that it is the same Person - and there's a Treshold of about 0.66 which tells you that it's most liely not the same Person. Hope that helps?  ====="", '> The Key to recognize Faces lays in comparing their Descriptors. These 128 Values represent the, as far as I can tell, the Landmarks of a Face - so the Postion of your Nose, your Eyes and so on.Just want to clearify something here, since this confuses a lot of people. The face descriptor does not have anything to do with the face landmarks. The face landmarks are comprised by 68 points (so it is a vector of length 136 comprising x and y values. The landmarks and the descriptor are produces by two different models.> I am trying to compare two faces in the same image and find the match. Could you please let me know how do I achieve this by using detectAllFaces method.Get all faces in the image with their corresponding descriptors and than compare the descriptors pairwise by computing the euclidean distance.=====', 'I am trying to find the faces in the selfie image holding their id card. but I got only one descriptors array from the below code.  Could you please let me know whats the issue ``` const resultSelfie = await faceapi    .detectAllFaces(selfie, getFaceDetectorOptions(faceDetectionNet))    .withFaceLandmarks()    .withFaceExpressions()    .withFaceDescriptors(), ```=====', 'If you only get one object back from the API call, than the face detector was able to find one face only in your image.=====', 'But my image have 2 face (Selfie _+ ID card) .  Is there any specific resolution for the uploaded image =====', 'You can try out different face detector options, first you can try to employ SSDMobilenetv1, which is the most accurate method currently. Another option would be to try out the TinyFaceDetector and tune the input size parameter. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/487;Consider newer SSD Mobilenet model versions;13;open;2019-11-27T20:09:13Z;2020-02-11T00:06:56Z;I see the current version only allows `ssdMobilenetv1` but seems like `v2` is already available. How hard is to get to use the newer version instead? Also, what's needed to _make it work_ on this project as a new model?Edit: I've seen some `v3` (_[here](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)_) but I don't know how stable it is. ;"[""In the course of the last couple of months I have been working hard on getting better and faster face detectors to face-api.js. SSD in fact is not the best method for face detection anymore, FPNs (feature pyramid networks) are achieving state of the art results nowadays.I am trying out different backbones for that purpose, mobilenetv3 is one of them. I have not decided yet, which backbone to use, still have to do some evaluation.I do not want to decide on any ETA yet, but the latest models I can come up with are much more lightweight and already achieve higher accuracy with way less parameters than the  SSDMobilenetV1 currently provided by face-api.js. I want to make sure, that the new models are as close to state of the art performance, while being as small and lightweight as possible.So the new model (maybe models) I will be releasing are going to deprecate the currently provided face detection models. That's one of the reasons I do not want do rush things.====="", ""That's awesome, thanks for your response! ====="", 'Any updates on this? No rush of course, just curious.=====', 'This was tagged with ""solution provided"" but still no news. Anything we can help with @justadudewhohacks ?=====', 'https://github.com/tensorflow/tfjs-models/tree/master/blazeface is also another alternative =====', 'One reason to add MobilenetV2 is that a face detection model trained on Openimage V4 is available here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.mdIt can increase accuracy of face detection due to larger dataset.=====', '@justadudewhohacks I have said above because I have experience that models trained on OpenImage V4 have much better accuracy and are more robust. The only hurdle to their use is the lack of support of MobileNetV2 by this library.=====', 'The model I am talking about is this one: http://download.tensorflow.org/models/object_detection/facessd_mobilenet_v2_quantized_320x320_open_image_v4.tar.gz=====', '> Any updates on this? No rush of course, just curious.Still working on this, I finally got the first versions of some models, that I am quite satisfied with. But still a lot of work has to be done, since I want these models not only to detect faces but also regress 5 point facial landmarks at the same time for face alignment.Unfortunately training such a model until convergence takes more than one to two weeks, which does slow down the entire process.=====', ""> https://github.com/tensorflow/tfjs-models/tree/master/blazeface is also another alternative@startupgurukul I recently saw this one yes, seems quite good on the first looks, but it was running very laggy on my phone and I couldn't find any detailed evaluation about mAP / AP compared to state of the art methods.====="", '> The model I am talking about is this one: http://download.tensorflow.org/models/object_detection/facessd_mobilenet_v2_quantized_320x320_open_image_v4.tar.gzThis one is 125mb in size, so very unlikely that someone will use this in a web application.=====', ""> > The model I am talking about is this one: http://download.tensorflow.org/models/object_detection/facessd_mobilenet_v2_quantized_320x320_open_image_v4.tar.gz> > This one is 125mb in size, so very unlikely that someone will use this in a web application.Out of curiosity if we wanted to use such a model, how could we go about doing so? I'm not well versed in ML and facial recognition but with the help of your API I have gotten by and learned a bit by doing some other research where appropriate. [Currently, I am just testing here-and-about](https://github.com/Infinitay/face-rec-photo-gallery) and **focused on accuracy** of the facial **recognitions**, primarily working with a primary dominant Asian facial data set.I was reading some other issues and read how, if I'm not mistaken and remembering correctly, you need to quantize the respective model, for example a [NasNet ](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet) or [mobilenet_v3](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet#mobilenet-v3-imagenet-checkpoints) model, in order to get the shards to use with face-api. That being said, I came across your [inflatable-unicorns](https://github.com/justadudewhohacks/inflatable-unicorns) repo that [has a project to quantize](https://github.com/justadudewhohacks/inflatable-unicorns/tree/master/quantize). Although, there aren't provided instructions and I'm having difficulty running it as a result and there is an error being thrown.I never posted an issue in the other repo because I wasn't sure if you were providing guidance on it since you decided to move it from this repo to another on it's own. However, if you're willing to assist me, I'll create the issue.====="", ""Give blazeface a try, now it has an option to just detect have without wasting CPU cycles on landmarks. PS: with eyes closed the landmarks model isn't doing great for Indians faces, Google may soon release a facemesh model soon, https://sites.google.com/view/perception-cv4arvr/facemesh![unnamed](https://user-images.githubusercontent.com/11027129/74201696-6e6f2200-4c90-11ea-90db-765843d046ed.gif)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/486;Getting descriptors using computeFaceDescriptor vs withFaceDescriptor;1;open;2019-11-25T09:08:35Z;2019-12-06T09:56:36Z;"I'm using NodeJS to do some facial recognition and I seem to get ""unknown"" matches when I use computeFaceDescriptor vs when I use `detectSingleFace().withFaceLandmarks().withFaceDescriptor()`. What's the difference between these two ways of finding a descriptor? Am I doing something wrong? This code returns an unknown match```const refImage = await canvas.loadImage(path),const descriptor = await faceapi.computeFaceDescriptor(refImage),const labeledDescriptors = new faceapi.LabeledFaceDescriptors(""somePerson"", [descriptor]),// Then compareconst queryImage = await canvas.loadImage(anotherPath),const singleQuery = await faceapi.detectSingleFace(queryImage, faceDetectionOptions)    .withFaceLandmarks()    .withFaceDescriptor(),const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors),// Outputs ""unknown""const bestMatch = faceMatcher.findBestMatch(singleQuery.descriptor),```This code returns ""somePerson"", which is the correct match```const refImage = await canvas.loadImage(path),const resultsRef = await faceapi.detectSingleFace(refImage, faceDetectionOptions)      .withFaceLandmarks()      .withFaceDescriptors(),const faceDescriptors = [resultsRef.descriptor],const labeledDescriptors = new faceapi.LabeledFaceDescriptors(""somePerson"", faceDescriptors),// Then compareconst queryImage = await canvas.loadImage(anotherPath),const singleQuery = await faceapi.detectSingleFace(queryImage, faceDetectionOptions)    .withFaceLandmarks()    .withFaceDescriptor(),const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors),// Outputs ""somePerson""const bestMatch = faceMatcher.findBestMatch(singleQuery.descriptor),```Also, as a sidenote, I'm looking at computing these descriptors in bulk for later use so if there's a more performant way of doing that than by using computeFaceDescriptor/withFaceDescriptor, I'd be happy to know!";"['> What\'s the difference between these two ways of finding a descriptor? Am I doing something wrong?The difference is that in the first example you skip face detection + face alignment.  `computeFaceDescriptor` only performs the `withFaceDescriptor()` step of the API call of your second example. This only works if your ""refImage"" is a cropped and aligned image of a face. If it is a regular image with all kinds of background and the face located at an arbitrary position in the image, the output will be garbage.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/485;Server Implementation Not Working;2;open;2019-11-25T07:35:08Z;2020-01-20T23:37:39Z;"Hello community,This might not be the issue with the faceapi but with my implementation, i am trying to do the face recognition in a real time. The frame comes from my webcam via socket as a base 64 string, all i am trying is to get it matched with my preloaded images.```function prepareCanvas(data) {  return new Promise((resolve, reject) => {    try {      const canvas = createCanvas(parseInt(data.width), parseInt(data.height))      var ctx = canvas.getContext(""2d"", { pixelFormat: ""RGBA24"" }),      var image = new Image(),      image.src = data.base64      ctx.drawImage(image, 0,0, parseInt(data.width), parseInt(data.height)),      resolve(tf.browser.fromPixels(canvas)),    } catch (err) {      console.log(""prepareCanvas error===>"", err)      reject(err)    }  })}``````async function matchWithVideoStream(data, classifierType) {  try {    let frame = await prepareCanvas(data)    const detections = await faceapi.detectAllFaces(frame, classifierType).withFaceLandmarks().withFaceExpressions().withFaceDescriptors(),    frame.dispose(),    const resizedDetections = faceapi.resizeResults(detections, { width: 800, height: 800 })    const results = resizedDetections.map((d) => {      return faceMatcher.findBestMatch(d.descriptor)    })    console.log('resuts', results)  } catch (err) {    console.log(""matchWithVideoStream error"", err)  }}```";"['On first sight I can not spot anything obvious causing your example to not work. Did you try to display the image to verify that you are indeed receiving the data / constucting the image tensor correctly?=====', ""I may have found a simpler solution to all this! Simply convert your image into _base64_ and store that as the _src_ property of the image!Imports include [node-canvas](https://github.com/Automattic/node-canvas):```jsconst canvas = require('canvas'),const { Canvas, Image } = canvas,faceapi.env.monkeyPatch({ Canvas, Image }),```Code:```js  const imgstring = await image2base64('./img.jpg'),  const data = new Image(),  data.src = new Buffer(imgstring, 'base64'),  data.width = 480,  data.height = 360,  const detections = await faceapi.detectAllFaces(data),```In my case I'm using [sharp](https://github.com/lovell/sharp) to load the image:```js  const lol = await sharp(imgPath).toBuffer(),  const data = new Image(),  data.src = lol,  data.width = 480,  data.height = 360,  const detections = await faceapi.detectAllFaces(data),```🙇\u200d♀ Thank you _face-api.js_ 🙇 =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/482;CPU vs GPU vs hardware side effects;4;open;2019-11-22T09:49:21Z;2019-12-08T02:05:37Z;Hello,I'm running FaceAPI on a raw webcam connected to a ChromeBit (so Chrome on ChromeOS). On the other hand I also capture raw audio using Script Processor to perform Speech Recognition (server side)It works very well to capture face, landmarks, etc ... so I let it run 1FPS to detect if there is someone in front of the camera.Each second the FaceAPI run the detection against the current frame. That create a CPU peak with side effects on the audio recording (look like some tremble / echo)My understanding is that even if JS is single thread, there is some side effect working on audio and video that could mess things up.Do you have some knowledge about tensorflow JS doing that kind of side effect ?I tried to move part of the audio processing in a Web Worker but there is still some issues;"[""Your question is less about TensorFlow and more about how javascript and browsers execute work. The browser will render choppy to the screen if the UI thread is blocked. This is the default thread. It is used for anything where the DOM interface is used on the object referenced. In this case, because you're processing a video stream, let's assume the processing of that video and handing off of that video to face-api is all being processed on the main UI thread. To optimize this, you could try only having the UI thread capture the video and convert it into data face-api could process. You could then run both on a worker thread which would not block the CPU thread. You end up possibly hitting a second issue though, which is that CPU bound processing is not the only issue. On machines with weak, old or integrated graphics cards, you can also become GPU bound. The UI thread also requires access to the GPU, and TensorFlow is not very polite in the way it splits work out to the GPU. Both cases result in choppy rendering which has nothing to do with the audio of a system. If your background process is dominating your CPU threads, that can also starve your browser of resources it needs to run the UI thread quickly. What you can do which will work, however, is to simplify the face detection model and framework used, or to offload this processing to the server where the language used to process it is a faster more low level threaded language. In the former camp, you lose features like landmarks and facial deltas and descriptors. In the latter, you can only scale by how much of your own processing power you want to pay for, and the bandwidth is the same as streaming video costs. Neither are ideal cases, but both work pretty well. https://github.com/auduno/headtrackr <-- this is a very very old project which used bleeding edge apis at the time and works fast, but has a limited feature set. It's incredible how fast you can get things to run when you lose the heft of tensorflow and big ML models.====="", ""Thanks for the explanation and the repo, very interresting I'll dig into it.I found some basic bug/optimisation on my code to narrow the CPU usage. I'll clean it to better use Worker.I found something weird, in Face-API.js the samples get a frame from the camera then setTimeout(). So I did the same thing with a threashold for the timeout (I saw other library doing also that)BUT, when I started to play with start/stop vs clearTimeout it seems this was scramble https://stackoverflow.com/questions/54753149/frames-captured-from-webcam-processed-and-drawn-on-canvas-appear-out-of-orderMay be it's my code, can we assume setTimeout / clearTimeout on webcam frame is sync (because JS engine is single threaded ?) I'll try to simplified my code to check that====="", 'Using setTimeout to defer to the next tick/event loop should really be considered a bad practice by now. Promises are way faster and nicer on the system than timeouts. They don’t run the same way, but produce the same result of yielding so other work can be done. It allows the javascript engine to decide how it wants to schedule the work and there isnt that nasty minumum 10ms limit between cycles like on setTimeout so all your broken apart work will actually finish faster. It really shows up when you’re running large batches of work  =====', 'Btw, if you do look at that headtrackr repo, fair warning that it requires you to do minor tweaks to the getUserMedia calls to make it work with the current standard. Remember, it was using draft specs and bleeding edge apis back when it came out and it was an abandoned project so it hasnt been fixed or advanced. Just an example of how fast and light things can be when optimized fully for constrained hardware. I wanted to get into easing and predictive tracking and pupils for glancing. I really dont care about facial features (ears, jaw, mouth, eyebrows, emotions) as much as i just want z depth, facial fingerprint, and view angle. If i know where your eyes are and how far away they are and where they’re looking and can animate field of view and depth of field and stuff in 3d, the head coupled perspective thing would get super cool for building some simple 3d games like world runners. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/480;descriptors, console errors;2;open;2019-11-21T06:43:54Z;2019-12-07T14:30:08Z;Good day. I was trying to figure out how will I get the descriptors of the two images that I will test both their similarities. When I try this code:![1](https://user-images.githubusercontent.com/58023366/69313841-a193ad80-0c6d-11ea-83fa-5fc47de99a22.PNG)I received in console log:![2](https://user-images.githubusercontent.com/58023366/69313853-ab1d1580-0c6d-11ea-8c01-2e0dddf3ff06.PNG)I'm still a novice at this kinds of topic but I'm trying my best to understand. Thank you for the future response. Regards;"[""The Error is telling you that you need load the right model first (in this case SsdMobilenetv1).You can find them [here](https://github.com/justadudewhohacks/face-api.js/tree/master/weights).So before trying to compare your Images do something like:```Javascriptasync function loadModel() {  await faceapi.loadSsdMobilenetv1Model('./assets/face-api-js/weights'),}```Also don't forget to call this Function somehow :).Where `./assets/face-api-js/weights` is your Directory containing the Files I've mentioned above. And you could also use [this Method](https://github.com/justadudewhohacks/face-api.js#face-recognition-by-matching-descriptors) to compare two images instead of using `euclideanDistance` but both is just fine. Have fun :)====="", 'To add to what @TonySpegel said, you also want to `await faceapi.detectSingleFace() ...` since it is async. Secondly you have to pass the descriptors and not the result objects to `facepi.euclideanDistance(result1.descriptor, result2.descriptor)`.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/479;Save results(descriptors);3;open;2019-11-20T22:56:51Z;2020-05-07T16:07:37Z;"When i save the results of faceapi.LabeledFaceDescriptors() to localstorage and then  JSON.parse results, I`m getting this error:FaceRecognizer.constructor - expected inputs to be of type LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array | Array<LabeledFaceDescriptors | WithFaceDescriptor<any> | Float32Array> even though im converting descriptors array to Float32Array.So the main question is how to correctly save the results (of faceapi.LabeledFaceDescriptors()) to JSON and also how to correctly parse them so I can use them with faceapi.FaceMatcher()Thank you for reply.I also tried this:const labeledFaceDescriptors = await loadLabeledImages()var json_str = ""{""parent"":"" + JSON.stringify(labeledFaceDescriptors) + ""}""// save the json_str to json file// Load json file and parsevar content = JSON.parse(json_str)for (var x = 0, x < Object.keys(content.parent).length, x++) {for (var y = 0, y < Object.keys(content.parent[x]._descriptors).length, y++) {var results = Object.values(content.parent[x]._descriptors[y]),content.parent[x]._descriptors[y] = new Float32Array(results),}}const faceMatcher = await createFaceMatcher(content),But the same error appeared";"['So somehow i manages to do this`// const labeledFaceDescriptors = await loadLabeledImages(),    // localStorage.setItem(""data"",JSON.stringify(labeledFaceDescriptors))    let f = await JSON.parse(localStorage.getItem(""data"")),    let a = [],    await f.map((item, index) => {      let arr = [],      arr[0] = new Float32Array(Object.keys(item._descriptors[0]).length),      arr[1] = new Float32Array(Object.keys(item._descriptors[1]).length),      for (let j = 0, j < 2, j++) {        for (let i = 0, i < Object.keys(item._descriptors[j]).length, i++) {          arr[j][i] = item._descriptors[j][i],        }      }      a[index] = new faceapi.LabeledFaceDescriptors(item._label, arr),    }),    console.log(a),    const faceMatcher = new faceapi.FaceMatcher(a, 0.6),`  =====', 'There are helpers for de/-serializing LabeledFaceDescriptors. This PR should answer your question: #397=====', 'From @powerdot> For community:>>`var labeledFaceDescriptorsJson = labeledFaceDescriptors.map(x=>x.toJSON())`>>and (not tested, but i think it works actually like this one)>`var labeledFaceDescriptors = labeledFaceDescriptorsJson.map( x=>faceapi.LabeledFaceDescriptors.fromJSON(x) ),`=====']"
https://github.com/justadudewhohacks/face-api.js/issues/478;Great work;1;open;2019-11-20T21:56:54Z;2019-12-07T14:34:12Z;Hey there just here to say that I follow you since opencv4nodejs and your work is awesome dude.I tried to use face-api.js and implement it in a react app, and it worked like a charm!I'm here to ask you something, could it be possible to add other classifiers or other models like cats and dogs for example? if yes what would be the path to it, I would be interested to learn about that.Anyway, keep up the great work! ;"[""Thanks!> I'm here to ask you something, could it be possible to add other classifiers or other models like cats and dogs for example? if yes what would be the path to it, I would be interested to learn about that.This is out of scope of this project, but I think there are plenty of tutorials on the web for how to train such a classifier with tensorflow. Then you can use tfjs-converter to use your tensorflow model with JS.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/475;Minimum number of images required for training data;1;open;2019-11-19T06:52:42Z;2019-12-07T14:40:32Z;Hi there,I had posted a question earlier and wanted to thank you for guiding in the correct path for it.I come again with a simple question but looking for an elaborate answer.What are the parameters that can be tweaked/fiddled with to achieve more accuracy on asian faces( specially thailand).???? Also will the number of images for training data affect the recognition result? We tried with only 3 photos per person for 10 people.The response is amazing with Indian faces (PS: i am from india), but its not that well with APAC regions. Although india comes under that.Kindly take out time to pursue this question and elaborate on the factors. Anything small could be really helpful for me.Thanks in advance,Regards from,A new recruit of the same boat;['> What are the parameters that can be tweaked/fiddled with to achieve more accuracy on asian faces( specially thailand).???? There is no parameter, unfortunately the model provided by face-api.js currently (which is the same as the dlib model) is biased towards western faces.> Also will the number of images for training data affect the recognition result?You probably mean the number of reference images. Possible, but there is no rule of thumb, you should play around with it and see what fits your case best. Usually 1 ref image is enough.=====']
https://github.com/justadudewhohacks/face-api.js/issues/474;Extremely different reading under the same lighting conditions;1;open;2019-11-18T23:35:01Z;2019-12-07T14:42:09Z;I am using TinyFacedetector with no options.readings for probabilities happiness/surprise were consistent at first (.01-.3). The next day, values seem to always be 3 or 5 decimal places (.003 or .005).Is there a reason why the detector isn't picking up as well? I did not change any settings[https://tinyurl.com/s7x93of](url);"[""> Is there a reason why the detector isn't picking up as well?Probably not enough training data with bad lightning conditions, would be my guess.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/472;How do a server face recognition with Node.js?;2;open;2019-11-14T22:36:18Z;2020-01-20T23:37:49Z;Hello everyone, well I have a question about face-api.js and I do not understand how a nodej.js server would read for example base64 or a file, and transfer it to a string where it says it was recognized or something , I have done it before with Angular and Typescript, but the thing now is to pass it as a service and for a server to take care of the heavy load of detecting the face and not the browser, since it is currently somewhat slow, I did not find solutions in java , more than javacv or opencv, and those solutions did not work for me when it is validation (from 1 to 1 not from 1 an) with only 3 or 12 images of the same person (probe with 3 and 12), and I have not found any tutorial of how to work in node.js (since in case I gave up on java) and now I am looking to do the recognition service only in node.js, but as I said, I only found the previous api that was face-recognition.js and I read that the same author made him obsolete, so I chose to look for this one but as I said before, I can't find a tutorial, more than the mini guide of the same github, which I did monkey.env which I don't know how I would do if it was just a server in node.js and that works in CanvasHtml I think, help me please: 'vin other words a web services in node.js;"['If you need real time, check detecting expression in #467, its same with recognition, using node-webRTC, if you only recognion a snapshot, you upload image from client and convert buffer image to Tensor and response with JSON Code from [stack overflow](https://stackoverflow.com/questions/58738178/need-recomendation-about-code-because-it-kill-my-programm)**NOTE**: consumption of CPU and RAM is very High```js""use strict"",require(""@tensorflow/tfjs-node""),const tf = require(""@tensorflow/tfjs""),const nodeFetch = require(""node-fetch""),const fapi = require(""face-api.js""),const path = require(""path""),const { createCanvas, createImageData } = require(""canvas""),const {  RTCVideoSink,  RTCVideoSource,  i420ToRgba,  rgbaToI420} = require(""wrtc"").nonstandard,fapi.env.monkeyPatch({ fetch: nodeFetch }),const MODELS_URL = path.join(__dirname, ""/weights""),const width = 640,const height = 480,Promise.all([  fapi.nets.tinyFaceDetector.loadFromDisk(MODELS_URL),  fapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL),  fapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL),  fapi.nets.faceExpressionNet.loadFromDisk(MODELS_URL)]),function beforeOffer(peerConnection) {  const source = new RTCVideoSource(),  const track = source.createTrack(),  const transceiver = peerConnection.addTransceiver(track),  const sink = new RTCVideoSink(transceiver.receiver.track),  let lastFrame = null,  function onFrame({ frame }) {    lastFrame = frame,  }  sink.addEventListener(""frame"", onFrame),  // TODO(mroberts): Is pixelFormat really necessary?  const canvas = createCanvas(width, height),  const context = canvas.getContext(""2d"", { pixelFormat: ""RGBA24"" }),  context.fillStyle = ""white"",  context.fillRect(0, 0, width, height),  const emotionsArr = {    0: ""neutral"",    1: ""happy"",    2: ""sad"",    3: ""angry"",    4: ""fearful"",    5: ""disgusted"",    6: ""surprised""  },  async function detectEmotion(lastFrameCanvas) {    const frameTensor3D = tf.browser.fromPixels(lastFrameCanvas),    const face = await fapi      .detectSingleFace(        frameTensor3D,        new fapi.TinyFaceDetectorOptions({ inputSize: 160 })      )      .withFaceExpressions(),    //console.log(face),    const emo = getEmotion(face),    frameTensor3D.dispose(),    return emo,  }  function getEmotion(face) {    try {      let mostLikelyEmotion = emotionsArr[0],      let predictionArruracy = face.expressions[emotionsArr[0]],      for (let i = 0, i < Object.keys(face.expressions).length, i++) {        if (          face.expressions[emotionsArr[i]] > predictionArruracy &&          face.expressions[emotionsArr[i]] < 1        ) {          mostLikelyEmotion = emotionsArr[i],          predictionArruracy = face.expressions[emotionsArr[i]],        }      }      //console.log(mostLikelyEmotion),      return mostLikelyEmotion,    } catch (e) {      return """",    }  }  let emotion = """",  const interval = setInterval(() => {    if (lastFrame) {      const lastFrameCanvas = createCanvas(lastFrame.width, lastFrame.height),      const lastFrameContext = lastFrameCanvas.getContext(""2d"", {        pixelFormat: ""RGBA24""      }),      const rgba = new Uint8ClampedArray(        lastFrame.width * lastFrame.height * 4      ),      const rgbaFrame = createImageData(        rgba,        lastFrame.width,        lastFrame.height      ),      i420ToRgba(lastFrame, rgbaFrame),      lastFrameContext.putImageData(rgbaFrame, 0, 0),      context.drawImage(lastFrameCanvas, 0, 0),      detectEmotion(lastFrameCanvas).then(function(res) {        emotion = res,      }),    } else {      context.fillStyle = ""rgba(255, 255, 255, 0.025)"",      context.fillRect(0, 0, width, height),    }    if (emotion != """") {      context.font = ""60px Sans-serif"",      context.strokeStyle = ""black"",      context.lineWidth = 1,      context.fillStyle = `rgba(${Math.round(255)}, ${Math.round(        255      )}, ${Math.round(255)}, 1)`,      context.textAlign = ""center"",      context.save(),      context.translate(width / 2, height),      context.strokeText(emotion, 0, 0),      context.fillText(emotion, 0, 0),      context.restore(),    }    const rgbaFrame = context.getImageData(0, 0, width, height),    const i420Frame = {      width,      height,      data: new Uint8ClampedArray(1.5 * width * height)    },    rgbaToI420(rgbaFrame, i420Frame),    source.onFrame(i420Frame),  }),  const { close } = peerConnection,  peerConnection.close = function() {    clearInterval(interval),    sink.stop(),    track.stop(),    return close.apply(this, arguments),  },}module.exports = { beforeOffer },```=====', ""I found [**a solution**](https://github.com/justadudewhohacks/face-api.js/issues/485#issuecomment-576459838) to a related problem that may help here.If you have an image on the hard disk, there is no problem:```js    const img = await canvas.loadImage('./images/myImage.jpg'),                const detections = await faceapi.detectAllFaces(img),```But if you don't want to write to disk first, you can just convert the image to _base64_, create a fake canvas, and you're done!For example:```js  const data = new Image(),  data.src = new Buffer(myImageAsBase64String, 'base64'),  data.width = 480,  data.height = 360,  const detections = await faceapi.detectAllFaces(data),```ps: my imports require [node-canvas](https://github.com/Automattic/node-canvas)```jsconst canvas = require('canvas'),const { Canvas, Image } = canvas,faceapi.env.monkeyPatch({ Canvas, Image }),```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/471;Would you mind sharing the datasets you use?;3;open;2019-11-14T21:21:08Z;2019-12-07T14:45:50Z;I want to train my own models and see I can get better then what you did, so having your datasets would be helpful;['I will list all the databases that I am training models on in future, for some of the recent models I already did. Most of them are trained on publicly available databases.=====', '@justadudewhohacks I see you shared the names for face detection ones. Which ones were you using for face keypoints?=====', 'I have been using the ibug challenge face dataset + a private dataset, I manually assembled, which I can not share.=====']
https://github.com/justadudewhohacks/face-api.js/issues/469;Where can I get model biased to asian faces?;3;open;2019-11-13T12:02:59Z;2019-12-07T14:47:06Z;The existing model is biased to western faces.Where can I found a model biased to asian faces.;['Are you talking about the face recognition model? There is currently no other model supported by face-api.js than the dlib one.=====', 'I am talking for face emotion prediction =====', 'Again there is none. Unfortunately datasets for facial expression recongition are very limited.=====']
https://github.com/justadudewhohacks/face-api.js/issues/465;Argument type in Canvas is not assignable to parameter type string | HTMLCanvasElement;1;open;2019-11-06T11:52:17Z;2019-12-07T14:51:48Z;Hi! Is there any way to convert node canvas into string or HTMLCanvasElement? I use js, not ts.![7qx4H1HkOxI](https://user-images.githubusercontent.com/42372955/68295449-80df2b80-00a3-11ea-80d0-98e2056dcf63.jpg);"[""Node canvas and HTMLCanvasElement are essentially the same to face-api.js. Not sure what kind of type checker you are using there, but simply tell it, that it's ok to pass node canvases into this function.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/450;Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA;6;open;2019-10-24T05:38:54Z;2021-01-03T02:50:39Z;"Logs: ```cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 42019-10-24 13:34:37.471423: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA/usr/local/lib/node_modules/ts-node/src/index.ts:245    return new TSError(diagnosticText, diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:commons/faceDetection.ts:19:10 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'SsdMobilenetv1' have no overlap.19   return net === faceapi.nets.ssdMobilenetv1            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~commons/faceDetection.ts:21:8 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'TinyFaceDetector' have no overlap.21     : (net === faceapi.nets.tinyFaceDetector          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~commons/faceDetection.ts:27:60 - error TS2345: Argument of type 'SsdMobilenetv1' is not assignable to parameter of type 'NeuralNetwork<any>'.  Types of property 'extractParamsFromWeigthMap' are incompatible.    Type '(weightMap: NamedTensorMap) => { params: NetParams, paramMappings: ParamMapping[], }' is not assignable to type '(weightMap: NamedTensorMap) => { params: any, paramMappings: ParamMapping[], }'.      Types of parameters 'weightMap' and 'weightMap' are incompatible.        Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.          Index signatures are incompatible.            Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.              Types of property 'flatten' are incompatible.                Type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-imag...' is not assignable to type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                    Types of property 'asScalar' are incompatible.                      Type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-imag...' is not assignable to type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                          Types of property 'as2D' are incompatible.                            Type '(rows: number, columns: number) => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-...' is not assignable to type '(rows: number, columns: number) => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core...'.                              Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                                Types of property 'asType' are incompatible.                                  Type '<T extends import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs...' is not assignable to type '<T extends import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(...'.                                    The 'this' types of each signature are incompatible.                                      Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                        Property 'relu6' is missing in type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' but required in type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...'.27 export const faceDetectionOptions = getFaceDetectorOptions(faceDetectionNet)                                                              ~~~~~~~~~~~~~~~~  node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor.d.ts:540:5    540     relu6<T extends Tensor>(this: T): T,            ~~~~~    'relu6' is declared here.    at createTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:245:12)    at reportTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:249:19)    at getOutput (/usr/local/lib/node_modules/ts-node/src/index.ts:357:34)    at Object.compile (/usr/local/lib/node_modules/ts-node/src/index.ts:415:32)    at Module.m._compile (/usr/local/lib/node_modules/ts-node/src/index.ts:493:43)    at Module._extensions..js (internal/modules/cjs/loader.js:700:10)    at Object.require.extensions.(anonymous function) [as .ts] (/usr/local/lib/node_modules/ts-node/src/index.ts:496:12)    at Module.load (internal/modules/cjs/loader.js:599:32)    at tryModuleLoad (internal/modules/cjs/loader.js:538:12)    at Function.Module._load (internal/modules/cjs/loader.js:530:3)```";"['What version of typescript are you using? face-api.js is compiled with TS 3.6.3, so upgrade your TS compiler.=====', 'I\'m using the latest version v3.6.4.<img width=""110"" alt=""螢幕快照 2019-10-29 上午9 54 15"" src=""https://user-images.githubusercontent.com/11001914/67731140-30782600-fa32-11e9-81ee-88bea35b2dd6.png"">=====', 'Me too...=====', 'Me too 3.7.4=====', 'I am on 3.9.3 and still get the same issue``` Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-07-21 16:00:24.813860: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1071ef570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-07-21 16:00:24.813925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version=====', 'I am using Typescript 4.1.3 and still same message> Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2, cpu backend was already registered. Reusing existing backend factory.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/445;sample.php:191 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0;1;open;2019-10-21T21:08:38Z;2019-10-26T07:35:47Z;"This errors always appears when i load the script of the model<img width=""866"" alt=""Screenshot 2019-10-22 at 5 07 35 AM"" src=""https://user-images.githubusercontent.com/3221892/67243346-f2a35c80-f489-11e9-812d-9d3705aa7890.png"">or you can view and inspect the element directlyhttps://appinsg.com/facial/sample.php";['This is probably because the URL you are loading the models from is not correct, e.g. the manifest. json files are not loaded correctly.Check the network tab and make sure models are loaded from the right URL.=====']
https://github.com/justadudewhohacks/face-api.js/issues/444;has block the finger print,the call is :getImageData;2;open;2019-10-21T04:46:56Z;2019-11-04T06:37:45Z;"## ProblemThe video output is smooth, but the bounding box and the predictions are broken as if the frame drops.## Source Code```this.peerVideo.addEventListener('play', () => {        if (this.canvas == null) {          this.canvas = faceapi.createCanvasFromMedia(this.peerVideo),          this.canvas.style.top = this.state.top,          this.canvas.style.left = this.state.left,          this.canvas.style.position = ""absolute"",          this.ctx = this.canvas.getContext('2d'),          document.body.append(this.canvas),        }        faceapi.matchDimensions(this.canvas, displaySize),        this.intervalId = setInterval(async() => {          if (this._isMounted) {            const detections = await faceapi.detectSingleFace(this.peerVideo, new faceapi.SsdMobilenetv1Options())              .withFaceExpressions(),            if (detections) {              const resizedDetections = faceapi.resizeResults(detections, displaySize),              if (this.ctx) {                this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height),              }              faceapi.draw.drawDetections(this.canvas, resizedDetections),              faceapi.draw.drawFaceExpressions(this.canvas, resizedDetections),            }          }        }, 1000 / 30),      }),```## Error Code```has block the finger print,the call is :getImageData```";['Please tell me whats the current and the expected behaviour?> has block the finger print,the call is :getImageDataWhere is this error thrown?=====', '`has block the finger print,the call is :getImageData `Where is this error coming from? Is it due to some setting in the browser? or a extension is causing this error?=====']
https://github.com/justadudewhohacks/face-api.js/issues/441;Examples work fine on chrome not on IE;3;open;2019-10-14T17:32:42Z;2020-03-25T20:19:24Z;Hi I am learning to use this very nice tool.It is possible to run the examples and my own local integrations on chrome.All other browsers dont work : Edge, IE11, firefox.1,As chrome is not my preferred browser i would like to get it running in something elseWhat am i missing or doing wrong. Is there a special need outside chrome.I do not get any error, it just does not work at all on IE11The examples keep loading forever without any progres. (green running line)2. it would be even great if it is possible to run just in node.js without browser, are there any examples of this ? (running in background with source an ip cam for face recognition)many thanks for every clue you can give on both :);"[""face-api.js definitely works in chrome and firefox, so use one of these. I haven't opened internet explorer in a long time to be honest, so there might be issues. If your preferred browser is IE you should probably overthink your choice in my opinion.Edit: I will not worry about IE support until edge will run on the chrome engine.====="", ""Thank you !Firefox works fine, i made a script error.I agree IE is not the way to go, javascript support is bad in IE.Are there any examples for server side face recognition ?I tried Nodejs examples, but these seem also to run in browser ( seen the GPU usage on client side, it looks like these run client side.)Maybe i am doing again something wrong.The goal is to incorporate face api.js in a home domotica scenario using an IP camera as presence detection.The clientside webpage is just a webinterface to the backend (tablet)Many thanks for this great interface ! I love it so far….MikeVan: Vincent Mühler [mailto:notifications@github.com]Verzonden: zaterdag 26 oktober 2019 09:44Aan: justadudewhohacks/face-api.js <face-api.js@noreply.github.com>CC: sledgemhammer <m_sledge_m@hotmail.com>, Author <author@noreply.github.com>Onderwerp: Re: [justadudewhohacks/face-api.js] Examples work fine on chrome not on IE (#441)face-api.js definitely works in chrome and firefox, so use one of these.I haven't opened internet explorer in a long time to be honest, so there might be issues. If your preferred browser is IE you should probably overthink your choice in my opinion.—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/441?email_source=notifications&email_token=AIVOHL2ZIYAW2O7OLBYPJLLQQPYN3A5CNFSM4JASAS62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECKCJLA#issuecomment-546579628>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIVOHL22UBXLKVQMZWX3RE3QQPYN3ANCNFSM4JASAS6Q>.====="", ""To make it work with Edge you need to set this config option:faceapi.tf.ENV.set('WEBGL_PACK', false),Works great for me on Edge now!If you only want to set it on IE/Edge environments you can do something like this:/** * detect IE * returns version of IE or false, if browser is not Internet Explorer */function detectIE() {  var ua = window.navigator.userAgent,  // Test values, Uncomment to check result …  // IE 10  // ua = 'Mozilla/5.0 (compatible, MSIE 10.0, Windows NT 6.2, Trident/6.0)',  // IE 11  // ua = 'Mozilla/5.0 (Windows NT 6.3, Trident/7.0, rv:11.0) like Gecko',  // Edge 12 (Spartan)  // ua = 'Mozilla/5.0 (Windows NT 10.0, WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0',  // Edge 13  // ua = 'Mozilla/5.0 (Windows NT 10.0, Win64, x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586',  var msie = ua.indexOf('MSIE '),  if (msie > 0) {    // IE 10 or older => return version number    return parseInt(ua.substring(msie + 5, ua.indexOf('.', msie)), 10),  }  var trident = ua.indexOf('Trident/'),  if (trident > 0) {    // IE 11 => return version number    var rv = ua.indexOf('rv:'),    return parseInt(ua.substring(rv + 3, ua.indexOf('.', rv)), 10),  }  var edge = ua.indexOf('Edge/'),  if (edge > 0) {    // Edge (IE 12+) => return version number    return parseInt(ua.substring(edge + 5, ua.indexOf('.', edge)), 10),  }  // other browser  return false,}=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/440;Which size  images  gives the best performance?;2;open;2019-10-14T13:06:55Z;2019-10-26T12:18:20Z;hi Vincent,im used many input size images ( images > 150 *150   ,  it you told in orther issuse) ! but i dont know Which size gives the best performance?;['The face-recognition net has an input size of 150x150, all other classification nets use an input size of 112x112, the size of the face after face extraction should be around the same size. The smaller they are the more blurrier they will be after upsampling and thus give worse results.So it depends on how large the face appear in your input image.=====', 'oh thanks for your anwser ^^=====']
https://github.com/justadudewhohacks/face-api.js/issues/438;Unable to integrate it with react native ;26;open;2019-10-12T07:30:37Z;2021-04-12T14:45:38Z;tried to integrate the face api in react native but tfjs-image-recognition is in fs format. is there any way to resolve issue. Really need help as this is a school project . Thanks;"['What is a fs format? Can describe the issue more precisely?=====', ""when i did an npm i face-api.js and import * as faceapi from 'face-api.js' into my page , it threw me an error fs module does not exist in module haste mapDid a check online and found the tjfs image recognition which is in fs format can only be work in web browser.So im wondering is that anyway I can solve this error and make it run on mobile? I really need help on these issues . thanks.====="", '> Did a check online and found the tjfs image recognition which is in fs format can only be work in web browser.I am maintaing tfjs-image-recognition base, but I still don\'t know what ""fs format"" should be. A more detailed error message would be helpful.=====', 'Just a quick ask is it possible for faceapi to be used on react native currently?=====', '<img width=""839"" alt=""Screenshot 2019-11-07 at 9 19 10 AM"" src=""https://user-images.githubusercontent.com/52208748/68352141-7aeb5800-0140-11ea-97f3-c5e2c791810d.png"">I suspect this maybe where the error come from since react native cannot read \'fs\' file format=====', ""I too am facing the same problem in React-NativeI have installed face-api.js and then imported it as `import * as faceapi from 'face-api.js',`Then I get the following errors``` bundling failed: Error: Unable to resolve module `fs` from `node_modules/tfjs-image-recognition-base/build/commonjs/env/createFileSystem.js`: fs could not be found within the project.If you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules: rm -rf node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*    at ModuleResolver.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph/ModuleResolution.js:186:15)    at ResolutionRequest.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph/ResolutionRequest.js:52:18)    at DependencyGraph.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph.js:282:16)    at Object.resolve (/root/React-native/faceapi/node_modules/metro/src/lib/transformHelpers.js:267:42)    at /root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:426:31    at Array.map (<anonymous>)    at resolveDependencies (/root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:423:18)    at /root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:275:33    at Generator.next (<anonymous>)    at asyncGeneratorStep (/root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:87:24)```====="", 'Hi @sedaplaksa @wulforr,It could very much be possible that `tfjs-image-recognition-base` is currently incompatible with React Native...Node is only used by the packager to serve/compile your app bundle, in other words, React Native apps don\'t run in the node environment.""fs"" stands for file system, and in order to access it in React Native, you would have to use something like [react-native-fs](https://www.npmjs.com/package/react-native-fs/v/1.2.0) or [rn-nodeify](https://www.npmjs.com/package/rn-nodeify) in order to bridge and talk to each platform\'s native APIs (the iOS/Android platforms, in this case). If you could find a way to integrate these into `tfjs-image-recognition-base` for your specific app use-case, then it should work.@justadudewhohacks please correct me if I\'m wrong in anything that I\'ve stated above.=====', ""I think its posible fix, you add 'fs empty' in [webpack](https://github.com/webpack-contrib/css-loader/issues/447#issuecomment-285598881) and create custom fetch in faceapi.env.monkeyPatch, but i dont know if this working in [react-native](https://github.com/react-boilerplate/react-boilerplate/issues/2279#issuecomment-405824492)====="", '> Hi @sedaplaksa @wulforr,> > It could very much be possible that `tfjs-image-recognition-base` is currently incompatible with React Native...> > Node is only used by the packager to serve/compile your app bundle, in other words, React Native apps don\'t run in the node environment.> > ""fs"" stands for file system, and in order to access it in React Native, you would have to use something like [react-native-fs](https://www.npmjs.com/package/react-native-fs/v/1.2.0) or [rn-nodeify](https://www.npmjs.com/package/rn-nodeify) in order to bridge and talk to each platform\'s native APIs (the iOS/Android platforms, in this case). If you could find a way to integrate these into `tfjs-image-recognition-base` for your specific app use-case, then it should work.> > @justadudewhohacks please correct me if I\'m wrong in anything that I\'ve stated above.i tried this method but it still did not work sadly  :(=====', 'Hi does anyone have any other different solutions? Or is it just not compatible at the moment ?=====', 'Well i removed face-api from react native and used it in node and sent my request from react native to node.=====', 'Do u mind sharing with me how u did it? Is there any latency during detection ?=====', 'Hi guys, @wulforr can you give more details of you solution, im  with the same problem, but i can turn it to server-side, i have to solve this issue on mobile app.=====', 'Did anyone try integrate face api with this adapter?https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native=====', ""> Did anyone try integrate face api with this adapter?https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native@artem-kushal may be one way to do this work, i didn't konw tfjs-react-native, i will try use it, please comment bellow if you got success...====="", '> Hi guys, @wulforr can you give more details of you solution, im with the same problem, but i can turn it to server-side, i have to solve this issue on mobile app.Sure @deividsoncs, I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.In server this image is compared to all the images in the database.If you wanna know more or anything in specific do let me know.=====', '> > Hi guys, @wulforr can you give more details of you solution, im with the same problem, but i can turn it to server-side, i have to solve this issue on mobile app.> > Sure @deividsoncs,> I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.> In server this image is compared to all the images in the database.> > If you wanna know more or anything in specific do let me know.Thanks @wulforr , but my problem is that a have to solve in a mobile app to send only the features extracted from the image captured in a mobile camera, because the image is taking so long time to uploado to the servers, im live at Brazil and the mobile internet connection is so slow in many places here. So, i wish do this at app, could you help on this? =====', ""> > Hi guys, @wulforr can you give more details of you solution, im with the same problem, but i can turn it to server-side, i have to solve this issue on mobile app.> > Sure @deividsoncs,> I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.> In server this image is compared to all the images in the database.> > If you wanna know more or anything in specific do let me know.Hi @wulforr, by any chance is your repo of this project public, if yes can you please tell me the name to check it out, I have been looking for examples of how to use face-api.js and I'm a little confused, thank you ====="", 'I assume the library can be implemented in a react native webview=====', ""If anyone manage to make face-api work with react-native, please share your solution.I'm trying to figure out as well.====="", 'use https://www.npmjs.com/package/react-native-fs instead of fs=====', '@yasahmed have you got face-api.js to work with react native ? can you post an example ?=====', ""as said before, you can use https://www.npmjs.com/package/react-native-fs instead of fs, but it's not enough.you'll need to install https://www.npmjs.com/package/@tensorflow/tfjs-react-native.use `faceapi.setEnv` to initialize, face-api is excpecting an Environment interface```{  Canvas: typeof HTMLCanvasElement  CanvasRenderingContext2D: typeof CanvasRenderingContext2D  Image: typeof HTMLImageElement  ImageData: typeof ImageData  Video: typeof HTMLVideoElement  createCanvasElement: () => HTMLCanvasElement  createImageElement: () => HTMLImageElement  fetch: (url: string, init?: RequestInit) => Promise<Response>}```I was using react-native-canvas for the canvas elementshere is a partial implementation```var RNFS = require('react-native-fs'),import Canvas, {  Image,  ImageData,  CanvasRenderingContext2D,} from 'react-native-canvas',import * as faceapi from 'face-api',import { cameraWithTensors } from '@tensorflow/tfjs-react-native',import { Camera } from 'expo-camera',const TensorCamera = cameraWithTensors(Camera),const createCanvasElement = function () {  if (Canvas) {    return new Canvas(),  }  throw new Error(    'createCanvasElement - missing Canvas implementation for RN environment',  ),},const createImageElement = function () {  if (Image) {    return new Image(),  }  throw new Error(    'createImageElement - missing Image implementation for RN environment',  ),},const readFile = function (filePath: string) {  try {    return RNFS.readFile(filePath),  } catch (e) {    console.error('error reading file', e),  }},const fileSystem = {  readFile,},const _fetch =  fetch ||  function () {    throw new Error(      'fetch - missing fetch implementation for browser environment',    ),  }, let env = {    Canvas: Canvas,    CanvasRenderingContext2D: CanvasRenderingContext2D,    Image: Image,    ImageData: ImageData,    Video: TensorCamera,    createCanvasElement,    createImageElement,    fetch: _fetch,    ...fileSystem,  },  faceapi.env.setEnv(env),```for loading the models i uploaded them to external storage (AWS S3)and used ` await faceapi.nets.tinyFaceDetector.loadFromUri('https://your-bucket-urls3.amazonaws.com/models'),`and I used this example to get a proper input that I can pass to the faceDetector: https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsxEverything haw worked without any errors but I couldn't get any results, also the detection process was very slow compared to web browser.Good Luck, and please let us know if you have any breakthrough   ====="", '@0xori im trying to do step that you mention, but im still getting error, did u manage to finish that code ?=====', 'I am facing the same issue. But somehow I managed to run the application without any errors up to the detection process following @0xori instruction except for the ""expo-camera"" I used ""react-native-camera"". But when I pass the camera ref  as a parameter to the faceapi.detectSingleFace() function, I\'m getting the following error, media.addEventListener(""load"", ""onLoad"") is not a function!=====', 'Is there anybody having the solution for using face-api.js in RN ? I am trying to  use some solutions above but it seems not to work TT =====']"
https://github.com/justadudewhohacks/face-api.js/issues/436;Android app face-api;8;open;2019-10-12T01:41:27Z;2020-02-06T12:10:28Z;Hi everyone. Do anyone had tried using this API in android? If yes, how did you include face-api.js in java? Thank you for the response.;"['Are you talking about react native? Tfjs recently launched react native support, but I am not sure how to integrate tfjs / libraries built on top of tfjs with react native.=====', ""Have you figured anything out RE: integrating tfjs / libraries built on top of tfjs within React Native? Currently trying to figure this out now so I can implement face-api in a RN app – would be great if you've discovered anything / have any pointers!====="", ""Hi I really need help too on this issue. I hope there's an implementation to it. Thank you very much====="", ""I've already implemented this api in android====="", '@Mactacs Can you please share the details/steps regarding the implementation for android. I am a newbie in android kindly share the implementation details.=====', 'Hi,In react-native i recommend use [expo-face-detector](https://docs.expo.io/versions/latest/sdk/facedetector/) it use tf-lite and it is very smooth and no require load models :)=====', 'Hi,     We are going with native android build. We have a node server which does the recognition using face api but would like to reduce the latency time by doing all the process in the app itself. Kindly let us know how the same can be achieved.=====', 'Hi,can anyone share the details/steps, how to implement face-api in android.Thanks in advance.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/435;Using model SSD MobileNet v1 detects only 2 faces max;5;open;2019-10-11T03:32:27Z;2019-12-18T11:01:48Z;So, I have tested the API in multiple computers. All of them have really good hardware. On my desktop the picture bbt1.jpg will have all 4 faces detected. However, there are 2 computers that no matter if I clone the Face API with no modifications and try with 4 different browsers, the faces detected on the picture bbt1.jpg will only be Raj and Leonard only (2 faces max) this only occurs when using SSD MobileNet v1. If I run the same build on my phone this doesn't occur and all faces are detected. So, it seems that the problem is only on the computers. Any ideas about a similar issue?;"['Can you please give me the exact hardware information of the computers it is not working for (CPU, GPU, OS).There still seems to be some issues with Intel Graphics Cards / (CPUs?).Some reference issues: #43 #332 https://github.com/tensorflow/tfjs/issues/510=====', 'Here are the specs:CPU: Intel i7-4770 GPU: Intel HD Graphics 4600OS: Windows 10Both have the same=====', 'Hi @justadudewhohacks ,There seems an issue with the tfjs webgl backend, It is being fixed by tfjs team.But the current version of face-api is not supporting latest tfjs version.When are you planning to upgrade it?=====', 'Upgrade to the latest tfjs-core version will come this weekend.=====', ""Hi @justadudewhohacks,This Issue still persist with the latest version of face-api and tfjs.Code works properly with 'cpu' backend on tfjs but fails to give proper result on 'webgl' backend on tfjs on the system mentioned above with Intel GPU :Intel GPU:Intel HD Graphics 4600 + win 7 ultimateAny suggestions/solution?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/432;Model loading from disk is slow in Google Cloud Function;3;open;2019-10-03T08:40:04Z;2021-01-30T12:41:43Z;"I created some sample code to process images and detect faces and its working great on my local,  I created a Google cloud function and realized that it's taking extremely long time to load the weights `console.log(""load faceDetectionNet"")    await faceDetectionNet.loadFromDisk('weights')    console.log(""load faceLandmark68Net"")    await faceapi.nets.faceLandmark68Net.loadFromDisk('weights')    console.log(""load faceRecognitionNet"")    await faceapi.nets.faceRecognitionNet.loadFromDisk('weights')   const referenceImage = await canvas.loadImage(imagePath)`I actually increased the timeout to 540 seconds , which is the maximum in GCF and still just make it to the last step and times out before it finishes. In my local it just takes 10 seconds . I know there is nothing wrong with the code . Have any body seen that ? any ideas to speed it up ?";['Hi: Have you found a workaround/solution?Do you have a github repo of your project?=====', 'I was not able to find a workaround for it to work on Google Cloud. I ended up not using the functions but just npm library =====', 'Thank you. I abandoned it timedout all the time=====']
https://github.com/justadudewhohacks/face-api.js/issues/429;Extracted images do not produce same descriptor as original;1;open;2019-09-30T16:25:34Z;2019-10-12T10:41:21Z;Currently I extract and save in a directory the image of each identified individual in a photo.  I have experimented with saving as lossless png or various levels of jpeg compression.  I then retrain the faceMatcher object with these new images, but even though it is somewhat successful at finding the bestMatch, the Euclidian Distance on the same person on the same photo for a match is often only around .5 and I would have predicted 0.  This even happens with a single extracted photo trying to match against itself.Is there a reason that extracted and saved images don't produce the same descriptor as in the original image?;"[""There is probably some issue with your code. Could you share the relevant code?> Is there a reason that extracted and saved images don't produce the same descriptor as in the original image?They should produce about the same descriptor, the distance might not be exactly 0 if there is some lossy compression or blurring going on, but it should still be much closer to 0 rather than 0.5.Maybe you can also share two example images, causing this issue.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/428;face not being detected on latest mater build;2;open;2019-09-30T14:11:38Z;2019-11-16T13:47:18Z;Hi!I'm having a problem having the examples work in my Windows dev machine. I was able to make the sample work in Mac, and was able to updated code with my own code. When I've moved it into my windows machine my code wasn't able to detect faces anymore.The sample code in https://justadudewhohacks.github.io/face-api.js/webcam_face_expression_recognition works, but when running it via node it doesn't seem to work.;"['Can you please give me the exact hardware information of the computer it is not working for (CPU, GPU, OS).There still seems to be some issues with Intel Graphics Cards / (CPUs?).=====', '**Fixed with setting tf-core version to 1.2.9**Same here. Works perfectly in vue-cli/dev, but doesn\'t work in production (nginx). If i\'m using tensors as input, gives this response: `Negative size values should be exactly -1 but got -112 for the slice() size at index 0.` somewhere at faceapi.detectAllFaces.If i use images from canvas, it gives me `Uncaught (in promise) DOMException: Failed to execute \'getImageData\' on \'CanvasRenderingContext2D\': The source height is 0.` though the image is captured correctly (the height and width are in console just before the faceapi call and are non-zero.It has been working in summer, then i\'ve added a minor tweak (to the GUI, not the face recognition process), and it all went south. Maybe some tf update or smth. I\'m using: tfjs 1.2.9faceapi 0.21.Tried to use older version which worked, and it used the following dependencies:""core-js"": ""2.6.5"",""face-api.js"": ""0.21.0""The output of `faceapi.detectAllFaces` on vue dev is:<img width=""291"" alt=""Screenshot 2019-11-09 at 10 06 30"" src=""https://user-images.githubusercontent.com/11751592/68524454-2d095800-02d8-11ea-99c3-8ce221beed7d.png"">the very same code/image on vue/prod: <img width=""560"" alt=""Screenshot 2019-11-09 at 10 07 55"" src=""https://user-images.githubusercontent.com/11751592/68524474-617d1400-02d8-11ea-8453-e388034da45f.png""><img width=""286"" alt=""Screenshot 2019-11-09 at 10 08 14"" src=""https://user-images.githubusercontent.com/11751592/68524475-617d1400-02d8-11ea-9943-db91e08e85d8.png"">it detects 419 faces (sic!) and their height/width is really <0=====']"
https://github.com/justadudewhohacks/face-api.js/issues/418;Using ES6 module directly inside a script ?;3;open;2019-09-18T22:50:06Z;2020-03-25T03:09:29Z;"Hello,I would like to use face-api.js using only the ES6 module (without any build step, and using the fantastic [pika manager](https://www.pika.dev/)).Like so:```<script type=""module"">  import faceapi from '/web_modules/face-api.js/dist/face-api.min.js'  console.log('faceapi.nets'),</script>```But it fails to load the module doing so, I can't manage to get the object. It tries to load fs, and other node specifics libs.I also get `Module {Symbol(Symbol.toStringTag): ""Module""}Symbol(Symbol.toStringTag): ""Module""`Thanks !";"[""> It tries to load fs, and other node specifics libs.fs should be the only one, otherwise it does not come from face-api.js. The issue might be one of two:1. The bundler / pika manager, that you are using does not handle conditional require statements correctly. face-api.js checks your environment, if you are running in a node environment, it will require fs.2. Your environment is mistaken for a node environment, e.g. [this](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/isNodejs.ts) check returns true. Could you please check what the following condition evaluates to in your environment:``` javascripttypeof global === 'object'    && typeof require === 'function'    && typeof module !== 'undefined'    // issues with gatsby.js: module.exports is undefined    // && !!module.exports    && typeof process !== 'undefined' && !!process.version```====="", ""Yes I think it is a pika issue, as I have this message when I do pika install:But it is strange, it is the first time I encounter this problem.  ``` @pika/web installing... vue/dist/vue.esm.browser.js, http-vue-loader/src/httpVueLoader.js, remarkable/dist/esm/index.browser.js, typeit/dist/typeit.min.js, vue-beautiful-chat/dist/index.js, botui/build/botui.js, face-api.js/dist/face-api.min.js, oscilloscope/dist/oscilloscope.es.jsThe 'this' keyword is equi⠼ @pika/web installing... vue/dist/vue.esm.browser.js, http-vue-loader/src/httpVueLoader.js, remarkable/dist/esm/index.browser.js, typeit/dist/typeit.min.js, vue-beautiful-chat/dist/index.js, botui/build/botui.js, face-api.js/dist/face-api.min.js, oscilloscope/dist/oscilloscope.es.js✖ 'crypto' is imported by 'node_modules/face-api.js/dist/face-api.min.js', but could not be resolved.  'crypto' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'node_modules/face-api.js/dist/face-api.min.js', but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'fs' is imported by 'node_modules/face-api.js/dist/face-api.min.js', but could not be resolved.  'fs' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'crypto' is imported by 'commonjs-external-crypto', but could not be resolved.  'crypto' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'commonjs-external-util', but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'fs' is imported by 'commonjs-external-fs', but could not be resolved.  'fs' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'node_modules/node-fetch/lib/fetch-error.js', but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'node_modules/node-fetch/index.js', but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'node_modules/node-fetch/lib/request.js', but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'commonjs-external-url', but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'node_modules/node-fetch/index.js', but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'node_modules/node-fetch/lib/response.js', but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'commonjs-external-http', but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'zlib' is imported by 'node_modules/node-fetch/index.js', but could not be resolved.  'zlib' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'zlib' is imported by 'commonjs-external-zlib', but could not be resolved.  'zlib' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'https' is imported by 'node_modules/node-fetch/index.js', but could not be resolved.  'https' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'https' is imported by 'commonjs-external-https', but could not be resolved.  'https' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/node-fetch/index.js', but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/node-fetch/lib/body.js', but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'commonjs-external-stream', but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/iconv-lite/lib/streams.js', but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/iconv-lite/lib/extend-node.js', but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/safer-buffer/safer.js', but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/iconv-lite/lib/streams.js', but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/iconv-lite/lib/extend-node.js', but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'commonjs-external-buffer', but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'string_decoder' is imported by 'node_modules/iconv-lite/encodings/internal.js', but could not be resolved.  'string_decoder' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✖ 'string_decoder' is imported by 'commonjs-external-string_decoder', but could not be resolved.  'string_decoder' is a Node.js builtin module that won't exist on the web. You can find modern, web-ready packages at https://www.pikapkg.com✔ @pika/web installed: vue/dist/vue.esm.browser.js, http-vue-loader/src/httpVueLoader.js, remarkable/dist/esm/index.browser.js, typeit/dist/typeit.min.js, vue-beautiful-chat/dist/index.js, botui/build/botui.js, face-api.js/dist/face-api.min.js, oscilloscope/dist/oscilloscope.es.js. [3.72s]```Thanks====="", 'I believe I\'m hitting this issue as well (without pika):```<script type=""module"">    import * as faceapi from \'./face-api.min.js\'    console.log(faceapi)    // Module\xa0{Symbol(Symbol.toStringTag): ""Module""}```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/417;Poor performance in browser;8;open;2019-09-18T07:31:23Z;2019-10-14T02:13:47Z;`fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()` takes randomly 5-15 seconds. Is it expected when doing face detection in browser?;"['No, depending on the device you are running this on, this should only take a few miliseconds.`faceapi.detectAllFaces(input)` by default also by default uses the SSDMobilenetV1 model, which can potentially be slower on some devices. You can try out the tiny face detector instead (see README).Also make sure the WebGL backend is utilized: `console.log(tf.getBackend())`.=====', 'what is `tf` and how to import it?=====', '```jsimport * as faceapi from ""face-api.js"",faceapi.tf.getBackend()```@avalanche1 =====', '@justadudewhohacks By saying ""a few miliseconds"", you mean ""a few hundreds miliseconds""? :)I\'m using face-api.js in a non-critical page and I got some stats.conf:* tiny face detector* face-api.js@0.20.1 It\'s roughly like these:Total users: about 20k.75% took below 500ms.10% took (500, 1500)ms.5% took 1000ms+.The extreme case did take more than 10 seconds.I\'ve tried face-api.js@0.20.1 on some computer, performance seems go down a little bit.What\'s your suggestions to improve performance? or gain some other stats?BTW, here are some cases I don\'t understand.```// took 1400ms to finish a single detectionwebgl: true, tf_backend: webgl, os: win7, cpu: Intel(R) Core(TM) i5-4200U CPU @ 1.60GHz// took 2400ms to finish a single detectionwebgl: null, tf_backend: cpu, os: win10, cpu: Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz```Thanks=====', '@zjlovezj, the tiny face detector model on my desktop machine with webgl backend and an input size of 160 runs at about 60 fps, so the time for inference is about 20ms or less. On my android it runs at 15 fps. The inference time obviously depends on the device.The question is, do your metrics include the shader compilation times? You should exclude the very first forward pass from your measurements.=====', 'Yes. I did exclude the first detection. And the numbers are average of 20 times of detection.=====', ""@zjlovezj I Think a repro would help, I'll try to hack up smth as well====="", ""@avalanche1 I don't think a repo would help much on this performance issue. The results are all different on different machine with different configuration.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/415;Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor';8;open;2019-09-16T20:40:25Z;2021-03-07T18:11:13Z;"HI, I use this library in a project Angular, I'm create a library that there is problem Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' and no build project, I'm try use 0.20.1 version I am trying to use the 0.20.1 version that was the one with which my project was built but currently it does not workthis is the problem shown by consoleBUILD ERRORnode_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.        Index signatures are incompatible.          Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.            Types of property 'flatten' are incompatible.              Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Types of property 'asScalar' are incompatible.                    Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Types of property 'as2D' are incompatible.                          Type '(rows: number, columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/nod...' is not assignable to type '(rows: number, columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/type...'.                            Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Types of property 'asType' are incompatible.                                Type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-rec...' is not assignable to type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, d...'.                                  The 'this' types of each signature are incompatible.                                    Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                      Property 'bytes' is missing in type 'Tensor<Rank.R2>' but required in type 'Tensor<Rank.R2>'.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceProcessor<TExtractorParams>' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyXception' is not assignable to the same property in base type 'NeuralNetwork<TinyXceptionParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'AgeGenderNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceRecognitionNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'Mtcnn' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'SsdMobilenetv1' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceDetector' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyYolov2' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.Error: node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.        Index signatures are incompatible.          Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.            Types of property 'flatten' are incompatible.              Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Types of property 'asScalar' are incompatible.                    Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Types of property 'as2D' are incompatible.                          Type '(rows: number, columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/nod...' is not assignable to type '(rows: number, columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/type...'.                            Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Types of property 'asType' are incompatible.                                Type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-rec...' is not assignable to type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, d...'.                                  The 'this' types of each signature are incompatible.                                    Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                      Property 'bytes' is missing in type 'Tensor<Rank.R2>' but required in type 'Tensor<Rank.R2>'.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceProcessor<TExtractorParams>' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyXception' is not assignable to the same property in base type 'NeuralNetwork<TinyXceptionParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'AgeGenderNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceRecognitionNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'Mtcnn' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'SsdMobilenetv1' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceDetector' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10,15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyYolov2' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.    at Object.<anonymous> (/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/ng-packagr/lib/ngc/compile-source-files.js:65:19)    at Generator.next (<anonymous>)    at fulfilled (/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/ng-packagr/lib/ngc/compile-source-files.js:4:58)";"['Looks like an issue with your typescript setup. Try use the latest typescript version. face-api.js 0.21.0 is compiled with typescript 3.6.3, so that should work.=====', ""Same problem here. I am using angular compiler v8.2.8 (which is the latest at the moment). This compiler requires typescript between 3.4.0 and 3.6.0, so version 3.6.3 can not work just 3.5.3.I've tried to decrease the version numbers for ngx-face-api, and face api as well to test which one is compiled with 3.5.3, but I'm getting always the same errors. So any other ideas?====="", ""Isn't angular supposed to support latest typescript versions? I would upgrade your compiler.====="", 'I get the same error on 20.1 now, it seems that base module uses 1.2.9 tfjs and the api uses 1.2.2For some reason npm installs tfjs-image-recognition-base 0.6.2 instead of 0.6.1=====', '> ¿No se supone que angular admite las últimas versiones de mecanografiado? Actualizaría tu compilador.The latest version of the angular compiler uses 3.6 at most, is there any definitive solution to use this library? ERROR in The Angular Compiler requires TypeScript >=3.4.0 and <3.6.0 but 3.6.4 was found instead.=====', 'Got the following error while updating angular v8 from v7 and modules are at following versions -  ""face-api.js"": ""^0.21.0"" -  ""typescript"": ""3.5.3""-  ""@tensorflow/tfjs-core"": ""^1.6.0""-  ""@angular/cli"": ""~8.3.25""-  ""@angular/compiler-cli"": ""^8.2.14"" -  ""@angular/core"": ""^8.2.14"",```ERROR in node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'AgeGenderNet\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/ageGenderNet/types"").NetParams, paramMappings: import(""...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceFeatureExtractor\' is not assignable to the same property in base type \'IFaceFeatureExtractor<FaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").FaceFeatureExtractorParams...\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceFeatureExtractor\' is not assignable to the same property in base type \'NeuralNetwork<FaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").FaceFeatureExtractorParams...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.            Index signatures are incompatible.              Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>\'.                Types of property \'flatten\' are incompatible.                  Type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\' is not assignable to type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\'.                    Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\'.                      Types of property \'asScalar\' are incompatible.                        Type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\' is not assignable to type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\'.                               Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\'.                            Types of property \'as2D\' are incompatible.                              Type \'(rows: number, columns: number) => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\' is not assignable to type \'(rows: number, columns: number) => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Ra...\'.                                Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\'.                                  Types of property \'asType\' are incompatible.                                    Type \'<T extends import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, dtype: ""string"" | ... 3 more ... | ""complex64"") => T\' is not assignable to type \'<T extends import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, dtype: ...\'.                                      The \'this\' types of each signature are incompatible.                                        Type \'T\' is not assignable to type \'Tensor<Rank.R2>\'.                                          Type \'Tensor<Rank.R2>\' is missing the following properties from type \'Tensor<Rank.R2>\': divNoNan, relu6          node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceFeatureExtractor\' is not assignable to the same property in base type \'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>\'.           Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").TinyFaceFeatureExtractorPa...\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceFeatureExtractor\' is not assignable to the same property in base type \'NeuralNetwork<TinyFaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").TinyFaceFeatureExtractorPa...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceProcessor<TExtractorParams>\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceProcessor/types"").NetParams, paramMappings: import(...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceRecognitionNet\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceRecognitionNet/types"").NetParams, paramMappings: im...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'Mtcnn\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/mtcnn/types"").NetParams, paramMappings: import(""D:/am w...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'SsdMobilenetv1\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/types"").NetParams, paramMappings: import...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceDetector\' is not assignable to the same property in base type \'TinyYolov2\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/types"").TinyYolov2NetParams,...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyYolov2\' is not assignable to the same property in base type \'TinyYolov2\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/types"").TinyYolov2NetParams,...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10,15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyXception\' is not assignable to the same property in base type \'NeuralNetwork<TinyXceptionParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ..., }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/xception/types"").TinyXceptionParams, paramMappings: imp...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.```=====', 'Same problem in Angular 10 (TypeScript 3.9.7). Did anyone find a solution or workaround for this issue?=====', '😩Facing the same issue in Angular 11 and TypeScript 4.1.2Any workaround for this issue?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/409;withFaceLandmark slow in Slackware;4;open;2019-09-12T04:48:22Z;2019-09-15T09:14:17Z;Hi @justadudewhohacks! I have a problem with this API. At first, I tried this API in intel nuc system unit with ubuntu os and it works perfectly. It can detect faces and create face landmarks less than a second. After, I change the os to Slackware. I transfer all the codes to Slackware with the same system unit and the code works but the thing here is that it is slower than the usual. It can detect the face fast but the withFaceLandMark was not. It can create face landmark about 10s. I don't know what is the cause of this problem.  Maybe with the os? ;"['Nodejs or browser? Which backend are you using on both machines (cpu / webgl / node gpu)?=====', 'Thank you for the response @justadudewhohacks. I\'m using a browser. I\'m sorry I don\'t know what backend I\'m using. Here is my code.<!DOCTYPE html><html>\t<head>                <title>Biometric Facial Recognition</title>                <link rel=""SHORTCUT ICON"" href=""/images/gecko.ico"">\t\t<meta name=""viewport"" content=""width=device-width, initial-scale=1"">  \t\t<link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css"">  \t\t<link rel=""stylesheet"" type=""text/css"" href=""/scripts/face-rec-models/style.css"">  \t\t<script src=""https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js""></script>  \t\t<script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js""></script>  \t\t<script src=""/scripts/face-api2.js""></script>        </head>               <div id=""body"" >\t\t\t<div id=""parent1"">      \t\t\t\t<div class=""margin"" style=""position: relative, float:left, margin-left: 10px, margin-top: 50px, border: 3px solid black,"">      \t\t \t \t\t<video id=""vidDisplay"" style=""height: 568px, width: 640px, display: inline-block, vertical-align: baseline,"" onloadedmetadata=""onPlay(this)"" autoplay=""true""></video>        \t\t\t\t<canvas id=""overlay"" style=""position: absolute, top: 0, left: 0,"" width = ""640px"" height = ""568px""/>      \t\t\t\t</div>    \t\t\t</div>\t        </body><script>  \t$(""#parent1"").hide(),  \tPromise.all([    \t\tfaceapi.nets.faceRecognitionNet.loadFromUri(\'/scripts/face-rec-models/models\'),    \t\tfaceapi.nets.faceLandmark68Net.loadFromUri(\'/scripts/face-rec-models/models\'),    \t\tfaceapi.nets.ssdMobilenetv1.loadFromUri(\'/scripts/face-rec-models/models\'),    \t\tfaceapi.nets.tinyFaceDetector.loadFromUri(\'/scripts/face-rec-models/models\')  \t]).then(start),  \tasync function start() {    \t\t$(\'#parent1\').show(),        \t\trun(),  \t}  \tasync function onPlay() {      \t\tconst videoEl = $(\'#vidDisplay\').get(0)      \t\tif(videoEl.paused || videoEl.ended ){        \t\treturn setTimeout(() => onPlay()),\t\t}        \tconst canvas = $(\'#overlay\').get(0),         \tconst options = getFaceDetectorOptions(),        \tconst result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks(),\t\tif (result) {            \t\t$(""#overlay"").show(),            \t\tconst dims = faceapi.matchDimensions(canvas, videoEl, true),            \t\tdims.height = 568,            \t\tdims.width = 640,            \t\tcanvas.height = 568,            \t\tcanvas.width = 640,            \t\tconst resizedResult = faceapi.resizeResults(result, dims),            \t\tfaceapi.draw.drawDetections(canvas, resizedResult),        \t}             \telse{            \t\t$(""#overlay"").hide(),       \t\t}      \t\tsetTimeout(() => onPlay()),    \t} \t async function run() {      \t\tconst stream = await navigator.mediaDevices.getUserMedia({ video: {} }),      \t\tconst videoEl = $(\'#vidDisplay\').get(0),      \t\tvideoEl.srcObject = stream,  \t}    \t// tiny_face_detector options  \tlet inputSize = 160,  \tlet scoreThreshold = 0.4,\tfunction getFaceDetectorOptions() {    \t\treturn  new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold }),  \t}\t</script></html>=====', '[code.txt](https://github.com/justadudewhohacks/face-api.js/files/3609496/code.txt)I tried the demo of face-api.js webcam face tracking with face landmark in slackware and unfortunately, it will create the box after 10 seconds. I tried also the demo to ubuntu on the same system unit and the box was created less than 1 second=====', ""You can run `console.log(tf.getBackend())` or `console.log(faceapi.tf.getBackend())` on both machines to verify whether the webgl backend is utilized. If it outputs 'cpu' then that's the reason why it's running so slow.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/400;Error: resolveContext2d / Total canvas memory use ;1;open;2019-09-05T12:52:04Z;2019-09-13T07:58:20Z;Hi, After opening my frame in Safari and let it run for more than 10 seconds. It overflows the memory and displays this error.Can anyone help me?Below are the log images.![Captura de Tela 2019-09-05 às 09 46 44](https://user-images.githubusercontent.com/22602151/64343347-c491b800-cfc2-11e9-939c-fa49351fab26.png)![Captura de Tela 2019-09-05 às 09 46 34](https://user-images.githubusercontent.com/22602151/64343358-cb202f80-cfc2-11e9-88a6-dca02e824610.png);['Please post the relevant code, otherwise I am not sure what you are doing. Might be some issue with your code or an issue with tfjs in safari.=====']
https://github.com/justadudewhohacks/face-api.js/issues/395;Facing Error at withFaceLandmarks().;2;open;2019-08-27T06:06:22Z;2019-09-23T08:04:55Z;"Hi @justadudewhohacks ,I am really impressed on your incredible work!First of all, I am currently working on biometric login screen that using RequireJs framework. I use faceapi.js for face detection & face recognition task. I manage to setup well in RequireJs and able run basic face detection but i facing error when add `withFaceLandmarks()`. below is my codeThis my model import:```var model = 'scripts/vendors/faceapi/model',await faceapi.loadMtcnnModel(model)await faceapi.loadFaceLandmarkModel(model)await faceapi.loadFaceRecognitionModel(model)```This is my main process at `onPlay()````async function onPlay() {	const videoEl = $('#inputVideo').get(0)	if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded()){		console.log(""Model not load or webcam off""),		return setTimeout(() => onPlay())	}	const options = new faceapi.MtcnnOptions(mtcnnForwardParams)	const result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks().withFaceDescriptor()	console.log(result),		if (result) {		const canvas = $('#overlay').get(0)		const dims = faceapi.matchDimensions(canvas, videoEl, true)		faceapi.draw.drawDetections(canvas, faceapi.resizeResults(result, dims))	}	setTimeout(() => onPlay())}```This are error from chrome console> Uncaught (in promise) TypeError: Cannot read property 'expandDims' of undefined>     at stack_ (face-api.js:100)>     at stack (face-api.js:100)>     at face-api.js:3590>     at face-api.js:100>     at t.scopedRun (face-api.js:100)>     at t.tidy (face-api.js:100)>     at Fe (face-api.js:100)>     at FaceLandmark68NetBase.postProcess (face-api.js:3575)>     at face-api.js:3603>     at face-api.js:100However when i use below code it work well without any error```const result = await faceapi.detectSingleFace(videoEl, options)//.withFaceLandmarks().withFaceDescriptor()console.log(result),```Hope you can help me.Thanks in advance!";"[""`expandDims` is a method provided by tfjs-core. Since FaceLandmark68NetBase.postProcess does not call expandDims directly, this error comes from some tfjs internal code. Might be some issue with requirejs + tfjs, but not sure what's going on there.What's the last face-api.js call where the error is thrown, what code is executed in face-api.js:3590?====="", 'This code produce that error.`await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks().withFaceDescriptor()`=====']"
https://github.com/justadudewhohacks/face-api.js/issues/377;[TinyYolov2]  detectFaces - expected options to be instance of TinyFaceDetectorOptions;3;open;2019-08-08T05:34:21Z;2019-09-20T03:31:53Z;Error: detectFaces - expected options to be instance of TinyFaceDetectorOptions | SsdMobilenetv1Options | MtcnnOptions | TinyYolov2Options            how use faceapi.detectSingleFace withFaceLandmarks  and withFaceDescriptor for TinyYolov2?examplefaceapi.detectSingleFace(videoEl,   new face.TinyYolov2Options() ).withFaceLandmarks().withFaceDescriptor();['`tyniOptions = new faceapi.TinyFaceDetectorOptions({      inputSize: 128,      scoreThreshold: 0.5    })`` const detections = await faceapi        .detectSingleFace(input,tyniOptions)        .withFaceExpressions()      console.log(detections)`=====', 'The export of TinyYolov2Options got lost along the way with some release it seems. There is no real reason to use TinyYolov2 anymore since TinyFaceDetector will perform better as @uriel2707  implied.The error message should probably be adjusted, but I will hold that off since I want to deprecate the MTCNN as well. I will keep this issue open as a reminder.=====', 'great thanks=====']
https://github.com/justadudewhohacks/face-api.js/issues/358;how to load the models in ionic 3.;10;open;2019-07-22T11:47:08Z;2020-01-20T07:05:21Z;"Fetch API cannot load file:///android_asset/models/face_recognition_model-weights_manifest.json. URL scheme ""file"" is not supported.";"[""Basically answered this here #73:> Hmm, that's odd. Usually browsers don't allow you to access files from your filesystem for security reasons, that's why the fetch request in model.load is failing in chrome (not sure why this works for you in firefox).> > If you look at my examples in this repo, you can see that I state, which folders can be publicly accessed (in server.js). This way you can load your model from an url e.g. net.load('models/my-model'). You can do the same for your model.> > With filepicker I mean, you can use an input element of type 'file', which is another way to browse and load files from your browser. There should be plenty of tutorials out there, explaining how to implement a file upload.Another way is to monkey patch `readFile` as described here: #153 and then use net.loadFromDisk. I am not familar with Ionic so I don't know what kind of filesystem methods you have available to implement the patch.====="", ""I have the same problems importing the modules, I am using Ionic 3 and I am trying to use monkeyPatch.faceapi.env.monkeyPatch({    readFile: filePath =>      Promise.all([        await faceapi.nets.tinyFaceDetector.loadFromUri('./models'),        await faceapi.nets.faceLandmark68Net.loadFromUri('./models'),        await faceapi.nets.faceRecognitionNet.loadFromUri('./models'),        await faceapi.nets.faceExpressionNet.loadFromUri('./models')      ]).then()}),====="", '@LeonardoDB not sure why you are using loadFromUri inside your patch. You are supposed to monkey patch readFile using ionics specific file system API.=====', 'Thanks @justadudewhohacks, your help was helpful in troubleshooting=====', '@rajkishoreandia @justadudewhohacks I am integrating face-api.js with an ionic3 project. With [this](https://stackoverflow.com/questions/56544635/using-face-api-js-in-cordova-with-android/), I was able to load the models for recognition. 1. Install [cordova-plugin-file](https://ionicframework.com/docs/v3/native/file/)2. Move all models to /src/assets/models3. In your page.ts, declare:`let filePathRoot = this.file.applicationDirectory + \'www/assets/models/\',`4. Create monkeyPatch```faceapi.env.monkeyPatch({       readFile: filePath =>         new Promise(resolve => {           this.file.resolveLocalFilesystemUrl(filePath).then(res => {             console.log(filePath),             if (res.isFile){               let fileExtension = filePath.split(""?"")[0].split(""."").pop(),               let fileName = filePath.split(""?"")[0].split(""/"").pop(),                 if (fileExtension === ""json"") {                   this.file.readAsText(filePathRoot, fileName).then((text) => {                     resolve(text),                   }),                 } else {                   this.file.readAsArrayBuffer(filePathRoot, fileName).then((arrayBuffer) => {                     resolve(new Uint8Array(arrayBuffer)),                   }),                 }             }         }),       }),      Canvas: HTMLCanvasElement,      Image: HTMLImageElement,      ImageData: ImageData,      Video: HTMLVideoElement,      createCanvasElement: () => document.createElement(""canvas""),      createImageElement: () => document.createElement(""img"")     }),```5. Load models from Disk```await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot),await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot),```6. Enjoy!```let resultsRef = await faceapi.detectAllFaces(image, new faceapi.TinyFaceDetectorOptions()),alert(JSON.stringify(resultsRef)),```Hope this helps. Any adjustments or improvements, please pass on to the community!=====', ""**@yagancadorin** Thank you for sharing this, i have a query in this that do you have an example reference to this code ? If not please explain this if you can 'Move all models to /src/assets/models'. Thank you.====="", 'Please can anyone explain how to integrate FaceApi.js library in an ionic 3 projecct and how to use it?Sharing some code would definitely help=====', '> **@yagancadorin** Thank you for sharing this, i have a query in this that do you have an example reference to this code ? If not please explain this if you can \'Move all models to /src/assets/models\'.> > Thank you.I will try to make it easier. After installing the plugin, follow the steps:1. Download [JS file](https://github.com/justadudewhohacks/face-api.js/tree/master/dist), move to /src/assets/js/ and include into your /src/index.html```[...]<script src=""cordova.js""></script><script src=""assets/js/face-api.min.js""></script>[...]```2. Download [models](https://github.com/justadudewhohacks/face-api.js/tree/master/weights) and move all files to your /src/assets/models/3. Declare faceapi in your /src/pages/home/home.ts ```[...]declare var faceapi : any,@Component({  selector: \'page-home\',  templateUrl: \'home.html\'})export class HomePage { }[...]```4. Create and call async loadmodel function ```[...]  async loadModels(){    // set path to load models    let filePathRoot = this.file.applicationDirectory + \'www/assets/models/\',    // faceapi settings    faceapi.env.monkeyPatch({      readFile: filePath =>        new Promise(resolve => {          let fileExtension = filePath.split(""?"")[0].split(""."").pop(),          let fileName = filePath.split(""?"")[0].split(""/"").pop(),          this.file.resolveLocalFilesystemUrl(filePathRoot + fileName).then(res => {            if (res.isFile){                if (fileExtension === ""json"") {                  this.file.readAsText(filePathRoot, fileName).then((text) => {                    resolve(text),                  }),                } else {                  this.file.readAsArrayBuffer(filePathRoot, fileName).then((arrayBuffer) => {                    resolve(new Uint8Array(arrayBuffer)),                  }),                }            }        }),      }),     Canvas: HTMLCanvasElement,     Image: HTMLImageElement,     ImageData: ImageData,     Video: HTMLVideoElement,     createCanvasElement: () => document.createElement(""canvas""),     createImageElement: () => document.createElement(""img"")    }),    // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot),    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot),    await faceapi.nets.faceLandmark68Net.loadFromDisk(filePathRoot),    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot),  }[..]```5. Use your global faceapi variable normaly in code to create faceMatcher and detect/recognize faces.```[...]let detections = await faceapi.detectAllFaces(shotCam, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors().withFaceExpressions(),[...]```=====', '@justadudewhohacks  @yagancadorin thanks for the response.I am now implementing the same in ionic 3 .but my models are not getting loaded from file path.i am getting following error.ERROR Error: Uncaught (in promise): [object Object].FileError--->code: 1message: ""NOT_FOUND_ERR""__proto__: ObjectThanks in advance!=====', ""Face detection is not happening @justadudewhohacks  @yagancadorin After loading the models if i pass the image into face Api     // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot),    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot),    await faceapi.nets.faceLandmark68Net.loadFromDisk(filePathRoot),    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot),    await faceapi.nets.ssdMobilenetv1.loadFromDisk(filePathRoot)    console.log('after loading detection models')    // displaying the fetched image content    const myImg = document.getElementById('myImg')    const detections = await    faceapi.detectAllFaces(myImg).withFaceLandmarks().withFaceDescriptors()    console.log('detections',detections)prints nothing in console=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/342;Catch internal error in promise.;2;open;2019-07-04T09:41:41Z;2019-07-08T06:28:13Z;Hi, I get this error in when calling the promise to detect face + landmarks. Is there any possibility to catch it? Im trying to get the error through then/catch but not reaching this code.`Fragment shader compilation failed.`and `Uncaught (in promise) Error: Failed to compile fragment shader.`;['I think being able to catch this error would require some solution similar to what is described here: https://github.com/tensorflow/tfjs/issues/756. Did you try to wrap the failing code into a simple try/catch? Does it not work?=====', 'Yes, I tried, but there is not error Throwed just printed and continue, so it keeps giving errors... :( =====']
https://github.com/justadudewhohacks/face-api.js/issues/340;Not working in Microsoft Edge - lack of TextEncoder support (since 0.20.1);1;open;2019-07-03T18:11:16Z;2020-08-31T13:52:21Z;"After bumping to v0.20.1, using Edge 44.18362.1.0, the console emits the following error:**'TextEncoder' is not defined**Microsoft's delightful Edge browser doesn't currently support TextEncoder or TextDecoder:  https://caniuse.com/#feat=textencoderThis fixes it:```                TextEncoder = function TextEncoder() {		}		TextEncoder.prototype.encode = function(s) {			const e = new Uint8Array(s.length),			for (let i = 0, i < s.length, i += 1) {				e[i] = s.charCodeAt(i),			}			return e,		}		TextDecoder = function TextDecoder() {					}		TextDecoder.prototype.decode = function(arr) {			let d = """",			for (let i = 0, i < arr.length, i += 1) {				d += String.fromCharCode(arr[i]),			}			return d,		} ";"[""I'm working with Next.JS and there are packages in my project that have text encoder. How can I use this in my project?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/332;[Intel, IPhone 6S] Problem Processing face-api.js;3;open;2019-06-24T15:21:14Z;2019-10-12T11:27:42Z; I am building an application that uses face-api.js and it was working just fine until I started getting this error everytime I try to access the website```Uncaught (in promise) Error: Failed to link vertex and fragment shaders.    at Wr (face-api.min.js:1)    at t.createProgram (face-api.min.js:1)    at face-api.min.js:1    at face-api.min.js:1    at t.getAndSaveBinary (face-api.min.js:1)    at t.compileAndRun (face-api.min.js:1)    at t.conv2dWithIm2Row (face-api.min.js:1)    at t.conv2d (face-api.min.js:1)    at Zt.engine.runKernel.x (face-api.min.js:1)    at face-api.min.js:1```After that It just stops and doesnt load anything else or perform any another action, I am stuck at this point, I wonder if its because of my GPU, but I dont know. **Please Help me!Thanks.**;"['OK I realised that there might be a problem with Intel (R) Graphics 3000,  the problem is that the other issue featuring this error #327 got a solution which for me doesnt work any help would be appreciated=====', '> I am building an application that uses face-api.js and it was working just fine until I started getting this error everytime I try to access the websiteWhen exactly did it start throwing those errors? This could be an issue of your device not supporting the WebGL backend, running out of memory or some browser issue. Which browser + version are you running this on?=====', "" I am running the lastest Chrome Stable Version 75.0.3770.100, I also tested it in Microsoft Edge and Microsoft Edge Developer Version all of them fully updated and all of them threw the same error. Probably it's a problem with the WebGL Backend but I have no idea how to solve it It also doesnt work on my iPhone 6S running iOS 12 and iOS 13 (Uodated recently) but it may be due to another error in my code I dont know because I cant debug It so easily.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/329;Blob is not defined NodeJS;1;open;2019-06-19T16:39:57Z;2019-06-24T16:29:01Z;Hey Vincent, getting an error of 'Blob is not defined'. What I'm trying to accomplish:Receive an image using multer and reading the face detections in realtime, then storing the face detections in a database.What I've tried to solve the issue:I've tried using node-fetch to retrieve the image and load it into faceapi detections, but node-fetch requires a URL.Tried using multiple packages to convert the image to a blob and use faceapi.BufferToImage to convert it to an HTMLImageElement,  but the packages don't do the job.Appreciate your help, and hopefully an answer will help anyone else having the same issue.I'll attach snippets of my code belowuserRouter.js where multer exists, usercontroller.js is fully imported here![Screen Shot 2019-06-19 at 11 32 02 AM](https://user-images.githubusercontent.com/50146137/59783791-9edbf900-9286-11e9-8fe7-5b2d45208007.png)userController.js where I implement the getDetections function![Screen Shot 2019-06-19 at 11 31 28 AM](https://user-images.githubusercontent.com/50146137/59783919-db0f5980-9286-11e9-84ba-ba3370d2863c.png)The error i receive (as you can see the file upload is typeof Object![Screen Shot 2019-06-19 at 11 25 25 AM](https://user-images.githubusercontent.com/50146137/59783961-f24e4700-9286-11e9-913d-810b422814bf.png);"['First, you need to monkey patch faceapi env.```const canvas = require(""canvas""),const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image, ImageData }),```Then, you need to load image with canvas module by giving it uploaded image buffer.```const getDetections = async (imageSource) => {    const img = await canvas.loadImage(imageSource.buffer),    const detections = await faceapi      .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())      .withFaceLandmarks()      .withFaceDescriptor()      .withFaceExpressions(),    return detections,  }```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/328;Running FaceAPI.js within Web Worker (slow);1;open;2019-06-18T21:51:05Z;2019-06-21T15:26:35Z;We found that the video is choppy when running the faceAPI on certain devices.  Specifically, on IOS devices.  Normally, this can be fixed by running the code in a Web Worker.  The problem is that in order to run within a Web Worker we need to pass in ImageData / not DOM related objects like Canvas/Video/Image.  It also appears to run much, much slower when we run in a Web Worker.  Is there a way we can make this work?  What do you suggest?  What type of object can we create on the Web Worker side from an ImageData object that will run at around the same speed?;"[""That's because if you run this on a web worker, you're splitting the CPU to another thread but doing nothing to split loads for the GPU. Transfer of assets to a web worker are not free/fast until browsers support a zero copy strategy like offscreen canvas. Chrome does. Firefox does behind a flag. Safari does not. Zero copy is akin to shifting which CPU thread owns the pointer to that memory object (hence no copying). So let's say you do use Chrome and you do use offscreen canvas to transfer, you'd still need to trick the underlying TensorFlow library doing the ML and the faceapi.js library (both) into believing they are in a normal web browser environment (not node.js, and not webworker)  so that they will process the data using Canvas. At that point, you'll have CPU work running off the main ui thread, and you'll have the GPU processing the ML quickly, but again the limitation there is that is only possible with Chrome and Firefox, and I don't know if mobile iOS versions have those same APIs supported. I filed tickets with Tensorflow and worked on faceapi.js to do exactly this, but the use case I had was a kiosk app on the desktop. Mobile is a long shot.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/324;How to run in background/unfocused tab?;5;open;2019-06-16T21:49:06Z;2019-07-09T18:33:57Z;I'm trying to monitor face position on a webcam even when the user is using another tab in their browser. The tab with face-api code and a webcam video is still there, just not active. This does not seem to work out of the box.I got it working partially by copying image frames from the webcam video into a canvas element every 1000ms and then detecting faces on that element instead. However, this only works reliably in Chrome, and I would like to get it working in all newer desktop browsers. Any ideas on how to detect faces in a background/unfocused tab in a better way?Thanks for an amazing tool! :);"['Browsers throttle the event loop to 1000ms for background tabs to conserve energy. You can use the PageVisibility API to detect when this happens and pause processing accordingly. This limit is applied to timers and RAF calls, so there is no way around it other than keeping the window active. =====', ""Hi @jeffreytgilbert :) I'm aware of the 1000ms throttling, the problem I'm facing is that I can't seem to get the face-api code to detect faces when in background tabs in other browsers than Chrome. Any ideas?====="", 'Are you seeing streaming events to process from video when the tabs are backgrounded? =====', 'I\'ve added`if(videoEl.paused) { console.log(""paused""), }` to the onPlay function in the face-api examples.It appears the video is paused when switching tabs in Safari and Firefox, but keeps running in Chrome.I\'m not sure if this is the way to check if there are streaming events?=====', ""you're definitely setting yourself up for a subpar experience if you're planning for this to run in the background of a browser tab. most browsers are erring to the side of being polite to the user, not being a replacement for native apps. probably things go more toward background processes are nerfed rather than less. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/319;Speed up the face matching process;1;open;2019-06-13T07:10:23Z;2019-06-21T03:06:47Z;Hi @justadudewhohacks As you wrote [before](https://github.com/justadudewhohacks/face-api.js/issues/147#issuecomment-441019706). The face match process is computing the every euclidean distance between the descriptor of your photo and the descriptors from your database. The computation time will increase with the database growing. Is there any methods to speed up the face matching process ?Thanks;"[""Just tried [tensorflow KNN classifier](https://github.com/tensorflow/tfjs-models/tree/master/knn-classifier) for face matching process, but do not know which is better. Anyone have ideas? ThanksA part of the training code is as follows```const classifier = knnClassifier.create()await Promise.all(      this.props.train.face.map(async face => {        const image = await faceapi.fetchImage(face)        const descriptors = await faceapi.computeFaceDescriptor(image)        const tensor = tf.tensor(descriptors)        classifier.addExample(tensor, 'Name')      }))```A part of the recognizing code is as follows```const results = await faceapi      .detectAllFaces(        image,        new faceapi.TinyFaceDetectorOptions({          inputSize: environment.tinyInputSize,          scoreThreshold: environment.tinyThreshold        })      )      .withFaceLandmarks(true)      .withFaceDescriptors() if (results) {      faceapi.matchDimensions(canvas, image)      const resizedResults = faceapi.resizeResults(results, image)      resizedResults.forEach(async ({ detection, descriptor }) => {        const tensor = tf.tensor(descriptor)        const predict = await this.classifier.predictClass(tensor, 1)        const label = predict.label.toString()        const options = { label }        const drawBox = new faceapi.draw.DrawBox(detection.box, options)        drawBox.draw(canvas)     })}```Full code can be found [here](https://github.com/Panepo/Uzuki/blob/master/src/pages/Train/TrainKNN.jsx#L151) and [here](https://github.com/Panepo/Uzuki/blob/master/src/pages/Sensor/SensorKNN.jsx#L215).=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/318;Train a tiny face detector;2;open;2019-06-12T07:40:59Z;2019-09-09T01:55:51Z;Can you share how you trianed your tiny_face_detect_model? It's really a small and good model,I want to retrain it in my business,very thanks;"['I used to train this model using https://github.com/justadudewhohacks/tfjs-tiny-yolov2. But I nowadays train models on colab instead of in the browser.=====', 'Hi, @justadudewhohacks , right now we are have a project to detect emotion , we plen to train with our own sample images too, would you share abit about ""colab"" you mention above ? anyway we also check this link `https://github.com/justadudewhohacks/tfjs-tiny-yolov2` to start learn how to train it=====']"
https://github.com/justadudewhohacks/face-api.js/issues/317;simple test/benchmark;1;open;2019-06-10T19:40:33Z;2019-06-11T12:52:33Z;"Hi Vincent (@justadudewhohacks),What about a simple test/benchmark with:https://www.windowscentral.com/sites/wpcentral.com/files/styles/xlarge/public/field/image/2014/11/worlds-largest-selfie.jpgMy result using github version:![Screenshot from 2019-06-10 21-20-11](https://user-images.githubusercontent.com/6576335/59220603-abe15400-8bc5-11e9-8800-68566329b9fe.png)I must have been doing something wrong, because decreasing the value of ""Min Confidence:"" did not do anything.It would be also nice if the selection on the right bottom detection would be more precise.Reference:1. https://github.com/peiyunh/tiny![Result from Finding Tiny Faces](https://raw.githubusercontent.com/peiyunh/tiny/master/selfie.png)2. https://www.researchgate.net/publication/329116152_Learning_Better_Features_for_Face_Detection_with_Feature_Fusion_and_Segmentation_Supervision![Result from Finding Tiny Faces](https://www.researchgate.net/profile/Binghui_Chen/publication/329116152/viewer/AS:704994110095367@1545095102465/background/2.png)Greetings,Tolik";"['> I must have been doing something wrong, because decreasing the value of ""Min Confidence:"" did not do anything.Hmm, could be a bug in the demo.> It would be also nice if the selection on the right bottom detection would be more precise.Agree, would be interesting to see, if we can train a better model than the current SSD Mobilenet V1, which is still feasible for web apps. In fact I started with training own face detectors, but have stopped working on it. Will pick up on this some time soon again, so thanks for all the links you shared!As far as the tiny faces in the background are concerned, they will be useless for further face classification tasks however, as they are simply too small in size as an input for any classification network. So I am not totally convinced yet, how such a face detector would benefit face-api,js or web applications in general.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/314;Face-api.js with 360 camera and A-Frame?;3;open;2019-06-06T15:08:35Z;2019-07-09T13:34:25Z;Hello,Has anyone tried using face-api.js with a 360 camera as a webcam (e.g. Ricoh Theta V), displaying the video feed in an <a-videosphere> in A-Frame, along with AR face markers?Been wondering if it is possible or has already been done.Would be interesting to see some examples!Thank you!;"[""Hello, Were you able to find a solution? I'm also needing to use the IFRAME html tag. Thank you very much!====="", ""> Hello,> > Were you able to find a solution? I'm also needing to use the IFRAME html tag.> > Thank you very much!Just in case you are confused, A-Frame (https://aframe.io/) is nothing to do with the HTML iframe element.====="", 'OHHH sorry, I already managed to solve my problem, anyway, my friend.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/311;Why not compatible with IE browsers? Is there a solution?;5;open;2019-06-04T05:44:17Z;2020-03-25T20:19:49Z;Why not compatible with IE browsers? Is there a solution?;"['I have same problem.=====', 'I would guess that support for IE is lacking because the last version of IE (11) is due for end-of-life in just a few months in January 2020.=====', ""It's around 10% and falling last I checked. Given the niche nature of projects developed with face recognition, you should prompt users to upgrade to a newer browser. There are options. Browsers older than IE 11 are basically nonexistent today thanks to the Windows 10 free edition baselining systems to at least IE 11 and/or Edge. ====="", 'Because IE does not support WebGL 2.0. Same with Edge.=====', ""To make it work with Edge you need to set this config option:faceapi.tf.ENV.set('WEBGL_PACK', false),Works great for me on Edge now!If you only want to set it for IE/Edge you can do this:/** * detect IE * returns version of IE or false, if browser is not Internet Explorer */function detectIE() {  var ua = window.navigator.userAgent,  // Test values, Uncomment to check result …  // IE 10  // ua = 'Mozilla/5.0 (compatible, MSIE 10.0, Windows NT 6.2, Trident/6.0)',  // IE 11  // ua = 'Mozilla/5.0 (Windows NT 6.3, Trident/7.0, rv:11.0) like Gecko',  // Edge 12 (Spartan)  // ua = 'Mozilla/5.0 (Windows NT 10.0, WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0',  // Edge 13  // ua = 'Mozilla/5.0 (Windows NT 10.0, Win64, x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586',  var msie = ua.indexOf('MSIE '),  if (msie > 0) {    // IE 10 or older => return version number    return parseInt(ua.substring(msie + 5, ua.indexOf('.', msie)), 10),  }  var trident = ua.indexOf('Trident/'),  if (trident > 0) {    // IE 11 => return version number    var rv = ua.indexOf('rv:'),    return parseInt(ua.substring(rv + 3, ua.indexOf('.', rv)), 10),  }  var edge = ua.indexOf('Edge/'),  if (edge > 0) {    // Edge (IE 12+) => return version number    return parseInt(ua.substring(edge + 5, ua.indexOf('.', edge)), 10),  }  // other browser  return false,}=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/310;How do I track the same person in video;3;open;2019-06-03T09:50:07Z;2019-07-22T13:26:50Z;How do I track the same person in video;"['Use face descriptor matching. Check out the face recognition tutorials and examples.=====', ""Thank you very much！When will the error be reported？    if (!params) {      throw new Error('FaceRecognitionNet - load model before inference')    }====="", ""params will not be defined if you didn't call the networks load method before attempting to forward an image through the net.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/302;Low face recognition accuracy for asian guys;3;open;2019-05-23T09:20:51Z;2020-03-16T01:14:10Z;We tried with our company employees (Asian faces) but sometimes the distance between 2 different faces are quite low (below 0.3), we can easily separate them with eyes but looks the face-api pre-trained model could not separate them apart, we tried some western faces and it can work well. So I am thinking if there are not so many Asian face samples in the training dataset so it may not perform very well towards them? If I have some Asian face dataset and want to train a new model, would you please advice how should I do the retrain? Thanks. ;"[""Yes your assumption is correct, the author of dlib (the project where the model is from) also states somewhere in his blog I think, that there is a bias towards western faces. I think you can retrain the model using the corresponding dlib example, but afterwards the model weights have to be converted to a the format that tensorflow uses. But I can help you with the latter one, if you really want to retrain the model.Other than that, I am planning to train an own face recognition model as well since I want to have a more web friendly and more efficient model for face-api.js. But I can't make any promises when that model will be included.====="", ""> Yes your assumption is correct, the author of dlib (the project where the model is from) also states somewhere in his blog I think, that there is a bias towards western faces. I think you can retrain the model using the corresponding dlib example, but afterwards the model weights have to be converted to a the format that tensorflow uses. But I can help you with the latter one, if you really want to retrain the model.> > Other than that, I am planning to train an own face recognition model as well since I want to have a more web friendly and more efficient model for face-api.js. But I can't make any promises when that model will be included.My project also encountered this problem and I had to use a traditional C/C++ face API. So I'm really looking forward to have this feature in the future. Also please please consider to bring liveness detection to face-api, so I can make sure it is a living person standing in front of a cam not a picture.====="", ""@dr1llc4t , hi, I am also interested in retrain the model with a new dataset, would you mind telling me how far have you gone? Or in which direction have you been working on? my email address is sxtgwzz@163.com. ( I don't know what language should I use here, so I just use the common one )Thanks!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/299;Degraded age detection performance of different systems;3;open;2019-05-22T05:41:13Z;2019-05-22T19:33:52Z;Hello,Recently I have integrated age & gender detection feature in my electron app. It is integrated and tested on laptop, and everything works perfect. Both age & gender prediction have very less error rate. But when I run the same application on another machine, the predictions are completely different than my machine. The error rate is very high for age detection. Gender detection works correctly.Below is the configuration of both the machines:-- My Machine (Works Perfect on this) --- DELL ( i3 processor, 8GB RAM, 2TB HDD, Intel HD Graphics, All drivers available)- Windows 10 64 bit- Electron 5.x- Node 10.15 latest LTS-- Target Machine ( Works Incorrrect )--- Lenovo ( i3 processor, 4 GB RAM, Intel HD Graphics, All drivers available)- Windows 7 64 bit- Electron 5.x- Node 10.15 ;"['Hey,There are still problems in the tfjs WebGL backend with both electron #262 #264 as well as Intel GPUs #43. The problem that you are seeing could be related to any of them or a combination thereof.Can you try to run the web age recognition example of this repo on your target machine? If this is not working properly outside of electron then it might be a precision issue with your Intel GPU. In any case, it still remains to be done to figure out the exact cause of these deviations from the expected results, e.g. which ops of tfjs-core are causing this, so that we can report more information to the tfjs team.=====', 'Well I tried the same using Chrome. I got same results. The result for my machine (that works perfectly) shows correct age (25 - 26 years).  and second machine (target) shows very less age (11 - 14 years).  I guess you can figure out which operation is taking it down.Secondly, I implemented face detection and recognition for live cams in electron and till now it is working good on staging deployment with 15-20 machines. Do you think it is a good idea to release my electron application with face recognition for 70 thousand computers around the country?Because, if this is a hardware issue ( precision issue of Intel GPU you mentioned ), what will be the success failure ratio of this feature for 70,000 machines? =====', ""Hmm, hard to say. I am only aware of some precision issues on INTEL GPUs that remain using the WebGL backend of tfjs, although they did a lot of fixes there, as well as some issues with electron, but I didn't investigate further in the latter one.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/294;Browser example not working on chrome version 74.0.3729.131;4;open;2019-05-13T23:12:02Z;2021-02-25T11:36:39Z;Hi There,The browser example doesn't seem to work on the Chrome browser version 74.0.3729.131.Below is the error which I see in the console:C:\fakepath(179,2-41): warning X3550: array reference cannot be used as an l-value, not natively addressable, forcing loop to unrollC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\fakepath(179,2-41): warning X3550: array reference cannot be used as an l-value, not natively addressable, forcing loop to unrollC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D ShadersUncaught (in promise) Error: Failed to link vertex and fragment shaders.    at linkProgram (tf-core.esm.js:17)    at e.createProgram (tf-core.esm.js:17)    at compileProgram (tf-core.esm.js:17)    at tf-core.esm.js:17    at e.getAndSaveBinary (tf-core.esm.js:17)    at e.compileAndRun (tf-core.esm.js:17)    at e.conv2dWithIm2Row (tf-core.esm.js:17)    at e.conv2d (tf-core.esm.js:17)    at ENV.engine.runKernel.x (tf-core.esm.js:17)    at tf-core.esm.js:17;['Hi,I am with the same error I verified that this only happens and machines with little memory and old processors I carry out the tests with machines with 8GB and Processors I5 worked perfectly, however the machines 4GB and Processes Phenom II X4 or similar gave error=====', 'I have intel I5 with 6GB and got same error. =====', 'getting same error, on 6GB, i3. however its working with 16gb i7. =====', '> Hi,> > I am with the same error I verified that this only happens and machines with little memory and old processors I carry out the tests with machines with 8GB and Processors I5 worked perfectly, however the machines 4GB and Processes Phenom II X4 or similar gave errorHi,I have 8gb and i5 and have the same problem on the Chrome, at the same time on the browser Safari works fine.=====']
https://github.com/justadudewhohacks/face-api.js/issues/289;Add support for Node.js v12;1;open;2019-05-08T12:11:33Z;2019-05-08T14:01:14Z;I am unable to install and run `face-api.js` with Node.js v12.### Steps to reproduce```cd face-api.jsnvm install 12nvm use 12npm install```### Expected result`face-api.js` installs correctly.### Actual result`npm` outputs a wall of errors. Most notably build errors with `fsevents@1.2.8` and `canvas@2.0.1` ### EnvironmentNode: `v12.2.0`face-api.js: `0.20.0`### CommentsAfter a bit of googling around it looks like indeed `fsevents` and `canvas` only add support for Node.js v12 in `fsevents@1.2.9` and `canvas@2.5.0` respectively, see https://github.com/fsevents/fsevents/pull/274 and https://github.com/Automattic/node-canvas/pull/1409;"[""Installing the package from npm works fine with node 12.2. Installing the dev dependencies of this repo just doesn't work, because I didn't upgrade them yet to be compatible with node 12.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/284;Stop recognition once face is a known detected;2;open;2019-05-03T01:56:56Z;2020-01-20T13:34:11Z;Hi, Is there a way to stop recognition once a known face is being recognized? Example: if I have a couple of unknown user appearing on web cam, there is only 1 person which is detected to be 'Person A', can i pause / prompt the system that there is one person that is identified? Do share your views how this can be done. Thank you in advance!;"[""You can stop the face recognition at any time sure. That's not something that face-api.js handles, the library simply does predictions frame by frame. That's something you handle in your code.====="", 'Hello, does somebody has found an answer to this?I don\'t know how to handle this via code.I have tried doing this:function stopStreamedVideo(videoElem) {  let stream = videoElem.srcObject,  let tracks = stream.getTracks(),  tracks.forEach(function(track) {    track.stop(),  }),  videoElem.srcObject = null,}But, if after that, I try to resume again the tracking via:function run() {console.log(""RUN""),var deviceId1 = ""b39987e633d921f2cbe84fa9c9a9308631f8b7f48f675e05582548f2808cb8bf"",stream1 = await navigator.mediaDevices.getUserMedia({ video: {deviceId: {exact: deviceId1}} }),videoEl1 = document.getElementById(\'inputVideo1\'),videoEl1.srcObject = stream1,}But what I get is an increase of the console output everytime I call this function.The first time the console outputs ""run""The second time outputs ""run"" twice.The third time outputs ""run"" three consecutive times..... and so onSo in the 10th or more iterations, the App crashes and the video output goes away, also with the face recognition=====']"
https://github.com/justadudewhohacks/face-api.js/issues/283;Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array;1;open;2019-05-02T12:47:00Z;2019-05-08T07:49:58Z;Hi, variable faceProfiles consists of following: ![image](https://user-images.githubusercontent.com/29422683/57069905-2faa4580-6cd6-11e9-98d7-c2d999e00e9c.png)now I want to use this faceProfile to create a faceMatcher using following code`    let labeledDescriptors = [],    faceProfiles.forEach(faceProfile => {        let descriptors = faceProfile.descriptors,        const labeledDescriptor = [            new faceapi.LabeledFaceDescriptors(faceProfile.name, descriptors)        ],        labeledDescriptors.push(labeledDescriptor)    }),    // Create face matcher (maximum descriptor distance is 0.5)    return new faceapi.FaceMatcher(        labeledDescriptors,        maxDescriptorDistance    ),`an then i get the following error code ![image](https://user-images.githubusercontent.com/29422683/57076126-21195980-6ce9-11e9-9e7f-3164d81e1e79.png);"[""Actually the error message says what you have to do. The descriptors are expected to be Float32Array's: `descriptors.map(desc => new Float32Array(desc))`.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/282;failed to link vertex and fragment shaders;9;open;2019-04-29T11:11:15Z;2021-02-19T03:29:04Z;"I import `""face-api.js"": {      ""version"": ""0.19.0"",      ""resolved"": ""https://registry.npmjs.org/face-api.js/-/face-api.js-0.19.0.tgz"",      ""integrity"": ""sha512-XbEIqqnLzlVIkvtLffFIl/Ltit3VAnJ1B3aFEc28pnkNVvqzJnBJbZVgbJ0JTvaafdSPJ5pFufC2t0KZJhwdpw=="",      ""requires"": {        ""@tensorflow/tfjs-core"": ""1.0.3"",        ""tfjs-image-recognition-base"": ""^0.5.1"",        ""tslib"": ""^1.9.3""      },      ""dependencies"": {        ""tslib"": {          ""version"": ""1.9.3"",          ""resolved"": ""https://registry.npmjs.org/tslib/-/tslib-1.9.3.tgz"",          ""integrity"": ""sha512-4krF8scpejhaOgqzBEcGM7yDIEfi0/8+8zDRZhNZZ2kjmHJ4hv3zCbQWxoJGz1iw5U0Jl0nma13xzHXcncMavQ==""        }      }    },`but I got an error in chrome 70 in windows, as follows:<img width=""487"" alt=""99"" src=""https://user-images.githubusercontent.com/4914876/56892535-859ea380-6ab2-11e9-9fd3-cb3accaa7224.png"">";"['If can I update tensorflow to `""dependencies"": {    ""@tensorflow/tfjs-converter"": ""1.1.0"",    ""@tensorflow/tfjs-core"": ""1.1.0"",    ""@tensorflow/tfjs-data"": ""1.1.0"",    ""@tensorflow/tfjs-layers"": ""1.1.0""  }`=====', '![EADBE271B539976EDD5C697E318419FB](https://user-images.githubusercontent.com/4914876/56936442-051b8980-6b2a-11e9-9fc3-ce68d4c89101.png)chrome 73 also has same issue...![7B2F30A04C43DB51A733B2F78618E808](https://user-images.githubusercontent.com/4914876/56936510-51ff6000-6b2a-11e9-9cf9-46d619d60a14.jpg)How to solve it ? please help me , thank you very much.=====', 'https://github.com/llSourcell/pose_estimation/issues/9give the answer, but I update to 1.0.4, still has the issue....sigh...=====', 'This either is a problem with your machine (I think the tfjs team has a sample page somewhere, where you can check the capabilites related to the WebGL backend), or the problem is due to unaligned package versions. face-api.js is currently using @tensorflow/tfjs-core 1.0.3, so you should use this version.=====', 'I got the same error. does you founded resolve to that? =====', ""The error also occurs in our app, but only with chrome on older devices (8gb RAM, intel core i5 M 560 2.67 GHz). On the same devices it works with firefox.  On new devices it works with chrome as well as with firefox.![image](https://user-images.githubusercontent.com/13392042/59786015-94415400-92c6-11e9-9738-0b4ab9e3bbe5.png)Then, I set `tf.ENV.set('WEBGL_PACK', false)` at the beginning of my app (from this [sstackoverflow-comment](https://stackoverflow.com/questions/55188619/ms-edge-script5022-failed-to-link-vertex-and-fragment-shaders)). On new devices it works again, on the older device the error `Box.constructor - expected box to be IBoundingBox | IRect, instead have ...` appears.![image](https://user-images.githubusercontent.com/13392042/59785982-7c69d000-92c6-11e9-8647-7e07740bd644.png)====="", ""@PutziSan I got this error too. I resolved it by setting below specific inputSize:```jsnew faceapi.TinyFaceDetectorOptions({      inputSize: 256, // this line solves 'Box.constructor - expected box to be IBoundingBox | IRect, instead ...'      scoreThreshold: 0.5,    }),```I believe blow inputSize also works:https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/tinyYolov2/TinyYolov2.ts#L27====="", ""Hi, > @PutziSan> I got this error too. I resolved it by setting below specific inputSize:> > ```js> new faceapi.TinyFaceDetectorOptions({>       inputSize: 256, // this line solves 'Box.constructor - expected box to be IBoundingBox | IRect, instead ...'>       scoreThreshold: 0.5,>     }),> ```> > I believe blow inputSize also works:> https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/tinyYolov2/TinyYolov2.ts#L27This didnt work for me. Is there any solution for Chrome ?====="", 'I was having this error when running the html served in a node environment. Running it from traditional apache works fine.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/281;Tensor size error;13;open;2019-04-29T08:41:34Z;2019-11-13T11:05:58Z;Hi! I have a problem while loading the model with `faceapi.nets.faceLandmark68Net.loadFromUri('/public/models')`The model is available at the `models` directory. But an error arises while loading the model selected. In this case `face_landmark_68_model-shard1` and `face_landmark_68_model-weights-manifest.json` are placed into the directory. But the following error appears:`Error: Based on the provided shape, [3,3,32,1], the tensor should have 288 values but has 64`Can someone help me please?EDIT: This error appears if I my loading function is not `async`, if I make it `async` another error appears (`regeneratorRuntime is not defined`)Thanks for your info;"['<img width=""849"" alt=""Captura de pantalla 2019-04-30 a las 13 48 29"" src=""https://user-images.githubusercontent.com/32225295/56959728-b3d1c100-6b4e-11e9-8dca-ac5b093a41c1.png"">This is shown in the Inspector Network Tab. Apparently the GET request for the shard is not complete, but returns a 200 code. Does anyone know why is this possible?=====', 'Where do you host your application? Or does this also happen to be the case on your local dev environment (localhost)?=====', 'It happens in my local environment. I solved the error of the shape by creating an extension to the `shard1` file and in the `json` file setting the new name of the shard at the end with the extension I placed.But still having the `regeneratorRuntime` and trying to solve it.=====', ""This means that the node version you are using doesn't seem to support async functions? Try updating nodejs to a more recent version or use babel if you have to use an older version.====="", 'Ok, my advances. I achieved to resolve it. And another problem arose 🤦\u200d♂. When using detectFaceLandmarks I am being returned a Promise. If I need to save those landmarks for the next videoFrame/Image how can I return that value outside the promise or is there any method to use faceapi ""synchronously""? **Sidenote**: if I use `faceapi.detectFaceLandmarks(image),` on full image (not just an image cropped to the face) am I doing something wrong? It returns unexpected values.=====', '> If I need to save those landmarks for the next videoFrame/Image how can I return that value outside the promise or is there any method to use faceapi ""synchronously""?I don\'t totally get what you are trying to achieve, but why don\'t you simply wait for the promise to resolve?> Sidenote: if I use faceapi.detectFaceLandmarks(image), on full image (not just an image cropped to the face) am I doing something wrong? It returns unexpected values.Yeah that\'s not going to work out, since the face landmark net expects the cropped face image as input, which is obtained after face extraction based on the face box detected from the face detector.=====', ""```async function detectLandmarks(image) {  const landmarks = await faceapi.detectFaceLandmarks(image),  return landmarks,}```> I don't totally get what you are trying to achieve, but why don't you simply wait for the promise to resolve?That was my attempt. I am trying to wait the promise to resolve, but the method I pasted above, inside it (before return) is not a Promise, is resolved, but after return, where I use it is a Promise again and I can only use its values with the `then` structure (which I don't want)====="", '`faceapi.detectFaceLandmarks(image)` returns a Promise and `await faceapi.detectFaceLandmarks(image)` resolves the Promise. Does that solve it?=====', ""Not at all. If I log the `landmarks` inside the method:```async function detectLandmarks(image) {  const landmarks = await faceapi.detectFaceLandmarks(image),  console.log(landmarks),  return landmarks,}```It doesn't print a promise but the value of the landmarks. (Here is where I said Yay! I got it).But when using the method like:```outsideLandmarks = detectLandmarks(outsideImage),console.log(outsideLandmarks)```Which simply calls the method defined with the await inside it, prints a Promise.Maybe I'm missing something or doing something wrong.Tell me if I didn't explain myself.Thanks for your patience====="", ""An async function always returns a Promise. Maybe I do not fully understand, what's the issue you are facing with the function returning a Promise?====="", ""```state = initialState,computedLandmarks = computeLandmarks(detectLandmarks(image)),updateState depending on the landmark computation,```This is what I'm trying to do more or less. With the promises what I have to do is:```state = initialState,computedLandmarks = detectLandmarks(image)     .then((landmarks)=>                computation = computeLandmarks(landmarks)                return computation),updateState depending on the landmark computation,```I cant do this because I cant extract the computation from the promise. So I cant update the state====="", 'I managed to solve it serving model files from a [cdn](https://gitcdn.xyz/)Use https://gitcdn.xyz/repo/justadudewhohacks/face-api.js/master/weights/ as `MODEL_URL`=====', 'This is a serious problem for using `parcel` to do development with this library:https://github.com/elwin013/parcel-plugin-static-files-copy/issues/37https://github.com/parcel-bundler/parcel/issues/1098https://github.com/tensorflow/tfjs/issues/924=====']"
https://github.com/justadudewhohacks/face-api.js/issues/280;@tensorflow/tfjs-node is not correctly recognised when running examples;7;open;2019-04-28T12:43:01Z;2021-09-17T03:14:54Z;I am trying to run the NodeJS examples. The examples run, but seem to not properly utilize the TensorFlow C++ binding.### Steps to reproduce```cd face-api.js/examples/examples-nodejsnpm itsc faceDetection.tsnode faceDetection.js```### Expected outcome:Examples runs without warnings.### Actual outcome:```============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================```### EnvironmentNode: `v11.14.0`face-api.js: `0.19.0`@tensorflow/tfjs-node: `1.0.2`### Additional InformationsI tried running the examples with older versions to exclude the possibility of a problem with my setup. I managed to run the examples for face-api.js versions `0.17.0` and `0.17.1` with face-api.js picking up the TensorFlow bindings correctly.;"[""Strange, can you provide the output of the following:``` javascriptimport tfnode from '@tensorflow/tfjs-node',console.log(tfnode.version)```====="", '> ```js> import tfnode from \'@tensorflow/tfjs-node\',> console.log(tfnode.version)> ```fails with ```faceDetection.ts:5:8 - error TS1192: Module \'""<some path>/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node/dist/index""\' has no default export.```I instead ran ```jsimport * as tfjsnode from \'@tensorflow/tfjs-node\',console.log(tfjsnode.version)```Which yields```{ \'tfjs-core\': \'1.1.2\',  \'tfjs-data\': \'1.1.2\',  \'tfjs-layers\': \'1.1.2\',  \'tfjs-converter\': \'1.1.2\',  tfjs: \'1.1.2\',  \'tfjs-node\': \'1.1.2\' }```**Edit:** I had to rerun the **Steps to reproduce** and apparently it now installs a more recent version of `tfjs` (`1.0.2` -> `1.1.2`). However, the problem (`Hi there 👋....`) remains even with version `1.1.2`.=====', 'Hmm, how about explicitly installing the correct tfjs-node version? `npm i @tensorflow/tfjs-node@1.0.2`=====', ""Ok, after ```npm i @tensorflow/tfjs-node@1.0.2```the example runs correctly, without warning. The version output is:```{ 'tfjs-core': '1.0.4',  'tfjs-data': '1.0.4',  'tfjs-layers': '1.0.4',  'tfjs-converter': '1.0.4',  tfjs: '1.0.4',  'tfjs-node': '1.0.2' }```However, now I am completely lost, what is happening here 🤔 Does this mean I need to make sure `face-api.js` and the example use the exact same version of `tfjs-node`?====="", 'face-api.js does not use tfjs-node at all. face-api.js is depending on tfjs-core. So in order to run face-api.js with the tfjs-node backend, you have to install a tfjs-node version, which is compatible with the tfjs-core version that face-api.js uses.=====', 'Sorry for the confusion.I Time Machine restored my repo to the state I had when I opened this issue. Now, I get```{ \'tfjs-core\': \'1.1.0\',  \'tfjs-data\': \'1.1.0\',  \'tfjs-layers\': \'1.1.0\',  \'tfjs-converter\': \'1.1.0\',  tfjs: \'1.1.0\',  \'tfjs-node\': \'1.1.0\' }============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================done, saved results to out/faceDetection.jpg```Aha, so I actually used `@tensorflow/tfjs-node@1.1.0`, not `@tensorflow/tfjs-node@1.0.2` as said in OP. Apologies! So, it looks like the problem arises when using `@tensorflow/tfjs-node>=1.1.0`. Indeed after ```diff@@ -2,7 +2,7 @@   ""author"": ""justadudewhohacks"",   ""license"": ""MIT"",   ""dependencies"": {-    ""@tensorflow/tfjs-node"": ""^1.0.2"",+    ""@tensorflow/tfjs-node"": ""~1.0.2"",     ""canvas"": ""^2.0.1"",     ""face-api.js"": ""../../""   }```and```npm install```It now runs correctly 🎉 ```{ \'tfjs-core\': \'1.0.4\',  \'tfjs-data\': \'1.0.4\',  \'tfjs-layers\': \'1.0.4\',  \'tfjs-converter\': \'1.0.4\',  tfjs: \'1.0.4\',  \'tfjs-node\': \'1.0.3\' }cpu backend was already registered. Reusing existing backenddone, saved results to out/faceDetection.jpg```=====', '> face-api.js does not use tfjs-node at all. face-api.js is depending on tfjs-core. So in order to run face-api.js with the tfjs-node backend, you have to install a tfjs-node version, which is compatible with the tfjs-core version that face-api.js uses.Ok, I think I got it now. However, this is valuable information and might be important for newcomers that try to get started with face-api.js in a Node.js environment. #### SuggestionShould we ...1. pin the `tfjs-node` version in the Node.js example, such that the example will run correctly on a freshly cloned repo and2. add a small comment somewhere in the Node.js example explaining that a compatible `tfjs-node` version is needed?If no objections, I am happy to send PR for the above mentioned.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/278;Face expression probabilities random on chrome android;9;open;2019-04-26T08:31:17Z;2019-05-17T12:18:57Z;"After loading and detecting face from html video element, the probability distribution is correct for a few frames (happy 0.99, sad 0.0000343) etc.After a few processed frames, the probabilities jump all over the place (see screenshots below).The same code works fine when using it in chrome on my desktop machine (Nvidia GTX 1060)Android 8.0.0Chrome 73.0.3683.90Samsung Galaxy S7![probabilities](https://user-images.githubusercontent.com/16301487/56792792-b0ff6500-680a-11e9-9cdf-e6c362da217e.png)```const model = FaceApiFaceDetectorModel.TINY_FACE_DETECTOR,const modelBaseUrl = '/assets/tfmodels',const minConfidence = 0.5,const inputSize = 256,const scoreThreshold = 0.5,const minFaceSize = 200,await loadFaceDetectorModel(model, modelBaseUrl),await faceapi.loadFaceExpressionModel(modelBaseUrl),const options = getFaceDetectorOptions(model, minConfidence, inputSize, scoreThreshold, minFaceSize),const result = await faceapi.detectSingleFace(video, options).withFaceExpressions(),```Might this be some kind of overflow?Edit: I tried it with the sample application (examples-browser) by updating the ""video face tracking"" example. The result is the same: Works on desktop, random values on mobile";"[""That's odd, especially since these are not even valid probabilities anymore. Looks like something is going wrong in `tf.softmax` here.====="", 'Maybe this is related: https://github.com/tensorflow/tfjs/issues/1488, although face-api does not do batch predictions, does it? The probabilities are correct on newer phones. Do you have any plans to upgrade your tfjs dependency? Might be this is already fixed.=====', 'The code that you posted does not do batch processing. Did you try to play around with enabling/disabling the specific features as @annxingyuan described in this issue? Does that help?=====', 'Yes, I tried all 3 suggestions. The predictions are ""better"" when setting `tf.ENV.set(\'WEBGL_PACK\', false)` (no more huge numbers), but still there are negative values and values in the range from -100 to 100.=====', 'Ok. All I can do for now is upgrade tfjs-core to latest, which I am working on now.=====', ""Thank you, I'll let you know in this issue if that helped====="", 'Ok I will have to postpone the upgrading, since tfjs-core 1.1.2 seems to not come with the platform specific check, which seems to be implemented in the current master. This breaks the unit tests, since the browser tests utilize a wrong fetch function.=====', 'Let me know if we can help=====', 'Update: seems to work now on chrome mobile 74.0.3729.136=====']"
https://github.com/justadudewhohacks/face-api.js/issues/277;Mobile browser getting stuck;4;open;2019-04-26T07:59:08Z;2020-06-09T06:02:34Z;Thanks for the cool library. I'm experiencing issues on mobile and would like to know whether there are any limitations or known issues for mobile? What happens in my case is that it appears as if tensor flow or another internal component gets stuck sometimes on the live face tracking and then the expression detection also gets stuck unless I manually position my phone so that my face fits in the stationary bound box. I'm using your demo link here https://justadudewhohacks.github.io/face-api.js/webcam_face_expression_recognition and viewing it on a Samsung S9+ running Chrome 73.0.3683.90. I've attached to my mobile via the Chrome debugger and no errors are being reported when I'm experiencing this issue.;"[""I believe I'm seeing a related issue on an iPhone 7 running iOS 12.3.1. The same demo link @die-rooikat  references above freezes on a single frame from my camera after I allow the use of the camera. The UI that is supposed to show over the camera feed never loads and the camera feed never updates. There are no errors in the debugger when connecting to my laptop.====="", 'Same issue here @nsbingham. Any update?=====', 'Did you find a solution for your question? =====', 'No update from my side issue still persists.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/274;DSFD: Dual Shot Face Detector;3;open;2019-04-23T21:40:17Z;2019-05-16T11:58:10Z;Hi, your solution is cool!What do we need to get following performance?https://github.com/TencentYoutuResearch/FaceDetection-DSFDDSFD achieves the best performance among all of the state-of-the-art face detectors based on the average precision (AP) across the three subsets, i.e., 96.6% (Easy), 95.7% (Medium) and 90.4% (Hard) on validation set, and 96.0% (Easy), 95.3% (Medium) and 90.0% (Hard) on test set.----One problem that I see is the model size:WIDERFace_DSFD_RES152.pth (459M)https://drive.google.com/uc?id=1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-&export=downloadWould be nice to get closer to this one ,-). Even if it takes more memory and time, there might be a use case for it.Thanks.;"[""Thanks for posting this! The model indeed is very big in size and complex:> Figure 2: Our DSFD framework uses a Feature Enhance Module (b) on top of a feedforward VGG/ResNet architecture...> WIDERFace_DSFD_RES152.pth (459M)But I will take a look at the paper, sometimes you can apply some of the ideas when training an own model. Also always good to have a reference of state of the art methods and curious how close you can get to state of the art with smaller and faster models, that are actually web friendly.To answer the following question:> What do we need to get following performance?https://github.com/TencentYoutuResearch/FaceDetection-DSFDI can't really tell you to be honest. First we would have to figure out, where the current models of face-api.js (ssd mobilenet and tiny face detector) fit into the picture performancewise.====="", 'Another reference for a good benchmark:https://github.com/deepinsight/insightface/tree/master/RetinaFace=====', 'The tensorflow implementation:https://github.com/610265158/DSFD-tensorflow=====']"
https://github.com/justadudewhohacks/face-api.js/issues/264;[electron] Face recognitions is not working properly in my project;22;open;2019-04-05T11:33:33Z;2020-03-18T13:53:45Z;I am doing a project in angular7 and electronjs. In my project I want to detect and recognise the face from given image. But I am facing a problem in face recognition it's not recognizing the face properly ![Screenshot from 2019-04-05 16-27-44](https://user-images.githubusercontent.com/34840376/55624393-505ca980-57c3-11e9-9c56-88534dfad676.png)But went I try with your example project face was recognizing properly.![Screenshot from 2019-04-05 16-29-46](https://user-images.githubusercontent.com/34840376/55624531-df69c180-57c3-11e9-8061-f6c99f90c572.png)![Screenshot from 2019-04-05 16-29-50](https://user-images.githubusercontent.com/34840376/55624532-e0025800-57c3-11e9-9f93-b2eef6c8b450.png) ;"['Hi !It is kind of hard to tell what is wrong in your project if we don\'t see your code. Could you add a code snippet with the involved code ?What do you want to achieve exaclty in first screenshot ? If I understand correctly, the first image is used to get face descriptor in order to populate a Face Matcher and you want to recognize this person in second image, am I right ?According to the few information we have, you are maybe not adding correctly face descriptor from first image to your face matcher.```_currentLabeledDescriptor = new faceapi.LabeledFaceDescriptors(""person1"", [_descriptorsFromFirstPicture])_localFaceMatcher  = new faceapi.FaceMatcher([_currentLabeledDescriptor])```Double check that _descriptorsFromFirstPicture is a Float32Array and that you have ""logical"" values (i,e, floats with a lot of digits and values in range [-1,1]).=====', 'yes, I want to recognise person from first image in Second image. Your App recognise it properly but my code is not recognising it.First Image Facedetection code( .ts ):    const firstFaceDescription = await faceapi      .detectSingleFace(img)      .withFaceLandmarks().withFaceDescriptor()    if (!isNullOrUndefined(firstFaceDescription))     {      const detectionsForSize = faceapi.resizeResults(firstFaceDescription,        { width: profileImgRef.width, height: profileImgRef.height })            const canvas = this.canvas1.nativeElement,      canvas.width = profileImgRef.width,      canvas.height = profileImgRef.height,      const detectionsArray = detectionsForSize.detection      const singleLabelDiscription = new faceapi.LabeledFaceDescriptors(""person1"", [firstFaceDescription.descriptor])      this.faceMatcher = new faceapi.FaceMatcher(singleLabelDiscription)      faceapi.drawDetection(canvas, detectionsArray, { withScore: true, withClassName: true })    }Second image detection code          const fullFaceDescriptions = await faceapi                      .detectAllFaces(img)                      .withFaceLandmarks().withFaceDescriptors()    if (!isNullOrUndefined(fullFaceDescriptions))     {      console.log(fullFaceDescriptions),            const detectionsForSize = faceapi.resizeResults(fullFaceDescriptions,         { width: groupImgRef.width, height: groupImgRef.height })            const canvas = this.canvas2.nativeElement,      canvas.width = groupImgRef.width,      canvas.height = groupImgRef.height,            const detectionsArray = detectionsForSize.map(fd => fd.detection)      // Face recognition      const results = fullFaceDescriptions.map(fd => this.faceMatcher.findBestMatch(fd.descriptor))      console.log(results)      const boxesWithText = results.map((bestMatch, i) => {        const box = detectionsArray[i].box        const text = bestMatch.toString()        const boxWithText = new faceapi.BoxWithText(box, text)        return boxWithText      })      faceapi.drawDetection(canvas, boxesWithText)    }![Screenshot from 2019-04-08 23-52-28](https://user-images.githubusercontent.com/34840376/55747322-9cc11700-5a59-11e9-8573-0ea1bfae61dd.png)One more question: How many face descriptor of each person will required for recognition that person=====', 'Since both distances are 0, odds are, that you are comparing the exact same image against eachother.> One more question: How many face descriptor of each person will required for recognition that personOne descriptor is sufficient.=====', ""@justadudewhohacks He is right. There's definitely a big problem with your library. I computed the face descriptors(as in your example) for DIFFERENT images of DIFFERENT( and i really mean different) people and guess what, in some of them i got the exact same face descriptor. But the strange thing is that this infamous descriptor is THE EXACT ONE showed by the indian man in the picture above(the picture shows only some of it but i'm sure 100% that the rest of it is equal to what i got) , which is:[-0.036157943308353424,0.11451219767332077,0.08395102620124817,-0.015166843309998512,-0.06600701808929443,-0.042419757694005966,-0.0490785650908947,0.008687270805239677,0.14321738481521606,-0.025509962812066078,0.2366054803133011,0.001498898956924677,-0.23820878565311432,-0.046752505004405975,0.0028616711497306824,0.08502069860696793,-0.14962242543697357,-0.06591969728469849,-0.10743647068738937,-0.12046073377132416,0.033088553696870804,0.00343224941752851,0.018290642648935318,0.009096329100430012,-0.09878937155008316,-0.2763112187385559,-0.07352373749017715,-0.1335674375295639,0.08969920873641968,-0.19571422040462494,0.012862971052527428,0.021690549328923225,-0.13443519175052643,-0.05735789239406586,-0.02766452543437481,0.02025718241930008,0.0033780643716454506,-0.08854390680789948,0.15200215578079224,-0.03676857054233551,-0.14023277163505554,-0.053991273045539856,-0.010764091275632381,0.2646256685256958,0.18705308437347412,0.044662196189165115,0.00966082513332367,-0.043214693665504456,0.06518538296222687,-0.2766679525375366,0.02771242894232273,0.1624835729598999,0.02950565330684185,0.12462048977613449,0.06644287705421448,-0.1700247824192047,0.0408409908413887,0.08843832463026047,-0.1381165087223053,0.03919530287384987,0.0404399111866951,-0.09420391917228699,-0.036383725702762604,-0.11028037220239639,0.21431322395801544,0.06784548610448837,-0.08882343769073486,-0.12961089611053467,0.10797084122896194,-0.14936871826648712,-0.033338963985443115,0.11759535223245621,-0.11118640750646591,-0.16549569368362427,-0.22307536005973816,0.10256917774677277,0.3594265282154083,0.19990438222885132,-0.1451532542705536,0.036446064710617065,-0.1241573691368103,-0.01249675266444683,0.025015369057655334,0.023310799151659012,-0.06697430461645126,0.016636021435260773,-0.09167926013469696,0.05546090751886368,0.1520289182662964,-0.07664483040571213,0.02443789131939411,0.18324145674705505,-0.05131388455629349,0.03371473029255867,-0.0032856762409210205,0.01570814847946167,-0.08626414090394974,0.053548894822597504,-0.06702996790409088,0.01747414842247963,0.12703940272331238,-0.1353486031293869,0.027928803116083145,0.06764037907123566,-0.17105481028556824,0.10737797617912292,-0.004590324591845274,-0.020738571882247925,0.03764114901423454,0.005512760952115059,-0.06117083877325058,-0.036734696477651596,0.23441985249519348,-0.26262736320495605,0.24768288433551788,0.23473739624023438,0.03640888258814812,0.14327383041381836,0.039577461779117584,0.1486268937587738,-0.04878677800297737,-0.06605279445648193,-0.1222165897488594,-0.028063280507922173,0.012238509021699429,-0.014763789251446724,-0.04189039394259453,-0.003825186751782894]How can 2 different images of two different persons have the EXACT same descriptor ?!? Here's my code(i'm using electron so i need to monkey patch it otherwise i get an error on the 'new Canvas()' constructor)```var path = 'js/face-api.js/models',await faceapi.loadSsdMobilenetv1Model(path)await faceapi.loadFaceLandmarkModel(path)await faceapi.loadFaceRecognitionModel(path)faceapi.env.monkeyPatch({\tCanvas: HTMLCanvasElement,\tImage: HTMLImageElement,\tImageData: ImageData,\tVideo: HTMLVideoElement,\tcreateCanvasElement: () => document.createElement('canvas'),\tcreateImageElement: () => document.createElement('img')}),let img = new Image(),async function loadImg() {      const faces = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors(),      if(faces.length){          faces.forEach(function(face, i){              console.log(face.descriptor)         }),      }}img.onload = loadImg,img.src = thumbAbsPath + '?t=' + Date.now(),```====="", ""It is hard to say what's going wrong without knowing your inputs. Can you share the images you are facing the issue with (reference and query image).====="", '> Since both distances are 0, odds are, that you are comparing the exact same image against eachother.>Both image are not same. And I have notice that this three face description has value(Float32Array) exactly same. How? my codehttps://github.com/Rohit-B-Kadam/Memento/tree/master/src/app/face-detectImagehttps://github.com/Rohit-B-Kadam/Memento/tree/master/Dataone warning i have seen  tf-core.esm.js:17 performance warning: READ-usage buffer was read back without waiting on a fence. This caused a graphics pipeline stall.=====', '@justadudewhohacks i created a repository here: https://github.com/phoenixsue/face-api-issuesI included 4 images, all of which produce the infamous descriptor i showed above. Maybe this descriptor is used for initializing the array but then something happens and  instead of returning an error that\'s what we get. As for the code, i\' m using electron 4.1.0. I downloaded the zip file of your library from this repository and included in the index.html file like this: <script type=""text/javascript"" src=""js/face-api.js/dist/face-api.js""></script>The rest of the code is shown in my previous comment.I also checked Rohit-B-Kadam image(refImage.jpg) and yes, it produces the same infamous descriptor.Try to Please try to solve this problem otherwise your library is unusable=====', ""Hmmm, i tried the same images in a simple web page with only your library loaded,  no server, no electron, no nothing and i got what seems the right descriptors.  So i think that the problem is with electron. I' m confused...====="", '@Rohit-B-Kadam @phoenixsue, the images you have shared work fine on a web application. Seems to be related to electron then. A similar issue with electron has been reported in #262. Which backend are you guys using on the web (webgl?, cpu?) and what backend in the electron app?@phoenixsue thanks for setting up an example. I will check it out on the weekend. If we figure out the ops, that are causing different results between web and electron we can certainly file this issue at tfjs.=====', '""Which backend are you guys using on the web (webgl?, cpu?) and what backend in the electron app?""I don\' t understand the question(i\'m an amateur programmer, you know). What do you mean by backend?Can you explain please?=====', 'You can check `faceapi.tf.getBackend()`, which will tell you whether the cpu or webgl backend is used. The webgl backend runs the ops on your gpu.=====', 'it says webgl. I have a good GPU(gtx 1060 3GB), so i think its good=====', ""@phoenixsue I tried out your example repo and indeed there is an issue with utilizing the webgl backend in electrons renderer thread. If I run the example on the CPU, e.g. by calling `faceapi.tf.setBackend('cpu')` the descriptors are calculated as expected.Now this is kind of a hairy situation. If we are going to file an issue at tfjs for that, we might have to figure out the exact cause of the issue, e.g. which ops are causing the divergence between the browser webgl backend and the webgl backend of the electron renderer thread. One approach would be to compare the outputs of each layer of the face recognition net. Since the face-recognition net is the only net utilizing regular convolutions, the issue might maybe reside in tf.conv2d.====="", ""@Vincent Mühler Thank you for your effort. I checked too and the descriptors seem to be the right ones BUT it' s painfully SLOW on the CPU :((. I don' t know how to help you. I understand nothing about the details and machine learning in general. Why don' t you just show them the example you have worked with and tell them what the problem is? They could fix it in no time. No?====="", 'i am using face-api for live browser recognisation using mtcnn,i follow the instruction in mtcnn documentation but when i try to run i got this error  Uncaught (in promise) TypeError: faceapi.drawDetection is not a function    at onPlay (script2.js:37)=====', '<!DOCTYPE html><html lang=""en""><head>  <meta charset=""UTF-8"">  <title>FaceDetect</title>    <script defer src=""face-api.min.js""></script>  <script defer src=""script2.js""></script>   <!-- <script src=""./js/commons.js""></script> -->  <!-- <script src=""./js/faceDetectionControls.js""></script> -->  <style>    body {      margin: 0,      padding: 0,      width: 100vw,      height: 100vh,      display: flex,      justify-content: center,      align-items: center,      background-color: azure,    }      </style></head><body>  <div style=""position: relative"" class=""margin"">    <video onplay=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>    <canvas id=""overlay"" />  </div></body></html>this my HTML=====', '[index.txt](https://github.com/justadudewhohacks/face-api.js/files/3455241/index.txt) this is html =====', '[script.txt](https://github.com/justadudewhohacks/face-api.js/files/3455247/script.txt)this is the script file=====', 'any one please help with the solution=====', 'Maybe in electron is best to use the main thread to use TF and not the render thread.In main thread I think we can use GPU backend with an Nvidia GPU.What do you think @justadudewhohacks ?=====', 'Hi, I am experiencing an issue. When I Downloaded the files and ran it I got a blank white screen pls help=====', '@ImposibleScience  ""Hi ! Can you help me about a problem I give no information about ?""If you want help, open a new issue first. How is your problem related to current issue in any way ??Then, consider making a REAL description of your problem.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/261;PRNet;2;open;2019-03-31T19:24:40Z;2019-04-11T12:07:08Z;Hi, any plans for https://github.com/YadiraF/PRNet ?Thanks.;['No specific plans, but looks quite interesting. Thanks for sharing!=====', 'Hi @justadudewhohacks. Another interesting solution with trained model: https://github.com/Microsoft/Deep3DFaceReconstruction=====']
https://github.com/justadudewhohacks/face-api.js/issues/260;Use withFaceLandmarks() and withFaceExpressions() on same code. ;5;open;2019-03-29T21:29:49Z;2019-06-17T15:58:47Z;Hi,I'm trying to add this line on the code webcamFaceExpressionRecognition.html:const result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks()but I get every time an error saying that I should load the module. Could you help me please.tanks, Jayme ;"[""Can you give me more information?Have you loaded the models?Can you give a screenshot or error message?I understand you haven't done the preloads.[Loading the models](https://github.com/justadudewhohacks/face-api.js#loading-the-models)##### (for TinyFaceDetector) Try this:```jsawait faceapi.nets.tinyFaceDetector.load('/models'),await faceapi.loadFaceRecognitionModel('/models'),await faceapi.loadFaceLandmarkModel('/models'),```====="", 'Hi,this is my error:![Captura de tela de 2019-04-01 12-57-22](https://user-images.githubusercontent.com/25558078/55341945-ff287f00-547d-11e9-9d92-decbeeebfb9c.png)The only change I made on the code was adding:const result2 = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks()=====', ""OK, i got it. You didn't load the landmarks.#### Import this files:`face_landmark_68_model-weights_manifest.json``face_landmark_68_model-shard1`#### And add this code:```jsawait faceapi.loadFaceLandmarkModel('/models/')```====="", ""> OK, i got it. You didn't load the landmarks.> > #### Import this files:> `face_landmark_68_model-weights_manifest.json`> `face_landmark_68_model-shard1`> > #### And add this code:> ```js> await faceapi.loadFaceLandmarkModel('/models/')> ```Hi, where can I see those files? Thank you.====="", '[Here it is](https://github.com/justadudewhohacks/face-api.js/tree/master/weights)=====']"
https://github.com/justadudewhohacks/face-api.js/issues/252;Error: TypeError: trackerFn is not a function at new e (tf-core.esm.js:17);4;open;2019-03-24T17:42:48Z;2019-12-28T18:30:09Z;Hi, I'm getting the following error when I tried to load a loadFaceLandmarkModel. I don't believe that is an error that is directly associated with face-api.js but an incompatibility of packages versions. I already created a question on stackoverflow to seek help for the comunity:https://stackoverflow.com/questions/55326589/error-typeerror-trackerfn-is-not-a-function-at-new-e-tf-core-esm-js17Can you please help me solve this problem?Thank you!;"["">  I don't believe that is an error that is directly associated with face-api.js but an incompatibility of packages versions.That's my guess as well. Which version of tfjs-core are you using?====="", 'I have a similar problem! I have used tfjs version >1.0.0 =====', 'same problem here=====', 'I was having the same problem. I resolved by inverting the package installation order: npm i @tensorflow/tfjs-node canvas face-api.jsThus, the version of tensorflow-core installed inside face-api is now 1.4.0. Otherwise version 1.2.9 had been installed.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/237;face_recognition_modal-shared1 error on iis;1;open;2019-03-05T17:53:03Z;2019-03-18T17:11:39Z;i'm using iis as server when using the library, face_recognition_modal-shared1 not recognized by the server since it hasn't any extensionany solution how to fix it ?thank you in advance;['You could try to rename the files and give them a custom file extension. But then you also have to include the extensions in the shard list of the corresponding weight manifest json file.=====']
https://github.com/justadudewhohacks/face-api.js/issues/235;Errors encountered: Realtime Tracking and Face Recognition using face-api.js;3;open;2019-03-04T01:48:51Z;2019-05-06T11:01:30Z;Hi, Can the admin / anyone here advise on how to go about applying Real-time Webcam Face Recognition in face-api.js?I tried out the examples based on the July 2018 article that was published by Vincent Mühler. However, I received errors.The link to where the article was found is as follows:https://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740Thanks in advance.;"['No errors in[ my demos](https://www.suijunqiang.top:3003/webcam_face_landmark_detection).If you want to use the API of ""Webcam face detection"", then you have to use https protocols on your web site.=====', 'I have the same issue, tried follow the steps from the article, but get errors. Does anybody have a full working example in clean HTML + JS? =====', '@AndreasRefThe function of “Webcam face detection” is based on the https， so you have to set your site to be https protocols first，then try the function again， you may try on my demo site above.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/232;how to add new Face expression ;3;open;2019-03-01T06:00:16Z;2019-03-23T11:08:41Z;How to add new face expressions like bored, tense etc. I added those expressions and add its images but i could not able to count its probabilities. It only takes first 7 expressions so how to increase those size ,too? What changes are needed and in which file ?![screenshot from 2019-02-28 18-45-13](https://user-images.githubusercontent.com/17745049/53619417-57692880-3c15-11e9-928a-5e53438717f4.png);"[""It's not possible to extend the current model like that. If you want to to recognize expressions, other than the 7 expressions I trained the model on, you will have to train your own model.====="", 'Can you guide me how to train my own model? which thing i need to modified in which file? you can contact me on samyak2083@gmail.com ,too.=====', ""Unfortunately it is not as easy as modifying some files. I have some messy training code in [this](https://github.com/justadudewhohacks/inflatable-unicorns/tree/master/train/faceExpressions) repo if that's helping you. If you want to train your own model, you can use the model architecture of this repo, but you will have to come up with own code for training.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/231;Saving and Loading Descriptors for future use;9;open;2019-03-01T03:34:44Z;2021-04-05T20:52:29Z;With ref https://github.com/justadudewhohacks/face-api.js#face-recognition-by-matching-descriptorsI've managed to train the faces and and saving the dataset with `JSON.stringify(labelledDescriptors)` to a static JSON.Is there a way to quickly load this dataset, or do I have to load the raw JSON and reinit the dataset with each `new faceapi.LabeledFaceDescriptors(name, descriptors)`?Is there method such as ```const labelledDescriptors = await faceapi.fetchDescriptors('/files/labeledDescriptors.json'),const faceMatcher = new faceapi.FaceMatcher(labelledDescriptors),```Pardon, if I didn't explain myself well. But the objective is to quickly loaded a saved labeledDescriptors for usage.;"['I see what you mean, currently there is no way other than what you suggested. The FaceMatcher class could be extended with a toJSON method for serializing and a fromJSON method for deserializing the state. Contributions are highly appreciated. :)=====', ""This is not the bestway but it's working. Set faceMatcher as global variable then you can call faceMatcher.findBestMatch(newdata) to compare new data with your saved dataset.```// Global variablevar faceMatcher,// Create Face Matcherasync function createFaceMatcher(data) {  const labeledFaceDescriptors = await Promise.all(data._labeledDescriptors.map(className => {    const descriptors = [],    for (var i = 0, i < className._descriptors.length, i++) {      descriptors.push(className._descriptors[i]),    }    return new faceapi.LabeledFaceDescriptors(className._label, descriptors),  }))  return new faceapi.FaceMatcher(labeledFaceDescriptors),}// Load json to backend  fs.readFile('demo.json', async function(err, data) {    if (err) {      console.log(err),    }    var content = JSON.parse(data),    for (var x = 0, x < content['_labeledDescriptors'].length, x++) {      for (var y = 0, y < content['_labeledDescriptors'][x]['_descriptors'].length, y++) {        var results = Object.values(content['_labeledDescriptors'][x]['_descriptors'][y]),        content._labeledDescriptors[x]._descriptors[y] = new Float32Array(results),      }    }    faceMatcher = await createFaceMatcher(content),  }),```====="", 'I will try to create it, but I am thinking to serialize the resultsRef : **resultsRef** = await faceapi.detectAllFaces(referenceImage, faceDetectionOptions)So we can detect faces in a batch process, store the information in a database (as json) and after recreate the objects from the information stored to run a specific comparation and so on.=====', ""It is working for the descriptor object.- descriptor - OK- detection- landmarks- unshiftedLandMarks- alignedRectcode I used to test the serialization to JSon and back` const jsonStr = JSON.stringify(resultsRef.map(res => res.descriptor )) fs.writeFileSync('./descriptor.json',jsonStr) const str = fs.readFileSync('./descriptor.json') let  obj = new Array(Object.values(JSON.parse(str.toString()))) let  arrayDescriptor = new Array(obj[0].length) let i = 0 obj[0].forEach(function(entry) {  arrayDescriptor[i++] = new Float32Array(Object.values(entry)) }), //const faceMatcher = new faceapi.FaceMatcher(resultsRef)const faceMatcher = new faceapi.FaceMatcher(arrayDescriptor))`====="", 'I also got it working with thishttps://gist.github.com/jonathanlurie/04fa6343e64f750d03072ac92584b5df=====', '  const labeledFaceDescriptors = await loadLabeledImages()  var json_str = ""{\\""parent\\"":"" + JSON.stringify(labeledFaceDescriptors) + ""}"" // save the json_str to json file    // Load json file and parse  var content = JSON.parse(json_str)  for (var x = 0, x < Object.keys(content.parent).length, x++) {    for (var y = 0, y < Object.keys(content.parent[x]._descriptors).length, y++) {      var results = Object.values(content.parent[x]._descriptors[y]),      content.parent[x]._descriptors[y] = new Float32Array(results),    }  }  const faceMatcher = await createFaceMatcher(content),function loadLabeledImages() {  const labels = [\'Black Widow\', \'Captain America\', \'Captain Marvel\', \'Hawkeye\', \'Jim Rhodes\', \'Thor\', \'Tony Stark\']  return Promise.all(    labels.map(async label => {      const descriptions = []      for (let i = 1, i <= 2, i++) {        const img = await faceapi.fetchImage(`https://raw.githubusercontent.com/WebDevSimplified/Face-Recognition-JavaScript/master/labeled_images/${label}/${i}.jpg`)        const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        descriptions.push(detections.descriptor)      }      return new faceapi.LabeledFaceDescriptors(label, descriptions)    })  )}// Create Face Matcherasync function createFaceMatcher(data) {    const labeledFaceDescriptors = await Promise.all(data.parent.map(className => {      const descriptors = [],      for (var i = 0, i < className._descriptors.length, i++) {        descriptors.push(className._descriptors[i]),      }      return new faceapi.LabeledFaceDescriptors(className._label, descriptors),    }))    return new faceapi.FaceMatcher(labeledFaceDescriptors),}=====', 'Check out #397.I added `FaceMatcher.fromJSON()` / `.fromPOJO()` and `LabeledFaceDescriptors.fromJSON()` / `.fromPOJO()`.`.fromJSON()` implementations take a JSON string and return a fully instantiated object.`.fromPOJO()` implementations take a Plain Old JavaScript Object and return a fully instantiated object.=====', '```// Parsing matchers to jsonfor(let key of Object.keys(updatedFile)) {      let matcher = updatedFile[key].matcher as FaceMatcher,      updatedFile[key].matcher = matcher.toJSON(),}fs.writeFileSync(this.paths.facesMatchers, JSON.stringify(updatedFile)),```This code throwing  me that exception: ```Unhandled Promise rejection: matcher.toJSON is not a function , Zone: <root> , Task: Promise.then , Value: TypeError: matcher.toJSON is not a function```Someone can explain to me why ?=====', 'why not save it to a file as raw text?I use react so I cant (simply) save files to my disk, so I found a bit nasty walkaround: instead of using JSON, you could use indexeddb to upload all the descriptors as they are, and then load them directly as an array, and just use:const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors),and if you really need to get the descriptors on a file: - a bit old but a way to access indexeddb local database: https://www.aaron-powell.com/posts/2012-10-05-indexeddb-storage/=====']"
https://github.com/justadudewhohacks/face-api.js/issues/227;Q: How to serialize/unserialize FullFaceDescriptions?;1;open;2019-02-27T05:04:06Z;2019-03-01T08:40:16Z;Detecting faces is great, but can be slow.  I'd like to save the detections to an IndexDB and do some math on them when all are completed.Is there a standard/easy way to do this?  I've got a Dexie hacky way with```db.addFace = async function(imageName, tileLocation, ffd) {  return await db.faces.add({    'image_name':imageName,    'tile_x':tileLocation.x,    'tile_y':tileLocation.y,    'x':ffd.detection.box.x,    'y':ffd.detection.box.y,    'w':ffd.detection.box.width,    'h':ffd.detection.box.height,    'score':ffd.detection.score,    'descriptor':ffd.descriptor,    'landmarks': ffd.landmarks,    'expressions': ffd.expressions  }),},```that I'm sure is a bad idea.;"[""It's not necessarely a bad idea, if you deserialize them correctly. Currently there is no easy way to do it, but I am happy to take PRs for implementing toJSON, fromJSON methods for all the helper classes.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/226;Performance issue on the firefox at low laptop of ThinkPad X201. ;1;open;2019-02-26T00:56:14Z;2019-03-01T08:45:46Z;"There is no landmark trace on the face when testing ""Webcam Face Landmark Detection"" on the laptop of ThinkPad X201 as well as ""Video Face Tracking"" and the video play is very slow.So my question is that any performance description for the project? or any hardware requirement able support face-api.js run well?Thanks & Best RegardsSui  ";['First of all, I would check whether tfjs is running correctly on your ThinkPad, e.g. whether the WebGL backend is registered correctly. Try to run some of the tfjs demos on your Pad first.=====']
https://github.com/justadudewhohacks/face-api.js/issues/225;"Error of ""no faces detected for label"" with 400 face images";2;open;2019-02-25T05:28:51Z;2019-04-11T07:59:00Z;"Hi I am trying to use tiny face model to do a face recognition api with 400 different faces. But I got ""no faces detected for labels"" error when I chose any input size larger than 128. The accuracy of the model is not very good based on the small input size. I am wondering if there is any solution to that. Thanks. Rui";"['""no faces detected for labels "" where does this Error come from?Please share some example images, so I can verify the issue. Also you can try out the SSD mobilenetv1 model for face detection.=====', 'I experienced a similar problem following the example at https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07I\'m loading the following models:      await faceapi.loadSsdMobilenetv1Model(),      await faceapi.loadFaceLandmarkModel(),      await faceapi.loadFaceRecognitionModel(),I tried the following. I take one image with multiple people. I run the face detection. I then save the images of the faces (a little bit unzoomed to see a bit more the person). Then I ran faceapi.detectSingleFace() on each of these images of a single face and it didn\'t find any face on any of the images. Which is quite interesting since those are actual faces that I found using the same library.I think the problem comes from the size of the images. The faces were extracted from a picture with many people and the resulting ""single face"" image are quite small and a bit blurry.I tried the following 2 things:  * I took one of the single faces images and used gimp to sharpen it. Then the library found a face on it.  * I used an original image with fewer faces on it so each face has a higher definition (https://parismatch.be/app/uploads/2018/10/0000307817-1100x715.jpg) and everything worked fine except for Ross face where I get the same problem, it doesn\'t find a face from the extracted face picture. I even tried to unzoom a bit to make it bigger but still not finding a face.I\'m a bit surprised that it doesn\'t find faces at all when it actually extracted from the same library. I guess the models are not the same for both functions.Thank you very much for all your work, this library is amazing!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/222;FaceLandmarks68: left and right eyes/brows are reversed.;10;open;2019-02-21T07:23:01Z;2021-01-15T06:46:18Z;Hello,`FaceLandmarks68.getLeftEye()` returns points corresponding to the right eye, and `FaceLandmarks68.getRightEye()` returns points corresponding to the left eye.Same issue for the `FaceLandmarks68.getLeftEyeBrow()` and `FaceLandmarks68.getRightEyeBrow()`.;"['Hi, thanks for reporting. I would highly appreciate PRs for fixing this. :)=====', '@a-bronx would you like to share your case so we can help?=====', 'Here is a quick test that every point of the right eye/brow has a correct horizontal position relative to the left eye/brow, given that ""right"" is closer to the origin than ""left"":```javascriptconst result = await faceLandmark68Net.detectLandmarks(imgElRect) as FaceLandmarks68expect(    Math.max(...result .getRightEyeBrow().map(_=>_.x)) <     Math.min(...result .getLeftEyeBrow().map(_=>_.x))).toBe(true),expect(    Math.max(...result .getRightEye().map(_=>_.x)) <     Math.min(...result .getLeftEye().map(_=>_.x))).toBe(true),```To fix it just rename FaceLandmarks68.getLeftXXX() to FaceLandmarks68.getRightXXX() and vice versa, it is just a labeling issue, all geometry is correct.NOTE: chances a low, but it still may be a breaking change for someone who do calculations based on landmark points.Sending you PR, but I cannot build the library and run unit tests on my Windows box without Python (and I have no interest to install it), so it will be a ""blind"" fix and it is up to you to verify and probably correct it.=====', ""That was a reason I asked to share your case because we can't swap names for one single case. There are others that were successful getting the right pointer results.====="", 'My case is a face liveness check based on certain statistic of relative movements of landmark points in a video stream. Before making the check, I validate the face geometry to ensure it is not distorted due to fast movement, glasses etc (which sometime happens), so these garbled ""Picasso-like"" frames do not affect the statistics. As part of this sanity check I verify that the right eye is correctly positioned relative to the left eye. The assumption is that in a normal (non-mirrored) photo/video frame the subjects\' right eye will be closer to the left border of the frame than the left one. But the `FaceLandmarks68` does not support this assumption, my code was not working and I had to debug to find out that method names are misleading.=====', 'So I think based on your example I see your solution is based on a non-mirrored image. Maybe to have an option or parameter passed will solve problem. Will try, test and let you know.=====', 'By default source images and source video streams come non-mirrored, so it should be a baseline. If I need to flip photo/video, I use CSS on the whole canvas (e.g. `transform: scaleX(-1),`) instead of processing the source frame, which is much cheaper, especially for a web camera video stream.But here is a catch: if I mirror the video in a browser using CSS , then I cannot use the `drawDetection()` anymore because score text under the bounded box becomes mirrored too and not readable. It is a non-issue for me as I draw a custom face boundary anyway, so just letting you know :)  But if you decide to make `drawDetection()` support drawing over a mirrored representation and flip text labels accordingly, then you need a parameter here.=====', 'Yes, Flip of the stream (mirroring) is something important (must have) when doing face recognition using a webcam. Hope we can get its support soon. =====', '> By default source images and source video streams come non-mirrored, so it should be a baseline. If I need to flip photo/video, I use CSS on the whole canvas (e.g. `transform: scaleX(-1),`) instead of processing the source frame, which is much cheaper, especially for a web camera video stream.> > But here is a catch: if I mirror the video in a browser using CSS , then I cannot use the `drawDetection()` anymore because score text under the bounded box becomes mirrored too and not readable. It is a non-issue for me as I draw a custom face boundary anyway, so just letting you know :) But if you decide to make `drawDetection()` support drawing over a mirrored representation and flip text labels accordingly, then you need a parameter here.After playing around a bit, I found a work around. First you need to create a second canvas that is not flipped. And then you need to use the customised draw function: `drawBox.draw` and `drawText.draw`.  You need to find the correct X coordinate, which is `canvas_new.width - box.x - box.width`=====', '> Yes, Flip of the stream (mirroring) is something important (must have) when doing face recognition using a webcam. Hope we can get its support soon.Just wondering if this option was ever added to the api=====']"
https://github.com/justadudewhohacks/face-api.js/issues/221;Liveness Detection;19;open;2019-02-20T21:34:56Z;2021-12-06T10:58:13Z;Hi,I am working on an internal room check-in system using this API. The detection speed and accuracy looks great. But there is a problem that people can use pictures on their phone or printed paper to unlock the door. I am wondering if there is any liveness detection (anti-spoofing) model we can integrate to make this API better. Thanks Rio;"['Also interested in liveness detection.One aspect of that could be blink detection, see #176.=====', 'Also interested! Something like that!http://cvlab.cse.msu.edu/project-face-anti.html=====', 'An article on one type of liveness detection using a convolutional neural network: https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/=====', 'Also Interested! =====', 'I would love to see Vincent @justadudewhohacks implements some kind like this https://github.com/richmondu/libfaceid=====', 'This is likely my #1 request for this fantastic library. =====', 'I have converted liveness keras model from [https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/](https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/) into tensorflowjs json model. anyone know how can i use this converted model in this library?![live](https://user-images.githubusercontent.com/8829689/66623410-1e1e7f80-ec1e-11e9-8861-2a29b3f6c405.JPG)=====', 'New open source solution: https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019[international download link model](https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/issues/52#issuecomment-527536144)[Testing code](https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/issues/72#issuecomment-550123281)Output:```./images/fake.jpg: FAKE!./images/real.jpg: REAL```=====', '> An article on one type of liveness detection using a convolutional neural network: https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/I tried that but the accuracy of the model is very poor.=====', ""Landmarks, specifically for eyes, changes in difference of distance between upper eye points and lower eyes points We tried to use the same for fatigue detection but sadly eyes closed, it wasn't perfoming well for Indians faces so had to move to a different solution. Summarizing, record a video run face-api notice how distances are changing, disproportionately compared to other landmarks, they are alive if not it's something static which may be moving around as a whole ====="", ""Hello there,Just wanted to suggest a direction on how to do this.There's a company which I won't disclose the name, who does the liveness detection based on the realness of the skin of the person.While capturing the video input, several screens of different colors are quickly displayed on the screen. The algorithm analyses the reflection of light on the skin and determines if it's real or not.Chey====="", 'I want code example for checking face liveness.=====', 'Hopefully this repo has an additional guide to detecting fake faces=====', '@ariffromeo, sorry, did you manage to incorporate the models?=====', 'excellent feature request. would love to have this.=====', '+100 on this request!=====', 'Please check following repo:https://github.com/Nikit333/Face-Liveness-Detection=====', 'I think this code will work.https://gist.github.com/kleysonr/d75494f239ad0dce561a55a624920693=====', 'I have a problem like this too. Please fix it 😍😍 .=====']"
https://github.com/justadudewhohacks/face-api.js/issues/218;Correct definition of borders and placement of landmarks for persons turned into profile.;3;open;2019-02-19T12:35:22Z;2019-03-23T11:13:08Z;Great work, but there is a need to define accurate image boundaries for profiles.;"['Can you try to rephrase your question please? I don\'t understand what you mean by ""image boundaries for profiles"".=====', 'Hi, I meant correctly defining the face boundaries in a profile (an example of use is to automatically cut face for documents)=====', 'Not sure what this means:> correctly defining the face boundaries The API gives you the bounding boxes of detected faces. You can adjust them as needed.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/215;improve Face Recognition accurary and stabilization in mtcnn from webcam;6;open;2019-02-15T05:44:21Z;2019-03-19T07:09:55Z;Hello Sir,first of all, congratulations for this wonderful API. you have done great work. using this (mtcnnFaceRecognitionWebcam.html) i can recognize face. but when i want to recognize multiple faces from the webcam i did not get the correct name on correct person. it shows other person name on other face.(e.g if i try to recognize two faces and first person is sachin and second person is rahul) than it shows rahul on sachin face and some time unknown. so i want to increase the accuracy to recognize the face it should label correct name on correct person as well as stability of bounding box (the bounding box should not be blink). please help me to increase the accuracy. i shall be very grateful to you.Thank You.;"[""Can you share an example clip or atleast some images illustrating the problem? Otherwise it will be hard to identify the actual problem.Also, I wouldn't recommend using the mtcnn anymore, instead I would use the TinyFaceDetector.====="", '!![first](https://user-images.githubusercontent.com/36258882/52846273-35f54080-312f-11e9-9fdd-09462cab3dad.png)![second](https://user-images.githubusercontent.com/36258882/52846276-368dd700-312f-11e9-8533-e3449c599d0d.png)![third](https://user-images.githubusercontent.com/36258882/52846279-368dd700-312f-11e9-8122-deffbeb635b8.png)![images](https://user-images.githubusercontent.com/36258882/52848564-13662600-3135-11e9-9b55-274da1455a43.gif)Dear sir,i have attached three images and gif file which will describe the problem that i am facing. in first.png name is correct with person face while in second.png name is not correct with same person face.i want it should display the correct name with correct person,and in third.png the bounding box not visible at all. the bounding box also is not stable means some time it visible and some it is not visible. i want it should be all time visible till it recognize face. **this is sachin face not amir.** but it shows amir some time with that face.sorry for my bad english. i hope you will understand my problem. can we set the distance for each person for recognize face.(e.g distance<=0.3 then it is sachin, if distance >0.3 then it is amir and so on..)please help me sir, i am waiting for your help.Thank You.=====', ""What's the size of the face rectangle and what type of GPU are you using (Intel?), might be a precision issue, because it's kind of suspicious, that all your matches are returning a distance lower than 0,6. Usually you want to use 0,6 as a threshold.====="", 'I have met seem issue [here](https://github.com/justadudewhohacks/face-api.js/issues/241).@sachinrke have you fixed it?@justadudewhohacks  I am not sure that  if I am using face-api.js in right way? so would you please give me a guid about how to recognize specific face in example-browser?Thanks & Best RegardsSui =====', 'You should read the introduction [article](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07). I tried to explain every single step there.=====', ""@justadudewhohacks Thanks for your response.It's my fault, now the matching accuracy has been risen to be 95% and more.Thanks again.Sui=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/213;Suggestion: No Yolo for single face detection;8;open;2019-02-10T21:37:27Z;2020-11-15T09:24:40Z;If we are doing single face detection, then we might need not yolo as we only need to predict one bounding box and we can just traditional cnn i.e. resnet or something and just add a regression layer at the end. I feel we can get huge speed performance improvements.Thanks,Rohan;"['Not sure if the performance improvements will be that huge to be honest, but you are right, in case of single face detection you do not need any yolo or ssd like cnn. The TinyFaceDetector is already pretty lightweight and fast so training such a single face detector is not that high on my priority list, as there is still a lot of other work remaining for this repo. I would be interested, if you wanted to try out training a simple cnn for single face detection and compare the performance to the other face detectors.=====', ""If you don't mind to use thrid party library then I have a suggestion that you can use Chrome Face Detection API https://medium.com/@joomiguelcunha/lets-play-with-chrome-s-face-detection-api-ca13017a958f then feed the extracted face into face-api.====="", 'I think there is no need to embedd the Chrome Face Detection API into face-api.js. If you really want to use this API over the face detectors in this package, then let it detect the rectangle and feed them to face-api.js.=====', 'Hi, I was wondering where (which files) the architecture for the tiny yolo v3. I want to run it in my setup (ipython notebook) to compare to my architecture.  =====', 'Hmm there is no tiny yolo v3 model in this repo. Which model are you referring to exactly?=====', 'Sorry I meant Tiny Face Detector which it was based on tiny yolo v2 according to the readme=====', 'Ahh ok, you can find the implementation [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/tinyYolov2/TinyYolov2.ts).=====', '@rohanmahajan1993 Did you manage to use the tiny face detector from Python?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/212;Loading the Models - Error;12;open;2019-02-08T19:14:30Z;2019-07-19T05:21:21Z;"I have been using the cordova face-api project together, when I'm loading the MODELS, I come across the following errors:CODE - 1:net.load(await faceapi.fetchNetWeights('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json'))ERROR - 1Fetch API cannot load file:///android_asset/www/js/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme ""file"" is not supported.i tryCODE - 2:await faceapi.nets.ssdMobilenetv1.loadFromDisk('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json'),ERROR - 2,Error: readFile - filesystem not available for browser environmenti tryCODE - 3:const res = await axios.get('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json', { responseType: 'arraybuffer' }),		const weights = new Float32Array(res.data),net.load(weights),ERROR - 3,Error: Based on the provided shape, [1,1,64,128], and dtype float32, the tensor should have 8192 values but has 2381    at assert (face-api.js:23)";"['Hey not really familar with cordova, but I am assuming there is no difference in the webview and a browser in the following I am going to state:> ERROR - 1Fetch API cannot load file:///android_asset/www/js/models/ssd_mobilenetv1_model-weights_manifest.json.URL scheme ""file"" is not supported.A client application in a browser can not access your file system, thus using the fetch API with local files doesn\'t work. The same goes for XHR/axios (CODE 3).> ERROR - 2,Error: readFile - filesystem not available for browser environmentUsing loadFromDisk only works in a nodejs environment and not in the browser.To conclude:If you want to load the model from disk, checkout how to read files from the webview with cordova and monkeyPatch readFile as I pointed out [here](https://github.com/justadudewhohacks/face-api.js/issues/153#issuecomment-443508739).=====', 'Good afternoon,I\'m trying to use the ""faceapi.env.monkeyPatch"" but gave me the error below could tell me what I\'m doing wrong. so I understand you\'re not loading the local filescodefaceapi.env.monkeyPatch({\t\t\tcreateCanvasElement: () => document.getElementById(\'myCanvas\'),\t\t\tcreateImageElement: () => document.getElementById(\'imageBase2\'),\t\t\treadFile: () => fs.readFile(\'./home/guilherme/Documentos/modelos/\')\t\t})const input = document.getElementById(\'imageBase2\'),\t\t\t\t\t\tlet fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors(),Error:Error: SsdMobilenetv1 - load model before inference at SsdMobilenetv1.forwardInput (face-api.js:4308)=====', 'You probably forgot to load the model `await faceapi.nets.ssdMobilenetv1.loadFromDisk(filePath)`. Also you probably want to monkey patch it like this readFile: (filePath) => fs.readFile(filePath)=====', '@guilhermefurtado23 did you solve it? If so, can you share the solution?=====', 'Anyone achieved to get it working on cordova or ionic?I have tried httpd plugin on ionic to start a server inside the device an serve files but getting also getting ""not allowed by Access-Control-Allow-Origin"".This api is great and It will be cool to load the models from local file system.=====', '> Anyone achieved to get it working on cordova or ionic?> I have tried httpd plugin on ionic to start a server inside the device an serve files but getting also getting ""not allowed by Access-Control-Allow-Origin"".> This api is great and It will be cool to load the models from local file system.I DID THESAME AND I AM GETTING THIS ERROR =====', ""IN MY OWN CASE I AM USING THE BROWSER AND AFTER THE INITIAL ERROR I HAD TO MOVE THE MODELS TO A SERVER SO I COULD LOAD THEM FROM THERE ONLY TO GET ANOTHER ERRORAccess to fetch at 'http:BTLINK/models/tiny_face_detector_model-weights_manifest.json' from origin 'null' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.====="", 'Hi paaber,if you can use an externar server you could try to create a .htacces file with this content:Header set Access-Control-Allow-Origin ""*""Header set Access-Control-Allow-Methods ""GET,PUT,POST,DELETE""Header set Access-Control-Allow-Headers ""Content-Type, Authorization""Next, upload the file to the same folder you are trying to download the models .I made it with the more basic ionos.com hosting and it works.I hope this helps you.=====', 'thank you for the reply, i just did as you\'ve instructed but i still getthe same error message i will keep on trying hopefully i get it working soonOn Tue, 11 Jun 2019 at 11:47, dymdev <notifications@github.com> wrote:> Hi paaber,> if you can use an externar server you could try to create a .htacces file> with this content:>> Header set Access-Control-Allow-Origin ""*""> Header set Access-Control-Allow-Methods ""GET,PUT,POST,DELETE""> Header set Access-Control-Allow-Headers ""Content-Type, Authorization"">> Next, upload the file to the same folder you are trying to download the> models .> I made it with the more basic ionos.com hosting and it works.>> I hope this helps you.>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/212?email_source=notifications&email_token=AMEGI5VFQK53PUVPVM673L3PZ57DFA5CNFSM4GWEVDCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXMW3UI#issuecomment-500788689>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AMEGI5VVDJ42GPMHWUDPMF3PZ57DFANCNFSM4GWEVDCA>> .>=====', 'Hi @dymdev, @guilhermefurtado23, @justadudewhohacks After many hours, get to run in Cordova with Android, it is worth remembering that I have no problems in the iOS, Electron and Browser.I posted the solution in https://stackoverflow.com/questions/56544635/using-face-api-js-in-cordova-with-android/56544636#56544636=====', 'hi dymdev so I finally got it to work after following your fix all I had to do was to change the http in the URL to https and my models are loading thanks =====', ""  async detectFace(){       console.log('in 1'),      const imageUpload = document.getElementById('imageUpload') as HTMLImageElement,      faceapi.env.monkeyPatch({        readFile: () => fs.readFile(MODEL_URL)        })      Promise.all([        await  faceapi.loadFaceRecognitionModel(MODEL_URL),        await faceapi.loadFaceLandmarkModel(MODEL_URL),        await faceapi.loadSsdMobilenetv1Model(MODEL_URL)      ]).then(start)      async function start() {}can somebody help me how to write this.i am using it ionic 3.@justadudewhohacks =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/210;Error running node example;6;open;2019-02-03T10:33:50Z;2020-08-14T23:12:00Z;"I followed the directions on the README and I get the following error:```npm ERR! path /Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/face-api.js-8127d704/node_modules/@babel/code-framenpm ERR! code ENOENTnpm ERR! errno -2npm ERR! syscall renamenpm ERR! enoent ENOENT: no such file or directory, rename '/Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/face-api.js-8127d704/node_modules/@babel/code-frame' -> '/Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/@babel/code-frame-dce0a722'npm ERR! enoent This is related to npm not being able to find a file.npm ERR! enoent npm ERR! A complete log of this run can be found in:npm ERR!     /Users/codydaig/.npm/_logs/2019-02-03T10_27_19_451Z-debug.log```I tried changing the version in package.json of ""@tensorflow/tfjs-node"" to ""0.1.21"" and performing a fresh install and get the same error. I've tried this on two different computers. Primary computer:Mac OS X 10.14.2. Node v 11.1.0npm v6.5.0Any suggestions? Thanks in advance!";"['Hi, @codydaig!I got the same error.To fix this I have just changed version of **tensorflow/tfjs-node** to 0.1.9:`""@tensorflow/tfjs-node"": ""^0.1.9"",`=====', 'Try cleaning your node_modules and reinstall them.=====', 'I’ve tried both of those several times and same error. =====', ""@justadudewhohacks Circling back to this. I've tried multiple times on multiple different computers, operating systems, etc... and am unable to get any of the examples to run. Any suggestions?====="", 'Same error here on Windows or Linux=====', 'I was having this issue. I rolled back to 0.1.9 as @NikolayYakovenko suggested, deleted my node_modules folder and package-lock, reinstalled with npm install, and restarted my development server- and this fixed it.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/197;TypeError: Illegal constructor.;9;open;2019-01-21T15:32:05Z;2021-12-06T10:46:08Z;"Hi, I get the following error:> TypeError: Illegal constructor.when I do the following:`const detection = await faceapi.detectSingleFace(video),`video is basically an HTML element (document.getElementById(""video-stream""))I also tried passing an id to the faceapi.detectSingleFace, same error.I am using **node v10.15.0**, webpack, babel and React.I had an error about tensorflow saying that fs module could not be resolved, but I guess this is not related. Just in case...Here is the full code:```javascriptasync function detectFaces(video, canvas, width, height) {	window.logger.log(""detectFaces""),	const detection = await faceapi.detectSingleFace(video),	console.log(detection),}function loadVideo(video, canvas, width, height) {	window.logger.log(""loadVideo""),	navigator.mediaDevices.getUserMedia({		video: {			width : width,			height: height		}	}).then(function(stream) {		video.srcObject = stream,		video.play(),		video.onloadedmetadata = function() {			detectFaces(video, canvas, width, height),		}	}).catch(function(error) {		console.log(error),	}),}```Thank you.";"['Hmm, can you post the full stacktrace please.=====', 'Hi , could you tell me how you solve the problem ,pleaseeI have the same error=====', 'same error here in an electron-vue app:I\'m using the import statement `import * as faceapi from ""face-api.js"",`then ```await faceapi.loadSsdMobilenetv1Model(""static/models""),let detections = await faceapi.detectAllFaces(this.images[i]),```stacktrace:```C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10 Uncaught (in promise) TypeError: Illegal constructor    at createCanvasElement (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10)    at createCanvas (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:10)    at Object.createCanvasFromMedia (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:22)    at C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:37    at Array.forEach (<anonymous>)    at new NetInput (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:22)    at Object.<anonymous> (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\toNetInput.js:53)    at step (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:133)    at Object.next (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:114)    at fulfilled (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:104)createCanvasElement @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10createCanvas @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:10createCanvasFromMedia @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:22(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:37NetInput @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:22(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\toNetInput.js:53step @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:133(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:114fulfilled @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:104Promise rejected (async)step @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:106(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:107__awaiter @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:103ComposableTask.then @ C:\\Projects\\face-blur\\node_modules\\face-api.js\\build\\commonjs\\globalApi\\ComposableTask.js:8VideoPage.vue?10d6:134 Uncaught (in promise) TypeError: Cannot read property \'detectAllFaces\' of undefined    at VueComponent._callee4$ (webpack-internal:///./node_modules/babel-loader/lib/index.js!./node_modules/vue-loader/lib/index.js?!./src/renderer/components/VideoPage.vue?vue&type=script&lang=js&:202:75)    at tryCatch (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:62:40)    at Generator.invoke [as _invoke] (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:296:22)    at Generator.prototype.(anonymous function) [as next] (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:114:21)    at step (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:17:30)    at eval (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:35:14)    at new Promise (<anonymous>)    at new F (webpack-internal:///./node_modules/core-js/library/modules/_export.js:36:28)    at eval (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:14:12)    at VueComponent.detectFaces (webpack-internal:///./node_modules/babel-loader/lib/index.js!./node_modules/vue-loader/lib/index.js?!./src/renderer/components/VideoPage.vue?vue&type=script&lang=js&:223:10)_callee4$ @ VideoPage.vue?10d6:134tryCatch @ runtime.js?96cf:62invoke @ runtime.js?96cf:296prototype.(anonymous function) @ runtime.js?96cf:114step @ asyncToGenerator.js?0f75:17(anonymous) @ asyncToGenerator.js?0f75:35F @ _export.js?63b6:36(anonymous) @ asyncToGenerator.js?0f75:14detectFaces @ VideoPage.vue?10d6:134_callee$ @ VideoPage.vue?10d6:64tryCatch @ runtime.js?96cf:62invoke @ runtime.js?96cf:296prototype.(anonymous function) @ runtime.js?96cf:114step @ asyncToGenerator.js?0f75:17(anonymous) @ asyncToGenerator.js?0f75:28Promise rejected (async)step @ asyncToGenerator.js?0f75:27(anonymous) @ asyncToGenerator.js?0f75:28Promise resolved (async)step @ asyncToGenerator.js?0f75:27(anonymous) @ asyncToGenerator.js?0f75:35F @ _export.js?63b6:36(anonymous) @ asyncToGenerator.js?0f75:14(anonymous) @ VideoPage.vue?10d6:62emitTwo @ events.js:131emit @ events.js:214```seems it tries to use the nodejs version, but it should use the browser version=====', ""Yes , I put this in my code and it works:faceapi.env.monkeyPatch({    Canvas: HTMLCanvasElement,    Image: HTMLImageElement,    ImageData: ImageData,    Video: HTMLVideoElement,    createCanvasElement: () => document.createElement('canvas'),    createImageElement: () => document.createElement('img')}) ====="", ""@ciobanudan97 thanks, that solved my problem. For me the problem only occurred after monkey patching the 'fetch' method. Patching `createCanvasElement` as described above solved the issue for me.====="", 'i am facing a similar error . can\'t seem to resolve it with the above mentioned  suggestions . here\'s the code`import { Component, Input, ViewChild, ElementRef, AfterViewInit, Inject } from \'@angular/core\',import * as faceapi from \'face-api.js\',import {WebcamImage, WebcamInitError, WebcamUtil} from \'ngx-webcam\',import {Subject,Observable} from \'rxjs\',import * as canvas from \'canvas\',import * as $ from \'jquery\',import { DOCUMENT } from \'@angular/common\', faceapi.env.monkeyPatch({  Canvas: HTMLCanvasElement,  Image: HTMLImageElement,  ImageData: ImageData,  Video: HTMLVideoElement,  createCanvasElement: () => document.createElement(\'canvas\'),  createImageElement: () => document.createElement(\'img\')  })@Component({  selector: \'app-root\',  templateUrl: \'./app.component.html\',  styleUrls: [\'./app.component.css\']})export class AppComponent implements AfterViewInit {  @Input() cameraName:string = """",  @ViewChild(\'myImage\',{ static: false }) imageInput: ElementRef,  title = \'imageRecognition\', private doc:Documentconstructor(@Inject(DOCUMENT) document) {  this.doc=document,}ngOnInit() {this.loadModels(),}ngAfterViewInit() {  } async loadModels() {  // load the models  const MODEL_URL = \'./assets/models/\'  await faceapi.loadSsdMobilenetv1Model(MODEL_URL)  await faceapi.loadFaceLandmarkModel(MODEL_URL)  await faceapi.loadFaceRecognitionModel(MODEL_URL)  const input = this.imageInput.nativeElement  let fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()   }   }`=====', 'this is reason : https://medium.com/@andreas.schallwig/do-not-laugh-a-simple-ai-powered-game-3e22ad0f8166@ciobanudan97 for him answer.=====', ""@step4  Like the problems I havewe need to manually monkey patch the environment back to a browser environment.add this after import face-api.js// configure face APIfaceapi.env.monkeyPatch({  Canvas: HTMLCanvasElement,  Image: HTMLImageElement,  ImageData: ImageData,  Video: HTMLVideoElement,  createCanvasElement: () => document.createElement('canvas'),  createImageElement: () => document.createElement('img')}),and it works====="", 'You must set options for that like tinyModel or Yolyo=====']"
https://github.com/justadudewhohacks/face-api.js/issues/195;LResNet100E-IR,ArcFace@ms1m-refine-v2;5;open;2019-01-16T23:07:16Z;2021-01-31T16:24:32Z;Hi,Please check/adopt the following model: https://github.com/deepinsight/insightface/wiki/Model-ZooIn praticular LResNet100E-IR,ArcFace@ms1m-refine-v2 looks good.Thanks;"[""Hi,Any more information on this, did you actually try it out? On first sight, the model doesn't look very web friendly to me, the raw weights seem to be about 250MB, plus one would have to investigate how a resnet100 architecture performs on the web.The MobileFaceNet model looks quite feasible.Will play around with this some time, thanks for sharing!====="", 'I downloaded it, and find out it include 3 files: log, .json .params, how can I use it as a Layer in my Keras neural network=====', 'The original model is for mxnet. There’s a port to tensorflow/Keras at https://github.com/shaoanlu/face_toolbox_keras=====', 'Maybe something to try:https://github.com/deepinsight/insightface/tree/master/detection/RetinaFaceInteresting comment:Third-party Modelsyangfly: RetinaFace-MobileNet0.25 (baidu cloud:nzof). WiderFace validation mAP: Hard 82.5. (model size: 1.68Mb)=====', '> The original model is for mxnet. There’s a port to tensorflow/Keras at https://github.com/shaoanlu/face_toolbox_kerasThanks for your help.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/194;Blob is not defined fetching local images;10;open;2019-01-15T15:07:34Z;2020-08-24T16:07:41Z;"Hi, i testing face-api.js and trying loading a local image for make face similarity and is throw me ""Blob is not defined"" in my nodeJS application, here the function where i try computing descriptors:```face.env.monkeyPatch({ fetch, Blob }),const threshold = 0.6,var descriptors = { desc1: null, desc2: null },function computarDescriptor(aDesc, aUri) {  // ~statics/pics/001_tmp.jpg  face.fetchImage(aUri).then(function(result) {    console.log('fecthImage: ' + result),    descriptors['desc' + aDesc] = face.computeFaceDescriptor(result),  }).catch(err => {    console.log(err),  })}```node-fetch and blob is installed and monkeyPatched and referenceErros is still there.Thanks";"[""Just added const Blob = require('node-fetch'), before bufferToImage function located at build/commonjs/dom/bufferToImage.js and worked fine..====="", ""Ahh, currently face.env.monkeyPatch doesn't patch Blob, thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?`global.Blob = require('blob'),`====="", ""> Ahh, currently face.env.monkeyPatch doesn't patch Blob, thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> `global.Blob = require('blob'),`After i define as global i get > TypeError: Right-hand side of 'instanceof' is not an object> tfjs-image-recognition-base\\src\\dom\\bufferToImage.ts:5:14====="", ""> > Ahh, currently face.env.monkeyPatch doesn't patch Blob, thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> > `global.Blob = require('blob'),`> > After i define as global i get> > > TypeError: Right-hand side of 'instanceof' is not an object> > > tfjs-image-recognition-base\\src\\dom\\bufferToImage.ts:5:14@justadudewhohacks Hi, I'm getting the same error as well. the fetchImage function isnt working after monkey patching fetch====="", ""Hi @enecumene, @justadudewhohacks I had a similar issue a few minutes ago. The only difference is that I was trying to fetch the image from the cloud (cloudinary).I solved it by using the 'loadImage' function from the node 'canvas' package i.eimport { loadImage } from 'canvas',const referenceImage = await loadImage(refImageUrl),It might work for local images. Hope this helps====="", 'Hi! I\'m working on API REST (Node v12.9.1 and Express 4.17.1) and have the same problem ""Blob is not defined"" when trying to fetch images from URL.I have tried all suggestions in this issue comments:**1.**  `global.Blob = require(\'blob\'), `and got same as @hanjeahwan > TypeError: Right-hand side of \'instanceof\' is not an object**2.** `const referenceImage = await loadImage(refImageUrl),` as @EziamakaNV suggested, but this doesn\'t work when I try to use faceapi.detectSingleFace() because is not HTMLImageElement, even though I already monkeypatched Image with Canvas...`detection = await faceapi.detectSingleFace(referenceImage).withFaceLandmarks().withFaceDescriptor(),`> Argument of type \'Image\' is not assignable to parameter of type \'TNetInput\'. Type \'Image\' is not assignable to type \'HTMLImageElement\'.If i add //@ts-ignore , then i get:> Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element idI also tried creating new Image() like this:```const canvas = require(\'canvas\'),const img = new Image()const canvasCreated = canvas.createCanvas(200, 200)const ctx = canvasCreated.getContext(\'2d\')img.onload = () => ctx.drawImage(img, 0, 0)img.onerror = err => { throw err }img.src = URL_TO_IMAGE```Result: > UnhandledPromiseRejectionWarning: TypeError: media.addEventListener is not a functionIf you have any other suggestion I would be happy to try! Thanks in advance for your amazing work @justadudewhohacks =====', 'Same scenario with @sebacampos currently I am building rest api with node. then I encountered this. =====', ""> Ahh, currently face.env.monkeyPatch doesn't patch Blob, thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> `global.Blob = require('blob'),`I am using this library in react and getting same problem with fetchImage function, could you please help me on this?====="", 'I did this long time ago so I don\'t remember very well, but this was my working implementation: **monkeypatching fetch AND canvas classes:**```tsimport { loadImage, Canvas, Image, ImageData } from \'canvas\',// @ts-ignorefaceapi.env.monkeyPatch({ Canvas, Image, ImageData, fetch }),export const detect = async (imageUrl: string): Promise<WithFaceDescriptor<any>> => {  let canvasImage,    try {    canvasImage = await loadImage(imageUrl),        if (!canvasImage) {      throw new Error(""Canvas not loaded.""),    }  } catch (error) {     throw new Error(""Could not load image.""),  }    const faces = await detectFaces(canvasImage),     return faces[0],} const detectFaces = (image: any): any => faceapi.detectAllFaces(image).withFaceLandmarks().withFaceDescriptors(),```Hope it helps=====', 'Actually the issue is i am fetching my labeled faces , from node api and Iwant to fetch that images with fetchImage , and it\'s not working,ThanksOn Mon, 24 Aug 2020, 7:29 p.m. Sebastian Campos, <notifications@github.com>wrote:> I did this long time ago so I don\'t remember very well, but this was my> working implementation: *monkeypatching fetch AND canvas classes:*>> import { loadImage, Canvas, Image, ImageData } from \'canvas\',> // @ts-ignorefaceapi.env.monkeyPatch({ Canvas, Image, ImageData, fetch }),>> export const detect = async (imageUrl: string): Promise<WithFaceDescriptor<any>> => {>   let canvasImage,>>   try {>     canvasImage = await loadImage(imageUrl),>>     if (!canvasImage) {>       throw new Error(""Canvas not loaded.""),>     }>   } catch (error) {>      throw new Error(""Could not load image.""),>   }>>   const faces = await detectFaces(canvasImage),>>   return faces[0],}> const detectFaces = (image: any): any => faceapi.detectAllFaces(image).withFaceLandmarks().withFaceDescriptors(),>> Hope it helps>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/194#issuecomment-679142698>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AHIVDUMFBRQ7HH6TUMZNWVTSCJW2LANCNFSM4GQETFOQ>> .>=====']"
https://github.com/justadudewhohacks/face-api.js/issues/190;Issue in Face Tilt webcam;2;open;2019-01-11T06:16:35Z;2019-01-11T14:33:42Z;I tried working on the issues under https://github.com/justadudewhohacks/face-api.js/issues/152.   It still doesn't work. The degree value generated is not even close. I have tried editing the drawBox() in face-api.js as such:function drawBox(ctx, x, y, w, h, options) {const imgEl = $('#video').get(0)var drawOptions = Object.assign(getDefaultDrawOptions(), (options || {})),ctx.strokeStyle = drawOptions.boxColor,ctx.lineWidth = drawOptions.lineWidth,const landmarks =detectLandmarks(imgEl)landmarks.then(function(result) {var landmarks=result,var dY = landmarks['positions'][45]['y'] - landmarks['positions'][36]['y']var dX = landmarks['positions'][45]['x'] - landmarks['positions'][36]['x']var degree = Math.atan(dY/dX)ctx.translate(x+w/2,y+h/2)ctx.rotate(degree)ctx.translate((-w/2)-x,(-h/2)-y)ctx.strokeRect(x, y, w, h),}),};['1/ ctx.rotate(degree) should be ctx.rotate(-degree)2/ You might create a new canvas, then translate the central point of the rotated image to the central point of the new canvas. After all, you run the face detection again in order to get aligned face.=====', '> 1/ ctx.rotate(degree) should be ctx.rotate(-degree)> > 2/ You might create a new canvas, then translate the central point of the rotated image to the central point of the new canvas. After all, you run the face detection again in order to get aligned face.I did make the changes. What I am trying to tell is that, the degree value generated (in radians or when converted into degrees ) is not correct. How do I make that right?=====']
https://github.com/justadudewhohacks/face-api.js/issues/176;Can I detect eye blink?;21;open;2018-12-31T04:54:12Z;2020-09-26T02:40:33Z;Hi, I get left and right eyes' landmarks by simply calling as below:```jsconst detectionWithLandmarks = await faceDetectionTask.withFaceLandmarks()const leftEye = detectionWithLandmarks.faceLandmarks.getLeftEye()const rightEye = detectionWithLandmarks.faceLandmarks.getRightEye()```What I want is getting points from eyes then doing a simple calculation to find if a user closes or enlarge their eyes. However, I find the points from eyes don't have a large difference when I close or enlarge my eyes. Is there possible to detect detailed eye expression by this library?;"[""Hi, Unfortunately it is not possible currently, the model doesn't approximate your eye contours correctly, when closing your eyes, since there are also not much samples in the training set for closed eyes.====="", 'Hi,I am not able to get the lefteye or righteye landmarks. Can you please help. I used below code:const faceDetectionTask = faceapi.detectSingleFace(videoEl, options)console.log(""Left Eye landmarks===========>"" + faceDetectionTask).faceLandmarks.getLeftEye()),=====', 'Your code is quite messed up, you should be doing something like the following:``` javascriptconst result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks()if (result) {  console.log(""Left Eye landmarks===========>"" + result.landmarks.getLeftEye()),}```=====', 'Thanks for quick reply. I used below code to get Mouth coordinates so that i can superimpose my image on it. I use below code to get it and it fetched me below results. Can you please confirm if _x and _y are pixcel distance from left and bottom of the webcam video. reason I am asking because i have to adjust my superimpose image based on this. Many Thanks?Code:******const landmarks2 = await faceapi.detectFaceLandmarks(videoEl)console.log(""Mouth position ===========>"" + JSON.stringify(landmarks2.getMouth())),Results: (Repetation of below array objects)********Mouth position ===========>[{""_x"":334.44507598876953,""_y"":283.48337173461914},{""_x"":333.2390594482422,""_y"":268.63128662109375},{""_x"":324.09584045410156,""_y"":258.9872074127197},{""_x"":336.6387176513672,""_y"":247.29134559631348},{""_x"":337.9642868041992,""_y"":241.43423080444336},{""_x"":330.92708587646484,""_y"":246.2387466430664},{""_x"":348.2770538330078,""_y"":266.6569519042969},{""_x"":342.8745651245117,""_y"":255.31597137451172},{""_x"":350.4652786254883,""_y"":278.01255226135254},{""_x"":345.7722854614258,""_y"":284.419641494751},{""_x"":360.87818145751953,""_y"":303.0000114440918},{""_x"":353.9234161376953,""_y"":297.5144290924072},{""_x"":332.1590042114258,""_y"":285.94871520996094},{""_x"":337.9452896118164,""_y"":249.26227569580078},{""_x"":345.11146545410156,""_y"":256.47751808166504},{""_x"":330.61424255371094,""_y"":242.48717308044434},{""_x"":335.2454376220703,""_y"":262.212610244751},{""_x"":335.30696868896484,""_y"":265.9950828552246},{""_x"":340.21644592285156,""_y"":273.7259101867676},{""_x"":342.5278091430664,""_y"":276.47120475769043}]webcam_face_tracking:178 Mouth position ===========>[{""_x"":322.53273010253906,""_y"":272.36589431762695},{""_x"":324.67933654785156,""_y"":248.39472770690918},{""_x"":317.44129180908203,""_y"":239.83108520507812},{""_x"":331.2320327758789,""_y"":224.28563117980957},{""_x"":331.48548126220703,""_y"":224.04098510742188},{""_x"":328.16890716552734,""_y"":233.9084815979004},{""_x"":341.9776153564453,""_y"":261.01123809814453},{""_x"":333.83033752441406,""_y"":258.4676170349121},{""_x"":344.9219512939453,""_y"":268.77044677734375},{""_x"":345.3641891479492,""_y"":275.1748752593994},{""_x"":362.11463928222656,""_y"":294.49201583862305},{""_x"":355.1399612426758,""_y"":288.5452651977539},{""_x"":324.7887420654297,""_y"":272.382173538208},{""_x"":332.88780212402344,""_y"":228.29031944274902},{""_x"":339.87476348876953,""_y"":235.70405960083008},{""_x"":326.5476608276367,""_y"":225.70733070373535},{""_x"":328.65230560302734,""_y"":256.69057846069336},{""_x"":333.9023208618164,""_y"":258.1000328063965},{""_x"":337.1007537841797,""_y"":264.8284149169922},{""_x"":347.21168518066406,""_y"":270.3109931945801}]=====', 'Yes the coordinates are relative to the input image / frame.=====', ""Hello. First of all thank you very much for this awesome library!!!We are also interested in liveness recognition and I've tried some estimates based on aspect ratio calculation. Unfortunately there is still a certain rate of false negatives.I think, that eye blink detection meight be the best solution to differentiate photos from real persons.So the only solution seems be to import a better model for this.Could you please describe how to train new input models or could you paste some links, where to find the information?Kind regards,Marko====="", 'The most difficult part of training a decent model is collecting the training data I would say. If you are interested in training an own model using tensorflowjs check out my article [18 Tips for Training your own Tensorflow.js Models in the Browser](https://itnext.io/18-tips-for-training-your-own-tensorflow-js-models-in-the-browser-3e40141c9091), which should give you a lot of useful hints on how to train your own model once you got your data set up.=====', ""Thank you very much for the quick response!!! If I can train a good model, I'll post it here.====="", 'I\'ve created a quantizized model and the corresponding manifest.json.Could you describe in short what I have to do, to load that model?Do I have to write another loader class?I always get the following error: **expected weightMap[dense0/conv0/filters] to be a Tensor4D, instead have undefined.**Where do I have to describe the structure of the network?The manifest looks like that:```[    {        ""paths"": [            ""blink-model-shard1""        ],        ""weights"": [            {                ""name"": ""conv2d_1/kernel"",                ""shape"": [                    3,                    3,                    1,                    32                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.2531706249012667,                    ""scale"": 0.002163851494882621,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""conv2d_1/bias"",                ""shape"": [                    32                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.11578530131601819,                    ""scale"": 0.0007328183627596088,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""conv2d_2/kernel"",                ""shape"": [                    2,                    2,                    32,                    64                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.41054220059338736,                    ""scale"": 0.0031824976790185066,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""conv2d_2/bias"",                ""shape"": [                    64                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.09900771177866878,                    ""scale"": 0.0005593656032693152,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""conv2d_3/kernel"",                ""shape"": [                    2,                    2,                    64,                    128                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.326127103730744,                    ""scale"": 0.0026300572881511612,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""conv2d_3/bias"",                ""shape"": [                    128                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.10953535319981621,                    ""scale"": 0.00045262542644552154,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_1/kernel"",                ""shape"": [                    1536,                    512                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.44373518789515776,                    ""scale"": 0.0034398076581019983,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_1/bias"",                ""shape"": [                    512                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.08901600627338184,                    ""scale"": 0.0009890667363709094,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_2/kernel"",                ""shape"": [                    512,                    512                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.3647143992723203,                    ""scale"": 0.004447736576491711,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_2/bias"",                ""shape"": [                    512                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.09677081785950006,                    ""scale"": 0.0010996683847670462,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_3/kernel"",                ""shape"": [                    512,                    1                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.2393317552173839,                    ""scale"": 0.0016736486378838035,                    ""dtype"": ""uint8""                }            },            {                ""name"": ""dense_3/bias"",                ""shape"": [                    1                ],                ""dtype"": ""float32"",                ""quantization"": {                    ""min"": -0.03532525151968002,                    ""scale"": 1.0,                    ""dtype"": ""uint8""                }            }        ]    }]```=====', 'Which model are you trying to load? Note, that to each neural network in this repo there are corresponding model files, you can not load your own models.=====', 'Thanks for the effort on this. Also interested in blink detection. I ran across this article which was useful -- https://www.kairos.com/blog/how-to-use-blink-detection -- pointing out that blinks tend to happen 4+ times per minute and last 300ms. And a key being that both eyes close simultaneously.=====', 'Another article on blink detection: https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/ Which utilizes dlib <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html> and appears to be based on the ""One Millisecond Face Alignment with an Ensemble of Regression Trees"" paper and the ~~HELEN dataset http://www.ifp.illinois.edu/~vuongle2/helen/.~~ 68 point iBUG 300-W dataset https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/.=====', 'can i use eye aspect ratio formula to calculate for blink detection ?=====', 'hi, I would be happy to know if I could use a blink formula too or if there are something else to ditect eyes blink =====', ""> Hi,> > Unfortunately it is not possible currently, the model doesn't approximate your eye contours correctly, when closing your eyes, since there are also not much samples in the training set for closed eyes.It seems like the eye contour doesn't change much at all when close the eyes. I wonder whether this is different from dlib facial landmark predictor, which uses iBUG 300-W dataset to train?====="", 'I expect to be able to get eye blink data!=====', 'Pyimagesearch blog has python code for blink. =====', 'grateful for the library, the eye blink would be interesting=====', 'detecting eye blink is the only thing stopping me from using this fantastic library :(=====', 'Can we check whether eye is being opened or closed with this library. If so, can i have code sample=====', ""> Hi,> > Unfortunately it is not possible currently, the model doesn't approximate your eye contours correctly, when closing your eyes, since there are also not much samples in the training set for closed eyes.your repository face-recognition.js, it's still using dlib for landmark and detect close eye correctly, but I can not acces the point of landmark, the example code its very shortly and just drawing the landmark point by line=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/162;Howto: Chopping up very large image into small enough tiles;9;open;2018-12-07T22:43:00Z;2019-01-17T23:45:16Z;I have a HD image (4k by 3k) and would like to get all faces in it.  The faces are very small in the distance, so I assume I have to tile the image into overlapping tiles of reasonable (500x500?) size, run the detector on each tile, handle overlaps, recombine into one superset.Has anyone else already done this?  Or, are there better ways to handle larger images?;"[""I think your approach is reasonable. If you have very small faces (relative to the image size) then SSD Mobilenetv1 will likely perform better than the TinyFaceDetector, but it won't necessarily detect all faces if you simply pass in your HD image without tiling it.====="", 'Any rough upper bounds (image size) to lower bounds (pixels of face size) that are likely to need tiling? =====', 'Not sure actually, it depends on the size of the faces appearing in an image relatively to the image size, since the images will be scaled down to the network input size. You would have to try out what works for you.=====', 'All the ""take an existing detection and adjust it"" are in terms of scaling, there doesn\'t seem to be an offset function.  Would you suggest1. Making one2. Somehow reaching in and setting private values3. Rebuilding the detections during that map step=====', ""Sorry, I didn't get the question. Do you mean shifting a Rect? You could simply create a new one `new faceapi.Rect(oldRect.x + offsetX, oldRect.y + offsetY, oldRect.width, oldRect.height)`. Or create a PR / issue at [tfjs-image-recognition-base](https://github.com/justadudewhohacks/tfjs-image-recognition-base) to implement a simple shift method in the Box class.====="", 'Clarification of my question: Given that I have an `const fullFaceDescriptions:Array<FullFaceDescription>`, and need to shift every det inside fullFaceDescriptions by some (x,y) amount - is there an easy way to do it?  There is the example\'s `const detectionsForSize = detections.map(det => det.forSize(input.width, input.height))` that scales all the detections, I\'m assuming a shift would be similar.Where I get a bit tripped up is what I need to shift, vs. what is relative, across - fd.detection- fd.landmarksBecause landmarks code mentions ""unshiftedLandmarks"", and ""IFaceLandmarks"" has ""shift: Point"" and ""[shiftBy](https://github.com/justadudewhohacks/face-api.js/blob/master/src/classes/FaceLandmarks.ts#L81)"" - which makes me think some parts of it are relative, or shift-able, and I\'d be reinventing if I stumbled in blindly and tried to make a new det.forShift()=====', 'Ahh I see. So the the face landmark positions are relative to the bounding box of the rectangle, since you predict them on the image patch extracted from the bounding box. Thus I introduced a shift for the landmark classes for the ease of drawing them, because they have to be shifted by the bounding box position in order to retreive the positions in the source image.So actually what you would have to do, is to calculate the correct bounding box for each fd.detection, e.g. shift them by the (x, y) offset of the image patch they have been detected for. Afterwards you would simply have to shift all landmarks by the shift of their bounding box (notice shiftBy(x, y) should actually be named ""setShift"", since it sets an offset for the relative landmarks, rather then adding an offset for every call).Unfortunately currently the utility is missing to easily shift instances of FaceDetection or the utility classes, so you would have to reconstruct these objects manually.=====', 'Understood.I may hack my local script and toss in a ""tileLocation=(x,y)"" for every detection, because I\'d rather spend the time figuring out how to deduplicate across tiles rather than how to correctly shift detections.    for (let imgNum = 0, imgNum < IMAGES.length, imgNum++) {        const imgName = IMAGES[imgNum],        console.groupCollapsed(`Image:${imgName}`),        asyncSrc(input, `images/${imgName}`),        const detections = [],        for (let x = 0, x < input.width - (TILE_DIMENSION / 2), x += (TILE_DIMENSION / 2)) {            for (let y = 0, y < input.height - (TILE_DIMENSION / 2), y += (TILE_DIMENSION / 2)) {                const tileLocation = {\'x\': x, \'y\': y},                drawTile(input, smallCanvas, tileLocation.x, tileLocation.y, TILE_DIMENSION, TILE_DIMENSION),                let fullFaceDescriptions = await faceapi.detectAllFaces(                    smallCanvas,                    new faceapi.SsdMobilenetv1Options({maxResults: 100})                ).withFaceLandmarks().withFaceDescriptors(),                console.log(`${imgName} (${tileLocation.x},${tileLocation.y}) detected faces:${fullFaceDescriptions.length}`),                fullFaceDescriptions.forEach(ffd => ffd.tileLocation = tileLocation), // *** THIS PART                detections.push(...fullFaceDescriptions),            }        }        console.log(`${imgName} saving ${detections.length} detections.`),        await set(input.src.substring(input.src.lastIndexOf(\'/\') + 1), detections),        console.groupEnd(),    }=====', 'I\'m now knee-deep in deduping across tiles.  In the case where two faces overlap, what is the way to tell which one is better?I\'m assuming:1. For all faces that overlap ""enough"" (>75% of the face area rectangle)2. Check that their euclideanDistance is ""small enough"" (<0.2)3. Pick the one with the highest `score`.  I was also thinking I could base it on other things like which has the highest max(all possible expressions) - but AFAIK score is my best bet for ""we really think this is the best face box.""=====']"
https://github.com/justadudewhohacks/face-api.js/issues/161;Is this package available for Angular?;10;open;2018-12-07T13:18:30Z;2021-02-15T13:24:41Z;I'm trying to include this package in Angular 7 via npm i face-api.js, but I'm always getting:error TS2304: Cannot find name 'faceapi'.Any hints or quick steps to include it correctly in Angular?;"[""import * as faceapi from 'face-api.js',seems to solve the issue====="", ""I am getting these warnings:`client:154 ./node_modules/@tensorflow/tfjs-core/dist/tf-core.esm.jsModule not found: Error: Can't resolve 'crypto' in '/home/qamaruddin/eclipse-workspace/imgproc-frontend-angular/node_modules/@tensorflow/tfjs-core/dist'warnings @ client:154onmessage @ socket.js:41EventTarget.dispatchEvent @ sockjs.js:170(anonymous) @ sockjs.js:887SockJS._transportMessage @ sockjs.js:885EventEmitter.emit @ sockjs.js:86WebSocketTransport.ws.onmessage @ sockjs.js:2961wrapFn @ zone.js:1188push../node_modules/zone.js/dist/zone.js.ZoneDelegate.invokeTask @ zone.js:421push../node_modules/zone.js/dist/zone.js.Zone.runTask @ zone.js:188push../node_modules/zone.js/dist/zone.js.ZoneTask.invokeTask @ zone.js:496invokeTask @ zone.js:1540globalZoneAwareCallback @ zone.js:1566client:154 ./node_modules/tfjs-image-recognition-base/build/es6/env/initialize.jsModule not found: Error: Can't resolve 'fs' in '/home/qamaruddin/eclipse-workspace/imgproc-frontend-angular/node_modules/tfjs-image-recognition-base/build/es6/env'`The camera opens but no face tracking overlay :(====="", 'Including the js doesn\'t seem to resolve it either:  <script src=""assets/face-api.js""></script>=====', ""It seems there are plenty of complaints which I have no clue how to solve:```ERROR in src/app/logger.service.ts(29,30): error TS2339: Property 'stack' does not exist on type '{}'.src/face/face.component.ts(120,42): error TS2345: Argument of type 'FaceDetectionWithLandmarks<FaceLandmarks68>[]' is not assignable to parameter of type 'FaceLandmarks | FaceLandmarks[]'.  Type 'FaceDetectionWithLandmarks<FaceLandmarks68>[]' is not assignable to type 'FaceLandmarks[]'.    Type 'FaceDetectionWithLandmarks<FaceLandmarks68>' is not assignable to type 'FaceLandmarks'.      Property '_shift' is missing in type 'FaceDetectionWithLandmarks<FaceLandmarks68>'.```====="", 'What typescript version are you using, maybe there is a mismatch between the tfjs-core version you installed and the version of face-api.js (should be 0.13.8)?=====', 'I am seeing this same problem with the latest clean build of an angular 7 application. For reference, here is the package.json for a minimal app I created just to test this error.{  ""name"": ""ang-face"",  ""version"": ""0.0.0"",  ""scripts"": {    ""ng"": ""ng"",    ""start"": ""ng serve"",    ""build"": ""ng build"",    ""test"": ""ng test"",    ""lint"": ""ng lint"",    ""e2e"": ""ng e2e""  },  ""private"": true,  ""dependencies"": {    ""@angular/animations"": ""~7.2.0"",    ""@angular/common"": ""~7.2.0"",    ""@angular/compiler"": ""~7.2.0"",    ""@angular/core"": ""~7.2.0"",    ""@angular/forms"": ""~7.2.0"",    ""@angular/platform-browser"": ""~7.2.0"",    ""@angular/platform-browser-dynamic"": ""~7.2.0"",    ""@angular/router"": ""~7.2.0"",    ""core-js"": ""^2.5.4"",    ""face-api.js"": ""^0.18.0"",    ""rxjs"": ""~6.3.3"",    ""tslib"": ""^1.9.0"",    ""zone.js"": ""~0.8.26""  },  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.13.0"",    ""@angular/cli"": ""~7.3.3"",    ""@angular/compiler-cli"": ""~7.2.0"",    ""@angular/language-service"": ""~7.2.0"",    ""@types/node"": ""~8.9.4"",    ""@types/jasmine"": ""~2.8.8"",    ""@types/jasminewd2"": ""~2.0.3"",    ""codelyzer"": ""~4.5.0"",    ""jasmine-core"": ""~2.99.1"",    ""jasmine-spec-reporter"": ""~4.2.1"",    ""karma"": ""~4.0.0"",    ""karma-chrome-launcher"": ""~2.2.0"",    ""karma-coverage-istanbul-reporter"": ""~2.0.1"",    ""karma-jasmine"": ""~1.1.2"",    ""karma-jasmine-html-reporter"": ""^0.2.2"",    ""protractor"": ""~5.4.0"",    ""ts-node"": ""~7.0.0"",    ""tslint"": ""~5.11.0"",    ""typescript"": ""~3.2.2""  }}=====', ""Boa noite pessoal.Alguém saber o que é isso ? Estou tentando criar a pasta dist com o comando ng build --prodERROR in src/app/components/camera/face-detection.1.ts(1,26): error TS2307: Cannot find module './js/face-api'.Obrigada por ter respondido a minha outra pergunta pessoal, ajudou bastante ! grata !!!!====="", 'The typescript version that google use to build typescript ends in .2222 i think, or something along those lines. If you try that version and it works, let us know on the ticket so it can be closed out :)=====', 'Thank you, my doubt has been resolved. The project I was studying was without the dist folder and the main.js file was missing, it was an upgrade in the gitignore file. But thank you so much for having responded. Thankful !=====', '@justadudewhohacks this can be closed. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/160;faceapi.detectAllFaces blocks the main thread for seconds on first call;4;open;2018-12-06T15:12:52Z;2018-12-09T18:54:05Z;I have been using this in a React project on a component's mount and it blocks completely the JS execution white it awaits for faceapi.detectAllFaces. This seems to be related with the warning that appears in the console Error: WebGL warning: getBufferSubData: Reading from a buffer with usage other than *_READ causes pipeline stalls. Copy through a STREAM_READ buffer.I could try and submit a PR, but I can't seem to find the actual calls or the buffers it uses, could you point me to it?;"['I never saw this warning before, but it comes from tfjs-core. I dont touch any WebGL in this library.The reason why the initial call takes longer, is due to the shaders being compiled, which is referred to as ""warmup time"".=====', ""Thanks for the quick reply. Is there any way we could split the warmup time into chunks and give JS time to breath, or maybe move the warmup to another thread? The problem is not that it takes long, it's that it hangs the browser. ====="", ""What I was commenting [here](https://github.com/justadudewhohacks/face-api.js/issues/87#issuecomment-432351672) might be an option we could try out:> Would probably be an enhancement to sprinkle some tf.nextFrame calls into to forward methods. But I would first recommend some closer investigation.If that doesn't help, then it's probably something, that has to be done by the WebGL engine of tfjs-core.====="", 'Get the same ""warning""/notice in my browser console :/ =====']"
https://github.com/justadudewhohacks/face-api.js/issues/159;videojs - as input source;9;open;2018-12-06T11:47:57Z;2021-04-26T19:42:23Z;i'm using video.js for playing the rtmp videos... which generates it's own html code... and i'm not able to give the same thing as an input to the face-api...  how can I send input from video.js as html element to the face-api ?;"['Well what is the type of the input you get out of video.js?=====', ""![vdjs-creen](https://user-images.githubusercontent.com/38370966/49633622-100d3a80-fa20-11e8-91cb-56c12186beb2.png)As shown in the photo, i have some generated html stuff, which plays my rtmp live stream, so i'm not able to send this input to face api, as it requires html Image or html Video tags====="", 'I see an video element in the dom tree. You can pass HTMLVideoElement as an input to the API too, see [this](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/videoFaceTracking.html).=====', '![playing-vdjs](https://user-images.githubusercontent.com/38370966/49645368-a3a33300-fa41-11e8-9dd5-718ce0d49435.png)Actually while video starts playing, the above one html tree is generated,  mistakenly I sent you initial html tree...(sorry for that)   the <video> tag is getting replaced with some **`<object>`** tag when it starts playing...this is the error description::**Uncaught (in promise) Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id**=====', ""That doesn't seem right. Video.js is an HTML5 capable video player. Why would it still be using a Flash plugin which has been disabled by nearly the entire web? Flash is blocked by default on Chrome and Safari, which represent well over half the marketshare of browsers out there. Is it possible to simply force video.js to use the HTML5 player rather than the swf player? If so, your problem should be solved. I recommend abandoning any Flash player tech as even Adobe has abandoned that project. ====="", ""I've getting the same issue too, now I'm trying to use videojs for rtmp source, with this, trying to face detection using face-api.js. How can I solve this, justadudewhohacksI need urgent help====="", 'I am not familar with VideoJS sorry, would be nice if anyone wants to investigate in this.=====', 'Has anything else come up about this? So with the FaceAPI and videojs?=====', 'Your input source should be a canvas. Whatever comes out of Video.js and the sources/formats it can pull from must be converted to canvas data ideally. Canvas has the advantages of being zero copy when moving to background threads, and in addition, TensorFlow is used by this library and it will only support the image types it knows about. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/157;face-api.js Trying to Initialize NodeJS Env in Electron Renderer Process;13;open;2018-12-01T17:05:24Z;2019-02-17T15:09:13Z;"About a month ago, I opened an issue #113 which was related to my lack of necessary hardware which I have solved.However, with the latest version of `face-api.js` when using the same project structure and similar files, I am unable to load `face-api.js` the way mentioned in #113 and am getting this error when I try to load `face-api.js` in a renderer process: ![error](https://user-images.githubusercontent.com/21309909/49330716-63831280-f560-11e8-879a-88d0b658b7d4.png)Extra Info:- Loading `face-api.js` in a renderer process of my electron app.- `face-api.js` version 0.16.1- Using `face-api.js` with `electron`This is the project structure:![dir-structure](https://user-images.githubusercontent.com/21309909/49330646-5ade0c80-f55f-11e8-8196-310797f027de.png)index.html:```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">    <title>Test</title></head><body>    <img id=""myImg"" src=""face.png"" alt="""">    <script src=""./node_modules/face-api.js/dist/face-api.js""></script>    <script src=""index.js""></script></body></html>```index.js:```javascriptvar img = document.getElementById(""myImg""),console.log(faceapi), // Just to check whether everything is working.(async () => {    await faceapi.loadSsdMobilenetv1Model('./models')    await faceapi.loadTinyFaceDetectorModel('./models')    await faceapi.loadMtcnnModel('./models')    await faceapi.loadFaceLandmarkModel('./models')    await faceapi.loadFaceLandmarkTinyModel('./models')    await faceapi.loadFaceRecognitionModel('./models')    // Used to work till 0.15.0 / 0.15.1 but doesn't work anymore.    var inf = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()    console.log(inf)})(),```Looking into the `face-api.js` file where the error was thrown, it seems as if `face-api.js` thinks I'm using NodeJS even when I'm loading it in the browser.Thanks a lot, in advance!";"['Hmm, the nodejs check is implemented [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/isNodejs.ts). If this check succeeds in the renderer process, then we will have to fix this.I will investigate in this issue.=====', '@justadudewhohacks Thank you! And it does seem like the NodeJS check succeeds in the renderer process.=====', '@justadudewhohacks Please get this fixed as soon as possible. I really need this to work for a project.Thank you.=====', ""The most straight forward solution would be to export the env inititializiation functions and call them manually: https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.tsFor now you can probably just manually monkey patch the environment back to a browser environment:```faceapi.env.monkeyPatch({    Canvas: HTMLCanvasElement,    Image: HTMLImageElement,    ImageData: ImageData,    Video: HTMLVideoElement,    createCanvasElement: () => document.createElement('canvas'),    createImageElement: () => document.createElement('img')})```====="", 'Thanks so much!=====', 'It worked!=====', 'Should be fixed now, see my answer in [this](https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3) issue.> I think the renderer process environment should now get initialized correctly, if you want to give face-api.js v0.16.2 a try.> > I also exported createBrowserEnv and createNodejsEnv, so in case there are still issues with incorrect initialization one can easily fix this by faceapi.env.setEnv(faceapi.env.createBrowserEnv()) for example.=====', 'Unfortunately, it didn’t work. I updated the module, removed the monkey patch, then tested my app, and it gave me the same error again :(=====', ""@justadudewhohacks are there any basic examples on using face-api with electron? I can't get it working and would really like to use it in a project====="", ""@johndouglas3 You should take a look at #113 to find how I configured mine.I'm using it in my app and it works like a charm(with the monkey patch though.)====="", '@Frixoe thank you for the quick reply, sorry I cant seem to figure it out to get it working. could you point me to a working example?is it in the render process or main process?how are you importing face-api.js?thank you!=====', ""I keep getting` Uncaught (in promise) Error: ENOENT: no such file or directory, open './models/tiny_face_detector_model-weights_manifest.json'`====="", '@johndouglas3Is your project up on GitHub? If so, a link to it would really help.P.s. Apologies for the late reply.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/155;TensorFlow model in Python;1;open;2018-12-01T10:08:27Z;2018-12-01T12:07:29Z;HiI am wondering if it is possible to share your Python code and TensorFlow models in python (at least for face detection and face landmark detection). Indeed, I really enjoyed the performance of your model, but I need to use it in Python.Regards,;"[""The SSD MobileNet v1 face detection model is actually originally a tensorflow model, check out the link to the repo in the README. The other models would have to be ported, but I think it's not hard to translate the tfjs code of the neural networks to tf with python.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/153;Ways of importing models;7;open;2018-11-30T18:28:49Z;2019-07-19T07:46:49Z;I'm building a Chrome extension and using face-api - rather than load the model from a URL, I'd like to include them in a bundle. Rather than loading them from a URL, I'd like to import the files and add them to the objects. Is there any supported way to do this?;"[""The models are binary files so you can not simply include them in your bundle like that. Doesn't chrome extension have a filesystem API? Why not simply load the models from the file system?====="", ""That would work! One issue is that even if I've got the models as a variable or something, I'm not sure how to actually load them - all of the docs point to a network call a la ```jsawait faceapi.nets.ssdMobilenetv1.loadFromUri('/models')```There's the option to load from a tf.NamedTensorMap, but can't seem to find that map anywhere? In models there's the manifest and shard file for a model, but the manifest causes it to crash.Have I missed something very obvious out?====="", ""The weight map is a mapping of tensor name to tensor, which is what is returned from tf.io.loadWeights for example.In your case you probably want to use await faceapi.nets.ssdMobilenetv1.loadFromDisk('/models'), but when using the filesystem api of chrome extension you will have to monkey patch the readFile function as done with the nodejs fs module [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts#L46-L52).``` javascriptfaceapi.env.monkeyPatch({  readFile: // your implemenation here})```====="", 'I had created Chrome extension and using face-api,see: https://github.com/shadowcz007/iknowu=====', 'the sample provided for chrome extension has very huge model. Extensions can not be that big size usually.The line mentioned below in face-api.js project? where can I get ""face_detection_model.weights"" **`net.load(await faceapi.fetchNetWeights(\'/models/face_detection_model.weights\'))`**=====', 'The raw .weights files can be pulled from the models [repo](https://github.com/justadudewhohacks/face-api.js-models).=====', '@justadudewhohacks can u tell me how to load the models in ionic 3 framework=====']"
https://github.com/justadudewhohacks/face-api.js/issues/152;Significant drop of the accuracy for tilted face;13;open;2018-11-29T15:03:02Z;2020-07-02T13:22:23Z;I am testing the different images and I experience significant drop of accuracy if I just feed the tilted face with >30*.I am using the prepared examples in browser and node-js and effects are similar: for the face looking straight into camera I get around 0.3 - 0.4, but if I tilt the face I go beyond 0.7. When testing the same images with dlib (which is the original network used in face-api.js if I understand correctly) I get the distance of <0.5 for the tilted faces.Do I need to additionally enable the alignment with face-api.js? Maybe I am missing something here.Here the example photos, where I get high distance with face-api.js, but is accepted by dlib:![grande1](https://user-images.githubusercontent.com/14967831/49232775-e5eec380-f3f4-11e8-9891-29a812eed371.jpg)![grande2](https://user-images.githubusercontent.com/14967831/49232779-e9824a80-f3f4-11e8-9887-b2c4c94a281b.jpg)But if I just manually rotate further the first photo:![new](https://user-images.githubusercontent.com/14967831/49233071-76c59f00-f3f5-11e8-9dd6-5cda24baef0e.jpg)the face is accepted and I get very similar result to dlib (around 0.48).;"[""Thanks for reporting this! The issue is, that face-api.js alignment doesn't rotate the images, it only centers them at the moment. I guess dlib does rotate them based on the face landmarks predicted by the shape predictor right?====="", 'Yes, I am not sure how exactly is it done to mimic it, but it is done here: http://dlib.net/dnn_face_recognition_ex.cpp.html in the function`extract_image_chip`Here even 5 landmark model is used instead of 68 point model. Maybe this 5 point model after quantization would be even smaller than the currently used model in face-api.js for face recognition :)I could try to make a PR with it, but I am normally writing in C++, someone would need to review it, thoroughly.=====', 'Maybe in future it would be beneficial to train a very small 5 point landmark model just for alignment, although the existing 68 point face landmark models should be really fast already, so not sure if thats worth it.A PR would be highly appreciated. Currently the alignment is done based on the 68 point face landmarks and is simply centering the face by some predefined ratio, that is close to how dlib does it. The rotation of the face is not considered yet, I think the main challenge would be to figure out, how to rotate the image content of an HTML canvas or image: [FaceLandmarks.ts](https://github.com/justadudewhohacks/face-api.js/blob/master/src/classes/FaceLandmarks.ts#L84-L97)=====', ""Thanks a lot! I'll try to come back with PR if I find some time for it.====="", 'Just gonna leave this issue open to keep track of it.=====', 'Hi Jendker,You can rotate the image by using landmark position at 36 (left eye) and 45 (right eye) as begin and end point then you calculate the angle of the line compared with horizontal.For example: https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/bbtFaceLandmarkDetection.htmlat line 42:`    function redraw() {      const canvas = faceapi.createCanvasFromMedia(currentImg)      $(\'#faceContainer\').empty()      $(\'#faceContainer\').append(canvas)      faceapi.drawLandmarks(canvas, landmarks, { lineWidth: drawLines ? 2 : 4, drawLines })    }`You can edit as`    function redraw() {      var canvas = faceapi.createCanvasFromMedia(currentImg)            var dY = landmarks[\'positions\'][45][\'y\'] - landmarks[\'positions\'][36][\'y\']      var dX = landmarks[\'positions\'][45][\'x\'] - landmarks[\'positions\'][36][\'x\']      var degree = Math.atan(dY/dX)      var ctx = canvas.getContext(""2d""),      ctx.rotate(-degree),      ctx.drawImage(currentImg,0,0),      $(\'#faceContainer\').empty()      $(\'#faceContainer\').append(canvas)      faceapi.drawLandmarks(canvas, landmarks, { lineWidth: drawLines ? 2 : 4, drawLines })    }`Hope I can help.=====', 'That could be something, thanks!I was thinking about a bit different approach with the alignment to follow what is done in dlib. Davisking is using the affine transformations with some approximations, which are quite complex and if we are using the original dlib face recognition model it would be advisable to follow it to get better recognition results. On the other hand it works still well if we are not using any transformation just image cropping :) That could be an intermediate solution.The other thing is how to add it neatly into face-api.js.=====', ""Hello everyone, I'm trying to use this API on my project to detect faces but I've noticed that I have a lot of different users that try to submit a photo with their faces tilted. Has been there any update on this particular issue?====="", ""I ended up using a solution recommended by Bajajar and it was fine for me. I didn't manage to get it working by changing the underlying face-api.js functions, my limited JavaScript skills were not sufficient. ====="", ""@Jendker thank you for your quick respose!I've noticed that the recognition API doesn't work well with tilted images but the detection is doing great!Thank you.====="", 'https://github.com/Jack-CV/FaceKit/tree/master/PCNIf face-api.js uses such a model, it will be able to recognize faces of all angles.Face-api.js can not meet my requirements because the landmarks are messy and unmatched on a lying face or a face that is flipped up and down.=====', '@wkdhkr thanks for sharing the repo, I will look into the paper and if it looks promising.The landmark model is not trained on data of faces that are rotated like 90 degrees. You could simply rotate the image in 90 degree steps and feed them into the model, but that obviously requires more computation time.=====', '@justadudewhohacks https://github.com/siriusdemon/pytorch-PCNThere is a Pytorch version of the above model. If this is converted into a model for Tensorflow.js with ONNX.js etc., it seems that it can be used with face-api.js. (I do not understand the problem of license well.)Alternatively, it might be possible to bridge the C implementation as a native module.By the way, Not only landmarks but also ssd mobile net can not detect inverted or rotated faces. Rather, if it is possible to detect the top and bottom of the face first, the landmark model as it is may be fine even with the current landmark model. Because processing only needs to rotate the face once.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/151;[Android] Error on Live Demos: Failed to compile fragment shader.;6;open;2018-11-28T08:06:59Z;2021-11-10T11:49:45Z;I accessed the demo site (https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/) on my Android device, but the face detection didn't work.There was such an error.```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.createProgram (/face-api.js/0-e459655e1a7a45356a06.js:250)    at compileProgram (/face-api.js/0-e459655e1a7a45356a06.js:250)    at /face-api.js/0-e459655e1a7a45356a06.js:250    at e.getAndSaveBinary (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.compileAndRun (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.conv2d (/face-api.js/0-e459655e1a7a45356a06.js:250)    at ENV.engine.runKernel.x (/face-api.js/0-e459655e1a7a45356a06.js:250)    at /face-api.js/0-e459655e1a7a45356a06.js:250    at e.scopedRun (/face-api.js/0-e459655e1a7a45356a06.js:250)```My device:Android 6.0.1Chrome 70.0.3538.30SONY Xperia A4;"['Which face detector did you use, the default one, e.g. TinyFaceDetector?=====', '@justadudewhohacks I got the same error in all detector...- Tiny Face Detector- SSD Mobilenet v1- MTCNN=====', ""That's odd, may be some issue with the WebGL backend on your device. Did run any tfjs application on your Xperia before? If not you could try setting up a simple page running a conv2d operation, to see whether it is an issue with your device and if so report an issue at tfjs.====="", ""This is a TFJS/device issue. I've hacked together a solution which works in my case (I'm also using face-api.js) here: https://github.com/tensorflow/tfjs/issues/952#issuecomment-468393905====="", '![image](https://user-images.githubusercontent.com/45730502/110116513-9c645e80-7ddd-11eb-8167-5f4e77082a92.png)Hi, I\'m also facing the same issue on Samsung Galaxy J2.I developed an app in Cordova and VueJs. It\'s working fine on some of the phones.Even if it does not work, it\'s ok for me but it does not throw any error to any of called functions.It\'s blocking the flow, so I\'m using a timeout of 10sec to close the page.But some devices taking more than 10sec to load models, here it creates a problem for me.I\'m using this monkey path code:<img width=""983"" alt=""Screenshot 2021-03-05 at 6 08 15 PM"" src=""https://user-images.githubusercontent.com/45730502/110116631-c61d8580-7ddd-11eb-8242-7d6c9884ab69.png"">Not getting where it\'s failing and where to add try-catch.Does anyone have a solution for this?=====', 'having the same issue on Oppo A37, is there a patch/solution available to this issue?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/143;Minimum and optimal android device specifications;6;open;2018-11-20T18:34:11Z;2020-08-03T14:09:06Z;Do you have some guidance of the minimum and optimal device specification to run face-api.js offline on a mobile device to detect faces, and to do facial recognition;"['Not really. It should run on newer devices for sure, I was even able to run it on my old android phone. The tiny face detector and the landmark nets are very mobile friendly.=====', ""@justadudewhohacks In the same line of thought... are there any current device restrictions you can think of? I know it won't run in IE11, but are there any other known issues?====="", 'Hm I actually never tried it in IE to be honest, I am using either Chrome or FireFox. But in general I would say, it should run on everything being able to run tfjs.=====', ""I've tested it in Chrome (PC and Android), Firefox (PC and Android), Edge (PC), Safari (on both Macbook and iPhone) and all seem to work! IE definitely doesn't work, as it doesn't support async functions.====="", 'Hmm, but your talking about the examples here, right? The source code of the library itself gets transpiled down to es5, which should run in IE11.=====', '@justadudewhohacks Is there any possibility that this should work on offline system?I am working on a chromium based project for school examination software, where student will give exam on a computer which is not connected to internet, exam software will be deployed in local LAN. So if I integrate this to detect face, will it work if there is no internet? Can prefetch the model (and any other files) needed to run this.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/107;Head position (Yaw, Roll, Pitch);24;open;2018-10-22T16:02:49Z;2021-03-07T18:25:09Z;Hi,first, congratulation for making such awesome lib!A great addition would be head position estimation : yaw, roll and pitch angle. Thank you for your great work.;"[""That's indeed a reasonable extension and it's actually on my roadmap for this repo. Can't make any forecast on when I will get to investigate in implementing this.====="", 'Do you suggest any way to implement these?=====', 'OpenFace does it and uses OpenCVs solvePnP implementation for head pose estimation: [LandmarkDetectorFunc.cpp](https://github.com/TadasBaltrusaitis/OpenFace/blob/be9b57703c2d7194f3d6354b46d22fe3a240141b/lib/local/LandmarkDetector/src/LandmarkDetectorFunc.cpp).Since we can not use OpenCV here an option would be to train a simple model that mimics a PnP solver.Apparently [deepgaze](https://github.com/mpatacchiola/deepgaze/blob/master/examples/ex_cnn_head_pose_estimation_images/ex_cnn_head_pose_estimation_images.py) already has models for head pose estimation, which we could look into.=====', '@justadudewhohacks have you begun work on this by chance? trying to figure out whether i should begin work on a PR or if its relatively close. =====', ""@ikeforrest, I haven't started working on this, I am currently working on age, gender, ethnicity recognition for the project. Any PRs and ideas for head position estimation are highly appreciated. :)====="", ""Sounds good!! I’ll see what I can put together. When do you think you’llwrap up age, gender, ethnicity? I would love to use that too haha.On Saturday, January 26, 2019, Vincent Mühler <notifications@github.com>wrote:> @ikeforrest <https://github.com/ikeforrest>, I haven't started working on> this, I am currently working on age, gender, ethnicity recognition for the> project. Any PRs and ideas for head position estimation are highly> appreciated. :)>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/107#issuecomment-457854476>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/ABLXL1NnrtTpkbGkTvHMBt10Qo2Nxx83ks5vHJ_7gaJpZM4XzqLv>> .>====="", ""Can't say for sure, I am currently training and evaluating models, so I can't tell about any ETA yet.====="", 'Any ETA Now?=====', '@justadudewhohacks  Please you check this option [ Implement solvePnP in tensorflow.js](https://github.com/sergidomenechguzy/tensorflowjs-solvepnp/blob/master/index.js)Thanks!=====', 'found a solvepnp implementation in javascripthttps://github.com/adamwhat/snapweb_js=====', 'Anyone got this working?We are looking for a solution to this but we are completely new to AI, training models and such.=====', ""Hi, with landmark detection I managed to have a first approximation with the following predicates :- The more is the mouth close to the ears position, the more the head is looking to the top and you get rx rotation- Depending on nose position relative to eyes you can get ry rotationIt's a first implementation and I think it can be improved (and it doesn't detect rz axis, more code is necessary for this one but you can use the eyes alignement for that)```    function getTop(l) {      return l        .map((a) => a.y)        .reduce((a, b) => Math.min(a, b)),    }    function getMeanPosition(l) {      return l        .map((a) => [a.x, a.y])        .reduce((a, b) => [a[0] + b[0], a[1] + b[1]])        .map((a: number) => a / l.length),    }    const useTinyModel = true,    faceapi      .detectSingleFace(someCanvas, new faceapi.TinyFaceDetectorOptions())      .withFaceLandmarks(useTinyModel)      .then((res) => {        // Face is detected        if (res) {          var eye_right = getMeanPosition(res.landmarks.getRightEye()),          var eye_left = getMeanPosition(res.landmarks.getLeftEye()),          var nose = getMeanPosition(res.landmarks.getNose()),          var mouth = getMeanPosition(res.landmarks.getMouth()),          var jaw = getTop(res.landmarks.getJawOutline()),          var rx = (jaw - mouth[1]) / res.detection.box.height,          var ry = (eye_left[0] + (eye_right[0] - eye_left[0]) / 2 - nose[0]) /              res.detection.box.width,          console.log(            res.detection.score, //Face detection score            ry, //Closest to 0 is looking forward            rx // Closest to 0.5 is looking forward, closest to 0 is looking up          ),        }else{          // Face was not detected        }      })```====="", '@RomaricMourgues  do you have the final code now to find the face direction .code that returns the correct face direction(right ,left, up,down )   =====', 'Unfortunately no, this code suits my needs, I only needed it for liveness tests... Of course you can approximatively detect direction with this additional code:``` function getTop(l) {    return l        .map((a) => a.y)        .reduce((a, b) => Math.min(a, b)),}function getMeanPosition(l) {    return l        .map((a) => [a.x, a.y])        .reduce((a, b) => [a[0] + b[0], a[1] + b[1]])        .map((a: number) => a / l.length),}const useTinyModel = true,faceapi    .detectSingleFace(someCanvas, new faceapi.TinyFaceDetectorOptions())    .withFaceLandmarks(useTinyModel)    .then((res) => {        // Face is detected        if (res) {            var eye_right = getMeanPosition(res.landmarks.getRightEye()),            var eye_left = getMeanPosition(res.landmarks.getLeftEye()),            var nose = getMeanPosition(res.landmarks.getNose()),            var mouth = getMeanPosition(res.landmarks.getMouth()),            var jaw = getTop(res.landmarks.getJawOutline()),            var rx = (jaw - mouth[1]) / res.detection.box.height + 0.5,            var ry = (eye_left[0] + (eye_right[0] - eye_left[0]) / 2 - nose[0]) /                res.detection.box.width,            console.log(                res.detection.score, //Face detection score                ry, //Closest to 0 is looking forward                rx // Closest to 0.5 is looking forward, closest to 0 is looking up            ),            let state = ""undetected"",            if (res.detection.score > 0.3) {                state = ""front"",                if (rx > 0.2) {                    state = ""top"",                } else {                    if (ry < -0.04) {                        state = ""left"",                    }                    if (ry > 0.04) {                        state = ""right"",                    }                }            }        } else {            // Face was not detected        }    })```=====', 'Hey @RomaricMourgues   Thanks for this code . Let me test it with multiple face case . =====', '@RomaricMourgues  In this video after pausing frame I got two face direction result 1) FRONT  if (ry <= 0.04 && ry >= -0.04) {                      state = ""front"",                    }2) TOP                if (rx > 0.2) {                  state = ""top"",                }  but one face is looking LEFT .Could you help please  RY  -0.027298410047406 RX 0.15585612787682257RY -0.13454089179573414 RX 0.239445432878375![image](https://user-images.githubusercontent.com/40135431/93209376-af299f00-f77b-11ea-8d0b-fab4b46b8e83.png)![image](https://user-images.githubusercontent.com/40135431/93208671-8bb22480-f77a-11ea-9b67-7ad7a139758d.png)=====', 'Hi @justadudewhohacks can we detect  face looking just forward not left or right and draw box to that particular face in a video having multiple faces  ?I will be glad if you could help me out Thanks=====', ""hi, just wanted to share https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.htmldemo https://storage.googleapis.com/tfjs-models/demos/facemesh/index.htmlI think it could be interesting to have a look into integrating something like that in face-api.js (or anyway it's a good alternative lib for this use case)====="", '@RomaricMourgues do u have any idea how can we detect the distance between camera and the face?  If u have any idea it will be helpful.=====', ""@Namrataijare unfortunately I don't, and I don't think this is possible:To know the distance of a 2D object from the camera, you need to know the object size (so for a head you can have a rough idea like 5cm between two eyes) but also the field of view of the camera.If you know both you can do some Pythagore: having the length of one edge, the opposite angle and supposing the triangle is isosceles (face looking toward the camera).So in theorie you can with this two information, but in practice I think the approximation of each measures will give very not precise results.====="", 'what progress of this pose angle detection :)=====', ""i've added function to my port of `face-api` <https://github.com/vladmandic/face-api>  as requested in <https://github.com/vladmandic/face-api/issues/40>  that extends `.withFaceLandmarks()` mehod to calculate angle values  and returns additional property in the resultset:```jsangle: {  pitch: 0.05355965542754132  roll: 0.045894150362739826  yaw: -0.19656298447653597}```the issue is that face-api landmarks are 2d,  so there is quite a lot of missing to properly calculate angles,  this is more of a best-guess than anything:```jsfunction calculateFaceAngle(mesh) {  const radians = (a1, a2, b1, b2) => Math.atan2(b2 - a2, b1 - a1),  const angle = { roll: <number | undefined>undefined, pitch: <number | undefined>undefined, yaw: <number | undefined>undefined },  if (!mesh || !mesh._positions || mesh._positions.length !== 68) return angle,  const pt = mesh._positions,  // roll is face lean left/right  // comparing x,y of outside corners of leftEye and rightEye  angle.roll = radians(pt[36]._x, pt[36]._y, pt[45]._x, pt[45]._y),  // yaw is face turn left/right  // comparing x distance of bottom of nose to left and right edge of face  //       and y distance of top    of nose to left and right edge of face  // precision is lacking since coordinates are not precise enough  angle.pitch = radians(pt[30]._x - pt[0]._x, pt[27]._y - pt[0]._y, pt[16]._x - pt[30]._x, pt[27]._y - pt[16]._y),  // pitch is face move up/down  // comparing size of the box around the face with top and bottom of detected landmarks  // silly hack, but this gives us face compression on y-axis  // e.g., tilting head up hides the forehead that doesn't have any landmarks so ratio drops  // value is normalized to range, but is not in actual radians  const bottom = pt.reduce((prev, cur) => (prev < cur._y ? prev : cur._y), +Infinity),  const top = pt.reduce((prev, cur) => (prev > cur._y ? prev : cur._y), -Infinity),  angle.yaw = 10 * (mesh._imgDims._height / (top - bottom) / 1.45 - 1),  return angle,}```if anyone has better suggestions, please tell me! :)  and i don't see how replacing solvePnP would improve things since model itself returns 2D landmarks,  so there is nothing to solve for to start withnow, i've also added angle calculations in my library: Human <https://github.com/vladmandic/human>  and since the facemesh there is far more detailed (478 points vs 58 points)  and results are in 3D instead of 2D,  it calculates actual angles with decent precision:```jscalculateFaceAngle = (mesh) => {    if (!mesh) return {},    const radians = (a1, a2, b1, b2) => Math.atan2(b2 - a2, b1 - a1),    const angle = {      // roll is face lean left/right      // looking at x,y of outside corners of leftEye and rightEye      roll: radians(mesh[33][0], mesh[33][1], mesh[263][0], mesh[263][1]),      // yaw is face turn left/right      // looking at x,z of outside corners of leftEye and rightEye      yaw: radians(mesh[33][0], mesh[33][2], mesh[263][0], mesh[263][2]),      // pitch is face move up/down      // looking at y,z of top and bottom points of the face      pitch: radians(mesh[10][1], mesh[10][2], mesh[152][1], mesh[152][2]),    },    return angle,  }```====="", ""Haven't tried myself but may be useful to users looking for 3D face mesh:* https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html====="", ""@oncet MediaPipe uses Blaze face+FaceMesh models, same as my library I've mentioned: <https://github.com/vladmandic/human>Except MediaPipe solution is pure WASM (only) solution (closed source) with only calling API exposed to JS.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/87;blocking browser hang issue;8;open;2018-09-09T11:12:07Z;2019-03-30T15:12:55Z;Checked demo link as well as ran the local setup - why it hangs the browser in a blocking way for the time being it is detecting/recognizing face?;"['Because for the time of forwarding an input through the net, the main thread is blocked.=====', 'Would probably be an enhancement to sprinkle some **tf.nextFrame** calls into to forward methods. But I would first recommend some closer investigation.=====', '@justadudewhohacks can those blocking operations be written node.js backend script?=====', ""I didn't quite get your question sorry. If your question is, whether you can use face-api.js with nodejs, then the answer is yes. See my comment in #55.====="", '@justadudewhohacks  thanks dude - that answered my query =====', 'maybe pass to web worker to do it ? 🤔=====', 'I tried to pass it to a web worker but had no success at all. If anyone manages to do it, I would be most interested.=====', 'i tried pass the job of load model to web worker do, but when getEnv() in tfjsImageRecognitionBase are fail because web worker cant access window object. Not sure anyway to bypass and direct tell the tfjsImageRecognitionBase is broswer env.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/77;faceapi.locateFaces returns empty array;2;open;2018-08-27T02:46:52Z;2018-08-27T06:44:13Z;Following #75, I have succeeded in downloading the models.But for some reason when on my android device using faceapi.locateFaces returns an empty array.Though it works flawlessly on my browser.Any pointers on what I should check ?;"['Interestingly though, I uploaded my code to my server at https://julianalimin.com/models/app/And for some reason it works on some phones only.Crashed my iPhone SE and does not work on my Asus Zenfone 3.Are there minimum requirements to work on phone?=====', ""Looking at your app, it seems you are using the ssd mobilenetv1 model for face detection. This one doesn't run on my android as well, probably because it requires too much resources.For mobile face detection I would recommend using the new tiny yolo v2 face detector, which uses separable convs instead of regular convolutions. This model is much lighter and way faster on mobile devices than ssd mobilenetv1 and MTCNN.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/63;Mobile Browser not working, Android 8.0, Chrome, React;4;open;2018-08-01T19:50:37Z;2018-08-07T15:40:09Z;Hi @justadudewhohacks I was trying the api on a Mobile browser, but is not working :worried: I tried it on a Android 8.0, Chrome, and I was using React.jsIf you want to try it too, here is the repo:https://github.com/BonnieMilian/face-api-webcam-react;"[""Just tested the examples on android with chrome. Seems that the SSD face detector is not working. Besides that, it should work fine, although it runs much slower on mobile. You could try MTCNN or Yolo (haven't tried Yolo on mobile yet) for face detection.====="", 'I try it on iOS, Safari.And work fine, chrome is not supporting something, chrome on the pc neither work=====', ""MTCNN doesn't work for me on android 6.0.1 and mobile chrome 68.0.3440.85====="", 'Related with this issue https://github.com/tensorflow/tensorflow/issues/18741Is chrome itself=====']"
https://github.com/justadudewhohacks/face-api.js/issues/59;Unstable MTCNN face landmark detection;10;open;2018-07-24T10:33:52Z;2019-08-05T07:30:44Z;Hi, we've been messing around with face-api and especially MTCNN face detection and 5 face landmarks detection. I think you guys did a great job on the lib overall 😄 However we were trying to get it work faster and more precisely. So far we've been able to get 7fps on a webcam stream.Below we recorded some gifs to visualize.![mtcnn-test-2](https://user-images.githubusercontent.com/37145307/43133009-bc9e9260-8f3d-11e8-8784-07cd2fdb0c00.gif)![mtcnn-test-1](https://user-images.githubusercontent.com/37145307/43133011-bcc64b66-8f3d-11e8-983b-e72577e84afe.gif)The questions I have regarding these are:1. What do you think could help us improve the amount of recognitions per second (the thing I called fps before) ?2. As you can see on the GIFs the recognitions are quite unstable even when the person is not moving, do you think there is some way that we could improve stability of the algorithm ?Hope I made it clear 😉 ;"['Hi,First of all I would try it out with better lightning. I found that the algorithm is very sensitive to lightning conditions, the shadows on your faces might make it less precise.Also could you tell me the parameters you are using, e.g. min face size and stage threshold values as well as, which browser you are using. Currenty the package works best in chrome.To get better fps, mainly increasing the min face size does help.=====', ""We are using  maxNumScales: 10,    scaleFactor: 0.709,    scoreThresholds: [0.6, 0.7, 0.7],    minFaceSize: 200And I was trying it with various min face sizes and 200 seemed to be optimal as higher values would require me to lean over the laptop so that my face is big enough. And we've been using chrome.====="", ""We also discovered this problem with lighting, however, we can't assure the optimal lighting. We want to use the app in various conditions.  ====="", 'You could also try to increase the scaleFactor by a bit, to make the detector compute at more scales to avoid ""blind spots"". You can by the way use `mtcnn.forwardWithStats` to receive information about the scales / image sizes that have been used in stage 1, e.g:```const { results, stats } = mtcnn.forwardWithStatsconsole.log(stats.pyramid)console.log(stats.scales)```Might help you finetune parameters.May I ask what GPUs are built in your laptops? 7 fps seems to be pretty slow.=====', ""I tried to improve the results by modifying scale factor, but didn't get astonishing result, perhaps I didn't do it thoroughly  enough.I am working on a laptop with Intel HD Graphics 6000 1536  MB, it's not a graphics demon but it handles basic graphics display quite well, I even used it for video editing and I must say it worked pretty smoothly 😉 ====="", ""I see. I think there are currently still issues with tfjs + Intel that have to be resolved, not sure if this influence the results you get with MTCNN.The results of the above posted GIFs actually don't look too bad, there are simply some blind spots. ====="", '$(document).ready(function() {    run()  })        async function run() {    // load the models    await faceapi.loadTinyFaceDetectorModel(\'./models\')    await faceapi.loadMtcnnModel(\'./models\')    await faceapi.loadFaceRecognitionModel(\'./models\')      const videoEl = document.getElementById(\'inputVideo\')    console.log(""what is happening""),    navigator.getUserMedia(      { video: {} },      stream => videoEl.srcObject = stream,      err => console.error(err)    ),    this.onPlay(videoEl),  }  async function onPlay(videoEl) {    // Promise.all([    //     faceapi.nets.ssdMobilenetv1.loadFromUri(\'./models\'),    //        faceapi.nets.tinyFaceDetector.loadFromUri(\'./models\'),    //       faceapi.nets.faceRecognitionNet.loadFromUri(\'./models\')    //      ])  const mtcnnForwardParams = {      maxNumScales: 10,    scaleFactor: 0.709,    scoreThresholds: [0.6, 0.7, 0.7],    minFaceSize: 200  },  // await faceapi.drawDetection(\'./models\')  //   await faceapi.drawLandmarks(\'./models\')//   const mtcnnResults = await faceapi.mtcnn(document.getElementById(\'inputVideo\'), mtcnnForwardParams)const overlay=document.getElementById(\'overlay\'),const mtcnnResults = await faceapi.mtcnn(document.getElementById(\'inputVideo\'), mtcnnForwardParams) faceapi.drawDetection(overlay, mtcnnResults.map(res => res.faceDetection), { withScore: false }) faceapi.drawLandmarks(overlay, mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: \'red\' })const options = new faceapi.MtcnnOptions(mtcnnParams) const input = document.getElementById(\'inputVideo\')const fullFaceDescriptions = await faceapi.detectAllFaces(input,options).withFaceLandmarks().withFaceDescriptors()// const alignedFaceBoxes = results.map(//     ({ faceLandmarks }) => faceLandmarks.align()//   )  //   const alignedFaceTensors = await extractFaceTensors(input, alignedFaceBoxes)  //   const descriptors = await Promise.all(alignedFaceTensors.map(//     faceTensor => faceapi.computeFaceDescriptor(faceTensor)//   ))  //   // free memory//   alignedFaceTensors.forEach(t => t.dispose())const labels = [\'face1\',\'face2\',\'praveen\']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `./${label}.jpg`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it\'s landmarks and face descriptor    const fullFaceDescription = await faceapi.detectAllFaces(img,new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }    const faceDescriptors = [fullFaceDescription.descriptor]    // console.log(label)     return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)   }) )// 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance) //console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))/*const boxesWithText =*/ results.map((bestMatch, i) => {    const box = fullFaceDescriptions[i].detection.box    const text = bestMatch.toString()    const boxWithText = new faceapi.BoxWithText(box, text)    drawBox.draw(canvas)    return boxWithText  })  let myCanvas = document.getElementById(\'overlay\'),  const context = myCanvas.getContext(\'2d\'),  context.clearRect(0, 0, myCanvas.width, myCanvas.height),  faceapi.drawDetection(overlay, boxesWithText) //   faceapi.drawDetection(\'overlay\', mtcnnResults.map(res => res.faceDetection), { withScore: false })//   faceapi.drawLandmarks(\'overlay\', mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: \'red\' })    setTimeout(() => this.onPlay(videoEl),150)  }  =====', 'this is my code,when i run it, it showing ncaught (in promise) TypeError: faceapi.drawDetection is not a function    at onPlay (script2.js:40)onPlay @ script2.js:40async function (async)onPlay @ script2.js:39onplay @ (index):50this error. please help to solve me=====', '@thirukumars faceapi.drawDetections is not a part of faceapi anymore, please use the up to date examples.=====', 'thank you, =====']"
https://github.com/justadudewhohacks/face-api.js/issues/47;Webworker;81;open;2018-07-12T13:18:27Z;2021-03-18T19:26:23Z;Hi there! Great work on this plugin!Has anybody managed to run this in a webworker?;"[""I haven't tried it but it should work. I am assuming you want to use a webworker so that the ui doesn't get blocked because of loading the models or during face detection/recognition? Do keep in mind that you won't be able to access dom elements from a webworker.====="", 'Wanted to use a tensor as input in the worker, to avoid DOM elements, would just transfer the image data to the worker and then transfer the results back m.=====', ""I did it and it works, but at the moment tfjs is not compatible with offscreen canvas (https://github.com/tensorflow/tfjs/issues/102) so you don't have access to the GPU from a webworker and the result is more than slow actually ... If your purpose is to get better performances I advice you to have a try with the mtcnn model which is really faster, see the 0.9.0 version of this project.====="", '@akofman Out of curiosity, how did you make it work ?=====', 'I had same experience with `tracking.js` library. I managed to to put its part to web worker https://github.com/eduardolundgren/tracking.js/issues/99 and completely get rid of the UI lags but code was just for POC=====', ""> I did it and it works, but at the moment tfjs is not compatible with offscreen canvas ([tensorflow/tfjs#102](https://github.com/tensorflow/tfjs/issues/102)) so you don't have access to the GPU from a webworker and the result is more than slow actually ...> If your purpose is to get better performances I advice you to have a try with the mtcnn model which is really faster, see the 0.9.0 version of this project.@akofman I also would be interested in how you got it working in a worker, if you're willing to share!====="", ""Bumping this for relevance because i'm now working on doing the same thing and with offscreen canvas. [My project is here](https://github.com/jeffreytgilbert/goto-booth) if anyone wants to check out why. I'm rendering a threejs scene and the face detection allows me to use face detection to control the perspective of the 3d scene. Each time the face detection runs at 100ms, the scene janks out briefly because the 3d part runs in about 1ms cpu time over 16ms budget for RAF calls, but the face detection part goes 40ms-100ms so you lose 3 frames at a time, making it look like the rendering is broken. Now I have to find a way to get it under budget for the reasons @thexiroy mentioned ====="", ""It's probably better to ask for help at tfjs regarding how to get this running in a webworker====="", ""For those interested, there is a pull request open here which was a bit dated, but I've been working on. The OffscreenCanvas support doesn't appear that involved, but there doesn't appear to be any special consideration for web workers or transferable objects, and those may take longer to integrate in. I did see there is a branch for web workers open, but I haven't been through it.https://github.com/tensorflow/tfjs-core/pull/1221====="", '@justadudewhohacks tfjs updates is a non-starter. Typescript wont have support for OffscreenCanvas until 3.5.0 and that\'s not officially released. Even if they do release it, it\'s currently buggy and TensorFlow wont build against 3.5.0 without ts-ignore hacks. Even if you do the ts-ignore hacks, the resulting build of tensorflow running in face-api.js barf due to those ignored incompatibilities. Models don\'t work. flattened maps don\'t work. Whole thing barfs. So, hacking created and updated tickets for those findings. The tfjs-core update thread above was updated. I also created a typescript issue that can be tracked here: https://github.com/Microsoft/TypeScript/issues/30998It looks like things on both those projects move pretty quickly, so hopefully this wont fall to the bottom of the thousands of filed issues on the pile and actually get some updates. For now, I will be attempting to create a fake interface in the worker thread which proxies back to the main js with updates and commands. The approach is similar to one that @mizchi took here: https://github.com/tensorflow/tfjs/issues/102#issuecomment-462167706The difference between @mizchi \'s approach and my approach will be that i am attempting to fool the tensorflow library into believing it is running under non-worker thread conditions using my new found knowledge of how it works (gained by trying to fix their code). The plan is to build a faux document object and window object, complete with interfaces and values the library checks for when creating a canvas. Instead, I\'ll return wrapped instances of OffscreenCanvas, maybe with Proxy Traps, and catch any calls by the library to APIs I haven\'t stubbed out and build adaptors for those to canvas. Because TensorFlow does not ever return canvas elements to be drawn to the screen, the only overhead I\'ll have to worry about is sending the data from video into the worker to be processed. Because I\'ll be doing this with ImageBitmap, those updates will be zero-copy transferable objects (low latency). I suppose this is somewhat of a ""shim"" pattern and could be added to face-api.js as an adaptor or different API call if it works. For anyone else following this path expecting that a heroic effort down this rabbit hole will maybe allow you to get this to work, a few notes you should consider:TensorFlow (Google) is a massive project written in Typescript (Microsoft) which is made up of monolithic modules here:`""dependencies"": {    ""@tensorflow/tfjs-converter"": ""1.1.0"",    ""@tensorflow/tfjs-core"": ""1.1.0"",    ""@tensorflow/tfjs-data"": ""1.1.0"",    ""@tensorflow/tfjs-layers"": ""1.1.0""  }`Each of those has some dependencies of their own. You\'ll end up having to update core, then update all the other modules the depend on core, then rebuild the whole tensorflow project with the same version of typescript, which in the case of this feature set, would be ""next"" or 3.5.0+, all of which aren\'t compatible with that version out of the box (at this time). Typescript appears to be driven primarily by features in the IE/Edge browser suite because Microsoft owns that project, and TensorFlow being Google but subject to limitations in Typescript means they A) have their own blessed version of Typescript, B) this is older than whatever the most recent release is, and C) is not as up to date with the features of the web as Google\'s Chrome browser team support. Maybe, eventually, once MS moves Edge to the Chromium/Blink/Whatever engine, possibly Typescript becomes one with the universe and offers support for these DOM features in sync with minimally Chrome and Edge, but ideally all major browsers. That would be awesome! But, back to the topic, the issues I ended up seeing were related to compilation errors stemmed from code that was doing type conversion like "" float32ToTypedArray "", including some map functions/loops. Those were throwing errors at compile time, but unit tests ran fine. I was never able to get browserstack tests to run correctly, so I\'m not sure if it really did work in the browser or not. Best of luck! @ me if you want to chat!=====', ""`    function isBrowser() {        return typeof window === 'object'            && typeof document !== 'undefined'            && typeof HTMLImageElement !== 'undefined'            && typeof HTMLCanvasElement !== 'undefined'            && typeof HTMLVideoElement !== 'undefined'            && typeof ImageData !== 'undefined',    }`This is a horrible function. If someone (me) wants to fake out a library into thinking it's in a browser, don't stifle that person by doing some oddball browser check (this is not how you detect if you're in a browser) and then be really silent and confusing when the library errors. I've been working against an error i thought was in tensorflow for hours only to realize it came from face-api.js code:Error: getEnv - environment is not defined, check isNodejs() and isBrowser()Tensorflow already has browser and node checks and it's own idea of environment. Why did you guys reinvent the wheel? :|====="", "">  I've been working against an error i thought was in tensorflow for hours only to realize it came from face-api.js codeI agree, that the error message might not be the best, but by looking at the stack trace one could have figured where the error message comes from.The browser check is that complex, because we want to only initialize the corresponding environment of the library in case we are in a valid browser or nodejs environment to avoid errors at runtime. In any other case, it is up to the user to initialize the environment manually. All environment specifics can be monkey patched using `faceapi.env.monkeyPatch`.====="", ""Ok, so I'm posting this update to let everyone in this thread know that it is possible today to fool both tensorflow and face-api.js into running in a web worker and that they will run GPU accelerated, however you shouldn't get your hopes way up for perfectly jank free UX. In my app, face detection takes approximately 60ms on a MacBook Pro (Retina, 15-inch, Mid 2015), which is only processing 640x480 stills. The stills are transferred to the worker using zero copy, so they avoid the serialize/deserialize and structured copy performance hits. The app itself is only taking 1-2ms for any given RAF cycle, but visual jank is still occurring on Chrome when the worker thread takes longer than expected. I'm not even seeing any GC issues. The jank appears to happen while processing microtasks. I see bunches of timers being set. I'd have to look further into the face-api.js source to see if it's breaking apart workloads into chunks using 0ms setTimeout calls. If it is, those should be converted to Promises. Allowing the browser to handle batch processing stacks of timeouts will definitely result in slower performance if that's what's happening. Timeouts can take 2-4ms to resolve and Promises are almost immediate. I believe the details of how promise scheduling is done is still on a per browser basis, but if you're in this thread, you're today only interested in the ones that support OffscreenCanvas, and that's Chrome. Chrome handles them async. Here's the admittedly over engineered code for creating a worker environment that tensorflow and face-api.js will run in:**Parent** `\t\tvar screenCopy = {},\t\tfor(let key in screen){\t\t\tscreenCopy[key] = +screen[key],\t\t}\t\tscreenCopy.orientation = {},\t\tfor(let key in screen.orientation){\t\t\tif (typeof screen.orientation[key] !== 'function') {\t\t\t\tscreenCopy.orientation[key] = screen.orientation[key],\t\t\t}\t\t}\t\tvar visualViewportCopy = {},\t\tif (typeof window['visualViewport'] !== 'undefined') {\t\t\tfor(let key in visualViewport){\t\t\t\tif(typeof visualViewport[key] !== 'function') {\t\t\t\t\tvisualViewportCopy[key] = +visualViewport[key],\t\t\t\t}\t\t\t}\t\t}\t\t\tvar styleMediaCopy = {},\t\tif (typeof window['styleMedia'] !== 'undefined') {\t\t\tfor(let key in styleMedia){\t\t\t\tif(typeof styleMedia[key] !== 'function') {\t\t\t\t\tstyleMediaCopy[key] = styleMedia[key],\t\t\t\t}\t\t\t}\t\t}\t\tlet fakeWindow = {},\t\tObject.getOwnPropertyNames(window).forEach(name => {\t\t\ttry {\t\t\t\tif (typeof window[name] !== 'function'){\t\t\t\t\tif (typeof window[name] !== 'object' && \t\t\t\t\t\tname !== 'undefined' && \t\t\t\t\t\tname !== 'NaN' && \t\t\t\t\t\tname !== 'Infinity' && \t\t\t\t\t\tname !== 'event' && \t\t\t\t\t\tname !== 'name' \t\t\t\t\t) {\t\t\t\t\t\tfakeWindow[name] = window[name],\t\t\t\t\t} else if (name === 'visualViewport') {\t\t\t\t\t\tconsole.log('want this?', name, JSON.parse(JSON.stringify(window[name]))),\t\t\t\t\t} else if (name === 'styleMedia') {\t\t\t\t\t\tconsole.log('want this?', name, JSON.parse(JSON.stringify(window[name]))),\t\t\t\t\t}\t\t\t\t}\t\t\t} catch (ex){\t\t\t\tconsole.log('Access denied for a window property'),\t\t\t}\t\t}),\t\tfakeWindow.screen = screenCopy,\t\tfakeWindow.visualViewport = visualViewportCopy,\t\tfakeWindow.styleMedia = styleMediaCopy,\t\tconsole.log(fakeWindow),\t\tlet fakeDocument = {},\t\tfor(let name in document){\t\t\ttry {\t\t\t\tif(name === 'all') {\t\t\t\t\t// o_O\t\t\t\t} else if (typeof document[name] !== 'function' && typeof document[name] !== 'object') {\t\t\t\t\t\tfakeDocument[name] = document[name],\t\t\t\t} else if (typeof document[name] === 'object') {\t\t\t\t\tfakeDocument[name] = null,\t\t\t\t} else if(typeof document[name] === 'function') {\t\t\t\t\tfakeDocument[name] = { type:'*function*', name: document[name].name },\t\t\t\t}\t\t\t} catch (ex){\t\t\t\tconsole.log('Access denied for a window property'),\t\t\t}\t\t}`**Worker**`Canvas = HTMLCanvasElement = OffscreenCanvas,HTMLCanvasElement.name = 'HTMLCanvasElement',Canvas.name = 'Canvas',function HTMLImageElement(){}function HTMLVideoElement(){}Image = HTMLImageElement,Video = HTMLVideoElement,// Canvas.prototype = Object.create(OffscreenCanvas.prototype),function Storage () {\tlet _data = {},\tthis.clear = function(){ return _data = {}, },\tthis.getItem = function(id){ return _data.hasOwnProperty(id) ? _data[id] : undefined, },\tthis.removeItem = function(id){ return delete _data[id], },\tthis.setItem = function(id, val){ return _data[id] = String(val), },}class Document extends EventTarget {}let window, document = new Document(),\t\t\t// do terrible things to the worker's global namespace to fool tensorflow\t\t\tfor (let key in event.data.fakeWindow) {\t\t\t\tif (!self[key]) {\t\t\t\t\tself[key] = event.data.fakeWindow[key],\t\t\t\t} \t\t\t}\t\t\twindow = Window = self,\t\t\tlocalStorage = new Storage(),\t\t\tconsole.log('*faked* Window object for the worker', window),\t\t\tfor (let key in event.data.fakeDocument) {\t\t\t\tif (document[key]) { continue, }\t\t\t\tlet d = event.data.fakeDocument[key],\t\t\t\t// request to create a fake function (instead of doing a proxy trap, fake better)\t\t\t\tif (d && d.type && d.type === '*function*') {\t\t\t\t\tdocument[key] = function(){ console.log('FAKE instance', key, 'type', document[key].name, '(',document[key].arguments,')'), },\t\t\t\t\tdocument[key].name = d.name,\t\t\t\t} else {\t\t\t\t\tdocument[key] = d,\t\t\t\t}\t\t\t}\t\t\tconsole.log('*faked* Document object for the worker', document),\t\t\tfunction createElement(element) {\t\t\t\t// console.log('FAKE ELELEMT instance', createElement, 'type', createElement, '(', createElement.arguments, ')'),\t\t\t\tswitch(element) {\t\t\t\t\tcase 'canvas':\t\t\t\t\t\t// console.log('creating canvas'),\t\t\t\t\t\tlet canvas = new Canvas(1,1),\t\t\t\t\t\tcanvas.localName = 'canvas',\t\t\t\t\t\tcanvas.nodeName = 'CANVAS',\t\t\t\t\t\tcanvas.tagName = 'CANVAS',\t\t\t\t\t\tcanvas.nodeType = 1,\t\t\t\t\t\tcanvas.innerHTML = '',\t\t\t\t\t\tcanvas.remove = () => { console.log('nope'), },\t\t\t\t\t\t// console.log('returning canvas', canvas),\t\t\t\t\t\treturn canvas,\t\t\t\t\tdefault:\t\t\t\t\t\tconsole.log('arg', element),\t\t\t\t\t\tbreak,\t\t\t\t}\t\t\t}\t\t\tdocument.createElement = createElement,\t\t\tdocument.location = self.location,\t\t\tconsole.log('*faked* Document object for the worker', document),`====="", '<img width=""1551"" alt=""Screen Shot 2019-04-23 at 9 05 50 PM"" src=""https://user-images.githubusercontent.com/54642/56628967-74b4e480-6611-11e9-9c41-5acc43803f8c.png"">Here\'s what I\'m seeing btw. I\'m going to try your suggestion of looking at using a different model that might process faster. =====', '<img width=""1551"" alt=""Screen Shot 2019-04-24 at 12 42 36 AM"" src=""https://user-images.githubusercontent.com/54642/56635420-2b718e80-662b-11e9-9b99-f44ce65ab957.png"">Check this out. This is what I\'m talking about when I\'m making this correlation. When timers are used in bulk, they appear to mess up the process scheduling by spamming the event loop. Promises don\'t appear to have the same problem. Chrome bundles them up nicely and still has the ability to handle requestAnimationFrame requests. I\'d like to see if there\'s a way in face-api.js to fix the workload splitting so it doesn\'t rely on setTimeout=====', "">  I'd like to see if there's a way in face-api.js to fix the workload splitting so it doesn't rely on setTimeoutHmm, actually there are no calls to setTimeout, tf.nextFrame or requestAnimationFrame in face-api.js. Could it be, that the async behaviour you are encountering here is due to downloading data from the GPU via `tf.data()`?====="", '<img width=""1092"" alt=""Screen Shot 2019-04-24 at 2 34 49 AM"" src=""https://user-images.githubusercontent.com/54642/56640845-96c25d00-6639-11e9-823c-d521f07b4632.png"">ok, possibly disproved the timer spam theory. I\'m now pointing to the GPU work. While I was overwriting everything sacred (window, document, etc) I rewrote the setTimeout function so it uses promises and request animation frame for 0 ms setTimeouts, and falls back to setInterval for actual timers. It worked exactly how I anticipated it would, except that the jank is still present and the only thing left to point a finger at is the GPU load that\'s 2 frames long. 👎 So, for everyone watching, probably keep your GPU load in mind. It can block things just like anything else.=====', ""**Timeout replacement code**// More really bad practices to fix closed libraries. Here we overload setTimeout to replace it with a flawed promise implementation which sometimes cant be canceled.let callStackCount = 0,const maxiumCallStackSize = 750, // chrome specific 10402, of 774 in my testssetTimeout = function (timerHandler, timeout) {\tlet args = Array.prototype.slice.call(arguments),\targs = args.length <3 ? [] : args.slice(2, args.length),\tif (timeout === 0) {\t\tif (callStackCount < maxiumCallStackSize) {\t\t\tvar cancelator = {cancelable: false },\t\t\tcallStackCount++,\t\t\tnew Promise(resolve=>{\t\t\t\tresolve(timerHandler.apply(self, args)),\t\t\t}),\t\t\treturn cancelator,\t\t} else {\t\t\trequestAnimationFrame(()=>{\t\t\t\ttimerHandler.apply(self, args),\t\t\t}),\t\t\tcallStackCount = 0,\t\t\treturn,\t\t}\t} \tconst i = setInterval(()=>{\t\tclearInterval(i),\t\ttimerHandler.apply(self, args),\t}, timeout),\treturn i,},clearTimeout = (id)=>{ console.log(id), if(id && id.cancelable === false) { console.error('woops. cant cancel a 0ms timeout anymore! already ran it'), } else { clearInterval(id),} },// var x = setTimeout((x,y,z)=>{console.log(x,y,z),}, 0, 'hello', 'im', 'cassius'),// var y = setTimeout((x,y,z)=>{console.log(x,y,z),}, 1000, 'hello', 'im', 'cassius'),// clearTimeout(x),// clearTimeout(y),====="", 'Is there any other ""cleaner"" way to do this ?@justadudewhohacks you mentioned the `faceapi.env.monkeyPatch` but how does it work exactly ?I mean, lets say I have a `main.js` that only do this:```jsconst worker = new Worker(\'worker.js\'),worker.postMessage(\'foo\'),```and a worker where I want to be able to do this:```jsimport * as faceapi from \'face-api.js\',faceapi.loadFaceExpressionMode(\'assets/models/\'),onmessage = function(event) {  console.log(event),}```where and how should I use the `faceapi.env.monkeyPatch` ?The error raised atm is the following:`Uncaught (in promise) Error: getEnv - environment is not defined, check isNodejs() and isBrowser()`. =====', ' @maximeparisse you would monkey patch environment specific after importing the package. In the nodejs examples we monkey patch Canvas, Image and ImageData for example, as shown [here](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-nodejs/commons/env.ts). Refer to the [Environment](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/types.ts) type to see what can be overridden.=====', '@justadudewhohacks : Thank you for your reply, i will give a shot and give my feedbacks here in case that can help others.=====', ""@justadudewhohacks : I've tried without success to do that in a web worker. I understand how you patched the env spec in nodejs but i can't see how i can reproduce it for a web worker.====="", ""@maximeparisse face-api.js only looks for those native methods as a node server vs browser check, per my example above. There are also tfjs detections. You have to set those values inside the worker before loading the libraries in order to fool the libraries into believing they're in a browser. If they fall into the node detection block, they will fail that check too, then bail out to a null result rather than a default (browser). A good patch to apply to face-api.js would be to add worker cases and change the detection to if/elseif/elseif/else style blocks so there is always a default case and more reasonable fallbacks. This is doable, but the trick is in supporting workers for browsers other than Chrome or Firefox with the flag set to enable OffscreenCanvas.  ====="", 'Anyone manage to get this working?=====', 'Yes. Turned out the integrated gpu was the biggest insurmountable bottleneck to avoid blocking the rendering pipeline. I’d be willing to revisit this once tensorflow and this lib have been updated to allow for offscreencanvas support, which is necessary to avoid excessive monkey patching and environmental fake outs to the two libs. =====', 'Also: https://caniuse.com/#feat=offscreencanvas=====', '@jeffreytgilbert It appears TensorFlow.js now supports Offscreen Canvas... At least according to this article: https://medium.com/@wl1508/webworker-in-tensorflowjs-49a306ed60aa - does that jive with what you\'re seeing? Should face-api/tfjs ""just work"" in WebWorkers now...?=====', 'I can sadly confirm that face-api does not Just Work, even with monkeyPatch.When I do the following in my worker:```import * as faceapi from \'face-api.js\',faceapi.env.monkeyPatch({ Canvas: OffscreenCanvas })```I get:```Uncaught Error: monkeyPatch - environment is not defined, check isNodejs() and isBrowser()    at Object.monkeyPatch (index.ts:38)```I\'ve checked the `isBrowser` module, and I\'ve done a TON of monkey patching of my own BEFORE calling monekyPatch(), and got the following checks to pass *in my own code*:```// isBrowserCheck is true in my testsconst isBrowserCheck = typeof window === \'object\'&& typeof document !== \'undefined\'&& typeof HTMLImageElement !== \'undefined\'&& typeof HTMLCanvasElement !== \'undefined\'&& typeof HTMLVideoElement !== \'undefined\'&& typeof ImageData !== \'undefined\'&& typeof CanvasRenderingContext2D !== \'undefined\',```My own monkey patching is based on @jeffreytgilbert \'s example above,  with a few edits to make it compile, and I added `CanvasRenderingContext2D = OffscreenCanvasRenderingContext2D,`.Bottom line: `faceapi.env.monkeyPatch` does not even try to monkey patch because of the error above. Anyone have any suggestions on how to get this to even work? GPU or no GPU, I just want to try to get it to work. (Chrome 79 on a brand new MacBook Pro 15"", so yes, OffscreenCanvas is supported.)=====', ""Update: Got it working.How? Use this gist: https://gist.github.com/josiahbryan/770ca1a9d72f1b35c13219ba84dc0495Import it into your worker. If you have a bundler setup for your worker, just do (assuming you put it in your `utils/` folder):```javascriptimport './utils/faceEnvWorkerPatch',```Don't need to call faceapi's monkeyPatch if you use that.Fair warning: That gist is NOT pretty. It is a conglomeration of hacks and workarounds and whatever else. But it works. Face detection is working for me now in a web worker.Ideally, face-api would support a WebWorker WITHOUT having to do that horrendous hack of a monkey patch I just uploaded, but, yeah. At least this works now.  ====="", 'I found my own way of monkey patching. Pretty simple but only supports OffscreenCanvas,```faceapi.env.setEnv(faceapi.env.createNodejsEnv()),faceapi.env.monkeyPatch({    Canvas: OffscreenCanvas,    createCanvasElement: () => {        return new OffscreenCanvas(480, 270),    },}),```No need to import canvas, supports OffscreenCanvas rigidly, and seems this is the easiest valid way.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/43;[Intel GPU] - run the example but the result isn‘t right;15;open;2018-07-08T10:25:19Z;2019-05-01T02:14:51Z;I download the source code and npm the example like the  'readme.md' said, but the result isn't right.the detection draw on the canvas like the pic.why this happen?![qq 20180708182022](https://user-images.githubusercontent.com/979374/42418845-9deb796e-82db-11e8-8287-022253e17169.png);"[""What kind of gpu are you using? I tried to run this on a intel gpu once and got the exact same picture. Maybe tfjs doesn't support intel gpus.====="", 'thank you . yes , I use the intel hd gpu. maybe I should change the computer=====', 'I see, maybe we should also report this to the tfjs team at some point. Unfortunately I only have access to a intel gpu sporadically, otherwise I could try to debug step by step, which operations are causing to return inconsistent results.=====', 'I will try to do it. but I should know the consistent results first.=====', 'I had the exact same issue and fixed it by changing the default GPU for the browser to my Nvidia GPU. Nvidia has decided to disable the GPU by default for Chrome and Firefox and use the Intel HD GPU. Source: https://superuser.com/questions/645918/how-to-run-google-chrome-with-nvidia-card-optimus=====', '@marine008 @justadudewhohacks @thexiroy Recently, I had investigated some precision issues on Intel GPUs. I had tested this issue on different Intel GPUs, but could not reproduce it.Testing platforms were as below,Intel(R) UHD Graphics 630 + win10Intel(R) HD Graphics 530 + win 10Intel(R) HD Graphics 630 + win 10Intel(R) UHD Graphics 630 + Ubuntu 18.10Intel(R) HD Graphics 530 + Ubuntu 17.10Intel(R) HD Graphics 630 + Ubuntu 18.04I want to know which platform could reproduce this issue. Was an old GPU on which you produced this issue? Thank you.=====', '@xhcao Intel(R) HD Graphics 4600 + win10=====', ""Hi, I'm from the TensorFlow.js team (which face-api.js uses). Can someone let me know if you can reproduce these precision problems using the latest tf@1.0.2? Thank you!====="", ""Latest version of face-api.js now runs on tfjs-core 1.0.3. The initial issue occured in the ssd mobilenet v1 face detector. One can simply verify if it's working now by running the face detection example, the ssd face detector is selected by default. So if any Intel GPU user would give that a try, that would be nice.====="", ""Thanks! We recently got a Lenovo Yoga X1 Windows laptop with integrated Intel HD 520 GPU.I just cloned the face-api.js repo, ran the examples and couldn't reproduce the problem.====="", 'Thanks. I could not reproduce this issue on Intel(R) HD Graphics 4600 + win 10 platform. Do you know the root-cause?=====', ""TF.js went through a lot of changes (packing, better memory layout/indexing) so it's hard to tell what exactly helped with numerical stability, but we do know that the `Haswell` chipset (Intel Graphics 4600) was the one that had numerical issues. It seems to me that we can close this issue.====="", '@dsmilkov I get access to an Intel GPU next week, I will double check if everything works as expect and if so will close here.=====', '@dsmilkov I could verify, that the SSD mobilenet model, which was subject of this issue, now works as expected on an Intel gpu.I noticed some precision differences between amd and intel gpu though, which can be seen in the output of the landmark detection model:### AMD:![landmarks-amd-gpu](https://user-images.githubusercontent.com/31125521/56611749-e5103580-6612-11e9-8c5b-29b24cf4cae6.png)### Intel:![landmarks-intel-gpu](https://user-images.githubusercontent.com/31125521/56611750-e5a8cc00-6612-11e9-8508-fc21ce9b376e.png)Not sure if the info helps, but except of an intitial regular tf.conv2d the model is composed of depthwise seperable convolutions, followed by a fully connected layer (tf.matMul) at the end.=====', ""Thanks. That's good feedback. If you find some extra time, would love if you can diff the outputs after each internal operation (activations) between the amd and the intel gpu. I'm curious to see if the difference start to occur after a specific op, or they slowly drift over multiple different ops.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/15;Safari performance is poor;7;open;2018-06-21T21:04:48Z;2020-03-22T17:05:42Z;"There is a known bug with tensorflow.js on Safari that keeps it from performing accurately (on mobile and Desktop). The good news is it was just recently fixed https://github.com/tensorflow/tfjs-core/pull/978 The changes are merged into master but not released as a new version yet.Here's what the difference looks like in practice. Chrome:<img width=""575"" alt=""screen shot 2018-06-21 at 17 00 33"" src=""https://user-images.githubusercontent.com/157106/41745516-2bba6652-7575-11e8-89ad-5a1adf5a30bf.png"">Safari:<img width=""582"" alt=""screen shot 2018-06-21 at 17 00 27"" src=""https://user-images.githubusercontent.com/157106/41745524-3272a522-7575-11e8-9917-bf627155a79e.png"">";"['I changed this dependency: `""@tensorflow/tfjs-core"": ""0.11.9""`And now it works on Desktop Safari, the results are almost exactly the same as Chrome:<img width=""590"" alt=""screen shot 2018-06-22 at 12 03 34"" src=""https://user-images.githubusercontent.com/157106/41786870-abeb50d0-7614-11e8-923f-f6cdae42d3d7.png"">Results on iOS Chrome and Safari are the same as well.Edit: sorry, I might have spoken too soon. Most of the time I run this demo on Safari and Chrome on mobile it crashes the page. When it does work, looking at the inspector I see:```[Error] Unhandled Promise Rejection: RangeError: Maximum call stack size exceeded.\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:995)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:1012)\tstep (face-api.js:294)\t(anonymous function) (face-api.js:268)\tinitializePromise```That last error repeats a few times.=====', ""Hmm okay, is that actually a tfjs issue and it's simply a matter of updating the tfjs-core version once they released a fix?====="", ""yes, tfjs-core is now updated (version 0.11.9 has the fix for safari).however it looks like there's still a bug inside face-api.js that's keeping it from running correctly on iOS (that's the trace\xa0i pasted above).update: i just tested the latest release of the code (with the quantized weights) and 0.11.9, and this bug is still there. it worked the first time, but crashes the page after i reload.====="", 'Hmm okay, maybe I get the chance to test it on safari some time.=====', 'Have you guys found any solution to make it work on ios mobile ?=====', ""I didn't, but if anyone has access to an IOS device + safari and can figure out, which ops cause issues on safari, we could report an issue at tfjs-core.====="", 'Do you have any plans to merge to new tfjs wasm backend? The facemesh seems to be a good place to start?https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html=====']"
https://github.com/justadudewhohacks/face-api.js/issues/7;Face Recognizer Input Size;10;open;2018-06-09T17:50:34Z;2019-10-14T13:02:08Z;Allow smaller sizes of face image inputs then 150 x 150.;"[""Hi Vincent,Curious how this can be achieved ? I thought that for the dlib's face recognizer model it is mandatory to provide the image size to be 150x150.See https://github.com/davisking/dlib/blob/master/examples/dnn_face_recognition_ex.cpp#L44Regards & thanksKapil====="", 'Hey kapil, I think the model has just been trained on 150x150 images but you can still pass on images of smaller dimensions. I also used 105x105 images sometimes in face-recogition.js. It doesnt perform as good as 150x150 but still it is possible.=====', 'Ok. I had made the assumption that we can not even pass an image smaller than 150x150. Thanks for the clarification.=====', 'For some reason the dlib net crashed for me when using images smaller than 105x105, so that was the smallest I was able to use.=====', ""Hi Vincent,I'm wondering, will you implement MobileNetV2 to retrain the face recognition models?Best,====="", 'I was thinking about training a tiny face recognition model at some point, but not sure if I get to do that anytime soon.Why do you think mobilenet v2 would be a good fit for that?=====', ""The MobileNetv2's results lie on the curve more than MobileNetv1 onehttps://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet====="", 'Hmm, but the current face recognition net is not a mobilenet, it is a simple resnet. Or did you mean the SSD MobileNet v1 face detector?=====', 'You are right on the SSD, my bad.=====', 'hi VincentWhich size gives the best performance? =====']"
https://github.com/justadudewhohacks/face-api.js/issues/5;Face Detector Batch Input;1;open;2018-06-09T17:47:41Z;2020-09-23T04:56:15Z;Apply score filtering and non max suppression to all input images in the post processing layer. Currently only the first image of the input batch is returned.;"[""Hi, how close is this feature to being finished? It would be very helpful for a project I'm working on.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/829;[ Electron ] Cannot import faceApi   Uncaught TypeError: this.util.TextEncoder is not a constructor;3;closed;2021-11-12T12:34:22Z;2021-11-16T19:33:45Z;I am trying to integrate it with electron+angluar application, it is working as expected in browser , but not working in electron ,I'm getting this error ```tf-core.esm.js:17 Uncaught TypeError: this.util.TextEncoder is not a constructor    at new t (tf-core.esm.js:17)    at Module.6636 (tf-core.esm.js:17)    at __webpack_require__ (bootstrap:19)    at Module.4687 (main.js:153594)    at __webpack_require__ (bootstrap:19)    at Module.1858 (detail.module.ts:13)    at __webpack_require__ (bootstrap:19)    at Module.7882 (shared.module.ts:15)    at __webpack_require__ (bootstrap:19)    at Module.158 (main.js:174358)````I know it is something related to tenserflow but I dont know solve it.;['@DOCSPLOIT  I am facing the same issue, did you find a solution ?=====', 'Yes I found it, you can either use node version and use ipc events from electron to angular or by puting the faceapi source as asset and include in `angular.json` and use it directly in angular, best of luck :crossed_fingers: :=====', '> > > Yes I found it, you can either use node version and use ipc events from electron to angular or by puting the faceapi source as asset and include in `angular.json` and use it directly in angular, best of luck 🤞 :Thanks for the feedbacks, appreciate it.=====']
https://github.com/justadudewhohacks/face-api.js/issues/827;Analysing images hosted on other domains;2;closed;2021-11-10T02:00:55Z;2021-11-10T19:59:49Z;I'm getting `SecurityError: The operation is insecure.` because I'm trying to analyse an image that's on my asset server (another domain). Is this the browser throwing the error or a limitation of `face-api.js`? :);"['when you load an image from an external domain, browser marks canvas as tainted and doesnt allow any user app to read canvas data  workaround is to setup a proxy to load such images or find a browser plugin that overrides browser setting (it cannot be done in user-mode apps, but plugins have near-root access so it can be done)  =====', ""Thanks @vladmandic, very helpful. I've decided to go down the route of a small node express app on my server that can process images locally and store the metadata for use later. I've switched over to your `Human` library. Very nice work there :)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/824;Bug-UnhandledPromiseRejectionWarning - OOM when allocating tensor with shape[1,256,256,64];21;closed;2021-09-24T21:02:28Z;2021-09-26T18:12:20Z;Hi,I tried to insert 10000 record to DB in loop using this link and got this error after 300 records.https://github.com/vladmandic/face-api/blob/master/demo/node-image.js```2021-09-24 23:59:17.493963: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[1,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu(node:21876) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 8Message: OOM when allocating tensor with shape[1,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu    at Object.<anonymous> (<anonymous>)```;"[""In production I will have same problem ? What can I do ?This the relevant code:``` const tensor = tf.tidy(() => { // create tensor from image data    const data = tf.tensor(Array.from(imageData?.data || []), [canvas.height, canvas.width, 4], 'int32'), // create rgba image tensor from flat array and flip to height x width    const channels = tf.split(data, 4, 2), // split rgba to channels    const rgb = tf.stack([channels[0], channels[1], channels[2]], 2), // stack channels back to rgb    const reshape = tf.reshape(rgb, [1, canvas.height, canvas.width, 3]), // move extra dim from the end of tensor and use it as batch number instead    return reshape,  }),```====="", ""you're using explicit functionality from <https://github.com/vladmandic/face-api>  so create issues there.tensor should never have shape `[1,256,256,64]` (and that is massive, thus OOM), it should be `[1,width,height,4]`. which means that your input `imageData` is somehow corrupt before it reaches this part. cant know for sure if i don't know how it was created up to that point.====="", ""At least two major things- you're writing to file and not waiting for write to complete before later reading it. Use writeFileSync- you're recreating faceapi class and reloading weights on each image. Not only extremely costly, but likely will cause issues sooner or later. On a major note, for diagnostics learn to check your interim results before posting issue. E.g., what is your canvasObj.width and height? Is it even valid. What about imageData? Etc. And for production, *always* check if any interim output is valid before passing it on to next function. You're basically running blind and when error happens, you don't know where it all started.====="", ""And a major suggestion - since you don't need canvas to draw anything, so how about avoid using it completely. Take a look at another demo that does native JPG decoding to tensor directly and without external library. ====="", 'And why are you writing to file and then reading it back when you already have buffer in memory.There is sooo much wrong with this.=====', '> And why are you writing to file and then reading it back when you already have buffer in memory.> > There is sooo much wrong with this.I need to keep this file for other system to show this file.=====', ""> * you're recreating faceapi class and reloading weights on each image. Not only extremely costly, but likely will cause issues sooner or later.How I can avoid it ?====="", ""A) you can keep it, but you don't have to re-read it immediately.B) how can you avoid recreating and reloading weights? By doing that during app startup and keeping it. That's as basic as it gets. Entire section 2 should be once per app, not once per image. ====="", ""> On a major note, for diagnostics learn to check your interim results before posting issue. E.g., what is your canvasObj.width and height? Is it even valid. What about imageData? Etc.> > And for production, _always_ check if any interim output is valid before passing it on to next function. You're basically running blind and when error happens, you don't know where it all started.Fixed:)====="", ""> A) you can keep it, but you don't have to re-read it immediately.> B) how can you avoid recreating and reloading weights? By doing that during app startup and keeping it. That's as basic as it gets. Entire section 2 should be once per app, not once per image.Only pass Entire section 2  to startup, where i keep it ? can you show me how?====="", ""No, NOT like this.And I can't show you how. That is basic app state management for JavaScript. I don't want to write the app for you, learn basics. From now on, I'll only reply to specific questions regarding Faceapi and only if diagnostics info has been provided, not just code copy&paste.====="", ""Sorry I understood you now.To create  this file and to called it from startup and from each register or compare? is it```const tf = require('@tensorflow/tfjs-node'),const path = require('path'),const canvas = require('canvas'),const faceapi = require('@vladmandic/face-api'),async function loadModals() {      faceapi.env.monkeyPatch({ Canvas: canvas.Canvas, Image: canvas.Image, ImageData: canvas.ImageData }),      const faceDetectionNet = new faceapi.FaceDetectionNet(),      const modelPath = '../../node_modules/@vladmandic/face-api/model',      await faceDetectionNet.loadFromDisk(path.join(__dirname, modelPath)),      await faceapi.nets.ssdMobilenetv1.loadFromDisk(path.join(__dirname, modelPath)),      await faceapi.nets.ageGenderNet.loadFromDisk(path.join(__dirname, modelPath)),      await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname, modelPath)),      await faceapi.nets.faceRecognitionNet.loadFromDisk(path.join(__dirname, modelPath)),      await faceapi.nets.faceExpressionNet.loadFromDisk(path.join(__dirname, modelPath)),}module.exports = { loadModals}```and called it from start up:app.listen(config.serverPort, async () => {          //.Load models  await  loadModals(),}and same from register and compare .====="", ""> And a major suggestion - since you don't need canvas to draw anything, so how about avoid using it completely. Take a look at another demo that does native JPG decoding to tensor directly and without external library.Which demo ? I looked on your demo and didn't see- please provide a link:)====="", ""A) No. Take a step back and think. Look what is CONSTANT vs what is CHANGING and should be inside `register`. In your case, `optionsSSDMobileNet` that gets created in step 5 should really be part of step 2 and entire step 2 should be be placed somewhere where its executed ONCE on app startup. Only thing that matters is that `optionsSSDMobileNet` is visible from within `register` method so it can be used from there. DO NOT RECREATE IT FOR EVERY IMAGE.B) You did not look. Function is even marked with comments:> // read image from a file and create tensor to be used by faceapi> // this way we don't need any monkey patchesNext time when you have  a question, step back  Think about it and research it a bit  Search for solutions before posting issues on GitHub  Post issues in a correct repository  GitHub Issues is for issues, not for learning how to code - Only if its an issue, then post  Youre close of being blocked  ====="", ""Hi,Thank a lot for your answers.It is very appreciated here :)I fixed all your remarks, and run again the app from vs code with 1000 and after 827 inserts it crash better than before 300 inserts. I insert **same** image every time.You said:tensor should never have shape [1,32,32,512] (and that is massive, thus OOM), it should be [1,width,height,4]. which means that your input imageData is somehow corrupt before it reaches this part. cant know for sure if i don't know how it was created up to that point.> > > 26/09/2021 07:51:15:647 faceRecognition.ts - info : register: Create tensor done successfully(tensor shape:1,403,716,3, tensor size:865644> > 2021-09-26 07:51:15.809551: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[1,32,32,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu> > (node:12320) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 8> > Message: OOM when allocating tensor with shape[1,32,32,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu> >     at Object.<anonymous> (<anonymous>)![image](https://user-images.githubusercontent.com/1079689/134794241-8d080f92-a3b2-48e2-adde-f327859555fa.png)====="", 'no actual information provided.=====', 'I used this code to register new faces into DB:https://github.com/vladmandic/face-api/blob/master/demo/node-image.jsMy installed ram is 16,GB , window 10 ,Processor: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz   2.71 GHz.I used `node-cashe` to keep the descriptors.=====', ""a) you DON'T USE my code - don't just provide link to it!!! you have your own code based on my code. so provide that.b) there is no diagnostic info provided that i mentioned several times before! what are interim states of variables. LOG!!!====="", ""STILL NO DIAG OUTPUT - LOG YOUR OPERATIONS. sorry for yelling, but i've said this how many times so far?What are the values of `tensor` or `imageData`And what is method `image.getImageData`? It's not here. Post full code on **GIST** not inline in issue as it's impossible to track after a while.And post LOG there as well.And when reporting error, execute as simply as possible. You're noting that `VSCode` runs out of memory. Don't - it impacts how NodeJS performs garbage collection. Make sure this error is reproducible from NodeJS directly.====="", '1.Logs:26/09/2021 19:37:18:616 faceRecognition.ts - info : register: Load models done successfully26/09/2021 19:37:18:620 faceRecognition.ts - info : register: Create image data done successfully(image path:D:\\images\\2021.09.26\\12838.jpeg, width:135, height:24026/09/2021 19:37:18:635 faceRecognition.ts - info : register: Create tensor done successfully(tensor shape:1,240,135,3, tensor size:972002.imageData`const imageData = image.getImageData(canvasObj), // read decoded image data from canvas     if(!imageData){       let createImageDataMessage = ""register: Create image data failed"",       utils.logger.log({         level: \'error\',         message: createImageDataMessage,         label: \'faceRecognition.ts\'       }),       return { success: false, error:createImageDataMessage }     }3. I will test this code NodeJS directly.=====', 'Step back. Do something else. Come back tomorrow. Re-read the thread and POST RELEVANT INFORMATION.What are the values of `tensor` or `imageData` ? Log them during execution and POST LOG!And still no idea what is `image.getImageData(canvasObj)` Where is code for that?Use GIST.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/823;What is the difference between minConfidence && distanceThreshold ?;1;closed;2021-09-24T19:02:43Z;2021-09-25T16:57:53Z;Hi,What is the difference between minConfidence && distanceThreshold ?Thanks,``` const ssdOptions = { minConfidence: 0.1, maxResults: 10 },      const optionsSSDMobileNet = new faceapi.SsdMobilenetv1Options(ssdOptions), // create options object      const faces = await faceapi // run detection        .detectAllFaces(tensor, optionsSSDMobileNet)        .withFaceLandmarks()        .withFaceExpressions()        .withFaceDescriptors()        .withAgeAndGender(),`let matcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.5),````;"[""`minConfidence` determines if face is face at all during detection - if detected face is below that level, it will not be analyzed at all and you don't get things like descriptor as a result.  `distanceThreshold` is threshold used when comparing two different faces.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/821;Compare two difffrent people with beards;6;closed;2021-09-23T12:03:45Z;2021-09-29T13:11:28Z;Hi,I register first picture and compare with second picture and get there are same ?First:![image](https://user-images.githubusercontent.com/1079689/134503082-9d669f91-e208-4416-8744-07c71ce52566.png)Second:![image](https://user-images.githubusercontent.com/1079689/134503189-21914c14-98a1-4b3d-85ea-5c97e1285c23.png)Why ? Is it possible ? How I overcome the issue ?Thanks,;"[""Because like I mentioned previously, in your code you're determining descriptor of an image, **NOT** descriptor of a face - and those two images do have large overall similarity in foreground, background and object itself. Only when face is detected and cropped and descriptor is determined per-face, you see large differences.Also, there is no such thing as **SAME** - there is a calculated distance factor and user-defined threshold. So what is the distance and if your threshold appears too high so you get false-positives, lower it.====="", 'Yes I change from 0.6 to0 0.5 the threshold  and it works:)let matcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.5),=====', 'And why this picture  not in DB and return label of exists face.![image](https://user-images.githubusercontent.com/1079689/134737241-0cde723f-cf2e-4a18-9506-cffd0dec48bc.png)=====', ""I wrote it 3 times so far - you are not comparing faces at all, you are comparing images!Go to previous comments to see what I'm talking about. ====="", 'I used this link :https://github.com/vladmandic/face-api/blob/master/demo/node-image.jsto register an image => comparing faces and although last face not in DB and return label of exists face.=====', 'With the link above it works very well -:)=====']"
https://github.com/justadudewhohacks/face-api.js/issues/820;Compare with a lot of pictures is not working well?;2;closed;2021-09-22T17:03:57Z;2021-09-25T08:59:12Z;"Hi,I tested compare on few images and it works, when i tested on 6500 picturesI get : what do I miss? ```{    ""sucesss"": true,    ""bestMatchPerson"": {        ""_label"": ""unknown"",        ""_distance"": 0.653006573156487    }}```Why ?";['When I used this link :I used this link :https://github.com/vladmandic/face-api/blob/master/demo/node-image.jsIt work very well -:)=====', 'Thanks=====']
https://github.com/justadudewhohacks/face-api.js/issues/819;Use npm-cache to load/get labeledFaceDescriptors is not working;1;closed;2021-09-21T08:38:43Z;2021-09-24T14:45:41Z;"Hi,I used npm-cache, not DB as manage my data.On start up server of node js I loading:`singeltonCache.set(""BbtFaceMatcher"", labeledFaceDescriptors),`On compare:```let labeledFaceDescriptors = singeltonCache.get(""BbtFaceMatcher""),let matcher = new faceapi.FaceMatcher(labeledFaceDescriptors),const bestMatchPerson =matcher.findBestMatch(singleResult.descriptor),```I got error when making findBestMatch:`TypeError: Method get TypedArray.prototype.length called on incompatible receiver [object Object]`What do I miss? When I debugged the code this the value of matcher:![image](https://user-images.githubusercontent.com/1079689/134256191-8b076362-bfef-4f36-b4a5-8d4aa6b57ecc.png)Thanks,";"[""I used useClones: false and solved the problem.var nodeCache = require('node-cache'),//https://www.npmjs.com/package/node-cachehttps://www.npmjs.com/package/node-cachemodule.exports = new nodeCache({ stdTTL: 0, checkperiod:0, useClones: false }),=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/817;Load image take too much time;54;closed;2021-09-14T06:26:56Z;2021-09-27T05:40:42Z;Hi,I used this face-api.js (https://github.com/justadudewhohacks/face-api.js) and I have to load 10000 images when the node server is loaded for future comparison operations.This the function that I load when server is loaded, it take 0.5 second to one picture, how can I improve it ?```**// fetch first image of each class and compute their descriptorsasync function createBbtFaceMatcher() {  await faceDetectionNet.loadFromDisk(path.join(__dirname, '../weights'))  await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname, '../weights'))  await faceapi.nets.faceRecognitionNet.loadFromDisk(path.join(__dirname, '../weights'))  const labeledFaceDescriptors = await Promise.all(classes.map(    async className => {      let descriptors: any = [],      let uri = getFaceImageUri(className, 1),      const img = await canvas.loadImage(uri),      if (img) {        let descriptor = await faceapi.computeFaceDescriptor(img),        if (descriptor) {          descriptors.push(descriptor),        }      }      return new faceapi.LabeledFaceDescriptors(        className,        descriptors      )    }  )) ```       return new faceapi.FaceMatcher(labeledFaceDescriptors)}**;"[""you havent said which *backend* you're using - since it's `nodejs` solution, i'm assuming `tfjs-node`?  anyhow, *javascript* is not a platform to run compute intensive operations concurrently - and you're triggering all `faceapi` detections inside a promise - don't do that, run simple loop instead  also, note that this original version of `face-api` is using hard-coded embedded version of `tfjs` 1.7 which actually uses `tensorflow.so` version 1.15 for execution in `tfjs-node` backend which is quite old  you may want to try newer port of `face-api` that uses `tfjs` 3.9 which relies on `tensorflow.so` 2.6 which implements more accelerated functions  ====="", ""switching to loop doesn't make it faster - after all, it's the same code. it does make it use far less memory and less chances of race scenarios leading to crashes while still running with the same performance  ====="", '@vladmandicThanks for answer.How can I speed the load of 100000 images in few seconds instead of half hour now?=====', '10,000=====', ""i just checked using your code (modified to work - it would help if you post fully working code when asking question) on my home server with i5-6500TE, so very low power and it's ~75ms per image, far from 0.5sec you're seeing.what's your hardware and what's the backend you're using (`console.log(tf.getBackend())`)?```jsconst fs = require('fs'),const canvas = require('canvas'),const faceapi = require('./dist/face-api.node.js'),async function main() {  faceapi.env.monkeyPatch({ Canvas: canvas.Canvas, Image: canvas.Image, ImageData: canvas.ImageData }),  const faceDetectionNet = new faceapi.FaceDetectionNet(),  await faceDetectionNet.loadFromDisk('model'),  await faceapi.nets.faceLandmark68Net.loadFromDisk('model'),  await faceapi.nets.faceRecognitionNet.loadFromDisk('model'),  const dir = fs.readdirSync('../human/samples/people'),  const descriptors = [],  const t0 = performance.now(),  for (const f of dir) {    const img = await canvas.loadImage(`../human/samples/people/${f}`),    const c = canvas.createCanvas(img.width, img.height),    const ctx = c.getContext('2d'),    ctx.drawImage(img, 0, 0, img.width, img.height),    const descriptor = await faceapi.computeFaceDescriptor(c),    if (descriptor) descriptors.push(descriptor),  }  const t1 = performance.now(),  console.log('time:', t1 - t0, 'average:', (t1 - t0) / dir.length, 'descriptors:', descriptors.length),}main(),``````logtime: 1687.4630840420723 average: 76.70286745645784 descriptors: 22```====="", ""a) what's the backend used? i'm only assuming it's the correct one `tjfs-node`b) try newer port of `faceapi` which uses tfjs 3.9.0 and tensorflow 2.6: [@vladmandic/face-api](https://github.com/vladmandic/face-api)  (i've been maintaining it for the past year since original is no longer maintained)====="", 'a)yes""dependencies"": {    ""@tensorflow/tfjs-core"": ""^0.13.11"",    ""@tensorflow/tfjs-node"": ""^0.1.17"",=====', ""Hi,I download  it throgh npm (@vladmandic/face-api)const faceapi1 = require('../../node_modules/@vladmandic/face-api/dist/face-api.node.js'),and modified the sample ?and got this error:(node:24184) UnhandledPromiseRejectionWarning: TypeError: tf8.io.weightsLoaderFactory is not a function    at FaceLandmark68Net.loadFromDisk (D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\@vladmandic\\face-api\\src\\NeuralNetwork.ts:106:31)    at ObWhat do i miss?====="", ""remove `@tensorflow/tfjs-core` as it's already included in `tfjs-node` and install **latest** `@tensorflow/tfjs-node` - you have obsolete version 0.1.17, latest is 3.9.0====="", ""I removed the core and installed the lates version of tenserflow and got this error:What do i miss ? * Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-build'node-pre-gyp' is not recognized as an internal or external command,operable program or batch file.====="", ""that means your `nodejs` on widows is not the best - it cannot run binary bindings as part of package installation process  imo, i'd reinstall `node` and `npm` and make sure that `node-gyp` compiler is installed `npm i -g node-gyp`  and make sure it's at least node v14, but i'd recommend v16====="", 'I installed node-pre-gyp + installed  lates version of tenserflow.=====', ""did you load `tf` before `faceapi`? new version requires tf to be loaded first as it cannot bind to binary distribution and it would be bad to do a fake bind.add `const tf = require('@tensorflow/tfjs-node'),` at the start, before you load `faceapi`====="", 'Now I uninstall both libraries and install tf before faceapi.=====', 'still same error=====', ""I said load, not just install? Can you show the code where you're loading both tf and faceapi? ====="", 'I got this error : on thus line:const descriptor = await faceapi1.computeFaceDescriptor(c),PS D:\\Camera\\Backup Face Recognition\\1\\face-recognition> npm run startDebugger attached.> face-recognition@1.0.0 start> ts-node-dev ./src/app.tsDebugger attached.[INFO] 19:00:19 ts-node-dev ver. 1.1.8 (using ts-node ver. 9.1.1, typescript ver. 4.4.2)Debugger attached.Platform node has already been set. Overwriting the platform with [object Object].cpu backend was already registered. Reusing existing backend factory.2021-09-14 19:00:22.463783: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.14/09/2021 19:00:22:503 app.js - info : Server start running at http://localhost:606014/09/2021 19:00:22:975 app.js - info : Server start listening at port:606014/09/2021 19:00:22:977 app.js - info : Server is start loading bbt face matchertensorflow(node:7716) UnhandledPromiseRejectionWarning: TypeError: forwardFunc is not a function    at D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:586:31          at D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:424:20          at Engine.scopedRun (D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:435:19)=====', ""you just posted the same app code - where is the load part?  e.g., where are your `require` or `import` statements?  judging by the log, you're loading tfjs twice and causing conflict  ====="", 'why would you assign function to a global variable? and i have no idea how your project is structured and what is a *controller* you mention - but anyhow, that is not a `face-api` question  my advise? refactor your code not to use global variables ever  =====', '1.I want to load all images when server init , and later consume it in other routes ?How do you refactor it? I used for compare activity.I tried to used node-cache instead, I create singelton from it to share but without success.With global is working.TypeError: Method get TypedArray.prototype.length called on incompatible receiver [object Object]2.What does _distance mean ?```{     ""bestMatchPerson"": {        ""_label"": ""unknown"",        ""_distance"": 0.7806719517998486    }}```3. Do i need both ""@vladmandic/face-api"": ""^1.5.2"", + ""face-api.js"": ""^0.22.2"", in pacage json ?4.  You said:(i\'ve been maintaining it for the past year since original is no longer maintained)What do you mean face-api.js is no longer maintined ?=====', ""1. i get the goal, but dumping stuff into `global` is always a bad idea. anyhow, your choice2. distance is euclidean distance. take a look at <https://github.com/vladmandic/human/wiki/Embedding#face-similarity> on how it's calculated.3. no, only one, never both. loading both causes conflicts you've already seen.4. correct. see notes at <https://github.com/vladmandic/face-api#note>====="", 'why are you importing with strange relative paths?```jsfaceapi = require(\'../../node_modules/@vladmandic/face-api/dist/face-api.node.js\'),```can be just```jsfaceapi = require(\'@vladmandic/face-api\'),```or if you want to be specific```jsfaceapi = require(\'@vladmandic/face-api/dist/face-api.node.js\'),```never import installed modules with relative paths inside `node_modules`and enclose your js code in markdown code quotes so it\'s actually readable - i cannot read this\'```js...```\'> Your package is maintained ?Yes. Just check git comit history to for any package to see when is the last update and how often it receives updates.> When i removed ""face-api.js"": ""^0.22.2"", from pacage json i got this error, i do not used in code at all.> [ERROR] 11:01:21 Error: Cannot find module \'@tensorflow/tfjs-core\'Something in the code is still referencing `@tensorflow/tfjs-core`, but it\'s no longer installed  I\'d clean `node_modules` and re-run `npm install` and then check both your code and `package-json.lock` file to see where is it referenced  My `face-api` for nodejs references just `@tensorflow/tfjs-node` package> Regading Eucliean distance, when i can say us good match and when is not ?Match is never 100%, so think of Euclidean distance as `chance of match`  What is a threshold for a good match? That is a personal choice - try what values are good for you as it highly depends on input images  =====', '2. Can you please give example of:a. When node server is loaded than load all images(10,000 is reasonable) using createBbtFaceMatcher  and insert into best data structure(not global variable as you said.b. When i register a new image is update the images data structure for opertaion in c.c. When i compare it used the images data structure for best match=====', ""3. I fixed to use: `const faceapi = require('@vladmandic/face-api')`====="", '5.I markdowned code quotes in all comments as you asked-:)6.I used these weights wjen working with  face-api.js, i need to change it to other files ?![image](https://user-images.githubusercontent.com/1079689/133630920-d8247f23-3f86-40f0-8dfd-9a73cfef8c94.png)=====', '> If i got ""_distance"": 0.7 (above) what does it mean and if I got 0.3(below) what does it mean ?distance 0 means identical and 1 means completely different. default threshold for matching is distance < 0.6btw, just choose one model, don\'t test for both  `ssd` is typically better than `tiny` in everything, `tiny` is included only for very low power devices> Can you please give example of:> a. When node server is loaded than load all images(10,000)> (createBbtFaceMatcher ) insert into best data structure(not global variable).> b. When i register a new image is update the images data structure for opertaion in c.> c. When i compare it used the images data structure for best matchthis is a general architecture question, perhaps this discussion is close to what you\'re looking for: <https://github.com/vladmandic/human/discussions/138> and <https://github.com/vladmandic/human/discussions/145>also see notes in <https://github.com/vladmandic/face-api/issues/51> for general json saveand notes on on multi-process analysis to fully utilize available hardware <https://github.com/vladmandic/face-api/issues/20>10,000 images is not a huge number, but it\'s getting there and it\'s worth it to architect right - you don\'t want to keep 10k objects in global variable ever. and what if it grows? solution should be such that you can switch to db store if needed so there is no need to ever load all in-memory=====', '1. How do I insert the 10k object to sql db?(createBbtFaceMatcher ), I defined coulum as json typeand later insert record for all labeledFaceDescriptors or for each labeledFaceDescriptor ?2. When I register i need to add new record to db and on compare i need to load all t from db  10k records is it fast ?  and later do?```let  descriptorFromDb = go to db and bring all descriptor,let faceMatcher = new faceapi.FaceMatcher(descriptorFromDb ), const singleResult = await faceapi        .detectSingleFace(referenceImage)        .withFaceLandmarks()        .withFaceDescriptor()   const bestMatchPerson =  global.faceMatcher .findBestMatch(singleResult.descriptor)```=====', ""> How do I insert the 10k object to db?(createBbtFaceMatcher ), how i defined in db ?That is far beyond this conversation. For a proper solution, I suggest using `mongodb` module  For a prototype, storing object as on disk is a start (`fs.writeFileSync('db.json', JSON.stringify(myObject))`)Anything is better than a) calculating them again-and-again on each startup, b) keeping in global variable> When I register i need to add new record to db and on compare i need to load all t from db 10k records is it fast ?`findBestMatcher` is simply a for loop through that runs matches for all records and returns one with lowest distance - not more and not less  So if you read records from DB yourself and pass them to match method them, get the same thing without needing to have all records in memory all the time. Of course, you don't want to read neither all records nor one-by-one, it should have some sane disk paging  But again, this is far beyond this conversation.====="", ""@vladmandic Thank you for you answers.But I don't understand your final solution if you please elaborate ? ====="", ""architecture is a personal choice. plus solution architecture is far beyond this scope  im sharing here a short writeup, but please don't go further with architecture questions - lets limit the scope to library issues  first, i suggest using an actual database. you might start with `nedb-promises` which allows you to easily switch to `mongodb` in the future if database grows a lot  structure is at least two different object tables: images and faces (image can have more than one face, don't assume it's always just one)  on server startup1. initialize database and pass handle to other module (such as module that handle uploads) so they can share access2. enumerate image files and for each check if its in the db or if its modification time is different than one in db3. if necessary process image and store file descriptors to faces object table and map from which file it came from4. store file details in files object tablethen on how to handle uploads1. on image file upload, trigger steps 3 and 4 from above2. run face match on all records to get best match if above threshold3. based on the best match find image file and return it to clientoptionally1. run file system watcher so it automatically finds new or modified files so you don't need to restart server to process additional images  2. use worker process pool for maximum performance so you can process as many images in parallel as you have cpu cores  =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/799;Performance optimization;19;closed;2021-05-29T19:02:11Z;2021-09-02T17:52:39Z;It takes usually 30-40 Seconds to detect faces. Is there any way I can do optimization and get results in 10-15 seconds???  ;"['it never takes 30-40 sec to detect faces, it\'s pretty much sub 1sec.now, if *initial* detection takes long time, that is most likely due to your configurating using `webgl` backend and `tfjs` takes some time to ""warmup"" the model (compile `gles` shaders and upload weighs as textures to gpu). but after initial detection, anything afterwards is really fast.on the other hand, if you used `wasm` backend, warmup doesn\'t exist, but actual detection is quite a lot of slower than using `webgl` (but still well below 1sec threshold).=====', 'What backend would your recommend for CPU optimization if we wanted to do detectallface with expression and not gender and age=====', ""it's less of a question which models are execution, more of a question where are they executed.each backend has it's pros and cons:- tfjs-node-gpu:  - pro: absolutely fastest  - con: can run in backend only, really messy compatibility matrix with nvidia cuda- tfjs-backend-webgl:  - pro: fast if client has a discrete gpu  - con: slow if client does not have a discrete gpu, slow initial startup, startup causes ui blocking- tfjs-backend-wasm:  - pro: fast startup, does not require gpu  - con: client must load wasm binaries from a trusted source so cannot use cdn easily, only mediocre performance at best, requires enabling browser simd flag on each client or performance is poor====="", ""I am using face-api on react and it's face-api older version I think which using  tensorflow 1 version. Should I change to @vladmandic/face-api then it can make performance improvements???? I have notice it shows very slow performance on mobile, mostly users will access this through mobile.====="", 'no, update is for compatibility reasons. any performance improvements are minor (~5-10%).=====', 'what changes should I do to boost its performance. In my case it is taking 30-40 seconds, here is my node_modules for face_api```""@mapbox/node-pre-gyp"": ""^1.0.4"",""@tensorflow/tfjs-core"": ""1.7.0"",""@tensorflow/tfjs-node"": ""1.7.0"",""face-api.js"": ""^0.22.2"",""canvas"": ""2.6.1"",```=====', 'we are going back and forth with generic statements - boost performance from what? on what platform?  30-40sec noted in your original post is NOT detect time, i already wrote that.  be as specific as possible. list exact times for each step. list your actual configuration. version of packages alone says nothing.  then i may be able to help.  =====', 'you still did not post exact times. nor did you post your configuration. i don\'t even know which backend are you using.i just tried using ""original sample"" link on my android based mobile phone and loading and warmup is ~2sec with any subsequent detection <0.2sec.and your site has soo many elements that face-api is lost between all the noise. not to mention that looking at minified code is not something i would do.i give up :(=====', 'My project backend is Nodejs with linux environment and frontend I have impliment on javascript. I am trying to get face LANDMARK .I don\'t know how to figure out time for each steps.. face-api works on tensiorflow v1 so it workis on olxder version of Node. I am using Node Version 8.          ```       ""@tensorflow/tfjs-core"": ""1.7.0"",       ""@tensorflow/tfjs-node"": ""1.7.0"",       ""canvas"": ""^2.7.0"",        ""face-api.js"": ""^0.22.2"",        faceapi.nets.ssdMobilenetv1.loadFromUri(path)        faceapi.nets.faceLandmark68Net.loadFromUri(path)        faceapi.nets.faceRecognitionNet.loadFromUri(path)        faceapi.nets.tinyYolov2.loadFromUri(path)       const mypic = document.getElementById(\'userimg\'),       const detections = await faceapi.detectAllFaces(mypic).withFaceLandmarks()```         I am accessing original image which is user uploads for face landmark detecttion, normally it is 960x1280.        =====', ""Well, I don't know why, but for me, the FIRST time, right after it opens my camera, it takes about 30 seconds to find the first face (even if I'm sitting still right in front of it), then, it works perfectly.Watching the logs, it's NOT related to loading resources...I pinpointed it to this peace of code:```console.log(1),faceapi.detectSingleFace(      video,      new faceapi.TinyFaceDetectorOptions({ minConfidence: 0.5 })    )      .withFaceLandmarks()    .then(...) // console.log(2),    .catch(...) // console.log(3),```I see the `1` in the log right away...then, about 30 seconds later I see the `2`.I call the function again when `then` is triggered...from this time on, it works smoothly.Tried it on a macbook pro in chrome and firefox.Interestingly enough...the samples/demos run as expected (really quickly).I'm rendering it on a react component but as far as my logs show, it's not being rendered or processed more times than it should.====="", ""@felipenmoura What you're talking about is called **warmup** and it heavily depends on the backend you're using  For example, `WASM` has fast warmup, but slower inference (meaning first frame is faster, but then every other frame is not that great) while `WebGL` has much slower warmup (that is most likely what you're seeing), but then inference for each frame is faster than using `WASM` backend  This is common for every TensorFlow model, not specific to FaceAPI at all  On a side-note, don't create `Options` object each time, do it outside of the loop during component initialization and then re-use it. That doesn't have impact on 30sec delay you're seeing, but will have significant benefit to overall performance  ====="", 'Hm I see. Interesting, thanks.Is there a way I can speed up this first process? I mean, is there a way I can swap/decide between wasm and webgl?=====', ""@felipenmoura simply initialize tensorflow with appropriate backend before calling `face-api` and `face-api` will use whatever is set:```jsimport * as tf from '@tensorflow/tfjs',import '@tensorflow/tfjs-backend-webgl',tf.setBackend('webgl'),await tf.ready(),```or```jsimport * as tf from '@tensorflow/tfjs',import * as wasm from '@tensorflow/tfjs-backend-wasm',wasm.setWasmPaths('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.8.0/dist/'),tf.setBackend('wasm'),await tf.ready(),```note that `tfjs` version and version of WASM binaries *must* match version used by `face-api`  (which in case of this original version is quite old, it's 1.7.0, definitely not latest 3.8.0)  or use an up-to-date port like <https://github.com/vladmandic/face-api>====="", ""Awesome, thanks.I see some interesting parts of documentation over there, that I hadn't seen yet!I did NOT have tensorflow installed as dependency...does it mean it was actually using my GPU/CPU regular cycles to process it?I'm now trying to build it using the updated repo you linked, thanks.====="", ""`FaceAPI` is built using TensorFlow/JS no matter what:- This version of `FaceAPI` has TFJS 1.7.0 embedded, that's why you don't have external dependencies- Newer version of `FaceAPI` has both embedded and non-embedded version and is based on TFJS 3.8.0  Now, TFJS can use different backends to execute actual ML operations:- CPU: ops are executed as interpreted JS code on CPU- WASM: ops are executed on CPU using compiled WASM library (thus the need to download WASM binary)- WebGL (Browser only): ops are executed on GPU using compile-on -the-fly GLSL code (thus there is a longer warmup period as GL code needs to be compiled for specific GPU and model weights uploaded to GPU as shaders)- TensorFlow (NodeJS only): ops are executed on CPU using compiled tensorflow library- TensorFlow-GPU (NodeJS only): ops are executed on GPU using compiled tensorflow library, but with help of CUDA libraries for GPU acceleration- WebGPU: New experimental GPU backend that will eventually replace WebGL====="", 'Awesome, thanks for this complete reply.I managed to start using wasm and the warmup period is perceivably faster :)=====', 'Hey there...sorry for pinging here again, but it seems like there\'s something I\'m missing!I\'m trying to dynamically import the dependencies only if the user selects the ""selfie"" option.It works, but it never uses wasm!```js        Promise.all([          import(\'@tensorflow/tfjs\'), // dynamic import          import(\'@tensorflow/tfjs-backend-wasm\') // dynamic import        ]).then(async ([tf, wasm]) => {          window.tf = tf,          wasm = wasm,          window.wasm = wasm,          wasm.setWasmPaths(\'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.9.0/dist/\'),          await tf.setBackend(\'wasm\'),          await tf.ready(),          await createScript(\'/face-detection-ia/face-api.min.js\', scrId), // imports the script from /public          faceapi = window.faceapi, // tried this to see if anything would change...but nope          try {            await Promise.all([              faceapi.nets.tinyFaceDetector.loadFromUri(\'/face-detection-ia/models/\'),              faceapi.nets.faceLandmark68Net.loadFromUri(\'/face-detection-ia/models/\'),            ]), // this also loads all the models OK            console.log(\'DONE loading AI stuff\'),            console.log(faceapi.tf.getBackend()), // webgl <<<<<----- !!!!!!!!!          } catch (error) {            console.error(\'ERROR loading AI stuff\', error),            return,          }        }),      }```It IS loading everything. It IS waiting for everything to be ready. It DOES NOT trigger any error...but it still uses webgl and has quite a poor performance and long warmup.Any idea what I might be missing here?> By the way...I had to do all that because I\'m using next.js and its SSR insists on trying to render it in the backend, requiring many other dependencies...I\'m trying to avoid that by ensuring it ONLY runs on client side.> By the way 2 ... My `@tensorflow/tfjs`\'s version is 3.9.0 as well as the `tfjs-backend-wasm`\'s version. Is this what you meant about matching versions?=====', ""> My @tensorflow/tfjs's version is 3.9.0 as well as the tfjs-backend-wasm's version. Is this what you meant about matching versions?No, that's smaller part of it. Important part is that `@tensorflow/tfjs-backend-wasm` and your wasm binary set by `wasm.setWasmPaths` are of the same version - which it looks like they are.> By the way...I had to do all that because I'm using next.js and its SSR insists on trying to render it in the backend,  > requiring many other dependencies...I'm trying to avoid that by ensuring it ONLY runs on client side.you need to tell `next.js` not to mess with dependencies  for example, this is `next.config.js` i've tested a while back:```jsmodule.exports = {  webpack: (config) => {    if (config.target === 'web') config.externals = [ 'fs', 'os', 'util' ],    return config,  },}```loading WASM modules via `next.js` is tricky at best.if you want to dynamically import wasm and not have SSR, i don't see where is that set?in the past i've done something like this:```jsconst wasm = dynamic(  () => import('@tensorflow/tfjs-backend-wasm'),  { ssr: false })```and that should be in the component init, not calling `faceapi` immediately afterwards - how would you then use `faceapi` for the second time? load modules again? > but it still uses webgl and has quite a poor performance and long warmup.after `tf.ready()`, do a `console.log(tf.getBackend()),` to see what's the actual backend usedbtw, this is no longer related to original issue - why not create a new one, 'how to use faceapi with wasm in next.js`and if you're using new `faceapi` (which it seems you are since you're loading wasm 3.9.0), open an issue in that git repository.====="", 'Sure thing.Thanks, I just created the issue over there:https://github.com/vladmandic/face-api/issues/65=====']"
https://github.com/justadudewhohacks/face-api.js/issues/798;Version of NodeJS for face-api?;1;closed;2021-05-28T14:09:20Z;2021-05-29T19:07:26Z;Hi I have installed Node version 16 it throws some method depreciated issue of tensorflow but It works well in Node 8.12.0, Is someone know what is the max version of Node I can use for this..? ;['this original version of `face-api` has not been updated in over a year, so no wonder there are issues with latest node.try newer port of `face-api` which works with both new versions of tfjs, typescript and nodejs (yes, nodejs 16 is tested):<https://github.com/vladmandic/face-api>=====']
https://github.com/justadudewhohacks/face-api.js/issues/797;faceapi is not defined ;1;closed;2021-05-24T06:58:42Z;2021-05-27T17:57:00Z;"const pkg = require(""../package.json"")const BASE_URL = process.env.BLUR_SRC || `https://unpkg.com/${pkg.name}/src`,let loadModel = false,let faceapi: anyconst loadScript = (src: string) => {  return new Promise(function (resolve, reject) {    const s = document.createElement('script'),    s.src = src,    s.onload = resolve,    s.onerror = reject,    document.head.appendChild(s),  }),},const loadfile = async () => {  console.log(""load script""),  console.log(`${BASE_URL}/models/face-api.min.js`)  await loadScript(`${BASE_URL}/models/face-api.min.js`),  console.log(""check""),  loadModels(),}const loadModels = async () => {  console.log(faceapi),  console.log(faceapi.nets),  await faceapi.nets.tinyFaceDetector.loadFromUri('/models'),  console.log(""check2""),   }**Uncaught (in promise) TypeError: Cannot read property 'nets' of undefined**";"[""first, what is this code? require is not a browser function.  and post more complete output, not just promise rejection.  anyhow, at the very least, you need `script.async=false` or `onload` may trigger too early.  but even then, *loaded* is not the same as *complete* (for *iife* scripts that means executed and registered in global namespace), you should also check for `readyState`. it's close, but not exact. dynamically loading iife scripts is not a great thing to do in general.  =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/794;TypeError: t.toFloat is not a function TensorFlow with React;1;closed;2021-05-12T15:47:54Z;2021-05-24T06:59:05Z;trying to run face-api.js on react appNetInput.js:139 Uncaught (in promise) TypeError: t.toFloat is not a function    at NetInput.js:139    at Array.map (<anonymous>)    at NetInput.js:139    at engine.js:327    at Engine.scopedRun (engine.js:337)    at Engine.tidy (engine.js:326)    at Module.tidy (globals.js:175)    at NetInput../face-api.js/node_modules/tfjs-image-recognition-base/build/es6/dom/NetInput.js.NetInput.toBatchTensor (NetInput.js:123)    at SsdMobilenetv1.ts:30    at engine.js:327;"[""`* is not a function` for any tensorflow function pretty much always comes down to tensorflow backend not registered correctly. which again comes down to multiple incompatible versions of tensorflow being used.  if you're using `tfjs` 2.x or 3.x, you'll end up with this because original `face-api` has not not been updated for a long time and includes embedded `tfjs` v1 which is NOT compatible with tfjs 2.x or tfjs 3.x.that's main reason why this port exists: <https://github.com/vladmandic/face-api>=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/788;detect landmarks in node js;1;closed;2021-05-01T14:35:46Z;2021-05-03T08:53:58Z;I am trying to detect facial landmarks in node js. My approach was the following:```const { createCanvas, loadImage } = require('canvas'),const MODEL_URL = `${__dirname}/public/models/`,await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODEL_URL),await faceapi.nets.faceLandmark68Net.loadFromDisk(MODEL_URL),await faceapi.nets.faceRecognitionNet.loadFromDisk(MODEL_URL),//create canvas for the faceapiconst width = wmin,const height = hmin,var images = [outputImg1, outputImg2], // 2 images from a local foldervar fullFaceDescriptions,for (var i = 0, i < 2, i++) { //create a canvas for each image    const canvas = createCanvas(width, height),    const context = canvas.getContext('2d'),    loadImage(__dirname + '/' + images[i]).then(image => {         context.drawImage(image, 0, 0), //draw image    }),    fullFaceDescriptions = await faceapi.detectSingleFace(canvas).withFaceLandmarks(),}```But I get the following error and I can't understand how to call the faceapi for an image stored in a local folder: Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id;"[""since you're already using `canvas` package, just tell `face-api` to use it as well as there is no native canvas support in `NodeJS`:```jsconst canvas = require('canvas'),const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Image, Canvas, ImageData })```or alternatively, convert your canvas to tensor and pass that tensor to `faceapi`, i wrote several examples how to do that recently.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/783;faceapi  dose not support on Angular ;1;closed;2021-04-19T20:01:42Z;2021-06-20T08:10:21Z;I run hand pose API from TF and want to run faceapi.js together, I occurred error on faceapi. please support angular and support TF core .js 3 and do not embed TF core on face api js;['part of exact reasons why this port exists: <https://github.com/vladmandic/face-api>=====']
https://github.com/justadudewhohacks/face-api.js/issues/782;TypeError: forwardFunc is not a function （React);3;closed;2021-04-19T07:20:05Z;2021-04-20T07:37:10Z;"I want to use faceapi to detect faces in webcam, but there is a problem with title 'TypeError: forwardFunc is not a function'.![截屏2021-04-19 下午3 17 20](https://user-images.githubusercontent.com/33031512/115196325-5c093780-a122-11eb-9523-3f7efb3af23f.png)here is the code```javascriptimport React, { ReactElement, useState, useEffect, useRef } from 'react'import { Row, Col, Button } from 'antd',import * as tf from '@tensorflow/tfjs',import './style.css',import * as faceapi from 'face-api.js',const TestFace = (): ReactElement => {  // state  const [displaySize, setDisplaySize] = useState<{ width: number , height: number,}>({ width: 1280 , height: 1270})  // refs  const videoEl = useRef<HTMLVideoElement>(null),  const canvasEl = useRef<HTMLCanvasElement>(null),  const captureEl = useRef<HTMLCanvasElement>(null),  // effects  useEffect(() => {    async function load() {      await loadFaceApiModels(),      loadCam(),    },    load(),  }, []),  const loadFaceApiModels = async () => {    console.log('loading faceapi'),    await faceapi.loadTinyFaceDetectorModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights'),    await faceapi.loadSsdMobilenetv1Model('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights'),    await faceapi.loadFaceLandmarkModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights'),    await faceapi.loadFaceRecognitionModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights'),    console.log('loaded faceapi'),      }  const loadCam = () => {    navigator.mediaDevices.getUserMedia({ video: { width: displaySize.width, height: displaySize.height}})      .then(stream => {        setIsLoading(false),        if (videoEl.current) {          videoEl.current.srcObject = stream,        }      })      .catch(err => {        console.error(err),      })  }  const onPlay = async (): Promise<void> => {    if (videoEl.current) {      const detection = await faceapi.detectSingleFace(videoEl.current), // where error cames      console.log(detection),    }  }  return (    <>      <Row        align=""middle""        justify=""center""      >        <Col>          <div>              <>                <video ref={videoEl} width=""1280"" height=""760"" onPlay={onPlay} muted playsInline autoPlay />                <canvas id=""overlay"" ref={canvasEl}></canvas>                <canvas id=""capture"" width=""224"" height=""224"" ref={captureEl}></canvas>              </>          </div>        </Col>      </Row>    </>  )}export default TestFace,```";"[""you are importing both tfjs and face-api:```jsimport * as tf from '@tensorflow/tfjs',import * as faceapi from 'face-api.js',```face-api has embedded version of tfjs 1.x and latest tfjs is 3.x and they are anything but compatible.  if you don't explicitly need tf, don't import it.but if you do, use newer port of face-api that is compatible with newer tfjs: <https://github.com/vladmandic/face-api>====="", '@vladmandic  Thanks for reply! I need `tf.js` to use my own model to predict the result,  I tried `https://github.com/vladmandic/face-api` , but in the scene to detect a face wearing a mask ,the `@vladmandic/face-api` have a worse performance than `face-api.js`. =====', ""ah, i saw that one - those are related.let's figure that one out.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/779;Webp support on node;8;closed;2021-04-15T04:51:04Z;2021-04-16T01:28:44Z;Currently based on node-canvas's arbitrary restrictions, it's not possible to use an optimized image format like webp when doing face recognition. There are some workarounds I've looked at that claim they work with webp like https://github.com/node-gfx/image but because face-api has checks on things like the `Image` constructor with `image instanceof Image` this separate library isn't usable.My best guess for this then would be converting webp to a tensor instead of a canvas. I looked around and found https://www.tensorflow.org/io/api_docs/python/tfio/image/decode_webp but of course that's available in the python api and not node. Whats the correct way to deal with this? I don't want to deoptimize my images if possible but right now it's looking like that's the only reasonable way to do things.;"[""seems that `@canvas/image` package supports `webp` via `@cwasm/webp`  and it does have a `getImageData` method  so load the image and convert it to tensor in a correct format```jsconst imageData = canvas.getImageData(0, 0, canvas.width, canvas.height),const tensor = tf.tidy(() => {  const data = tf.tensor(Array.from(imageData.data), [canvas.height, canvas.width, 4], 'int32'), // create rgba image tensor from flat array  const channels = tf.split(data, 4, 2), // split rgba to channels  const rgb = tf.stack([channels[0], channels[1], channels[2]], 2), // stack channels back to rgb  const reshape = tf.reshape(rgb, [1, canvas.height, canvas.width, 3]), // move extra dim from the end of tensor and use it as batch number instead  return reshape,}),```====="", 'I\'m confused where I would use this, it seems like this is for serializing a canvas with an image into a webp output but I\'m looking for the opposite to get a webp buffer into a canvas I can use for face recognition.```tsconst image = await imageFromBuffer(webpBuffer),const detections = await faceapi  .detectAllFaces(image)  .withFaceDescriptors(),```I figured that what I was originally not passing in the custom `Image` into ```tsimport { Image } from ""@canvas/image""import { Canvas, ImageData } from ""canvas""faceapi.env.monkeyPatch({ Image, Canvas, ImageData })```But canvas still doesn\'t recognize it as a correct `Image` format (`TypeError: Image or Canvas expected`) which could be an issue with `node-canvas` and not faceapi https://github.com/Automattic/node-canvas/issues/1258Sorry if my question wasn\'t clear.=====', ""what i wrote is `webp` (assuming it's already loaded) to `tensor` - and you can just pass that tensor to `face-api````jsconst detections = await faceapi.detectAllFaces(tensor).withFaceDescriptors(),```and to load `webp` image, it's `canvas.src = 'some-path.webp'` and then execute everything else in `canvas.onload` event  (`canvas` here is instance of `@canvas/image`, not `canvas`)pretty sure `monkeyPatch` is not even necessary in that case at all, could be just to make `face-api` happy with a defined object, but it's pretty much completely unused  ====="", ""end-to-end, just tried and works fine without the need for `canvas` or `monkeyPatch`  ```jsconst fs = require('fs'),const tf = require('@tensorflow/tfjs-node'),const image = require('@canvas/image'),const faceapi = require('@vladmandic/faceapi'),const modelPath = 'model/',const imageFile = 'demo/sample1.webp',const ssdOptions = { minConfidence: 0.1, maxResults: 10 },async function main() {  await faceapi.nets.ssdMobilenetv1.loadFromDisk(modelPath),  const optionsSSDMobileNet = new faceapi.SsdMobilenetv1Options(ssdOptions),  const buffer = fs.readFileSync(imageFile),  const canvas = await image.imageFromBuffer(buffer),  const imageData = image.getImageData(canvas),  console.log('image:', imageFile, canvas.width, canvas.height),  const tensor = tf.tidy(() => {    const data = tf.tensor(Array.from(imageData.data), [canvas.height, canvas.width, 4], 'int32'), // create rgba image tensor from flat array    const channels = tf.split(data, 4, 2), // split rgba to channels    const rgb = tf.stack([channels[0], channels[1], channels[2]], 2), // stack channels back to rgb    const reshape = tf.reshape(rgb, [1, canvas.height, canvas.width, 3]), // move extra dim from the end of tensor and use it as batch number instead    return reshape,  }),  console.log('tensor:', tensor.shape, tensor.size),  const result = await faceapi.detectAllFaces(tensor, optionsSSDMobileNet)  console.log('results:', result),}main(),```====="", 'I wasn\'t using your fork of face-api but I decided to give it a shot because I had `@tensorflow/tfjs-node` pinned to `1.7.4` because of face-api compatibility issues. I tried this using the following code```tsimport * as tf from ""@tensorflow/tfjs-node"",import { getImageData, imageFromBuffer } from ""@canvas/image"",import * as faceapi from ""@vladmandic/face-api"",import {  WithFaceDescriptor,  WithFaceLandmarks,  FaceDetection,} from ""@vladmandic/face-api"",export async function detectFaces(buf: Buffer): Promise<Detection> {  const canvas = await imageFromBuffer(buf),  const optionsSSDMobileNet = new faceapi.SsdMobilenetv1Options({    minConfidence: 0.1,    maxResults: 10,  }),  const imageData = getImageData(canvas),  if (!imageData) {    throw Error(""No image data found""),  }  const tensor = tf.tidy(() => {    const data = tf.tensor(      Array.from(imageData.data),      [canvas.height, canvas.width, 4],      ""int32""    ), // create rgba image tensor from flat array    const channels = tf.split(data, 4, 2), // split rgba to channels    const rgb = tf.stack([channels[0], channels[1], channels[2]], 2), // stack channels back to rgb    const reshape = tf.reshape(rgb, [1, canvas.height, canvas.width, 3]), // move extra dim from the end of tensor and use it as batch number instead    return reshape,  }),  console.log(""tensor:"", tensor.shape, tensor.size),  const detections = await faceapi    .detectAllFaces(tensor, optionsSSDMobileNet)    .withFaceLandmarks()    .withFaceDescriptors(),  return detections,}```But I\'m running into `TypeError: forwardFunc is not a function`. I tried clearing `node_modules` and making sure `tfjs` and `tfjs-node` versions are matching```    ""@canvas/image"": ""^1.0.1"",    ""@tensorflow/tfjs"": ""3.3.0"",    ""@tensorflow/tfjs-node"": ""3.3.0"",    ""@vladmandic/face-api"": ""1.1.12"",    ""canvas"": ""2.7.0"",```I tried to also set `tf.setBackend(\'cpu\')` explicitly as per https://github.com/tensorflow/tfjs/issues/3883 but that didn\'t seem to change the error either.Starting to wonder if wastefully converting the image back to jpeg is a better approach at this point haha=====', ""i tried your code (mostly) as-is and it works:just had to add actual model loading:```js  const modelPath = 'node_modules/@vladmandic/face-api/model/',  await faceapi.nets.ssdMobilenetv1.loadFromDisk(modelPath),  await faceapi.nets.faceLandmark68Net.loadFromDisk(modelPath),  await faceapi.nets.faceRecognitionNet.loadFromDisk(modelPath),```and then just invoked it at the end with```jsconst buf = fs.readFileSync('test.webp'),detectFaces(buf),```so question is how are you compiling/bundling this (as it's a ts syntax)?i've tried with```shelltsc src/test.ts --outDir dist/```and ```shellesbuild --bundle --format=cjs --platform=node --external:@tensorflow --target=es2018 src/test.ts >dist/test.js```and both work finealso, check if you get a sane output from```js  console.log(faceapi.version, tf.version.tfjs, faceapi.tf.version.tfjs, faceapi.tf.getBackend()),```====="", ""Ahh, I didn't realize I was running the `monkeyPatch` method which broke tensorflow somehow, seems like it's working now! Thank you so much for taking the time to look into this 🙏 much appreciated.====="", ""You're welcome 😁=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/777;Can we use the trained weights with python?;2;closed;2021-04-10T04:45:33Z;2021-04-12T10:46:10Z;;"[""face-api does not use prebuild models, they are implemented in tfjs inside face-api itself.  so weights are only that - weights. you'd need to recreate models manually in python.or use alternative prebuilt models that have same functionality, for example take a look at <https://github.com/vladmandic/human/wiki/Models>of course, you'd need to implement output parsing yourself - for some models it's trivial and for some not so much.====="", 'Thanks @vladmandic=====']"
https://github.com/justadudewhohacks/face-api.js/issues/769;Face detection stops when back/forwarding on video (React);12;closed;2021-03-15T14:41:51Z;2021-03-16T15:51:39Z;"Hi! I've written this piece of code to have a video player with some basic functions: go 5 seconds backward-forward, play/pause buttons, and a checkbox to show/hide the face-api canvas with its detections.```import {React, useRef} from ""react"",import './App.css',import * as faceapi from ""face-api.js/dist/face-api.min.js"",function App() {    let container = useRef(),    let video = useRef(),    let modelsLoaded = false,    let checkboxChecked = false,    let canvas,    let displaySize,    function backward(){        video.current.currentTime -= 5,    }    function forward(){        video.current.currentTime += 5,    }    function play(){        video.current.play(),    }    function pause(){        video.current.pause(),    }    function checkboxChanged() {        checkboxChecked = !checkboxChecked,        if(checkboxChecked === true)            loadModels(),    }    function loadModels(){        if(modelsLoaded === false){            console.log(""Loading models..."")            Promise.all([                faceapi.nets.tinyFaceDetector.loadFromUri('./models'),                faceapi.nets.faceLandmark68Net.loadFromUri('./models'),                faceapi.nets.faceRecognitionNet.loadFromUri('./models'),                faceapi.nets.faceExpressionNet.loadFromUri('./models')            ])                .then(() => {                    console.log(""Models loaded""),                    modelsLoaded = true,                    createCanvas(),                    drawCanvas(),                })                .catch((err) => console.log(err)),        }        else{            drawCanvas(),        }    }    function createCanvas(){        canvas = faceapi.createCanvasFromMedia(video.current),        canvas.style.position = ""absolute"",        container.current.append(canvas),        displaySize = { width: video.current.videoWidth, height: video.current.videoHeight },        faceapi.matchDimensions(canvas, displaySize),    }    async function drawCanvas(){        if(checkboxChecked){            canvas.style.display = ""block"",            const detections = await faceapi                .detectAllFaces(video.current, new faceapi.TinyFaceDetectorOptions())                .withFaceLandmarks()                .withFaceExpressions(),            const resizedDetections = faceapi.resizeResults(detections, displaySize),            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height),            faceapi.draw.drawDetections(canvas, resizedDetections),            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),            faceapi.draw.drawFaceExpressions(canvas, resizedDetections),            requestAnimationFrame(drawCanvas),        }        else{            canvas.style.display = ""none"",        }    }    return (        <>            <div ref={container} id={""face-api-container""} style={{display: ""flex""}}>                <video ref={video} id={""video""} controls autoPlay={true} muted={true} src=""sintel.mp4""></video>            </div>            <button id={""backward_btn""} onClick={backward}>Bwd 5s</button>            <button id={""forward_btn""} onClick={forward}>Fwd 5s</button>            <button id={""play_btn""} onClick={play}>Play</button>            <button id={""pause_btn""} onClick={pause}>Pause</button>            <br/>            <input type={""checkbox""} onChange={checkboxChanged}            />face-api        </>    ),}export default App,```This works fine, despite a slight framerate drop in the video while face-api is enabled. When I press the backward (or forward) button, though, the face detection stops: the canvas stops being updated. Curiously, this does not happen with the play and pause buttons.How should I change my code in order to prevent this behaviour and keep the face-api canvas updated?Thanks in advance.";"[""when you move through the video, it takes a bit of time for buffering and preparing first available frame  and you're drawing immediately which is basically a null value and everything goes wrong  before grabbbing a frame, you should check for `video.readyState > 2`  <https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/readyState>====="", '> before grabbbing a frame, you should check for `video.readyState > 2`Hello! Where in the code should I make this check? The problem is that, if I press the backward/forward button, the program stops looping the `drawCanvas()` function.=====', ""there are better solutions, but this would be quick & easy - inside your `drawCanvas` function have that if statement just before calling `faceapi.detectAllFaces` so if it's false, it basically skips entire faceapi processing block and immediately triggers `requestAnimationFrame`.  since `requestAnimationFrame` is frame-limited, its ok.also extend the if with null check and paused check - something like```jsif (video && !video.paused && video.readyState > 2) {  const detections = await faceapi  ...}requestAnimationFrame(drawCanvas),```but on the other hand, you're calling `drawCanvas` immeditely when models are loaded and it's luck that loading of models takes longer than preparing of video - you should call that from `video.onloadeddata` event instead to make sure you don't even trigger `drawCanvas` before video is ready.====="", 'The addition of the `video.readyState` check made it work, together with a little adjustement of the previous code. This is the final result:```import {React, useRef, useEffect} from ""react"",import \'./App.css\',import * as faceapi from ""face-api.js/dist/face-api.min.js"",function App() {    let container = useRef(),    let video = useRef(),    let modelsLoaded = false,    let checkboxChecked = false,    let canvas,    let displaySize,    function backward(){        video.current.currentTime -= 5,        checkLoadModels(),    }    function forward(){        video.current.currentTime += 5,        checkLoadModels(),    }    function play(){        video.current.play(),    }    function pause(){        video.current.pause(),    }    function checkboxChanged() {        checkboxChecked = !checkboxChecked,        checkLoadModels(),    }    function checkLoadModels(){        if(checkboxChecked === true){            loadModels(),        }    }    function loadModels(){        if(modelsLoaded === false){            console.log(""Loading models..."")            Promise.all([                faceapi.nets.tinyFaceDetector.loadFromUri(\'./models\'),                faceapi.nets.faceLandmark68Net.loadFromUri(\'./models\'),                faceapi.nets.faceRecognitionNet.loadFromUri(\'./models\'),                faceapi.nets.faceExpressionNet.loadFromUri(\'./models\')            ])                .then(() => {                    console.log(""Models loaded""),                    modelsLoaded = true,                    createCanvas(),                    drawCanvas(),                })                .catch((err) => console.log(err)),        }        else{            drawCanvas(),        }    }    function createCanvas(){        canvas = faceapi.createCanvasFromMedia(video.current),        canvas.style.position = ""absolute"",        container.current.append(canvas),        displaySize = { width: video.current.videoWidth, height: video.current.videoHeight },        faceapi.matchDimensions(canvas, displaySize),    }    async function drawCanvas(){        console.log(""drawCanvas""),        if(checkboxChecked && video.current && video.current.readyState>2){            canvas.style.display = ""block"",            const detections = await faceapi                .detectAllFaces(video.current, new faceapi.TinyFaceDetectorOptions())                .withFaceLandmarks()                .withFaceExpressions(),            const resizedDetections = faceapi.resizeResults(detections, displaySize),            canvas.getContext(\'2d\').clearRect(0, 0, canvas.width, canvas.height),            faceapi.draw.drawDetections(canvas, resizedDetections),            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),            faceapi.draw.drawFaceExpressions(canvas, resizedDetections),        }        else{            canvas.style.display = ""none"",        }        requestAnimationFrame(drawCanvas),    }    return (        <>            <div ref={container} id={""face-api-container""} style={{display: ""flex""}}>                <video ref={video} id={""video""} controls autoPlay={true} muted={true} src=""sintel_excerpt.mp4""></video>            </div>            <button id={""backward_btn""} onClick={backward}>Bwd 5s</button>            <button id={""forward_btn""} onClick={forward}>Fwd 5s</button>            <button id={""play_btn""} onClick={play}>Play</button>            <button id={""pause_btn""} onClick={pause}>Pause</button>            <br/>            <input type={""checkbox""} onChange={checkboxChanged}            />face-api        </>    ),}export default App,```I did not need the `!video.paused` check at all, since it is ok to display the face-api canvas while the video is paused.Thank you very much! If you do not have further suggestions, I\'ll be closing the issue 😃=====', ""re: `video.paused` - yes, it's ok to display canvas while paused, but you don't want to run processing again and again on the same still frame. and since if `video.paused` you'd be also skipping call to `clearRect` so last frame will stay displayed.but anyhow, glad it helped :)off-topic: i'm maintaining newer fork of `face-api.js`, optimized for ES2018 and TensorFlow 3.x  <https://github.com/vladmandic/face-api>====="", '> you don\'t want to run processing again and again on the same still frameYou\'re right. I fixed that, it should be fine now:```async function drawCanvas(){    console.log(""drawCanvas""),    if(checkboxChecked){        if(!video.current.paused && video.current.readyState>2){            canvas.style.display = ""block"",            const detections = await faceapi                .detectAllFaces(video.current, new faceapi.TinyFaceDetectorOptions())                .withFaceLandmarks()                .withFaceExpressions(),            const resizedDetections = faceapi.resizeResults(detections, displaySize),            canvas.getContext(\'2d\').clearRect(0, 0, canvas.width, canvas.height),            faceapi.draw.drawDetections(canvas, resizedDetections),            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),            faceapi.draw.drawFaceExpressions(canvas, resizedDetections),        }    }    else {        canvas.style.display = ""none"",    }    requestAnimationFrame(drawCanvas),}```> i\'m maintaining newer fork of face-api.js, optimized for ES2018 and TensorFlow 3.xGreat! Are you going to reduce/remove that framerate drop that is perceived on the video while face-api is working?=====', ""> Great! Are you going to reduce/remove that framerate drop that is perceived on the video while face-api is working?not sure what you mean?  the fact that canvas does not update at high frame rate?  that's not a `face-api` issue, that's the way you use it. `face-api` cannot process faster than your CPU/GPU resources, but if you want to have smooth video output, then you should draw canvas in a separate loop from the processing loop and get rid of `awaits` since JS is single-threaded, so everything stops when you hit it.something like:```jslet lastKnownResizedDetections = null,function drawCanvas(){  if(!video.current.paused && video.current.readyState>2 && lastKnownResizedDetections){    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height),    faceapi.draw.drawDetections(canvas, resizedDetections),    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),    faceapi.draw.drawFaceExpressions(canvas, resizedDetections),  }  requestAnimationFrame(drawCanvas),}function processCanvas(){  if(!video.current.paused && video.current.readyState>2){    faceapi    .detectAllFaces(video.current, new faceapi.TinyFaceDetectorOptions())    .withFaceLandmarks()    .withFaceExpressions(),    .then((detections) => {      lastKnownResizedDetections = faceapi.resizeResults(detections, displaySize),      requestAnimationFrame(processCanvas),    }),  }}```this way `drawCanvas` will execute at 60FPS and `processCanvas` will update as fast as it can based on your CPU/GPU performance.of course, this is still not perfect 60FPS since JS is single-threaded, so it will time-slice between two functions.to make it even more independent, processing should be in a separate web worker thread, but that's tricky since `face-api` uses DOM objects.====="", 'How are `drawCanvas()` and `processCanvas()` linked? Are they called in the places where I call `drawCanvas()`?```faceapi.draw.drawDetections(canvas, resizedDetections),faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),faceapi.draw.drawFaceExpressions(canvas, resizedDetections),```Here, did you mean `lastKnownResizedDetections` in place of `resizedDetections`?=====', ""> How are drawCanvas() and processCanvas() linked? Are they called in the places where I call drawCanvas()?Yes, you trigger them once both in your `loadModels`> Here, did you mean lastKnownResizedDetections in place of resizedDetections?I renamed it just to make a point that it's now a global object that will update whenever it can in a second loop, but first loop can always access it. Of course, typos mean that I forgot to rename it inside calls to `faceapi.draw`====="", ""Ok, I've tried it as well but I see no improvement w.r.t. my previous code. I guess I'll leave it as before.Thank you again for your help!====="", ""one more - you don't want to create a new instance of config object for each frame: `new faceapi.TinyFaceDetectorOptions()`.  store that in a variable at the time when you're finished loading models.  ====="", 'Got it. Thanks!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/768;TypeError: forwardFunc_1 is not a function;6;closed;2021-03-13T04:12:29Z;2021-03-25T11:32:48Z;"I am new to face-api.js/Tensorflow.js so I might be overlooking something very obvious.I installed face-api.js and Tensorflow using: `npm i face-api.js canvas @tensorflow/tfjs-node`Everything worked fine until I added `require('@tensorflow/tfjs-node'),` at the top of the code.Here's the full code:```require('@tensorflow/tfjs-node'),const { loadImage,Canvas, Image, ImageData,createCanvas } = require('canvas')const fs= require('fs'),const faceapi = require('face-api.js'),faceapi.env.monkeyPatch({ Canvas, Image, ImageData,createCanvas }),Promise.all([   faceapi.nets.ssdMobilenetv1.loadFromDisk('models'),   faceapi.nets.faceRecognitionNet.loadFromDisk('models'),   faceapi.nets.faceLandmark68Net.loadFromDisk('models')   ]).then(async () => {    data={},    const image1= await loadImage(""test.png""),    const result = await faceapi.detectSingleFace(image1).withFaceLandmarks().withFaceDescriptor(),    data[""test.png""]={},    let left=result.landmarks.getLeftEye(),    data[""test.png""].left=left,    let right=result.landmarks.getRightEye(),    data[""test.png""].right=right,    console.log(data),}),```The code would have worked perfectly fine, had `require('@tensorflow/tfjs-node'),` not been added. The error I get when I run the full code is:```2021-03-13 09:27:30.500937: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].(node:1428) UnhandledPromiseRejectionWarning: TypeError: forwardFunc_1 is not a function    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3179:55    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3002:22    at Engine.scopedRun (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3012:23)    at Engine.tidy (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3001:21)    at kernelFunc (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3179:29)    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3200:27    at Engine.scopedRun (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3012:23)    at Engine.runKernelFunc (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3196:14)    at mul_ (FolderName\node_modules\face-api.js\node_modules\@tensorflow\tfjs-core\dist\ops\binary_ops.js:327:28)    at Object.mul (FolderName\node_modules\face-api.js\node_modules\@tensorflow\tfjs-core\dist\ops\operation.js:46:29)(Use 'node --trace-warnings ...' to show where the warning was created)(node:1428) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag '--unhandled-rejections=strict' (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:1428) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```Here's the list of packages installed: [list.txt](https://github.com/justadudewhohacks/face-api.js/files/6134141/list.txt). I have uninstalled/re-installed the node modules multiple times. Tried using the previous versions of ""@tensorflow/tfjs-node"".I am using Node.js v14.16.0";"['this version of `face-api.js` is not compatible with tfjs 2.0+ or 3.0+, only obsolete 1.x.  why it worked before you added `tfjs-node`? because `face-api.js` actually includes bundled version of `tfjs-core` 1.x.  once you added `tfjs-node`, it overrode global `tf` namespace, but its a much newer version and not compatible.  now, you can either install obsolete `tfjs-node` 1.x or use newer port of `face-api.js` (changes to make it compatible are pretty significant):  <https://github.com/vladmandic/face-api>  <https://www.npmjs.com/package/@vladmandic/face-api>=====', ""> this version of `face-api.js` is not compatible with tfjs 2.0+ or 3.0+, only obsolete 1.x.> why it worked before you added `tfjs-node`? because `face-api.js` actually includes bundled version of `tfjs-core` 1.x.> once you added `tfjs-node`, it overrode global `tf` namespace, but its a much newer version and not compatible.> > now, you can either install obsolete `tfjs-node` 1.x or use newer port of `face-api.js` (changes to make it compatible are pretty significant):> https://github.com/vladmandic/face-api> https://www.npmjs.com/package/@vladmandic/face-apiSo I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.====="", ""@TheRealTechWiz>So I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.That was the active version of TFJS at the time this package was last updated - if you check, you'll see it hasn't been updated in about a year. That's exactly why I've started a new fork - to modify `face-api` for TFJS 2.x and 3.x compatibility.And why are you marking the response with thumbs down? Just because you don't like what is said? I've offered a solution.====="", ""> @TheRealTechWiz> > > So I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.> > That was the active version of TFJS at the time this package was last updated - if you check, you'll see it hasn't been updated in about a year. That's exactly why I've started a new fork - to modify `face-api` for TFJS 2.x and 3.x compatibility.> > And why are you marking the response with thumbs down? Just because you don't like what is said? I've offered a solution.Thanks for the fast reply. I think the docs need to be updated. is this the fork you are referring to https://github.com/vladmandic/face-api it doesn't say forked from this repo. So i am little confused.====="", ""> Thanks for the fast reply. I think the docs need to be updated.Yes, but if the author is no longer maintaining the package, how can they be updated?> is this the fork you are referring to https://github.com/vladmandic/face-api it doesn't say forked from this repo. So i am little confused.It says that explicitly in the readme:## NoteThis is updated **face-api.js** with latest available TensorFlow/JS as the original is not compatible with **tfjs 2.0+**.  Forked from [face-api.js](https://github.com/justadudewhohacks/face-api.js) version **0.22.2** which was released on March 22nd, 2020  Currently based on **`TensorFlow/JS` 3.3.0**  *Why?* I needed Face-API that does not cause version conflict with newer versions of TensorFlow  And since original Face-API was open-source, I've released this version as well  Changes ended up being too large for a simple pull request  and it ended up being a full-fledged version on its own  Plus many features were added since original inception  <br>## DifferencesCompared to [face-api.js](https://github.com/justadudewhohacks/face-api.js) version **0.22.2**:- Compatible with `TensorFlow/JS 2.0+ & 3.0+`- Compatible with `WebGL`, `CPU` and `WASM` TFJS Browser backends- Compatible with both `tfjs-node` and `tfjs-node-gpu` TFJS NodeJS backends- Updated all type castings for TypeScript type checking to `TypeScript 4.2`- Switched bundling from `UMD` to `ESM` + `CommonJS` with fallback to `IIFE`  Resulting code is optimized per-platform instead of being universal  Fully tree shakable when imported as an `ESM` module  Browser bundle process uses `ESBuild` instead of `Rollup`- Typescript build process now targets `ES2018` and instead of dual ES5/ES6  Resulting code is clean ES2018 JavaScript without polyfills- Removed old tests, docs, examples- Removed old package dependencies (`karma`, `jasmine`, `babel`, etc.)- Updated all package dependencies- Updated TensorFlow/JS dependencies since backends were removed from `@tensorflow/tfjs-core`- Updated mobileNetv1 model due to `batchNorm()` dependency- Added `version` class that returns JSON object with version of FaceAPI as well as linked TFJS- Added test/dev built-in HTTP & HTTPS Web server- Removed `mtcnn` and `tinyYolov2` models as they were non-functional in latest public version of `Face-API`  Which means valid models are **tinyFaceDetector** and **mobileNetv1**  *If there is a demand, I can re-implement them back.*- Added `face angle` calculations that returns `roll`, `yaw` and `pitch`- Added `typdoc` automatic API specification generation during build- Added `changelog` automatic generation during build<br>## Credits- Original project: [Face-API](https://github.com/justadudewhohacks/face-api.js)- Original model weighs: [Face-API](https://github.com/justadudewhohacks/face-api.js-models)- ML API Documentation: [Tensorflow/JS](https://js.tensorflow.org/api/latest/)====="", ""Thanks. i'll switch from this to your fork. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/763;How to get detected frame when input is <video> tag;5;closed;2021-03-02T15:56:33Z;2021-03-05T15:36:18Z;Hello,I'm working on a project that detects human face in some specific angles in real-time and when everything are correct, it captures that frame.I'm able to get media stream from camera of the device and fetch in HTML video tag. I have tried two options below:1. Capturing frames continuously from video tag (by using opencv.js) and process that frames by face-api.js. The processing time of each frame is very slow on low performance devices.2. Using video tag as input of face-api.js directly. The processing time of each frame is acceptable. I decided to choose the second option. However, I don't know how to get exactly the frame of input video tag that had been processed by face-api.js to return detection results.If I capture frame from video before face-api.js processes that video. The frame will be lagging when comparing with detection results.Thanks.;"[""blindly capturing video frame takes ~2ms, so it's not really a slow operation - and you definitely don't need opencv.js for that.  now, actually reading `imagedata` from a frame easily takes ~20ms - i guess that's what opencv.js does and thats why its slow.  do this once:```jsconst video = document.getElementById('your-video-id'),const canvas = document.createElement('canvas'),canvas.width = video.videoWidth,canvas.height = video.videoHeight,const ctx = canvas.getContext('2d'),```and then redraw each frame and pass canvas object to face-api instead of passing HTMLVideoElement:```jsctx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, canvas.width, canvas.height),faceapi.detectAllFaces(canvas, ...),```it's even faster with `gl` context, but quite a lot more complex.====="", ""Thank you very much for your comment.I need to do some pre/post-processing by using opencv.js. That why I need opencv.js for capturing frames.I don't know that face-api.js accepts HTML canvas tag as input (My bad for don't read the document carefully). So I do a redundant conversion image data from canvas to img tag (to pass img tag to face-api :D). This conversion is heavy part.Now, I can capture frame by using opencv.js, do some pre-processing, drawing frame to canvas and pass the canvas to face-api. It works without any problems.On the other hand, do you think that face-api should return processing frame along detection results when passing video as input?Because I think that, in almost cases, in the end, frame is what we need, detection results just helps us decide which is correct frame to take.====="", ""> On the other hand, do you think that face-api should return processing frame along detection results when passing video as input?> Because I think that, in almost cases, in the end, frame is what we need, detection results just helps us decide which is correct frame to take.What you're asking for, I do in my own library. But `face-api.js` doesn't process input - it passes it as-is to `tfjs` which converts it to tensor. No reason why `face-api.js` wouldn't include that tensor in the output structure.Btw, not exactly what you want, but there is a helper function that can be used to convert processed tensors back to canvas, either for full image or for regions - `extractFaces`.====="", ""Haha, sorry. Please don't mind. I just want to share my thought to improve this project.Thank you very much for your help.I will close this issue.====="", 'you can create a feature request in my updated port <https://github.com/vladmandic/face-api>=====']"
https://github.com/justadudewhohacks/face-api.js/issues/760;Where are the models?;2;closed;2021-02-17T00:20:13Z;2021-02-18T23:15:25Z;Installed via npm, `npm install face-api.js`. In the resulting `/node_modules/face-api.js` directory there's no models or weights directory. The instructions in the readme don't indicate what the proper way to get the models is.;['Hi @geuis, you can clone the Face-api.js repo and copy the weights directory in this you will get all the weights (models). if you got it then close the issue.=====', '...or use the newer fork, `npm install @vladmandic/face-api`=====']
https://github.com/justadudewhohacks/face-api.js/issues/759;two concurrent video feeds, mask rendering degrades and almost stops after 10 min..;1;closed;2021-02-16T01:21:15Z;2021-02-16T14:57:49Z;"over about 10-15minutes, the masking (with landmarks, no points) of my two concurrent, different video feeds goes from immediately reactive to slower and slower to the point it barely reacts. I've included two snippets of the performance (10 sec clip or so). It looks like `getImageData` ( *think* within `extractFaces`?) starts to drag from under 50ms to over 250ms. This happens on both a Mac laptop and a Mac iphone x. Any thoughts/ideas for how to address? -my canvas is 435x256 and the method containing the `draw` function is invoked asynchronously-my video is hidden, and is 640x480```if (canvasOverlayBuff && canvasOverlayBuff.width > 0) {      canvasOverlayBuff.width = 435,      canvasOverlayBuff.height = 256,      const drawObj = new self.window.faceapi.draw.DrawFaceLandmarks(        lastLandmarks[`${mode}Video${pId}`]?.landmarks,        {          drawLines: true,          drawPoints: false,        }      ).draw(canvasOverlayBuff),      console.log(""faces drawn ""),```<img width=""431"" alt=""Screen Shot 2021-02-15 at 8 10 18 PM"" src=""https://user-images.githubusercontent.com/659092/108007119-2c776b00-6fcb-11eb-8a4b-62bfd2cd3ef0.png""><img width=""455"" alt=""Screen Shot 2021-02-15 at 8 07 54 PM"" src=""https://user-images.githubusercontent.com/659092/108007121-2da89800-6fcb-11eb-84ad-2f1c38d86af9.png"">";"[""I figured out my issue(s). 1. I was invoking `setTimeout` multiple times within `useEffect`. 2. I wasn't clearing the `Timeout`fix use `setInterval` (more appropriate anyway), store the id in state, and clear the interval upon cleanup == AWESOME performancehere's the code if this helps others:```let [intervalId, setIntervalId] = useState(null),  useEffect(() => {    if (participants) {      if (!intervalId) {        intervalId = setInterval(function () {          pullAndPostFrameTimer(mode, participants),        }, 100),  // now that I'm using setInterval, and correctly,  I can really knock this render number way down      }    }    return function cleanup() {      clearInterval(intervalId),    },  }),    [participants],  return <div />,```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/750;Use face expression model on hidden video feed;5;closed;2021-01-16T20:40:07Z;2021-01-20T16:46:18Z;"Hello,Just found out about this awesome repo, and I'm having a great time using the face expressions API. Maybe because I was smiling too much (for the testing). Anyway, In my app, I want to check if the user is smiling but without displaying the camera video on the screen.For now, I can check if the user is smiling or not with a pretty good accuracy, but the video has to be on my screen.How can I remove the video from the screen?Here is the js code:```<video id=""video"" width=""500"" height=""500"" autoplay muted ></video>async startVideo() {      let stream = null,      try {        stream = await navigator.mediaDevices.getUserMedia({          audio: false,          video: { width: 1280, height: 720 },        }),        this.video.srcObject = stream,      } catch (e) {        console.log(e),      }    },this.video = document.getElementById('video'),    Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri('/models'),      faceapi.nets.faceExpressionNet.loadFromUri('/models'),    ]).then(this.startVideo),    this.video.addEventListener('play', () => {      console.log('video started'),      setInterval(async () => {        const detections = await faceapi.detectAllFaces(          this.video,          new faceapi.TinyFaceDetectorOptions(),        ).withFaceExpressions(),        detections.forEach((detection) => {          if (detection.expressions.happy >= 0.7) {            this.isHappy = true,          } else {            this.isHappy = false,          }        }),      }, 100),    }),```The app is in vue.js, so I only took the parts that matter.Your help is greatly appreciated.Cheers,";"['video element does not have to rendered to work, just add style=""display: none"" to the video tag.=====', 'You can hide the video by setting the `style=""display: none,""`  of the video. The video tag then becomes: ```<video id=""video"" width=""500"" height=""500"" style=""display: none,"" autoplay muted ></video>```or using JS:```var vid = document.createElement(""video""), vid.setAttribute(\'id\', \'video\'),vid.setAttribute(\'autoplay\', \'muted\'),document.body.appendChild(vid),document.getElementById(""video"").style.display = ""none"",```=====', 'Thank you so much @vladmandic  and @sudarshan-parvatikar ! I had of course tried to use ""display: none,"" but it doesn\'t work somehow. Could it be due to vue.js and its virtual dom?Anyway, I created the element in javascript as suggested by @sudarshan-parvatikar, and it works! I still don\'t know why it works this way and not the other, but I\'m glad it does. Thanks again guys, you are awesome! =====', ""Most likely it is Vue that is avoiding placing element with display:none, but can't do that when it's added programmatically via JS.Best way to determine what is Vue doing is to open inspector in browser and see which elements are present on the page.====="", ""@hamza-bentahar glad it worked out. I suggest trying out @vladmandic 's recommendation as I don't know Vue and I use that snippet in Vanilla JS . Also, if your issue is solved, I'd suggest closing this issue. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/744;facial recognition via similarity , plus hardware recommendation;2;closed;2021-01-06T16:05:24Z;2021-01-08T09:19:12Z;"Hello. This is not exactly an issue. But I hope I can get some answers here as I'm a beginner.First of all thanks for building this awesome library. So here's my question.I'm building a very simple, moderate traffic facial recognition system. Maybe just less than 2000 faces a day. At midnight, the face list will reset to zero. I found out how to get face descriptor from webcam using this library.What I'd like to do is to save the descriptor in backend array in memory. So from my understanding, I can use euclideanDistance.Let's say I've already added 100 descriptors in the array X in memory. So in order to find detect if a given face exists in the array, I can do this (pseudocode)function findFace(newfacedescriptor) begin  for each descriptor in  X begin      distance = calculate euclideanDistance(descriptor, newfacedescriptor)     if (distance < threshold) begin       showmessage(""Found one !!"")        exit,     end  endendis my understanding correct? I'm kinda confused on the difference between euclideanDistance function and findBestMatchf unction. I assume findBestMatch is an encapsulation of euclideanDistance functionsecond question, what sort of hardware do you recommend for better performance? would something like jetson nano or a pc with 3d gpu be advantageous ?any help is greatly appreciated !!";['yes, `findBestMatch()` is just an encapsulation to find the best match using same `euclideanDistance()` function.do note that `euclideanDistance()` is not a gpu-accelerated operation, it simply runs a sum of squares of each item in face descriptor array and returns a square root of the final sum - that is the definition of Euclidean distance, not specific to face-api library. so general-purpose cpu will handle it better than specialized board like jetson. on the other hand, you do need a decent hardware acceleration (jetson or a pc with a gpu) to run facial detection and grab all the descriptors in the real time to start with.=====', 'thanks very much for the help ! so much clearer to me now!=====']
https://github.com/justadudewhohacks/face-api.js/issues/740;How to createCanvasFromMedia with actually canvas element;1;closed;2020-12-19T19:16:01Z;2020-12-22T00:37:24Z;"Hi, i´m trying to implement a solution with my camera, which works with a live stream solution though the rtsp protocol plus JSMpegPlayer library. The stream is showed on a canvas element, like this:##HTML with canvas element - the video goes here###canvas id=""video"" /canvas###The JS script implemented####    player = new JSMpeg.Player('ws://localhost:9999', {      canvas: document.getElementById('video'), audio: false // Canvas should be a canvas DOM element    })	How can i integrate this with face api? First i tried to use createCanvasFromMedia method, but HTMLCanvas element is not an acceptable parameter. So, i guess my doubt is how to createMedia from an external camera, not the camera from my notebook is it possible?";"[""Got it, it's quite simple, you have only to get thecontent with the method captureStream, so, the video content will bestream = video.captureStream(25),const videoEl = document.getElementById('myVideo')videoEl.srcObject = streamClosing this.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/735;Uncaught TypeError: Cannot read property 'loadFromUri' of undefined;3;closed;2020-12-07T16:53:12Z;2020-12-08T06:52:36Z;So I was just doing this youtube tutorial about face detection but in my scripts file I get an error.`const video = document.getElementById('video')Promise.all([    faceapi.nets.tnyFaceDetector.loadFromUri('/model'),    faceapi.nets.faceLandmark68Net.loadFromUri('/model'),    faceapi.nets.faceRecognitionNet.loadFromUri('/model'),    faceapi.nets.faceExpressionNet.loadFromUri('/model')])function startVideo() {    navigator.getUserMedia(        { video: {} },        stream => video.srcObject = stream,        err => console.error(err)    )}startVideo()`the Error:`Uncaught TypeError: Cannot read property 'loadFromUri' of undefined    at script.js:5`Kindly help me, I am stuck;"['You have a typo: replace **tnyFaceDetector** with **tinyFaceDetector**=====', 'Thanks I fixed that but now I get another error after typing the full code.The error on the console:`face-api.min.js:1 Fetch API cannot load file:///C:/models/tiny_face_detector_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1lp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1hp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1vp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1t.loadFromUri @ face-api.min.js:1(anonymous) @ script.js:4face-api.min.js:1 Fetch API cannot load file:///C:/models/face_landmark_68_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1lp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1hp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1vp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1t.loadFromUri @ face-api.min.js:1(anonymous) @ script.js:5face-api.min.js:1 Fetch API cannot load file:///C:/models/face_recognition_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1lp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1hp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1vp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1t.loadFromUri @ face-api.min.js:1(anonymous) @ script.js:6face-api.min.js:1 Fetch API cannot load file:///C:/models/face_expression_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1lp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1hp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1vp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1t.loadFromUri @ face-api.min.js:1(anonymous) @ script.js:7face-api.min.js:1 Uncaught (in promise) TypeError: Failed to fetch    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at face-api.min.js:1    at new Promise (<anonymous>)    at p (face-api.min.js:1)    at lp (face-api.min.js:1)    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1lp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1hp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1vp @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1t.loadFromUri @ face-api.min.js:1(anonymous) @ script.js:4Promise.then (async)(anonymous) @ script.js:8`The code in my JS file:`const video = document.getElementById(\'video\')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri(\'/models\'),  faceapi.nets.faceLandmark68Net.loadFromUri(\'/models\'),  faceapi.nets.faceRecognitionNet.loadFromUri(\'/models\'),  faceapi.nets.faceExpressionNet.loadFromUri(\'/models\')]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}video.addEventListener(\'play\', () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width, height: video.height }  faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()    const resizedDetections = faceapi.resizeResults(detections, displaySize)    canvas.getContext(\'2d\').clearRect(0, 0, canvas.width, canvas.height)    faceapi.draw.drawDetections(canvas, resizedDetections)    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    faceapi.draw.drawFaceExpressions(canvas, resizedDetections)  }, 100)})`And thanks again @treedbox =====', 'Oh solved it,I was using some other extension to open my .html file in browser and know I used the live server extension and it solved for me.Thanks @treedbox =====']"
https://github.com/justadudewhohacks/face-api.js/issues/729;Error: toNetInput - Problem with canvas.loadImage() + faceapi.detectAllFaces();5;closed;2020-11-17T23:34:45Z;2020-11-24T04:29:46Z;"Hey!My name is Gabriel and I'm using face-api.js to apply FaceRecognition on my final essay, on my way to graduate.With the objective of trying the face-api and understand how it works, I created the following test.js:```import '@tensorflow/tfjs-node',import faceapi from ""face-api.js"",import * as canvas from 'canvas',const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image, ImageData }),const faceDetectionNet = faceapi.nets.ssdMobilenetv1,const minConfidence = 0.5,const faceDetectionOptions = new faceapi.SsdMobilenetv1Options({ minConfidence }),async function runRecognition() {        await faceDetectionNet.loadFromDisk('./weights'),    await faceapi.nets.faceLandmark68Net.loadFromDisk('./weights'),    await faceapi.nets.faceRecognitionNet.loadFromDisk('./weights'),    const REFERENCE_IMAGE = './bbt1.jpg',    const QUERY_IMAGE = './bbt2.jpg',    const referenceImage = await canvas.default.loadImage(REFERENCE_IMAGE),    const queryImage = await canvas.default.loadImage(QUERY_IMAGE),    const detections_ref = await faceapi.detectAllFaces(referenceImage, faceDetectionOptions)        .withFaceLandmarks()        .withFaceDescriptors(),    const detections_query = await faceapi.detectAllFaces(queryImage, faceDetectionOptions)        .withFaceLandmarks()        .withFaceDescriptors(),    const faceMatcher = new faceapi.FaceMatcher(detections_ref),    detections_query.forEach(fd => {        const bestMatch = faceMatcher.findBestMatch(fd.descriptor),        console.log(bestMatch.toString()),    }),}runRecognition(),```But I wasn't able to make it work, right now, I'm getting the following error:""UnhandledPromiseRejectionWarning: Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id    at C:\Users\gabri\Desktop\TCC\face-api-teste\node_modules\face-api.js\build\commonjs\dom\toNetInput.js:38:35""Does anyone have any idea of what the problem is?Thanks for the help 😄 ";"[""what is the result and type of `const referenceImage = await canvas.default.loadImage(REFERENCE_IMAGE),` ?if you're not drawing on canvas in nodejs, you don't actually need 3rd party libraries and monkey patches just to load image and use it in `face-api` - you can use native `tfjs-node` methods directly, take a look at <https://github.com/vladmandic/face-api/blob/master/example/node.js>, specifically 'image()' method.====="", ""Thanks for the tip, I'll try it as in your example as soon as I can, and then I comment here if it worked.About the result of `const referenceImage = await canvas.default.loadImage(REFERENCE_IMAGE),`, I logged the object and it printed:[Image:1366x768 ./bbt1.jpg complete]====="", ""that doesn't look like `Image` type, that looks like something canvas internal.`face-api` perfoms check:   `input instanceof Image || input instanceof Canvas || input instanceof Video` and if that fails, you get your error `expected media to be of type ...`took a quick look at canvas and their example shows how to create canvas and then load image into it and not how to use `canvas.default` to create image type directly.```jsconst referenceImage = await canvas.loadImage(REFERENCE_IMAGE),const myCanvas = canvas.createCanvas(200, 200)const ctx = myCanvas.getContext('2d')ctx.drawImage(image, 0, 0, 200, 200)```and now you have `myCanvas` that you can pass to face-api.but like i said, you don't need canvas at all.====="", ""I'm sorry for taking too long to get back here with news!But thanks a lot @vladmandic, you saved me and my group! hahahaYou were right, I refactored my code to something more alike your example, and it worked nice and smooth!I'll set this issue as solved, but only for purpose of sharing knowledge, could you please explain to me when would I need or not need canvas?Thanks again!====="", ""@Gabe-Soares if you want to draw results on top of an image (and then you'd probably also want to save that image to disk) - then you need a canvas.but if you're just displaying results as text, no need for canvas as everything can be done without it.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/723;Unhandled Rejection (Error): 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen;1;closed;2020-11-04T05:16:30Z;2020-11-04T10:53:28Z;I keep getting this error while using TinyFaceDetector for face detection. I am using another tfjs model as well so it is conflicting.![Screenshot (6)](https://user-images.githubusercontent.com/62165403/98072218-8a080180-1e8b-11eb-9491-5cb358075390.png);['> I keep getting this error while using TinyFaceDetector for face detection.> I am using another tfjs model as well so it is conflicting.> > ![Screenshot (6)](https://user-images.githubusercontent.com/62165403/98072218-8a080180-1e8b-11eb-9491-5cb358075390.png)This issue is alredy raised here [https://github.com/justadudewhohacks/face-api.js/issues/719#issue-733674343](url)@shailjaa please raise your concerns there and mark this as duplicate.=====']
https://github.com/justadudewhohacks/face-api.js/issues/722;Loading models in Angular;6;closed;2020-11-03T22:18:00Z;2021-02-16T05:19:35Z;"I'm trying to load the ""models"" downloaded in my project, it compiles but showing this following error in console: ![image](https://user-images.githubusercontent.com/43259638/98045958-183aa400-1e08-11eb-80c8-c34a6ee607d5.png)All examples I have found are in Vanilla or Node.js, so I have  doubts if I'm loading the models in right place of Angular class, the current code of class is: `  @ViewChild('videoPlayer') videoplayer : HTMLVideoElement,  async ngOnInit() {    Promise.all([      await faceapi.nets.tinyFaceDetector.loadFromUri('./models'),      await faceapi.nets.faceLandmark68Net.loadFromUri('./models'),      await faceapi.nets.faceRecognitionNet.loadFromUri('./models'),      await faceapi.nets.faceExpressionNet.loadFromUri('./models')    ]).then(() => {      this.startVideo(),    })  }  async startCanvas(){    // const canvas = faceapi.createCanvasFromMedia(this.videoplayer),        setInterval(async () => {      const detections = await faceapi.detectAllFaces(this.videoplayer,         new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions(),        console.log(detections),    }, 100)  }  startVideo(){    navigator.getUserMedia(      { video: {} },      stream => {        this.videoplayer.srcObject = stream,      },      err => console.error(err)    ),  }`Current path of ""models"" folder:![image](https://user-images.githubusercontent.com/43259638/98046459-04437200-1e09-11eb-9f6c-be2e0b98e6eb.png)Am I putting the models in a wrong place or I should to change where my code load the ""models"" in page?";"['Hello, try moving the models folder to **assets**.![image](https://user-images.githubusercontent.com/25928891/98683328-473aa380-233b-11eb-9a33-58c79479df99.png)=====', 'I have no idea how to implement this in angular. can you please share github of working code in angular?=====', '@thelionleo take a look in a project named ""projeto-tcc"" in my profile, I did the implementation for Angular, just configure and access localhost:800/tests to see the results=====', '> Hello, try moving the models folder to **assets**.> > ![image](https://user-images.githubusercontent.com/25928891/98683328-473aa380-233b-11eb-9a33-58c79479df99.png)I did the changes as you sugested, I had to make other changes to models loading correctly, for example, instead of calling the model loaders on init and inside a Promise.all(), I have to load them inside a async function and call it in ngAfterViewInit().If someone have difficult to load the plugin in project, see ""projeto-tcc"" in my profile, it\'s not finished yet but is possible to see this plugin working in tests page. =====', '> @thelionleo take a look in a project named ""projeto-tcc"" in my profile, I did the implementation for Angular, just configure and access localhost:800/tests to see the resultsthere\'s too much happening there.  is it an implementation of face-api? would it be too much to ask if you can create a new github for me for angular? just the basics to get me started.=====', '@thelionleo Here is my [repo](https://github.com/imrushi/face-api-js-angular) I have implemented the Face-api.js in angular. I hope this will help you.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/706;Question: Are the face descriptors generated compatible with dlib-generated face descriptors?;4;closed;2020-10-12T14:21:42Z;2020-12-01T15:28:36Z;Here's what I want to achieve:I have a corpus of dlib face descriptors. I want to:1. take an image that was previously processed by dlib2. extract a face using face-api.js3. still get true positive results when querying the dlib-generated data with the face-api.js descriptors.Is that possible?;"[""Have you tried it? The face descriptors here are 128-item float arrays - don't know what dlib does. Guessing the chances of them being from the same underlying network model are slim, sorry. But ymmv...====="", ""Yes, I think it depends largely on the model. I ended up going with a fork of this repo that resolves a ton of problems I was having. I think because the colour models used between node and java are different, then the floats produced are potentially slightly different as well? Not to mention different platforms and their float precision differences and endianness.In the end, though, I don't think the differences weren't enough to prevent a match. ====="", ""What fork did you use? I might want to check it out myselfOn Tue, Dec 1, 2020, 4:25 AM Nathan Trevivian <notifications@github.com>wrote:> Yes, I think it depends largely on the model. I ended up going with a fork> of this repo that resolves a ton of problems I was having. I think because> the colour models used between node and java are different, then the floats> produced are potentially slightly different as well? Not to mention> different platforms and their float precision differences and endianness.> In the end, though, I don't think the differences weren't enough to> prevent a match.>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/706#issuecomment-736339911>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ABEZELCNRIL66VMPX7NIAI3SSSZAZANCNFSM4SM3XOIA>> .>====="", '> What fork did you use? I might want to check it out myselfhttps://github.com/vladmandic/face-api=====']"
https://github.com/justadudewhohacks/face-api.js/issues/694;Browser faster then node.;2;closed;2020-09-17T09:55:53Z;2021-03-16T09:56:59Z;faceExpressionRecognition faster in the browser than on the node. How to fix it?;"['![Screenshot from 2020-09-17 14-29-18](https://user-images.githubusercontent.com/18086617/93465548-a0fe8e80-f8f3-11ea-9fc1-1fd51368199e.png)Also, I have some messages before the script of js start.=====', ""messages are normal, they are coming from `tfjs-node` package when it's loading.regarding performance, are you using `tfjs-node` or `tfjs-node-gpu` with `cuda`-enabled acceleration?  if you want to get GPU acceleration that you have by default in you browser via WebGL, you need to use `tfjs-node-gpu` and must have `cuda` installed - otherwise its running on CPU and it's definitely much slower.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/693;canvas can't detected with jsmpeg and face-api.js;2;closed;2020-09-09T09:40:15Z;2020-09-11T02:56:26Z;"(English is not my first language, sorry)I use ""rtsp-to-stream"" which is use to transcoding and jsmpeg to play my camera.```""node-rtsp-stream"": ""0.0.9"",```and I use ""videoFaceTracking.html"" , this examples to do experiment.this is my Html code:```html<div style=""position: relative"" class=""margin"">      <!-- <video src=""bbt.mp4"" id=""inputVideo"" autoplay muted loop playsinline></video> -->      <canvas id=""myPlayer-min-left"" class=""my-player-min"" ></canvas>      <canvas id=""overlay""></canvas> </div>```and my jsmpeg code```js$(document).ready(function() {      renderNavBar('#navbar', 'video_face_tracking')      // jsmpeg start      const myPlayerL = document.getElementById('myPlayer-min-left')      var urlL = 'ws://localhost:9530'      this.playerL = new JSMpeg.Player(urlL, { canvas: myPlayerL })      initFaceDetectionControls()      run()    })```and I change `onPlay()` args like this:```js // start processing framesonPlay(document.getElementById('myPlayer-min-left'))```then I want to get some info by console , so I change `onPlay`  body like this```js      if (drawBoxes) {        if (resizedResults && resizedResults.length !== 0) {          // here is my change          console.log('detect face success....',  results)        }        faceapi.draw.drawDetections(canvas, resizedResults)      }      if (drawLandmarks) {        faceapi.draw.drawFaceLandmarks(canvas, resizedResults)      }```next is my question.I find that it doesn't have any output unless I set my-Player-min to ""display: none""```css<style>    .my-player-min {      display: none,    }  </style>```So I wonder why this is happening";"[""The problem is solved! The reason is that the 'getContext(' webGL ')' mode is used by default when drawing canvas in Jsmpeg, while the 'getContext(' 2D ')' mode of the same canvas is needed in face-APi.js, resulting in conflicts. However, face-APi.js does not throw any error exception, which makes it difficult to find the problem====="", 'sorry. Forget to add the solution. Just add one attribute `disableGl`  in the JSMPEG```jsthis.playerL = new JSMpeg.Player(urlL, { canvas: myPlayerL,  disableGl: true  })```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/692;"IONIC: faceapi.detectAllFaces ""Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.""";1;closed;2020-09-08T21:47:26Z;2020-09-09T18:41:16Z;"Hello I'm trying to detect faces in realtime using the device camera but in ionic. I replicated the example provided  [here](https://github.com/WebDevSimplified/Face-Detection-JavaScript), it works well in the browser with `ng serve`, but it fails when running in Android. It throws the following error when executing `faceapi.detectAllFaces()`Any idea of what could be happening?### Error ```E/Capacitor/Console: File: http://localhost/vendor-es2015.js - Line 39108 - Msg: ERROR Error: Uncaught (in promise): IndexSizeError: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.    Error: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:531954        at Array.map (<anonymous>)        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:531848        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:1801        at Object.next (http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:1906)        at n (http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:677)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3470:30)        at Object.onInvoke (http://localhost/vendor-es2015.js:62348:33)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3469:36)        at Zone.run (http://localhost/polyfills-es2015.js:3229:47)```Here is my HTML```html<div id=""container"" #container>    <video width=""960"" height=""720"" muted autoplay id=""video""></video></div>```Heres is my TypeScript code: ```typescriptimport * as faceapi from '../../../assets/tfjs/face-api.min.js'import { Plugins, FilesystemDirectory, FilesystemEncoding } from '@capacitor/core',const { Filesystem } = Plugins,async ngOnInit() {    await this.loadModels(),  }async loadModels() {    //set path to load models    let filePathRoot = 'http://localhost/assets/',    // faceapi settings    faceapi.env.monkeyPatch({      readFile: filePath =>        new Promise(resolve => {          let fileExtension = filePath.split(""?"")[0].split(""."").pop(),          let fileName = filePath.split(""?"")[0].split(""/"").pop(),          if (fileExtension === ""json"") {            fetch(filePathRoot + fileName)              .then((response) => {                resolve(response.text()),              })          } else {           //Retrieving shard files from File system using Cordova Plugin            Filesystem.readFile({              path: fileName,              directory: FilesystemDirectory.Documents,              encoding: FilesystemEncoding.UTF16            }).then(file => {              const str = file.data,              var buf = new ArrayBuffer(str.length * 2),              var bufView = new Uint16Array(buf),              for (var i = 0, strLen = str.length, i < strLen, i++) {                bufView[i] = str.charCodeAt(i),              }              resolve(new Uint8Array(buf)),            })          }        }),      Canvas: HTMLCanvasElement,      Image: HTMLImageElement,      ImageData: ImageData,      Video: HTMLVideoElement,      createCanvasElement: () => document.createElement(""canvas""),      createImageElement: () => document.createElement(""img"")    }),   // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot),    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot),    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot),    this.startVideo(),  } startVideo() {    const video = document.getElementById(""video"") as HTMLVideoElement,    const container = document.getElementById(""container"") as HTMLDivElement,    video.onplaying = () => {      console.log('Camera loaded')      this.analyze(video, container),    },    navigator.mediaDevices.getUserMedia({      audio: false,      video: {        width: { ideal: 960 },        height: { ideal: 720 },      }    }).then((stream) => {      video.srcObject = stream,    }).catch((err0r) => {      console.log(err0r)      video.src = ""assets/video-test.mp4"",      video.play(),    }),  } analyze(source: HTMLVideoElement | HTMLImageElement, content) {    const canvas = faceapi.createCanvasFromMedia(source),    content.append(canvas),    const displaySize = { width: source.width, height: source.height }    faceapi.matchDimensions(canvas, displaySize)    this.intervalId = setInterval(async () => {      console.log(source.height, source.width)      const detections = await faceapi.detectAllFaces(source, new      faceapi.TinyFaceDetectorOptions()).withFaceExpressions(), //FAILS HERE      console.log(JSON.stringify(detections))      const resizedDetections = faceapi.resizeResults(detections, displaySize)      canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)      faceapi.draw.drawDetections(canvas, resizedDetections)      faceapi.draw.drawFaceExpressions(canvas, resizedDetections)    }, 100)  }```";"[""The problem was caused because the model file wasn't being loaded correctly. Here is the right way to load it from local file in Capacitor.```javascript Filesystem.readFile({                path: fileName,                directory: FilesystemDirectory.External              }).then(file => {                const byteCharacters = atob(file.data),                const byteNumbers = new Array(byteCharacters.length),                for (let i = 0, i < byteCharacters.length, i++) {                  byteNumbers[i] = byteCharacters.charCodeAt(i),                }                const bufView = new Uint8Array(byteNumbers),                resolve(bufView),              })```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/691;Add my own model predition?;5;closed;2020-09-07T11:35:44Z;2021-04-04T08:45:15Z;Hi there, first of al congratulations and thank you for the great project that you share with all us.Im kinda new in the area,  so sorry about something i can wrong.I had trained a facemaks model to detect if some face have a mask, not have or is bad wearing it.I would like to add / concat the result to faceapi analys im trying figure how do it,  anyone can give a hand with that? would be great have al the results togheter.Thank you in advance.;"['What you need is called ""custom classifier"" and there is a tutorial here:https://codelabs.developers.google.com/codelabs/tensorflowjs-teachablemachine-codelab/index.html#0=====', '@JediahDizon Thank You, I have already the model trained, just want create a new function like "".detectFacemask()"" that can be concatened to the rest of faceapi functions and add the result to the json return.=====', ""Ahh I see. Well I'm not sure how the face-api do its black box magic so I don't know about diving deep in the face-api code to add a new function to detect a mask.However, if I were to implement it on the top of my head, I would make it so that I can have the classifier run independently with the face-api library. So you got two sensors for a face, one that detects the face, and one that identifies if a mask exist. Benefit of this is when the mask is not a human face and cannot be detected by face-api, you would still know there is a mask there.Downside is a hit in performance because you have two neural networks running at the same time. I dunno how bad, it could be insignificant or drastic. So I would  test it out if this solution is plausible.====="", 'Well finally i found a way to do it, just adding some code and a function.Thx for the help anyway.=====', ""@raulocho  I'm doing a system to detect a mask now, see that you already have a solution. I wonder if this part of the code is open source, where can I find it?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/673; Uncaught (in promise) Error: failed to fetch: (404) Not Found, from url: http://localhost:8601/face-api.js/models/ssd_mobilenetv1_model-weights_manifest.json;3;closed;2020-08-01T10:51:05Z;2020-08-07T00:29:06Z;"Tried using Face-api.js in a web based application, For now we have the models folder in nodemodules/face-api.js/models. but we get the above error. May i know which is the right place to have the models folder?This is the code used.const { Canvas, Image, ImageData } = canvas	faceapi.env.monkeyPatch({ Canvas, Image, ImageData })		console.log(faceapi.nets), (async () => {	await faceapi.nets.ssdMobilenetv1.loadFromUri('face-api.js/models'),		const detections = await faceapi.detectAllFaces(this.video),	console.log(""face found ""),	console.log(detections),	})(),    let media = navigator.mediaDevices.getUserMedia({      video: true,      audio: false    }),    media.then((stream) => {      this.video.srcObject = stream,    }),";"[""What did you figure out here @naveen-robotixedu? Dealing with a similar issue... Thanks! (I'm not using express backend)====="", ""> What did you figure out here @naveen-robotixedu? Dealing with a similar issue... Thanks! (I'm not using express backend)Created a build folder for the website and just placed the models folder in the build folder====="", 'wow you just saved my life. I was stressing the fu&^ out =====']"
https://github.com/justadudewhohacks/face-api.js/issues/666;Error installing example;2;closed;2020-07-22T01:46:11Z;2020-07-23T00:56:55Z;## Error installing example#### I followed the README.md step by step, even with the node and the updated npm, I was returned with the following error:![image](https://user-images.githubusercontent.com/51277667/88124396-63a3ac00-cba3-11ea-9850-73fb5254970c.png)![image](https://user-images.githubusercontent.com/51277667/88124409-69998d00-cba3-11ea-8e23-f04fd4207c9c.png)![image](https://user-images.githubusercontent.com/51277667/88124416-73bb8b80-cba3-11ea-93ac-444759eab3ce.png)### My environment:#### I am using the Linux Ubuntu distribution, in its latest LTS version, at 20.04. I have a version of the node at 12.16.3 and npm at 6.14.4;['From what I can see, you seem to be in the wrong directory when you are running the scripts. Are you trying to run the examples? In which case you need to traverse to the right dir face-api.js->examples-> examples browser folder for browser based examples. Example-node for node js examples. And then you do an npm install. Hope this helps. =====', 'OMG that was it, thank you, and forgiveness for the lack of attention on my part=====']
https://github.com/justadudewhohacks/face-api.js/issues/641;Loading models in Electron App - Renderer Process;1;closed;2020-06-18T16:38:00Z;2020-06-19T07:03:16Z;"Hi,I am unable to load the models  in the Electron app renderer process. Here is what I am doing:```[const tf = require('@tensorflow/tfjs'),//require('v8-compile-cache'), // improves speed of application load.// Optional Load the binding:// Use '@tensorflow/tfjs-node-gpu' if running with GPU.require('@tensorflow/tfjs-node'),const faceapi = require('face-api.js'),const canvas = require('canvas'),const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image, ImageData })// Get the models(async () => {  await faceapi.nets.ssdMobilenetv1.loadFromDisk('../models').catch((err) => console.log('Error Loading Mobile Net model')),  console.log('Loaded Mobile Net model '),})(),](url)````I get a ""Loaded Mobile Net mode"" message on my console.I then try and call this:  let detectedFaces = await faceapi.detectAllFaces(initialImageCanvas),`inside another async function. But I get this error:`[C:\Users\draru\Electron-App\nvisage\node_modules\face-api.js\build\commonjs\ssdMobilenetv1\SsdMobilenetv1.js:24 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (C:\Users\draru\Electron-App\nvisage\node_modules\face-api.js\build\commonjs\ssdMobilenetv1\SsdMobilenetv1.js:24)    at SsdMobilenetv1.<anonymous> (C:\Users\draru\Electron-App\nvisage\node_modules\face-api.js\build\commonjs\ssdMobilenetv1\SsdMobilenetv1.js:58)    at step (C:\Users\draru\Electron-App\nvisage\node_modules\tslib\tslib.js:141)    at Object.next (C:\Users\draru\Electron-App\nvisage\node_modules\tslib\tslib.js:122)    at fulfilled (C:\Users\draru\Electron-App\nvisage\node_modules\tslib\tslib.js:112)](url)`What am I doing wrong? I have the models in a models directory one level up from the renderer.js function.Any help would be much appreciated!";['OK. I realized that I had downloaded the models wrong using curl. Closing this issue.=====']
https://github.com/justadudewhohacks/face-api.js/issues/635;WebGL is not supported on this device;1;closed;2020-06-08T20:05:00Z;2020-06-13T18:49:42Z;On device(J6+), the WebGL2 is unavailable.I'm using `faceapi.js 0.22.2` and `@tensorflow/tfjs-core 1.7.0`.Have some configuration for use WebGL instead WebGL2 ?![IMG_20200608_165922322](https://user-images.githubusercontent.com/11878210/84074920-bd606600-a9a9-11ea-83c1-04f8c8d0b387.jpg);['I have installed the Firefox in mobile and it´s is done.=====']
https://github.com/justadudewhohacks/face-api.js/issues/629;face-api conflicting with @tensorflow-models/coco-ssd;1;closed;2020-06-03T14:12:39Z;2020-06-03T15:21:44Z;"As you can tell by the title, I'm trying to make use of both api's on the same page.If I try using them on different pages, they work perfectly, but once together, I have issues.I'm importing the scripts like this```<script src=""node_modules/@tensorflow/tfjs/dist/tf.js""> </script><script src=""node_modules/@tensorflow-models/coco-ssd/dist/coco-ssd.js""> </script><script src=""node_modules/face-api.js/dist/face-api.js""></script>```They obviously are conflicting because depending on the order in which I include them, one will work and the other won't.With the order shown above, object recognition works thanks to coco-ssd but face recognition doesn't. I get this error:```Uncaught (in promise) TypeError: t.batchNormalization is not a function    at tf-core.esm.js:17    at engine.js:606    at engine.js:425    at Engine.scopedRun (engine.js:436)    at Engine.tidy (engine.js:423)    at kernelFunc (engine.js:606)    at engine.js:619    at Engine.scopedRun (engine.js:436)    at Engine.runKernelFunc (engine.js:616)    at Uu (tf-core.esm.js:17)```When I invert them, I get this:```Uncaught (in promise) TypeError: backend.batchNorm is not a function    at forward (batchnorm.js:95)    at tf-core.esm.js:17    at tf-core.esm.js:17    at t.scopedRun (tf-core.esm.js:17)    at t.tidy (tf-core.esm.js:17)    at f (tf-core.esm.js:17)    at tf-core.esm.js:17    at t.scopedRun (tf-core.esm.js:17)    at t.runKernelFunc (tf-core.esm.js:17)    at batchNorm_ (batchnorm.js:110)```I tried importing them in a main file and then bundling them (using webpack) like mentioned on this issue #61 but it doesn't help.Any help would be appreciated, thanks";['Newer version of Tensorflow conflicts with face-api.js.Using @tensorflow/tfjs@1.7.4 works=====']
https://github.com/justadudewhohacks/face-api.js/issues/626;Can't run or compile nodejs examples;1;closed;2020-05-30T02:21:11Z;2020-05-31T04:41:11Z;"I'm having trouble running the nodejs examples.```node -v v12.16.3``````tsc -vVersion 3.9.3``````ts-node -vv8.10.2```After installing all dependencies, not ts-node nor tsc works. These are the results from running:**ts-node ageAndGenderRecognition.ts**```face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-backend-cpu/src/backend_cpu.ts:24const nonMaxSuppressionV3 = kernel_impls.nonMaxSuppressionV3,                                         ^TypeError: Cannot read property 'nonMaxSuppressionV3' of undefined    at Object.<anonymous> (/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-backend-cpu/src/backend_cpu.ts:24:42)    at Module._compile (internal/modules/cjs/loader.js:1133:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1153:10)    at Module.load (internal/modules/cjs/loader.js:977:32)    at Function.Module._load (internal/modules/cjs/loader.js:877:14)    at Module.require (internal/modules/cjs/loader.js:1019:19)    at require (internal/modules/cjs/helpers.js:77:18)    at Object.<anonymous> (/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs/dist/tf.node.js:25:22)    at Module._compile (internal/modules/cjs/loader.js:1133:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1153:10)```Also when trying to compile the ts files by running: **tsc ageAndGenderRecognition.ts**```node_modules/@tensorflow/tfjs-backend-cpu/dist/backend_cpu.d.ts:64:111 - error TS2694: Namespace '""/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-core/dist/backends/backend_util""' has no exported member 'FusedBatchMatMulConfig'.64     fusedBatchMatMul({ a, b, transposeA, transposeB, bias, activation, preluActivationWeights }: backend_util.FusedBatchMatMulConfig): Tensor3D,                                                                                                                 ~~~~~~~~~~~~~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-cpu/dist/kernels/Max_impl.d.ts:17:20 - error TS2305: Module '""../../../tfjs-core/dist""' has no exported member 'TypedArray'.17 import { DataType, TypedArray } from '@tensorflow/tfjs-core',                      ~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-cpu/dist/kernels/Transpose_impl.d.ts:17:20 - error TS2305: Module '""../../../tfjs-core/dist""' has no exported member 'TypedArray'.17 import { DataType, TypedArray } from '@tensorflow/tfjs-core',                      ~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-webgl/dist/backend_webgl.d.ts:26:10 - error TS2305: Module '""../../tfjs-core/dist""' has no exported member 'BackendValues'.26 import { BackendValues } from '@tensorflow/tfjs-core',......```It seems to me that it has something to do with the versions I'm running maybe?";"[""OK I followed this instructions and ended up workinghttps://technodezi.co.za/Post/running-face-apijs-or-tfjs-node-on-a-raspberry-pi-and-nodejsSome of the steps can't be done thoug as they are for linux.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/623;"@tensorflow/tfjs-node: ""This repository has been archived""";2;closed;2020-05-22T09:07:50Z;2020-05-26T19:22:02Z;Hi,The instructions are recommending to install `@tensorflow/tfjs-node` but that repository is archived ([link](https://github.com/tensorflow/tfjs-node)) since Aug 2019. Should I use [tensorflow/tfjs](https://github.com/tensorflow/tfjs) instead?Thanks;['It seems you are confused.Npm package [@tensorflow/tfjs-node](https://www.npmjs.com/package/@tensorflow/tfjs-node) already uses https://github.com/tensorflow/tfjs instead of https://github.com/tensorflow/tfjs-node=====', 'Ohh. It is confusing, thanks for pointing it out.=====']
https://github.com/justadudewhohacks/face-api.js/issues/615;Error: 'softmax' not yet implemented or not found in the registry. Did you forget to import the kernel?;2;closed;2020-05-05T00:42:23Z;2020-05-15T17:43:38Z;I have some error in withAgeAndGender() node.jsDoes anybody can help me?Error: 'softmax' not yet implemented or not found in the registry. Did you forget to import the kernel?;['I believe this is an error with the versions. You need to look at the package.json file and see that the @tensorflow/tfjs-node version is 1.7.0. Run the command: npm i @tensorflow/tfjs-node@1.7.0 This should fix the problem, because I had the same problem.=====', '> I believe this is an error with the versions. You need to look at the package.json file and see that the @tensorflow/tfjs-node version is 1.7.0.> > Run the command: npm i @tensorflow/tfjs-node@1.7.0> > This should fix the problem, because I had the same problem.Thank you!! It fixed this problem!=====']
https://github.com/justadudewhohacks/face-api.js/issues/612;Uncaught (in promise) TypeError: faceapi.draw.drawDetection is not a function;1;closed;2020-05-04T08:43:39Z;2020-05-04T09:41:16Z;"$(document).ready(function() {  run()})async function run() {  await faceapi.loadMtcnnModel('models/')  await faceapi.loadFaceRecognitionModel('models/')  console.log(""Loaded""),  const videoEl = document.getElementById('inputVideo')  navigator.getUserMedia(    { video: {} },    stream => videoEl.srcObject = stream,    err => console.error(err)  )}async function onPlay(videoEl) {  const mtcnnForwardParams = {  minFaceSize: 200}const mtcnnResults = await faceapi.mtcnn(document.getElementById('inputVideo'), mtcnnForwardParams)faceapi.draw.drawDetection('overlay', mtcnnResults.map(res => res.faceDetection), { withScore: false })faceapi.draw.drawLandmarks('overlay', mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: 'red' })const options = new faceapi.MtcnnOptions(mtcnnParams)const input = document.getElementById('inputVideo')const fullFaceDescriptions = await faceapi.detectAllFaces(input, options).withFaceLandmarks().withFaceDescriptors()const labels = ['Salman']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    const imgUrl = `/opt/lampp/htdocs/Youcode/facer/labeled_images/Salman/${label}.png`    const img = await faceapi.fetchImage(imgUrl)     const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()      if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }        const faceDescriptors = [fullFaceDescription.descriptor]    return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)  }))const maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))results.forEach((bestMatch, i) => {  const box = fullFaceDescriptions[i].detection.box  const text = bestMatch.toString()  const drawBox = new faceapi.draw.DrawBox(box, { label: text })  drawBox.draw(canvas)})  setTimeout(() => onPlay(videoEl))}";"[""faceapi.draw.drawDetections('overlay', mtcnnResults.map(res => res.faceDetection), { withScore: false })faceapi.draw.drawFaceLandmarks('overlay', mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: 'red' })=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/589;[node.js version] error when load models from disk;1;closed;2020-04-07T02:50:21Z;2020-04-07T03:45:49Z;I ran into this error when use face-api in node environment```Error: ENOENT: no such file or directory, open './weights/face_landmark_68_model-weights_manifest.json'```Here my folder structure:![image](https://user-images.githubusercontent.com/23404165/78624644-55f83e00-78b4-11ea-9374-67a6c4254baa.png)In `face-comparison.js`:```jsasync function faceComparison(image) {  try {     await faceapi.nets.faceLandmark68Net.loadFromDisk('./weights'),     await faceapi.nets.faceRecognitionNet.loadFromDisk('./weights'),     await faceDetectionNet.loadFromDisk('./weights'),     // do stuff  }  catch(err) { console.log(err), }}```I tried fs.readFile to read the json file and it worked.;"[""Never mind. I use `path.join(__dirname, './weights')` and it works=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/556;where is the demo that can upload image;1;closed;2020-02-21T04:13:16Z;2020-02-23T12:49:47Z;the [online demo](https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection) can upload image to predict.but when i fetch the master and tag 0.22.1 to local,the demo only can use url to predict.could u tell me where to fetch the version that can upload image to predict;['#557 =====']
https://github.com/justadudewhohacks/face-api.js/issues/555;Can not loadTinyFaceDetectorModel after build from react;1;closed;2020-02-20T13:41:47Z;2020-02-20T14:15:46Z;"Hello,Thanks for the cool project !It does quite work well with creat-react-app ""dev mode"" but doesn't work after build the sources. Error log is such as below,tf-core.esm.js:17 Uncaught (in promise) Error: Based on the provided shape, [1,1,32,64], the tensor should have 2048 values but has 807![image](https://user-images.githubusercontent.com/3087809/74938678-bf002100-53ee-11ea-9ceb-69e8dd3d464c.png)The test code is herehttps://github.com/UihyunKim/face-api-build-testAm I miss something?";"[""It seems the issue from the backend related thing.. The main reason is that tiny_face_detector_model-shard1 is not loaded from tf-core because I ran temporary server from serve -s build. It doesnt serve the xxxshard1 file appropriately.When I deploy this app on netlify the xxxshard1 file is loaded and it's working.I'll close this issue. =====""]"
https://github.com/justadudewhohacks/face-api.js/issues/549;How gender identification works using face image?;1;closed;2020-02-12T04:41:42Z;2020-04-26T11:13:04Z;Could you please elaborate on how the system extracts gender from the image?which algorithm you used to classify gender from the face.Thanks!!;"[""It's based on convolutional neural networks, which extract features from a cropped image of a face and classify them image into one of the two categories of male and female.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/536;Requirements & Getting started;2;closed;2020-01-28T05:13:18Z;2020-01-28T12:27:54Z;Hey this is really good work. But I would like to suggest you to include requirements with minimum version numbers of dependencies for installing this package and simple getting started check list which ensures all dependencies have been installed correctly. I have followed the README.md file steps in order. I was not able to run it one go. Because ts-node npm package was not installed. The guide doesn't specify it either.Getting started in one go without errors would simply increase the confidence of the user.;"[""The only dependency of face-api.js is tfjs-core, just make sure you are installing the same tfjs-core version that face-api.js uses (see package.json).If you are having trouble with typescript then use a typescript version that is compatible with the version this project is compiled with (again see package.json).> I have followed the README.md file steps in order. I was not able to run it one go. Because ts-node npm package was not installed. The guide doesn't specify it either.Using tools such as ts-node or typescript is not a requirement in order to use face-api.js and I think it should be self explanatory that you have to install it first in order to use such a tool.====="", '@justadudewhohacks, thanks for explaining. New to node.js.I thought Node.js handles typescript just like we do `node myscript.js` way.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/535;faceDescriptors of different faces are exactly the same? What am I doing wrong?;6;closed;2020-01-25T11:18:31Z;2020-02-07T21:33:25Z;Hi I am running`faceapi.detectSingleFace(image).withFaceLandmarks().withAgeAndGender().withFaceDescriptor(),`of different images/faces and most not all of them have the exact same faceDescriptor values. This seems wrong. Looking at their detection object, they clearly are different. What am I doing wrong?Here is my code where I try to find unique faces.```export const detectFace = async (image, frameNumber, detectionArray, uniqueFaceArray) => {  // detect expression  const face = await faceapi.detectSingleFace(image).withFaceLandmarks().withAgeAndGender().withFaceDescriptor(),  console.log(frameNumber),  // check if a face was detected  if (face !== undefined) {    const { age, gender, descriptor, detection } = face,    const { relativeBox, score } = detection,    const size = Math.round(relativeBox.height * 100),    const scoreInPercent = Math.round(score * 100),    // console.log(face),    if (size < FACE_SIZE_THRESHOLD || scoreInPercent < FACE_DETECTION_CONFIDENCE_SCORE) {      console.log('detected face below size or confidence threshold!'),      return undefined,    }    // create full copy of array to be pushed later    const copyOfDescriptor = descriptor.slice(),    console.log(detection),    console.log(uniqueFaceArray),    // console.log(copyOfDescriptor),    // initialise the faceId    let faceId = 0,    // check for uniqueness    // if the uniqueFaceArray is empty just push the current descriptor    // else compare the current descriptor to the ones in the uniqueFaceArray    const uniqueFaceArrayLength = uniqueFaceArray.length,    if (uniqueFaceArrayLength === 0) {      uniqueFaceArray.push(copyOfDescriptor),    } else {      // compare descriptor value with all values in the array      for (let i = 0, i < uniqueFaceArrayLength, i += 1) {        const dist = faceapi.euclideanDistance(copyOfDescriptor, uniqueFaceArray[i]),        console.log(`${faceId}, ${frameNumber}`),        console.log(dist),        // if no match was found add the current descriptor to the array marking a unique face        if (dist < FACE_UNIQUENESS_THRESHOLD) {          // console.log(`face matches face ${i}`),          faceId = i,          break,        } else if (i === uniqueFaceArrayLength - 1) {          console.log('face is unique'),          uniqueFaceArray.push(copyOfDescriptor),          faceId = uniqueFaceArrayLength,        }      }    }    // console.log(`frameNumber: ${frameNumber}, Score: ${score}, Size: ${size}, Gender: ${gender}, Age: ${age}`),    detectionArray.push({      faceId,      frameNumber,      score: scoreInPercent,      size,      gender,      age: Math.round(age),    })    return detection,  }  console.log('no face detected!'),  return undefined,}```;"['> exact same faceDescriptor valueThis is indeed very suspicious, can you post an example image where this issue occurs?The only two reasons I can think of is, that somethings wrong with your code, or theres something going wrong in the tfjs backend due to your system environment.Could you give some information about your environment? (browser or nodejs, which browser / nodejs version, your OS and which backend you are using, cpu or webgl)=====', ""Thanks for looking into this. **Latest findings**The issue with the same faceDescriptors only occurs with larger image files and only after a certain number of images.With my MoviePrint app I am using e.g. 20 images of a movie and feed it into face-api.js. When I feed in low resolution images (320x240), everything works as expected. When I do this with the same full image size (1920x1080), after the 3rd detected face it keeps throwing out the same faceDescriptor. Does that help?**Setup and 2 example images:**OS: macOS 10.14.6 MojaveNode version: v13.7.0Electron: 4.2.10Chromium: 69.0.3497.128face-api.js: 0.22.0![UNDER ARMOUR 'RISE'-147477837 mp4-frame000201](https://user-images.githubusercontent.com/4619772/73300549-7d29f380-4211-11ea-8dc2-86d6ba80756a.jpg)![UNDER ARMOUR 'RISE'-147477837 mp4-frame000401](https://user-images.githubusercontent.com/4619772/73300441-4bb12800-4211-11ea-8cd7-9a1019d98db8.jpg)====="", 'You are probably right that it has something to do with my setup. I will investigate further.=====', ""Somehow I believe that the problem is with the image input as my boxes are already way off. I had not noticed that before.![Screenshot 2020-02-03 at 23 16 03](https://user-images.githubusercontent.com/4619772/73695849-405f7000-46db-11ea-81ea-b94b740c1a6e.png)My app is using opencv4nodejs to get the image data. I have tried 3 ways of getting my input into `faceapi.detectAllFaces(input)`.The first approach via an actual canvas brought the result seen in the image above:```faceapi.env.monkeyPatch({  Canvas: HTMLCanvasElement,  Image: HTMLImageElement,  ImageData,  Video: HTMLVideoElement,  createCanvasElement: () => document.createElement('canvas'),  createImageElement: () => document.createElement('img')}),const mat = vid.read(),const input = document.getElementById('myCanvas'),input.height = mat.rows,input.width = mat.cols,const imgData = new ImageData(new Uint8ClampedArray(matRescaled.getData()),matRescaled.cols,matRescaled.rows),const ctx = input.getContext('2d'),ctx.putImageData(imgData, 0, 0),const detections = await faceapi.detectAllFaces(input),```Then I tried the other suggested method using node-canvas, but I don't know how to get from the opencv Mat to the image format needed. They both end in an error. I tried this.```const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image, ImageData }),const mat = vid.read(),const input = new ImageData(  new Uint8ClampedArray(mat.getData()),  mat.cols,  mat.rows),const detections = await faceapi.detectAllFaces(input),```and that approach```const { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image }),const mat = vid.read(),const input = new Image(),input.width = matRescaled.cols,input.height = matRescaled.rows,input.src = matRescaled.getData(),const detections = await faceapi.detectAllFaces(input),```What am I doing wrong to get from opencv Mat to the needed input format?====="", 'Forgot to post the error I am getting on the last approach and noticed that there is already an issue on that - https://github.com/justadudewhohacks/face-api.js/issues/194=====', ""Finally I made it work and it seems that reading the images properly also has solved the originally stated issue. I was unfortunately not successful with `node-canvas`, but made it work with `tfjs-node`. Thanks to @whyboris for the tip!```import * as tf from '@tensorflow/tfjs-node',const input = tf.node.decodeJpeg(jpgImage),```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/534;Error GPU usage(CUDNN_STATUS_INTERNAL_ERROR);1;closed;2020-01-25T01:00:10Z;2020-01-25T02:09:05Z;"@tensorflow/tfjs-node-gpu: ""^1.4.0""""face-api.js"": ""0.22.0""Driver Version: 440.48.02    CUDA Version: 10.2GeForce GTX 1650Ubuntu 18.04I try run Win10 and have some problem` nodemon --exec ./node_modules/.bin/ts-node -- ./src/index.ts[nodemon] 1.19.4[nodemon] to restart at any time, enter `rs`[nodemon] watching dir(s): *.*[nodemon] watching extensions: ts,json[nodemon] starting `./node_modules/.bin/ts-node ./src/index.ts`2020-01-25 00:51:57.358207: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-01-25 00:51:57.388648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz2020-01-25 00:51:57.390138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ab0c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-01-25 00:51:57.390202: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version2020-01-25 00:51:57.391997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.12020-01-25 00:51:57.811615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.811823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.695pciBusID: 0000:01:00.02020-01-25 00:51:57.811958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.02020-01-25 00:51:57.812750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.02020-01-25 00:51:57.813434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.02020-01-25 00:51:57.813587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.02020-01-25 00:51:57.814517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.02020-01-25 00:51:57.815262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.02020-01-25 00:51:57.817569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-25 00:51:57.817619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.817815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.817973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 02020-01-25 00:51:57.817988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.02020-01-25 00:51:57.856422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:2020-01-25 00:51:57.856459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      02020-01-25 00:51:57.856464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N2020-01-25 00:51:57.856537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.856729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.856907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2020-01-25 00:51:57.857088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3548 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].raven@2.6.4 alert: no DSN provided, error reporting disabledmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soon2020-01-25 00:52:20.197202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-25 00:52:20.917478: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR2020-01-25 00:52:20.921141: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR(node:5734) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 2Message: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.    at NodeJSKernelBackend.executeSingleOutput (/home/user/aa/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:193:43)    at NodeJSKernelBackend.conv2d (/home/user/aa/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:744:21)    at engine_1.ENGINE.runKernelFunc.x (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/conv.ts:205:25)    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:586:31    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at kernelFunc (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:586:20)    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:599:23    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.runKernelFunc (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:596:10)    at conv2d_ (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/conv.ts:204:22)    at Object.conv2d (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:45:24)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/pointwiseConvLayer.ts:12:18    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at Object.pointwiseConvLayer (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/pointwiseConvLayer.ts:10:13)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/mobileNetV1.ts:37:15    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at Object.mobileNetV1 (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/mobileNetV1.ts:34:13)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:35:24    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at SsdMobilenetv1.forwardInput (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:31:15)    at SsdMobilenetv1.<anonymous> (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:62:14)    at step (/home/user/aa/node_modules/tslib/tslib.js:136:27)    at Object.next (/home/user/aa/node_modules/tslib/tslib.js:117:57)    at fulfilled (/home/user/aa/node_modules/tslib/tslib.js:107:62)    at processTicksAndRejections (internal/process/task_queues.js:94:5)(node:5734) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:5734) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code`";['Helped `export TF_FORCE_GPU_ALLOW_GROWTH=true` =====']
https://github.com/justadudewhohacks/face-api.js/issues/528;How can I increase brightness and contrast in the webcam video stream before passing it to face api?;2;closed;2020-01-18T12:52:05Z;2020-02-15T18:53:10Z;Hello, thank you for the convenient tool. Is it possible to do preprocessing like change brightness and contrast for video stream before doing detections? Will it help to improve the quality of detections? Can you give advice about how can I do this, and debug it?Thank you for your help.;"[""> Will it help to improve the quality of detections?Since I am training most of the models with data augmentation I think you will not get any significant benefit. But it's definitely possible to implement such preprocessing operations with tfjs in case you want to see whether it makes any difference.====="", 'Thank you very much. I found the solution. It can be done using the TensorFlow function `tf.browser.fromPixels`. ```javascriptconst gamma = 0.5const tensor = tf.browser.fromPixels(video)const sourceImg = tf.mul(tensor, tf.scalar(1/255))const norm = tf.pow(sourceImg,  tf.scalar(gamma))tf.browser.toPixels(norm, canvas)```Here is a full example https://jsfiddle.net/shvedaction/wcue6xmf/44/=====']"
https://github.com/justadudewhohacks/face-api.js/issues/523;tfjs-core version dependency;3;closed;2020-01-15T07:56:16Z;2020-02-11T09:37:25Z;The `README.md` states that   > JavaScript face recognition API for the browser and nodejs implemented on top of tensorflow.js core (tensorflow/tfjs-core)which then links to the recently depricated https://github.com/tensorflow/tfjs-core. Is it still based on this version or the [new tfjs](https://github.com/tensorflow/tfjs)? I've seen some issues where @justadudewhohacks references tfjs-core version 1.4 but I'm not sure. ;"[""I just forgot to update the README with the link to the new repo, thanks for the hint. I try to keep face-api.js updated with the most recent tfjs-core version of course.> I've seen some issues where @justadudewhohacks references tfjs-core version 1.4 but I'm not sure.What kind of issue, a common issue is, that people end up with two different versions of tfjs-core, due to another dependency pointing requiring a different tfjs-core version, than the one that face-api.js uses.====="", "" >  I try to keep face-api.js updated with the most recent tfjs-core version of course.Ah, ok. Thanks for clarifying. > What kind of issueI just tried finding some again, but I must habe misread as I can't find any anymore.  > a common issue is, that people end up with two different versions of tfjs-coreIt appears that this is what I was reading.====="", 'Handled in [PR-539](https://github.com/justadudewhohacks/face-api.js/pull/539).=====']"
https://github.com/justadudewhohacks/face-api.js/issues/518;Error: euclideanDistance: arr1.length !== arr2.length;5;closed;2020-01-08T03:02:04Z;2020-12-22T09:59:38Z;While running video face recognition locally, I stored face vectors in browser localstorage by using JSON.stringify(), and when webcam starts, I convert all the JSON strings back into LabeledFaceDescriptors using the following code:`var FINALVECTORS = []    for (let i=0, i < localStorage.length , i++) {        var reVector = JSON.parse(window.localStorage.getItem(localStorage.key(i)))        var LFD = new faceapi.LabeledFaceDescriptors(reVector['label'], [new Float32Array(reVector['descriptors'][0])])        FINALVECTORS.push(LFD)`following that, I pass the array FINALVECTORS into faceapi.FaceMatcher and start comparing:`const faceMatcher = new faceapi.FaceMatcher(FINALVECTORS,0.5)    var idVar = setInterval(async () => {        console.log('start interval')        const detections2 = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor()        if (detections2) {            const results = faceMatcher.findBestMatch(detections2.descriptor) // error here        }`Can't seem to find anything wrong, I am pretty sure the LabeledFaceDescriptor was instantiated correctly. Any help would be appreciated!;"['Maybe you have figured it out by now. For what its worth.I remember an issue I had once which might be related. The size of localStorage is limited and varies from browser to browser. When I was storying too much data I experienced data loss. You could look into using the browsers indexedDB instead of localStorage. This might give you more space.=====', 'Double check the format of your `LabeledFacialDescriptors`. I had this error recently, and it turns out I was accidentally saving my facial descriptors as `{label: String, descriptors: [Object]}`, where `descriptors` was an array of objects, where the objects were results from ```jsawait faceapi.detectAllFaces(image, faceapiOptions).withFaceLandmarks().withFaceDescriptors()[0].descriptor // note here, detecting all faces, referencing the first```Instead of saving the descriptors as an array, or rather Float32Array.How I solved the issue was instead of saving the descriptors as:```js{    label: <label>,    descriptors: <result-from-the-above-snippet>}```I saved them properly as:```jsnew faceapi.LabeledFaceDescriptors(<label>, <result-from-the-above-snippet>).toJSON()```=====', 'I solved it after awhile but completely forgot how I did it, but anyway I am no longer working on this so I will close this issue, thanks for your replies though! Appreciate it @Infinitay @fakob =====', '@adumbz  I am having the same issue can you please help me with it?=====', ""@prkhrv Hi there, it's been too long since I last used this, I can't remember what I did to solve it! But I would think it was something like @Infinitay said above, maybe the format of the descriptor is wrong.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/516;How to improve speed of detectSingleFace(video).withFaceLandmarks().withFaceDescriptor() ?;5;closed;2019-12-30T05:01:02Z;2020-04-30T15:52:40Z;When loading in browser it takes about 12s to start recognising my face. With some console logging I found the issue to be faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor(), which takes up most of the time. How can I reduce it to about 3s instead?```video.addEventListener('play',()=>{    const faceMatcher = new faceapi.FaceMatcher(vectors,0.5)    var idVar = setInterval(async () => {        const detections2 = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor()        const results = faceMatcher.findBestMatch(detections2.descriptor) // for detectSingleFace        if (labels.includes(results['label'])) {            document.getElementById('msg').innerHTML=(`Hello, ${results['label']}!`)            clearInterval(idVar)            // setTimeout(window.close,5000)    }    },500)})```;"[""Purchasing good GPU may help (especially NVIDIA good gpu, like GTX 1060). If you using CPU backend, then i don't know, for me 10 fps more than enough on i9-9900K ====="", '@adumbz  can i get full your code=====', 'I am no longer working on this project so I will close this issue, appreciate your replies @dalisoft @fuadkhalis =====', 'Hi, I am having the same time issue. Did you manage to make the process faster? Thank you. =====', ""Hi @idkidk-idk, sorry for the late reply. I did not manage to do it, suspected it was due to GPU as I was mostly working on my laptop with integrated graphics. I did not notice this issued being raised up much, I would think that this program was built more for a single setup with continuous use, instead of short separate executions. I eventually switched to a paid API with FaceX for conversion and comparison using vectors returned from their API. It was a lot more lightweight as the heavy work was done on their side, and wasn't too expensive for short term projects (7usd per 5000 calls per month). Could get results in about 1-3s depending on your implementation. Hope this helps!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/511;mtcnn is deprecated and will be removed soon;1;closed;2019-12-24T12:57:29Z;2020-01-29T09:49:42Z;Hey there!Just interested why MTCNN is deprecated and what should we use instead?Is there any topic to read or discussion about it?MTCNN is described as fast and lightweight alternative to SSD Mobilenet v1 in your [great article](https://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740) and that deprecation surprised me quite a lot :)Thanks!;['The TinyFaceDetector should perform way better than MTCNN, the article is a bit out of date, sorry for that, I might update the article soon. MTCNN is more or less an artifact, which I do not want to maintain anymore. Furthermore I am working on bringing state of the art face detection to this project, which will still take some time however. Until then I would recommend to use the TinyFaceDetector.=====']
https://github.com/justadudewhohacks/face-api.js/issues/508;One face id for one face;8;closed;2019-12-17T05:59:54Z;2020-02-03T11:28:45Z;Different ids are generated every time for one face id, is there a range for each face id, so that if it is in that range we can understand it is that face.;"[""I assume an embedding is being generated. (a multidimensional vector)Then it's up to you to determine a distance ([euclidean](https://en.wikipedia.org/wiki/Euclidean_distance#n_dimensions), for example) in this space that would define being the same person or not (maybe if distance > 5 then it's not the same person) It's up to you to determine a good distance value based on the model being used and the distance function being used.====="", 'thanks, I check and will write you=====', 'I get this data  http://prntscr.com/qcduhuWhat data or digits do I need to use to do that calculation=====', 'The descriptor, I imagine.=====', 'Hi ) how can we detect faces from afar?=====', 'Blaze face for tfjs works well for small faces in a video https://www.npmjs.com/package/@tensorflow-models/blazeface?linkId=80168838=====', 'thanks.=====', 'when I use this module I get this error, can you help me? tfjs-core:17 Uncaught (in promise) TypeError: Ot.makeTensor is not a function=====']"
https://github.com/justadudewhohacks/face-api.js/issues/507;Towards Head Pose Recognition;1;closed;2019-12-16T13:19:32Z;2019-12-30T15:37:25Z;"I was able to compile OpenCV solvePnP to wasm and identify head pose using the facial landmarks provided by face-api.**I would appreciate if the confidence of the points were informed so that I'm only using ""good"" points to solve the PnP and I'm able to ignore occluded points.**- Is that a low hanging fruit or would a new network be needed?And as far as work should be done...- Would this project be willing to support this approach to solving head pose as ""oficial""? Should I focus my efforts on a fork to be potentially accepted as PR here or should I work on a library to be used along face-api.js?OpenCV solves the pnp really fast (16fps on webcam example with it disabled to 15fps with it enabled) but it is a HUGE to download (I was able to trim it down to 2MB -no compression- and I believe it could decrease more)[Try it on my fork, run browser examples and check webcam landmark demo](https://github.com/lucasavila00/face-api.js) [(beware: really **BAD UGLY** code just for experimenting right now)](https://github.com/lucasavila00/face-api.js/blob/f0cb95576e9b54fd231a694fb16eb07727285d6f/examples/examples-browser/views/webcamFaceLandmarkDetection.html#L180)Here is a demo of cv.solvePnPRansac(solver=SOLVEPNP_UPNP)  + cv.solvePnPRefineVVS + kalman filter on the lowest resolution using tinynet for both networks.![ezgif com-video-to-gif](https://user-images.githubusercontent.com/12750442/70925321-ccbdb280-2009-11ea-99b9-cf94bb12e42b.gif)";"[""I ended up training an AI that solves the PnP problem.It's being used to run my project: https://filtrou.meThe project is open sourced here: https://github.com/lucasavila00/filtroumeFeel free to use my code and AI any way you would like!@justadudewhohacks would you be willing to integrate it with face-api.js?I wrote a blog post explaining how stuff works: https://filtrou.me/build-one-yourself/If you're curious as to how to train the AI there is a notebook explaining everything here: https://github.com/lucasavila00/filtroume/blob/master/tf/all.ipynb=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/500;face-api is incompatible with TFJS 1.3;1;closed;2019-12-09T18:19:41Z;2019-12-16T08:40:41Z;Using face-api with recent TensorflowJS builds results in an error when loading models.See: https://github.com/justadudewhohacks/face-api.js/issues/455We suspect this is because the saved models using by face-api are too old and need to be re-trained or re-exported to be compatible with new TFJS releases;['They dont need to be retrained, I upgraded tfjs-core to 1.4.0. =====']
https://github.com/justadudewhohacks/face-api.js/issues/499;TensorFlow Synchronous processing on Node.js;1;closed;2019-12-09T18:07:42Z;2020-01-28T16:14:37Z;Per Tensorflow's documentation, the node.js implementation is synchronous and takes (https://www.tensorflow.org/js/guide/nodejs - Production considerations).From looking at the code, it appears to me that no special action was taken in face-api to address this, meaning that the main thread will be blocking when running a face detection on Node JS. Am I correct in my understanding?(We're using face-api on a web server to detect faces in uploaded images).;"[""> From looking at the code, it appears to me that no special action was taken in face-api to address this, meaning that the main thread will be blocking when running a face detection on Node JS. Am I correct in my understanding?Correct, face-api.js doesn't touch any of the backend logic, it's only build on top of it.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/488;I'm having a issue in running the example;1;closed;2019-11-28T14:44:43Z;2019-12-07T10:04:38Z;I'm having following issue,cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].Error: The specified module could not be found.<MyLoc>\face-api.js\examples\examples-nodejs\node_modules\@tensorflow\tfjs-node\dist\index.js:44:16)    at Module._compile (internal/modules/cjs/loader.js:1121:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1160:10)    at Module.load (internal/modules/cjs/loader.js:976:32)    at Function.Module._load (internal/modules/cjs/loader.js:884:14);"[""This error doesn't come from face-api.js, it comes from tfjs-node. Please make sure you are using the same tfjs-core and tfjs-node version as suggested in the package.json of the example.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/484;Is there a way to convert these models to keras?;2;closed;2019-11-24T03:35:10Z;2019-12-07T01:42:56Z;Really nice work, impressed by the speed and acuracy. Is there a way to convert these models to keras? I just want to use it in python env. Thanks.;['I basically answered this here: #393. There is no straight forward way to do so. You can of course rebuild the models with tensorflow, then deserialize the weights in a similar way and then convert the tensorflow model to a keras model.In future models that I am training with tensorflow, I will also upload the .pb file of the trained model so that people can use it with python.=====', 'ok thanks=====']
https://github.com/justadudewhohacks/face-api.js/issues/476;how to train a image in faceapi.fetchImage with base64?;2;closed;2019-11-19T08:53:33Z;2019-11-27T07:41:32Z;i want to train a base64 image in faceapi.fetchImage but i get error from it , do you have a reference to solve it?thank you;['do you have solution for it ? i am also looking for same=====', '> do you have solution for it ? i am also looking for sameactually  faceapi.fetchImage() will be generate a base64 source , so you can dirrectly put base64 in faceapi.fetchImage()ex : let img = await faceapi.fetchImage(base64_image),let detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor(),you can check it through console.log=====']
https://github.com/justadudewhohacks/face-api.js/issues/473;Unexpected token < in JSON at position 6;1;closed;2019-11-15T12:37:41Z;2019-11-18T11:00:55Z;"```async function detectarFace(){	await faceapi.loadSsdMobilenetv1Model('/models'),	if(inputRosto.files && inputRosto.files[0]){		var reader = new FileReader(),		reader.readAsDataURL(inputRosto.files[0]),		reader.onload = async event => {			imagemPreview.src = event.target.result,			displayInformacoes.innerHTML = ""processando..."",			let deteccao = await faceapi.detectSingleFace(imagemPreview)										.withFaceLandmarks()										.withFaceExpressions()										.withAgeAndGender(),		}	}}```![Capturar](https://user-images.githubusercontent.com/9268549/68944016-861b3500-078b-11ea-93a2-d278d41a20a1.PNG)";['See my answer in #445.=====']
https://github.com/justadudewhohacks/face-api.js/issues/468;Wont load the shards just the manifest;3;closed;2019-11-08T21:13:51Z;2020-08-02T16:25:45Z;"I'm trying to load the tinyFaceDetector and faceLandmark68TinyModel.  The manifests load just fine but I don't see the shards coming in.  In the console I'm getting an ""Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0"".here is my code```import ""./main.scss"",import * as FaceApi from ""face-api.js"",window.addEventListener(""load"", function() {  async function loadModels() {    await Promise.all([      FaceApi.nets.tinyFaceDetector.loadFromUri(""/models/""),      FaceApi.nets.faceLandmark68TinyNet.loadFromUri(""/models/""),    ]),    const image = document.querySelector("".img""),    const detection = await FaceApi.detectSingleFace(      image,      new FaceApi.TinyFaceDetectorOptions()    ).withFaceLandmarks,    console.log(detection),  }  try {    loadModels(),  } catch (e) {    console.log(e),  }}),```any help would be greatly appreciatedthanks!!";"['I would check:1. the network tab at web inspector,  it should be doing two ""fetch""2. the JSON downloaded is complete and correctI had a similar problem in which my framework was not loading non-extension files (as the shards)=====', 'Yeah Thanks,That is what it ended up being, my bundler was not bundling the shards.=====', 'How did you resolve the bundling issue and/or the non-extension issue (did you add .weights to the ext? That did not work for me..) Thanks!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/466;TypeError: st().registerTensor is not a function ;3;closed;2019-11-07T08:50:26Z;2019-12-07T14:47:56Z;![Screenshot from 2019-11-05 18-19-47](https://user-images.githubusercontent.com/53167951/68373125-cf102f80-0174-11ea-96ad-f9a59ba241c8.png)I get this error while using `withFaceLandmarks()` . Many other functions like `detectSingleFace()` `detectAllFaces()` cause the same error if I use other models that are not `tinyFaceDetector`.Can anyone help me ? Many thanks.Here is my code:```    await faceapi.nets.tinyFaceDetector.loadFromUri(modelsUri),    await faceapi.nets.faceLandmark68TinyNet.loadFromUri(modelsUri),    const camera = new Camera({      fps,      onSnapshot: processImage,    }),    const cameraSettings = await camera.start(),    async function processImage(image) {      const face = await faceapi                          .detectSingleFace(image, new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold }))                          .withFaceLandmarks(true),   }```;"[""I'm also seeing this issue. My setup is essentially the faceRecognition example to a T. I haven't been able to figure out what the problem is as of yet. Some help would be appreciated.====="", 'Downgrading tfjs-core appears to fix it (via [this other issue](https://github.com/justadudewhohacks/face-api.js/issues/455#issuecomment-548987538) )=====', 'Same issue as #455.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/463;TypeError: forwardFunc is not a function;2;closed;2019-11-05T13:22:15Z;2021-12-12T13:55:27Z;"Hi!  Well, my question is on the title, so here is my full code:```/////////////////////////////////////////Requirements'use strict',require('@tensorflow/tfjs-node'),const tf = require('@tensorflow/tfjs'),const { createCanvas, createImageData, Image } = require('canvas'),const nodeFetch = require('node-fetch'),const fapi = require('face-api.js'),const path = require('path'),const width = 640,const height = 480,fapi.env.monkeyPatch({ fetch: nodeFetch }),const MODELS_URL = path.join(__dirname, '/examples/video-compositing/models'),/////////////////////////////////////////Load ModelsPromise.all([    fapi.nets.tinyFaceDetector.loadFromDisk(MODELS_URL),    fapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL),    fapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL),    fapi.nets.faceExpressionNet.loadFromDisk(MODELS_URL)]).then(beforeOffer),/////////////////////////////////////////After loading models run this functionfunction beforeOffer() {    // TODO(mroberts): Is pixelFormat really necessary?    const canvas = createCanvas(width, height),    const context = canvas.getContext('2d', { pixelFormat: 'RGBA24' }),/////////////////////////////////////////Get an image and convert it into tensor 3d    async function loadLocalImage(imgPath) {        try {            let img = new Image(),            img.onload = () => {                context.drawImage(img, 0, 0),            }            img.onerror = err => {                console.log(err),            }            img.src = imgPath,            const image = tf.browser.fromPixels(canvas),            return image,        } catch (e) {            console.log(e),        }    }/////////////////////////////////////////Get image tensor    async function getImage(imgPath) {        try {            const image = await loadLocalImage(imgPath),            return image,        } catch (e) {            console.log(e),        }    }/////////////////////////////////////////Show results    async function detections() {        const img = await getImage(""/home/ilya/Downloads/putin.png""),        const result = await fapi.detectAllFaces(img, new fapi.TinyFaceDetectorOptions()).withFaceLandmarks(),        console.log(result),    }    detections(),}```Here is an error:![ucZDChN3RKQ](https://user-images.githubusercontent.com/42372955/68211463-abb87980-ffe8-11e9-8b0c-c6e79b189cf0.jpg)When i use just `await fapi.detectAllFaces(img, new fapi.TinyFaceDetectorOptions())` without `.withFaceLandmarks()` all works fine and i get ```[  FaceDetection {    _imageDims: Dimensions { _width: 640, _height: 480 },    _score: 0.7238702505633767,    _classScore: 0.7238702505633767,    _className: '',    _box: Box {      _x: -5.652424869069339,      _y: 4.018378046097197,      _width: 76.10965370740514,      _height: 61.51865117662532    }  }]```";"[""So i’ve installed tfjsnode, tfjs, tfjs-core  1.2.9 version and all works fine.```[  {    detection: FaceDetection {      _imageDims: [Dimensions],      _score: 0.9932203603310482,      _classScore: 0.9932203603310482,      _className: '',      _box: [Box]    },    landmarks: FaceLandmarks68 {      _imgDims: [Dimensions],      _shift: [Point],      _positions: [Array]    },    unshiftedLandmarks: FaceLandmarks68 {      _imgDims: [Dimensions],      _shift: [Point],      _positions: [Array]    },    alignedRect: FaceDetection {      _imageDims: [Dimensions],      _score: 0.9932203603310482,      _classScore: 0.9932203603310482,      _className: '',      _box: [Box]    }  }]```====="", 'I got this error and the reason is because face-api only works with `@tensorflow/tfjs-node` version 1. So if you do `yarn add @tensorflow/tfjs-node@1.7.4` it should work.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/458;Cannot Get descriptor of undefined error;1;closed;2019-11-01T11:08:59Z;2021-09-20T11:15:45Z;"Hi Victor, absolutely fallen in love with your library! However, I keep getting this error `Cannot read property 'descriptor' of undefined` the weird thing is when I first started working with face-api it was working perfectly then all of a sudden this issue arose. My code:```const video = document.getElementById('video'),var users = [],var models = [],var canvas,var displaySize,var interVal,var faceMatcher,var labeledFaceDescriptors,var previousUser = '',var userData = '',faceapi.tf.getBackend(),faceapi.tf.ENV.set('WEBGL_PACK', false)Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri('..//models'),    faceapi.nets.faceLandmark68Net.loadFromUri('..//models'),    faceapi.nets.faceRecognitionNet.loadFromUri('..//models'),    faceapi.nets.faceExpressionNet.loadFromUri('..//models'),    faceapi.nets.ssdMobilenetv1.loadFromUri('../models')]).then(startVideo),function startVideo() {    navigator.getUserMedia({            video: {}        },        stream => video.srcObject = stream,        err => {            console.error(err),            alert(""please check if your webcam is working then refresh this page.""),            document.querySelector("".loading-text"").innerHTML = ""Error!"",            document.querySelector("".first"").style.display = ""none"",        }    )}/* -------------------------------------------------------------------------- *//*                         Facial Expression Detection                        *//* -------------------------------------------------------------------------- */function facialExpressions() {    video.addEventListener('play', () => {        const canvas = faceapi.createCanvasFromMedia(video)        document.body.append(canvas)        const displaySize = {            width: video.width,            height: video.height        }        faceapi.matchDimensions(canvas, displaySize)        setInterval(async () => {            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions(),            const resizedDetections = faceapi.resizeResults(detections, displaySize),            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height),            faceapi.draw.drawDetections(canvas, resizedDetections),            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),            faceapi.draw.drawFaceExpressions(canvas, resizedDetections),        }, 100)    })},/* -------------------------------------------------------------------------- *//*                             Facial Recognition                             *//* -------------------------------------------------------------------------- */var continueRunning = true,var first = document.querySelector("".first""),var second = document.querySelector("".second""),first.addEventListener(""click"", function () {    continueRunning = false,    alert(""stopped""),    video.removeEventListener(""play"", startRecognitionEngine),    document.querySelector("".loading-text"").innerHTML = ""Engine Stopped."",    document.querySelector("".first"").style.display = ""none"",    document.querySelector("".second"").style.display = ""block"",    stopMonitoring(),}),second.addEventListener(""click"", function () {    // Working Code Below    continueRunning = true,    window.location.reload(),    /* startRecognitionEngine(),    document.querySelector("".first"").style.display = ""block"",    document.querySelector("".second"").style.display = ""none"", */}),/* -------------------------------------------------------------------------- *//*                             Interval Functions                             *//* -------------------------------------------------------------------------- */function stopMonitoring() {    clearInterval(interVal),}function startMonitoring() {    interVal = setInterval(recog, 500),}async function recog() {    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors(),    const resizedDetections = faceapi.resizeResults(detections, displaySize),    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height),    // faceapi.draw.drawFaceExpressions(canvas, resizedDetections)    const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor)),    results.forEach((result, i) => {        const box = resizedDetections[i].detection.box,        let res = result.toString(),        res = res.replace(/[()]/g, ' ').replace(/ /g, ' ').replace(/[0-9]/g, ' ').replace(/[.,\s]/g, ' '),        if (result.toString() !== ""unknown"") {            if (previousUser !== res) {                previousUser = res,                console.log(res),                getUser(res.trim()),                console.log(""Sent!""),            }        }        const drawBox = new faceapi.draw.DrawBox(box, {            label: result.toString()        }),        drawBox.draw(canvas)    }),}async function startRecognitionEngine() {    displaySize = {        width: video.width,        height: video.height    }    canvas = await faceapi.createCanvasFromMedia(video)    document.querySelector("".right"").appendChild(canvas)    faceapi.matchDimensions(canvas, displaySize),    labeledFaceDescriptors = await loadLabeledImages(),    console.log(labeledFaceDescriptors),    console.log(labeledFaceDescriptors),    document.querySelector(""#video"").style.visibility = ""visible"",    document.querySelector("".loading-text"").style.display = ""none"",    document.querySelector("".ready-text"").style.display = ""block"",    document.querySelector("".first"").style.display = ""block"",    faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.5),    startMonitoring(),}async function facialRecognition() {    video.addEventListener('play', startRecognitionEngine),},var index = 0,async function loadLabeledImages() {    console.log(""Fetching Descriptors...""),    const labels = await users,    const nums = await models,    return Promise.all(        labels.map(async label => {            const descriptions = []            for (let i = 1, i <= nums[index], i++) {                const re = /^[a-zA-Z]+[a-zA-Z]+$/,                var user = label,                if (user.trim().indexOf(' ') != -1) {                    var uname = label.split(' '),                    var folder = uname[0] + '%20' + uname[1],                    console.log(""DOLDER"", folder)                } else {                    var folder = label,                    console.log(""FOLDER"", folder)                }                let imgUrl = `https://firebasestorage.googleapis.com/v0/b/facerecognitionapp-af6b6.appspot.com/o/${folder}%2F${i}.jpg?alt=media&token=1dd488c9-09a2-45cd-8fda-d06260e64426`,                const img = await faceapi.fetchImage(imgUrl),                // const img = await faceapi.fetchImage(`https://raw.githubusercontent.com/WebDevSimplified/Face-Recognition-JavaScript/master/labeled_images/${label}/${i}.jpg`)                const detections = await faceapi.detectSingleFace(img)                .withFaceLandmarks(true)                .withFaceDescriptors(),                console.log(imgUrl),                descriptions.push(detections.descriptor)                index++,            },            console.log("".""),            return new faceapi.LabeledFaceDescriptors(label, descriptions),        })    )}(async function getUsers() {    console.log(""Fetching Users""),    let friendsRef = db.collection(`users`),    var allUsers = await friendsRef        .orderBy(""userName"")        .onSnapshot(snapshot => {            let udocs = snapshot.docChanges(),            if (udocs.length > 0) {                udocs.forEach((user, index) => {                    users.push(user.doc.data().userName),                    models.push(user.doc.data().numOfModels),                    // console.log(user.doc.data().userName),                }),                // songs.splice(0, 1),                allUsers(),                console.log(""Done Fetching...""),                facialRecognition(),            } else {                console.error(""User Retrieval Failed""),            }        }),})(),async function getUser(uname) {    let userRef = db.collection(`users`),    var singleUser = await userRef        .where(""userName"", ""=="", uname)        .onSnapshot(snapshot => {            let udocs = snapshot.docChanges(),            if (udocs.length > 0) {                udocs.forEach((user, index) => {                    userData = user.doc.data(),                    if (user.doc.data().atWork === false) {                        db.collection(""users"").doc(user.doc.id).set({                            atWork: true                        }, {                            merge: true                        }),                        console.log(userData),                    } else {                        console.log(""Logged Out!""),                    }                }),                singleUser(),                console.log(""Found User:""),            } else {                console.log(""Unknown""),                console.log(uname),            }        }),},```";['@Dave-Nandwa you managed to solve the problem, I have a similar one, it works on many computers, but on some it does not - the problem is that sometimes after withFaceDescriptor () - detections is undefined. =====']
https://github.com/justadudewhohacks/face-api.js/issues/455;Issue loading: TypeError: Nt.makeTensor is not a function;24;closed;2019-10-29T23:11:35Z;2021-06-20T16:43:10Z;I just started and in the latest version I am getting the following error in the browser:```tf-core.esm.js:17 Uncaught (in promise) TypeError: Nt.makeTensor is not a function    at Sn (tf-core.esm.js:17)    at kn (tf-core.esm.js:17)    at o (tf-core.esm.js:17)    at Fh (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at tf-core.esm.js:17```I installed `face-api.js` with `npm install face-api.js`. Now using it as follows:```import * as faceapi from 'face-api.js',await faceapi.nets.tinyFaceDetector.loadFromUri('/models'),```As soon as I try to load the model, I get the type error above. Any idea's where it is going wrong? I saw this https://github.com/tensorflow/tfjs/issues/2194#issuecomment-546187719 but couldn't see a solution to that. Thanks!;"['Hi, I just got the same error(tfjs-core@1.3.1) and I downgraded to tfjs-core@1.3.0, and this error disappeared, however, a new error rendered. Anyway, you could have a try.=====', 'When downgrading tfjs-core to 1.2.9 I get this error when trying to use the librarySsdMobilenetv1.js:26 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (SsdMobilenetv1.js:26)    at SsdMobilenetv1.<anonymous> (SsdMobilenetv1.js:81)    at step (tslib.es6.js:196)    at Object.next (tslib.es6.js:127)    at fulfilled (tslib.es6.js:80)here is the lock file of a project where this library still works.[yarnlockfile.txt](https://github.com/justadudewhohacks/face-api.js/files/3787177/yarnlockfile.txt)=====', '@Spodeopieter, I think your last error has to do with not asynchronously loading in your model. Are you sure it is being loaded in correctly before you try to use it?=====', '@luucv  this is how I am loading the models : `  loadModels = async () => {`    `const MODEL_URL = ""/models"",`   ` await faceapi.loadSsdMobilenetv1Model(MODEL_URL),`   ` await faceapi.loadFaceLandmarkModel(MODEL_URL),`   ` await faceapi.loadFaceRecognitionModel(MODEL_URL),` ` },`=====', ""@luucv have the same error right now. I installed face-api.js via yarn. I retried it with `npm install face-api.js` and it worked without any problem.I'm using webpack-encore with Symfony.====="", 'Have the same problem.Try to delete package-lock.json, node_modules. Then install the latest version (currently 0.21.0)=====', 'The issue is most certainly due to your package managers installing a tfjs core version other than 1.2.9 (which is the one the latest face-api.js version uses). May be an issue with yarn?Try deleting your yarn locks / package locks and do a clean installation. If for some reason the issue still remains, try explicitly installing `npm i @tensorflow/tfjs-core@1.2.9`.=====', '@justadudewhohacks Thanks for the Tip!I\'ve solved my problem :DYou do not just have to say `""@tensorflow/tfjs-core"": ""1.2.9""` in the dependencies-section of the package.json, you also have to prevent _tfjs-image-recognition-base_ from getting a newer version:```""resolutions"": {    ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""}```=====', ""I've solved by taking these steps:1.  delete tensorflow node module2. `npm i @tensorflow/tfjs-node@1.2.9`3. `npm i @tensorflow/tfjs-code@1.2.9`The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.====="", 'I have same problem.```""@tensorflow/tfjs-core"": ""1.2.9"",""@tensorflow/tfjs-node"": ""1.2.9"",""face-api.js"": ""^0.21.0"",```5 days ago, It was operating normally. =====', '> @justadudewhohacks Thanks for the Tip!> I\'ve solved my problem :D> > You do not just have to say `""@tensorflow/tfjs-core"": ""1.2.9""` in the dependencies-section of the package.json, you also have to prevent _tfjs-image-recognition-base_ from getting a newer version:> > ```> ""resolutions"": {>     ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""> }> ```This helped. looks like `tfjs-image-recognition-base` was secretly installing the latest tfjs behind our backs.=====', 'have same problem, resolutions no helped=====', 'installing face-api with npm instead of yarn seems to solve the problem for me.=====', ""> installing face-api with npm instead of yarn seems to solve the problem for me.I'm running into the same issue with yarn, installing with npm solved me problem as well.====="", '> looks like `tfjs-image-recognition-base` was secretly installing the latest tfjs behind our backs.I added the `resolutions` section as suggested above and by [this help doc](https://yarnpkg.com/lang/en/docs/selective-version-resolutions/) but it was apparently ignored by yarn cuz it resolved to 1.3.1 anyway per yarn.lock.  I\'ve got a yarn monorepo so I\'m sure there\'s either some logical reason or some bug as to why that was the case.I was able to get around it by **manually editing** the `yarn.lock` to change the section for `""@tensorflow/tfjs-core@^1.2.9"":` to be identical to the section for `""@tensorflow/tfjs-core@1.2.9"":` (no caret).  But that seems super hacky and tenuous.  If this is working in npm but not yarn that seems like a yarn bug to me.=====', ""> I've solved by taking these steps:> > 1. delete tensorflow node module> 2. `npm i @tensorflow/tfjs-node@1.2.9`> 3. `npm i @tensorflow/tfjs-code@1.2.9`> > The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.This solved the same issue I was having. Hope this helps somebody. ====="", 'So for me nothing worked what did work was this**FOR YARN USERS**1. add ""resolutions"": {    ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""}2. in the package add these lines  ""dependencies"": {    ""@tensorflow/tfjs-core"": ""1.2.9"",    ""face-api.js"": ""^0.21.0"",    ""next"": ""9.1.4"",    ""react"": ""16.12.0"",    ""react-dom"": ""16.12.0"",    ""react-stickynode"": ""^2.1.1"",    ""styled-components"": ""^4.4.1"",    ""tfjs-image-recognition-base"": ""^0.6.2""  }actually installing tfjs-image-recognition-base worked!=====', ""I have this same issue. I get the error: ![image](https://user-images.githubusercontent.com/39976117/69595561-c44afc80-0fb4-11ea-8a2a-4819275fe322.png)when I try to load any model. I have the models in `public/models` and am trying to access them with ``` await faceapi.nets.ssdMobilenetv1.loadFromUri('/models') ```I've tried all the fixes mentioned above and I don't think it's an issue with versioning, or npm. Is there another way to load the models? Thanks!====="", 'Also having the same issue, though my error message reads:`engine_1.ENGINE.makeTensor is not a function`Tried some of the above fixes with no luck. Using tfjs 1.2.9 also.=====', ""We need to upgrade to TFJS 1.3.x for other reasons.  Any idea what's causing this problem with newer TFJS packages?  I don't see `makeTensor` anywhere in the code here or in `tfjs-image-recognition-base`I'm guessing maybe the models need to be regenerated for TF 1.3.x?====="", ""> I've solved by taking these steps:> > 1. delete tensorflow node module> 2. `npm i @tensorflow/tfjs-node@1.2.9`> 3. `npm i @tensorflow/tfjs-code@1.2.9`> > The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.It solved my problem.====="", 'I upgraded the tfjs-core version the latest release. Also I decided to move tfjs-image-recognition-base to face-api.js, so yarn should not have any reason to install different versions of tfjs-core anymore unless you or one if your dependencies is explicitly installing another version.=====', 'I\'m getting a ""Kt.makeTensor is not a function"", more specifically: Uncaught (in promise) TypeError: Kt.makeTensor is not a function    at Rn (face-api.min.js:2426)    at In (face-api.min.js:2407)    at e (face-api.min.js:11444)    at Vh (face-api.min.js:11445)    at face-api.min.js:12054    at Array.forEach (<anonymous>)    at face-api.min.js:12053    at Array.forEach (<anonymous>)    at face-api.min.js:12047    at face-api.min.js:103I\'ve tried all these resolutions and none of them worked. Could anyone help please?=====', '> I\'m getting a ""Kt.makeTensor is not a function"", more specifically:> Uncaught (in promise) TypeError: Kt.makeTensor is not a function> at Rn (face-api.min.js:2426)> at In (face-api.min.js:2407)> at e (face-api.min.js:11444)> at Vh (face-api.min.js:11445)> at face-api.min.js:12054> at Array.forEach ()> at face-api.min.js:12053> at Array.forEach ()> at face-api.min.js:12047> at face-api.min.js:103> > I\'ve tried all these resolutions and none of them worked. Could anyone help please?I\'m facing the same problem. Is there any fix available for it?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/451;Improve face match accuracy;1;closed;2019-10-27T17:36:39Z;2019-11-21T10:51:07Z;Hi, faceapi.js is amazing, I currently use it in my login system for 3 months, registered ~ 50 people, and they logged in 4 times a day, I use maxDistanceDescriptor equal to 0.45-0.5 in FaceMatcher, each person has 1-3 descriptorsDuring this time, the match has been wrong 4 times, at least those that I have been able to detect.Example:Petter's photo-> Its detected as Mike with distance of 0.44Snapshots from : - Fixed tablet camera login, it have resolution its from 2MP and size of 640x480 - Mobile Camera - resolution dynamic (each people)How can I improve or verify that the matching is correct? Add more descriptors? How many? Update descriptors each month? I will planning register ~ 1000 people :)faceMatcher:``    faceMatcher = new faceapi.FaceMatcher (        labeledDescriptors,        maxDescriptorDistance      ),``Thank you;['Improved little, I add 10 photos per people and working with maxDescriptors 0.45=====']
https://github.com/justadudewhohacks/face-api.js/issues/449;Can I use face-api.js with ReactJS? ;1;closed;2019-10-24T01:18:38Z;2019-10-26T07:27:18Z;I would like to run a web application using face-api as backend and ReactJS as frontend. It is possible?Can someone help me? ;['Yes, just google face-api.js react and some projects will pop up. There is even a tutorial on medium: https://towardsdatascience.com/facial-recognition-spa-for-bnk48-idol-group-using-react-and-face-api-js-ad62b43ec5b6=====']
https://github.com/justadudewhohacks/face-api.js/issues/448;Getting a match with children;1;closed;2019-10-23T03:58:31Z;2019-10-23T22:49:38Z;I'm using the following code to loop through my sample images using the following code:		const imageFile = 'eric_2.jpg',	   await faceapi.nets.ssdMobilenetv1.loadFromDisk('./models/ssd_mobilenetv1'),	   await faceapi.nets.faceLandmark68Net.loadFromDisk('./models/face_landmark_68'),	   await faceapi.nets.faceRecognitionNet.loadFromDisk('./models/face_recognition'),	   await faceapi.nets.ageGenderNet.loadFromDisk('./models/age_gender_model'),          // This method returns the files in the specified path	   const files = await api.listFiles(`${__dirname}/images/`),	   const image1 = await canvas.loadImage(`./images/${imageFile}`),	   const detection1 = await faceapi.detectSingleFace(image1, new faceapi.SsdMobilenetv1Options()).withFaceLandmarks().withFaceDescriptor(),	   	   for(const file of files){		 			const image2 = await canvas.loadImage(`./images/${file}`),			const detection2 = await faceapi.detectSingleFace(image2, new faceapi.SsdMobilenetv1Options()).withFaceLandmarks().withFaceDescriptor(),			const distance = await faceapi.euclideanDistance(detection1.descriptor, detection2.descriptor),			var match = '=> not a match',			if(distance < 0.7)				match = '=> match',			console.log(`Euclidean distance from image: ${imageFile} to ${file} is ${distance} ${match}`),			console.log(''),		}	}I take it back it seems like I get a match with everyone I have taken using my phone to my image**Here is the code output of my kids that is saying its a match:Euclidean distance from image: eric_1.jpg to dj.jpg is 0.4787967301362279 => matchEuclidean distance from image: eric_1.jpg to drew.jpg is 0.5003818530582609 => match**Celebrities I'm testing that does not match:Euclidean distance from image: eric_1.jpg to brad_pitt.jpg is 0.7968919207835512 => not a matchEuclidean distance from image: eric_1.jpg to cindy_crawford.jpg is 0.8775858144569341 => not a matchEuclidean distance from image: eric_1.jpg to kaley_cuoco_1.jpg is 0.8474952774223865 => not a matchEuclidean distance from image: eric_1.jpg to kaley_cuoco_2.jpg is 0.8581044739989803 => not a matchMore pictures of me that matches.Euclidean distance from image: eric_1.jpg to eric_1.jpg is 0 => matchEuclidean distance from image: eric_1.jpg to eric_2.jpg is 0.36870495014601334 => matchEuclidean distance from image: eric_1.jpg to eric_3.jpg is 0.680211514000396 => matchThe images of me and my kids are 3456x4608 72dpi.  The images of the celebrities I just downloaded from the web 1435x2154 300dpi.I downloaded the models from the URL https://github.com/justadudewhohacks/face-api.js-modelsAm I doing something wrong?  This is my first attempt using this API.Thanks;['This can be disregarded.   After looking at the landmarks I created I noticed that my pictures were rotated 90degrees.  After rotating them back to properly show the right position it gives me the right results now.=====']
https://github.com/justadudewhohacks/face-api.js/issues/447;How do i train the models for my own images;1;closed;2019-10-22T12:53:41Z;2019-10-26T07:32:44Z;"Hi THE Dude who hacks,I see in the documentation mentioning this ""The model is not limited to the set of faces used for training, meaning you can use it for face recognition of any person, for example yourself. You can determine the similarity of two arbitrary faces by comparing their face descriptors, for example by computing the euclidean distance or using any other classifier of your choice.""but no placeholder or caveat to be found with respect to training the models on my own images,Please suggest/guide the path to be taken.";['> but no placeholder or caveat to be found with respect to training the models on my own images,You do not train your own model. Check out the examples and tutorials first, these should answer your question.=====']
https://github.com/justadudewhohacks/face-api.js/issues/434;Eculidean issues when doing face Recognization;5;closed;2019-10-04T12:46:23Z;2021-06-14T23:13:29Z;Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at euclideanDistance (euclideanDistance.js:2)    at FaceMatcher.js:60    at Array.map (<anonymous>)    at FaceMatcher.computeMeanDistance (FaceMatcher.js:59)    at FaceMatcher.js:72    at Array.map (<anonymous>)    at FaceMatcher.matchDescriptor (FaceMatcher.js:69)    at FaceMatcher.findBestMatch (FaceMatcher.js:79)    at camera.jsx:78    at Array.map (<anonymous>)    at camera.jsx:78;"['This is my code```import React, { Component } from \'react\',import * as faceapi from \'face-api.js\'const MODEL_URL=\'/models\'// var labeledFaceDescriptors=0,// let count=0,class camera extends Component{  constructor(props) {    super(props),    this.videoTag = React.createRef()    this.state={       detection:null,       video:null    }    this.detect=this.detect.bind(this),  }   componentDidMount() {    // getting access to webcam    navigator.mediaDevices    .getUserMedia({video: true})    .then(stream =>    this.videoTag.current.srcObject = stream,    this.loadModels(),    this.detect()     )    .catch(console.log),  }  loadModels(){     faceapi.loadFaceDetectionModel(MODEL_URL),     faceapi.loadSsdMobilenetv1Model(MODEL_URL),   faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),   faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),     faceapi.loadFaceLandmarkModel(MODEL_URL),     faceapi.loadFaceRecognitionModel(MODEL_URL)  }  detect=async ()=>{    const videoTag=document.getElementById(\'videoTag\'),    const canvas=document.getElementById(\'myCanvas\'),        console.log(""geeting""),    const displaySize = { width: videoTag.width, height: videoTag.height },    faceapi.matchDimensions(canvas, displaySize),    setInterval(async () => {    let fullFaceDescriptions = await faceapi.detectAllFaces(videoTag).withFaceLandmarks(),    const value=fullFaceDescriptions.length,    this.setState({detection:value}),    console.log(""hiii"", this.state.detection),     fullFaceDescriptions = faceapi.resizeResults(fullFaceDescriptions, displaySize),     const labels = [\'praveen\',""p""]     const labeledFaceDescriptors = await Promise.all(     labels.map(async label => {     // fetch image data from urls and convert blob to HTMLImage element     const imgUrl = `/img/${label}.png`     const img = await faceapi.fetchImage(imgUrl)          // detect the face with the highest score in the image and compute it\'s landmarks and face descriptor     const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()     // console.log(fullFaceDescription.expressions.asSortedArray().toString().expression),     if (!fullFaceDescription) {        throw new Error(`no faces detected for ${label}`)     }          const faceDescriptors = [fullFaceDescription.descriptor]     return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)   }) )       const maxDescriptorDistance = 0.5     const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance)     const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))     results.forEach((bestMatch, i) => {          const box = fullFaceDescriptions[i].detection.box     const text = bestMatch.toString()  //this for basMatch name detection         const drawBox = new faceapi.draw.DrawBox(box, { label: text })   // canvas.getContext(\'2d\').clearRect(0, 0, canvas.width, canvas.height)   // faceapi.draw.drawDetections(canvas, fullFaceDescriptions)   //FaceExpression=fullFaceDescription  *(face-api.js)*   //this function calcultae the average expression *from face-api.js       drawBox.draw(canvas),     })   canvas.getContext(\'2d\').clearRect(0,` 0, canvas.width, canvas.height),    faceapi.draw.drawDetections(canvas, fullFaceDescriptions),    faceapi.draw.drawFaceLandmarks(canvas, fullFaceDescriptions)        }, 200),  }                render() {    return(       <div>        <div>          <video id=""videoTag"" style={{position:""absolute"", top:0}}                 ref={this.videoTag}                 width={500}                 height={500}                 autoPlay          ></video>        </div>        <div>          <canvas id=""myCanvas"" style={{position:""absolute"", top:0}}                                          height={500}                                          width={500}>          </canvas>         </div>         <h1 >{this.state.detection}</h1>      </div>                          ),  }}export default camera,```=====', 'i found the answer,in the fullFaceDescriptions is should include .withFaceDescriptor()=====', ""@thirukumars I try the above code but I got 'draw' is not exported from 'face-api.js' (imported as 'faceapi').can you explain me how to achieve this in react js====="", '@thirukumars since you are using `faceapi.detectAllFaces` , you should add `.withFaceDescriptors()` and not `.withFaceDescriptor()`. There should be an **s** there since it is not for a single face.=====', ""> @thirukumars I try the above code but I got 'draw' is not exported from 'face-api.js' (imported as 'faceapi').> can you explain me how to achieve this in react js@abdulkadhir do this in react it will work`import * as faceapi from 'face-api.js'`then use it where ever like this` faceapi.draw.drawDetections(canvasRef.current, resizedDetections)   faceapi.draw.drawFaceLandmarks(canvasRef.current, resizedDetections)   faceapi.draw.drawFaceExpressions(canvasRef.current, resizedDetections)`=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/433;createCanvasFromMedia canvas only works in Ionic browser serve but not working after compiled into Android;1;closed;2019-10-04T02:11:55Z;2019-10-08T02:57:31Z;I used face-api.js in my Ionic 4 project to detect face from camera stream. however the detection canvas only works in browser `ionic serve` and not working after i compiled it into Android apk. I can see all my models loaded in Android debug console but none of the `<canvas>` appended into my html. @justadudewhohacks  where did i do wrong?```  videoLoad() {    this.elementRef.nativeElement.querySelector('video')      .addEventListener('play', async () => {        const labeledFaceDescriptors = await this.loadLabeledImages()        const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)        const canvas = faceapi.createCanvasFromMedia(this.video)        document.getElementById('detectN').appendChild(canvas)        const displaySize = { width: this.video.width, height: this.video.height }        faceapi.matchDimensions(canvas, displaySize)        await faceapi.nets.tinyFaceDetector.loadFromUri('https://www.test.com/models'),        await faceapi.nets.faceLandmark68Net.loadFromUri('https://www.test.com/models'),        await faceapi.nets.faceRecognitionNet.loadFromUri('https://www.test.com/models'),        await faceapi.nets.faceExpressionNet.loadFromUri('https://www.test.com/models'),        await faceapi.nets.ssdMobilenetv1.loadFromUri('https://www.test.com/models'),        setInterval(async () => {          const detections = await faceapi.detectAllFaces(this.video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors().withFaceExpressions()          const resizedDetections = faceapi.resizeResults(detections, displaySize)          const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)          results.forEach((result, i) => {            const box = resizedDetections[i].detection.box            const drawBox = new faceapi.draw.DrawBox(box, { label: result.toString() })            drawBox.draw(canvas)          })        }, 800)      })  }```;['done figured it myself, not using `createCanvasFromMedia` but using Canvas tag directly inside HTML. Solved, thank you=====']
https://github.com/justadudewhohacks/face-api.js/issues/431;Recognition another faces;2;closed;2019-10-01T23:44:02Z;2019-10-12T10:36:47Z;How to train a model, to detect another peoples in face recognition?;['You just need to store all face descriptors in a json. Try to look this issue #231 or try this repo https://github.com/Mactacs/face-recognition-on-browser=====', 'You do not train a new model, with the existing model you are able to recognize any peoples face. If you are unsure how to do so, take a look at the tutorials and examples.=====']
https://github.com/justadudewhohacks/face-api.js/issues/427;npm run test fails with (404) Not Found, from url: weights_uncompressed/face_landmark_68_model.weights;1;closed;2019-09-29T15:38:11Z;2019-10-26T07:57:59Z;"```$ npm run test> face-api.js@0.21.0 test /Users/taylott/Projects/face-api.js> karma start29 09 2019 11:39:22.349:WARN [filelist]: Pattern ""/Users/taylott/Projects/face-api.js/weights_uncompressed/**/*"" does not match any file.29 09 2019 11:39:22.357:WARN [filelist]: Pattern ""/Users/taylott/Projects/face-api.js/weights_unused/**/*"" does not match any file.29 09 2019 11:39:22.770:INFO [compiler.karma-typescript]: Compiling project using Typescript 3.6.329 09 2019 11:39:29.286:INFO [compiler.karma-typescript]: Compiled 163 files in 6417 ms.29 09 2019 11:39:31.384:INFO [bundler.karma-typescript]: Bundled imports for 160 file(s) in 1594 ms.29 09 2019 11:39:33.000:WARN [karma]: No captured browser, open http://localhost:9876/29 09 2019 11:39:33.085:INFO [karma-server]: Karma v4.3.0 server started at http://0.0.0.0:9876/29 09 2019 11:39:33.085:INFO [launcher]: Launching browsers Chrome with concurrency unlimited29 09 2019 11:39:33.096:INFO [launcher]: Starting browser Chrome29 09 2019 11:39:36.086:INFO [Chrome 77.0.3865 (Mac OS X 10.14.3)]: Connected on socket 36m6lvhEuAIMS1riAAAA with id 21459220Chrome 77.0.3865 (Mac OS X 10.14.3) LOG: 'running tests on WebGL backend'Chrome 77.0.3865 (Mac OS X 10.14.3): Executed 3 of 179 SUCCESS (0 secs / 0.007 secs)Chrome 77.0.3865 (Mac OS X 10.14.3) faceLandmark68Net, uncompressed uncompressed weights computes face landmarks for squared input FAILED	Error: FaceLandmark68Net - load model before inference	    at FaceLandmark68Net.FaceProcessor.runNet (src/faceProcessor/FaceProcessor.ts:40:13 <- src/faceProcessor/FaceProcessor.js:68:31)	    at src/faceLandmarkNet/FaceLandmark68NetBase.ts:60:24 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:123:41	    at /var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13546:22	    at Engine.scopedRun (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13556:23)	    at Engine.tidy (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13545:21)	    at Object.tidy (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:964:28)	    at FaceLandmark68NetBase.forwardInput (src/faceLandmarkNet/FaceLandmark68NetBase.ts:59:15 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:120:31)	    at src/faceLandmarkNet/FaceLandmark68NetBase.ts:76:29 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:200:61	    at /var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13546:22	    at Engine.scopedRun (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13556:23)Chrome 77.0.3865 (Mac OS X 10.14.3) faceLandmark68Net, uncompressed uncompressed weights computes face landmarks for rectangular input FAILED	Error: FaceLandmark68Net - load model before inference	    at FaceLandmark68Net.FaceProcessor.runNet (src/faceProcessor/FaceProcessor.ts:40:13 <- src/faceProcessor/FaceProcessor.js:68:31)	    at src/faceLandmarkNet/FaceLandmark68NetBase.ts:60:24 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:123:41	    at /var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13546:22	    at Engine.scopedRun (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13556:23)	    at Engine.tidy (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13545:21)	    at Object.tidy (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:964:28)	    at FaceLandmark68NetBase.forwardInput (src/faceLandmarkNet/FaceLandmark68NetBase.ts:59:15 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:120:31)	    at src/faceLandmarkNet/FaceLandmark68NetBase.ts:76:29 <- src/faceLandmarkNet/FaceLandmark68NetBase.js:200:61	    at /var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13546:22	    at Engine.scopedRun (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:13556:23)Chrome 77.0.3865 (Mac OS X 10.14.3) ERROR  An error was thrown in afterAll  Error: failed to fetch: (404) Not Found, from url: weights_uncompressed/face_landmark_68_model.weights      at Object.<anonymous> (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:22185:31)      at step (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:142:27)      at Object.next (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:123:57)      at fulfilled (/var/folders/_c/r39j1sgj7qzdf184by_ctp2m0000gn/T/karma-typescript-bundle-720674ME1lwIQubwH.js:113:62)Chrome 77.0.3865 (Mac OS X 10.14.3): Executed 5 of 179 (2 FAILED) ERROR (0.087 secs / 0.013 secs)```";"['`npm run test` runs all testcases including tests for uncompressed models, which will fail if you didn\'t download the uncompressed model weights manually.Try the following scripts, which are also used by the CI `(npm run test-exclude-uncompressed):````""test-exclude-uncompressed"": ""set EXCLUDE_UNCOMPRESSED=true&& karma start"",""test-browser-exclude-uncompressed"": ""set EXCLUDE_UNCOMPRESSED=true&& karma start --single-run"",""test-node-exclude-uncompressed"": ""set EXCLUDE_UNCOMPRESSED=true&& npm run test-node"",```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/425;node js examples not working;1;closed;2019-09-28T16:26:21Z;2019-10-26T07:58:10Z;"when I run the node js example I am getting errors when compiling and running.using cmd: """"npx ts-node faceDetection.ts"" cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 4Cannot find module '[*localpath]\face-api.js\examples\examples-nodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v3\tfjs_binding.node'*****************************************************************************************using cmd: ""npx tsc faceDetection.ts""node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, readonly ACTIVE_UNIFORMS: number, readonly ALIASED_LINE_WIDTH_RANGE: number, ... 554 more ..., readonly WAIT_FAILED: number, }', but here has type '{ new (): WebGL2RenderingContext, prototype: WebGL2RenderingContext, readonly ACTIVE_ATTRIBUTES: number, readonly ACTIVE_TEXTURE: number, readonly ACTIVE_UNIFORMS: number, readonly ALIASED_LINE_WIDTH_RANGE: number, ... 555 more ..., readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number, }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  node_modules/typescript/lib/lib.dom.d.ts:16450:13    16450 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.**************************************************************************************I actually want to use faceapi in an angular app but I am getting some errors regarding 'faceapi.env.monkeyPatch({ Canvas, Image, ImageData })' so I tried to take  reference from this example but got this error. please help.**************************************************************************************";['Why are you running nodejs examples with npx? You should be using the browser environment of face-api.js.=====']
https://github.com/justadudewhohacks/face-api.js/issues/424;How to detect all faces from local file node js;5;closed;2019-09-24T22:55:34Z;2019-10-27T17:20:28Z;Hey, I'm using node js. I want to detect all the faces available from file buffer or a link to it. (like `./uploads/images/${filename}`)this is possible at this library ?;"[""I'm actually looking to do the same thing , so we do not have to process the file multiple times. we want to store the face data in the database. I was able to convert the data in the array coming from detectAllFaces to a JSON data (this is probably what you are looking for Noam).. this worked fine . but my struggle now is to revert it back and use it .. if there is any tip about doing that, would be great. Thanks====="", 'Can you show me an example for `detectAllFaces()` that runs on an image file ?=====', 'check this out : https://github.com/justadudewhohacks/face-api.js/issues/231=====', 'You have to create the image data from your link / data buffer. face-api.js accepts input images as Tensors, HTML image, canvas or video elements.For nodejs you can use the node-canvas package to read images from disk, furthermore tfjs-node has utility to create Tensors from png/jpeg image buffers read via fs.readFile.=====', 'Hi, in NodeJS with new version of tensorflow if more easy and decode buffer directly with new API [tf.node.decodeImage](https://js.tensorflow.org/api_node/1.2.7/#node.decodeImage):I hope help to any```function getPhotoTensor(pathPhoto) {  return fs.readFile(pathPhoto).then(buffer => {    return tf.node.decodeImage(new Uint8Array(buffer)),  }),}// then use function const input = await getPhotoTensor(""image/person.jpg""),const results = await faceapi      .detectAllFaces(input)      .withFaceLandmarks()      .withFaceDescriptors(),```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/423;Face Recognition on Webcam Implementation: Cannot read property 'findBestMatch' of null;1;closed;2019-09-23T16:37:47Z;2019-10-26T07:58:39Z;"Urgent: Facing this issue in the implementation of face recognition on the webcam camera for the browser. Getting ""Uncaught (in promise) TypeError: Cannot read property 'findBestMatch' of null"" Error even though the input of findBestMatch is a valid float32 array (see image below for details)![image](https://user-images.githubusercontent.com/40192780/65444611-f92abe00-de62-11e9-9bb8-2fb3a992a2ac.png)Anyone has a codebase of successful implementation of webcam with face-recognition? Or webcam with facial recognition and emotion detection combined would be an immense help. Thanks in advance!Attached the current codebase to share:  ```// DETECT FACES  const video = document.getElementById('video')  let faceMatcher = null,  Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),    faceapi.nets.faceLandmark68Net.loadFromUri('/models'),    faceapi.nets.faceRecognitionNet.loadFromUri('/models'),    faceapi.nets.faceExpressionNet.loadFromUri('/models'),    faceapi.nets.ssdMobilenetv1.loadFromUri('/models')  ]).then(start)  function startVideo() {    navigator.getUserMedia(      { video: {} },      stream => video.srcObject = stream,      err => console.error(err)    )  }  async function start() {    startVideo()    console.log(""Started"")    const labeledFaceDescriptors = await loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)    console.log(labeledFaceDescriptors)    console.log(""Mid"")    console.log(""END"")  }  video.addEventListener('play', () => {    const canvas = faceapi.createCanvasFromMedia(video)    document.body.append(canvas)    const displaySize = { width: video.width, height: video.height }    faceapi.matchDimensions(canvas, displaySize)    setInterval(async () => {      // FACE REC      faceapi.matchDimensions(canvas, displaySize)      const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors()      const resizedDetections = faceapi.resizeResults(detections, displaySize)      const results = resizedDetections.map(d => {        console.log(d.descriptor)        faceMatcher.findBestMatch(d.descriptor)      })      results.forEach((result, i) => {        const box = resizedDetections[i].detection.box        const drawBox = new faceapi.draw.DrawBox(box, { label: result.toString() })        drawBox.draw(canvas)      })    }, 150)  })  function loadLabeledImages() {    const labels = ['Jeremy', 'Black Widow', 'Captain America', 'Captain Marvel', 'Hawkeye', 'Jim Rhodes', 'Thor', 'Tony Stark']    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 1, i <= 2, i++) {          const img = await faceapi.fetchImage(`./labeled_images/${label}/${i}.jpg`)          const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()          descriptions.push(detections.descriptor)        }        return new faceapi.LabeledFaceDescriptors(label, descriptions)      })    )  }```";"[""In the start method of your code you are creating a local variable of faceMatcher:> const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)Thus your global faceMatcher instance is null. That's why you are receiveing this error.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/422;How to train a model that can recognize my face?;1;closed;2019-09-23T08:43:45Z;2019-10-12T10:57:33Z;Hi, I am trying to utlize face recognition model in a project and I want to recognize my own face. However I am still very new to tensorflow and this great project, I need some help to guide me. I really appreciate any help! Thanks.;['You do not train a new model, with the existing model you are able to recognize any peoples face. If you are unsure how to do so, take a look at the tutorials and examples first. =====']
https://github.com/justadudewhohacks/face-api.js/issues/421;Train model for face recognition;1;closed;2019-09-21T13:54:19Z;2019-09-21T15:07:21Z;Hi @justadudewhohacks, thanks for your awesome works. I'd like to get help in how to train my own model for my images so that I can recognize the faces within new dataset. It's great if you can show me some examples on this.Thanks,Phi;"[""I've had the answer over here. Again excellent works, thank you so much!https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/420;load model before inference;5;closed;2019-09-20T13:43:30Z;2020-08-24T16:41:37Z;"<!doctype html><html>	<head>		<meta charset=""utf-8"">		<title>title</title>		<!-- build:css css/main.min.css -->		<link rel=""stylesheet"" href=""css/main.css"">		<!-- endbuild -->	</head>	<body>		https://sun9-58.userapi.com/c638524/v638524146/57f04/76YNYJz82jY.jpg				<img id=""myImg"" src="""">				<button>get img</button>		<script src=""js/jquery.js""></script>		<script src=""js/face-api.js""></script>		<!-- build:js js/js.min.js -->		<script src=""js/main.js""></script>		<!-- endbuild -->	</body></html>main.js$(document).ready(function() {	$('button').on('click', getImg),		async function getImg(){  		  		const image = await faceapi.fetchImage('https://sun9-58.userapi.com/c638524/v638524146/57f04/76YNYJz82jY.jpg')		console.log(image instanceof HTMLImageElement) // true		// displaying the fetched image content		const myImg = document.getElementById('myImg')		myImg.src = image.src		const detections = await faceapi.detectAllFaces(image, await new faceapi.SsdMobilenetv1Options({ minConfidence: 0.6 })),		//console.log(detections)		console.log(detections)	}}),Error:face-api.js:4675 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (face-api.js:4675)    at SsdMobilenetv1.<anonymous> (face-api.js:4709)    at step (face-api.js:78)    at Object.next (face-api.js:59)    at fulfilled (face-api.js:49)";"[""You should call the method to load the models as stated [here](https://github.com/justadudewhohacks/face-api.js#loading-the-models) in the doc before using the 'detectAllFaces' method in your case====="", 'Hi can you give me a code snippet how to do in react js I have tried but giving me same error?=====', 'Hi. Even I get the same error on react js.  I have included it.Promise.all([            faceapi.nets.tinyFaceDetector.loadFromUri(MODELURL),            faceapi.nets.faceLandmark68Net.loadFromUri(MODELURL),            faceapi.nets.ssdMobilenetv1.loadFromUri(MODELURL)            ]).then(console.log(""Loaded models"")).catch(e=>console.log(e))It outputs ""Loaded models\', but it still shows the above error=====', 'Hello, for anyone looking for the solution to React js, you need to download the ""models"" folder from https://github.com/justadudewhohacks/face-api.js/tree/master/weights and add the folder to your ""public"" folder.After that, you can simply load it via Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri(""/models""),faceapi.nets.faceLandmark68Net.loadFromUri(""/models""),faceapi.nets.ssdMobilenetv1.loadFromUri(""/models"")])=====', 'Hi, I\'m getting the same error despite loading the models, I also have the models in my public folder and provided the paths correctly but still getting the same error.Script File:const MODEL_URL = ""/weights"",const net = new faceapi.SsdMobilenetv1()Promise.all([    net.loadFromUri(MODEL_URL),    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),]).then((val) => {    console.log(val)}).catch((err) => {    console.log(err)})Express Server :const express = require(""express"")const path = require(\'path\')const app = express()const hbs = require(""hbs"")const publicFolder = path.join(__dirname, \'../public\')const viewsPath = path.join(__dirname, ""../views"")app.use(express.static(publicFolder)),app.set(""view engine"", ""hbs""),app.set(""views"", viewsPath),app.get(\'/\', (req, res) => {    res.render(\'index\')})app.listen(7000, () => {    console.log(""Listening On port 7000"")})=====']"
https://github.com/justadudewhohacks/face-api.js/issues/419;Is there any solution to tell the different between real face and face from a picture;2;closed;2019-09-20T02:34:16Z;2019-10-12T11:03:25Z;What is the best solution on how to differentiate the face detection result of a person in front of a camera from a person just holding an image of him. Thank you in advance and thanks again for your wonderful API :);['there is model called liveness detection that can spot between fake and real person. it would be good if face-api.js can load the model too. i have the keras model and converted it into js model. i just dont know how to implement it into this api. u can read it here [https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/](https://www.pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/)=====', 'face-api.js does not provide a model for that task, I am not even sure, how a neural network would be able to solve that task.@ariffromeo if you have a Keras model, then you can convert it to a web model and run it using the tfjs-converter.=====']
https://github.com/justadudewhohacks/face-api.js/issues/416;Laggy on mobile browsers;4;closed;2019-09-18T02:12:41Z;2019-09-19T06:03:12Z;the camera example on android browser is slow and laggy. any plan to make the model smaller and faster for mobile browsers?;"['Which model are you talking about?=====', ""> Which model are you talking about?I tested all three models in the 'webcam face tracking' example，they all appear slow on mobile browers.====="", ""I get ~15 fps on my android phone using the TinyFaceDetector with mobile chrome. It's probably not possible to get much faster than this by creating an even smaller model.If you need even faster models for mobile devices, maybe you can manage it to train your own TFLite model, but you will have to embedd it into a native android / ios app instead of a web app then.I am not sure, how much room for optimization in the WebGL backend of tfjs there is remaining to make things run even faster, but I also noticed the tfjs team working on a webgpu backend. Maybe once they release that backend we will see even better performance in browsers.====="", 'I see. Thanks for your explanation.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/414;Use a another face detection model instead of the existed models in the repo;1;closed;2019-09-16T04:39:28Z;2019-10-12T11:13:47Z;Hi @justadudewhohacks,I really love your work and find that it is useful for my work.Today, I want to try some another models (face detection) that not exists in your repo and model that I want run is resnet-ssd such [here](https://github.com/LZQthePlane/Face-detection-base-on-ResnetSSD/blob/master/Res10_300x300_SSD_iter_140000.caffemodel), it use caffe model and I try it under python enviroment with opencv dnn module successfully.Can you support me for this work ?Thanks in advance.;['The easiest way that I could think of right now is to convert the caffe model to tensorflow and then use tfjs-converter, to convert it to a web model.=====']
https://github.com/justadudewhohacks/face-api.js/issues/412;loading models in react;3;closed;2019-09-13T06:04:03Z;2019-10-12T11:14:12Z;"import React, { Component } from 'react',import * as faceapi from 'face-api.js'class camera extends Component{    constructor(props) {        super(props),        this.videoTag = React.createRef()        this.state={            video:null        }        this.detect=this.detect.bind(this),    }    componentDidMount() {        // getting access to webcam            navigator.mediaDevices            .getUserMedia({video: true})            .then(stream =>             this.videoTag.current.srcObject = stream,             this.detect()               )              .catch(console.log),             }                     detect=async ()=>{          const videoTag=document.getElementById('videoTag'),          console.log(""geeting"")          const canvas=document.getElementById('myCanvas'),          await faceapi.nets.ssdMobilenetv1.loadFromUri('/models')           faceapi.nets.ssdMobilenetv1.loadFromUri('./models')          const displaySize = { width: videoTag.width, height: videoTag.height }          faceapi.matchDimensions(canvas, displaySize)          setInterval(async () => {          const detections = await faceapi.detectAllFaces(videoTag)          console.log(detections.length),          const resizedDetections = faceapi.resizeResults(detections, displaySize)          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)          faceapi.draw.drawDetections(canvas, resizedDetections)        }, 200)      }                render() {        return(             <div>                <video id=""videoTag""                        ref={this.videoTag}                        width={500}                        height={500}                        autoPlay                        ></video>                      <canvas id=""myCanvas""                                height={500}                                width={500}></canvas>                      </div>                              ),    }}export default camera,while loading ssd mobilenet model in react this error is geetingUncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0";"[""please check wheather i've had mistaken or missing something====="", 'Check you have copied the models correctly. It happened to me. Maybe you copied the json incorrectly.=====', 'Also check the network tab an make sure the uris your models are fetched from are correct.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/410;Unable to use camera on smartphone using local example;4;closed;2019-09-13T03:09:42Z;2019-10-12T11:14:43Z;My phone's camera works on the live demo. However, it won't work using the code from examples-browser. Any ideas? I have tried different browsers and it only works on desktop and with the live demo.;"['Please be more specific about what is not working / where it fails (which API call). Might be that you are not laoding the models correctly.=====', ""So if I run the webcam face recognition through https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking using either a computer or a smartphone. It will ask for permission and open the camera perfectly fine.However, when running locally => webcamFaceDetection.html (original code after cloning the repository) on a computer, the camera will work. But it won't open or ask for permission on Android even using different browsers. The view will load, but it won't show any errors. The html container for the camera will just be empty. Let me know if you have any questions. Thanks. ====="", ""Does it run the main loop or does it exit at some point? Please provide more detail, e.g. what exactly is not working, or at what point in the code it's stalling.====="", 'Closing because of inactivity.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/408;expected media to be of type HTMLImageElement;4;closed;2019-09-11T00:18:38Z;2019-09-15T19:24:31Z;"```const faceapi = require('face-api.js'),require('@tensorflow/tfjs-node'),const fetch = require('node-fetch'),const path = require('path'),faceapi.env.monkeyPatch({ fetch: fetch }),const MODELS_URL = path.join(__dirname, '/models/'),const jsdom = require(""jsdom""),const { JSDOM } = jsdom,global.window = new JSDOM(`<!DOCTYPE html><img id=""myImg"" src=""image.png"" />`, { resources: ""usable"", url: ""file:///"" + __dirname + ""/"" }).window,global.document = window.document,global.HTMLElement = window.HTMLElement,`async function detectFaceCommand(message) {    await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODELS_URL),    await faceapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL),    await faceapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL),    image = document.getElementById('myImg'),    let fullFaceDescriptions = await faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor(),}```This just gives me `Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id`Outputting `image` to the console shows that it is an HTMLImageElement yet it still says its not.";"[""Might be that the image element returned by JSDOM is not a valid HTMLImageElement, e.g. `img instanceof HTMLImageElement` returns false. If that's the case you could try to monkey patch HTMLImageElement somehow.====="", ""I messed around with it for a bit and `image instanceof HTMLImageElement` returns true. In the code snippet I provided I realized `global.HTMLElement = window.HTMLElement` was supposed to be `global.HTMLImageElement = window.HTMLImageElement` but I still get `expected media to be of the type HTMLImageElement`I got around this issue by adding `Image: window.HTMLImageElement` to monkeypatch but now I get `TypeError: canvas.getContext is not a function` in getContext2dOrThrow.js so I'm assuming I need to do similar to `env.Canvas` with monkeyPatch but I cant figure out what I need to set it to.If I set it to window.HTMLCanvasElement it dosen't work because in /build/commonjs/index.js line 38 tries `new Canvas()` but Canvas is an HTMLCanvasElement so what is env.Canvas supposed to be?====="", ""> I got around this issue by adding Image: window.HTMLImageElement to monkeypatch but now I get TypeError: canvas.getContext is not a function in getContext2dOrThrow.js so I'm assuming I need to do similar to env.Canvas with monkeyPatch but I cant figure out what I need to set it to.Exactly, check out the nodejs examples that utilize the canvas node package to monkey patch Image, Canvas and ImageData.I am actually not sure in what kind of environment you are running your code, but you also wanna try monkey patching the constructors of Image and Canvas:``` javascriptfaceapi.env.monkeyPatch({   createCanvasElement: () => document.createElement('canvas'),   createImageElement: () => document.createElement('img') })```     ====="", ""Thanks a ton this worked:```faceapi.env.monkeyPatch({    fetch: fetch,    Canvas: window.HTMLCanvasElement,    Image: window.HTMLImageElement,    ImageData: canvas.ImageData,    createCanvasElement: () => document.createElement('canvas'),    createImageElement: () => document.createElement('img')}),```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/407;force usage of F16 textures;3;closed;2019-09-10T10:40:05Z;2019-10-12T11:15:11Z;"tfjs-core: Make it possible to force usage of F16 textures. #1902https://github.com/tensorflow/tfjs/pull/1902appears in tf-core 1.2.9 release (only f16 textures yet, not math!).1. can you (we :) add and test f16 textures?2. they added in ""To Do"": ""Add flag (defaults to false) that would turn on mediump precision."" Can we confirm, that mediump precision is useful in real aplications? (than they will add the flag more quickly :)E.g. image classification/detection + colab + NVIDIA GPU?";"['So you are asking me to upgrade the tfjs-core version to latest?=====', ""not sure about detailes (I'm from webgl-dev-list https://www.ibiblio.org/e-notes/webgl/gpu/mul/sgemm.htm )1. if you just add  tf.webgl.forceHalfFloat()you will half GPU memory footprint, get ~10% acceleration and save battery.2. with mediump precision you can get x2 acceleration but for some reason e.g. SSD model works wrong with fp16 math (for TFjs team). Can we optimize any useful model for fp16 math (NVIDIA can :)? More detailes?====="", ""These flags you are talking about can be enabled via tf. face-api.js doesn't deal with such kind of things, it just utilizes tfjs as a backend. So you can simply enable those flags via tf and the models of face-api.js will make use of them. > with mediump precision you can get x2 acceleration but for some reason e.g. SSD model works wrong with fp16 math (for TFjs team). Can we optimize any useful model for fp16 math (NVIDIA can :)? More detailes?I am not familar with such kind of optimizations. Again such optimizations are probably something that have to be done in the tfjs backend and not face-api.js, but correct me if I am wrong.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/406;Box Bounding Error;6;closed;2019-09-09T04:25:56Z;2021-02-19T03:31:12Z;![image](https://user-images.githubusercontent.com/37760966/64503235-a696c100-d2fc-11e9-9fac-3ae8582b63a3.png)on windows7 i got the error abovehow can i fix it?;"[""Hi, may I give you a few suggestions?1. Search issues before post. There is a good chance that your question has already been asked.2. Don't post screenshots only. Images are not search friendly. Make sure to post the key information in text.There is no perfect answer to your question as far as I know.One possible cause is that your graphic driver is too old.Maybe you can read this thread to get more information: #282====="", 'Your backend returns invalid values for some math operations. Which browser / os / hardware (CPU / GPU) are you using?=====', '> get感谢你的建议！我以后会注意的=====', ""> 感谢你的建议！我以后会注意的??? English please, I can't read that.====="", 'Closing because of inactivity.=====', 'I was having this error when running the html served in a node environment. Running it from traditional apache works fine.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/404;Ionic Capacitor - loadFromUri;5;closed;2019-09-06T13:28:37Z;2020-11-16T20:37:20Z;After i changed from a ionic cordova to capacitor build i experience issues loading the models.On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message```Error: Uncaught (in promise): Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607    Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607        at d (http://localhost/vendor.js:100321:4453)        at dn (http://localhost/vendor.js:100321:71403)        at fn (http://localhost/vendor.js:100321:70965)        at o (http://localhost/vendor.js:100321:444184)        at _c (http://localhost/vendor.js:100321:444219)        at http://localhost/vendor.js:100321:463380        at Array.forEach (<anonymous>)        at http://localhost/vendor.js:100321:463354        at Array.forEach (<anonymous>)        at http://localhost/vendor.js:100321:463142```In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.I load ssd with`await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models'),`;"[""> After i changed from a ionic cordova to capacitor build i experience issues loading the models.> On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > ```> Error: Uncaught (in promise): Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607>     Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607>         at d (http://localhost/vendor.js:100321:4453)>         at dn (http://localhost/vendor.js:100321:71403)>         at fn (http://localhost/vendor.js:100321:70965)>         at o (http://localhost/vendor.js:100321:444184)>         at _c (http://localhost/vendor.js:100321:444219)>         at http://localhost/vendor.js:100321:463380>         at Array.forEach (<anonymous>)>         at http://localhost/vendor.js:100321:463354>         at Array.forEach (<anonymous>)>         at http://localhost/vendor.js:100321:463142> ```> > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > I load ssd with> `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models'),`I use ionic with capacitor and load the models in this way:`await faceapi.loadSsdMobilenetv1Model('/assets/models'),``await faceapi.loadFaceLandmarkModel('/assets/models'),`The extension of the models changed them to bin![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)Similarly in the .json files![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)Running on android and ios  🎉🎉====="", ""> > > > After i changed from a ionic cordova to capacitor build i experience issues loading the models.> > On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > ```> > Error: Uncaught (in promise): Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607> >     Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607> >         at d (http://localhost/vendor.js:100321:4453)> >         at dn (http://localhost/vendor.js:100321:71403)> >         at fn (http://localhost/vendor.js:100321:70965)> >         at o (http://localhost/vendor.js:100321:444184)> >         at _c (http://localhost/vendor.js:100321:444219)> >         at http://localhost/vendor.js:100321:463380> >         at Array.forEach (<anonymous>)> >         at http://localhost/vendor.js:100321:463354> >         at Array.forEach (<anonymous>)> >         at http://localhost/vendor.js:100321:463142> > ```> > > > > > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > I load ssd with> > `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models'),`> > I use ionic with capacitor and load the models in this way:> > `await faceapi.loadSsdMobilenetv1Model('/assets/models'),`> `await faceapi.loadFaceLandmarkModel('/assets/models'),`> > The extension of the models changed them to bin> > ![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)> > Similarly in the .json files> > ![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)> > Running on android and ios 🎉🎉Thy this solved my issue :)====="", ""> > > After i changed from a ionic cordova to capacitor build i experience issues loading the models.> > > On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > > ```> > > Error: Uncaught (in promise): Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607> > >     Error: Based on the provided shape, [1,1,512,512], the tensor should have 262144 values but has 216607> > >         at d (http://localhost/vendor.js:100321:4453)> > >         at dn (http://localhost/vendor.js:100321:71403)> > >         at fn (http://localhost/vendor.js:100321:70965)> > >         at o (http://localhost/vendor.js:100321:444184)> > >         at _c (http://localhost/vendor.js:100321:444219)> > >         at http://localhost/vendor.js:100321:463380> > >         at Array.forEach (<anonymous>)> > >         at http://localhost/vendor.js:100321:463354> > >         at Array.forEach (<anonymous>)> > >         at http://localhost/vendor.js:100321:463142> > > ```> > > > > > > > > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > > I load ssd with> > > `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models'),`> > > > > > I use ionic with capacitor and load the models in this way:> > `await faceapi.loadSsdMobilenetv1Model('/assets/models'),`> > `await faceapi.loadFaceLandmarkModel('/assets/models'),`> > The extension of the models changed them to bin> > ![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)> > Similarly in the .json files> > ![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)> > Running on android and ios 🎉🎉> > Thy this solved my issue :)Thank you. Solved my issue as well====="", 'Hello @MMadume, @esmircm and @SimonScholl I am also working in Ionic capacitor and facing some issues with loading a model. Can you please help me with that? Can you share how did you load your model ? Thank you=====', 'Hi I rename models to .bin and change this in json but say error in android2020-11-16 15:35:27.015 24121-24121/systemsweb.net E/Capacitor/Console: File: http://localhost/vendor-es2015.js - Line 41309 - Msg: ERROR Error: Uncaught (in promise): Error: SsdMobilenetv1 - load model before inference    Error: SsdMobilenetv1 - load model before inference        at SsdMobilenetv1.push../node_modules/face-api.js/build/es6/ssdMobilenetv1/SsdMobilenetv1.js.SsdMobilenetv1.forwardInput (http://localhost/tab3-tab3-module-es2015.js:13925:19)        at SsdMobilenetv1.<anonymous> (http://localhost/tab3-tab3-module-es2015.js:13959:35)        at step (http://localhost/vendor-es2015.js:123912:23)        at Object.next (http://localhost/vendor-es2015.js:123893:53)        at fulfilled (http://localhost/vendor-es2015.js:123883:58)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3470:30)        at Object.onInvoke (http://localhost/vendor-es2015.js:73011:33)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3469:36)        at Zone.run (http://localhost/polyfills-es2015.js:3229:47)        at http://localhost/polyfills-es2015.js:3963:40=====']"
https://github.com/justadudewhohacks/face-api.js/issues/403;face landmark detection model Architecture?;5;closed;2019-09-06T09:19:19Z;2019-10-12T16:06:34Z;What is the architecture of the face_landmark_68_model. I wanted to train my own landmark detection model with lesser points. please let me know the model architecture.;"['ssd model or tinyYolo or mtcnn we can use any of those threeOn Fri, Sep 6, 2019 at 2:49 PM RAKESH_v <notifications@github.com> wrote:> What is the architecture of the face_landmark_68_model. I wanted to train> my own landmark detection model with lesser points. please let me know the> model architecture.>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/403?email_source=notifications&email_token=AJOGHY3P43J52VXYTSDIGZ3QIIOCXA5CNFSM4IUG3F62YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HJYF42Q>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AJOGHY7V5CYQJQG34FRC7NDQIIOCXANCNFSM4IUG3F6Q>> .>=====', ""@thirukumars correct me if I'm wrong, I think all those model are only for detecting faces, If the faces are detected then, they are using face_landmark_68_model to detect landmark.====="", ""> @thirukumars correct me if I'm wrong, I think all those model are only for detecting faces, If the faces are detected then, they are using face_landmark_68_model to detect landmark.Correct.From the README:> 68 Point Face Landmark Detection ModelsThis package implements a very lightweight and fast, yet accurate 68 point face landmark detector. The default model has a size of only 350kb (face_landmark_68_model) and the tiny model is only 80kb (face_landmark_68_tiny_model). Both models employ the ideas of depthwise separable convolutions as well as densely connected blocks. The models have been trained on a dataset of ~35k face images labeled with 68 face landmark points.The implementation is a bit hidden, but you can find the code of the forward method [here](https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceFeatureExtractor/FaceFeatureExtractor.ts#L15-L36).====="", 'helloplease i have question ,i need to know how the function will take the landmark of face?and what is the landmark will take it?thank u very much=====', 'hellowhy there is no answer?thank u =====']"
https://github.com/justadudewhohacks/face-api.js/issues/399;is we can object detection in face-API;2;closed;2019-09-05T07:36:32Z;2019-09-13T07:59:42Z;we want to do the object detection using yolo in this by using this api;"[""face-API using tiny yolo model v2[https://pjreddie.com/darknet/yolov2/](url)Recommend[https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07)Loaded models ```await Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri('/weights'),      faceapi.nets.faceExpressionNet.loadFromUri('/weights')    ])```set confing tiny yolo```tyniOptions = new faceapi.TinyFaceDetectorOptions({      inputSize: 128,      scoreThreshold: 0.5    })```detect function``` const detections = await faceapi        .detectSingleFace(input, tyniOptions)        .withFaceExpressions()```====="", 'This API is not for arbitrary object detection. You can detect faces with it.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/394;Loading models in Django framework;7;closed;2019-08-26T23:38:02Z;2021-08-31T08:50:28Z;Hi Vincent,I am really impressed on your incredible work!First of all, I am currently working on real-time face detection implementation on my web application based on Django framework. I followed the steps from youtube for implementing this feature that you have provided in the description section. This implementation is perfectly working when I have implemented on VSCode with live server extension. However, somehow this implementation doesnt work on my web application with django server. There is an issue with loading models I guess.. If you are familiar with django framework, is there any configuration do i need to add? Thanks in advance! video.js:`Promise.all([  faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),  faceapi.nets.tinyFaceDetector.loadFromUri('/models'),  faceapi.nets.faceLandmark68Net.loadFromUri('/models'),  faceapi.nets.faceRecognitionNet.loadFromUri('/models'),  faceapi.nets.faceExpressionNet.loadFromUri('/models')]).then(startVideo)`console log:`Uncaught (in promise) Error: failed to fetch: (404) Not Found, from url ``face-api.min.js:1 GET http://127.0.0.1:8000/models/ssd_mobilenetv1_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_landmark_68_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_recognition_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)`If I run my web application on heroku server, Im getting this error`Failed to load resource: the server responded with a status of 404 (Not Found) /models/tiny_face_detector_model-weights_manifest.json:1 `;"['> If you are familiar with django framework, is there any configuration do i need to add?I am not familar with django sorry.> face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)Why are you loading your models from `http://127.0.0.1:8000/` when you should actually load them from the heroku server?=====', 'Closing because of inactivity.=====', ""@dytebk did you found any solution? I have currently the same problem. I've created a question there: https://stackoverflow.com/questions/58965228/app-does-not-include-some-of-the-files-on-heroku====="", '> Hi Vincent,> > I am really impressed on your incredible work!> First of all, I am currently working on real-time face detection implementation on my web application based on Django framework. I followed the steps from youtube for implementing this feature that you have provided in the description section. This implementation is perfectly working when I have implemented on VSCode with live server extension. However, somehow this implementation doesnt work on my web application with django server. There is an issue with loading models I guess.. If you are familiar with django framework, is there any configuration do i need to add?> Thanks in advance!> > video.js:> > `Promise.all([ faceapi.nets.ssdMobilenetv1.loadFromUri(\'/models\'), faceapi.nets.tinyFaceDetector.loadFromUri(\'/models\'), faceapi.nets.faceLandmark68Net.loadFromUri(\'/models\'), faceapi.nets.faceRecognitionNet.loadFromUri(\'/models\'), faceapi.nets.faceExpressionNet.loadFromUri(\'/models\') ]).then(startVideo)`> > console log:> > `Uncaught (in promise) Error: failed to fetch: (404) Not Found, from url `> `face-api.min.js:1 GET http://127.0.0.1:8000/models/ssd_mobilenetv1_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_landmark_68_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_recognition_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)`> > If I run my web application on heroku server, Im getting this error> > `Failed to load resource: the server responded with a status of 404 (Not Found) /models/tiny_face_detector_model-weights_manifest.json:1 `Hi Dytebk, in Django do you need to refenciate to static folder. I solved putting script.js in the same file index.html, and moving /modes inside /static folder:`{% load static%}<!DOCTYPE html><html lang=""en""><head>  <meta charset=""UTF-8"">  <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">  <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">  <title>Document</title>  <script src=""{% static ""js/recdata_new.js"" %}""></script>  <script src=""{% static ""js/face-api.min.js"" %}""></script><!--   <script src=""{% static ""js/script.js"" %}""> -->      <style>    body {      margin: 0,      padding: 0,      width: 100vw,      height: 100vh,      display: flex,      justify-content: center,      align-items: center,    }    canvas {      position: absolute,    }  </style></head><body>  <video id=""video"" width=""720"" height=""560"" autoplay muted></video><script type=""text/javascript"">const video = document.getElementById(\'video\')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri(\'{% static \'/models\' %}\'),  faceapi.nets.faceLandmark68Net.loadFromUri(\'{% static \'/models\' %}\'),  faceapi.nets.faceRecognitionNet.loadFromUri(\'{% static \'/models\' %}\'),  faceapi.nets.faceExpressionNet.loadFromUri(\'{% static \'/models\' %}\'),  faceapi.nets.ageGenderNet.loadFromUri(\'{% static \'/models\' %}\')]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}video.addEventListener(\'play\', () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width, height: video.height }  faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()    const resizedDetections = faceapi.resizeResults(detections, displaySize)    canvas.getContext(\'2d\').clearRect(0, 0, canvas.width, canvas.height)    faceapi.draw.drawDetections(canvas, resizedDetections)    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    faceapi.draw.drawFaceExpressions(canvas, resizedDetections)  }, 100)})  </script></body></html>`Regards!=====', 'Here is also working example: https://github.com/khashashin/gibbface/blob/master/frapp/templates/frapp/frapp.html=====', 'Failed to load resource: the server responded with a status of 404 (Not Found):8000/models/face_landmark_68_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found):8000/models/face_recognition_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found)face-api.min.js:1 Uncaught (in promise) Error: failed to fetch: (404) Not Found, from url: http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1n @ face-api.min.js:1:8000/models/face_expression_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found)=====', ""anyone help to solve this error to load models can i give path for inside the js file? means can we used load static in js ? if yes pleas help <-------------------------------- ths is my code ----------------------------------->const video = document.getElementById('video')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri(`{% static '/models' %}`),  faceapi.nets.faceLandmark68Net.loadFromUri(`{% static '/models' %}`),  faceapi.nets.faceRecognitionNet.loadFromUri(`{% static '/models' %}`),  faceapi.nets.faceExpressionNet.loadFromUri(`{% static '/models' %}`)]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}video.addEventListener('play', () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width, height: video.height }  faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()    const resizedDetections = faceapi.resizeResults(detections, displaySize)    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)    faceapi.draw.drawDetections(canvas, resizedDetections)    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    faceapi.draw.drawFaceExpressions(canvas, resizedDetections)  }, 100)})=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/393;Convert tfjs model to keras;1;closed;2019-08-26T02:12:10Z;2019-10-12T11:16:41Z;"hello，I'm glad to see your this excellent repo，but I met a demand and need to convert the tfjs model （age_gender_model-shard1，age_gender_model-weights_manifest.json）to the keras model. When using the [tfjs-converter](tfjs-converter) method, I found a problem, the field ""modelTopology"" isn't in the json file. How can I get the content of this part, I hope you can give some help.";"['I have no clue sorry. Also keep in mind that although the weights_manifest.json files provided in this repo are in the manifest format of tfjs, the models are not / might not be valid tfjs web models.If you can figure out what information is stored in ""modelTopology"" you could try to assemble them manually.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/391;illegal instruction (core dumped);3;closed;2019-08-20T10:30:09Z;2021-04-09T03:41:47Z;To speed up face scanning in face-api.js, I used @tensorflow/tfjs-node as written in the readme. Everything is working okay on the local machine and in the docker (on local machine).When i try to import @tensorflow/tfjs-node (via `require('@tensorflow/tfjs-node')`) on nodejs server, the application is crashing with error. The error message is the following: > illegal instruction (core dumped)Without it line everything works okay, but very slowly.How to resolve problem with “illegal instruction (core dumped)”?;['The problem was resolved after configuring the server.=====', '@vadymkop how did you fix the issue?=====', '> The problem was resolved after configuring the server.configuring what on the server??=====']
https://github.com/justadudewhohacks/face-api.js/issues/388;Module For Face/Age/gender Detection  Not Working in NODE.JS /Cordova Camera Upload Using Multer;3;closed;2019-08-19T10:58:29Z;2019-10-26T08:00:01Z;"Module For Face/Age/gender Detection  Not Loading In Browser from a URIIssue :I am not able to load the module correctly for Age/gender detection using Http request. I am using express in the backend and i cannot use 15mb of file all the time. However i only need 1 model for my requirements . Due to less documentation i am not able to identify which model to load on my front end for gender/age detection to minimise bandwidth usage and speed up app.Please suggest.Note : I am not using TypeScript . By Making your project in TypeScript is has made transition even harder as we do not require TS for anything. and the validation messsages might have worked when conversion oj Object at JS but due to TS it has made life harder here. We hoped to use Face APi but cannot continue lie this. Error Message with Multer```Image File :{ fieldname: 'imageverify',  originalname: '1567030996427.jpg',  encoding: '7bit',  mimetype: 'image/jpeg',  public_id: 'gender/imageverify-1567030997917',  version: 1567031005,  signature: '30f09b6a4ea72e2409dd8d0b15821f97644dfa4c',  width: 4032,  height: 3024,  format: 'jpg',  resource_type: 'image',  created_at: '2019-08-28T22:23:25Z',  tags: [],  bytes: 411900,  type: 'upload',  etag: 'fd1291756909ab991c5afa3988391422',  placeholder: false,  url: 'http://res.cloudinary.com/hookup/image/upload/v1567031005/gender/imageverify-1567030997917.jpg',  secure_url: 'https://res.cloudinary.com/hookup/image/upload/v1567031005/gender/imageverify-1567030997917.jpg',  original_filename: 'file' }{ lastModified: 1567031006333,  lastModifiedDate: 2019-08-28T22:23:26.333Z,  type: 'image/jpeg',  size: 411900,  name: '1567030996427.jpg' }Promise { <pending> }(node:34199) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id(node:34199) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```Made Some Other Changes But does not work```log = console.log.bind(console),async function execute(imgPath) {        // Trying To Convert To File Imput Image Upload Format        var obj = {},        obj.lastModified = Date.now(),        obj.lastModifiedDate = new Date(),        obj.type = imgPath.mimetype        obj.size = imgPath.bytes,        obj.name = imgPath.originalname,        log(obj),        var returnValue = await faceapi.detectSingleFace(obj, new faceapi.SsdMobilenetv1Options()).withAgeAndGender(),        log(returnValue),        return returnValue,    },        app.post('/get/picture/validator', mutlerUpload.single(""imageverify""), function(req, resp, next) {        log('/get/picture/validator'),        var image = req.files || req.file,        //var encoded = image.buffer.toString('base64'),        log(""Image File :""),        log(image),        var returnValue = execute(image.url),        log(returnValue),        resp.send(),    }),```If possible kindly look into it.";"[""> await faceapi.detectSingleFace(obj, new faceapi.SsdMobilenetv1Options()).withAgeAndGender(),The `obj` variable you are passing into the api is not a valid input, it should either be a HTMLImageElement or a Tensor. You are just passing some self assembled javascript object in there in your code example. That's why you receive the error.====="", 'If possible can you pls help me out with creating a Obj which can be captured via device camera so that i can upload it to backend via REST API. some documentation would be appreciated for Apache Cordova based system. This Project is such a GEM . We are having limited knowledge in ML and has tried all methods to make it work. Maybe some leads would be appreciated=====', 'If your goal is to fetch an image from an URI, take a look at [this](https://github.com/justadudewhohacks/face-api.js#fetch-and-display-images-from-an-url) codesnippet provided by the README.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/387;How can i fix this error;1;closed;2019-08-15T19:12:13Z;2019-09-13T08:53:39Z;face-api.min.js:1 Uncaught (in promise) Error: Failed to link vertex and fragment shaders.    at Wr (face-api.min.js:1)    at t.createProgram (face-api.min.js:1)    at face-api.min.js:1    at face-api.min.js:1    at t.getAndSaveBinary (face-api.min.js:1)    at t.compileAndRun (face-api.min.js:1)    at t.conv2dWithIm2Row (face-api.min.js:1)    at t.conv2d (face-api.min.js:1)    at Zt.engine.runKernel.x (face-api.min.js:1)    at face-api.min.js:1Wr @ face-api.min.js:1t.createProgram @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.getAndSaveBinary @ face-api.min.js:1t.compileAndRun @ face-api.min.js:1t.conv2dWithIm2Row @ face-api.min.js:1t.conv2d @ face-api.min.js:1Zt.engine.runKernel.x @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.runKernel @ face-api.min.js:1conv2d_ @ face-api.min.js:1conv2d @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.tidy @ face-api.min.js:1t.tidy @ face-api.min.js:1Kh @ face-api.min.js:1r.runMobilenet @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.tidy @ face-api.min.js:1t.tidy @ face-api.min.js:1r.forwardInput @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1n @ face-api.min.js:1Promise.then (async)o @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1(anonymous) @ face-api.min.js:1e.runAndExtendWithFaceDetections @ face-api.min.js:1e.withFaceLandmarks @ face-api.min.js:1(anonymous) @ script.js:24face-api.min.js:1 C:\fakepath(179,2-41): warning X3550: array reference cannot be used as an l-value, not natively addressable, forcing loop to unrollC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\fakepath(179,2-41): warning X3550: array reference cannot be used as an l-value, not natively addressable, forcing loop to unrollC:\fakepath(179,2-41): error X3500: array reference cannot be used as an l-value, not natively addressableC:\fakepath(158,7-60): error X3511: forced to unroll loop, but unrolling failed.C:\fakepath(156,7-60): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D Shadershow can i fix this issue;['Duplicate of #327=====']
https://github.com/justadudewhohacks/face-api.js/issues/384;Frame drops when setting imageData tensor3D as input;1;closed;2019-08-14T07:42:20Z;2019-09-19T07:41:51Z;frame drops when I set a tensor3D object as input, is there any way to optimize? the code below is only for demostration, coz I can only access webcam fame data in a webview env.const data = ctx.getImageData(0, 0, vw, vh)const elem = faceapi.tf.browser.fromPixels(data)const options = new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold })const result = await faceapi.detectSingleFace(elem, options).withFaceLandmarks();"['What does ""frame drops"" mean? You also have to dispose the elem tensor after the API call (elem.dispose()), othewise you will end up with memory leaks.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/382;Some kind of isReady method;2;closed;2019-08-12T08:23:31Z;2020-06-18T07:40:46Z;Hi, is there some kind of `faceapi.isDetectorLoaded` or similar to avoid calling detection if it is not ready yet?That would be great.;['Yes `net.isLoaded` will tell you if the network parameters have been loaded.=====', 'You are right! Sorry for the inconvenience. Im closing this issue.=====']
https://github.com/justadudewhohacks/face-api.js/issues/381;Ml5.js and face-api.js are not available at the same time.;2;closed;2019-08-10T17:12:59Z;2019-10-12T11:20:41Z;"I wanted to try face-api.js and installed it. But when I import both ml5.js and face-api.js at the same time, I have a problem. So I cloned https://github.com/ml5js/ml5-library/ And I tried to build, install and use it via yarn build myself but I get the same problem.<img width=""800"" alt=""problem"" src=""https://user-images.githubusercontent.com/31428960/62824778-87155700-bbdd-11e9-99e7-69f0084d67b6.PNG"">";['This error comes from ml5.js so I am not sure why this error occurs. My guess is that the tfjs-core versions of your ml5.min.js script and the version used by face-api.js are conflicting?=====', 'Closing because of inactivity.=====']
https://github.com/justadudewhohacks/face-api.js/issues/379;Is there anything predictable about where face landmark points are on that face feature?;2;closed;2019-08-08T16:56:28Z;2019-08-26T14:41:41Z;Some examples to drive what I'm getting at:1) If I get all the points on the eyebrow, are there always 5 points?2) Similarly, can I assume the middle point (3) is in the middle of the eyebrow?3) For noses, can I assume the fourth point is always the tip of the nose?;['Hey @trevordunn , since I am doing something similar, here is a [photo](https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg) @justadudewhohacks posted on another issue.The points are always the same from what I have seen.For the nose you can `landmarks.getNose()` and get the array with the nose points.The tip will be the element with index 4.Similarly for the eyebrows.=====', 'Cool. Thanks!=====']
https://github.com/justadudewhohacks/face-api.js/issues/378;face Expression;4;closed;2019-08-08T12:24:25Z;2019-09-05T07:35:42Z;;"['i need the face Expression should come in console.please make an try(i need)=====', 'i mean if it happy means,it print happy in console like that=====', '@thirukumarstry to`  const detections = await faceapi        .detectSingleFace(this.video, this.cf.tyniOptions)        .withFaceExpressions()`` console.log(detections.expressions)`=====', ""i console.log in FaceExpressionDetection in  face-api.js file,it's workingand thank you for your response=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/376;tsc compilation error;5;closed;2019-08-08T03:29:56Z;2019-08-21T01:50:09Z;"Hi All,Just got an error on running node program:```yarn run v1.16.0$ D:\Github\Minazuki\node\node_modules\.bin\ts-node detection.ts ../images/pos1.jpgcpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 42019-08-08 11:12:39.517916: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:245    return new TSError(diagnosticText, diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:utils/faceDetection.ts:19:10 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'SsdMobilenetv1' have no overlap.19   return net === faceapi.nets.ssdMobilenetv1            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~utils/faceDetection.ts:21:7 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'TinyFaceDetector' have no overlap.21     : net === faceapi.nets.tinyFaceDetector         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~utils/faceDetection.ts:26:60 - error TS2345: Argument of type 'TinyFaceDetector' is not assignable to parameter of type 'NeuralNetwork<any>'.  Types of property 'extractParamsFromWeigthMap' are incompatible.    Type '(weightMap: NamedTensorMap) => { params: TinyYolov2NetParams, paramMappings: ParamMapping[], }' is not assignable to type '(weightMap: NamedTensorMap) => { params: any, paramMappings: ParamMapping[], }'.      Types of parameters 'weightMap' and 'weightMap' are incompatible.        Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.          Index signatures are incompatible.            Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.              Types of property 'flatten' are incompatible.                Type '() => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>' is not assignable to type '() => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                    Types of property 'asScalar' are incompatible.                      Type '() => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>' is not assignable to type '() => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                          Types of property 'as2D' are incompatible.                            Type '(rows: number, columns: number) => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' is not assignable to type '(rows: number, columns: number) => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                                Types of property 'asType' are incompatible.                                  Type '<T extends import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, dtype: ""string"" | ... 3 more ... | ""complex64"") => T' is not assignable to type '<T extends import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T, dtype: ""string"" | ... 3 more ... | ""complex64"") => T'.                                    The 'this' types of each signature are incompatible.                                      Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                        Property 'bytes' is missing in type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' but required in type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.26 export const faceDetectionOptions = getFaceDetectorOptions(faceDetectionNet)                                                              ~~~~~~~~~~~~~~~~  node_modules/@tensorflow/tfjs-core/dist/tensor.d.ts:374:5    374     bytes(): Promise<Uint8Array[] | Uint8Array>,            ~~~~~    'bytes' is declared here.    at createTSError (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:245:12)    at reportTSError (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:249:19)    at getOutput (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:362:34)    at Object.compile (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:395:32)    at Module.m._compile (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:473:43)    at Module._extensions..js (module.js:663:10)    at Object.require.extensions.(anonymous function) [as .ts] (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:476:12)    at Module.load (module.js:565:32)    at tryModuleLoad (module.js:505:12)    at Function.Module._load (module.js:497:3)error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```The root cause of this error maybe because I just upgrade typescript from 3.5.2 to 3.5.3 and @tensorflow/tfjs-node from 1.2.3 to 1.2.7Anyone here have this error and solutions?Thanks";"['Same error=====', 'I had the same error.It is mainly the @tensorflow/tfjs-node version 1.2.7 issues.I think face-api.js dependent on the @tensorflow/tfjs-node  version 1.2.2 which you can read the package.json in root of this repos.You can uninstall @tensorflow/tfjs-node and reinstall face-api.js then it will be fine.=====', 'I already fix the error:* Clear npm modules directory and cache* I user typescript and I add this line to tsconfig.json -> ""skipLibCheck"": true,* npm iAnd all is ok=====', 'Hi @xperiafan13-rom Can you share your package.json and tsconfig.json ? ThanksI just try and get the same error again.My those two files are:```  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.2.7"",    ""canvas"": ""^2.6.0"",    ""face-api.js"": ""^0.20.1""  },  ""devDependencies"": {    ""@types/node"": ""^12.7.2"",    ""prettier"": ""^1.18.2"",    ""ts-node"": ""^8.3.0"",    ""typescript"": ""^3.5.3""  }``````{  ""compilerOptions"": {    ""skipLibCheck"": true  },  ""exclude"": [    ""node_modules"",    ""build"",    ""doc"",    ""scripts"",    ""acceptance-tests"",    ""webpack"",    ""jest"",    ""src/setupTests.ts"",    ""coverage""  ]}```=====', '`{  ""name"": ""rem_services"",  ""version"": ""0.0.1"",  ""description"": ""REM REST Services"",  ""main"": ""dist/server.js"",  ""scripts"": {    ""build"": ""tsc"",    ""dev"": ""ts-node lib/server.ts"",    ""start"": ""nodemon ./dist/server.js"",    ""prod"": ""npm run build && npm run start""  },  ""author"": ""xperiafan13"",  ""license"": ""ISC"",  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.0.2"",    ""@types/express"": ""^4.17.0"",    ""@types/node"": ""^12.7.1"",    ""awesome-qr"": ""^1.2.0"",    ""axios"": ""^0.19.0"",    ""ba64"": ""^3.0.9"",    ""body-parser"": ""^1.19.0"",    ""canvas"": ""^2.5.0"",    ""cloudinary"": ""^1.14.0"",    ""dateformat"": ""latest"",    ""ethers"": ""^4.0.20"",    ""express"": ""^4.17.1"",    ""face-api.js"": ""^0.19.0"",    ""file-encryptor"": ""^0.1.1"",    ""firebase-admin"": ""latest"",    ""http"": ""0.0.0"",    ""image-hash"": ""^3.3.9"",    ""int-encoder"": ""^1.1.1"",    ""inversify"": ""^5.0.1"",    ""inversify-express-utils"": ""^6.3.2"",    ""jsonpack"": ""^1.1.5"",    ""jsonwebtoken"": ""^8.5.1"",    ""lodash"": ""latest"",    ""mongodb"": ""latest"",    ""mongoose"": ""^5.6.9"",    ""mustache-express"": ""^1.2.8"",    ""node-gd"": ""^1.6.0"",    ""node-schedule"": ""^1.3.2"",    ""nodemailer"": ""latest"",    ""oidc-provider"": ""^5.5.6"",    ""p-iteration"": ""^1.1.8"",    ""rand-token"": ""latest"",    ""reflect-metadata"": ""^0.1.13"",    ""socket.io"": ""^2.2.0"",    ""speakeasy"": ""^2.0.0"",    ""ts-node"": ""^8.3.0"",    ""typescript"": ""^3.5.3"",    ""web3"": ""^1.2.1"",    ""web3-eth-accounts"": ""^1.2.1""  }}``{  ""compilerOptions"": {    ""module"": ""commonjs"",    ""moduleResolution"": ""node"",    ""experimentalDecorators"": true,    ""skipLibCheck"": true,    ""pretty"": true,    ""sourceMap"": true,    ""resolveJsonModule"": true,    ""noImplicitAny"": false,    ""allowJs"": true,    ""declaration"": false,    ""target"": ""es6"",    ""outDir"": ""./dist"",    ""baseUrl"": ""./lib""  },  ""include"": [    ""lib/**/*.ts"",    ""lib/**/*.js""  ],  ""exclude"": [    ""node_modules""  ]}`=====']"
https://github.com/justadudewhohacks/face-api.js/issues/374;React live webcam example;7;closed;2019-08-05T21:04:32Z;2019-10-12T11:21:16Z;Hello there,I was looking at your live demos and did not find the codebase for those. I am Interested in the webcam live demo version but for react.Could you please either create an example page or point to the code base for that live demo?And not sure also where the models are currently stored in your repo.Thank you for the help, this is a very cool library!Keep it up please.;"[""Especially considering that:```[jules:...js/examples/examples-nodejs]$ ts-node faceDetection.ts                                                                                              (master✱) zsh: command not found: ts-node[jules:...js/examples/examples-nodejs]$ tsc faceDetection.ts                                                                                                  (master✱) zsh: command not found: tsc[jules:...js/examples/examples-nodejs]$ node faceDetection.js                                                                                                 (master✱) internal/modules/cjs/loader.js:628    throw err,    ^Error: Cannot find module '/Users/jules/Git/face-api.js/examples/examples-nodejs/faceDetection.js'    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:625:15)    at Function.Module._load (internal/modules/cjs/loader.js:527:27)    at Function.Module.runMain (internal/modules/cjs/loader.js:839:10)    at internal/main/run_main_module.js:17:11 {  code: 'MODULE_NOT_FOUND',  requireStack: []}```====="", ""Hi @julesmoretti [Here](https://panepo.github.io/Uzuki/)'s a face-api.js demo with react.The code for learning faces and recognizing via webcam can be found [here](https://github.com/Panepo/Uzuki/blob/master/src/pages/Train/Train.jsx) and [here](https://github.com/Panepo/Uzuki/blob/master/src/pages/Sensor/Sensor.jsx).====="", 'i has project Nextjs https://github.com/linhbkhn95/face-frontend-nextjs=====', '@Panepo - thank you for the info, the [link](https://panepo.github.io/Uzuki/) you provided points to a pink page. Can you point out the working demo of the two other links ([1](https://github.com/Panepo/Uzuki/blob/master/src/pages/Train/Train.jsx), [2](https://github.com/Panepo/Uzuki/blob/master/src/pages/Sensor/Sensor.jsx)) you added?@linhbkhn95 - Thank you for your repo, I tried running it and it does not work sadly.So far I have a codebase that seems to work but I do not think it is up to date as the `drawLandmarks` is not aligned with my face. Also I am not sure how to have the landing box over my face.... Anyways I would love to have the bounding boxes over the webcam live feed but my code is dated and not very clean... any pointers would be great.Here is the modified code from this repo: https://github.com/BonnieMilian/face-api-webcam-react```import React, { Component } from \'react\',import \'./App.css\',import * as faceapi from \'face-api.js\',import styled from \'styled-components\',// import WebCamPicture from \'./components/WebCamPicture.js\',import Webcam from \'react-webcam\',const videoConstraints = {  width: 350,  height: 350,  facingMode: \'user\',},const MODEL_URL = \'/models\'const minConfidence = 0.6// const testImage = \'/img/bonnie.jpg\'const MainContainer = styled.div`  background: red,  width: 100vw,  height: 100vh,  display: flex,  justify-content: flex-start,  align-items: center,  flex-direction: column,`,const TopRowDiv = styled.div`  height: ${props => props.height}%,  width: inherit,  background: yellow,  flex-direction: row,  justify-content: flex-start,  display: flex,`,const BottomRowDiv = styled.div`  background: green,  width: inherit,  flex: 1,`,const HorizontalRange = styled.input`  position: absolute,  width: 100%,  top: 0,  left: 0,`,const VerticalRange = styled.input`  height: 100%,  position: absolute,  transform: rotate(90deg),  width: 100vh,  right: -50vh,  // width: 10px,  top: 0,  // writing-mode: bt-lr, /* IE */  // -webkit-appearance: slider-vertical, /* WebKit */`,const VideoDiv = styled.div`  text-align: center,  width: ${props => props.width}%,  height: 100%,  display: flex,  justify-content: center,  align-items: center,  // flex-direction: row,  background: #0f2338, /* Old browsers */  background: -moz-linear-gradient(top, #0f2338 0%, #2d73aa 100%), /* FF3.6-15 */  background: -webkit-linear-gradient(top, #0f2338 0%,#2d73aa 100%), /* Chrome10-25,Safari5.1-6 */  background: linear-gradient(to bottom, #0f2338 0%,#2d73aa 100%), /* W3C, IE10+, FF16+, Chrome26+, Opera12+, Safari7+ */  overflow: hidden,`,const Canvas = styled.canvas `  position: absolute,  border: 1px solid red,`,export default class App extends Component {  constructor(props){    super(props),    this.fullFaceDescriptions = null,    this.canvas = React.createRef(),    this.canvasPicWebCam = React.createRef(),    this.webcam = React.createRef(),    this.state = {      width: 50,      height: 50,      imageSrc: null,    }  }  async componentDidMount() {    await this.loadModels(),    // const testImageHTML = document.getElementById(\'test\')    // this.drawHTMLImage(this.canvas.current,testImageHTML,296,296),    // await this.getFullFaceDescription(this.canvas.current),    // this.drawDescription(this.canvas.current),    setInterval(() => {      this.capture(),    }, 1000),  }  async loadModels () {    //await faceapi.loadModels(MODEL_URL)    await faceapi.loadFaceDetectionModel(MODEL_URL)    await faceapi.loadFaceLandmarkModel(MODEL_URL)    await faceapi.loadFaceRecognitionModel(MODEL_URL)  }  capture = () => {    const imageSrc = this.webcam.current.getScreenshot(),    //console.log(""Take Picture""),    // this.props.landmarkPicture(imageSrc),    // this.setState({ imageSrc }),    this.landmarkWebCamPicture(imageSrc),  },  landmarkWebCamPicture = (picture) => {    const ctx = this.canvasPicWebCam.current.getContext(""2d""),    var image = new Image(),    image.onload = async () => {      ctx.drawImage(image,0,0),      await this.getFullFaceDescription(this.canvasPicWebCam.current),      this.drawDescription(this.canvasPicWebCam.current),    },    image.src = picture,  }  getFullFaceDescription = async (canvas) => {    console.log(canvas),    this.fullFaceDescriptions = await faceapi    .allFaces(canvas, minConfidence),    console.log(this.fullFaceDescriptions),  }  drawDescription = (canvas) => {    this.fullFaceDescriptions.forEach((fd, i) => {      console.log(""drawDescription"", i, fd),      faceapi.drawLandmarks(canvas, fd.landmarks, { drawLines: false })    })  }  drawHTMLImage(canvas,image,width,height){    const ctx = canvas.getContext(""2d""),    ctx.drawImage(image,0,0,width,height),  }  render() {    return (      <div className=""App"" >        <MainContainer>          <VerticalRange            type=""range""            name=""points""            min=""0""            max=""100""            step=""0.1""            value={this.state.height}            onChange={(event) => this.setState({ height: event.target.value })}          />          <HorizontalRange            type=""range""            name=""points""            min=""0""            max=""100""            step=""0.1""            value={this.state.width}            onChange={(event) => this.setState({ width: event.target.value })}          />          <TopRowDiv            height={this.state.height}          >            <VideoDiv              width={this.state.width}            >              <Webcam                audio={false}                height={350}                ref={this.webcam}                screenshotFormat=""image/jpeg""                width={350}                videoConstraints={videoConstraints}              />              <Canvas ref={this.canvasPicWebCam} width={350} height={350} />            </VideoDiv>          </TopRowDiv>          <BottomRowDiv />        </MainContainer>      </div>    ),  }}```Thank you=====', ""Hi @julesmoretti A pink page? Read the readme and get [instructions](https://github.com/Panepo/Uzuki)If there's something different with the screenshot, please use a Tensorflow.js supported browsers like Chrome and Firefox, and javascript should be enabled.====="", '@Panepo - thank you, I see what you mean its a very sophisticated demo you made, I am just looking for a very stripped down version of it. But will look through your code thank you.=====', '> Could you please either create an example page or point to the code base for that live demo?I am not going to add react examples to this repo sorry, there is no react specific things about this API, so by understanding the existing examples, you should be able to integrate face-api.js with React.> And not sure also where the models are currently stored in your repo.They are located in the [weights](https://github.com/justadudewhohacks/face-api.js/tree/master/weights) folder.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/373;failed to compile fragment shader error;3;closed;2019-08-05T11:43:59Z;2020-02-06T13:48:41Z;While running my code ,it works properly 2 to 3 times,after that my laptop goes to sleep for few second,and get this error in my console.Couldn't parse line number in error: tf-core.esm.js:17 #version 300 es    precision highp float,    precision highp int,    precision highp sampler2D,    in vec2 resultUV,    out vec4 outputColor,    const vec2 halfCR = vec2(0.5, 0.5),    struct ivec5    {      int x,      int y,      int z,      int w,      int u,    },    struct ivec6    {      int x,      int y,      int z,      int w,      int u,      int v,    },    uniform float NAN,    #define isnan(value) isnan_custom(value)          bool isnan_custom(float val) {        return (val > 0. || val < 0. || val == 0.) ? false : true,      }        bvec4 isnan_custom(vec4 val) {      return bvec4(isnan(val.x), isnan(val.y), isnan(val.z), isnan(val.w)),    }          const float INFINITY = uintBitsToFloat(uint(0x7f800000)),              #define round(value) newRound(value)      int newRound(float value) {        return int(floor(value + 0.5)),      }      ivec4 newRound(vec4 value) {        return ivec4(floor(value + vec4(0.5))),      }        int imod(int x, int y) {      return x - y * (x / y),    }    int idiv(int a, int b, float sign) {      int res = a / b,      int mod = imod(a, b),      if (sign < 0. && mod != 0) {        res -= 1,      }      return res,    }    //Based on the work of Dave Hoskins    //https://www.shadertoy.com/view/4djSRW    #define HASHSCALE1 443.8975    float random(float seed){      vec2 p = resultUV * seed,      vec3 p3  = fract(vec3(p.xyx) * HASHSCALE1),      p3 += dot(p3, p3.yzx + 19.19),      return fract((p3.x + p3.y) * p3.z),    }    vec2 uvFromFlat(int texNumR, int texNumC, int index) {  int texR = index / texNumC,  int texC = index - texR * texNumC,  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),}vec2 packedUVfrom1D(int texNumR, int texNumC, int index) {  int texelIndex = index / 2,  int texR = texelIndex / texNumC,  int texC = texelIndex - texR * texNumC,  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),}    vec2 packedUVfrom2D(int texelsInLogicalRow, int texNumR,  int texNumC, int row, int col) {  int texelIndex = (row / 2) * texelsInLogicalRow + (col / 2),  int texR = texelIndex / texNumC,  int texC = texelIndex - texR * texNumC,  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),}    vec2 packedUVfrom3D(int texNumR, int texNumC,    int texelsInBatch, int texelsInLogicalRow, int b,    int row, int col) {  int index = b * texelsInBatch + (row / 2) * texelsInLogicalRow + (col / 2),  int texR = index / texNumC,  int texC = index - texR * texNumC,  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),}    float getChannel(vec4 frag, vec2 innerDims) {    vec2 modCoord = mod(innerDims, 2.),    return modCoord.x == 0. ?      (modCoord.y == 0. ? frag.r : frag.g) :      (modCoord.y == 0. ? frag.b : frag.a),  }  float getChannel(vec4 frag, int dim) {    float modCoord = mod(float(dim), 2.),    return modCoord == 0. ? frag.r : frag.g,  }    float sampleTexture(sampler2D textureSampler, vec2 uv) {      return texture(textureSampler, uv).r,    }      void setOutput(vec4 val) {      outputColor = val,    }  uniform sampler2D A,uniform int offsetA,uniform sampler2D B,uniform int offsetB,    ivec4 getOutputCoords() {      ivec2 resTexRC = ivec2(resultUV.yx *                             vec2(128, 4096)),      int index = resTexRC.x * 4096 + resTexRC.y,            int b2 = index / 524288,      index -= b2 * 524288,          int b = index / 2048,      index -= b * 2048,      int r = 2 * (index / 16),      int c = imod(index, 16) * 2,      return ivec4(b2, b, r, c),    }      vec4 getA(int b2, int b, int row, int col) {      int index = b2 * 524288 + b * 2048 + (row / 2) * 16 + (col / 2),      int texR = index / 724,      int texC = index - texR * 724,      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(724, 725),      return texture(A, uv),    }      vec4 getAAtOutCoords() {      ivec4 coords = getOutputCoords(),            vec4 outputValue = getA(coords.x, coords.y, coords.z, coords.w),      return outputValue,    }      vec4 getB(int index) {      vec2 uv = packedUVfrom1D(        1, 16, index),      return texture(B, uv),    }      vec4 getBAtOutCoords() {      ivec4 coords = getOutputCoords(),            vec4 outputValue = getB(coords.w),            return vec4(outputValue.xy, outputValue.xy),        }        vec4 binaryOperation(vec4 a, vec4 b) {        return a + b,      }      void main() {        vec4 a = getAAtOutCoords(),        vec4 b = getBAtOutCoords(),        vec4 result = binaryOperation(a, b),                setOutput(result),      }tf-core.esm.js:17 Uncaught (in promise) Error: Failed to compile fragment shader.    at Vt (tf-core.esm.js:17)    at t.createProgram (tf-core.esm.js:17)    at tf-core.esm.js:17    at tf-core.esm.js:17    at t.getAndSaveBinary (tf-core.esm.js:17)    at t.compileAndRun (tf-core.esm.js:17)    at t.packedBinaryOp (tf-core.esm.js:17)    at t.add (tf-core.esm.js:17)    at At.runKernel.$a (tf-core.esm.js:17)    at tf-core.esm.js:17;['I am assuming you run out of gpu memory, which models are you using, can you share some code?=====', 'Closing because of inactivity, if this is still an issue, feel free to reopen and provide the requested information.=====', 'hi , in samsung problem J5 and tab A e motorola G4 i found problem =====']
https://github.com/justadudewhohacks/face-api.js/issues/372;Best approach to process large sets of images (100-500);1;closed;2019-08-02T23:20:20Z;2019-09-18T14:13:08Z;Hi!First of all, thank you for this great library.I have to process an array of images, with 100-500 images: I need to find a particular face, given in input, among those 100-500 faces.I did some tests with **ssdMobilenetv1** (I might need its accuracy): it took 48 seconds to process 100 images and a total of 500 faces (running in the browser and using detectAllFaces).The problem is that, in the worst case, I might need to pass all the images.What's the best approach for dealing with so many images?Thank you in advance!;"[""I can't think of any other approach other than what you are already doing.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/370; Uncaught (in promise) TypeError: faceapi.drawDetection is not a function;2;closed;2019-08-02T07:48:51Z;2019-08-05T07:33:01Z;$(document).ready(function(){run()})async function run(){await faceapi.loadMtcnnModel('./models'),await faceapi.loadFaceRecognitionModel('./models'),const videoEl=document.getElementById('inputVideo')navigator.getUserMedia({video:{}},stream=>videoEl.srcObject=stream,err=>console.err(err))}async function onPlay(videoEl){setInterval( async ()=>{console.log('it is running'),const mtcnnForwardParams = {    maxNumScales: 10,    scaleFactor: 0.709,    scoreThresholds: [0.6, 0.7, 0.7],    minFaceSize: 200  }  const canvas=document.getElementById('overlay')const mtcnnResults=await faceapi.mtcnn(videoEl,mtcnnForwardParams)canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)faceapi.drawDetection(canvas, mtcnnResults.map(res => res.faceDetection), { withScore: false })  faceapi.drawLandmarks(canvas, mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: 'red' })},1000)}this is my javascript code,i got this error,please help to sort outscript2.js:27 Uncaught (in promise) TypeError: faceapi.drawDetection is not a functionat script2.js:27;['in faceapi.js also there is no drawDetection( 0,0 ,0 ) with three arguments=====', 'faceapi.drawDetections is not part of faceapi anymore, please use the up to date examples.=====']
https://github.com/justadudewhohacks/face-api.js/issues/369;Not able to load image from local node js server ;6;closed;2019-07-31T13:52:34Z;2019-09-18T14:15:08Z;"i tried the following two ways o loading image and output of the following codes are commemnted   1st try``` var imageAsBase64 = fs.readFileSync(path, 'base64'),          console.log('Exists ' + typeof (imageAsBase64)) // output  :Exists string                              const image = await faceapi.bufferToImage(imageAsBase64)              .then(res =>{console.log(res)})              .catch(e=> console.log(""Error e ""+e)) //output :Error e TypeError: Right-hand side of 'instanceof' is not an object```2nd try```   var imageAsBase64 = fs.readFileSync(path),            console.log('Exists ' + typeof (imageAsBase64)) // Exists object                            const image = await faceapi.fetchImage(imageAsBase64)                .then(res =>{console.log(res)})                .catch(e=> console.log(""Error e ""+e)) //Error e Type Error: Only absolute URLs are supported```should i missing something or i need to try different way please help..";"['You should be able to just pass the file path like this `faceapi.fetchImage(path)`. I am not 100% sure, but I am pretty certain.=====', '> You should be able to just pass the file path like this `faceapi.fetchImage(path)`. I am not 100% sure, but I am pretty certain.i tried but the error is same if you want to look at my code will uppload my repo to github ```     var location = req.files[0].destination        var imgFile = req.files[0].filename,        if(!fs.existsSync(fspath.join(location,imgFile))) {         console.log(""Not exists"")      }else{        console.log(\'Exists\')  //yes Exists      }           const image = await faceapi.fetchImage(fspath.join(location,imgFile))            .then(res =>{console.log(res)})            .catch(e=> console.log(""Error e ""+e)) // Error e TypeError: Only absolute URLs are supported```=====', 'I am not really sure then. I really do not know this library well enough to answer this question beyond the answer I gave.=====', '> i tried but the error is same if you want to look at my code will uppload my repo to githubCould you go ahead and do that?=====', '> > i tried but the error is same if you want to look at my code will uppload my repo to github> > Could you go ahead and do that?[Link to repo](https://github.com/MilindModi/FaceDemo)=====', ""`faceapi.bufferToImage` returns uses Blob under the hood, so you would need a node polyfill for that.`faceapi.fetchImage` as the name implies uses fetch under the hood, thus it doesn't work with filepaths to local files.I would recommend you to use the node canvas package for reading and writing images as stated in the README and shown in the node examples.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/366;Use non Nvidia GPU in Node;1;closed;2019-07-30T17:44:14Z;2019-10-12T11:21:39Z;I have an Intel GPU, if I use this in browser I get WebGL acceleration, but if I use Node I only get CPU backend, because the lack of CUDA cores.Can I use WebGL in Node to use my Intel graphics card acceleration?;"[""> Can I use WebGL in Node to use my Intel graphics card acceleration?As far as I know that's not possible, but your probably better off asking such kinds of questions to the tfjs community.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/364;tracking ;3;closed;2019-07-29T09:33:21Z;2019-10-12T11:22:17Z;i am building project tracking realtime, 1s i getScreenshot from webcam, i must convert img to blod then sumbit to server,then detection. i must read file save local disk becase face-api only read from path file. can you read file from base 64 or blob ? i love face-api. i want finish project then write medium page for face-api :D;['> can you read file from base 64 or blob ?Yes, either use the node canvas package as shown in the examples or use check out the decoding/encoding ops provided by tfjs-node.=====', 'you can help me a example code ?=====', 'Take a look at the nodejs examples.=====']
https://github.com/justadudewhohacks/face-api.js/issues/362;How to develop server side recognition model using faceapi, nodejs and express?;3;closed;2019-07-26T03:40:04Z;2019-10-12T11:22:46Z;Hi ,I am trying to develop a real time face recognition model using faceapi js using node and express. I have tried to recognize the face using the below code.[https://gist.github.com/Dannybrown2710/5e30c063da936ee947d8fff474f9f57b](url)But it seems that it takes a minute to train and recognize a model. Is there any way to improve recognizing speed to get real time results as in browser?;"['t has idea for you, you only load model once when start server. I am bulding project use sails server, detected and tracking realtime with camera ip.=====', ""I did this demo website: https://facelogin.vaa.redIt's a basic user authentication with facial recognition using faceapi-js. Runs with node.js, express, and socket.io. GitLab repo where you can find the code: https://gitlab.com/vaared/facelogin====="", ""> Is there any way to improve recognizing speed to get real time results as in browser?Probably if you have an Nvidia GPU, otherwise you won't get GPU acceleration in node.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/361;Model ssd;2;closed;2019-07-24T03:33:07Z;2019-07-29T07:11:17Z;i can set scoreThreshold for model SSD ?;"['What you are looking for is the ""minConfidence"" parameter of [SSDMobilenetV1Options](https://github.com/justadudewhohacks/face-api.js#ssdmobilenetv1options).=====', 'tks you :D =====']"
https://github.com/justadudewhohacks/face-api.js/issues/360;faceapi.loadSsdMobilenetv1Model;5;closed;2019-07-22T17:20:05Z;2019-09-18T14:26:45Z;i am trying to load the Ssd face detector in angularjs controller. when i load the model with the files under a public asset folder ,instead of the promise returning bounding boxes it return undefined . can `someone` tell me if i am missing something?``;"['you can view detail ?=====', 'I am not sure i understand what you mean by view detail .=====', 'no i am sorry, i want to view detail error of you :D=====', ""I think you should share some code of what you are doing. Yet I didn't quite understand your problem. Loading the model does not give you the bounding boxes, you have to call faceapi.detectAllFaces or faceapi.detectSingleFace. Check out the README for more info.====="", 'Closing because of inactivity, if this is still an issue, feel free to reopen and provide the requested information.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/359;Determine Face Shape;2;closed;2019-07-22T16:23:05Z;2019-09-18T14:27:15Z;Would it be possible to detect the face shape? e.g. round, square, oval, etc. Not sure if training data exists with this information.;"[""you can add feature_vector for my face ,in docs has example.const labeledDescriptors = [  new faceapi.LabeledFaceDescriptors(    'obama',    [descriptorObama1, descriptorObama2]  ),  new faceapi.LabeledFaceDescriptors(    'trump',    [descriptorTrump]  )]const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors)====="", 'Maybe it is possible to find a rule, based on the 68 face landmark points?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/357;Uncompressed Weights for Age and Gender;1;closed;2019-07-20T06:07:30Z;2019-07-22T12:47:52Z;Hi @justadudewhohacksCan you also release uncompressed weights for Age and Gender?Uncompressed weights are needed for running locally on mobile/cordovaThanks!;['Not sure why you need these with cordova, but I just uploaded the uncompressed weights.=====']
https://github.com/justadudewhohacks/face-api.js/issues/355;Tiny Face Detector inputSize has strange effect on detect result?;1;closed;2019-07-18T08:31:24Z;2019-10-12T11:23:34Z;"I run some test on demo site: https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detectionAnd found that when I choose different inputSize, some detected a face, some(224) didn't.Is it a bug?<img width=""472"" alt=""faceapi-160"" src=""https://user-images.githubusercontent.com/388222/61441841-0619c580-a979-11e9-8634-5eaf3f49109a.png""><img width=""462"" alt=""faceapi-224"" src=""https://user-images.githubusercontent.com/388222/61441912-29dd0b80-a979-11e9-8e46-141242fcba56.png""><img width=""451"" alt=""faceapi-320"" src=""https://user-images.githubusercontent.com/388222/61441942-33667380-a979-11e9-80bd-fef6a256568d.png""><img width=""462"" alt=""faceapi-416"" src=""https://user-images.githubusercontent.com/388222/61441971-3eb99f00-a979-11e9-8eb4-3e0645e76d14.png"">Tested on MacBook Pro 2017 13'. Macos 10.14.5. Chrome 75.0.3770.142(64bit). ";['Not really a bug, it just happens for some scales with the current model. If in doubt and you can afford it, run the prediction on different input sizes for better scalability.=====']
https://github.com/justadudewhohacks/face-api.js/issues/354;angular, ng serve always get error。only happend when face-api.js>0.20.1, which with 'draw'.;3;closed;2019-07-18T03:47:15Z;2019-07-22T08:27:37Z;"This is the wrong message I got.  ```ERROR in node_modules/@tensorflow/tfjs-core/dist/io/browser_files.d.ts(24,34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/http.d.ts(23,31): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/indexed_db.d.ts(31,34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/local_storage.d.ts(41,34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@types/offscreencanvas/index.d.ts(10,22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(11,22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(12,22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(17,43): error TS2304: Cannot find name 'ImageBitmapSource'.node_modules/@types/offscreencanvas/index.d.ts(18,43): error TS2304: Cannot find name 'ImageBitmapSource'.node_modules/@types/offscreencanvas/index.d.ts(27,53): error TS2304: Cannot find name 'CanvasState'.node_modules/@types/offscreencanvas/index.d.ts(27,66): error TS2304: Cannot find name 'CanvasTransform'.node_modules/@types/offscreencanvas/index.d.ts(27,83): error TS2304: Cannot find name 'CanvasCompositing'.node_modules/@types/offscreencanvas/index.d.ts(28,9): error TS2304: Cannot find name 'CanvasImageSmoothing'.node_modules/@types/offscreencanvas/index.d.ts(28,31): error TS2304: Cannot find name 'CanvasFillStrokeStyles'.node_modules/@types/offscreencanvas/index.d.ts(28,55): error TS2304: Cannot find name 'CanvasShadowStyles'.node_modules/@types/offscreencanvas/index.d.ts(28,75): error TS2304: Cannot find name 'CanvasFilters'.node_modules/@types/offscreencanvas/index.d.ts(28,90): error TS2304: Cannot find name 'CanvasRect'.node_modules/@types/offscreencanvas/index.d.ts(29,9): error TS2304: Cannot find name 'CanvasDrawPath'.node_modules/@types/offscreencanvas/index.d.ts(29,25): error TS2304: Cannot find name 'CanvasText'.node_modules/@types/offscreencanvas/index.d.ts(29,54): error TS2304: Cannot find name 'CanvasImageData'.node_modules/@types/offscreencanvas/index.d.ts(29,71): error TS2304: Cannot find name 'CanvasPathDrawingStyles'.node_modules/@types/offscreencanvas/index.d.ts(30,9): error TS2304: Cannot find name 'CanvasTextDrawingStyles'.node_modules/@types/offscreencanvas/index.d.ts(30,34): error TS2304: Cannot find name 'CanvasPath'.node_modules/@types/offscreencanvas/index.d.ts(42,53): error TS2304: Cannot find name 'CanvasRenderingContext2DSettings'.node_modules/tfjs-image-recognition-base/build/commonjs/dom/awaitMediaLoaded.d.ts(1,115): error TS2304: Cannot find name 'unknown'.```  this is my package.json . ``` {  ""name"": ""face-api"",  ""version"": ""0.0.0"",  ""scripts"": {    ""ng"": ""ng"",    ""start"": ""ng serve"",    ""build"": ""ng build"",    ""test"": ""ng test"",    ""lint"": ""ng lint"",    ""e2e"": ""ng e2e""  },  ""private"": true,  ""dependencies"": {    ""@angular/animations"": ""^6.1.0"",    ""@angular/common"": ""^6.1.0"",    ""@angular/compiler"": ""^6.1.0"",    ""@angular/core"": ""^6.1.0"",    ""@angular/forms"": ""^6.1.0"",    ""@angular/http"": ""^6.1.0"",    ""@angular/platform-browser"": ""^6.1.0"",    ""@angular/platform-browser-dynamic"": ""^6.1.0"",    ""@angular/router"": ""^6.1.0"",    ""@tensorflow/tfjs-node"": ""^1.2.3"",    ""canvas"": ""^2.5.0"",    ""core-js"": ""^2.5.4"",    ""draw"": ""0.0.0"",    ""face-api.js"": ""^0.20.1"",    ""rxjs"": ""~6.2.0"",    ""zone.js"": ""~0.8.26""  },  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.8.0"",    ""@angular/cli"": ""~6.2.3"",    ""@angular/compiler-cli"": ""^6.1.0"",    ""@angular/language-service"": ""^6.1.0"",    ""@types/jasmine"": ""~2.8.8"",    ""@types/jasminewd2"": ""~2.0.3"",    ""@types/node"": ""~8.9.4"",    ""codelyzer"": ""~4.3.0"",    ""jasmine-core"": ""~2.99.1"",    ""jasmine-spec-reporter"": ""~4.2.1"",    ""karma"": ""~3.0.0"",    ""karma-chrome-launcher"": ""~2.2.0"",    ""karma-coverage-istanbul-reporter"": ""~2.0.1"",    ""karma-jasmine"": ""~1.1.2"",    ""karma-jasmine-html-reporter"": ""^0.2.2"",    ""protractor"": ""~5.4.0"",    ""ts-node"": ""~7.0.0"",    ""tslint"": ""~5.11.0"",    ""typescript"": ""^2.9.2""  }}```   Is it because my typescript version can't match?  Can someone help me see what's wrong?";"['the edition of typescript is not matched.   should us v-3.xxx with face-api.js-0.20.1=====', ""@929853209 , Is your issue got resolved ?I'm also facing exactly same issue.====="", '> @929853209 , Is your issue got resolved ?> I\'m also facing exactly same issue.""face-api.js"": ""^0.20.1"",""typescript"": ""~3.4.3""this is my version in package.json ,there are some new typescript api in face-api.js-0.20.1，i have to update my angular version which can use typescript-3.4.3   if you want to use typescript-2.xxx,  must with face-api.js-0.19xxx, which without \'faceapi.draw\'=====']"
https://github.com/justadudewhohacks/face-api.js/issues/350;best practice for face recognition ?;2;closed;2019-07-15T03:04:30Z;2019-07-26T02:34:25Z;Hello,I have some questions?1. In example, you use 5 sample images for one face. What is the best descriptors size for one face ?  More is better ?2. Which size is the best aligned image for computing descriptors ? In example, aligned image is 150x150. Should I use larger aligned image ?Thx.;['Hello, I had the same doubts as you.I did tests with smaller size images, did training with 10 of my images and did not have significant improvements compared to when I trained with only 1 image.I think an image is enough already.=====', '> In example, you use 5 sample images for one face. What is the best descriptors size for one face ? More is better ?Yes you can do that, but usually having one high quality sample image (frontal face image) is enough.> Which size is the best aligned image for computing descriptors ? In example, aligned image is 150x150. Should I use larger aligned image ?Using larger input sizes will have no effect, since it will get downscaled to 150x150 anyways. Using smaller sizes will make the input blurry when upscaling, thus the lower the resolution, the less precise the descriptor might be.=====']
https://github.com/justadudewhohacks/face-api.js/issues/349;models/face_landmark_68_model-shard1  not fount 404;1;closed;2019-07-10T12:01:06Z;2019-10-12T11:23:47Z;I when placing the files in a subpath of my server, the project can not find the file: models / face_landmark_68_model-shard1 Files with .json are localized but not others.How to solve? ;['Check the network tab, e.g. the URI your client attempts to fetch the shards from. Most likely the URI is invalid, or the files are not accessible under that URI.=====']
https://github.com/justadudewhohacks/face-api.js/issues/348;i can  train  my model ?;1;closed;2019-07-10T03:00:42Z;2019-07-22T13:04:15Z;;"[""face-api.js doesn't provide the means to train own models, if that was the question.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/346;Error in detect face process;3;closed;2019-07-08T19:29:27Z;2019-10-12T11:24:29Z;"hi, i have a error in ```const resultsRef =  await faceapi.detectSingleFace(referenceImage, faceDetectionOptions.faceDetectionOptions)      .withFaceLandmarks()      .withFaceDescriptors(),```is a node backend, when starting it works correctly, but after a few attempts, it dies in that specific line where the faces of the image are detected.when puts a try catch to the await, throws the next error""TypeError: faceapi.detectSingleFace(...).withFaceLandmarks(...).withFaceDescriptors is not a function""Node: 9.0SO: Centos7sorry for my bad english.";"['Hi,Using `detectSingleFace` it has to be `withFaceDescriptor` and not the plural form `withFaceDescriptors`-=====', '> Hi,> > Using `detectSingleFace` it has to be `withFaceDescriptor` and not the plural form `withFaceDescriptors`-sorry, my bad but I still have the same problem, I run the server, made about 5 successful consultations and then it does not work anymore.`await faceapi.detectSingleFace(referenceImage, faceDetectionOptions.faceDetectionOptions)      .withFaceLandmarks()      .withFaceDescriptors(),`  begins to return ""resultsRef=[]"", and it does not work anymore even if i restart the server=====', 'You probably mean resultsRef = undefined. `detectSingleFace` returns undefined if no face has been detected.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/345;Preventing memory leak with detectAllFaces() ?;9;closed;2019-07-05T20:03:33Z;2021-08-27T04:09:14Z;I am using `detectAllFaces()` with `TinyFaceDetector`. Every 500ms I feed it a new image to analyze. I noticed that I'm getting a memory leak because new `canvas` elements keep getting created and are never disposed of (I can see them in the Developer Tools' **Canvas** tab).What is the proper way to dispose of all these temporary `canvas` ?P.S. Thanks for a great library!;"['I just realized that the memory issue I am having is related to a leak in the CameraPreview cordova module. Sorry for the false report...=====', ""Well, it seems that `detectAllFaces()` may present an issue after all... Or, maybe, I do not understand how the library works.As far as I can tell, each time `detectAllFaces()` is called with an `HTMLImageElement` as the first parameter, it creates a `<canvas>` element and stores it in an array. This is done in `NetInput.js` at line 35-36: ```javascriptvar canvas = input instanceof env.getEnv().Canvas ? input : createCanvasFromMedia(input),_this._canvases[idx] = canvas,```As far as I can tell, these canvases never get garbage collected. At least, I can see them pile up in the Developer Tools' Canvas tab. Could there be a problem there?====="", ""I'm going crazy from getting random crashes... but I think it's something else... I'll close this until I get better data. ====="", 'Hey Jean-Philippe :)> As far as I can tell, each time detectAllFaces() is called with an HTMLImageElement as the first parameter, it creates a <canvas> element and stores it in an array. This is done in NetInput.js at line 35-36:Yes, this is correct.> As far as I can tell, these canvases never get garbage collected.They should get garbage collected, otherwise you will run out of memory pretty quickly. Maybe you can share some code of what you are doing? If these canvases do not get garbage collected, then there is probably some reference dangling around somewhere.Maybe you are unintenendly storing the NetInputs on some global object? Maybe the Dev Tools you are using itself cause the memory leak, because it is holding references to the canvases for debugging purposes. Same goes for logging the objects to the console, which will prevent them from being garbage collected until you clear your console.=====', 'Hey Vincent!I didn\'t know if you would remember our previous exchanges... :-) As you can see, I am (once again) using some of your software for one of my projects. I\'m developing an iPhone app using Phonegap. I\'m running face-api.js to perform face detection (no recognition, no landmark detection or anything else) from the phone\'s camera. I get a rate of about 4fps on an iPhone 5S, which is fine for what I need. However, after 1300-1600 detections, the app crashes and I don\'t know why. One of the Cordova plugins I was using leaked memory but it has since been fixed. I have no hints as to what\'s going on... The only thing that makes me think it might somehow be related to face-api.js, is that when I remove the detection code, it takes longer to crash (but it does crash anyway, so it\'s probably not related to face-api.js).Here\'s the code I am using: ```javascriptasync detectFromBase64ImageData(base64ImageData, mimeType = ""image/jpeg"") {  return new Promise(resolve => {    this.image.addEventListener(""load"", async () => {      const faces = await this.detect(this.image),      resolve(faces),    }, {once: true}),    this.image.src = ""data:"" + mimeType + "",base64,"" + base64ImageData,  }),}async detect(source) {  let faces,  try {     faces = await faceapi.detectAllFaces(source, this.options),  } catch (err) {    console.info(err),  }  return faces,}```The `detectFromBase64ImageData()` function gets called repeatedly with the image data from the camera. It works fine for about 12 minutes (or 1500 detections) and then the app crashes. My tests seem to indicate that memory usage is going up...Any ideas?=====', ""Hmm, I can't spot anything obvious, that would cause a leak. I didn't run in such issues on my mobiles devices before.You could try out the webcam examples on the face-api demo page with your iphone. If that's working you atleast know for sure, that it's not an issue with face-api.js.====="", ""> You could try out the webcam examples on the face-api demo page with your iphone. If that's working you atleast know for sure, that it's not an issue with face-api.js.Yep, that's a good idea. But, like I said, I don't think the problem is with face-api.js. That's why I close the issue myself. From the crash logs, I think the problem is that the application draws too much CPU over too long.Anyway, thanks for the input!====="", ""I can confirm that `detectAllFaces` is leaking memory. I pass frames to a WebWorker with postMessage and transfer lists, then create an ImageData in the WebWorker as shown below.If I run the code as shown, the Chrome process memory balloons and grows DURING the frame processing. (E.g. while the webworker is inside `detectAllFaces`. Within ~3 frames, Chrome is using 25% of the RAM and growing. After ~30 seconds, Chrome has consumed nearly all of my 16 GB of RAM.As soon as I comment out the line that has `detectAllFaces` - memory usage flatlines. Does not grow or change. And that is the ONLY code change I make - with detectAllFaces, memory leaks. Without that call, no memory leak. Therefore, something is going wrong with detectAllFaces - because there are no other references to the given imgData other than detectAllFaces.Any ideas on how to diagnose further or, hopefully, fix?```// Earlier in the code ...faceapi.nets.ssdMobilenetv1.loadFromUri('/models')this.faceDetectorOptions = new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }),// For every message from the parent thread, we do...const imgData = new ImageData(\tnew Uint8ClampedArray(message.data),\tmessage.width,\tmessage.height),const img = faceapi.createCanvasFromMedia(imgData),const results = await faceapi.detectAllFaces(img, this.faceDetectorOptions)```====="", 'i have same issue.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/344;Angular Face Matcher Promise.all undefined;3;closed;2019-07-05T16:13:23Z;2019-07-15T23:31:23Z;Hello Developers, i have a problem with face-api.js and Angularmy code:```@ViewChild('imageElement', {static: true, read: ElementRef}) image: ElementRef,  @ViewChild('videoElement', {static: true, read: ElementRef}) video: ElementRef,  @ViewChild('loginElement', {static: true, read: ElementRef}) login: ElementRef,  results: any = null,  constructor(private http: UsuariosService) {  }  ngOnInit() {  }  ngAfterViewInit(): void {    if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {      navigator.mediaDevices.getUserMedia({video: true, audio: false}).then(stream => {        this.video.nativeElement.srcObject = stream,      }),    }    this.promesa(),  }  // Funciones personalizadas  async promesa() {    return await Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri('./assets/models'),      faceapi.nets.faceLandmark68Net.loadFromUri('./assets/models'),      faceapi.nets.faceRecognitionNet.loadFromUri('./assets/models'),      faceapi.nets.faceExpressionNet.loadFromUri('./assets/models')    ]).then(() => {      this.video.nativeElement.play(),    }),  }  async initVideo() {    try {      await this.loadLabeledImages(),      Swal.fire({        title: 'Espere',        type: 'info',        text: 'Cargando a los usuarios.'      }),      Swal.showLoading(),      setTimeout(async () => {        Swal.close(),        const values = await new faceapi.FaceMatcher(await this.results, 0.6),        const canvas = await faceapi.createCanvasFromMedia(this.video.nativeElement),        this.login.nativeElement.append(canvas),        canvas.setAttribute('class', 'canvas'),        const displaySize = {width: this.video.nativeElement.offsetWidth, height: this.video.nativeElement.offsetHeight},        faceapi.matchDimensions(canvas, displaySize),        setInterval(async () => {          const detections = await faceapi.detectAllFaces(this.video.nativeElement,            new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors(),          const resizedDetections = faceapi.resizeResults(detections, displaySize),          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height),          faceapi.draw.drawDetections(canvas, resizedDetections),          const results = resizedDetections.map(d => values.findBestMatch(d.descriptor)),          results.forEach((result, i) => {            console.log('Resultados: ' + result),            const box = resizedDetections[i].detection.box,            const drawBox = new faceapi.draw.DrawBox(box, {label: result.toString()}),            drawBox.draw(canvas),          }),          // faceapi.draw.drawFaceLandmarks(canvas, resizedDetections),        }, 100),      }, 20000),    } catch (e) {      console.log(e),    }  }  loadLabeledImages() {    console.log('Entro a loadLabeledImages()'),    this.http.getItems().subscribe(async resp => {      const labels = [],      for (const usuario of resp) {        labels.push(usuario.nombre),      }      return Promise.all(        labels.map(async label => {          const descriptions: Float32Array[] = [],          for (const usuario of resp) {            for (const images of usuario.images) {              const url = 'data:' + images.tipo + ',base64,' + images.imagen,              const img = await faceapi.fetchImage(url),              const detections = await faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())                .withFaceLandmarks().withFaceDescriptor(),              if (typeof detections !== 'undefined') {                descriptions.push(Float32Array.from(detections.descriptor)),              }            }          }          return new faceapi.LabeledFaceDescriptors(label, descriptions),        })      ).then(value => {        this.results = value,      }),    }),  }```in the initvideo() function I tried to put this```const x = await this.loadLabeledImages(),const values = await new faceapi.FaceMatcher( x, 0.6),``````loadLabeledImages() {    console.log('Entro a loadLabeledImages()'),    this.http.getItems().subscribe(async resp => {      const labels = [],      for (const usuario of resp) {        labels.push(usuario.nombre),      }      return Promise.all(        labels.map(async label => {          const descriptions: Float32Array[] = [],          for (const usuario of resp) {            for (const images of usuario.images) {              const url = 'data:' + images.tipo + ',base64,' + images.imagen,              const img = await faceapi.fetchImage(url),              const detections = await faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())                .withFaceLandmarks().withFaceDescriptor(),              if (typeof detections !== 'undefined') {                descriptions.push(Float32Array.from(detections.descriptor)),              }            }          }          return new faceapi.LabeledFaceDescriptors(label, descriptions),        })      ),    }),```but the function loadLabeled does not wait for the result and error tells me that **x is undefined** in the FaceMatcher and leaves the Try and goes to the CatchThe only solution I have found is to put a settimeout but in case they are more faces it will take longer and it will be longer and I can not be guessing the time it will take, I tried everything, how I solve it;"['Looks like you are awaiting ""loadLabeledImages"", but loadLabeledImages has no return value. It neither is an async function, nor does it return a promise, so awaiting it will have no effect. You probably want to turn this.http.getItems() into a Promise and return it.=====', '> Parece que está esperando ""loadLabeledImages"", pero loadLabeledImages no tiene valor de retorno. Ni es una función asíncrona, ni devuelve una promesa, por lo que esperarla no tendrá ningún efecto. Probablemente desee convertir this.http.getItems () en una Promesa y devolverlo.ok, i will try.=====', ""it's working, thank you.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/343;RangeError: attempting to construct out-of-bounds;2;closed;2019-07-04T15:20:44Z;2019-07-05T22:29:30Z;Hello,When I pass my project online I have this error (on localhost all work great online I have this error)`RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer face-api.js:100:444722`This error break the load of the models or the models isn't loaded and this error appears ?Any idea ?;['After some search I hav found something weird. I lose some octets on model files.Now I have this error after update to the latest version of face-api```RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer tf-core.esm.js:17:443843Error: Based on the provided shape, [512,2], the tensor should have 1024 values but has 986 tf-core.esm.js:17:4452RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer tf-core.esm.js:17:443843```=====', 'I have found the error. The error comes from Filezilla. Just choose tranfert -> binary for the tranfert of the model files and all work fine. If necessary clear the navigator cache.=====']
https://github.com/justadudewhohacks/face-api.js/issues/341;Limitations of this api;2;closed;2019-07-04T01:18:42Z;2019-07-12T05:26:10Z;Is there a limit of faces that this api can recognize?;['Not sure if I understood the question correctly. There is no limit, since the face recognition model has been trained to describe the features of a face in form of a 128 dimensional vector, which should be unique to a persons face. Thus you can recognize any face by computing the similarity (euclidean distance) of a face to other faces.=====', 'Thank you for the clarificationOn Sat, Jul 6, 2019, 1:58 PM Vincent Mühler <notifications@github.com>wrote:> Not sure if I understood the question correctly. There is no limit, since> the face recognition model has been trained to describe the features of a> face in form of a 128 dimensional vector, which should be unique to a> persons face. Thus you can recognize any face by computing the similarity> (euclidean distance) of a face to other faces.>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/341?email_source=notifications&email_token=AL2IQO3ZHTNARUHAVWJSIETP6AX77A5CNFSM4H5PFPRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZKTFEI#issuecomment-508899985>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AL2IQO6FOG7PD5I3DNGGNNLP6AX77ANCNFSM4H5PFPRA>> .>=====']
https://github.com/justadudewhohacks/face-api.js/issues/338;License;1;closed;2019-06-28T11:26:04Z;2019-07-22T13:06:26Z;Good morning,I saw you use tensorflow.I would like to know if I can use this library (face-api and tensorflow) for commercial use.Thank you.;['face-api.js has a MIT license and is built on top of tfjs-core, which comes with the Apache Version 2.0 license. So yes you can use it for commercial purposes.=====']
https://github.com/justadudewhohacks/face-api.js/issues/337;Detecting face direction or gaze?;3;closed;2019-06-27T16:54:40Z;2019-07-22T13:06:49Z;I LOVE this repo. So easy and simple to use. Thanks for this!I'm looking for a way to detect the direction a face is looking or maybe even gaze detection. Are there any models out there for this that we can just throw into the `weights` folder and play with? Do models need to be converted in a certain way to be used by this API?;"[""face-api.js does not have a model for gaze detection.> Do models need to be converted in a certain way to be used by this API?face-api.js doesn't prescribe or provide an API for custom models. You can simply use tfjs for own models.====="", ""Sorry for my basic questions. I'm not very familiar with ML frameworks or TF, so making my own model is beyond my abilities right now.I'm basically hoping to use something like [OpenVINO's head pose model](https://github.com/opencv/open_model_zoo/blob/master/intel_models/head-pose-estimation-adas-0001/head-pose-estimation-adas-0001.prototxt). I would try to convert it with [this](https://github.com/chaosmail/prototxt-parser).====="", 'Since the model you posted is a .prototxt file, I am assuming that this is a Caffe Model. You can convert tensorflow models to tfjs model using [tfjs-converter](https://github.com/tensorflow/tfjs-converter), but this would require you to convert the Caffe Model to a tensorflow model first. Alternatively you can reimplement the network structure described in the .prototxt file with tfjs-core.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/336;Issue With Euclidian distance of Facematcher;3;closed;2019-06-27T10:53:49Z;2019-07-23T04:54:11Z;"const video = document.getElementById('input')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri('/models'),  faceapi.nets.faceLandmark68Net.loadFromUri('/models'),  faceapi.nets.faceRecognitionNet.loadFromUri('/models'),  faceapi.nets.faceExpressionNet.loadFromUri('face-api.js-master/weights'),  faceapi.nets.ssdMobilenetv1.loadFromUri('face-api.js-master/weights'),]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}video.addEventListener('play', () => {  const canvas = faceapi.createCanvasFromMedia(video)  canvas.style.position=""absolute""  document.body.append(canvas)  const displaySize = { width: video.width, height: video.height }    faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor()  //  detections = faceapi.resizeResults(detections,displaySize)  const displaySize1 = {width:canvas.height, height:canvas.height}    const resizedDetections = faceapi.resizeResults(detections, displaySize1)  canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)    faceapi.draw.drawDetections(canvas,detections)    const labels =['Rakesh']    const labeledFaceDescriptors = await Promise.all(     labels.map(async label => {       const descriptions = []       for (let i = 1, i <= 2, i++) {         const img = await faceapi.fetchImage(`https://raw.githubusercontent.com/rakesh553/labels/master/${label}/${i}.jpg`)         const detections1 = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()         descriptions.push(detections1.descriptor)         if (!detections1) {            console.log(""no faces detected for ${label}"")          }       }          return new faceapi.LabeledFaceDescriptors(label, descriptions)     })   )       const maxDescriptorDistance = 0.6    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance)const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor)) results.forEach((bestMatch ,i)=>{     const box = resizedDetections[i].detection.box     const text = bestMatch.toString()     const drawBox = new faceapi.draw.Drawbox(box,{label:text})     drawBox.draw(canvas) })}, 100)})This code when I run in chrome tells me thatit has some property height which is not defined";['Can you show me the error message / stack trace and on which line this error occurs please.=====', 'If this is still an issue, feel free to reopen and post the error message / stack trace.=====', 'Hi Sir,I apologise for the late reply. I had other project in process so I couldnot answer to the issue.I got my application done. Thank you the supportand replyingThanks & RegardsRakeshOn Mon, Jul 22, 2019 at 6:37 PM Vincent Mühler <notifications@github.com>wrote:> Closed #336 <https://github.com/justadudewhohacks/face-api.js/issues/336>.>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/336?email_source=notifications&email_token=ALFSEVYJ2XDFRPEUBDKR5ADQAWWJNA5CNFSM4H32XKPKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOST7VGFA#event-2499760916>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/ALFSEV2GQ4I2SFPJYADAQLLQAWWJNANCNFSM4H32XKPA>> .>=====']
https://github.com/justadudewhohacks/face-api.js/issues/335;How to identify if it is a real person or a picture in front of webcam.;4;closed;2019-06-27T03:41:39Z;2019-08-13T12:10:55Z;Is there's a way on how to identify who's in front of the webcam a real person or picture?;['No there is no such model provided by face-api.js, which could distinguish.=====', 'Is this possible in the future? Thank you. :)=====', '> Is this possible in the future? Thank you. :)Not sure if this is going to be a feature, I am not even sure how to approach this problem / if it is possible to train such a model.=====', 'Is there a way to disable recognition if Image is used and not the real person ?=====']
https://github.com/justadudewhohacks/face-api.js/issues/334;extendStatics$1;2;closed;2019-06-26T14:33:58Z;2019-07-22T13:09:06Z;Hi,first of all, I would like thank you for all your works.I encounter a problem when I use images on faceDetection and others api but not with the webcam, here is the error ![image](https://user-images.githubusercontent.com/52248745/60188417-87c17c00-982f-11e9-9fed-3d8e4632eeb5.png)here is the problem p.byteLength = 81881 and not 81888 !![image](https://user-images.githubusercontent.com/52248745/60188459-99a31f00-982f-11e9-880d-919f03b4a165.png)I have been surching for a long time, can you help me please ?;['`decodeWeights` comes from tfjs-core. Looks like the weight or shard files you are trying to load are malformed somehow.=====', 'If this is still an issue feel free to reopen.=====']
https://github.com/justadudewhohacks/face-api.js/issues/331;Distance away picture cannot be detected in tiny face detector model.;4;closed;2019-06-24T08:15:02Z;2019-07-23T02:15:26Z;Working fine in Ssd MobileNet, however when I using tiny face detector it will not working to detect a distance away picture. ;"['Same issue too=====', 'The SSD model is able to handle much smaller faces than the tiny face detection model I trained.=====', ""@justadudewhohacks I used SSD model before and it's very nice, however it's very lag, that's why I tried tiny detection. Is the possible reason why it's lag because of my machine? or need higher GPU or else?====="", ""> Is the possible reason why it's lag because of my machine? or need higher GPU or else?Yes, the SSD model requires much more resources and will run slower than the tiny model.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/327;Error: Failed to link vertex and fragment shaders.;6;closed;2019-06-18T15:03:51Z;2019-07-13T14:42:06Z;"I get this error:![‏‏errorImg](https://user-images.githubusercontent.com/18192032/59695587-2546fb00-91f3-11e9-8896-5373c99a0c19.PNG)package.json: > {  ""name"": ""eyes-blinking"",  ""version"": ""0.0.0"",  ""scripts"": {    ""ng"": ""ng"",    ""start"": ""ng serve"",    ""build"": ""ng build"",    ""test"": ""ng test"",    ""lint"": ""ng lint"",    ""e2e"": ""ng e2e""  },  ""private"": true,  ""dependencies"": {    ""@angular/animations"": ""~7.2.0"",    ""@angular/cdk"": ""~7.3.7"",    ""@angular/common"": ""~7.2.0"",    ""@angular/compiler"": ""~7.2.0"",    ""@angular/core"": ""~7.2.0"",    ""@angular/flex-layout"": ""^8.0.0-beta.25"",    ""@angular/forms"": ""~7.2.0"",    ""@angular/material"": ""^7.3.7"",    ""@angular/platform-browser"": ""~7.2.0"",    ""@angular/platform-browser-dynamic"": ""~7.2.0"",    ""@angular/router"": ""~7.2.0"",    ""core-js"": ""^2.5.4"",    ""face-api.js"": ""^0.20.0"",    ""hammerjs"": ""^2.0.8"",    ""ngx-webcam"": ""^0.2.2"",    ""rxjs"": ""~6.3.3"",    ""tracking"": ""^1.1.3"",    ""tslib"": ""^1.9.0"",    ""webpack"": ""^4.32.0"",    ""zone.js"": ""~0.8.26"",    ""@tensorflow/tfjs-core"": ""1.0.4"",    ""tfjs-image-recognition-base"": ""^0.5.1""  },  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.13.0"",    ""@angular/cli"": ""~7.3.6"",    ""@angular/compiler-cli"": ""~7.2.0"",    ""@angular/language-service"": ""~7.2.0"",    ""@types/jasmine"": ""~2.8.8"",    ""@types/jasminewd2"": ""~2.0.3"",    ""@types/node"": ""~8.9.4"",    ""@types/tracking"": ""^1.1.29"",    ""codelyzer"": ""~4.5.0"",    ""jasmine-core"": ""~2.99.1"",    ""jasmine-spec-reporter"": ""~4.2.1"",    ""karma"": ""~4.0.0"",    ""karma-chrome-launcher"": ""~2.2.0"",    ""karma-coverage-istanbul-reporter"": ""~2.0.1"",    ""karma-jasmine"": ""~1.1.2"",    ""karma-jasmine-html-reporter"": ""^0.2.2"",    ""protractor"": ""~5.4.0"",    ""ts-node"": ""~7.0.0"",    ""tslint"": ""~5.11.0"",    ""typescript"": ""~3.2.2"",    ""@tensorflow/tfjs-node"": ""^1.0.2""  }}";"['Similar error, browser Chrome 75.0.3770.100 (Windows 10, 64 bit)face-api.js v0.20.0 and 0.19.0tf-core.esm.js:4786 C:\\fakepath(180,2-134): error X3500: array reference cannot be used as an l-value, not natively addressableC:\\fakepath(159,7-117): error X3511: forced to unroll loop, but unrolling failed.C:\\fakepath(157,7-117): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\\fakepath(180,2-134): error X3500: array reference cannot be used as an l-value, not natively addressableC:\\fakepath(159,7-117): error X3511: forced to unroll loop, but unrolling failed.C:\\fakepath(157,7-117): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\\fakepath(180,2-134): warning X3550: array reference cannot be used as an l-value, not natively addressable, forcing loop to unrollC:\\fakepath(180,2-134): error X3500: array reference cannot be used as an l-value, not natively addressableC:\\fakepath(159,7-117): error X3511: forced to unroll loop, but unrolling failed.C:\\fakepath(157,7-117): error X3511: forced to unroll loop, but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D ShadersUncaught (in promise) Error: Failed to link vertex and fragment shaders.    at xr (tf-core.esm.js:4786)    at e.createProgram (tf-core.esm.js:5381)    at tf-core.esm.js:5578    at tf-core.esm.js:8507    at e.getAndSaveBinary (tf-core.esm.js:8524)    at e.compileAndRun (tf-core.esm.js:8506)    at e.conv2dWithIm2Row (tf-core.esm.js:8277)    at e.conv2d (tf-core.esm.js:8282)    at We.engine.runKernel.x (tf-core.esm.js:9797)    at tf-core.esm.js:1494=====', 'try this: right click on computer -> properties -> task manager -> display adapters -> right click on your graphic card (I have Intel (R) HD Graphics 3000) and update driver, disable device, enable device. It should do the job =====', 'That didnt solved my problemAnd I have Intel (R) HD Graphics 3000 too=====', 'This error comes from the tfjs-core WebGL backend. Could be some device issue, for example some android devices too have the same issue: https://github.com/tensorflow/tfjs/issues/952, #151.Closing since this is not really something that face-api.js has control over, probably better to file an issue at tfjs. I would also recommend playing around with some tfjs examples first, to see whether this issue occurs and which ops might cause this issue.=====', ""@justadudewhohacks there's any workaround for this?In my case it's failing on all Samsung devices and Motorola.====="", 'That didnt solved my problemAnd I have Intel (R) HD Graphics 4000 too, I5 and 6GB=====']"
https://github.com/justadudewhohacks/face-api.js/issues/326;Does face-api not support face similarity between video and image matching? Now I'm converting my face into canvas and comparing it with the image. That's a low recognition rate.   ;1;closed;2019-06-18T03:19:22Z;2019-07-22T13:16:19Z;Does face-api not support face similarity between video and image matching? Now I'm converting my face into canvas and comparing it with the image. That's a low recognition rate.;['The type of input (video, image) does not matter. There is probably some issue with your code or the reference image you are using. If this is still an issue, feel free to reopen and post the relevant code + reference Image you are using + an example query image, that was not recognized correctly.=====']
https://github.com/justadudewhohacks/face-api.js/issues/322;How to identify if it is a real person or a picture in front of webcam.;3;closed;2019-06-15T06:24:51Z;2019-08-13T12:18:30Z;Is there's a way on how to identify who's in front of the webcam a real person or picture? ;"['@justadudewhohacks I need your help please.=====', ""@esmeromichael @justadudewhohacks I'm interested too!Will be realy interesting to distinguish a person or photograph.====="", '+1 🙋\u200d♂️=====']"
https://github.com/justadudewhohacks/face-api.js/issues/316;Working with Videos in NodeJS;3;closed;2019-06-10T08:01:47Z;2019-07-22T13:22:43Z;I did not see any video examples in your nodejs folder.I was wondering if this is supported yet, or is it just images currently?I tried looking into it, and it seems I need to find a way to convert a video file to a `TNetInput`? Does that sound right?I also looked for `HTMLVideoElement` polyfills for Node, but did not find any.;['face-api.js is not concerned with decoding video in nodejs, thus you might want to use any kind of polyfill (if exists) or video decoding package to generate tensors for video frames. For example I use opencv4nodejs in conjunction with face-api.js a lot.=====', 'How do you integrate opencv4nodejs with face-api.js? Can you please be a little more specific. Thanks!=====', '@VKairon you can simply convert from an opencv Mat to a tensor using the following code:``` javascript// assuming img is a BGR Matconst data = new Uint8Array(img.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)const imgTensor = faceapi.tf.tensor3d(data, [img.rows, img.cols, 3])```=====']
https://github.com/justadudewhohacks/face-api.js/issues/315;Model Licenses;1;closed;2019-06-10T01:09:20Z;2019-07-22T13:20:58Z;Are the models included in this repo licensed under MIT? I'm doing an art project for a senior hum class at my college and wanted to make sure I didn't violate any of the license agreements of software I was using. Thanks!;"[""All of the model files available in this repo are licensed under MIT yes.If you are really concerned about licenses of some of the original model weights, that I haven't trained by myself, then check out the license agreements for the [SSD Mobilenet V1](https://github.com/yeephycho/tensorflow-face-detection) face detector, the [MTCNN](https://github.com/kpzhang93/MTCNN_face_detection_alignment) and the [face recognition model](https://github.com/davisking/dlib-models).All other models are created by myself and you can use them for whatever purpose you want.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/313;LabeledFaceDescriptor Issue;2;closed;2019-06-05T17:40:43Z;2019-07-22T13:25:14Z;Hello Vincent,I know this question has been asked a few times, and I've looked at the solutions but couldn't resolve my issue using the available solutions. Just not sure what to change to a Float32Array.I will attach my code below!Thank you![Screen Shot 2019-06-05 at 12 40 07 PM](https://user-images.githubusercontent.com/50146137/58977340-1aba4980-878f-11e9-9e15-afef385d3968.png)![Screen Shot 2019-06-05 at 12 40 19 PM](https://user-images.githubusercontent.com/50146137/58977341-1aba4980-878f-11e9-80bc-6a3177e4cf56.png);"['![Screen Shot 2019-06-05 at 12 42 36 PM](https://user-images.githubusercontent.com/50146137/58977469-6d940100-878f-11e9-8366-9684442836a9.png)=====', ""Hi, `detectAllFaces` returns all faces (a javascript array), so fullFaceDescriptions.descriptor will be undefined in your code. Use detectSingleFace` if that's what you intended to do, or extract the descriptors from the array.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/312;Slow in nodejs project;8;closed;2019-06-05T14:21:21Z;2020-03-14T19:53:09Z;Hello!I am trying to setup simple face detection in nodejs env. Out of the box everything run fine with the exception of speed. It takes 12+ seconds for a single face detection.`require('@tensorflow/tfjs-node')`is included!On execution i see:> cpu backend was already registered. Reusing existing backendthen> Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up > dramatically, install our node backend, which binds to TensorFlow C++, by running npm i > @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call > require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit > https://github.com/tensorflow/tfjs-node for more details.When i install older versions of the two packages:`npm install @tensorflow/tfjs-node@0.3.0 face-api.js@0.14.3`there is significant speed improvement, BUT i need age and gender detection , which is not present below face-api.js < 0.20.0I also tryed:@tensorflow/tfjs-node version 1.0.2but then i get:> (node:29723) UnhandledPromiseRejectionWarning: TypeError: tfc.ENV.findBackend is not a > function>    at nodeBackend (/home/pi/testproject/node_modules/@tensorflow/tfjs-> node/dist/ops/op_utils.js:25:28)How can i use age and gender recognition and get faster speed under nodejs?**Environment**Raspberry PiNode: v8.10.0face-api.js: 0.20.0 / 0.14.3@tensorflow/tfjs-node: 1.1.2 / 1.0.2 / 0.3.0;"['@tensorflow/tfjs-node 1.0.2 + @tensorflow/tfjs-core 1.0.3 should work with face-api.js 0.20.0. This is the setup that the nodejs examples use. The error message you posted might indicate, that you are not using tfjs-core 1.0.3.=====', 'Hi @justadudewhohacks, I am also facing the same issue. I have run the face-api.js on browser for facial expression and landmark detection, which was comparatively very fast, but in nodejs, it was damn too slow, (i.e approx 1 frame in 5-6sec). Please help me out how I can do to fix this issue.=====', 'As I answered above, make sure the package versions line up correctly.=====', 'I also had the problem of agreeing and spent about 17 seconds in nodejs. Please see if my reference package is correct.![image](https://user-images.githubusercontent.com/24919652/59317744-dc74db00-8cf6-11e9-8787-d12070eab440.png)=====', 'Thank you for the help  @justadudewhohacks!More than 10 times faster when > @tensorflow/tfjs-node 1.0.2 + @tensorflow/tfjs-core 1.0.3=====', ""I'm getting following error, please help to resolvecpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 4module.js:549    throw err,    ^Error: Cannot find module 'D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v3\\tfjs_binding.node'    at Function.Module._resolveFilename (module.js:547:15)    at Function.Module._load (module.js:474:25)    at Module.require (module.js:596:17)    at require (internal/module.js:11:18)    at Object.<anonymous> (D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules\\@tensorflow\\tfjs-node\\dist\\index.js:44:16)    at Module._compile (module.js:652:30)    at Object.Module._extensions..js (module.js:663:10)    at Module.load (module.js:565:32)    at tryModuleLoad (module.js:505:12)    at Function.Module._load (module.js:497:3)====="", ""> I'm getting following error, please help to resolve> > cpu backend was already registered. Reusing existing backend factory.> Platform node has already been set. Overwriting the platform with [object Object].> node-pre-gyp info This Node instance does not support builds for N-API version 4> node-pre-gyp info This Node instance does not support builds for N-API version 4> module.js:549> throw err,> ^> > Error: Cannot find module 'D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules@tensorflow\\tfjs-node\\lib\\napi-v3\\tfjs_binding.node'> at Function.Module._resolveFilename (module.js:547:15)> at Function.Module._load (module.js:474:25)> at Module.require (module.js:596:17)> at require (internal/module.js:11:18)> at Object. (D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules@tensorflow\\tfjs-node\\dist\\index.js:44:16)> at Module._compile (module.js:652:30)> at Object.Module._extensions..js (module.js:663:10)> at Module.load (module.js:565:32)> at tryModuleLoad (module.js:505:12)> at Function.Module._load (module.js:497:3)Same issue here. Impossible to include @tensorflow. It installs without errors. Tried many versions. Still not able to include.====="", '@0x6c23 Did you find any solution for this error? =====']"
https://github.com/justadudewhohacks/face-api.js/issues/308;pre-processing;1;closed;2019-05-31T10:17:01Z;2019-07-22T13:26:05Z;Hello. I want to annoy you again.How does your photo pre-processing process take place? I see you use: const meanRgb = [122.782, 117.001, 104.298]      const normalized = normalize (batchTensor, meanRgb) .div (tf.scalar (255)) as tf.Tensor4Dand resize images. But I found that while reading the docs, it showed that relying on 68 points of face you will get face image and resize the image to the face. So when will the image processing process take place? Before or after getting a face?thank you very much.;['The pre processing of the network input will be applied at the beginning of each networks forward function.=====']
https://github.com/justadudewhohacks/face-api.js/issues/307;loading models in angular ;2;closed;2019-05-28T15:09:47Z;2020-10-31T16:43:02Z;hi, I try to load models from my app component who located in: myproject/src/app/openCam/opencamComponent.tsI run: Promise.all([ faceapi.nets.faceLandmark68Net.loadFromUri('./models')  ]),I put the models folder in:  myproject/src/models. I got the error: ERROR Error: Uncaught (in promise): Error: failed to fetch: (404) Not Found, from url: http://localhost:4200/models/face_landmark_68_model-weights_manifest.jsonError: failed to fetch: (404) Not Found, from url: http://localhost:4200/models/face_landmark_68_model-weights_manifest.jsonmy models folder contains all files from: face-api.js/weights/ in this github repository. I know little about AI and I want to landmark only eyes. ;"[""according the taturial.. I pat models folder into assets and changed the code line to:     await faceapi.nets.faceLandmark68Net.loadFromUri('assets/models'),====="", 'The complete path to your model is project/src/assets/models ?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/306;Improve speed of first detection?;3;closed;2019-05-27T03:39:00Z;2020-01-09T03:14:49Z;"The first call to detect a photo takes about 10 seconds, and then takes milliseconds for all subsequent detections. I read about the speed in #32  issue, but I'd like to know how to pre-solve some bottleneck that happens only on first detection.--Is there any way to call some function to prepare before starting detection and avoid this initial delay? Taking into consideration that the user needs to do an action (click a button) to start the face detection.I am already doing the initiated boot load. According to the code.**App Init()**```const MODEL_URL = ""/static/models"",await faceapi.loadSsdMobilenetv1Model(MODEL_URL),await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),await faceapi.loadFaceDetectionModel(MODEL_URL),await faceapi.loadFaceRecognitionModel(MODEL_URL),navigator.mediaDevices  .getUserMedia({ video: { frameRate: { ideal: 10, max: 15 } } })  .then(stream => {    this.cameraPreview.srcObject = stream,    this.cameraPreview.style.display = ""block"",  })  .catch(err => {    alert(""error""),  }),```**Call Detect**```start(){    configProcessFace(),    detectFace(),}configProcessFace() {  this.faceOptions = faceapi.SsdMobilenetv1Options({    minConfidence: 0.8,    maxResults: 1  }),},async detectFace() {  const useTinyModel = false,  const fullFaceDescription = await faceapi    .detectSingleFace(this.canvasPreview, this.faceOptions)    .withFaceLandmarks(useTinyModel)    .withFaceDescriptor(),  if (fullFaceDescription && fullFaceDescription.detection.box) {    .......  }  window.setTimeout(() => {    this.detectFace(),  }, 400),},```";"['It is actually due to all the shader programs being compiled on the first run when using the WebGL backend.> Is there any way to call some function to prepare before starting detection and avoid this initial delay? Yes, you can simply call the prediction chain you are using for an empty image or tensor initially, which will compile everything upfront. Just make sure when using a tensor, that the input size matches the input size of the face detector, for the SSD Mobilenet detector it is 512x512.=====', '@justadudewhohacks  Perfect.  I push a picture with a face, in the proportions 512x512 and did the recognition while loading the application. When the user will recognize it, it takes 1 second.For consult:```prepareFaceDetector() {      let base_image = new Image(),      base_image.src = ""/static/img/startFaceDetect.jpg"",      base_image.onload = function() {        const useTinyModel = true,        const fullFaceDescription = faceapi          .detectSingleFace(base_image, new faceapi.TinyFaceDetectorOptions())          .withFaceLandmarks(useTinyModel)          .withFaceDescriptor()          .run()          .then(res => {            console.log(""--------> "" + JSON.stringify(res)),          }),      },}```=====', ""Hi @luisdemarchi , I am facing the same issue for first detection, takes about 12s for me. Would you mind explaining in simpler terms how you made it faster? My code currently looks something like this:```const faceMatcher = new faceapi.FaceMatcher(FINALVECTORS,0.5)var idVar = setInterval(async () => {\tconsole.log('start capturing intervals')\tconst detections2 = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions({inputSize:128})).withFaceLandmarks().withFaceDescriptor()\tif (detections2) {\t\tconst results = faceMatcher.findBestMatch(detections2.descriptor)\t\tif (labels.includes(results['label'])) {\t\t\tdocument.getElementById('msg').innerHTML=(`Hello, ${results['label']}!`)\t\t\tconsole.log('done')\t\t\tclearInterval(idVar)\t\t\t// setTimeout(window.close, 3000)````the bulk of the time is spent at at faceapi.detectSingleFace, how did you reduce the time a user would spend here?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/305;Undefined when face-api is busy;1;closed;2019-05-24T09:55:10Z;2019-05-27T12:46:56Z;Hi, I don't know if it is really an issue or is my problem.I am trying to use the library, and as it is async, I am sending frames to detect continuously.My question is: What does the library return if you ask it to process another frame before it has finished processing the previous one? Does it return `undefined`? ;['No the api is able to handle that, as far as I know the execution of the shader programs will simply be queued and scheduled to be executed on your GPU. However, I do not recommend to do that extensively and rather schedule prediction calls one after another, but you can experiment with that of course. You will see when performance starts to drop.=====']
https://github.com/justadudewhohacks/face-api.js/issues/304;Descriptors;8;closed;2019-05-24T02:22:41Z;2019-06-05T15:01:12Z;Thanks with your lib.How do you get the descriptor array?I see you have 68 face points. how do you use it?I am very grateful if you can answer.Thank you and your lib.;"['PART 1 - Setting up the source image ( The face you want to compare with )1. Extract the descriptors using chain of methods. Consider it as **source image**.`const sourceImageResult = await this.faceapi.detectSingleFace(imageElement, faceDetectionOptions)                .withFaceLandmarks()                .withFaceDescriptor()`2. Create face matcher for the results of source image:`let faceMatcher = new faceapi.FaceMatcher(sourceImageResult)`3. Draw that image on canvas if you want by resizing and drawing functions from examples.PART 2 - Comparing the target face with source face using descriptors1. Extract the descriptors using chain of methods. Consider it as **Target image**.`const targetImageResult = await this.faceapi.detectSingleFace(imageElement, faceDetectionOptions)                .withFaceLandmarks()                .withFaceDescriptor()` 2. Use the facematcher here with target image descriptors:`let bestmatch = faceMatcher.findBestMatch(targetImageResult.descriptor)`**bestmatch** is a JSON object containing distance (between source and target) and labelNote: While using the methods for detecting face, keep an eye on what you are using ( detectSingleFace OR detectAllFaces ), because **detectSingleFace will give you single object** with result and **detectAllFaces will give you array of object** of result.So if you use **detectAllFaces **, while using faceMatcher.findBestMatch, you can use descriptors using map array function like this `resizeResults.map(det => det.descriptor)[0]` or you can loop on it.=====', 'thanks with  your answer. But, that i want to know is method get descriptors. can you help me ?- very sorry for bothering you=====', '`faceapi.detectSingleFace(imageElement, faceDetectionOptions) .withFaceLandmarks() .withFaceDescriptor()`returns face descriptor object=====', ""- Thank you very much for your answers even though it's not what I needs. maybe it's because I don't show the question clearly.-My question : I want to know how to calculate the final result as an array of descriptor values.====="", ""> * Thank you very much for your answers even though it's not what I needs. maybe it's because I don't show the question clearly.>   -My question : I want to know how to calculate the final result as an array of descriptor values.I think, you are confused, 68 face **landmarks** it's different to 128 Float32Array face descriptor, what do you do? what it is you application?====="", ""> I think, you are confused, 68 face **landmarks** it's different to 128 Float32Array face descriptor, what do you do? what it is you application?- I want to understand how it works.- how to get from a face to an array of descriptions. I don't know how this code executes the sequence:\xa0const resultsRef = await faceapi.detectAllFaces (referenceImage, faceDetectionOptions)\xa0\xa0\xa0\xa0.withFaceLandmarks ()\xa0\xa0\xa0\xa0.withFaceDescriptors ()- I think there must be a way to calculate it to produce that description and that's what I want to know.Thank you for your interest and help me.====="", 'Besides from what @imtiyazs  and @punisher97 already explained to you, `await faceapi.detectAllFaces(img, faceDetectionOptions).withFaceLandmarks() .withFaceDescriptors()` returns an array of prediction, where each element has a property descriptor, which holds the descriptor array.> How do you get the descriptor array?I see you have 68 face points. how do you use it?Or is the question rather, how the descriptors are computed and how they relate to the 68 face landmark points?The face landmarks are computed on the extracted face image obtained based on the bounding boxes predicted by the face detector. Based on the 68 points, the bounding boxes are adjusted to be centered on the face. Then from the aligned face image the face recognition net predicts the face descriptors (under the hood it calls `faceapi.computeFaceDescriptors(alignedImg)`).In this article I explained everything in more detail: https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07=====', 'thank you very much.It really helped me better understand this.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/301;Some questions;2;closed;2019-05-23T05:24:30Z;2019-05-27T08:17:56Z;Hi you. First of all,  thanks with lib of you. I have some questions : - is pretreatment image in lib?  If have, you can help me find it in your code?Thanks again.I hope you answer my question.;['Hi,What do you mean with pretreatment image? Image preprocessing? There is logic in this lib to create a batch tensor from all inputs including resizing and padding of input images, which is implemented [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/dom/NetInput.ts#L121-L153) followed by [normalization](https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceFeatureExtractor/FaceFeatureExtractor.ts#L25-L26).=====', 'Thanks very much.That is the answer I need.thank you again.Vào 14:10, T.5, 23 Th5, 2019 Vincent Mühler <notifications@github.com> đãviết:> Hi,>> What do you mean with pretreatment image? Image preprocessing? There is> logic in this lib to create a batch tensor from all inputs including> resizing and padding of input images, which is implemented here> <https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/dom/NetInput.ts#L121-L153>> followed by normalization> <https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceFeatureExtractor/FaceFeatureExtractor.ts#L25-L26>> .>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/301?email_source=notifications&email_token=AK73CPGKKT5KSGRXZWPASJTPWY7PRA5CNFSM4HO2BBPKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWBJAKQ#issuecomment-495095850>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AK73CPAWEZFWOCRAWL2MI2TPWY7PRANCNFSM4HO2BBPA>> .>=====']
https://github.com/justadudewhohacks/face-api.js/issues/298;To make changes in src which will reflect in face-api js call from nodejs like in running example-nodejs .;3;closed;2019-05-20T13:29:21Z;2019-05-27T08:26:17Z;I would like to make some changes in the src(like simple console.log() statements) and it doesn't reflect in running example-nodejs faceExpressionRecognition.ts file. Please suggest changes I need to done to show my logs in running examples.;['If you are making changes in the source code of face-api.js, then you also have to compile the typescript via tsc.=====', 'Thanks. But I have compiled through npm run-script build in the main directory then only compiled facialExpressionRecognition through tsc.Can you please also share any research involved behind the project to make architectures and on what data it is trained and accuracy on test data etc.=====', 'If you compile the example via tsc, then you have to execute the javascript files using nodejs, but I guess you are already doing so?> Can you please also share any research involved behind the project to make architectures and on what data it is trained and accuracy on test data etc.I am already including this kind of information in the README, for any further models that I am training I will also include benchmark results and test accuracy, as I did for the ageGenderNet in the last release.Other than that, I am closing this issue, as your question is not really related to this project, but rather a basic javascript/typescript question.=====']
https://github.com/justadudewhohacks/face-api.js/issues/297;matchDimensions of responsive video element;1;closed;2019-05-20T10:29:17Z;2019-05-20T12:53:33Z;"The canvas overlay is sadly not resizing correctly if the video element is responsive.```html<div id=""cam"">	<video onloadedmetadata=""onPlay()"" id=""inputvideo"" autoplay muted></video>	<canvas id=""overlay""></canvas></div>``````jsasync function onPlay() {	const videoEl = $('#inputvideo').get(0),	if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())		return setTimeout(() => onPlay())	const options = getFaceDetectorOptions(),	const result = await faceapi.detectSingleFace(videoEl, options),	if (result) {		const canvas = $('#overlay').get(0),		let displaySize = {			width: $('#cam video').width(),			height: $('#cam video').height()		},		let dims = faceapi.matchDimensions(canvas, displaySize),		// const dims = faceapi.matchDimensions(canvas, videoEl, true),		// faceapi.draw.drawDetections(canvas, faceapi.resizeResults(result, dims)),		let box = [			result[""_box""][""_x""],			result[""_box""][""_y""],			result[""_box""][""_width""],			result[""_box""][""_height""]		],		let c = document.getElementById(""overlay""),		let ctx = c.getContext(""2d""),		let currentFilter = document.getElementById('current-filter'),		ctx.drawImage(currentFilter, ...box),		if (debug) {			ctx.rect(...box),			ctx.stroke(),		}	} else {		const canvas = $('#overlay').get(0),		const dims = faceapi.matchDimensions(canvas, videoEl, true),	}	setTimeout(() => onPlay()),}async function run() {	await changeFaceDetector(),	const stream = await navigator.mediaDevices.getUserMedia({ video: {} }),	const videoEl = $('#inputvideo').get(0),	videoEl.srcObject = stream,}async function changeFaceDetector() {	await faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),	if (!isFaceDetectionModelLoaded()) {		await getCurrentFaceDetectionNet().load('/'),	}}function getFaceDetectorOptions() {	return new faceapi.SsdMobilenetv1Options({ minConfidence }),}function getCurrentFaceDetectionNet() {	return faceapi.nets.ssdMobilenetv1,}function isFaceDetectionModelLoaded() {	return !!getCurrentFaceDetectionNet().params}```Project: Place an image over the face of a detected person and upload it to a server.";"[""not the best solution, but I just adjusted the dimensions of the canvas _after_ the draw:```js$('#overlay').height($('#inputvideo').height()),$('#overlay').width($('#inputvideo').width()),```=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/296;MobilenetV2 to tensorlfowjs webmodel;2;closed;2019-05-15T07:16:12Z;2019-05-23T05:57:50Z;Hey!Can you share me your scripts on how you converted the weights provided in https://github.com/yeephycho/tensorflow-face-detection to tfjs. I tried this but hit unsupported ops issue.tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='embeddings' saved_model/ output/ValueError: Unsupported Ops in the model before optimizationQueueDequeueUpToV2, FIFOQueueV2I saw https://github.com/justadudewhohacks/face-api.js/issues/50, but don't konw how to transform it ;"['Hey,Sorry but there is no script. I did it mostly manually. If the tensorflowjs_converter says, that there are unsupported ops, then these ops remain to be implemented by the tfjs team. I am not familar with what these ops are doing ""QueueDequeueUpToV2, FIFOQueueV2"" but in some cases it is possible to skip postprocessing operations at the end of the graph and implement them manually.Maybe you want to try to visualize the graph using tensorboard first, to figure out, where these ops occur in the graph.=====', 'Thank for your answer, I tried restore a new graph and drop these unsupported ops node,It worked=====']"
https://github.com/justadudewhohacks/face-api.js/issues/295;How to improve performance using node.js with openblas;7;closed;2019-05-14T11:51:04Z;2019-05-28T01:32:44Z;"I'm also experiencing slowness problem when using faceRecognition on api-faces. I use windows and my tests are taking around three seconds to identify the face.I already installed openblas on windows, but it looks like faces-api.js does not recognize it.Sorry, you are requested to use face-recognition.js and this repository is saying This package is pretty much obsolete. I recommend you to switch to face-api.js.Note: I'm already using dependencies:""@tensorflow/tfjs-node"": ""^1.0.2""Please help me, I'm loving this api.";"['Does it print any warning when importing tfjs-node? If there is a version mismatch, it might not be able to speed up things via tfjs-node, but using ""@tensorflow/tfjs-node"": ""^1.0.2"" should actually work.Also keep in mind that face-api.js does not use openblas.=====', 'Thank you for the answer, please.See package.json:{\xa0\xa0""name"": ""ussrecognizeface"",\xa0\xa0""version"": ""1.0.0"",\xa0\xa0""description"": ""face identifier"",\xa0\xa0""main"": ""server.js"",\xa0\xa0""scripts"": {\xa0\xa0\xa0\xa0""test"": ""echo \\"" Error: no test specified \\ ""&& exit 1"",\xa0\xa0\xa0\xa0""start"": ""nodemon server.js --ignore * .marko.js""\xa0\xa0},\xa0\xa0""author"": ""Ulisses Silva de Souza"",\xa0\xa0""license"": ""ISC"",\xa0\xa0""dependencies"": {\xa0\xa0\xa0\xa0""@ tensorflow / tfjs-node"": ""1.0.2"",\xa0\xa0\xa0\xa0""canvas"": ""2.0.1"",\xa0\xa0\xa0\xa0""express"": ""^ 4.16.4"",\xa0\xa0\xa0\xa0""face-api.js"": ""^ 0.20.0"",\xa0\xa0\xa0\xa0""marko"": ""^ 4.16.13""\xa0\xa0},\xa0\xa0""devDependencies"": {\xa0\xa0\xa0\xa0""nodemon"": ""^ 1.19.0""\xa0\xa0}}See the log after npm install://////////////\xa0HOME LOG //////////////////////////////////> @ tensorflow / tfjs-node @ 1.0.2 install C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ @tensorflow \\ tfjs-node> node scripts / install.js* Downloading libtensorflow[=============================== 1150733 / bps 100% 0.0s* Building TensorFlow Node.js bindings> canvas@2.0.1 install C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download[canvas] Success: ""C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ canvas \\ build \\ Release \\ canvas-prebuilt.node"" is installed via remote> nodemon@1.19.0 postinstall C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ nodemon> node bin / postinstall || exit 0Love nodemon? You can now support the project via the open collective:\xa0> https://opencollective.com/nodemon/donatenpm WARN ussrecognizeface@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules \\ fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {""os"": ""darwin"", ""arch"": ""any""} (current: {""os"": ""win32"", ""arch"": ""x64""})added 392 packages from 253 contributors and audited 2600 packages in 70.31sfound 0 vulnerabilities////////////// FIM LOG //////////////////////////////////See the log after faceRecognition.js:done, saved results to out/queryImage.jpg=====', 'Can the pc processor be the cause of being slow?My machine:Intel (R) Core (TM) i7-3537U CPU @ 2.00GHz 2.50 GHzMemory: 8GBsystem type: 64-bit=====', ""Hmm it could be, just curious, are you actually importing `import '@tensorflow/tfjs-node',` in your code?Also, do you mean the entire faceRecognition example is running for ~3s? Keep in mind that this example is processing 2 images, detecting all faces on them and for each detected face computing the landmarks and face descriptors, so 3s seems to be reasonable on the CPU.Usually in the browser execution is much faster since ops are run on the GPU via WebGL. If you have an nvidia gpu you could also use the gpu version of tfjs-node.You can also use the tiny face detector instead of the ssd mobilenet detector, which is faster but less accurate.====="", '1 - Yes I am using require (""@ tensorflow / tfjs-node""),see your env.js:""use strict"",exports .__ esModule = true,// import nodejs bindings to native tensorflow,// not required, but will speed things up drastically (python required)require (""@ tensorflow / tfjs-node""),// implements nodejs wrappers for HTMLCanvasElement, HTMLImageElement, ImageDatavar canvas = require (\'canvas\'),exports.canvas = canvas,var faceapi = require (""../../../ face-api.js""),// patch nodejs environment, we need to provide an implementation of// HTMLCanvasElement and HTMLImageElement, additionally an implementation// of ImageData is required, in case you want to use the MTCNNvar Canvas = canvas.Canvas, Image = canvas.Image, ImageData = canvas.ImageData,faceapi.env.monkeyPatch ({Canvas: Canvas, Image: Image, ImageData: ImageData}),2 - Yes it is taking 3 seconds to compare images.3 - As I\'m developing an application that will compare a base image with several others in the repository, then 3 seconds is a lot.But thanks for the help.=====', 'Alright, yes 3s is a lot, thus it is better to get some GPU acceleration for example by using `@tensorflow/tfjs-node` in your nodejs app. But you will need an nvidia GPU.=====', 'I have NVIDIA GeForce GT 740m.However, I spent several hours trying to configure @ tensorflow / tfjs-node-gpu without success.Would you take a step by step?Using gpu I would increase the speed in how much?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/286;React native support;1;closed;2019-05-07T09:23:58Z;2019-05-08T08:14:44Z;Please let me know if there is support for react native and if not what changes can be done to support react native;"[""According to @nkreeger in https://github.com/tensorflow/tfjs/issues/1346:> We don't fully support React Native - it is something we have on our roadmap but we haven't had a chance to figure out how to do this correctly.face-api.js is built on top of tfjs-core, so the environment this library can be used in is mainly determined by where tfjs-core runs as well.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/285;Error with loadSsdMobilenetv1Model;1;closed;2019-05-05T10:34:38Z;2019-05-05T15:52:28Z;I have implemented it on my Angular project and when I try localy everything is ok (detect faces, compare and get the euclidean distance) but when I try it after deployment (ng build) and I start the app, I have the following error: **Error: Uncaught (in promise): RangeError: byte length of Float32Array should be a multiple of 4**![image](https://user-images.githubusercontent.com/50211349/57192536-e5bd9b80-6f31-11e9-8f4f-bdf583162f3b.png)Could anyone help me? I think the problem is not with assets directory because the error is not a 404 not found.Thanks.;"[""Everything it's ok. It was because FileZilla transfer files ASCII by default.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/279;examples for authentication with face id?;3;closed;2019-04-27T22:54:39Z;2019-06-07T10:08:29Z;Is it possible with this library to authenticate a user via facial recognition? If so how would I do that?;"[""Sure is.You'd need to get the descriptors of the people you want to authenticate (via a photo or something) and store that. Then you compare it to the descriptors of the person on the camera video feed and if you're u nder the threshold (0.6 or so is good, from my experience), that's a match.====="", ""Ok cool. Thanks Sent from my iPhone> On Apr 27, 2019, at 3:57 PM, ScottDellinger <notifications@github.com> wrote:> > Sure is.> > You'd need to get the descriptors of the people you want to authenticate (via a photo or something) and store that. Then you compare it to the descriptors of the person on the camera video feed and if you're u nder the threshold (0.6 or so is good, from my experience), that's a match.> > —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub, or mute the thread.====="", 'Anyone willing to set this up for a fee?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/265;What is best way to compare 2 faces? [Question];2;closed;2019-04-05T16:21:21Z;2019-04-08T20:34:10Z;Hello.I'm very noob in topic about face recognition and only second day playing with it.I read acticles from author of these libraries 'Vincent Mühler' face-recognition.js and face-api.js tutorials on https://medium.com.So I tried to compare 2 faces using opencv4nodejs and face-api.js in nodejs back-end.But using `face_api.computeFaceDescriptor(reference_image),` is very slow.All I need is compare only 2 faces. There is no need for training identity in my use case.Please advice me what is best way to compare 2 faces only in nodejs back-end?Thank you.;"['You are probably not using tfjs-node, which speeds things up a lot.=====', ""You were right, I wasn't using tfjs-node. Because it wasn't working in my current machine MacOs 10.11 El Capitan. There is some issue with __clock_time in tfjs-node compiled binary. But I tried to install on linux machine and CPU backend it is so much improved. Thank you.`Compute face descriptor performance: 188ms`=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/262;[electron] produces different results compared to web;6;closed;2019-04-01T09:25:10Z;2019-04-10T05:58:43Z;;['???=====', '**Info:**Loading face-api.js in a renderer process of my electron app.face-api.js version 0.19.1Using face-api.js with electron![1](https://user-images.githubusercontent.com/13446367/55370581-7c5efd00-552d-11e9-938a-c2f56e7f4eb4.jpeg)![2](https://user-images.githubusercontent.com/13446367/55370617-98fb3500-552d-11e9-9464-54c5832808a1.jpeg)**electron app：**await faceapi.detectSingleFace(videoEl, options) === undefined**WEB**![3](https://user-images.githubusercontent.com/13446367/55370727-0d35d880-552e-11e9-80e8-975b8c32a34f.jpg)**electron app**![4](https://user-images.githubusercontent.com/13446367/55370776-440bee80-552e-11e9-8b99-3e65cff92ade.jpg)=====', '@justadudewhohacks =====', '![1554278009373](https://user-images.githubusercontent.com/13446367/55462260-e6a89800-5628-11e9-90d8-bb6621a8bf89.jpg)=====', 'Which backends are you using on the web and electron example, webgl or cpu?=====', '@justadudewhohacks Thanks for you，i replaced electron with nw.js, nw.js is ok.=====']
https://github.com/justadudewhohacks/face-api.js/issues/257;errors【Based on the provided shape the tensor should have 512 values but has 482】after built;2;closed;2019-03-27T10:54:29Z;2020-07-09T10:09:30Z;hey，I hava a problem.the code is running successfully in my development environment, but when I built, it's show a error as 【Based on the provided shape the tensor should have 512 values but has 482】the number is changed value when I turn to the other modelI'm sure the resources is loaded 200.but the error is also exist. And I don't use Vue , just the es6;['@nannan9507  Did you find the solution for this ?=====', 'Make sure you clear your browser cache.=====']
https://github.com/justadudewhohacks/face-api.js/issues/255;version of @tensorflow/tfjs-node;1;closed;2019-03-27T10:03:55Z;2019-03-27T10:10:11Z;"when i use newest version of  @tensorflow/tfjs-node, ""faceapi.detectSingleFace()"" will become like 100 times slower than using v0.2.3";['duplicate of #254 =====']
https://github.com/justadudewhohacks/face-api.js/issues/254;Update to work with latest @tensorflow/tfjs-node;3;closed;2019-03-26T14:49:09Z;2020-12-05T03:42:44Z;When testing using the latest @tensorflow/tfjs-node it does not load correctly and you end up with this error:`============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================`Processing images then takes a good 10 secondsI think face-api.js is only compatible with older versions of @tensorflow/tfjs-node as it references ^0.2.3. After downgrading to this version my image processing time drops down to 0.4 seconds per image. Let me know if there is a way to run the latest @tensorflow/tfjs-node with face-api.jsThanks;"[""> I think face-api.js is only compatible with older versions of @tensorflow/tfjs-node as it references ^0.2.3. That's correct, face-api.js currently uses tfjs-core version 0.14.2, thus you need to install a compatible tfjs-node version.PR for upgrading to latest tfjs-core version is already up #246. Will upgrade as soon as possible.====="", 'Thanks for the prompt reply, just wanted to raise as when trying to run the code initially in NodeJS I hit this issue and it was unclear how to resolve it.Look forward to the update for the tfjs-core.Thanks=====', 'What are the best versions of face-api, tfjs-core and tfjs-node to run together?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/251;need help;3;closed;2019-03-24T06:25:04Z;2019-05-08T08:24:10Z;i need help with how to run the code localy in my computer like if it is possible step by step ;['Think you are looking for this: [https://github.com/justadudewhohacks/face-api.js/tree/master/examples](https://github.com/justadudewhohacks/face-api.js/tree/master/examples)There should be enough information in there to get it all up and running. It is all in typescript so you need to make sure that you have that installed first.=====', '@lemidiriba you can follow the steps [here](https://github.com/justadudewhohacks/face-api.js/blob/master/README.md#running-the-examples).=====', 'tanks for you support.On Fri, Mar 29, 2019 at 2:24 PM Cihat SENGUN <notifications@github.com>wrote:> @lemidiriba <https://github.com/lemidiriba> you can follow the steps here> <https://github.com/justadudewhohacks/face-api.js/blob/master/README.md>.>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/251#issuecomment-477963936>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AlqLFuwrCZvMIeY3UsruQLm7E600Trnyks5vbfgEgaJpZM4cFVYk>> .>=====']
https://github.com/justadudewhohacks/face-api.js/issues/244;weights missing for FaceLandmark68Net;7;closed;2019-03-12T22:02:56Z;2019-03-18T21:40:25Z;Should there be a manifest.json file and a weight file for FaceLandmark68Net in the weights directory?When I try to use Mtcnn, it seems to want that model to have been loaded.;"['> To load a model, you have provide the corresponding manifest.json file as well as the model weight files (shards) as assets. =====', ""If you only use the mtcnn then you don't need that model.====="", ""I am only using the mtcnn model. I have this in my code and I verified that it is getting executed:```jsfaceApi.loadMtcnnModel('/weights'),```My weights directory contains the files mtcnn_model-shard1, mtcnn_model-weights_manifest.json, and many other shard and manifest files. But I get this error:```FaceProcessor.js:35 Uncaught (in promise) Error: FaceLandmark68Net - load model before inference    at FaceLandmark68Net.FaceProcessor.runNet (FaceProcessor.js:35)```It's as if the mtcnn model depends on another model. Is that possible?====="", ""Nope that's not possible. You are apparently calling the landmark prediction API, as the error suggests. What does the rest of your code look like?====="", 'I see what is causing the error now.```js    const faces = await faceApi.detectAllFaces(video, options)      .withFaceLandmarks()      .withFaceDescriptors(),```If I remove the calls to `withFaceLandmarks` and `withFaceDescriptors`, the error goes away.Where can I get the model files that are required to use those methods? I have files that begin with `face_landmark_68_model-` and `face_landmark_68_tiny_model_`, but it seems those are not the ones I need.=====', ""face_landmark_68_model for face landmark prediction and face_recognition_model for computing face descriptors. The reason why you still get the error is, because you are not loading them:``` javascriptawait faceapi.loadFaceLandmarkModel('/weights')await faceapi.loadFaceRecognitionModel('/weights')```====="", 'Thanks!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/243;How to train and have my own model;4;closed;2019-03-11T23:20:34Z;2019-07-10T03:45:59Z;Hi, your lib seen really good, however I feel the need for a tutorial on how to use it for custom face recognition.I still cannot figure it out how to train a model with my own images;"[""You don't train a new model. Check out the introduction [article](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07).====="", 'Hi, can you please advise how did you convert dlib face landmarks model into tensorflow format? =====', '@alex77t I did not do so. I trained a face landmark detector from scratch, as stated in the README.=====', 'how to train model to detection my face?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/241;"the  ""BBT Face Matching"" functions accuracy is very low & slow in ""example-browser"" when using my own photos.";2;closed;2019-03-09T01:15:09Z;2019-03-20T11:05:52Z;"I have no idea why the accuracy is very low(10%), what I did as below.I have created 2 new folders under example/images and edited 5 png photos for each folder(one is mine), also change ""classes"" in the file of example-browser/public/js/bbt.js to be ""const classes = ['asuix','asui']"", when I refresh the running code, the accuracy is very low.Would you please give some advises?Thanks & Best RegardsSui";"['You probably forgot to perform face alignment. If you are referring to the bbt face matching example: this example expects the face images to be aligned already.=====', ""@justadudewhohacksThanks for your response.It's my fault, now the matching accuracy has been risen to be 95% and more.Thanks again.Sui=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/240;Error: SsdMobilenetv1 - load model before inference;1;closed;2019-03-08T15:44:35Z;2019-03-08T15:50:27Z;"Cant load the tiny face detector, always gives this error:Error: SsdMobilenetv1 - load model before inferencethis is my code:`import * as api from 'face-api.js',class Detector {    private canvas : any,    public async run() : Promise<any> {        const detections = await api.detectSingleFace(this.canvas),    }    /**     *     * @returns {Promise<any>}     */    public async load() : Promise<any> {        this.loadCanvas(),        await this.loadModel(),    }    /**     *     * @returns {Promise<any>}     */    private async loadModel() : Promise<any>{        await api.loadTinyFaceDetectorModel('/models/tiny_face_detector'),    }    /**     *     */    private loadCanvas() : void{        this.canvas = document.getElementById('img'),        if(!this.canvas)console.warn(""canvas not found""),    }}document.addEventListener(""DOMContentLoaded"", async event => {    const detector = new Detector(),    await detector.load(),    await detector.run(),}),`";['im sorry, i should add the tinymodel options as argument this is mentioned in de docs.=====']
https://github.com/justadudewhohacks/face-api.js/issues/239;"How to add my own face picture into the API of ""BBT Face Matching""?";1;closed;2019-03-07T11:23:36Z;2019-03-07T13:19:12Z;"I am trying to put my face picture into ""BBT Face Matching"", but I have no idea how.I have read the code of bbtFaceMatching.html and confused about ""await faceapi.loadFaceRecognitionModel('/')"", is it mean I have to training a new model of my face picture first?Any one can help? I am new guy at image recognize field.Thanks & Best RegardsSui ";['Ooops, I have figured the reason out, it has to make 5 face pictures under a folder with the name into images/ and the demo code will use ***1.png fetch first image of each class and compute their descriptors.So far the function so good.Thanks=====']
https://github.com/justadudewhohacks/face-api.js/issues/238;Is there anyway to extract all faces from Webcam?;3;closed;2019-03-06T08:51:25Z;2019-03-12T08:48:00Z;"I am trying to extract all faces from webcam, but there is no API to do so, although ""face extraction"" API able to extract faces from a picture but not on a video?I'm not sure that is a good idea if extracting faces from Webcam each frame by ""face extraction"" API.So, any commentsThanks & Best RegardsSui";"['Hi,> I\'m not sure that is a good idea if extracting faces from Webcam each frame by ""face extraction"" API.This is exactly the approach I would choose. Of course, you are not going to analyze every frame. One per second could be way enough depending on what you are planning to do.You could take a look at the code on [this issue](https://github.com/justadudewhohacks/face-api.js/issues/203).Then again, it really depends on what you want to do. Above code was designed for a back-end only environment. If you want to perform face extraction on a browser, you can just take a look at the corresponding example in this repo.Regards=====', ""Hi, @erenaud3 It's really helpful, I will try your advise.BYW,  the demo fps of Webcam face detection on my laptop able to in the range of 20-40, so I believe it can be get a well performance on the web browser.Again, thanks for your help and keep in touch.Thanks & Best Regards====="", '@erenaud3 Hello again, I have met some issue for the project of face-api.js.So would you please add my whatsapp/wechat/qq to your friend list for easing commutations.I am looking forward to get your feedback.Thanks & Best RegardsSui=====']"
https://github.com/justadudewhohacks/face-api.js/issues/236;Face Detector Dataset;1;closed;2019-03-05T04:50:58Z;2019-03-26T03:32:46Z;Hi, thanks for the great library. May I know which dataset you used to train the face detector?Prasad;['It is a wild mixture of publicly available databases, to be honest I lost track of which. I used Face Detection Data set and Benchmark (FDDB) for testing though.=====']
https://github.com/justadudewhohacks/face-api.js/issues/230;implementation on Raspberry pi;4;closed;2019-02-28T05:59:58Z;2021-09-27T11:58:14Z;can we use Faceapi.js on raspberry pi 3;"[""If tfjs / tfjs-node is running on raspberry pi than face-api.js is too.  You would have to try it out, I can't say for sure.====="", 'Hi! Did you manage to do it? Or any more info on this? Having a super hard time making my program work on Chromium for my RPI :(=====', 'HiI know this is an old issue, but I want to share my findings. I have recently after a long battle managed to get face-api running on a Raspberry PI, it comes down to getting tfjs-node running. You can have a look at my blog post: **[https://technodezi.co.za/Post/running-face-apijs-or-tfjs-node-on-a-raspberry-pi-and-nodejs](https://technodezi.co.za/Post/running-face-apijs-or-tfjs-node-on-a-raspberry-pi-and-nodejs)**I opted to rather run it from a node application as directly inside chromium was way too slow. Tensorflow on raspberry pi is still slow, but better.If you still struggle I will be more than willing to assist.=====', '> Hi> > I know this is an old issue, but I want to share my findings. I have recently after a long battle managed to get face-api running on a Raspberry PI, it comes down to getting tfjs-node running. You can have a look at my blog post: **https://technodezi.co.za/Post/running-face-apijs-or-tfjs-node-on-a-raspberry-pi-and-nodejs**> > I opted to rather run it from a node application as directly inside chromium was way too slow. Tensorflow on raspberry pi is still slow, but better.> > If you still struggle I will be more than willing to assist.Hello @TechnoDezi, I think your post is down, do you have it somewhere else? I am trying to run face-api with chromium on ubuntu for rpi4 but its not working because of WebGL =====']"
https://github.com/justadudewhohacks/face-api.js/issues/229;Datasets;1;closed;2019-02-28T02:46:28Z;2019-03-27T14:07:11Z;I am trying to test out my new face detector which doesn't use yolo but automatically regresses the bounding boxes for single face detector. We have been using a youtube dataset but the aspect ratio doesn't match the youtube dataset so when resize our images, the face gets distorted. I was wondering if you could point to some good datasets to use which have resolutions that match the webcam.;['Your face detector should be independent from the aspect ratio. I would preprocess your images such that they are squared before training your network. You can scale down your images and pad the minor dimension with zeros. Optionally you can also center the image.=====']
https://github.com/justadudewhohacks/face-api.js/issues/228;Trying to get this to work in Angular;8;closed;2019-02-27T23:19:34Z;2019-03-27T14:07:06Z;"I have been trying to get this to work in an Angular component, without success, and would love a bit of help/direction. Await dies on the vine and I'm stumped as to what to do next beyond throwing more darts at the wall. Is there a debug flag somewhere?**[package.json]**""face-api.js"": ""^0.18.0"",**[component.html]**< div style=""position: relative"" class=""margin"">  < video id=""videoEl"" autoplay muted>< /video>  < canvas id=""overlay"">< /canvas>< /div>**[component.ts]**import { Component, OnInit } from '@angular/core',import * as $ from 'jquery',// implements nodejs wrappers for HTMLCanvasElement, HTMLImageElement, ImageDataimport * as canvas from 'canvas',import * as faceapi from 'face-api.js',import { CPU_ENVS } from '@tensorflow/tfjs-core/dist/test_util',// patch nodejs environment, we need to provide an implementation of// HTMLCanvasElement and HTMLImageElement, additionally an implementation// of ImageData is required, in case you want to use the MTCNNconst { Canvas, Image, ImageData } = canvas,faceapi.env.monkeyPatch({ Canvas, Image, ImageData }),const TINY_FACE_DETECTOR = 'tiny_face_detector',@Component({  selector: 'app-facetrack',  templateUrl: './facetrack.component.html',  styleUrls: ['./facetrack.component.css']})export class FacetrackComponent implements OnInit {  // tiny_face_detector options  inputSize = 512,  scoreThreshold = 0.5,  withFaceLandmarks = false,  withBoxes = true,  videoEl,  stream,  constructor(  ) {  }  ngOnInit() {    this.loadModels(),  }  // ======================================================================  // BEGIN: startTracking  // ======================================================================  async startTracking() {    let cnvs,    const vEl = <HTMLCanvasElement>document.getElementById('videoEl'),    const vElwidth = vEl.width,    const vElheight = vEl.height,    console.log('start tracking...'),    const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 320, scoreThreshold: 0.5 }),    const ts = Date.now(),    setTimeout(function () {      console.log('this.videoEl = ' + vEl),      // const detections = faceapi.detectSingleFace(vEl).withFaceLandmarks(),      const detections = faceapi.detectSingleFace(vEl, options),      console.log('detections'),      console.log(detections),      // resize the detected boxes in case your displayed image has a different size then the original      const detectionsForSize = faceapi.resizeResults(detections, { width: vElwidth, height: vElheight }),      console.log('detectionsForSize'),      console.log(detectionsForSize),      // draw them into a canvas      cnvs = document.getElementById('overlay'),      // this.canvas.getContext('2d'),      canvas.width = vEl.width,      canvas.height = vEl.height,      faceapi.drawDetection(cnvs, detectionsForSize, { withScore: true }),    }, 5000),  }  // ======================================================================  // END: startTracking  // ======================================================================  // ======================================================================  // BEGIN: cameraConnect  // ======================================================================  async cameraConnect() {    // try to access users webcam and stream the images    // to the video element    this.stream = await navigator.mediaDevices.getUserMedia({ video: {} }),    this.videoEl = $('#videoEl').get(0),    this.videoEl.srcObject = this.stream,    if (this.videoEl.srcObject.active === true) {      this.startTracking(),    } else {      console.log('Camera not connected...'),    }  }  // ======================================================================  // END: cameraConnect  // ======================================================================  // ======================================================================  // BEGIN: loadModels  // ======================================================================  async loadModels() {    // load the models    await faceapi.loadTinyFaceDetectorModel('./assets/weights/'),    await faceapi.loadFaceRecognitionModel('./assets/weights/'),    this.cameraConnect(),  }  // ======================================================================  // END: loadModels  // ======================================================================}";"['The output for ""console.log(detections),"" is: DetectSingleFaceTask {input: video#videoEl, options: TinyFaceDetectorOptions}input: video#videoEloptions: TinyFaceDetectorOptions {_name: ""TinyFaceDetectorOptions"", _inputSize: 320, _scoreThreshold: 0.5}__proto__: DetectFacesTaskBase=====', 'The output for ""console.log(detectionsForSize),"" is:DetectSingleFaceTask {input: video#videoEl, options: TinyFaceDetectorOptions}input: video#videoEloptions: TinyFaceDetectorOptions {_name: ""TinyFaceDetectorOptions"", _inputSize: 320, _scoreThreshold: 0.5}__proto__: DetectFacesTaskBase=====', 'You forgot the await keyword: `const detections = await faceapi.detectSingleFace(vEl, options)`, detectSingleFace returns a Promise. Therefore your function also has to be async.=====', 'When I use ""await faceapi.detectSingleFace(vEl, options)"" nothing happens. Is there a debug flag or a way to get some console output?=====', ""I had the same problem but instead of giving to detectSingleFace the video to eat I gave it a canvas I in wich I draw the video's frames. Seems to be working. I would yet be interested to know why feeding the video to the function doesn't work in this context.====="", '@Getsuren l have the same problem, but l open a new tab: Live Demos page, my app  seems to be working. =====', 'I think you are facing the same issue as #129. I added a comment how to fix this. I still have to fix the example code.=====', 'Closing here. If the issue still persists after following the advice in my comment above, feel free to reopen.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/224;Build without tfjs;4;closed;2019-02-24T20:45:08Z;2019-03-01T11:56:44Z;I use faceapi on a web page that already imports tfjs using their CDN.Is there a build of faceapi that doesn’t include tfjs so that it’ll be smaller and no warning about tfjs having been included twice?;"['You mean a bundled face-api.js script? No there is no such build. There would also probably be conflicts regarding the tfjs-core version, if there is a mismatch.=====', 'Right, something like Google does when you use their CDN bundles.For example, when I want to use body-pix, I simply have to do:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix""></script>```It\'d be nice to be able to add face-api.js below in order to use body-pix and face-api on the same webapp.Laurent=====', ""Why are you pulling all the scripts from CDNs and don't bundle tfjs, body-pix and face-api.js together?====="", 'Because it’s easy to try things in pure JavaScript without having to setup a dev environment that needs TypeScript.But I should try.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/217;Installation of @tensorflowtfjs-node is not possible;2;closed;2019-02-16T03:28:28Z;2019-03-23T11:14:11Z;This is when I installed it.I can't solve it.What shouldI do?E:\jslife_WX\wxjslife_rebuild_face>npm install> @tensorflow/tfjs-node@0.2.3 install E:\jslife_WX\wxjslife_rebuild_face\node_modules\@tensorflow\tfjs-node> node scripts/install.js* Downloading libtensorflowevents.js:173      throw er, // Unhandled 'error' event      ^Error: connect ETIMEDOUT 172.217.160.80:443    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1083:14)Emitted 'error' event at:    at TLSSocket.socketErrorListener (_http_client.js:397:9)    at TLSSocket.emit (events.js:197:13)    at emitErrorNT (internal/streams/destroy.js:82:8)    at emitErrorAndCloseNT (internal/streams/destroy.js:50:3)    at processTicksAndRejections (internal/process/next_tick.js:76:17)npm WARN less-loader@4.1.0 requires a peer of webpack@^2.0.0 || ^3.0.0 || ^4.0.0 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-data@0.1.7 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-layers@0.9.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed.You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-converter@0.7.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@0.2.3 install: `node scripts/install.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node@0.2.3 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.;"['network error, Try using vpn=====', ""Closing because it's not related to face-api.js.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/216;Use more than one file to train face recognition;2;closed;2019-02-15T16:02:18Z;2019-03-21T13:48:31Z;I am working to create a node js server to perform facerecognition, using face-api.js.I have two questions regarding the architecture of such system.#### 1. Use more than one file to trainFirstly, I would like to train the face recognition server withreference faces stored on several input files (let's call them trainimages). There is a single face on each train image.  F.ex.:    train_images/Hannah.jpg    train_images/John.jpg    train_images/Tom.jpg    train_images/Jessie.jpg    ..Is there already a way in face-api.js to load a batch of train imagesand store the resulting features in a single object?#### 2. Use more than one train image for each personSecondly, I think that the system would be more robust if severalimages of the same persons are used to train the face features. F.ex.:    train_images/Hannah_0001.jpg    train_images/Hannah_0002.jpg    train_images/Hannah_0003.jpg    train_images/Tom_0001.jpg    train_images/Tom_0002.jpg    ..So I'd like to use more than one picture for each person fortraining. Maybe this can be achieved by merging data from more thanone picture of the same person. Is this possible with face-api.js?;"['Yes you can use multiple descriptors per label for face recognition by using the average distance of all descriptors for example. What you are probably looking for is the [FaceMatcher](https://github.com/justadudewhohacks/face-api.js#face-recognition-by-matching-descriptors) helper class.=====', ""I finally tried this out and definitely FaceMatcher was what I needed.The following example (taken from README, I somewhat missed it) describes exactly how to solve my problem:    const labeledDescriptors = [      new faceapi.LabeledFaceDescriptors(        'obama',        [descriptorObama1, descriptorObama2]      ),      new faceapi.LabeledFaceDescriptors(        'trump',        [descriptorTrump]      )    ]    const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors)\tThank you.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/214;Question: Weights & detected descriptors;2;closed;2019-02-14T13:23:11Z;2019-02-15T11:05:23Z;I'm pretty new to the face-recognition & neural network universe. I started with face-recognition.js for my first test project because the api looked simple to me. It also worked very well for me until I noticed the obsolete-message.I've two questions:1) What are weight models? Is it an manifest which gives the neural nets the necessary information for face recognition / landmarks / etc or does it already contain some information of persons? Like the one of the bbt people. If it contains people information, how do I train my own weights? (Which probably would answer my second question)2) I've a database with images of people. I don't want to detect the faces every run from skratch. How do I save the results? In face-recognition.js I just could write `recognizer.serialize()` and save it to a file / database.How can I achieve this with face-api.js?Sorry for the dumb questions - as a beginner in this topic I'm totally confused right now.None the less - thanks for this awesome library!EDIT:to 2) The detection and recognization is already working. It's just that I don't know how I can cache / save the parsed descriptors for a later use. I just wanna compare my known images with the uploaded photo.;"[""Hi,> 1. What are weight models? Is it an manifest which gives the neural nets the necessary information for face recognition / landmarks / etc or does it already contain some information of persons? Like the one of the bbt people. If it contains people information, how do I train my own weights? (Which probably would answer my second question)Think of a neural network as a very complex function with a lot of parameters. Model weights correspond to these parameters and since there are usually are a ton of them, we store them in a binary format and the value of every parameter is figured out during the learning process. They are learned by giving the neural net an input, forwarding it through the network and afterwards telling the network how wrong the predicted output is from what the real output should be (loss). Based on the loss value, all model weights in the network are updated in a process called backpropagation.> 2\\. I've a database with images of people. I don't want to detect the faces every run from skratch. How do I save the results? In face-recognition.js I just could write `recognizer.serialize()` and save it to a file / database.>     How can I achieve this with face-api.js?All that you have done in face-recognition.js with `recognizer.serialize()` saving the face descriptors. You would simply store the descriptors of each person in a database along with the persons name or something. The descriptors are just arrays with 128 entries.====="", ""Thanks for your answer! That helped me alot.I was thinking about saving the Float32Array from the descriptors but I wasn't sure if it's the correct way to go.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/209;Slow at the first Check;3;closed;2019-02-01T17:41:54Z;2019-02-01T18:48:17Z;fullFaceDescriptions = await faceapi                                                    .detectSingleFace(videoEl, new faceapi.SsdMobilenetv1Options({ inputSize:256 }))                                                    .withFaceLandmarks()                                                    .withFaceDescriptor()desc=fullFaceDescriptions.descriptor1st time when the above code is executed, it takes around 3-4 seconds. Successive execution takes around half a second. Why is that so?;"['yeah because it loads the models first & then it works fine.=====', ""> yeah because it loads the models first & then it works fine.I have include these lines before that codeawait faceapi.loadSsdMobilenetv1Model('/src/store/models/')await faceapi.loadFaceLandmarkTinyModel('/src/store/models/')await faceapi.loadFaceLandmarkModel('/src/store/models/')await faceapi.loadFaceRecognitionModel('/src/store/models/')====="", 'Basically answered this in #160:> The reason why the initial call takes longer, is due to the shaders being compiled, which is referred to as ""warmup time"".=====']"
https://github.com/justadudewhohacks/face-api.js/issues/208;Unable to run node facedetection.js;1;closed;2019-02-01T11:21:36Z;2019-03-23T11:17:32Z;"Hi,Using typescript had executed the command tsc faceDetection.tsfaceDetection.ts(5,7): error TS1005: ',' expected.faceDetection.ts(7,9): error TS1005: ',' expected.faceDetection.ts(9,21): error TS1005: ',' expected.faceDetection.ts(9,27): error TS1005: ',' expected.faceDetection.ts(9,37): error TS1005: '=' expected.faceDetection.ts(10,28): error TS1005: ',' expected.faceDetection.ts(10,35): error TS1005: ',' expected.faceDetection.ts(10,50): error TS1005: '=' expected.faceDetection.ts(12,50): error TS1005: ',' expected.faceDetection.ts(12,53): error TS1005: '=' expected.While executing the second command ""node facedetection.js"" getting the below error. Need your help to resolve the error inorder to check how it runs.node facedetection.jsinternal/modules/cjs/loader.js:611    throw err,    ^Error: Cannot find module './commons'    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:609:15)    at Function.Module._load (internal/modules/cjs/loader.js:535:25)    at Module.require (internal/modules/cjs/loader.js:663:17)    at require (internal/modules/cjs/helpers.js:20:18)    at Object.<anonymous> (root path\FaceAPI\face-api.js-master\face-api.js-master\examples\examples-nodejs\facedetection.js:1:79)    at Module._compile (internal/modules/cjs/loader.js:734:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:745:10)    at Module.load (internal/modules/cjs/loader.js:626:32)    at tryModuleLoad (internal/modules/cjs/loader.js:566:12)    at Function.Module._load (internal/modules/cjs/loader.js:558:3)";"[""What's the typescript version you are using? Should work with a recent version. You can also run the examples directly with ts-node.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/207;Wanting to add angle detection;4;closed;2019-01-31T21:56:53Z;2020-09-19T09:37:43Z;Hi,I wanted to add angle detection and was possibly hoping to contribute this back to faceapi. I was hoping to see what architecture, couldn't find in the js code, which you have for the 68 keypoint detector that outputs the keypoints.  I wanted to have a net that gives you the landmarks and the angles. If you could also providethe dataset, that would be awesome too.Thanks,Rohan;"[""It is possible to estimate head pose from face landmarks, but it's not implemented yet. The issue for tracking this features is here: #107.====="", 'I have been playing around with different architectures, mobilenet,  and you seem to get better results when you train from image to predict angle instead of going from 2d keypoints to angles. This is why I was hoping to to understand what architecture you were using, so I can validate this.=====', 'Ahh sorry. The architecture I used for the face landmark detection net is basically made up by densely connected blocks, which utilize depthwise separable convolutions instead of regular ones. You can find the code for that in the forwardInput method [here](https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceFeatureExtractor/FaceFeatureExtractor.ts#L24-L34) and the corresponding dense block implementation is [here](https://github.com/justadudewhohacks/face-api.js/blob/master/src/faceFeatureExtractor/denseBlock.ts#L30-L55). Basically the FaceFeatureExtractor returns bottleneck features, which you can use and stack your own classifier on top. The face landmark net simply has a fully connected layer stacked on top of the feature extractor, which outputs a vector with entries corresponding to 68 points with x and y coordinates.=====', 'Is it possible to get difference between the face detected is from a image/video or is real human face?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/206;Face-api js Error in faceFeatureExtractor while building in angular;4;closed;2019-01-31T09:51:04Z;2019-04-12T19:43:29Z;"Added face-api.js via npm module in my angular application.Added  **import * as faceapi from 'face-api.js'** line to component and did ng build. Below is the error which I am getting`ERROR in node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(11,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(11,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(11,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(11,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(11,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(13,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(13,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(13,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(17,1): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(11,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(11,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(11,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(11,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(11,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(13,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(13,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(13,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(17,1): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(17,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(17,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(17,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(17,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(17,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19,71): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(23,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(23,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(23,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(27,1): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(12,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(12,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(12,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(12,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(12,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(14,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(14,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(14,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(18,1): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(19,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(19,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(19,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(19,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(19,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(21,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(21,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(21,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(25,1): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(20,24): error TS1110: Type expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(20,31): error TS1003: Identifier expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(20,83): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(20,84): error TS1131: Property or signature expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(20,96): error TS1005: '=' expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(22,5): error TS1128: Declaration or statement expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(22,36): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(22,51): error TS1005: ',' expected.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(26,1): error TS1128: Declaration or statement expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(26,24): error TS1110: Type expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(26,31): error TS1003: Identifier expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(26,43): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(26,44): error TS1131: Property or signature expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(26,56): error TS1005: '=' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(28,5): error TS1128: Declaration or statement expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(28,36): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(28,51): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(32,5): error TS1128: Declaration or statement expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(32,40): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(32,74): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(32,103): error TS1109: Expression expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(32,112): error TS1005: ',' expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(41,5): error TS1128: Declaration or statement expected.node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/TinyYolov2.d.ts(42,1): error TS1128: Declaration or statement expected.`**Below are my Package.Json dependencies** ""dependencies"": {    ""@angular/animations"": ""^5.2.0"",    ""@angular/cdk"": ""^5.2.0"",    ""@angular/common"": ""^5.2.0"",    ""@angular/compiler"": ""^5.2.0"",    ""@angular/core"": ""^5.2.0"",    ""@angular/forms"": ""^5.2.0"",    ""@angular/http"": ""^5.2.0"",    ""@angular/platform-browser"": ""^5.2.0"",    ""@angular/platform-browser-dynamic"": ""^5.2.0"",    ""@angular/platform-server"": ""^5.2.0"",    ""@angular/router"": ""^5.2.0"",    ""@nguniversal/module-map-ngfactory-loader"": ""^5.0.0-beta.5"",    ""@types/chrome"": ""0.0.77"",    ""angular-styled-components"": ""0.0.17"",    ""angular2-datetimepicker"": ""^1.1.1"",    ""aspnet-prerendering"": ""^3.0.1"",    ""bootstrap"": ""^3.3.7"",    ""browser-detect"": ""^0.2.28"",    ""bulma"": ""^0.7.2"",    ""core-js"": ""^2.4.1"",    ""face-api.js"": ""^0.18.0"",    ""json-server"": ""^0.14.0"",    ""logrocket"": ""^0.6.17"",    ""moment"": ""^2.22.2"",    ""moment-timezone"": ""^0.5.23"",    ""ng-pick-datetime"": ""^5.2.6"",    ""ng-pick-datetime-moment"": ""^1.0.5"",    ""ngx-moment"": ""^3.3.0"",    ""ngx-spinner"": ""^2.0.0"",    ""rxjs"": ""^5.5.6"",    ""zone.js"": ""^0.8.19""  }";"['Hmm something obviously went wrong, try cleaning your node_modules or atleast face-api.js and tfjs-image-recognition-base and try to reinstall face-api.js.=====', 'Thanks for quick reply. The issue is fixed when I update ts-lint, typescript, ts-node packages. Closing issue=====', 'Could you share which versions you used? I am also running into the exact same problem.Typescript: 3.4.3Angular: 5.2.11Ionic: 4.18.0Node: 8.11.2My tsconfig.json looks like this:````{  ""compilerOptions"": {    ""allowSyntheticDefaultImports"": true,    ""declaration"": false,    ""emitDecoratorMetadata"": true,    ""experimentalDecorators"": true,    ""lib"": [      ""dom"",      ""es2015""    ],    ""module"": ""es2015"",    ""moduleResolution"": ""node"",    ""sourceMap"": true,    ""target"": ""es5""  },  ""include"": [    ""src/**/*.ts""  ],  ""exclude"": [    ""node_modules"",    ""src/**/*.spec.ts"",    ""src/**/__tests__/*.ts""  ],  ""compileOnSave"": false,  ""atom"": {    ""rewriteTsconfig"": false  }}=====', 'I fixed this. Upgrading the globally installed version of Typescript was not enough because Ionic defaults back to a locally installed one. Updating this one (e.g. `npm install typescript@3.4.3 --save-dev` solved the issue for me.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/205;Memory leaks with detectAllFaces;6;closed;2019-01-30T11:04:10Z;2019-09-05T00:12:41Z;"Hi,I have this coffeescript code snippet :```# Analyse the new frame    analyseFrame: (next = _.noop) ->            @testCount = 200 unless (typeof(@testCount) is 'number')            # Skip if not capturing        return unless Service.isCapturing            # get frame        _frame = Service.videoCapture.getFrame()            # get frame date        @currentFrameTime = Date.now()            # clear old faces in history        @refreshFaceHistory(@currentFrameTime)            #convert frame to a tensor        try            _data = new Uint8Array(_frame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)            _tensorFrame = tfjs.tensor3d(_data, [_frame.rows, _frame.cols, 3])        catch _err            @log.error ""Error instantiating tensor !!!""            @log.error _err.message        faceapi.detectAllFaces(_tensorFrame, @faceDetectionOptions).then (_detectedFaces) =>            @log.debug _detectedFaces                    # fill face history with detceted faces            _detectedFaces = @fillFacesHistory(_detectedFaces)                    # draw boxes on image            Service.videoCapture.drawFaceBoxes(_frame, _detectedFaces)                    # Get partial time            Service.frameDuration = Date.now() - @currentFrameTime                    # write latency on image            Service.videoCapture.writeLatency(_frame, Service.frameDuration)                    # show image            Service.faceRecoUtils.showImage(_frame)                    # Call next            _delayNextFrame = Math.max(0, 1000/@options.fps - Service.frameDuration)                    # DEBUG MEMORY DUMP every 30 sec              if @testCount > (5*30)                        @testCount = 0                  _dumpPath = path.resolve(__dirname + ""./../memoryDump/"" + @currentFrameTime + '.heapsnapshot')                heapdump.writeSnapshot _dumpPath                               setTimeout =>                # console.log ""Next frame : #{_delayNextFrame}ms - TOTAL : #{_frameDuration}ms""                @analyseFrame()            , (_delayNextFrame)                        @testCount++```I feel like detectAllFaces is causing memory leaks. In deed, the corresponding nodejs process gains approximately 0.5 to 1% memory usage for each processed frame ! So my RAM get totally filled really quickly. I tried to get memory dump using heapdump but I don't see anything anormal (all my dumps have same size on Google Chrome inspector). So this could means that memory leaks come from the C++ size of face-api.js, right ?At first, I thought the leak could comes from combination of loops and promises or from Service.videoCapture. But I get similar results with the below code. The only difference being that RAM is filled slower. Which seems normal since the image used is smaller than my webcam's frames.```# Analyse the new frame    analyseFrame: (next = _.noop) ->        # Skip if not capturing        return unless Service.isCapturing        _currentFunctionCallTime = Date.now()        # get frame        _frame = Service.videoCapture.getFrame()        # if analyzer is ready, we can take a frame from webcam and process it        if @analyzerIsReady            @analyzerIsReady = false            # get frame date            @currentFrameTime =_currentFunctionCallTime            # get frame            _analyzedFrame = _frame            # clear old faces in history            @refreshFaceHistory(@currentFrameTime)            @log.debug ""faceHistory.length = "", Service.faceHistory.length            _analyzedFrame = cv.imread('/home/loophole/eventbots-packages/eb/tiki-service-facedetection/models/bbt3.jpg')            #convert frame to a tensor            try                _data = new Uint8Array(_analyzedFrame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)                _tensorFrame = tfjs.tensor3d(_data, [_analyzedFrame.rows, _analyzedFrame.cols, 3])            catch _err                @log.error ""Error instantiating tensor !!!""                @log.error _err.message            faceapi.detectAllFaces(_tensorFrame, @faceDetectionOptions).then (_detectedFaces) =>                # fill face history with detceted faces                _detectedFaces = @fillFacesHistory(_detectedFaces)                # Get partial time                Service.frameDuration = Date.now() - @currentFrameTime                # Call next                _delayNextFrame = Math.max(0, 1000/@options.fps - Service.frameDuration)                setTimeout =>                    @analyzerIsReady = true                , (_delayNextFrame)        _callDuration = Date.now() - _currentFunctionCallTime        _delayNextCall = Math.max(0, 1000/@options.callPerSec - _callDuration)        setTimeout =>                @analyseFrame()            , (_delayNextCall)```";"[""Hey, there is no C++ side of face-api.js. If you are really experiencing memory leaks, it either comes from tensorflow or opencv4nodejs, which I find very unlikely as well since I didn't experience any kind of leaks in long running applications using opencv4nodejs + face-api.js with similar code.One thing that bothers me though, I don't see you disposing the tensor `_tensorFrame`, which you create for every frame. If you do not dispose it after your done with it, e.g. `_tensorFrame.dispose()`, then your application will indeed leak memory. You can also check the tracked memory of tfjs using `tf.memory()`.====="", ""I wasn't aware I had to dispose the tensor myself... I thought Node JS garbage collector would do it when taking care of _tensorFrame. Guess I was a little bit too demanding haha !Then again, thank you for your help :+1: ====="", 'There is still something that bothers me though... How come I don\'t have any memory leaks with this code ?```# Analyse the new frame    analyseFrame: (next = _.noop) ->        # Skip if not capturing        return unless Service.isCapturing        # get frame        _frame = Service.videoCapture.getFrame()        # get frame date        @currentFrameTime = Date.now()        # clear old faces in history        @refreshFaceHistory(@currentFrameTime)        @log.debug ""faceHistory.length = "", Service.faceHistory.length        #convert frame to a tensor        try            _data = new Uint8Array(_frame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)            _tensorFrame = tfjs.tensor3d(_data, [_frame.rows, _frame.cols, 3])        catch _err            @log.error ""Error instantiating tensor !!!""            @log.error _err.message        # Get partial time        Service.frameDuration = Date.now() - @currentFrameTime        # Call next        _delayNextFrame = Math.max(0, 1000/@options.fps - Service.frameDuration)        setTimeout =>            # console.log ""Next frame : #{_delayNextFrame}ms - TOTAL : #{_frameDuration}ms""            @analyseFrame()        , (_delayNextFrame)```Is that because a copy of _tensorFrame is sent to detectAllFaces when passed as parameter (in my first snippet) ? So _tensorFrame in analyzeFrame is just replaced at each frame ?=====', 'If you do not dispose the tensors you create, you will always leak memory. Try tracking `tf.memory()`, if the number of allocated tensors is growing with each frame,  you are leaking memory.=====', 'Hi!I having similar issue with this code in a loop:```\t\timg = await commons.canvas.loadImage(data),\t\tdetections = await faceapi.detectAllFaces(img, faceDetectionOptions).withFaceLandmarks().withFaceDescriptors(),```In few seconds I fill all my RAM, how can I dispose something to avoid memory leak?Thanks!=====', 'Solved!, only happend when run in debug mode, maybe never dispose because of that.Thanks!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/204;Safari iOS Exceeded WebGL Texture Size ;3;closed;2019-01-26T03:56:59Z;2020-01-31T10:35:27Z;Thanks for open-sourcing this library. It works perfectly for me on desktop Safari and Chrome. But it doesn't work for me on either browsers for iPhone. On Safari, I get the following error when I try loading the face detector and landmark detectors:`Unhandled Promise Rejection: Error: Requested texture size [6321x16] greater than WebGL maximum on this browser / GPU [4096x4096] in 161.50cc6bac.chunk.js:13429`Would you know how to handle this issue? Thanks.;"[""Unfortunately I don't have access to mobile safari. Which example are you referring to and which function call is throwing this error, can you post a stacktrace?====="", ""Actually, I can run the demo api on Ipad Safari, but it's very sluggish. I am wondering why is that so. I am wondering if a surface pro could do better since I want to use it on a tablet. ====="", 'We were facing this problem also in our project. We were able to resolve this by manually loading the face-api.js rather than loading from npm.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/203;Webcam feed on NodeJs environment;6;closed;2019-01-24T14:18:02Z;2020-01-24T23:53:28Z;Hi,I would like to perform live face detection in a nodejs environment but I don't quiet see how to do that.Initially, I was capturing frames from my webcam using opencv4nodejs:```cap = new cv.VideoCapture(videoDeviceId),(...)_frame = cap.read(),```But then I would have to find a way to convert _frame to a Canvas, right ? At least if I'm using a monkey path like this:```const { Canvas, Image, ImageData } = canvasfaceapi.env.monkeyPatch({ Canvas, Image, ImageData })```Do I have other options ? Would it be possible to use a monkey patch with some opencv4nodejs' elements ?I saw this line in README.md:```Alternatively you can simply construct your own tensors from image data and pass tensors as inputs to the API.```Okay, but how can I achieve that ? If I look at detectAllFaces' signature, I can see the following:```detectAllFaces(input: TNetInput, options?: FaceDetectionOptions): DetectAllFacesTask```And if I look further, I can see that TNetInput can be a TNetInputArg, which can be a TResolvedNetInput, which can be a tf.Tensor3D or tf.Tensor4D.So I would have to retrieve data from _frame and use them to instantiate a tf.Tensor3D ?Thanks !;"['Okay, so I managed to get a small working snippet:```this.currentFrameTime = Date.now(),// get frame using opencv4nodejs_frame = Service.videoCapture.getFrame(),try {    // create tensor  _tensorFrame = tfjs.tensor3d(_frame.getDataAsArray()),} catch (_error) {  _err = _error,  this.log.error(""Error instantiating tensor !!!""),  this.log.error(_err.message),}faceapi.detectAllFaces(_tensorFrame, this.faceDetectionOptions).then((function(_this) {  return function(_detectedFaces) {    var _delayNextFrame, _paintedCurrentFrame,    // draw face boxes on frame    _paintedCurrentFrame = Service.videoCapture.drawFaceBoxes(_frame, _detectedFaces),    // compute latency    Service.frameDuration = Date.now() - _this.currentFrameTime,    // display image    Service.faceRecoUtils.showImage(_paintedCurrentFrame),   // compute delay needed to have a constant fps    _delayNextFrame = Math.max(0, 1000 / _this.options.fps - Service.frameDuration),    return _this.log.debug(""Latency: "", Service.frameDuration),  },})(this)),// try to have a constant fpssetTimeout((function(_this) {  return function() {    return _this.analyseFrame(),  },})(this), _delayNextFrame),```So my problem is, this is really really slow ! Like way slower that your web-example (from 1000ms to 3500ms for one frame). In issue #44, you said that ""If the code is running on the cpu in node, it would probably be very slow."". As a matter of fact, I am running code on cpu. I tried to run it on GPU but I still couldn\'t make it (issue with cuDNN, cuda and tensorflow versions). Anyway, that doesn\'t change anything because the production computers won\'t have GPUs.Could you explain the speed gap between node and browser ? Because if I understand correctly, you are running your code on CPU in browser examples, right ? Second question: face-api.js isn\'t designed to run on NodeJS and I better stick to face-recognition.js, right ?=====', '> In issue #44, you said that ""If the code is running on the cpu in node, it would probably be very slow."". As a matter of fact, I am running code on cpu. I tried to run it on GPU but I still couldn\'t make it (issue with cuDNN, cuda and tensorflow versions). Anyway, that doesn\'t change anything because the production computers won\'t have GPUs.Sorry for the confusion here. You can run the tfjs code on the cpu in node without using the native tensorflow backend, which is very slow. If you are installing tfjs-node (cpu or gpu) you will notice a pretty decent performance boost.> Could you explain the speed gap between node and browser? Because if I understand correctly, you are running your code on CPU in browser examples, right ?It depends. The browser examples should optimally run on the webgl backend and not on the cpu. If you have a mediocre to decent gpu running the code in the browser on the webgl backend will probably be faster than running on the tfjs-node cpu backend (native tensorflow backend).1000ms for faceDetection in the browser is pretty slow, probably the SSD Mobilenet face detector is too heavy for your machine or the webgl backend is not registered. For example ssd mobilenetv1 runs on roughly 40 fps on my gpu. You could try using the TinyFaceDetector instead, which I would recommend anyways for realtime apps, if you are tracking your face via webcam for example._frame.getDataAsArray() is probably also a bottleneck, depending on the size of your image. To convert a cv.Mat to tensor try the following, using mat.getData() instead:``` javascript// assuming img is a BGR Matconst data = new Uint8Array(img.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)const imgTensor = faceapi.tf.tensor3d(data, [img.rows, img.cols, 3])```=====', ""Sorry, I think I wasn't clear: I got 1000ms in nodejs, not in brower.But anyway _frame.getDataAsArray() was the real problem here. Your code is way more efficient.With TinyFaceDetector, the detection takes from 80 to 200 ms. My tensor creation was taking from 1100 to 1500ms  while your's only takes from 10 to 30 ms !! Thanks a lot for your help.====="", ""In case someone comes along after me looking for **exactly** the code needed.As a repo: https://github.com/mamacker/faceme```import * as tfjs from '@tensorflow/tfjs-node',import * as faceapi from 'face-api.js',import * as fs from 'fs',import * as cv from 'opencv',const minConfidence = 0.5,/*const faceDetectionNet = faceapi.nets.ssdMobilenetv1,const faceDetectionOptions = new faceapi.SsdMobilenetv1Options({minConfidence}),*/const faceDetectionNet = faceapi.nets.tinyFaceDetector,const faceDetectionOptions = new faceapi.TinyFaceDetectorOptions({minConfidence}),let globalFrame = null,let lastFrame = null,function processFrame() {  if (globalFrame == null) return,  if (lastFrame == globalFrame) return,  lastFrame = globalFrame,  let tFrame = faceapi.tf.tensor3d(globalFrame, [480, 640, 3])  faceapi.detectAllFaces(tFrame, faceDetectionOptions).then((faces) => {    console.log(faces),  }),}setInterval(processFrame, 100),async function run() {  await faceDetectionNet.loadFromDisk('./weights'),  let video = '/dev/video0',  let cap = new cv.VideoCapture(video),  let capture = () => {    cap.read(function(err, frame) {      if (frame.width() > 0) {        let data = new Uint8Array(frame.getData().buffer),        globalFrame = data,      }      setTimeout(capture, 0),    }),  }  capture(),}run()```====="", ""Thank you @mamacker for the code 🙇\u200d♂️ Thanks to you I was able to remove `canvas` from my project 🙇\u200d♂️Full code: https://github.com/whyboris/extract-faces-nodeI'm using [sharp](https://github.com/lovell/sharp) for image loading (as I need other image manipulations later)Relevant piece:```tsconst sharp = require('sharp'),const imgBuffer: Buffer = await sharp('./some/img/path.jpg').toBuffer(),const imgTensor = tf.node.decodeJpeg(imgBuffer),const detections = await faceapi.detectAllFaces(imgTensor),```====="", ""> Thank you @mamacker for the code 🙇\u200d♂️ Thanks to you I was able to remove `canvas` from my project 🙇\u200d♂️> > Full code: https://github.com/whyboris/extract-faces-node> > I'm using [sharp](https://github.com/lovell/sharp) for image loading (as I need other image manipulations later)> > Relevant piece:> > ```ts> const sharp = require('sharp'),> const imgBuffer: Buffer = await sharp('./some/img/path.jpg').toBuffer(),> const imgTensor = tf.node.decodeJpeg(imgBuffer),> const detections = await faceapi.detectAllFaces(imgTensor),> ```This the correct mode, using decodeJpeg and most faster=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/202;Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array;4;closed;2019-01-24T04:50:00Z;2019-05-02T16:24:28Z;Hi, I use your api with nodejs, this is my code:onst referenceImage = await canvas.loadImage(REFERENCE_IMAGE)    const queryImage = await canvas.loadImage(QUERY_IMAGE)    const results = await faceapi        .detectAllFaces(referenceImage)        .withFaceLandmarks()        .withFaceDescriptors()    const labeledDescriptors = [new faceapi.LabeledFaceDescriptors('obama', Float32Array.from(results[0].descriptor))]    const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors)But i get Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32ArrayI already try:    const labeledDescriptors = [new faceapi.LabeledFaceDescriptors('obama', results[0].descriptor)]andconst labeledDescriptors = [new faceapi.LabeledFaceDescriptors('obama', results[0].descriptor.map(desc => new Float32Array(desc)))]I execute this like a npm script: node script.js;"[""It should be as follows:``` javascript// using only one reference descriptor for class 'obama'const descriptorsObama = [  new Float32Array(results[0].descriptor)]const labeledDescriptors = [  new faceapi.LabeledFaceDescriptors('obama', descriptorsObama )]```====="", 'Thanks!=====', 'what should you do in this case if you have multiple descriptors for each class? =====', 'To proccess al descriptors i do :await forEach(this.files, async (img) => {            try {                const desc = await faceapi                    .detectSingleFace(img)                    .withFaceLandmarks()                    .withFaceDescriptor(),                results.push(Array.prototype.slice.call(desc.descriptor)),            } catch (e) {                console.log(e),            }        }),and:const descriptors = [],                    obj.biometric.descriptors.forEach(element => {                        descriptors.push(new Float32Array(element)),                    }),                    const faceMatcher = new faceapi.FaceMatcher([                        new faceapi.LabeledFaceDescriptors(prefix, descriptors)                    ]),                    const singleResult = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor(),where element is image to campare with array of descriptors=====']"
https://github.com/justadudewhohacks/face-api.js/issues/201;Web examples don't work on firefox;2;closed;2019-01-23T14:18:28Z;2019-01-24T12:44:33Z;Hi !I noticed that web example doesn't work on firefox.More precisely, the asynchronous functions detectSingleFace and detectAllFaces never end.I can't explain why and honestly, I don't even know why I tried this package on Firefox since every single complex project I run into doesn't seem to work on that browser.But maybe you could specify it on README's package ?Regards;"['Are you referring to the same issue as #129? Otherwise should work fine on firefox.=====', ""Yep, that's exactly the same issue ! Sorry, I should have check better. I am closing this one.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/200;nodejs examples can't working;7;closed;2019-01-23T08:08:35Z;2019-01-28T09:40:56Z;"macos    node v10.7.0     npm 6.6.0     tfjs-core ""0.13.8""  git clone https://github.com/justadudewhohacks/face-api.js.gitcd face-api.js/examples/examples-nodejsnpm its-node faceDetection.tslocalhost:examples-nodejs hwanpenn$ ts-node faceDetection.ts2019-01-23 15:48:46.382968: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPUsupports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend(node:33079) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function    at new Tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)    at Function.Tensor.make (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:430:12)    at Object.tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/tensor_ops.ts:99:17)    at _loop_1 (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/io_utils.ts:129:15)    at Object.decodeWeights (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/dist/io/io_utils.js:132:9)    at /Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:221:13    at Array.forEach (<anonymous>)    at /Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:216:22    at Array.forEach (<anonymous>)    at Object.<anonymous> (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:197:25)(node:33079) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:33079) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.**similar with Can´t get nodejs examples working #164  ，but my tfjs-core version is right， i dont know how to fix it  ，Browser examples works fine.**";"['Try to clean your node_modules and start over with a fresh npm install. The error message does not come from face-api.js, but from tfjs-core.=====', 'Some error=====', '**thanks @justadudewhohacks  ， love you ^_^, with your suggest i fix that，but。。。**TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(3,16): error TS2354: This syntax requires an imported helper but module \'tslib\' cannot be found.faceDetection.ts(5,26): error TS2339: Property \'loadFromDisk\' does not exist on type \'SsdMobilenetv1\'.faceDetection.ts(10,23): error TS2339: Property \'createCanvasFromMedia\' does not exist on type \'typeof ""/Users/hwanpenn/Downloads/hwanpenn/face01/src/index""\'.faceDetection.ts(11,11): error TS2339: Property \'drawDetection\' does not exist on type \'typeof ""/Users/hwanpenn/Downloads/hwanpenn/face01/src/index""\'.environment ：ts-node v7.0.1    node v11.6.0    typescript v2.8.4**same with Could not run faceDetection.ts #142now im trying fix this**=====', ""**cd   /face-api.js/  and npm install  ，now we will hava tslib^1.9.3it seem work  ，no “TSError: ⨯ Unable to compile TypeScript:” again，**but new error  （）：2019-01-28 13:43:14.462031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPUsupports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend(node:88557) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function    at new Tensor (/Users/hwanpenn/Downloads/hwanpenn/face01/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)**Isn't that my first mistake?   Wrapped around a large circle ， fuck...**====="", ""**i want to die ， i fix the problem upstairs，and i have a new one again。。。**2019-01-28 15:40:11.349408: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:228    return new TSError(diagnosticText, diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:../../src/ssdMobilenetv1/outputLayer.ts(73,7): error TS2345: Argument of type '[number, number | undefined]' is not assignable to parameter of type 'number[] | [number, number, number] | [number, number, number, number] | [number, number] | [number] | [number, number, number, number, number] | [number, number, number, number, number, number]'.  Type '[number, number | undefined]' is not assignable to type '[number, number]'.    Type 'number | undefined' is not assignable to type 'number'.      Type 'undefined' is not assignable to type 'number'.    at createTSError (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:228:12)    at getOutput (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:334:40)    at Object.compile (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:367:11)    at Module.m._compile (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:413:43)    at Module._extensions..js (internal/modules/cjs/loader.js:700:10)====="", ""Hold on for a second. > (node:33079) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function> at new Tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)I think this is an issue with tfjs-node@0.1.19, which is the current in the package.json of the nodejs examples. It should actually be tfjs-node@0.1.21 (same as in the root package.json).> TSError: ⨯ Unable to compile TypeScript:> ../../src/ssdMobilenetv1/outputLayer.ts(73,7): error TS2345: Argument of type '[number, number | undefined]' is not assignable to parameter of type 'number[] | [number, number, number] | [number, number, number, number] | [number, number] | [number] | [number, number, number, number, number] | [number, number, number, number, number, number]'.> Type '[number, number | undefined]' is not assignable to type '[number, number]'.> Type 'number | undefined' is not assignable to type 'number'.> Type 'undefined' is not assignable to type 'number'.This is a typescript 3 specific error, which indicates that you are not using ts-node v7.0.1 anymore. I am currently updating the dependencies of face-api.js including upgrading the typescript version to latest.====="", '1.git clone https://github.com/justadudewhohacks/face-api.js.git2.examples/examples-nodejs/package.json ""@tensorflow/tfjs-node"": ""^0.1.19""-->""^0.1.21""3.cd face-api.js    npm install    （fix tslib）    npm audit fix       （add karma-typescript@3.0.13）4.cd face-api.js/examples/examples-nodejs    npm install    ts-node faceDetection.ts **you are right @justadudewhohacks ，It should be tfjs-node@0.1.21 ，it works ，now i love you more** =====']"
https://github.com/justadudewhohacks/face-api.js/issues/199;Can't install typescript project;4;closed;2019-01-22T15:36:59Z;2019-01-28T10:00:45Z;Hi,When I try to execute the following command in root's repository,```tsc```I get this error : ```loophole@loophole:~/packages/vision/face-api.js$ tscsrc/ssdMobilenetv1/outputLayer.ts:73:7 - error TS2345: Argument of type '[number, number | undefined]' is not assignable to parameter of type 'number[] | [number, number] | [number, number, number, number] | [number, number, number, number, number] | [number, number, number] | [number] | [number, number, number, number, number, number]'.  Type '[number, number | undefined]' is not assignable to type '[number, number]'.    Type 'number | undefined' is not assignable to type 'number'.      Type 'undefined' is not assignable to type 'number'.73       [batchSize, scores.shape[1]]         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~Found 1 error.```Am I doing something wrong ? I am a total newbie to TypeScript so excuse me if my question is silly.;"['Whats the typescript version you are using? Should compile with latest.=====', 'I am using version 3.2.4, which seems to be the latest.=====', 'Oh nevermind, are you trying to compile face-api.js? Currently face-api.js is using typescript 2.8.4 to compile the project (see package.json), if you want to compile it using the locally installed typescript version simply use the npm script (npm run tsc).=====', ""Yes, that's exactly what I was trying to do ! Thanks a lot !=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/198;Loading Images;2;closed;2019-01-22T12:42:46Z;2021-06-03T01:50:45Z;When Loading images we need to put the images in the public folder so that server can host that public folder, it doesn't work if I provide a relative path to my images that is not present inside the public folder.;"[""That's because a browser can not simply access your filesystem. You have to make any kind of assets publicly available via a webserver.====="", 'When public url for my react js app is root(www.abcd.com), everything work well. But if I change public url to relative path (www.abcd.com/face), I face error ""....load model before inference"". export const loadModels = async () => {  const MODEL_URL = `${process.env.PUBLIC_URL}/models`,  await faceapi.loadTinyFaceDetectorModel(MODEL_URL),  await faceapi.loadFaceLandmarkTinyModel(MODEL_URL),  await faceapi.loadFaceRecognitionModel(MODEL_URL),},what happen. And how do I fix it? Should I change the model url to a relative path???=====']"
https://github.com/justadudewhohacks/face-api.js/issues/196;Base64 Image;5;closed;2019-01-18T18:46:50Z;2020-04-05T19:14:32Z;Love the library, having fun implementing it into various different things.Is there a way to pass a base64 encoded image to it instead of a canvas?;"[""You can simply create an image element and set it's src to the base64 string, then pass this one to the api.====="", ""Get error 'The object is in an invalid state. drawImage' using base64 in src img tag.====="", 'i am also using base64 image but not getting right way to use it. fetchImage() method only accept string argument. Can someone place point right solution for this?=====', 'how it can be done on nodejs Due to lack of documentation for nodejs its quite hard to get things working=====', ""Hallo, I've the same problem with base64 images.I've simply modified the faceReconigtion.ts example in example-node directory in this way:```const referenceImage = await canvas.loadImage(REFERENCE_IMAGE)//const queryImage = await canvas.loadImage(QUERY_IMAGE)const queryImage = new Image()queryImage.src = 'data:image/png,base64,' + base64img   const resultsRef = await faceapi.detectAllFaces(referenceImage, faceDetectionOptions)    .withFaceLandmarks()    .withFaceDescriptors()```base64img contains the base64 encoded image.Running the code I get this error:(node:6628) UnhandledPromiseRejectionWarning: TypeError: media.addEventListener is not a function    at C:\\Users\\ ...... \\node_modules\\face-api.js\\src\\dom\\awaitMediaLoaded.ts:25:11    at new Promise (<anonymous>)    at Object.awaitMediaLoaded (C:\\Users\\.... \\node_modules\\face-api.js\\src\\dom\\awaitMediaLoaded.ts:6:10)    at C:\\Users\\...... \\node_modules\\face-api.js\\src\\dom\\toNetInput.ts:53:54    at Array.map (<anonymous>)    at Object.<anonymous> (C:\\Users\\........\\node_modules\\face-api.js\\src\\dom\\toNetInput.ts:53:16)    at step (C:\\Users\\..........\\node_modules\\tslib\\tslib.js:139:27)    at Object.next (C:\\Users\\..............\\node_modules\\tslib\\tslib.js:120:57)    at C:\\Users\\............\\node_modules\\tslib\\tslib.js:113:75    at new Promise (<anonymous>)(node:6628) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 2)(node:6628) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/193;Speed Improvement ;6;closed;2019-01-14T14:06:59Z;2019-01-18T05:39:06Z;"This is not an issue but rather question for suggestion.Currently, i'm using on nodejs with:    ""face-api.js"": ""^0.17.1"",and not @tensorflow/tfjs-node because it seems to be broken right now.My Goal is to detect all the faces on images. For this, i'm using SSNMobileNetv1Model. It gives great result. But i takes around 10-12 sec to complete the task. Is there any way i can improve it either by using multithreading or any other meathod?";"['You should definitely be using @tensorflow/tfjs-node to speed things up.=====', 'I did as per your suggestion and this is what i got.Without tfjs-node:11479.518ms with tkjs-node: 11707.518msIt doesn\'t look like tfjs-node is help alot. Is it because of my hardware limitation?using: i5-7th gen, graphic cad=intel.This is complete output:``` nodejs Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm install> canvas@2.3.1 install /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download[canvas] Success: ""/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/canvas/build/Release/canvas.node"" is installedvia remote> nodemon@1.18.9 postinstall /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/nodemon> node bin/postinstall || exit 0npm notice created a lockfile as package-lock.json. You should commit this file.npm WARN face_detection_using_faceapi.js@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})added 297 packages from 195 contributors and audited 2381 packages in 130.679sfound 0 vulnerabilities Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm run dev> face_detection_using_faceapi.js@1.0.0 dev /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI> nodemon --exec ts-node --require dotenv/config   ./src/index.ts[nodemon] 1.18.9[nodemon] to restart at any time, enter `rs`[nodemon] watching: *.*[nodemon] starting `ts-node --require dotenv/config ./src/index.ts`starting app...Using SSNMobileModel============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================canvas loaded...Detecting facesdefault: 11479.975msTotal number of faces: 17saving file...1.jpeg /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/out[nodemon] clean exit - waiting for changes before restart^[[A^C Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm install --save @tensorflow/tfjs-node> @tensorflow/tfjs-node@0.2.3 install /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/@tensorflow/tfjs-node> node scripts/install.js* Downloading libtensorflow[==============================] 6318482/bps 100% 0.0s* Building TensorFlow Node.js bindings> protobufjs@6.8.8 postinstall /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/protobufjs> node scripts/postinstallnpm WARN @tensorflow/tfjs-converter@0.7.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-data@0.1.7 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-layers@0.9.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN face_detection_using_faceapi.js@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})+ @tensorflow/tfjs-node@0.2.3added 30 packages from 13 contributors and audited 2454 packages in 51.439sfound 0 vulnerabilities Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm run dev> face_detection_using_faceapi.js@1.0.0 dev /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI> nodemon --exec ts-node --require dotenv/config   ./src/index.ts[nodemon] 1.18.9[nodemon] to restart at any time, enter `rs`[nodemon] watching: *.*[nodemon] starting `ts-node --require dotenv/config ./src/index.ts`starting app...Using SSNMobileModel============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================canvas loaded...Detecting facesdefault: 11707.518msTotal number of faces: 17saving file...1.jpeg /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/out[nodemon] clean exit - waiting for changes before restart```=====', ""Hmm this indicates, that you are not using the node backend in the second run:```============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================```Not sure why that is if tfjs-node installed correctly. Probably better to ask at tfjs what went wrong here.====="", ""What do you by I'm not using node backend in the 2nd run?On Tue 15 Jan, 2019, 2:01 PM Vincent Mühler <notifications@github.com wrote:> Hmm this indicates, that you are not using the node backend in the second> run:>> ============================>> Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.>> ============================>>> Not sure why that is if tfjs-node installed correctly. Probably better to> ask at tfjs-node what went wrong here.>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/193#issuecomment-454307196>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHyDee5RcE1ZcPykov4CX6U9HmI1NR-4ks5vDZHagaJpZM4Z-UN2>> .>====="", ""Seems you are installing tfjs-node, but the backend doesn't get registered properly in your application for some reason.====="", 'It was mistake from my side. I forgot to load the required module. It working perfectly fine now.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/191;Can't load JSON as custom node package;6;closed;2019-01-11T10:43:42Z;2019-03-23T11:19:29Z;I'm trying to load the models in a standard application and as an importable node package. Everything in the app is working brilliantly except where I am trying to load the models into face-api. The path being referenced is wrong for the node package version and it's trying to load from the root of the app rather than the root of the node package.All models are stored in a folder relative to this script `weights/`Code:```/**     * injects the models json to FaceAPI.js     * @returns {Promise<void> | *}     * @todo find make better by not changing path...     */    loadModels() {        const manifest = path.join(__dirname, 'weights'),        return faceapi.loadTinyFaceDetectorModel(manifest)            .then(() => {                this.options = new faceapi.TinyFaceDetectorOptions({                    minFaceSize: this.minFaceSize,                    minConfidence: this.minConfidence,                    inputSize: this.inputSize,                }),            })            .catch(error => {                console.error(`Unable to load models for Face API: ${error}`),            }),    }```;"['I am not sure how your issue is related to this package. Did you try `path.resolve` instead of `path.join`?=====', ""Yes, tried that. It seems the path is always relative to the site rather than the node module. I've managed to find a workaround by referencing an external URL which isn't great but will do for now. Is it possible to just pass in an object rather than JSON?====="", '`__dirname` is always relative to the root of the node process, not sure why you want to load model relative to node_modules?> Is it possible to just pass in an object rather than JSON?Pass in an object to what? You pass in the path (string) to the load methods, not JSON.=====', ""So how would one pass in the JSON as a relative path then? If my script is in say a folder called `faceapi` and my models are in` faceapi/models` how would I link that? When I run pron it is trying to reference the root i.e. google.com/weights when I want it to reference the relative folder to the script that's calling it ====="", ""You would simply do `faceapi.loadTinyFaceDetectorModel('faceapi/model')`.Is faceapi/models located in your assets (web app) or are you actually trying to load the model from disk in a nodejs process. If you attempt to load from disk you have to use faceapi.nets.tinyFaceDetector.loadFromDisk('faceapi/model'). (also see the README section about loading models)====="", 'Okay, thank you. Let me give that a go. Appreciate the help at the weekend!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/189;SsdMobilenetv1 - load model before inference;3;closed;2019-01-10T12:26:31Z;2021-07-28T07:49:16Z;This is how i'm loading the modules.``` typescript        await this.loadTinyModel(),        console.log('model loaded'),        let canvas = await this.loadCanvas(filePath),        let tt = await faceapi.detectAllFaces(canvas),    private async loadTinyModel() {        await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname, 'models')),    }    private async loadCanvas(filePath: string) {        return canvas.loadImage(filePath),    }```But getting errors:``` node(node:27555) UnhandledPromiseRejectionWarning: Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:26:13)    at SsdMobilenetv1.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:60:14)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)    at Object.next (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:114:57)    at fulfilled (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:104:62)    at <anonymous>(node:27555) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:27555) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```;"[""You are calling detectAllFaces, which assumes (by default) that the ssdMobilenetv1 model is loaded:`await faceapi.nets.ssdMobilenetv1.loadFromDisk(path.join(__dirname, 'models'))`.====="", 'So `SsdMobileNetv1` always should be loaded?Could I use `tinyFaceDetector` only?UPDATE: sorry, found an answer https://github.com/justadudewhohacks/face-api.js#detecting-faces :)=====', '> So `SsdMobileNetv1` always should be loaded?> Could I use `tinyFaceDetector` only?> > UPDATE: sorry, found an answer https://github.com/justadudewhohacks/face-api.js#detecting-faces :)what you add in youe old code?i have same problem brother.can you tell mi how you solve it.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/186;fetch implementation is missing;9;closed;2019-01-07T13:36:33Z;2020-12-22T20:25:50Z;Like the title says, fetch implemtation is missing.face-api version: 0.17.1``` typescriptimport * as faceapi from 'face-api.js',class App {    private MODEL_URL: string,    constructor() {        console.log('starting app...'),        console.log(process.env.MODELS),        this.MODEL_URL = process.env.MODELS,    }    start() {        try {            this.loadModels(),            console.log(`Models loadedd successfully....`),        } catch (error) {            console.error(error),        }    }    private async loadModels() {        await faceapi.loadSsdMobilenetv1Model(this.MODEL_URL)    }}```(node:25181) UnhandledPromiseRejectionWarning: Error: fetch - missing fetch implementation for nodejs environment    at fetch (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/env/createNodejsEnv.ts:24:11)    at Object.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/dom/fetchOrThrow.ts:9:21)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)    at Object.next (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:114:57)    at /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:107:75    at new Promise (<anonymous>)    at Object.__awaiter (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:103:16)    at Object.fetchOrThrow (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/build/commonjs/dom/fetchOrThrow.js:6:20)    at Object.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/dom/fetchJson.ts:4:17)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)(node:25181) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of anasync function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 4)(node:25181) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```;"['The error message pretty much says it, you either need to use a fetch polyfill for nodejs or you can monkeyPatch it: `faceapi.env.monkeyPatch({ fetch: /*your implementation here*/ })`.=====', ""Shouldn't it be implemented by the face-api itself? If not, then is there any specific reason for it?====="", ""In tfjs-core fetch is being used for fetching and decoding model shards, thus you won't get around this. You can polyfill fetch installing the `node-fetch` package if you really need to load models from remote uris in nodejs or you could simply load the models from local filesystem instead.====="", 'I have the same error @justadudewhohacks, I have installed node-fetch-polyfill but still not working. Am I misinterpreting it?```import express from \'express\',import path from \'path\',import * as faceapi from \'face-api.js\',import fetch from \'node-fetch-polyfill\',var app = express(),app.use(express.static(\'public\'))app.get(\'/\', function (req, res) {   res.sendFile(path.join(__dirname + \'/index.html\')),})var server = app.listen(8081, function () {   var host = server.address().address   var port = server.address().port   console.log(""Example app listening at http://%s:%s"", host, port),})app.get(\'/test\', async function(req, res, next){    const MODEL_URL = \'/models\',    try {        await faceapi.loadSsdMobilenetv1Model(MODEL_URL),    } catch (e) {        console.log(e),    }   console.log(dataToSendToClient),   //var dataToSendToClient = {\'labeledFaceDescriptors\': \'hello\'},   var JSONdata = JSON.stringify(dataToSendToClient),   res.send(JSONdata),})```=====', ""### BrowserLoading the models on the browser was working fine.```javascriptconst MODELS_URL = 'path/to/models',// Load the face detection modelsawait faceapi.loadSsdMobilenetv1Model(MODELS_URL),// Load the face landmark modelsawait faceapi.loadFaceLandmarkModel(MODELS_URL),// Load the face recognition modelsawait faceapi.loadFaceRecognitionModel(MODELS_URL),       ```<br>However, I faced the same issue on Node.js.<br>### Node.jsI found the solution by looking through the example code in the repository.```javascriptimport path from 'path',// Import a fetch implementation for Node.jsimport fetch from 'node-fetch',// Make face-api.js use that fetch implementationfaceapi.env.monkeyPatch({ fetch: fetch }),// Try experimenting on wherever your models are located. Mine are from up one folder// You will get 'Error: Only absolute urls are supported' if you don't specify the absolute pathconst MODELS_URL = path.join(__dirname, '/../models/face-api'),async function doSomething() {  // Load the face detection models  await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODELS_URL),  // Load the face landmark models  await faceapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL),  // Load the face recognition models  await faceapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL),  // Do something else  ...}```====="", 'i have the same issue @cefjoeii but your solution is not work for me.This give me another error > face-recognition.ts(15,27): error TS2322: Type \'typeof fetch\' is not assignable to type \'(url: string, init?: RequestInit) => Promise<Response>\'.  Types of parameters \'init\' and \'init\' are incompatible.    Type \'RequestInit\' is not assignable to type \'import(""/home/duy/find_user_nodejs/node_modules/@types/node-fetch/index"").RequestInit\'.      Types of property \'body\' are incompatible.        Type \'BodyInit\' is not assignable to type \'import(""/home/duy/find_user_nodejs/node_modules/@types/node-fetch/index"").BodyInit\'.          Type \'Blob\' is not assignable to type \'BodyInit\'.            Type \'Blob\' is missing the following properties from type \'ArrayBuffer\': byteLength, [Symbol.toStringTag]=====', ""You don't need fetch nor monkeypatch. Just use the special API calls to load the model files from local storage (the tutorials don't mention those, but README at npm does), like this:```jsconst faceapi = require('face-api.js'),const tf = require('@tensorflow/tfjs-node'),const Canvas = require('canvas'),const Image = require('canvas'),const ImageData = require('canvas'),const MODEL_URL = `${__dirname}/face-models/`,faceapi.nets.ssdMobilenetv1.loadFromDisk(MODEL_URL)    .then(faceapi.nets.faceLandmark68Net.loadFromDisk(MODEL_URL))    .then(faceapi.nets.faceRecognitionNet.loadFromDisk(MODEL_URL))    .catch(error => {        error.log(error),    }),```====="", '(node:12474) UnhandledPromiseRejectionWarning: Error: only absolute urls are supported    at /home/vvm/tube/node_modules/node-fetch-polyfill/index.js:47:19=====', '> ```js> path.joi> ```I did the same and now I get `Only absolute URLs are supported`Update----------I found what was my mistakeI was calling `faceapi.nets.faceRecognitionNet.loadFromUrl` instead of `faceapi.nets.faceRecognitionNet.loadFromDisk`=====']"
https://github.com/justadudewhohacks/face-api.js/issues/185;npm install fails;1;closed;2019-01-07T12:45:23Z;2019-01-17T09:14:25Z;"Like the title says, npm install fails.The problem seems to be with @tensorflow/tfjs-node.```0 info it worked if it ends with ok1 verbose cli [ '/usr/bin/node',1 verbose cli   '/usr/bin/npm',1 verbose cli   'install',1 verbose cli   '--save',1 verbose cli   '@tensorflow/tfjs-node' ]2 info using npm@6.4.13 info using node@v8.12.04 verbose npm-session aa04fe0278e471d85 silly install loadCurrentTree6 silly install readLocalPackageData7 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs-node 11ms (from cache)8 silly pacote tag manifest for @tensorflow/tfjs-node@latest fetched in 25ms9 timing stage:loadCurrentTree Completed in 288ms10 silly install loadIdealTree11 silly install cloneCurrentTreeToIdealTree12 timing stage:loadIdealTree:cloneCurrentTree Completed in 1ms13 silly install loadShrinkwrap14 timing stage:loadIdealTree:loadShrinkwrap Completed in 111ms15 silly install loadAllDepsIntoIdealTree16 silly resolveWithNewModule @tensorflow/tfjs-node@0.2.1 checking installable status17 silly removeObsoleteDep removing @tensorflow/tfjs-node@0.1.19 from the tree as its been replaced by a newer version or is no longer required18 silly removeObsoleteDep removing @tensorflow/tfjs@0.13.3 from the tree as its been replaced by a newer version or is no longer required19 silly removeObsoleteDep removing @tensorflow/tfjs-converter@0.6.5 from the tree as its been replaced by a newer version or is no longer required20 silly removeObsoleteDep removing @types/long@3.0.32 from the tree as its been replaced by a newer version or is no longer required21 silly removeObsoleteDep removing protobufjs@6.8.8 from the tree as its been replaced by a newer version or is no longer required22 silly removeObsoleteDep removing @protobufjs/aspromise@1.1.2 from the tree as its been replaced by a newer version or is no longer required23 silly removeObsoleteDep removing @protobufjs/base64@1.1.2 from the tree as its been replaced by a newer version or is no longer required24 silly removeObsoleteDep removing @protobufjs/codegen@2.0.4 from the tree as its been replaced by a newer version or is no longer required25 silly removeObsoleteDep removing @protobufjs/eventemitter@1.1.0 from the tree as its been replaced by a newer version or is no longer required26 silly removeObsoleteDep removing @protobufjs/fetch@1.1.0 from the tree as its been replaced by a newer version or is no longer required27 silly removeObsoleteDep removing @protobufjs/inquire@1.1.0 from the tree as its been replaced by a newer version or is no longer required28 silly removeObsoleteDep removing @protobufjs/float@1.0.2 from the tree as its been replaced by a newer version or is no longer required29 silly removeObsoleteDep removing @protobufjs/path@1.1.2 from the tree as its been replaced by a newer version or is no longer required30 silly removeObsoleteDep removing @protobufjs/pool@1.1.0 from the tree as its been replaced by a newer version or is no longer required31 silly removeObsoleteDep removing @protobufjs/utf8@1.1.0 from the tree as its been replaced by a newer version or is no longer required32 silly removeObsoleteDep removing @types/long@4.0.0 from the tree as its been replaced by a newer version or is no longer required33 silly removeObsoleteDep removing @types/node@10.12.0 from the tree as its been replaced by a newer version or is no longer required34 silly removeObsoleteDep removing long@4.0.0 from the tree as its been replaced by a newer version or is no longer required35 silly removeObsoleteDep removing @tensorflow/tfjs-core@0.13.8 from the tree as its been replaced by a newer version or is no longer required36 silly removeObsoleteDep removing @types/seedrandom@2.4.27 from the tree as its been replaced by a newer version or is no longer required37 silly removeObsoleteDep removing @types/webgl-ext@0.0.30 from the tree as its been replaced by a newer version or is no longer required38 silly removeObsoleteDep removing @types/webgl2@0.0.4 from the tree as its been replaced by a newer version or is no longer required39 silly removeObsoleteDep removing seedrandom@2.4.3 from the tree as its been replaced by a newer version or is no longer required40 silly removeObsoleteDep removing @tensorflow/tfjs-layers@0.8.3 from the tree as its been replaced by a newer version or is no longer required41 silly removeObsoleteDep removing adm-zip@0.4.11 from the tree as its been replaced by a newer version or is no longer required42 silly removeObsoleteDep removing bindings@1.3.0 from the tree as its been replaced by a newer version or is no longer required43 silly removeObsoleteDep removing progress@2.0.1 from the tree as its been replaced by a newer version or is no longer required44 http fetch GET 200 https://registry.npmjs.org/adm-zip 15ms (from cache)45 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs 16ms (from cache)46 http fetch GET 200 https://registry.npmjs.org/bindings 17ms (from cache)47 http fetch GET 200 https://registry.npmjs.org/progress 16ms (from cache)48 silly pacote range manifest for adm-zip@^0.4.11 fetched in 18ms49 silly resolveWithNewModule adm-zip@0.4.13 checking installable status50 silly pacote range manifest for bindings@~1.3.0 fetched in 19ms51 silly resolveWithNewModule bindings@1.3.1 checking installable status52 silly pacote range manifest for progress@^2.0.0 fetched in 18ms53 silly resolveWithNewModule progress@2.0.3 checking installable status54 silly pacote range manifest for @tensorflow/tfjs@~0.14.1 fetched in 98ms55 silly resolveWithNewModule @tensorflow/tfjs@0.14.1 checking installable status56 http fetch GET 304 https://registry.npmjs.org/https-proxy-agent 628ms (from cache)57 silly pacote range manifest for https-proxy-agent@^2.2.1 fetched in 713ms58 silly resolveWithNewModule https-proxy-agent@2.2.1 checking installable status59 http fetch GET 200 https://registry.npmjs.org/node-fetch 729ms60 silly pacote range manifest for node-fetch@^2.3.0 fetched in 738ms61 silly resolveWithNewModule node-fetch@2.3.0 checking installable status62 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs-layers 15ms (from cache)63 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs-converter 22ms (from cache)64 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs-core 23ms (from cache)65 silly pacote version manifest for @tensorflow/tfjs-layers@0.9.1 fetched in 24ms66 silly resolveWithNewModule @tensorflow/tfjs-layers@0.9.1 checking installable status67 silly pacote version manifest for @tensorflow/tfjs-converter@0.7.1 fetched in 31ms68 silly resolveWithNewModule @tensorflow/tfjs-converter@0.7.1 checking installable status69 silly pacote version manifest for @tensorflow/tfjs-core@0.14.2 fetched in 52ms70 silly resolveWithNewModule @tensorflow/tfjs-core@0.14.2 checking installable status71 http fetch GET 200 https://registry.npmjs.org/@tensorflow%2ftfjs-data 647ms72 silly pacote version manifest for @tensorflow/tfjs-data@0.1.4 fetched in 761ms73 silly resolveWithNewModule @tensorflow/tfjs-data@0.1.4 checking installable status74 http fetch GET 200 https://registry.npmjs.org/@types%2flong 12ms (from cache)75 http fetch GET 200 https://registry.npmjs.org/protobufjs 16ms (from cache)76 silly pacote range manifest for @types/long@~3.0.32 fetched in 22ms77 silly resolveWithNewModule @types/long@3.0.32 checking installable status78 silly pacote range manifest for protobufjs@~6.8.6 fetched in 22ms79 silly resolveWithNewModule protobufjs@6.8.8 checking installable status80 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2faspromise 15ms (from cache)81 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2fbase64 14ms (from cache)82 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2fcodegen 14ms (from cache)83 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2feventemitter 14ms (from cache)84 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2ffetch 14ms (from cache)85 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2ffloat 13ms (from cache)86 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2finquire 13ms (from cache)87 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2fpath 12ms (from cache)88 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2fpool 13ms (from cache)89 http fetch GET 200 https://registry.npmjs.org/@protobufjs%2futf8 14ms (from cache)90 silly pacote range manifest for @protobufjs/aspromise@^1.1.2 fetched in 19ms91 silly resolveWithNewModule @protobufjs/aspromise@1.1.2 checking installable status92 silly pacote range manifest for @protobufjs/fetch@^1.1.0 fetched in 18ms93 silly resolveWithNewModule @protobufjs/fetch@1.1.0 checking installable status94 silly pacote range manifest for @protobufjs/eventemitter@^1.1.0 fetched in 20ms95 silly resolveWithNewModule @protobufjs/eventemitter@1.1.0 checking installable status96 silly pacote range manifest for @protobufjs/codegen@^2.0.4 fetched in 21ms97 silly resolveWithNewModule @protobufjs/codegen@2.0.4 checking installable status98 silly pacote range manifest for @protobufjs/base64@^1.1.2 fetched in 22ms99 silly resolveWithNewModule @protobufjs/base64@1.1.2 checking installable status100 silly pacote range manifest for @protobufjs/float@^1.0.2 fetched in 20ms101 silly resolveWithNewModule @protobufjs/float@1.0.2 checking installable status102 silly pacote range manifest for @protobufjs/inquire@^1.1.0 fetched in 21ms103 silly resolveWithNewModule @protobufjs/inquire@1.1.0 checking installable status104 silly pacote range manifest for @protobufjs/path@^1.1.2 fetched in 21ms105 silly resolveWithNewModule @protobufjs/path@1.1.2 checking installable status106 silly pacote range manifest for @protobufjs/pool@^1.1.0 fetched in 21ms107 silly resolveWithNewModule @protobufjs/pool@1.1.0 checking installable status108 silly pacote range manifest for @protobufjs/utf8@^1.1.0 fetched in 21ms109 silly resolveWithNewModule @protobufjs/utf8@1.1.0 checking installable status110 silly pacote range manifest for @types/long@^4.0.0 fetched in 6ms111 silly resolveWithNewModule @types/long@4.0.0 checking installable status112 http fetch GET 200 https://registry.npmjs.org/long 6ms (from cache)113 http fetch GET 200 https://registry.npmjs.org/@types%2fnode 8ms (from cache)114 silly pacote range manifest for long@^4.0.0 fetched in 12ms115 silly resolveWithNewModule long@4.0.0 checking installable status116 silly pacote range manifest for @types/node@^10.1.0 fetched in 13ms117 silly resolveWithNewModule @types/node@10.12.18 checking installable status118 http fetch GET 200 https://registry.npmjs.org/@types%2fseedrandom 4ms (from cache)119 http fetch GET 200 https://registry.npmjs.org/@types%2fwebgl-ext 5ms (from cache)120 http fetch GET 200 https://registry.npmjs.org/@types%2fwebgl2 4ms (from cache)121 http fetch GET 200 https://registry.npmjs.org/seedrandom 4ms (from cache)122 silly pacote version manifest for @types/seedrandom@2.4.27 fetched in 6ms123 silly resolveWithNewModule @types/seedrandom@2.4.27 checking installable status124 silly pacote version manifest for @types/webgl-ext@0.0.30 fetched in 7ms125 silly resolveWithNewModule @types/webgl-ext@0.0.30 checking installable status126 silly pacote version manifest for @types/webgl2@0.0.4 fetched in 6ms127 silly resolveWithNewModule @types/webgl2@0.0.4 checking installable status128 silly pacote version manifest for seedrandom@2.4.3 fetched in 7ms129 silly resolveWithNewModule seedrandom@2.4.3 checking installable status130 http fetch GET 200 https://registry.npmjs.org/utf8 25ms131 silly pacote range manifest for node-fetch@~2.1.2 fetched in 85ms132 silly resolveWithNewModule node-fetch@2.1.2 checking installable status133 http fetch GET 200 https://registry.npmjs.org/@types%2fnode-fetch 135ms134 http fetch GET 200 https://registry.npmjs.org/utf8/-/utf8-2.1.2.tgz 89ms135 silly pacote range manifest for utf8@~2.1.2 fetched in 153ms136 silly resolveWithNewModule utf8@2.1.2 checking installable status137 silly pacote range manifest for @types/node-fetch@^2.1.2 fetched in 173ms138 silly resolveWithNewModule @types/node-fetch@2.1.4 checking installable status139 http fetch GET 200 https://registry.npmjs.org/debug 5ms (from cache)140 silly pacote range manifest for debug@^3.1.0 fetched in 53ms141 silly resolveWithNewModule debug@3.2.6 checking installable status142 http fetch GET 304 https://registry.npmjs.org/agent-base 178ms (from cache)143 silly pacote range manifest for agent-base@^4.1.0 fetched in 213ms144 silly resolveWithNewModule agent-base@4.2.1 checking installable status145 http fetch GET 304 https://registry.npmjs.org/es6-promisify 127ms (from cache)146 silly pacote range manifest for es6-promisify@^5.0.0 fetched in 144ms147 silly resolveWithNewModule es6-promisify@5.0.0 checking installable status148 http fetch GET 304 https://registry.npmjs.org/es6-promise 142ms (from cache)149 silly pacote range manifest for es6-promise@^4.0.3 fetched in 168ms150 silly resolveWithNewModule es6-promise@4.2.5 checking installable status151 http fetch GET 200 https://registry.npmjs.org/ms 16ms (from cache)152 silly pacote range manifest for ms@^2.1.1 fetched in 34ms153 silly resolveWithNewModule ms@2.1.1 checking installable status154 timing stage:loadIdealTree:loadAllDepsIntoIdealTree Completed in 2441ms155 timing stage:loadIdealTree Completed in 2583ms156 silly currentTree examples-nodejs156 silly currentTree ├── @protobufjs/aspromise@1.1.2156 silly currentTree ├── @protobufjs/base64@1.1.2156 silly currentTree ├── @protobufjs/codegen@2.0.4156 silly currentTree ├── @protobufjs/eventemitter@1.1.0156 silly currentTree ├── @protobufjs/fetch@1.1.0156 silly currentTree ├── @protobufjs/float@1.0.2156 silly currentTree ├── @protobufjs/inquire@1.1.0156 silly currentTree ├── @protobufjs/path@1.1.2156 silly currentTree ├── @protobufjs/pool@1.1.0156 silly currentTree ├── @protobufjs/utf8@1.1.0156 silly currentTree ├── @types/long@3.0.32156 silly currentTree ├── @types/seedrandom@2.4.27156 silly currentTree ├── @types/webgl-ext@0.0.30156 silly currentTree ├── @types/webgl2@0.0.4156 silly currentTree ├── abbrev@1.1.1156 silly currentTree ├── ansi-regex@2.1.1156 silly currentTree ├── aproba@1.2.0156 silly currentTree ├── are-we-there-yet@1.1.5156 silly currentTree ├── balanced-match@1.0.0156 silly currentTree ├── brace-expansion@1.1.11156 silly currentTree ├── canvas@2.2.0156 silly currentTree ├── chownr@1.1.1156 silly currentTree ├── code-point-at@1.1.0156 silly currentTree ├── concat-map@0.0.1156 silly currentTree ├── console-control-strings@1.1.0156 silly currentTree ├── core-util-is@1.0.2156 silly currentTree ├── debug@2.6.9156 silly currentTree ├── deep-extend@0.6.0156 silly currentTree ├── delegates@1.0.0156 silly currentTree ├── detect-libc@1.0.3156 silly currentTree ├── fs-minipass@1.2.5156 silly currentTree ├── fs.realpath@1.0.0156 silly currentTree ├── gauge@2.7.4156 silly currentTree ├── glob@7.1.3156 silly currentTree ├── has-unicode@2.0.1156 silly currentTree ├── iconv-lite@0.4.24156 silly currentTree ├── ignore-walk@3.0.1156 silly currentTree ├── inflight@1.0.6156 silly currentTree ├── inherits@2.0.3156 silly currentTree ├── ini@1.3.5156 silly currentTree ├── is-fullwidth-code-point@1.0.0156 silly currentTree ├── isarray@1.0.0156 silly currentTree ├── long@4.0.0156 silly currentTree ├── minimatch@3.0.4156 silly currentTree ├── minimist@0.0.8156 silly currentTree ├── minipass@2.3.5156 silly currentTree ├── minizlib@1.1.1156 silly currentTree ├── mkdirp@0.5.1156 silly currentTree ├── ms@2.0.0156 silly currentTree ├── nan@2.12.1156 silly currentTree ├── needle@2.2.4156 silly currentTree ├── node-pre-gyp@0.11.0156 silly currentTree ├── nopt@4.0.1156 silly currentTree ├── npm-bundled@1.0.5156 silly currentTree ├── npm-packlist@1.2.0156 silly currentTree ├── npmlog@4.1.2156 silly currentTree ├── number-is-nan@1.0.1156 silly currentTree ├── object-assign@4.1.1156 silly currentTree ├── once@1.4.0156 silly currentTree ├── os-homedir@1.0.2156 silly currentTree ├── os-tmpdir@1.0.2156 silly currentTree ├── osenv@0.1.5156 silly currentTree ├── path-is-absolute@1.0.1156 silly currentTree ├── process-nextick-args@2.0.0156 silly currentTree ├─┬ protobufjs@6.8.8156 silly currentTree │ └── @types/long@4.0.0156 silly currentTree ├─┬ rc@1.2.8156 silly currentTree │ └── minimist@1.2.0156 silly currentTree ├── readable-stream@2.3.6156 silly currentTree ├── rimraf@2.6.2156 silly currentTree ├── safe-buffer@5.1.2156 silly currentTree ├── safer-buffer@2.1.2156 silly currentTree ├── sax@1.2.4156 silly currentTree ├── seedrandom@2.4.3156 silly currentTree ├── semver@5.6.0156 silly currentTree ├── set-blocking@2.0.0156 silly currentTree ├── signal-exit@3.0.2156 silly currentTree ├── string_decoder@1.1.1156 silly currentTree ├── string-width@1.0.2156 silly currentTree ├── strip-ansi@3.0.1156 silly currentTree ├── strip-json-comments@2.0.1156 silly currentTree ├── tar@4.4.6156 silly currentTree ├── util-deprecate@1.0.2156 silly currentTree ├── wide-align@1.1.3156 silly currentTree ├── wrappy@1.0.2156 silly currentTree └── yallist@3.0.2157 silly idealTree examples-nodejs157 silly idealTree ├── @protobufjs/aspromise@1.1.2157 silly idealTree ├── @protobufjs/base64@1.1.2157 silly idealTree ├── @protobufjs/codegen@2.0.4157 silly idealTree ├── @protobufjs/eventemitter@1.1.0157 silly idealTree ├── @protobufjs/fetch@1.1.0157 silly idealTree ├── @protobufjs/float@1.0.2157 silly idealTree ├── @protobufjs/inquire@1.1.0157 silly idealTree ├── @protobufjs/path@1.1.2157 silly idealTree ├── @protobufjs/pool@1.1.0157 silly idealTree ├── @protobufjs/utf8@1.1.0157 silly idealTree ├── @tensorflow/tfjs-converter@0.7.1157 silly idealTree ├── @tensorflow/tfjs-core@0.14.2157 silly idealTree ├─┬ @tensorflow/tfjs-data@0.1.4157 silly idealTree │ └── node-fetch@2.1.2157 silly idealTree ├── @tensorflow/tfjs-layers@0.9.1157 silly idealTree ├── @tensorflow/tfjs-node@0.2.1157 silly idealTree ├── @tensorflow/tfjs@0.14.1157 silly idealTree ├── @types/long@3.0.32157 silly idealTree ├── @types/node-fetch@2.1.4157 silly idealTree ├── @types/node@10.12.18157 silly idealTree ├── @types/seedrandom@2.4.27157 silly idealTree ├── @types/webgl-ext@0.0.30157 silly idealTree ├── @types/webgl2@0.0.4157 silly idealTree ├── abbrev@1.1.1157 silly idealTree ├── adm-zip@0.4.13157 silly idealTree ├── agent-base@4.2.1157 silly idealTree ├── ansi-regex@2.1.1157 silly idealTree ├── aproba@1.2.0157 silly idealTree ├── are-we-there-yet@1.1.5157 silly idealTree ├── balanced-match@1.0.0157 silly idealTree ├── bindings@1.3.1157 silly idealTree ├── brace-expansion@1.1.11157 silly idealTree ├── canvas@2.2.0157 silly idealTree ├── chownr@1.1.1157 silly idealTree ├── code-point-at@1.1.0157 silly idealTree ├── concat-map@0.0.1157 silly idealTree ├── console-control-strings@1.1.0157 silly idealTree ├── core-util-is@1.0.2157 silly idealTree ├── debug@2.6.9157 silly idealTree ├── deep-extend@0.6.0157 silly idealTree ├── delegates@1.0.0157 silly idealTree ├── detect-libc@1.0.3157 silly idealTree ├── es6-promise@4.2.5157 silly idealTree ├── es6-promisify@5.0.0157 silly idealTree ├── fs-minipass@1.2.5157 silly idealTree ├── fs.realpath@1.0.0157 silly idealTree ├── gauge@2.7.4157 silly idealTree ├── glob@7.1.3157 silly idealTree ├── has-unicode@2.0.1157 silly idealTree ├─┬ https-proxy-agent@2.2.1157 silly idealTree │ ├── debug@3.2.6157 silly idealTree │ └── ms@2.1.1157 silly idealTree ├── iconv-lite@0.4.24157 silly idealTree ├── ignore-walk@3.0.1157 silly idealTree ├── inflight@1.0.6157 silly idealTree ├── inherits@2.0.3157 silly idealTree ├── ini@1.3.5157 silly idealTree ├── is-fullwidth-code-point@1.0.0157 silly idealTree ├── isarray@1.0.0157 silly idealTree ├── long@4.0.0157 silly idealTree ├── minimatch@3.0.4157 silly idealTree ├── minimist@0.0.8157 silly idealTree ├── minipass@2.3.5157 silly idealTree ├── minizlib@1.1.1157 silly idealTree ├── mkdirp@0.5.1157 silly idealTree ├── ms@2.0.0157 silly idealTree ├── nan@2.12.1157 silly idealTree ├── needle@2.2.4157 silly idealTree ├── node-fetch@2.3.0157 silly idealTree ├── node-pre-gyp@0.11.0157 silly idealTree ├── nopt@4.0.1157 silly idealTree ├── npm-bundled@1.0.5157 silly idealTree ├── npm-packlist@1.2.0157 silly idealTree ├── npmlog@4.1.2157 silly idealTree ├── number-is-nan@1.0.1157 silly idealTree ├── object-assign@4.1.1157 silly idealTree ├── once@1.4.0157 silly idealTree ├── os-homedir@1.0.2157 silly idealTree ├── os-tmpdir@1.0.2157 silly idealTree ├── osenv@0.1.5157 silly idealTree ├── path-is-absolute@1.0.1157 silly idealTree ├── process-nextick-args@2.0.0157 silly idealTree ├── progress@2.0.3157 silly idealTree ├─┬ protobufjs@6.8.8157 silly idealTree │ └── @types/long@4.0.0157 silly idealTree ├─┬ rc@1.2.8157 silly idealTree │ └── minimist@1.2.0157 silly idealTree ├── readable-stream@2.3.6157 silly idealTree ├── rimraf@2.6.2157 silly idealTree ├── safe-buffer@5.1.2157 silly idealTree ├── safer-buffer@2.1.2157 silly idealTree ├── sax@1.2.4157 silly idealTree ├── seedrandom@2.4.3157 silly idealTree ├── semver@5.6.0157 silly idealTree ├── set-blocking@2.0.0157 silly idealTree ├── signal-exit@3.0.2157 silly idealTree ├── string_decoder@1.1.1157 silly idealTree ├── string-width@1.0.2157 silly idealTree ├── strip-ansi@3.0.1157 silly idealTree ├── strip-json-comments@2.0.1157 silly idealTree ├── tar@4.4.6157 silly idealTree ├── utf8@2.1.2157 silly idealTree ├── util-deprecate@1.0.2157 silly idealTree ├── wide-align@1.1.3157 silly idealTree ├── wrappy@1.0.2157 silly idealTree └── yallist@3.0.2158 silly install generateActionsToTake159 timing stage:generateActionsToTake Completed in 12ms160 silly diffTrees action count 20161 silly diffTrees add @tensorflow/tfjs-converter@0.7.1162 silly diffTrees add @tensorflow/tfjs-core@0.14.2163 silly diffTrees add node-fetch@2.1.2164 silly diffTrees add @tensorflow/tfjs-layers@0.9.1165 silly diffTrees add @types/node@10.12.18166 silly diffTrees add @types/node-fetch@2.1.4167 silly diffTrees add es6-promise@4.2.5168 silly diffTrees add es6-promisify@5.0.0169 silly diffTrees add agent-base@4.2.1170 silly diffTrees add ms@2.1.1171 silly diffTrees add debug@3.2.6172 silly diffTrees add utf8@2.1.2173 silly diffTrees add @tensorflow/tfjs-data@0.1.4174 silly diffTrees add @tensorflow/tfjs@0.14.1175 silly diffTrees add adm-zip@0.4.13176 silly diffTrees add bindings@1.3.1177 silly diffTrees add https-proxy-agent@2.2.1178 silly diffTrees add node-fetch@2.3.0179 silly diffTrees add progress@2.0.3180 silly diffTrees add @tensorflow/tfjs-node@0.2.1181 silly decomposeActions action count 160182 silly decomposeActions fetch @tensorflow/tfjs-converter@0.7.1183 silly decomposeActions extract @tensorflow/tfjs-converter@0.7.1184 silly decomposeActions preinstall @tensorflow/tfjs-converter@0.7.1185 silly decomposeActions build @tensorflow/tfjs-converter@0.7.1186 silly decomposeActions install @tensorflow/tfjs-converter@0.7.1187 silly decomposeActions postinstall @tensorflow/tfjs-converter@0.7.1188 silly decomposeActions finalize @tensorflow/tfjs-converter@0.7.1189 silly decomposeActions refresh-package-json @tensorflow/tfjs-converter@0.7.1190 silly decomposeActions fetch @tensorflow/tfjs-core@0.14.2191 silly decomposeActions extract @tensorflow/tfjs-core@0.14.2192 silly decomposeActions preinstall @tensorflow/tfjs-core@0.14.2193 silly decomposeActions build @tensorflow/tfjs-core@0.14.2194 silly decomposeActions install @tensorflow/tfjs-core@0.14.2195 silly decomposeActions postinstall @tensorflow/tfjs-core@0.14.2196 silly decomposeActions finalize @tensorflow/tfjs-core@0.14.2197 silly decomposeActions refresh-package-json @tensorflow/tfjs-core@0.14.2198 silly decomposeActions fetch node-fetch@2.1.2199 silly decomposeActions extract node-fetch@2.1.2200 silly decomposeActions preinstall node-fetch@2.1.2201 silly decomposeActions build node-fetch@2.1.2202 silly decomposeActions install node-fetch@2.1.2203 silly decomposeActions postinstall node-fetch@2.1.2204 silly decomposeActions finalize node-fetch@2.1.2205 silly decomposeActions refresh-package-json node-fetch@2.1.2206 silly decomposeActions fetch @tensorflow/tfjs-layers@0.9.1207 silly decomposeActions extract @tensorflow/tfjs-layers@0.9.1208 silly decomposeActions preinstall @tensorflow/tfjs-layers@0.9.1209 silly decomposeActions build @tensorflow/tfjs-layers@0.9.1210 silly decomposeActions install @tensorflow/tfjs-layers@0.9.1211 silly decomposeActions postinstall @tensorflow/tfjs-layers@0.9.1212 silly decomposeActions finalize @tensorflow/tfjs-layers@0.9.1213 silly decomposeActions refresh-package-json @tensorflow/tfjs-layers@0.9.1214 silly decomposeActions fetch @types/node@10.12.18215 silly decomposeActions extract @types/node@10.12.18216 silly decomposeActions preinstall @types/node@10.12.18217 silly decomposeActions build @types/node@10.12.18218 silly decomposeActions install @types/node@10.12.18219 silly decomposeActions postinstall @types/node@10.12.18220 silly decomposeActions finalize @types/node@10.12.18221 silly decomposeActions refresh-package-json @types/node@10.12.18222 silly decomposeActions fetch @types/node-fetch@2.1.4223 silly decomposeActions extract @types/node-fetch@2.1.4224 silly decomposeActions preinstall @types/node-fetch@2.1.4225 silly decomposeActions build @types/node-fetch@2.1.4226 silly decomposeActions install @types/node-fetch@2.1.4227 silly decomposeActions postinstall @types/node-fetch@2.1.4228 silly decomposeActions finalize @types/node-fetch@2.1.4229 silly decomposeActions refresh-package-json @types/node-fetch@2.1.4230 silly decomposeActions fetch es6-promise@4.2.5231 silly decomposeActions extract es6-promise@4.2.5232 silly decomposeActions preinstall es6-promise@4.2.5233 silly decomposeActions build es6-promise@4.2.5234 silly decomposeActions install es6-promise@4.2.5235 silly decomposeActions postinstall es6-promise@4.2.5236 silly decomposeActions finalize es6-promise@4.2.5237 silly decomposeActions refresh-package-json es6-promise@4.2.5238 silly decomposeActions fetch es6-promisify@5.0.0239 silly decomposeActions extract es6-promisify@5.0.0240 silly decomposeActions preinstall es6-promisify@5.0.0241 silly decomposeActions build es6-promisify@5.0.0242 silly decomposeActions install es6-promisify@5.0.0243 silly decomposeActions postinstall es6-promisify@5.0.0244 silly decomposeActions finalize es6-promisify@5.0.0245 silly decomposeActions refresh-package-json es6-promisify@5.0.0246 silly decomposeActions fetch agent-base@4.2.1247 silly decomposeActions extract agent-base@4.2.1248 silly decomposeActions preinstall agent-base@4.2.1249 silly decomposeActions build agent-base@4.2.1250 silly decomposeActions install agent-base@4.2.1251 silly decomposeActions postinstall agent-base@4.2.1252 silly decomposeActions finalize agent-base@4.2.1253 silly decomposeActions refresh-package-json agent-base@4.2.1254 silly decomposeActions fetch ms@2.1.1255 silly decomposeActions extract ms@2.1.1256 silly decomposeActions preinstall ms@2.1.1257 silly decomposeActions build ms@2.1.1258 silly decomposeActions install ms@2.1.1259 silly decomposeActions postinstall ms@2.1.1260 silly decomposeActions finalize ms@2.1.1261 silly decomposeActions refresh-package-json ms@2.1.1262 silly decomposeActions fetch debug@3.2.6263 silly decomposeActions extract debug@3.2.6264 silly decomposeActions preinstall debug@3.2.6265 silly decomposeActions build debug@3.2.6266 silly decomposeActions install debug@3.2.6267 silly decomposeActions postinstall debug@3.2.6268 silly decomposeActions finalize debug@3.2.6269 silly decomposeActions refresh-package-json debug@3.2.6270 silly decomposeActions fetch utf8@2.1.2271 silly decomposeActions extract utf8@2.1.2272 silly decomposeActions preinstall utf8@2.1.2273 silly decomposeActions build utf8@2.1.2274 silly decomposeActions install utf8@2.1.2275 silly decomposeActions postinstall utf8@2.1.2276 silly decomposeActions finalize utf8@2.1.2277 silly decomposeActions refresh-package-json utf8@2.1.2278 silly decomposeActions fetch @tensorflow/tfjs-data@0.1.4279 silly decomposeActions extract @tensorflow/tfjs-data@0.1.4280 silly decomposeActions preinstall @tensorflow/tfjs-data@0.1.4281 silly decomposeActions build @tensorflow/tfjs-data@0.1.4282 silly decomposeActions install @tensorflow/tfjs-data@0.1.4283 silly decomposeActions postinstall @tensorflow/tfjs-data@0.1.4284 silly decomposeActions finalize @tensorflow/tfjs-data@0.1.4285 silly decomposeActions refresh-package-json @tensorflow/tfjs-data@0.1.4286 silly decomposeActions fetch @tensorflow/tfjs@0.14.1287 silly decomposeActions extract @tensorflow/tfjs@0.14.1288 silly decomposeActions preinstall @tensorflow/tfjs@0.14.1289 silly decomposeActions build @tensorflow/tfjs@0.14.1290 silly decomposeActions install @tensorflow/tfjs@0.14.1291 silly decomposeActions postinstall @tensorflow/tfjs@0.14.1292 silly decomposeActions finalize @tensorflow/tfjs@0.14.1293 silly decomposeActions refresh-package-json @tensorflow/tfjs@0.14.1294 silly decomposeActions fetch adm-zip@0.4.13295 silly decomposeActions extract adm-zip@0.4.13296 silly decomposeActions preinstall adm-zip@0.4.13297 silly decomposeActions build adm-zip@0.4.13298 silly decomposeActions install adm-zip@0.4.13299 silly decomposeActions postinstall adm-zip@0.4.13300 silly decomposeActions finalize adm-zip@0.4.13301 silly decomposeActions refresh-package-json adm-zip@0.4.13302 silly decomposeActions fetch bindings@1.3.1303 silly decomposeActions extract bindings@1.3.1304 silly decomposeActions preinstall bindings@1.3.1305 silly decomposeActions build bindings@1.3.1306 silly decomposeActions install bindings@1.3.1307 silly decomposeActions postinstall bindings@1.3.1308 silly decomposeActions finalize bindings@1.3.1309 silly decomposeActions refresh-package-json bindings@1.3.1310 silly decomposeActions fetch https-proxy-agent@2.2.1311 silly decomposeActions extract https-proxy-agent@2.2.1312 silly decomposeActions preinstall https-proxy-agent@2.2.1313 silly decomposeActions build https-proxy-agent@2.2.1314 silly decomposeActions install https-proxy-agent@2.2.1315 silly decomposeActions postinstall https-proxy-agent@2.2.1316 silly decomposeActions finalize https-proxy-agent@2.2.1317 silly decomposeActions refresh-package-json https-proxy-agent@2.2.1318 silly decomposeActions fetch node-fetch@2.3.0319 silly decomposeActions extract node-fetch@2.3.0320 silly decomposeActions preinstall node-fetch@2.3.0321 silly decomposeActions build node-fetch@2.3.0322 silly decomposeActions install node-fetch@2.3.0323 silly decomposeActions postinstall node-fetch@2.3.0324 silly decomposeActions finalize node-fetch@2.3.0325 silly decomposeActions refresh-package-json node-fetch@2.3.0326 silly decomposeActions fetch progress@2.0.3327 silly decomposeActions extract progress@2.0.3328 silly decomposeActions preinstall progress@2.0.3329 silly decomposeActions build progress@2.0.3330 silly decomposeActions install progress@2.0.3331 silly decomposeActions postinstall progress@2.0.3332 silly decomposeActions finalize progress@2.0.3333 silly decomposeActions refresh-package-json progress@2.0.3334 silly decomposeActions fetch @tensorflow/tfjs-node@0.2.1335 silly decomposeActions extract @tensorflow/tfjs-node@0.2.1336 silly decomposeActions preinstall @tensorflow/tfjs-node@0.2.1337 silly decomposeActions build @tensorflow/tfjs-node@0.2.1338 silly decomposeActions install @tensorflow/tfjs-node@0.2.1339 silly decomposeActions postinstall @tensorflow/tfjs-node@0.2.1340 silly decomposeActions finalize @tensorflow/tfjs-node@0.2.1341 silly decomposeActions refresh-package-json @tensorflow/tfjs-node@0.2.1342 silly install executeActions343 silly doSerial global-install 160344 verbose correctMkdir /home/Faiz/.npm/_locks correctMkdir not in flight, initializing345 verbose lock using /home/Faiz/.npm/_locks/staging-dd932a75574a907f.lock for /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging346 timing audit compress Completed in 3ms347 info audit Submitting payload of 7406bytes348 silly doParallel extract 20349 silly extract @tensorflow/tfjs-converter@0.7.1350 silly extract @tensorflow/tfjs-core@0.14.2351 silly extract node-fetch@2.1.2352 silly extract @tensorflow/tfjs-layers@0.9.1353 silly extract @types/node@10.12.18354 silly extract @types/node-fetch@2.1.4355 silly extract es6-promise@4.2.5356 silly extract es6-promisify@5.0.0357 silly extract agent-base@4.2.1358 silly extract ms@2.1.1359 silly extract debug@3.2.6360 silly extract utf8@2.1.2361 silly extract @tensorflow/tfjs-data@0.1.4362 silly extract @tensorflow/tfjs@0.14.1363 silly extract adm-zip@0.4.13364 silly extract bindings@1.3.1365 silly extract https-proxy-agent@2.2.1366 silly extract node-fetch@2.3.0367 silly extract progress@2.0.3368 silly extract @tensorflow/tfjs-node@0.2.1369 silly tarball trying @tensorflow/tfjs-converter@0.7.1 by hash: sha512-kMQqM3GI5bBl6YQ98jCJNu49NvAWGeI3iVxO3ZqOYtN90lb/+3dSBelDo2LHFXc8jnJHpFOwkFPSZlCVrVGRag==370 silly tarball trying @tensorflow/tfjs-core@0.14.2 by hash: sha512-VVbcu6H3ioKCkfkep/gQASfzPnQt3C5v+4ppH9pQ6Lf0lD+l3NMuMJYxa8Wjac1TfiWhFEX58bJvhpMfTGsUlg==371 silly tarball trying node-fetch@~2.1.2 by hash: sha1-q4hOjn5X44qUR1POxwb3iNF2i7U=372 silly tarball trying @tensorflow/tfjs-layers@0.9.1 by hash: sha512-TZTi59E3rGdVTJCf/AEX1arigPp0426FJLGfiKzTXp3skEuubwDM9XlwiXWcB5+l+Pjvwg4FMNXwEyajXIxX2w==373 silly tarball trying @types/node@^10.1.0 by hash: sha512-fh+pAqt4xRzPfqA6eh3Z2y6fyZavRIumvjhaCL753+TVkGKGhpPeyrJG2JftD0T9q4GF00KjefsQ+PQNDdWQaQ==374 silly tarball trying @types/node-fetch@^2.1.2 by hash: sha512-tR1ekaXUGpmzOcDXWU9BW73YfA2/VW1DF1FH+wlJ82BbCSnWTbdX+JkqWQXWKIGsFPnPsYadbXfNgz28g+ccWg==375 silly tarball trying es6-promise@^4.0.3 by hash: sha512-n6wvpdE43VFtJq+lUDYDBFUwV8TZbuGXLV4D6wKafg13ldznKsyEvatubnmUe31zcvelSzOHF+XbaT+Bl9ObDg==376 silly tarball trying es6-promisify@^5.0.0 by hash: sha1-UQnWLz5W6pZ8S2NQWu8IKRyKUgM=377 silly tarball trying agent-base@^4.1.0 by hash: sha512-JVwXMr9nHYTUXsBFKUqhJwvlcYU/blreOEUkhNR2eXZIvwd+c+o5V4MgDPKWnMS/56awN3TRzIP+KoPn+roQtg==378 silly tarball trying ms@^2.1.1 by hash: sha512-tgp+dl5cGk28utYktBsrFqA7HKgrhgPsg6Z/EfhWI4gl1Hwq8B/GmY/0oXZ6nF8hDVesS/FpnYaD/kOWhYQvyg==379 silly tarball trying debug@^3.1.0 by hash: sha512-mel+jf7nrtEl5Pn1Qx46zARXKDpBbvzezse7p7LqINmdoIk8PYP5SySaxEmYv6TZ0JyEKA1hsCId6DIhgITtWQ==380 silly tarball trying utf8@~2.1.2 by hash: sha1-H6DZJw6b6FDZsFAn9jUZv0ZFfZY=381 silly tarball trying @tensorflow/tfjs-data@0.1.4 by hash: sha512-YwaWNZnJj++QFEHQ1AKLqn2fvnmSp1X6CJ5YL5XJhq+m8P0AoouW9IpumCgO6WSjnD1M83/cVGZXzDIgJ4IlLg==382 silly tarball trying @tensorflow/tfjs@~0.14.1 by hash: sha512-fo68B4FNh/JBYAjArlLFsx2etE1kxCHG11bzsy01b2jZLrM9HE+jgmiNUGuHqN7aLu/CVqB76wDAtlBT4AnPsg==383 silly tarball trying adm-zip@^0.4.11 by hash: sha512-fERNJX8sOXfel6qCBCMPvZLzENBEhZTzKqg6vrOW5pvoEaQuJhRU4ndTAh6lHOxn1I6jnz2NHra56ZODM751uw==384 silly tarball trying bindings@~1.3.0 by hash: sha512-i47mqjF9UbjxJhxGf+pZ6kSxrnI3wBLlnGI2ArWJ4r0VrvDS7ZYXkprq/pLaBWYq4GM0r4zdHY+NNRqEMU7uew==385 silly tarball trying https-proxy-agent@^2.2.1 by hash: sha512-HPCTS1LW51bcyMYbxUIOO4HEOlQ1/1qRaFWcyxvwaqUS9TY88aoEuHUY33kuAh1YhVVaDQhLZsnPd+XNARWZlQ==386 silly tarball trying node-fetch@^2.3.0 by hash: sha512-MOd8pV3fxENbryESLgVIeaGKrdl+uaYhCSSVkjeOb/31/njTpcis5aWfdqgNlHIrKOLRbMnfPINPOML2CIFeXA==387 silly tarball trying progress@^2.0.0 by hash: sha512-7PiHtLll5LdnKIMw100I+8xJXR5gW2QwWYkT6iJva0bXitZKa/XMrSbdmg3r2Xnaidz9Qumd0VPaMrZlF9V9sA==388 silly tarball trying @tensorflow/tfjs-node@latest by hash: sha512-cFSY+zJCfSb5mXSZdH/kEMkhVsPz9UmnTx6ahC9oe/1kIZhSrhs7nPMMyH/Q+RYJ6DEUhPKK40hy0TlUf4Tmqw==389 silly tarball no local data for node-fetch@~2.1.2. Extracting by manifest.390 silly tarball no local data for @tensorflow/tfjs-layers@0.9.1. Extracting by manifest.391 silly tarball no local data for @types/node-fetch@^2.1.2. Extracting by manifest.392 silly tarball no local data for @tensorflow/tfjs-data@0.1.4. Extracting by manifest.393 silly tarball no local data for @tensorflow/tfjs@~0.14.1. Extracting by manifest.394 silly tarball no local data for node-fetch@^2.3.0. Extracting by manifest.395 silly tarball no local data for @tensorflow/tfjs-node@latest. Extracting by manifest.396 silly tarball no local data for @tensorflow/tfjs-converter@0.7.1. Extracting by manifest.397 silly extract bindings@~1.3.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/bindings-99adb094 (300ms)398 silly extract ms@^2.1.1 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/ms-8d03503f (302ms)399 silly extract utf8@~2.1.2 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/utf8-7b55e10a (303ms)400 silly extract es6-promisify@^5.0.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/es6-promisify-593752c3 (304ms)401 silly extract progress@^2.0.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/progress-2aa9ae8c (303ms)402 silly extract agent-base@^4.1.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/agent-base-c40e4212 (307ms)403 silly extract https-proxy-agent@^2.2.1 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/https-proxy-agent-282f062d (304ms)404 silly extract debug@^3.1.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/debug-7aa2eeaa (307ms)405 silly extract adm-zip@^0.4.11 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/adm-zip-e70502d1 (308ms)406 silly extract @types/node@^10.1.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@types/node-c11d9d70 (313ms)407 silly extract es6-promise@^4.0.3 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/es6-promise-e968b849 (315ms)408 http fetch GET 200 https://registry.npmjs.org/node-fetch/-/node-fetch-2.1.2.tgz 144ms409 silly extract node-fetch@~2.1.2 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/node-fetch-1e98cb80 (396ms)410 http fetch GET 200 https://registry.npmjs.org/node-fetch/-/node-fetch-2.3.0.tgz 228ms411 silly extract node-fetch@^2.3.0 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/node-fetch-a7b61fa5 (480ms)412 silly extract @tensorflow/tfjs-core@0.14.2 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-core-95cc8544 (595ms)413 http fetch GET 200 https://registry.npmjs.org/@types/node-fetch/-/node-fetch-2.1.4.tgz 1048ms414 silly extract @types/node-fetch@^2.1.2 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@types/node-fetch-a02445ab (1302ms)415 timing audit submit Completed in 1753ms416 http fetch POST 200 https://registry.npmjs.org/-/npm/v1/security/audits/quick 1752ms417 timing audit body Completed in 0ms418 http fetch GET 200 https://registry.npmjs.org/@tensorflow/tfjs-node/-/tfjs-node-0.2.1.tgz 2417ms419 silly extract @tensorflow/tfjs-node@latest extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-node-41fb56c6 (2671ms)420 http fetch GET 200 https://registry.npmjs.org/@tensorflow/tfjs-data/-/tfjs-data-0.1.4.tgz 4185ms421 silly extract @tensorflow/tfjs-data@0.1.4 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-data-1dcc6297 (4439ms)422 http fetch GET 200 https://registry.npmjs.org/@tensorflow/tfjs-converter/-/tfjs-converter-0.7.1.tgz 4693ms423 silly extract @tensorflow/tfjs-converter@0.7.1 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-converter-d84ccb6d (4984ms)424 http fetch GET 200 https://registry.npmjs.org/@tensorflow/tfjs-layers/-/tfjs-layers-0.9.1.tgz 6831ms425 silly extract @tensorflow/tfjs-layers@0.9.1 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-layers-b9a68292 (7081ms)426 http fetch GET 200 https://registry.npmjs.org/@tensorflow/tfjs/-/tfjs-0.14.1.tgz 8855ms427 silly extract @tensorflow/tfjs@~0.14.1 extracted to /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging/@tensorflow/tfjs-b8d18bde (9106ms)428 timing action:extract Completed in 9115ms429 silly doReverseSerial unbuild 160430 silly doSerial remove 160431 silly doSerial move 160432 silly doSerial finalize 160433 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-converter434 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-core435 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-data/node_modules/node-fetch436 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-layers437 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@types/node438 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@types/node-fetch439 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/es6-promise440 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/es6-promisify441 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/agent-base442 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules/ms443 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules/debug444 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/utf8445 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-data446 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs447 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/adm-zip448 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/bindings449 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent450 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/node-fetch451 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/progress452 silly finalize /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node453 timing action:finalize Completed in 27ms454 silly doParallel refresh-package-json 20455 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-converter456 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-core457 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-data/node_modules/node-fetch458 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-layers459 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@types/node460 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@types/node-fetch461 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/es6-promise462 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/es6-promisify463 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/agent-base464 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules/ms465 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules/debug466 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/utf8467 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-data468 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs469 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/adm-zip470 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/bindings471 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent472 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/node-fetch473 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/progress474 silly refresh-package-json /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node475 timing action:refresh-package-json Completed in 30ms476 silly doParallel preinstall 20477 silly preinstall @tensorflow/tfjs-converter@0.7.1478 info lifecycle @tensorflow/tfjs-converter@0.7.1~preinstall: @tensorflow/tfjs-converter@0.7.1479 silly preinstall @tensorflow/tfjs-core@0.14.2480 info lifecycle @tensorflow/tfjs-core@0.14.2~preinstall: @tensorflow/tfjs-core@0.14.2481 silly preinstall node-fetch@2.1.2482 info lifecycle node-fetch@2.1.2~preinstall: node-fetch@2.1.2483 silly preinstall @tensorflow/tfjs-layers@0.9.1484 info lifecycle @tensorflow/tfjs-layers@0.9.1~preinstall: @tensorflow/tfjs-layers@0.9.1485 silly preinstall @types/node@10.12.18486 info lifecycle @types/node@10.12.18~preinstall: @types/node@10.12.18487 silly preinstall @types/node-fetch@2.1.4488 info lifecycle @types/node-fetch@2.1.4~preinstall: @types/node-fetch@2.1.4489 silly preinstall es6-promise@4.2.5490 info lifecycle es6-promise@4.2.5~preinstall: es6-promise@4.2.5491 silly preinstall es6-promisify@5.0.0492 info lifecycle es6-promisify@5.0.0~preinstall: es6-promisify@5.0.0493 silly preinstall agent-base@4.2.1494 info lifecycle agent-base@4.2.1~preinstall: agent-base@4.2.1495 silly preinstall ms@2.1.1496 info lifecycle ms@2.1.1~preinstall: ms@2.1.1497 silly preinstall debug@3.2.6498 info lifecycle debug@3.2.6~preinstall: debug@3.2.6499 silly preinstall utf8@2.1.2500 info lifecycle utf8@2.1.2~preinstall: utf8@2.1.2501 silly preinstall @tensorflow/tfjs-data@0.1.4502 info lifecycle @tensorflow/tfjs-data@0.1.4~preinstall: @tensorflow/tfjs-data@0.1.4503 silly preinstall @tensorflow/tfjs@0.14.1504 info lifecycle @tensorflow/tfjs@0.14.1~preinstall: @tensorflow/tfjs@0.14.1505 silly preinstall adm-zip@0.4.13506 info lifecycle adm-zip@0.4.13~preinstall: adm-zip@0.4.13507 silly preinstall bindings@1.3.1508 info lifecycle bindings@1.3.1~preinstall: bindings@1.3.1509 silly preinstall https-proxy-agent@2.2.1510 info lifecycle https-proxy-agent@2.2.1~preinstall: https-proxy-agent@2.2.1511 silly preinstall node-fetch@2.3.0512 info lifecycle node-fetch@2.3.0~preinstall: node-fetch@2.3.0513 silly preinstall progress@2.0.3514 info lifecycle progress@2.0.3~preinstall: progress@2.0.3515 silly preinstall @tensorflow/tfjs-node@0.2.1516 info lifecycle @tensorflow/tfjs-node@0.2.1~preinstall: @tensorflow/tfjs-node@0.2.1517 timing action:preinstall Completed in 3ms518 silly doSerial build 160519 silly build @tensorflow/tfjs-converter@0.7.1520 info linkStuff @tensorflow/tfjs-converter@0.7.1521 silly linkStuff @tensorflow/tfjs-converter@0.7.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules522 silly build @tensorflow/tfjs-core@0.14.2523 info linkStuff @tensorflow/tfjs-core@0.14.2524 silly linkStuff @tensorflow/tfjs-core@0.14.2 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules525 silly build node-fetch@2.1.2526 info linkStuff node-fetch@2.1.2527 silly linkStuff node-fetch@2.1.2 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-data/node_modules as its parent node_modules528 silly build @tensorflow/tfjs-layers@0.9.1529 info linkStuff @tensorflow/tfjs-layers@0.9.1530 silly linkStuff @tensorflow/tfjs-layers@0.9.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules531 silly build @types/node@10.12.18532 info linkStuff @types/node@10.12.18533 silly linkStuff @types/node@10.12.18 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules534 silly build @types/node-fetch@2.1.4535 info linkStuff @types/node-fetch@2.1.4536 silly linkStuff @types/node-fetch@2.1.4 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules537 silly build es6-promise@4.2.5538 info linkStuff es6-promise@4.2.5539 silly linkStuff es6-promise@4.2.5 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules540 silly build es6-promisify@5.0.0541 info linkStuff es6-promisify@5.0.0542 silly linkStuff es6-promisify@5.0.0 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules543 silly build agent-base@4.2.1544 info linkStuff agent-base@4.2.1545 silly linkStuff agent-base@4.2.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules546 silly build ms@2.1.1547 info linkStuff ms@2.1.1548 silly linkStuff ms@2.1.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules as its parent node_modules549 silly build debug@3.2.6550 info linkStuff debug@3.2.6551 silly linkStuff debug@3.2.6 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/https-proxy-agent/node_modules as its parent node_modules552 silly build utf8@2.1.2553 info linkStuff utf8@2.1.2554 silly linkStuff utf8@2.1.2 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules555 silly build @tensorflow/tfjs-data@0.1.4556 info linkStuff @tensorflow/tfjs-data@0.1.4557 silly linkStuff @tensorflow/tfjs-data@0.1.4 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules558 silly build @tensorflow/tfjs@0.14.1559 info linkStuff @tensorflow/tfjs@0.14.1560 silly linkStuff @tensorflow/tfjs@0.14.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules561 silly build adm-zip@0.4.13562 info linkStuff adm-zip@0.4.13563 silly linkStuff adm-zip@0.4.13 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules564 silly build bindings@1.3.1565 info linkStuff bindings@1.3.1566 silly linkStuff bindings@1.3.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules567 silly build https-proxy-agent@2.2.1568 info linkStuff https-proxy-agent@2.2.1569 silly linkStuff https-proxy-agent@2.2.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules570 silly build node-fetch@2.3.0571 info linkStuff node-fetch@2.3.0572 silly linkStuff node-fetch@2.3.0 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules573 silly build progress@2.0.3574 info linkStuff progress@2.0.3575 silly linkStuff progress@2.0.3 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules576 silly build @tensorflow/tfjs-node@0.2.1577 info linkStuff @tensorflow/tfjs-node@0.2.1578 silly linkStuff @tensorflow/tfjs-node@0.2.1 has /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules as its parent node_modules579 timing action:build Completed in 3ms580 silly doSerial global-link 160581 silly doParallel update-linked 0582 silly doSerial install 160583 silly install @tensorflow/tfjs-converter@0.7.1584 info lifecycle @tensorflow/tfjs-converter@0.7.1~install: @tensorflow/tfjs-converter@0.7.1585 silly install @tensorflow/tfjs-core@0.14.2586 info lifecycle @tensorflow/tfjs-core@0.14.2~install: @tensorflow/tfjs-core@0.14.2587 silly install node-fetch@2.1.2588 info lifecycle node-fetch@2.1.2~install: node-fetch@2.1.2589 silly install @tensorflow/tfjs-layers@0.9.1590 info lifecycle @tensorflow/tfjs-layers@0.9.1~install: @tensorflow/tfjs-layers@0.9.1591 silly install @types/node@10.12.18592 info lifecycle @types/node@10.12.18~install: @types/node@10.12.18593 silly install @types/node-fetch@2.1.4594 info lifecycle @types/node-fetch@2.1.4~install: @types/node-fetch@2.1.4595 silly install es6-promise@4.2.5596 info lifecycle es6-promise@4.2.5~install: es6-promise@4.2.5597 silly install es6-promisify@5.0.0598 info lifecycle es6-promisify@5.0.0~install: es6-promisify@5.0.0599 silly install agent-base@4.2.1600 info lifecycle agent-base@4.2.1~install: agent-base@4.2.1601 silly install ms@2.1.1602 info lifecycle ms@2.1.1~install: ms@2.1.1603 silly install debug@3.2.6604 info lifecycle debug@3.2.6~install: debug@3.2.6605 silly install utf8@2.1.2606 info lifecycle utf8@2.1.2~install: utf8@2.1.2607 silly install @tensorflow/tfjs-data@0.1.4608 info lifecycle @tensorflow/tfjs-data@0.1.4~install: @tensorflow/tfjs-data@0.1.4609 silly install @tensorflow/tfjs@0.14.1610 info lifecycle @tensorflow/tfjs@0.14.1~install: @tensorflow/tfjs@0.14.1611 silly install adm-zip@0.4.13612 info lifecycle adm-zip@0.4.13~install: adm-zip@0.4.13613 silly install bindings@1.3.1614 info lifecycle bindings@1.3.1~install: bindings@1.3.1615 silly install https-proxy-agent@2.2.1616 info lifecycle https-proxy-agent@2.2.1~install: https-proxy-agent@2.2.1617 silly install node-fetch@2.3.0618 info lifecycle node-fetch@2.3.0~install: node-fetch@2.3.0619 silly install progress@2.0.3620 info lifecycle progress@2.0.3~install: progress@2.0.3621 silly install @tensorflow/tfjs-node@0.2.1622 info lifecycle @tensorflow/tfjs-node@0.2.1~install: @tensorflow/tfjs-node@0.2.1623 verbose lifecycle @tensorflow/tfjs-node@0.2.1~install: unsafe-perm in lifecycle true624 verbose lifecycle @tensorflow/tfjs-node@0.2.1~install: PATH: /usr/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node/node_modules/.bin:/home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.bin:/home/Faiz/.npm-global/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/home/Faiz/.config/composer/vendor/bin:/home/Faiz/.local/bin:/home/Faiz/bin:/home/Faiz/.config/composer/vendor/bin625 verbose lifecycle @tensorflow/tfjs-node@0.2.1~install: CWD: /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node626 silly lifecycle @tensorflow/tfjs-node@0.2.1~install: Args: [ '-c', 'node scripts/install.js' ]627 silly lifecycle @tensorflow/tfjs-node@0.2.1~install: Returned: code: 1  signal: null628 info lifecycle @tensorflow/tfjs-node@0.2.1~install: Failed to exec install script629 timing action:install Completed in 43260ms630 verbose unlock done using /home/Faiz/.npm/_locks/staging-dd932a75574a907f.lock for /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs/node_modules/.staging631 timing stage:rollbackFailedOptional Completed in 123ms632 timing stage:runTopLevelLifecycles Completed in 55512ms633 silly saveTree examples-nodejs633 silly saveTree ├─┬ @tensorflow/tfjs-node@0.2.1633 silly saveTree │ ├─┬ @tensorflow/tfjs@0.14.1633 silly saveTree │ │ ├─┬ @tensorflow/tfjs-converter@0.7.1633 silly saveTree │ │ │ ├── @types/long@3.0.32633 silly saveTree │ │ │ └─┬ protobufjs@6.8.8633 silly saveTree │ │ │   ├── @protobufjs/aspromise@1.1.2633 silly saveTree │ │ │   ├── @protobufjs/base64@1.1.2633 silly saveTree │ │ │   ├── @protobufjs/codegen@2.0.4633 silly saveTree │ │ │   ├── @protobufjs/eventemitter@1.1.0633 silly saveTree │ │ │   ├─┬ @protobufjs/fetch@1.1.0633 silly saveTree │ │ │   │ └── @protobufjs/inquire@1.1.0633 silly saveTree │ │ │   ├── @protobufjs/float@1.0.2633 silly saveTree │ │ │   ├── @protobufjs/inquire@1.1.0633 silly saveTree │ │ │   ├── @protobufjs/path@1.1.2633 silly saveTree │ │ │   ├── @protobufjs/pool@1.1.0633 silly saveTree │ │ │   ├── @protobufjs/utf8@1.1.0633 silly saveTree │ │ │   ├── @types/long@4.0.0633 silly saveTree │ │ │   ├── @types/node@10.12.18633 silly saveTree │ │ │   └── long@4.0.0633 silly saveTree │ │ ├─┬ @tensorflow/tfjs-core@0.14.2633 silly saveTree │ │ │ ├── @types/seedrandom@2.4.27633 silly saveTree │ │ │ ├── @types/webgl-ext@0.0.30633 silly saveTree │ │ │ ├── @types/webgl2@0.0.4633 silly saveTree │ │ │ └── seedrandom@2.4.3633 silly saveTree │ │ ├─┬ @tensorflow/tfjs-data@0.1.4633 silly saveTree │ │ │ ├── @types/node-fetch@2.1.4633 silly saveTree │ │ │ ├── node-fetch@2.1.2633 silly saveTree │ │ │ └── utf8@2.1.2633 silly saveTree │ │ └── @tensorflow/tfjs-layers@0.9.1633 silly saveTree │ ├── adm-zip@0.4.13633 silly saveTree │ ├── bindings@1.3.1633 silly saveTree │ ├─┬ https-proxy-agent@2.2.1633 silly saveTree │ │ ├─┬ agent-base@4.2.1633 silly saveTree │ │ │ └─┬ es6-promisify@5.0.0633 silly saveTree │ │ │   └── es6-promise@4.2.5633 silly saveTree │ │ └─┬ debug@3.2.6633 silly saveTree │ │   └── ms@2.1.1633 silly saveTree │ ├── node-fetch@2.3.0633 silly saveTree │ ├── progress@2.0.3633 silly saveTree │ ├─┬ rimraf@2.6.2633 silly saveTree │ │ └─┬ glob@7.1.3633 silly saveTree │ │   ├── fs.realpath@1.0.0633 silly saveTree │ │   ├─┬ inflight@1.0.6633 silly saveTree │ │   │ ├─┬ once@1.4.0633 silly saveTree │ │   │ │ └── wrappy@1.0.2633 silly saveTree │ │   │ └── wrappy@1.0.2633 silly saveTree │ │   ├── inherits@2.0.3633 silly saveTree │ │   ├─┬ minimatch@3.0.4633 silly saveTree │ │   │ └─┬ brace-expansion@1.1.11633 silly saveTree │ │   │   ├── balanced-match@1.0.0633 silly saveTree │ │   │   └── concat-map@0.0.1633 silly saveTree │ │   ├── once@1.4.0633 silly saveTree │ │   └── path-is-absolute@1.0.1633 silly saveTree │ └─┬ tar@4.4.6633 silly saveTree │   ├── chownr@1.1.1633 silly saveTree │   ├─┬ fs-minipass@1.2.5633 silly saveTree │   │ └─┬ minipass@2.3.5633 silly saveTree │   │   ├── safe-buffer@5.1.2633 silly saveTree │   │   └── yallist@3.0.2633 silly saveTree │   ├── minipass@2.3.5633 silly saveTree │   ├── minizlib@1.1.1633 silly saveTree │   ├─┬ mkdirp@0.5.1633 silly saveTree │   │ └── minimist@0.0.8633 silly saveTree │   ├── safe-buffer@5.1.2633 silly saveTree │   └── yallist@3.0.2633 silly saveTree └─┬ canvas@2.2.0633 silly saveTree   ├── nan@2.12.1633 silly saveTree   └─┬ node-pre-gyp@0.11.0633 silly saveTree     ├── detect-libc@1.0.3633 silly saveTree     ├─┬ needle@2.2.4633 silly saveTree     │ ├─┬ debug@2.6.9633 silly saveTree     │ │ └── ms@2.0.0633 silly saveTree     │ ├─┬ iconv-lite@0.4.24633 silly saveTree     │ │ └── safer-buffer@2.1.2633 silly saveTree     │ └── sax@1.2.4633 silly saveTree     ├─┬ nopt@4.0.1633 silly saveTree     │ ├── abbrev@1.1.1633 silly saveTree     │ └─┬ osenv@0.1.5633 silly saveTree     │   ├── os-homedir@1.0.2633 silly saveTree     │   └── os-tmpdir@1.0.2633 silly saveTree     ├─┬ npm-packlist@1.2.0633 silly saveTree     │ ├── ignore-walk@3.0.1633 silly saveTree     │ └── npm-bundled@1.0.5633 silly saveTree     ├─┬ npmlog@4.1.2633 silly saveTree     │ ├─┬ are-we-there-yet@1.1.5633 silly saveTree     │ │ ├── delegates@1.0.0633 silly saveTree     │ │ └─┬ readable-stream@2.3.6633 silly saveTree     │ │   ├── core-util-is@1.0.2633 silly saveTree     │ │   ├── isarray@1.0.0633 silly saveTree     │ │   ├── process-nextick-args@2.0.0633 silly saveTree     │ │   ├── string_decoder@1.1.1633 silly saveTree     │ │   └── util-deprecate@1.0.2633 silly saveTree     │ ├── console-control-strings@1.1.0633 silly saveTree     │ ├─┬ gauge@2.7.4633 silly saveTree     │ │ ├── aproba@1.2.0633 silly saveTree     │ │ ├── has-unicode@2.0.1633 silly saveTree     │ │ ├── object-assign@4.1.1633 silly saveTree     │ │ ├── signal-exit@3.0.2633 silly saveTree     │ │ ├─┬ string-width@1.0.2633 silly saveTree     │ │ │ ├── code-point-at@1.1.0633 silly saveTree     │ │ │ ├─┬ is-fullwidth-code-point@1.0.0633 silly saveTree     │ │ │ │ └── number-is-nan@1.0.1633 silly saveTree     │ │ │ └─┬ strip-ansi@3.0.1633 silly saveTree     │ │ │   └── ansi-regex@2.1.1633 silly saveTree     │ │ ├── strip-ansi@3.0.1633 silly saveTree     │ │ └── wide-align@1.1.3633 silly saveTree     │ └── set-blocking@2.0.0633 silly saveTree     ├─┬ rc@1.2.8633 silly saveTree     │ ├── deep-extend@0.6.0633 silly saveTree     │ ├── ini@1.3.5633 silly saveTree     │ ├── minimist@1.2.0633 silly saveTree     │ └── strip-json-comments@2.0.1633 silly saveTree     └── semver@5.6.0634 warn examples-nodejs No description635 warn examples-nodejs No repository field.636 verbose stack Error: @tensorflow/tfjs-node@0.2.1 install: `node scripts/install.js`636 verbose stack Exit status 1636 verbose stack     at EventEmitter.<anonymous> (/usr/lib/node_modules/npm/node_modules/npm-lifecycle/index.js:301:16)636 verbose stack     at emitTwo (events.js:126:13)636 verbose stack     at EventEmitter.emit (events.js:214:7)636 verbose stack     at ChildProcess.<anonymous> (/usr/lib/node_modules/npm/node_modules/npm-lifecycle/lib/spawn.js:55:14)636 verbose stack     at emitTwo (events.js:126:13)636 verbose stack     at ChildProcess.emit (events.js:214:7)636 verbose stack     at maybeClose (internal/child_process.js:915:16)636 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5)637 verbose pkgid @tensorflow/tfjs-node@0.2.1638 verbose cwd /home/Faiz/GitRepos/face-api.js/examples/examples-nodejs639 verbose Linux 4.19.13-200.fc28.x86_64640 verbose argv ""/usr/bin/node"" ""/usr/bin/npm"" ""install"" ""--save"" ""@tensorflow/tfjs-node""641 verbose node v8.12.0642 verbose npm  v6.4.1643 error code ELIFECYCLE644 error errno 1645 error @tensorflow/tfjs-node@0.2.1 install: `node scripts/install.js`645 error Exit status 1646 error Failed at the @tensorflow/tfjs-node@0.2.1 install script.646 error This is probably not a problem with npm. There is likely additional logging output above.647 verbose exit [ 1, true ]```";['If you have issues installing @tensorflow/tfjs-node, please ask at tfjs for help with troubleshooting.=====']
https://github.com/justadudewhohacks/face-api.js/issues/183;is there a central repo for these weights?;2;closed;2019-01-05T15:40:08Z;2019-01-06T17:14:53Z;hello!happy new year!my question is, there are these weights. are this are generated by you. or is there a central repo to load these weights or i have to copy them into my project to make it work?;['Hi, as stated in the readme, the SSD, MTCNN and face recognition models come from external repos, I just ported them to tfjs. The other models have been trained by myself.In general you only copy the models that you need for your application and serve them as static assets.=====', 'ok, thanks=====']
https://github.com/justadudewhohacks/face-api.js/issues/182;Latest build broke detection: detector never resolves;3;closed;2019-01-04T06:59:11Z;2019-01-06T07:20:37Z;The latest changes have broken face-api for us. The last commit to work is 68ff853005635c4669c6452bd7bbee6a312e3dc0. Essentially the detector promise never resolves.We setup this way (all weights are loaded):```var options = new window.faceapi.TinyFaceDetectorOptions(),var detector = window.faceapi.detectSingleFace(video, options).withFaceLandmarks().withFaceDescriptor(),```Then we call```detector.run().then(function(results){.... }))```This used to work before. However, if we remove `.withFaceLandmarks().withFaceDescriptor()` it **does** work with latest commit.We tried rebuilding .. to no avail. ;"[""Hmm odd, I can not reproduce this issue. Are you sure this doesn't result in an unhandled Promise rejection? Did you load all the models?Maybe you could set up a small repo for me to reproduce this issue?====="", 'Thanks so much for responding! You are amazing! Yes, all the models are loaded. I have a feeling it could be a tensorflow issue or we are not handling promise rejection properly with the manual approach vs await.Will require some serious debugging on our side. Will update this issue when we figure it out.=====', ""We figure it out!We were creating the `detector` before the `video` started streaming (it worked before, so we had no reason to assume it would break with new tensorflow image lib). Want us to help with the doc in the README so it's clear to other devs?=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/178;Registration of backend webgl failed;4;closed;2019-01-02T08:46:17Z;2020-09-24T13:58:06Z;"Nice to meet you Author,I use it in my html,it`s my code ：`<html><head><title>我的第一个 HTML 页面</title></head><body><img id=""myImg"" src=""~/Content/scripts/jquery/445e2c1c0e833b46ff9e12e89e9ddebb.jpg"" /><script src=""~/Content/scripts/jquery/face-api.js""></script><script>   async function init () {        const detections1 = await faceapi.detectFaceLandmarksTiny(faceImage),        console.log(detections),    })init()</body></html>`Nothing can be change , in my Google Chrome `s f12 show me ![1546418554 1](https://user-images.githubusercontent.com/26179617/50584953-85e4b480-0ead-11e9-98aa-525caab7dfa4.png)i didn`t know what`s happen.but i use npm i,npm start ,it`s great.please help me,thank you very much.                                                                                                                                    Yours              zt";['Hmm, did you check, whether your device is able to run the WebGL backend? The error message comes from tfjs when initializing your environment once you first load the script.=====', 'I have this issue on a old device with intel graphics (Haswell) but which still can run other webgl examples (three.js).=====', 'I am also having this issue while using it on Moto G3 mobile=====', 'i have this on linux with nvidia 1070 maxq (mobile) gpu (currently used for webgl)error:* firefox 80 (linux)* brave Version 1.14.81 Chromium: 85.0.4183.102 (Official Build) (64-bit)working correctly:* chromium Version 85.0.4183.121 (Official Build) snap (64-bit)=====']
https://github.com/justadudewhohacks/face-api.js/issues/175;how can i retrain a bunch of people of my family and the faceapi would recognize/classify the faces?;2;closed;2018-12-27T17:15:31Z;2018-12-27T21:11:45Z;Hello!How are you?first of all this api is super cool!!!2nd, it is cool that i can extract an area of box there is an actual face. now after i found the face, how can i classify the face if it is papa, mama and the other members of the family? i see that in the api, although the example shows in the big bang theory you can classify the guys, while how that it should be used to do it?thanks a lot,patrik;"['Hi,You don\'t retrain the model, you compute the face descriptors of the faces and then compare their similarity with euclidean distance. Have a look at the ""faceRecognition"" examples or check out the [introduction article](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07).=====', 'ok , thanks=====']"
https://github.com/justadudewhohacks/face-api.js/issues/174;how can i recognize faces instead of canvas instead of simple buffer in node?;7;closed;2018-12-27T17:09:21Z;2018-12-28T16:38:54Z;hello,how are you?i am trying to create a home made recognition system, that can open the door , turn on the lamps etc ...but if i am on raspberry pi , i probably wont be able to use the canvas npm and it has a lot overhead. is there any way to give the image input a simple buffer?thanks for your response,patrik;"['Hi,Sure you can pass a tensor instead of a canvas as input to the neural nets as well.=====', 'so you think if i create a 3d matrix is enough - RGB?=====', ""Yes, the nets expect RGB input, so you can create either tensors of shape [height, width, channels] or [batchSize, height, width, channels] for batch input. Note batch inputs don't work for the face detection nets currently.====="", 'ciao,so do you think if i only want to use on server i should rather use opencv4nodejs or face-api.js is still better to use?=====', 'opencv4nodejs is for general image processing. If you only need face detection, recognition, classification etc. use face-api.js=====', 'ok,  and another question, i do not have to train at all? one image and the face distance is enough, right? =====', 'Exactly.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/172;Suggestions on emotion detection (I'm guessing using landmarks?);3;closed;2018-12-20T17:09:02Z;2019-01-17T09:16:08Z;"[Google Vision API](https://cloud.google.com/vision/docs/reference/rest/v1p3beta1/images/annotate#FaceAnnotation) has a nice  ""joyLikelihood""  Has anyone done the same with landmarks, or is it an entirely separate model that usually works on expressions?I'm trying to ""find the best smile"" out of a series of images, didn't want to reinvent wheels.";"['I am currently training a model for facial expression/emotion recognition, landmarks are not sufficient for that task maybe they are sufficient for simply detecting smiling gestures.=====', 'Yay!  Hope it trains well!  Really rough ""this is an open source project and nothing is guaranteed because everyone is doing it out of the kindness of their hearts"" best-case ETA?Tangent: I wonder if landmarks + the face descriptor would result in anything good.   I imagine the face descriptor to be a ""baseline"" of the face\'s underlying structure, then the landmarks are what the face is currently doing, which kinda-sorta-maybe represents expression.  And orientation, so that is tough.  So a `landmarks.orientationNormalized() - faceDescriptor = expressionData` sorta thing.=====', '> this is an open source project and nothing is guaranteed because everyone is doing it out of the kindness of their heartsAnd out of passion :)Still trying out different architectures and improving the dataset. The models I trained so far didn\'t achieve more than 70 - 80% accuracy on the test set so I am still trying. Seems it\'s not an easy task to distinguish between certain expressions such as fear and surprise or sadness and anger. But atleast the accuracy for happiness/smiling usually is about 95%.> best-case ETAI hope by the end of next week I will have published something or atleast pushed some demos somewhere to play around with.> I imagine the face descriptor to be a ""baseline"" of the face\'s underlying structureHmm, that\'s a vague statement, I am not sure whether you can draw reasonable conclusions about facial expressions from the face descriptor. The face descriptor is simply a feature vector and the net that produces those has been trained to output descriptors such, that they have minimal distance to descriptors of faces of the same subjects and maximal distance to faces of different subjects.But I could imagine that a combination of the probability for happiness from the upcoming face expression predictor + the landmark positions of the mouth region would do the trick for your usecase.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/171;Custom labels when using FullFaceDescription;3;closed;2018-12-17T10:16:43Z;2019-03-23T11:19:58Z;For now I can either use LabeledFaceDescription (without getting the box info) or FullFaceDescription (without getting a label). I need to display the recognized user (from a detection box) and a label above (the username basically). Is there a way to do it?;"['There is no utility for more complex drawing of text other then a single line of text, so you will have to draw it manually. There is also [faceapi.drawText](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/dom/drawText.ts), which you could use.=====', ""@justadudewhohacks okay, how can I get the label from the faceMatcher.findBestMatch() call? When you create the faceapi.FaceMatcher, it accepts either the Array<LabeledFaceDescription> (which doesn't have a box I can use, only descriptors) or Array<FullFaceDescription> (which has the detection and a box inside, but labels are auto-generated). Or am I missing something?Thanks in advance for the answer anyway!====="", ""`faceMatcher.findBestMatch()` returns a [FaceMatch](https://github.com/justadudewhohacks/face-api.js/blob/master/src/classes/FaceMatch.ts), which has a label and distance property.The labels are assigned automatically if you don't label your detection results manually, e.g. by passing an array of [LabeledFaceDescriptors](https://github.com/justadudewhohacks/face-api.js/blob/master/src/classes/LabeledFaceDescriptors.ts).=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/169;Improve accuracy and stabilization?;11;closed;2018-12-14T16:26:20Z;2019-03-23T11:21:03Z;is there any easy way to improve to the accuracy and stabilize landmarks? trying with TINY_FACE_DETECTOR on mobile, sometimes landmarks go out of the face.;"['There are no parameters to tune the landmark prediction process, if that was your question.=====', 'Okay .. are you planning for any release to address this? thanks =====', 'Can you show some screenshots please of what you mean by ""sometimes landmarks go out of the face.""?=====', '![capture3](https://user-images.githubusercontent.com/19422397/50052970-ed330280-0152-11e9-83e5-c04bd475ec77.PNG)Here you can see the landmarks are bit misplaced.=====', 'Hmm strange, they really look a bit off, are you using the default or the tiny model?Also keep in mind that the models are designed to run at realtime rather than providing state of the art accuracy.=====', 'tiny_face_detector_model-shard1 and face_landmark_68_model-shard1=====', 'I observed this usually happens when the face is near to camera.=====', 'is there any solution to this?=====', ""Also depends on the input size of the tiny face detector and how large the face appears in your image. What's the input size you are using for the face detector? Try increasing the input size a bit, maybe the faces are just too small, which will make them become blurry inputs.====="", 'Increasing input size to 224 has improved the landmarks positions. But quite misplaced at the edges when I turn right. And the landmarks are bit shaky. The face covers more than 1/4 of camera preview.  thanks=====', 'Closing because the actual question of this issue seems to be answered.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/168;Uncaught (in promise) DOMException: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source width is 0.;6;closed;2018-12-13T23:58:19Z;2019-04-08T05:25:46Z;My loop was running happily, but consistently dies on this one image.Should getContext2dOrThrow also be checking for 0 width canvas?```Uncaught (in promise) DOMException: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source width is 0.    at http://localhost:8080/face-api.js:2145:55    at Array.map (<anonymous>)    at http://localhost:8080/face-api.js:2141:53    at step (http://localhost:8080/face-api.js:326:27)    at Object.next (http://localhost:8080/face-api.js:307:57)    at http://localhost:8080/face-api.js:300:75    at new Promise (<anonymous>)    at __awaiter$1 (http://localhost:8080/face-api.js:296:16)    at extractFaces (http://localhost:8080/face-api.js:2110:16)    at ComputeAllFaceDescriptorsTask.<anonymous> (http://localhost:8080/face-api.js:5318:54)```;"['Check the rectangles that you are passing to extractFaces, seems they are not correct.=====', 'I am passing them directly from     let fullFaceDescriptions = await faceapi.detectAllFaces(                    smallCanvas,                    new faceapi.SsdMobilenetv1Options({maxResults: 100})                ).withFaceLandmarks().withFaceDescriptors(),Is it possible that `faceapi.detectAllFaces()` is returning a bad rectangle?=====', 'Hmm that shouldnt be the case. Can you log the rectangle dimensions and your Image dimensions. Also could provide a repo for me to reproduce the issue?Edit.: Sorry just saw in the stacktrace that its occuring inside the task chain. Could you first of all share the image for which this issue occurs?=====', ""I can't share it publicly, sorry, don't know all the people in the picture.  But if I can get it to repro on a picture that I have full rights to, will do so.====="", 'Closing, if this is still an issue feel free to reopen and share some information about how to reproduce this.=====', ""It's a Cross-domain issue with WebGL textures.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/166;Does JS (and face-api) benefit if you run the model loading in parallel?;2;closed;2018-12-11T18:45:52Z;2018-12-13T19:05:31Z;Maybe more of a general understanding of JS, but if I loaded the models using what I *think* is parallel calls:    await Promise.all([        faceapi.loadSsdMobilenetv1Model(MODEL_URL),        faceapi.loadFaceLandmarkModel(MODEL_URL),        faceapi.loadFaceRecognitionModel(MODEL_URL)    ]),does that help any?1. Yes, it is like loading multiple HTML tags, the browser is smart about doing it in parallel.2. No, that isn't how JS rolls, and it would take too long to explain.3. It is going to get cached anyhow, don't worry about it.4. Other:);"[""It shouldn't hurt to do it that way and could be slightly faster, not sure to be honest. I guess the main limiting factor still is your bandwidth.After the models have been cached, there should be no difference.====="", ""Ok, then nbd (but good to know that they don't have to be loaded in any particular order!)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/165;Detecting open mouth from webcam feed;5;closed;2018-12-11T16:59:03Z;2019-03-23T11:23:38Z;Hey, thanks so much for sharing this project, it's awesome!Just wondering what's the easiest way to detect when a user opens and closes their mouth? I've used `landmarks.getMouth()` and logged the result into the console, but I can't find any documentation for which points relate to which parts of the mouth, so I'm not sure where to start calculating.Cheers;"['Have a look at this [figure](https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg). Using `landmarks.getMouth()` you simply get an array of Points 49 to 68.=====', 'Perfect, thanks=====', 'The co-ordinates in the `getMouth()` array don\'t seem to bear much relation to the actual position of the mouth on the face being tracked. I\'ve added the following:```setInterval(() => {  mouth = landmarks.getMouth()  if (mouth) {    checker = document.getElementById(\'pointChecker\')    checker.innerHTML = \'\'    mouth.map(function(point){      span = document.createElement(\'span\')      checker.appendChild(span)      span.style.left = point.x +\'px\'      span.style.top = point.y +\'px\'    })  }}, 100)```But the results don\'t match up to the mouth on the face being tracked by the draw function:<img width=""409"" alt=""screen shot 2018-12-12 at 16 07 25"" src=""https://user-images.githubusercontent.com/23400003/49882443-92cf3480-fe28-11e8-819c-d68acf6d6a2e.png"">=====', 'Are you sure your styles are not simply messed uo, is the parent absolutely positioned?landmarks.getMouth() is used in the drawing function, that is used to draw the outline of the face (green) so the coordinates should be right.=====', 'Closing, if this is still an issue feel free to update and reopen it.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/164;Can´t get nodejs examples working;6;closed;2018-12-09T18:25:13Z;2018-12-12T18:11:57Z;Hello, can't figure out how to run examples for nodejs. Browser examples works fine.  Followed instructions in readme:>git clone https://github.com/justadudewhohacks/face-api.js.git>>cd face-api.js/examples/examples-nodejs>npm i>> tsc faceDetections>Output: https://pastebin.com/huEH2RVQ>and>ts-node faceDetection.ts>Output: https://pastebin.com/7AFfM6kiMachine is Ubuntu 16,04 on AWS. ;"['Right, i have that npm not installed. How can i fix this?=====', '@ollfa Check out the [tsconfig](https://github.com/justadudewhohacks/face-api.js/blob/master/tsconfig.json) of this repo. Looks like a typescript issue.Second link looks like an issue with tfjs. Are you using the same tfjs-core version as face-api.js (0.13.8)?@persone16 npm comes with nodejs.=====', 'I think my problem connected with python version, i have installed 3+, but for this need lower than 3 version=====', ""You shouldn't need python to run the nodejs examples.You only need python (or pip for that matter) to install @tensorflow/tfjs-node, but I am also using python 3.6 and it works fine.====="", 'It installed, then i deleted from variable path python 3.7., and then i installed python 2.7.=====', 'Solved, thanks! Was using wrong version of tfjs-core. =====']"
https://github.com/justadudewhohacks/face-api.js/issues/163;Huge waiting time;2;closed;2018-12-09T17:35:52Z;2019-01-17T09:17:42Z;It just take 8 seconds to recognise the reference image with Tiny Face Detector... any ideas why it takes THAT long? :/;"['Because your computer sucks. =====', ""Can you be a bit more precise about what takes so long? The tiny face detector doesn't do any face recognition, it simply detects face bounding boxes.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/154;"SOLVED: Webpack warning about not found ""fs"" module";8;closed;2018-11-30T20:55:27Z;2021-04-16T10:37:40Z;"I get the following warning when building my webapp with webpack:```[1] WARNING in ../node_modules/tfjs-image-recognition-base/build/es6/env/initialize.js[1] Module not found: Error: Can't resolve 'fs' in '/Users/username/project/node_modules/tfjs-image-recognition-base/build/es6/env'[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/env/initialize.js[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/env/index.js[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/index.js[1]  @ ../node_modules/face-api.js/build/es6/index.js[1]  @ ./src/index.tsx```I had to add the following to my webpack config to suppress the warning:```node: {   fs: ""empty""}```It's weird because the target in my webpack config is set to `web`.Am I doing something wrong? If not, this issue may help other people with the same warning.";"[""Thanks for reporting your solution!It's because webpack sees the require('fs') in the code, but although it won't be required in a browser environment. face-api.js is not only a library for the browser anymore, if you are running in a nodejs environment fs will be required and used for reading model files from disk.====="", ""Gotcha. Do you think there's a way to suppress the warning on the library side so that webpack doesn't complain?====="", ""Thanks for your awesome work btw, it's really impressive how easy it is to implement facial detection using your library. Love it 🙏====="", ""Hmm not sure if it's possible from the side of the library. The issue is the conditional require, since webpack cannot evaluate the condition. So I think your solution is the correct one, telling webpack, that it is safe to ignore the fs module.Edit: The only option I see, is not to require fs at all and monkeyPatch fs into the environment manually, as done with the canvas implementation etc.====="", ""Alright, I don't have time to investigate that monkeyPatch thing so I'll just close the issue for now. Thanks again.====="", ""to ignore the error. add this code on your webpack.mix.js`mix.webpackConfig({    node: {        fs: 'empty',    }})`====="", 'For webpack 5 you need:```jsresolve: {    fallback: {      fs: false    }  }```This really should be fixed in this lib though. Having to do this in my webpack config is clunky.=====', '@maccuaa your solution worked for me. The warning went away. For anyone curious as to how the resolve option fixes the issue, please read the webpack docs https://webpack.js.org/configuration/resolve/#resolvefallback=====']"
https://github.com/justadudewhohacks/face-api.js/issues/150;Using face detection on archived videos with NodeJS;5;closed;2018-11-27T18:02:30Z;2019-01-17T09:21:20Z;Sorry if I'm missing something in the documentation but I'm not quite sure how would I extract and match faces on a video located somewhere in the server? Since it's expecting HTML elements and node canvas doesn't seem to be compatible with a function that loads a video. ;"['You will have to read and decode the frames of the video and create tensors out of the image data using a third party package. For example opencv4nodejs (which is probably overkill for simply reading video) or node-ffmpeg.=====', ""Thank you very much, ffmpeg was the package I was thinking about since I later need to do some video manipulation anyway :) in a similar question, would it be preferred to convert the image directly into a tensor or loading each one to the canvas with the loadImage function? I can't seem to find any decent comparison in performance and I'm not in power to do a full test right now.====="", 'Sorry for late reply, if you get the decoded image data using ffmpeg then there is no need to convert it to a canvas, you can straight create a tensor from the data, which should also be faster. The main reason I use the canvas package in nodejs is because of the decoding of images.=====', 'Yes, you forgot the `await` keyword, so detections in your case is a Promise:`const detections = await faceapi.detectAllFaces(videoElement, options)`=====', 'Thank you very much. I figured that out and deleted the comment before refreshing the page(before seeing your reply). Never thought you would reply immediately!@justadudewhohacks=====']"
https://github.com/justadudewhohacks/face-api.js/issues/149;How can I start just with tinyFaceDetector?;3;closed;2018-11-27T03:06:24Z;2018-12-04T06:46:49Z;seems all code packing up into face-api, so how can I get this 190k tiny face detector package?;['after study,I still have no idea how to start with tiny face detector... I can not found manifest.json...=====', 'You can get all the weights and manifest files are [here](https://github.com/justadudewhohacks/face-api.js/tree/master/weights). The files with the prefix `tiny_face_detector` is what you want to get.=====', 'well,I got it,thank you=====']
https://github.com/justadudewhohacks/face-api.js/issues/148;CreateCanvas crashing in react js;3;closed;2018-11-26T05:29:50Z;2019-01-17T09:21:28Z;Uncaught (in promise) TypeError: Illegal constructor    at eval (index.js:19)    at createCanvas (createCanvas.js:15)    at createCanvasFromMedia (createCanvas.js:26)    at eval (NetInput.js:43)    at Array.forEach (<anonymous>)    at new NetInput (NetInput.js:28)    at eval (toNetInput.js:59)    at step (tslib.es6.js:117)    at Object.eval [as next] (tslib.es6.js:98)    at fulfilled (tslib.es6.js:88);"['Could you share a repo for me to reproduce this?Also:- in which browser does this occur?- seems you are not using the latest version of face-api.js, which version are you using?=====', ""Am not sure that whether am using it correctly .I have reproduced it in [issue148](https://github.com/ashithrahul/issue148)when i add this import '@tensorflow/tfjs-node',am getting this errorUncaught TypeError: util_1.promisify is not a function    at Object.eval (file_system.js:42)    at eval (file_system.js:297)    at Object../node_modules/@tensorflow/tfjs-node/dist/io/file_system.js (index_bundle.js:877)    at __webpack_require__ (index_bundle.js:725)    at fn (index_bundle.js:102)    at eval (index.js:5)    at Object../node_modules/@tensorflow/tfjs-node/dist/index.js (index_bundle.js:865)    at __webpack_require__ (index_bundle.js:725)    at fn (index_bundle.js:102)    at eval (App.js:23)if remove this import  am getting above mentioned errorAm using chrome Version 70.0.3538.110 (Official Build) (64-bit)====="", ""Oh I see. You are not supposed to use @tensorflow/tfjs-node nor node-canvas in a react application, since your app won't run in a nodejs environment, but in the browser.Remove those dependencies and the faceapi.env.monkeyPatch call from your App.js.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/147;How to find similar faces in the database?;4;closed;2018-11-22T10:08:24Z;2019-01-17T09:21:37Z;"Hello!I have the photo№1 with need face. And i have a base of 100 photos. Among these 100 photos there is a photo with the original face from 1 photo.How to implement the mechanism of search for similar persons in the database? It is possible?I need to collect the descriptors of all these photos in database?`const fullFaceDescription = await faceapi.detectSingleFace(input).withFaceLandmarks().withFaceDescriptor()`And then compare the descriptors with the descriptor photo №1 (distance)? Or i need use ""Face Recognition by Matching Descriptors""?Tell me how to do please. I want to understand the algorithm of the necessary actions.Thank you advance!";"['I dont fully understand the question, but you want to compute the descriptor for every face in the database and then compare them by computing the euclidean distance between the descriptor of your photo and the descriptors from your database. As a threshold value to detwrmine whether two faces are similar you can use 0.6.=====', ""@justadudewhohacks Sorry for my bad English..I'll try to explain betterI have a photo of my face. I want to find photos with similar faces among 100 other photos. But I want to keep these 100 photos in my database. I have database with links to 100 images and their descriptors. What is the fastest way (method) to find let's say 5 most similar photos from 100?Like on schema![Schema Face](http://t.torly.ru/schemface.png)====="", '@justadudewhohacks Is it possible to store descriptors in the database?=====', 'This library provides you the means to decide, whether two faces show the same person or not, by computing the euclidean distance of their descriptors. From their on you will have to figure out an algorithm to solve your problem, given the means that face-api.js provides.Regarding your second question: A descriptor is nothing more than a Float32Array with 128 values, so yes you can store them in a database.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/146;Request;2;closed;2018-11-21T15:12:39Z;2018-11-26T12:32:16Z;faceapi.allFacesMtcnnis there any function to get single face mtcnn not all faces available ???i working on face recognition login database and i need to get ONLY 1 face;['Did you read the readme? Check out detectSingleFace.=====', 'this works, I have used it always returns the most prominent person in the shot await faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor(),=====']
https://github.com/justadudewhohacks/face-api.js/issues/145;Loading labeledFaceDescriptors into React state from backend;2;closed;2018-11-21T12:06:19Z;2018-11-22T12:12:22Z;"Hi Vincent,I am trying to make a React frontend with facial recognition and want to send the labeledFaceDescriptors from a backend into state when the page loads. When the labeledFaceDescriptor is made it console logs as follows and can be used by the faceMatcher:[LabeledFaceDescriptors, LabeledFaceDescriptors, LabeledFaceDescriptors]0: LabeledFaceDescriptors {_label: ""milo"", _descriptors: Array(1)}1: LabeledFaceDescriptors {_label: ""rick"", _descriptors: Array(1)}2: LabeledFaceDescriptors {_label: ""teo"", _descriptors: Array(1)}However when I save this on the backend and send this through to the frontend is comes out as follows and faceMatcher will not use is despite the structure being the same:(3) [{…}, {…}, {…}]0: {_label: ""milo"", _descriptors: Array(1)}1: {_label: ""rick"", _descriptors: Array(1)}2: {_label: ""teo"", _descriptors: Array(1)}I think the issue has something to do with it losing the LabeledFaceDescriptors and it being replaced with  {…}.Is it possible to get around this anyway?ThanksRick";['I have got past this issue now, thanks anyway=====', 'Okay, just for future reference:. You can not simply use object rest spead (... operator) or object.assign to serialize a class instance.=====']
https://github.com/justadudewhohacks/face-api.js/issues/144;Face detection don't work on AWS Lambda due to node-canvas not working on node 8.10;24;closed;2018-11-21T00:38:45Z;2021-09-09T15:44:50Z;Hi there!I am trying to get this up and running on AWS Lambda but are having trouble as it seem the 'canvas' package is broken on node 8.10. I am trying to get some face detection going.https://github.com/Automattic/node-canvas/issues/1252#issuecomment-437598572It looks like its kind of a pain to try and run any other version of node on AWS Lambda, so I started trying to use it without the canvas package. As you mention in the README and I also noticed that the faceapi.locateFaces() function also takes a Tensor4D class as input. I have never used Tensorflow and I am a little confused as how to turn a ArrayBuffer from axios into a correctly shaped Tensor4D object.I am fetching a jpeg image using axios.I found the tf.tensor4d function but not sure what shape and dtype it should be.Do you have any idea?My code so far:```jsconst { data: imageBuffer } = await axios.get(url, {	responseType: 'arraybuffer'})const imageTensor = tf.tensor4d(imageBuffer, [?, ?, ? ,?])const faces = await detector.locateFaces(imageTensor)```Error messages look similar to this one:`Error: Based on the provided shape, [1,2,3,4], and dtype float32, the tensor should have 24 values but has 68695`Any help is greatly appreciated!;"[""Hey man, hows it going?Ohh that's a pity, node-canvas is obviously the easiest way to use face-api.js with node.Well you can either pass in tf.Tensor3Ds or tf.Tensor4Ds with a batch size of 1:`const imageTensor = tf.tensor3d(imageBuffer, [height, width, channels])`, where channels should be 3 and in RGB order.However, the imageBuffer you receive from axios is most likely jpeg or png encoded right? You have to create the image tensor from the raw image data, so you will have to decode the data first. There are probably npm packages, which can do that for you. For example I have seen people using ffmpeg for this.====="", ""Thanks!Tried the get-pixels npm package and it works!get-pixels return a 4 channel array all the time, even for jpegs 🤷 , so had to remove the alpha channel.```typescriptconst pixels = await new Promise<Pixels>(resolve => {\tgetPixels(url, (err, pixels: Pixels) => {\t\tif (err) {\t\t\tthrow err\t\t}\t\tconsole.log('pixels:', pixels)\t\tresolve(pixels)\t})})// remove alpha channelconst RGBValues = []pixels.data.forEach((px, i) => {\tif ((i + 1) % 4 != 0) {\t\tRGBValues.push(px)\t}})const imageTensor = tf.tensor3d(RGBValues, [pixels.shape[1], pixels.shape[0], 3]) as anyconst faces = await detector.locateFaces(imageTensor)```Now to the task of getting it up on Lambda without breaking their code size limit, wish me luck!====="", ""> Thanks!> > Tried the get-pixels npm package and it works!> > get-pixels return a 4 channel array all the time, even for jpegs 🤷 , so had to remove the alpha channel.> > ```ts> const pixels = await new Promise<Pixels>(resolve => {> \tgetPixels(url, (err, pixels: Pixels) => {> \t\tif (err) {> \t\t\tthrow err> \t\t}> \t\tconsole.log('pixels:', pixels)> \t\tresolve(pixels)> \t})> })> > // remove alpha channel> const RGBValues = []> pixels.data.forEach((px, i) => {> \tif ((i + 1) % 4 != 0) {> \t\tRGBValues.push(px)> \t}> })> const imageTensor = tf.tensor3d(RGBValues, [pixels.shape[1], pixels.shape[0], 3]) as any> const faces = await detector.locateFaces(imageTensor)> ```> > Now to the task of getting it up on Lambda without breaking their code size limit, wish me luck!Thanks, that helped me. But it's quite slowly (1000ms) compared to canvas ( < 10ms ).====="", 'tensorflow team is working on a solution: https://github.com/tensorflow/tfjs/issues/298#issuecomment-442263569=====', 'Nice!=====', 'It wasn\'t easy, but I finally got it running in a Lambda on AWS. The tfjs-node package includes a native library and has to be built using the correct environment. I achieved this using Docker and the `lambci/lambda:nodejs8.10` image from the https://github.com/lambci/docker-lambda project. This image seem to be one the best images to simulate the Lambda environment.The built tfjs-node module includes a 110mb binary file. `@tensorflow/tfjs-node/deps/lib/libtensorflow.so`The Lambda size limit is 250mb (uncompressed).Well, with 140mb left for other modules it should be fine right? I tried deploying the package using `serverless deploy` resulting in this cryptic error message.`ENOENT: no such file or directory, open \'/Users/bobmoff/Projects/picular/serverless-face-api/node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so\'`The file exists alright, but its a symlink that points to `libtensorflow.so -> /var/task/node_modules/@tensorflow/tfjs-node/deps/lib/libtensorflow.so`Ahh, the symlink was created inside the docker container and it is a not a relative link. Ok, so i delete the symlink and created a new one that is relative.`libtensorflow.so -> ../../deps/lib/libtensorflow.so`But zipping up the package, using `serverless deploy` results in a zip that when uncompressed is 343mb ?? Using Disk Inventory X on the uncompressed folder I saw this:<img width=""965"" alt=""two-binaries"" src=""https://user-images.githubusercontent.com/1033236/50040999-e40c3d80-004d-11e9-9b6c-a6ba5e412ae5.png"">Why are there 2 large binaries? Shouldn\'t one of them just be a symlink ? The symlink is gone, and replaced with a copy of the binary. Hmm. After a bit of research I learned that the default behaviour when zipping symlinks is that they get ""resolved"" (link is followed and original content is copied). Ok. But there is a --symlinks flag for the zip program that can modify this behaviour to keep symlinks as they are, instead of resolving them. This solved the size issue, nut now I realised that I depend on a local project that is linked that actually need to be resolved when packaged. I couldn\'t find any way to specify what folder/file should be resolved or not when packaging through serverless. There is a way to exclude files/folders from package though. I excluded the symlink.```yamlpackage:    exclude:        - node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so```So using the following commands I first package the project (excluding the symlink) and then manually include the symlink again into the zip archive. (`-y` is the shorthand for `--symlinks`)```sls package --package my-artifactszip -y my-artifacts/serverless-face-api.zip node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so```Now I have a package that is 233mb! Yeah, 17mb to spare! Hehe. Ofc there are more size reduction that can be made be excluding more stuff, like excluding unused weights/models from face-api etc.. But I was just happy to get below the limit.Happy as a unicorn on christmas I deployed my neat ""little"" package to AWS.`sls deploy --package my-artifacts`.. and lived happily ever after. =====', ""`libtensorflow.so` appears to be 183mb now! don't suppose you could share your `package.json` for version numbers?====="", 'Sure, here are my deps, haven upgraded in a while :)```javascript{    ""@tensorflow/tfjs-node"": ""^0.2.1"",    ""axios"": ""^0.18.0"",    ""face-api.js"": ""^0.17.1"",    ""get-pixels"": ""^3.3.2"",    ""lodash"": ""^4.17.11"",    ""module-alias"": ""^2.1.0"",    ""moment"": ""^2.23.0"",    ""monk"": ""^6.0.6"",    ""url-join"": ""^4.0.0""}```=====', 'The tensorflow package (version 1.2.3) is now 294MB!  `libtensorflow.so.1.14.0` is now 216MB, and `libtensorflow_framework.so.1.14.0` is another 35MB. Pretty sure there is no way to get this package into Lambda. @MatthewAlner did you manage to get it working with a newer version? Or do I need to roll all the way back to the versions used by @bobmoff=====', 'Perhaps you could try to deploy using https://www.openfaas.com/ =====', ""I didn't in the end 😞====="", 'Thanks for all the helpful info in this issue, and of course thanks again to @justadudewhohacks . I did eventually manage to get it working on Lambda, but it wasn\'t easy. I ran into all the same problems that @bobmoff did, and if I\'d read his post more carefully at the start I would have saved myself a lot of time (in particular the `libtensorflow.so` linking issue). I\'m using the Node 10.x runtime.The newer versions of `tfjs-node` just get bigger and bigger, especially their bundled `.so` files. There is no way I could find to make the newer versions fit within Lambda\'s 250MB limit, so I had to roll way back to `v0.2.1`. You also need to roll `face-api.js` back to a similarly old version, as it seems `face-api.js` is coupled to a specific version of `tfjs-node`. Below is my deps:```  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^0.2.1"",    ""face-api.js"": ""^0.17.1"",    ""get-pixels"": ""^3.3.2""  }```If you really wanted to use the latest version of Tensorflow and Face-api, I suspect you could use a technique like this https://github.com/lucleray/tensorflow-lambda, which basically uploads a zipped copy of the dependencies, and then unzips them into `/tmp` before running them. There is obviously overhead for unzipping the files, but if your lambda instances are often reused then that initial overhead fades away.=====', ""Hey @bobmoff, just wondering if you had any issues with memory leaks in your setup? My face detection lambdas get reused most of the time because they get called so often, and I've noticed the memory used slowly grows with each invocation, until it finally breaks the Lambda memory limit and starts again. No idea if it's `get-pixels`, `tfjs-node` or `face-api.js` that is the issue.====="", 'Howdy @henrydawson. No, sorry. I havent experienced that, but that is probably due to the fact that I havent had the pressure like you seem to have, so my containers/lambdas have probably been recycled before that happenes.Sorry to not be of any more help :/=====', 'Hi @henrydawson and all,just some feedback on canvas running on lambda and also a question. I\'ve managed to make canvas run on lambda by using https://github.com/lambci/docker-lambda to make some tests. Thanks to the ""build"" image I had to run a bash shell on the docker build image to:1/ run npm install 2/ copy 3 libraries from the docker build image /lib64 to the root of my project: libblkid.so.1, libmount.so.1, libuuid.so.1I\'m now trying to use https://github.com/lucleray/tensorflow-lambda for tensorflow but with no luck. My little project is based on one of the examples from the repository. It uses the common directory as in the examples directory and I\'ve tryed to subsitute the import \'@tensorflow/tfjs-node\', from the env.ts file with import ""tensorflow-lambda"" but it does not work. It looks like tfjs-node have a mechanism to supersede the tfjs-core functions but this is not happening with tensorflow-lambda. Any hint on how I could make this happen ? Thanks in advance !  =====', ""@bobmoff i am new to this, and wanted to ask if you've got an example code to get started with face-api.js and AWS lambda? ====="", ""> @bobmoff i am new to this, and wanted to ask if you've got an example code to get started with face-api.js and AWS lambda?Sorry. Not using this any more. But Lambda have upped the limit to 10GB for each function so size should not be a problem any longer.====="", '@bobmoff I really wanna move this facial recognition to lambda. Any help would be highly appreciated. I tried with this version also```""dependencies"": {    ""@tensorflow/tfjs-node"": ""^0.2.1"",    ""face-api.js"": ""^0.17.1"",    ""get-pixels"": ""^3.3.2""  }```but still limit exceeding. =====', 'Are you reaching the max limit of 10gb ?The issue I had was getting below 250mb.=====', '> Are you reaching the max limit of 10gb ?> > The issue I had was getting below 250mb.Going above deployment package unzipped limit size (250mb)=====', 'If you switch using containers you can use 10 gb.https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/=====', ""Hi, I did get this working on Lambda a long time ago, but that was with older versions, and I ran into a lot of issues (especially the hard to resolve memory issues mentioned above). Eventually, I gave up on Lambda and just run it on EC2 using spot instances, which was cheaper and better for my workload. It was also simpler as I didn't have mess around with the file size issues.That said, I recently used EFS with Lambda for some image processing that required extra space, and it was really easy to use. I imagine you could use EFS to get around all the size limits.https://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/====="", '@bobmoff @henrydawson thank you both of you for pointing me in the direction. I will check both `aws lambda container` and `EFS`.=====', ""@bobmoff I have successfully deployed in Lambda using containers. But I'm facing one issue. I'm just doing `faceapi.detectSingleFace().withFaceLandmarks().withFaceDescriptor()` in lambda to detect single face and returning that output as a response via API Gateway. I can't able to return without doing `JSON.stringify(result)` and If I do stringify the result, that result not working in final comparison. I dont know what I'm doing wrong.update: I managed to fix it. And its working fine now.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/142;Could not run faceDetection.ts;2;closed;2018-11-20T00:28:15Z;2018-12-04T20:49:09Z;"I hope you to tell me, error reason why.here wrote, environment, actions and resultthanks------------- environment -------------$ uname -rv4.4.0-139-generic #165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018$ node -vv11.2.0$ npm ls typescript ts-node -g/usr/local/lib├── ts-node@7.0.1└── typescript@3.1.6------------- environment -------------------------- tried-and-result -------------$ cd /home/tobisaki/try2$ git clone https://github.com/justadudewhohacks/face-api.js.git$ cd face-api.js/examples/examples-nodejs/$ ls -la合計 64drwxrwxr-x  4 tobisaki tobisaki  4096 11月 20 08:36 .drwxrwxr-x  6 tobisaki tobisaki  4096 11月 19 23:19 ..drwxrwxr-x  2 tobisaki tobisaki  4096 11月 19 23:19 commons-rw-rw-r--  1 tobisaki tobisaki   479 11月 19 23:19 faceDetection.ts-rw-rw-r--  1 tobisaki tobisaki   704 11月 19 23:19 faceLandmarkDetection.ts-rw-rw-r--  1 tobisaki tobisaki  1649 11月 19 23:19 faceRecognition.tsdrwxrwxr-x 88 tobisaki tobisaki  4096 11月 20 09:07 node_modules-rw-rw-r--  1 tobisaki tobisaki 28902 11月 20 08:36 package-lock.json-rw-rw-r--  1 tobisaki tobisaki   197 11月 20 08:36 package.json$ npm i$ ts-node faceDetection.ts/usr/local/lib/node_modules/ts-node/src/index.ts:261    return new TSError(diagnosticText, diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(3,16): error TS2354: This syntax requires an imported helper but module 'tslib' cannot be found.faceDetection.ts(5,26): error TS2339: Property 'loadFromDisk' does not exist on type 'SsdMobilenetv1'.faceDetection.ts(10,23): error TS2339: Property 'createCanvasFromMedia' does not exist on type 'typeof import(""/home/tobisaki/try2/face-api.js/src/index"")'.faceDetection.ts(11,11): error TS2339: Property 'drawDetection' does not exist on type 'typeof import(""/home/tobisaki/try2/face-api.js/src/index"")'.    at createTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:261:12)    at getOutput (/usr/local/lib/node_modules/ts-node/src/index.ts:367:40)    at Object.compile (/usr/local/lib/node_modules/ts-node/src/index.ts:558:11)    at Module.m._compile (/usr/local/lib/node_modules/ts-node/src/index.ts:439:43)    at Module._extensions..js (internal/modules/cjs/loader.js:733:10)    at Object.require.extensions.(anonymous function) [as .ts] (/usr/local/lib/node_modules/ts-node/src/index.ts:442:12)    at Module.load (internal/modules/cjs/loader.js:620:32)    at tryModuleLoad (internal/modules/cjs/loader.js:560:12)    at Function.Module._load (internal/modules/cjs/loader.js:552:3)    at Function.Module.runMain (internal/modules/cjs/loader.js:775:12)------------- tried-and-result -------------";"['So Sorry!!I overlook faild packages when npm inisatll.Vulnerability Audited package was found by command(npm audit).                       === npm audit security report ===                                                                                                        \\# Run  npm install --save-dev karma-typescript@3.0.13  to resolve 1 vulnerability...It was fixed by command(npm audit fix).But ...My environment not have GUP. Result was$ ts-node faceDetection.ts 2018-11-20 15:06:58.359383: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend2018-11-20 15:07:02.048092: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.2018-11-20 15:07:02.068154: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.2018-11-20 15:07:02.082211: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.=====', 'I\'m seeing the same problem. The suggested fix doesn\'t seem to apply in my case. This is what I get (after manually installing tslib, which it was complaining about first):```% ts-node -vts-node v7.0.1node v11.2.0typescript v3.2.1los@ssdbuntu:~/src/face-api.js/examples/examples-nodejs^master ±% ts-node faceDetection.ts/home/los/n/lib/node_modules/ts-node/src/index.ts:261    return new TSError(diagnosticText, diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(5,26): error TS2339: Property \'loadFromDisk\' does not exist on type \'SsdMobilenetv1\'.faceDetection.ts(10,23): error TS2339: Property \'createCanvasFromMedia\' does not exist on type \'typeof import(""/home/los/src/face-api.js/src/index"")\'.faceDetection.ts(11,11): error TS2339: Property \'drawDetection\' does not exist on type \'typeof import(""/home/los/src/face-api.js/src/index"")\'.    at createTSError (/home/los/n/lib/node_modules/ts-node/src/index.ts:261:12)    at getOutput (/home/los/n/lib/node_modules/ts-node/src/index.ts:367:40)    at Object.compile (/home/los/n/lib/node_modules/ts-node/src/index.ts:558:11)    at Module.m._compile (/home/los/n/lib/node_modules/ts-node/src/index.ts:439:43)    at Module._extensions..js (internal/modules/cjs/loader.js:733:10)    at Object.require.extensions.(anonymous function) [as .ts] (/home/los/n/lib/node_modules/ts-node/src/index.ts:442:12)    at Module.load (internal/modules/cjs/loader.js:620:32)    at tryModuleLoad (internal/modules/cjs/loader.js:560:12)    at Function.Module._load (internal/modules/cjs/loader.js:552:3)    at Function.Module.runMain (internal/modules/cjs/loader.js:775:12)```Any help would be appreciated=====']"
https://github.com/justadudewhohacks/face-api.js/issues/141;I don't have acccess to all functionality of the API;2;closed;2018-11-19T19:05:52Z;2018-11-20T11:35:11Z;"I can only load models through the loadModels() function, not the individual loading functions as described in the 'Usage' section of the repository. Any other function gives me errors such as ""faceapi.nets.ssdMobilenetv1.loadFromUri is not a function"".I got the face-api.js file from the 'dist' folder and included that globally, is this an incorrect way of accessing the API?(I am working with JS and React for the first time)";['You are probably not using the latest version. If you are using react, then you probably use a bundler such as webpack, parcel etc. and can just npm install it to get the latest version.=====', 'Turned out I was indeed not using the latest version. I apologise for the stupid question and appreciate the quick response very much. Very cool repository!=====']
https://github.com/justadudewhohacks/face-api.js/issues/139;Datasets;2;closed;2018-11-17T22:15:04Z;2019-02-18T09:53:22Z;Hi, I was trying to learn deep learning and found your projects very cool. I wanted to train my own model and was wondering if you would mind sharing the dataset you used for the tiny_face_detector_model and the dataset you used for the 68k landmark detector.Thanks,Rohan;['Hello Rohan,Send me a mail to the email address on my github profile then I will give you permission to download the dropbox folder for the datasets.=====', '@justadudewhohacks would you mind sharing your landmark dataset(~35k), thanks.=====']
https://github.com/justadudewhohacks/face-api.js/issues/138;Face Recognition using IP Camera;3;closed;2018-11-17T03:01:44Z;2019-07-28T01:57:34Z;"Hi @justadudewhohacks, is it possible that the API supports face recognition using IP Camera? I got a camera with an IP address which is embedded to HTML as <img> with src is ""../stream/video/mjpeg"". I'd like to show the camera on browsers with face recognized. I've read the webcam example + the face recognition example but don't know how to make it work with IP Camera. Do you have any idea?Thanks";"['Hmm I am not sure how to show an IP camera stream on a HTMLVideoElement, but if there is a way, there is probably some tutorial out there.Anyways, this issue is not related to face-api.js.=====', ""I have the same problemBut I have implemented the ip camer image method from node-rtsp-streamBut can't integrate face-api.jsCan you help me?====="", ""> I have the same problem> But I have implemented the ip camer image method from node-rtsp-stream> But can't integrate face-api.js> Can you help me?You can develop a proxy that get the stream, process the frames and expose a websocket service to drop the image in base64 string format.Later in JS you can connect to this websocket and get the image base64 frame, convert to image and use it. You can use this aproach even in a web browser.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/136;How to compare two faces on two pictures in the browser?;7;closed;2018-11-16T07:27:51Z;2021-06-12T05:30:21Z;Hi, everybody! I want to compare two faces in two photos with your beautiful face-api.js.But I do not quite understand the algorithm how to do it... Can you show me a simple example of how to do this in a browser?Thank you!;"['You can simply compare their face descriptors by euclidean distance:``` javascriptconst distance = faceapi.euclideanDistance(descriptor1, descriptor2)```You can get the FullFaceDescription of all faces in an image as described in the README:``` javascriptconst fullFaceDescriptions = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors()```A FullFaceDescription contains the detection, landmarks and descriptor as shown [here](https://github.com/justadudewhohacks/face-api.js#interface-full-face-description)=====', '@justadudewhohacks Hello friend!Thank you!Like this?:```<html>    <head>        <script src=""/face/face/face-api.js""></script>    </head>    <body>        <img id=""myImg1"" src=""/site.com/oo/o1.jpg"" />        <img id=""myImg2"" src=""/site.com/oo/o2.jpg"" />    </body>    <script>    const input1 = document.getElementById(\'myImg1\'),    const input2 = document.getElementById(\'myImg2\'),    const detection1 = faceapi.detectAllFaces(input1).withFaceLandmarks().withFaceDescriptors(),    const detection2 = faceapi.detectAllFaces(input2).withFaceLandmarks().withFaceDescriptors(),    const distance = faceapi.euclideanDistance(detection1, detection2),    console.log(distance),    </script></html>```=====', 'detectAllFaces returns an array of FullFaceDescriptions, which have a property called descriptor. Please have a look at the link I posted above.=====', "" [ { detection:      FaceDetection {       _imageDims: [Dimensions],       _score: 0.9839361906051636,       _classScore: 0.9839361906051636,       _className: '',       _box: [Box] },    landmarks: FaceLandmarks68 { _imgDims: [Dimensions], _shift: [Point], _positions: [Array] },    unshiftedLandmarks: FaceLandmarks68 { _imgDims: [Dimensions], _shift: [Point], _positions: [Array] },    alignedRect:      FaceDetection {       _imageDims: [Dimensions],       _score: 0.9839361906051636,       _classScore: 0.9839361906051636,       _className: '',       _box: [Box] },    descriptor:      Float32Array [       -0.07600060850381851,       0.09135700017213821,       0.01828434318304062,       -0.10295865684747696,       -0.11790119111537933,       -0.07359399646520615,       -0.0860370621085167,       -0.1646803319454193,       0.17641472816467285,====="", 'is there any way to compare two faces using the node ?=====', ""> is there any way to compare two faces using the node ?I guess you can use this`import { euclideanDistance } from 'face-api.js'`====="", '> @justadudewhohacks Hello friend!> Thank you!> Like this?:> > ```> <html>>     <head>>         <script src=""/face/face/face-api.js""></script>>     </head>>     <body>>         <img id=""myImg1"" src=""/site.com/oo/o1.jpg"" />>         <img id=""myImg2"" src=""/site.com/oo/o2.jpg"" />>     </body>>     <script>>     const input1 = document.getElementById(\'myImg1\'),>     const input2 = document.getElementById(\'myImg2\'),>     const detection1 = faceapi.detectAllFaces(input1).withFaceLandmarks().withFaceDescriptors(),>     const detection2 = faceapi.detectAllFaces(input2).withFaceLandmarks().withFaceDescriptors(),>     const distance = faceapi.euclideanDistance(detection1, detection2),>     console.log(distance),>     </script>> </html>> ```this code showing this error,,,,Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference=====']"
https://github.com/justadudewhohacks/face-api.js/issues/134;running face Similarity on node js;1;closed;2018-11-13T18:58:12Z;2018-11-13T21:40:45Z;how can i run it serverside with node js? How good is the performance with node js?;['Performance depends on your CPU and if your machine is capable of running CUDA accelerated tfjs-node. Check out the nodejs examples to get started with face-api.js + nodejs. There is no difference in the API between browser and nodejs.=====']
https://github.com/justadudewhohacks/face-api.js/issues/133;Error on latest version of tensorflowjs;1;closed;2018-11-13T10:28:48Z;2018-11-13T21:30:33Z;"I got an error when implement the latest version of tensorflowjs in html file `<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest""></script>`> Uncaught (in promise) Error: Failed to compile fragment shader.>     at createFragmentShader (tfjs@latest:2)>     at e.createProgram (tfjs@latest:2)>     at compileProgram (tfjs@latest:2)>     at tfjs@latest:2>     at e.getAndSaveBinary (tfjs@latest:2)>     at e.compileAndRun (tfjs@latest:2)>     at e.maxPool (tfjs@latest:2)>     at ENV.engine.runKernel.x (face-api.js:23)>     at tfjs@latest:2>     at e.scopedRun (tfjs@latest:2)It looks like FaceRecognitionModel can only compatible tensorflowjs under 0.13.0.";['Answered here: #128. Always use the same tfjs-core version as face-api.js, which is 0.13.8 now.=====']
https://github.com/justadudewhohacks/face-api.js/issues/131;Error: Based on the provided shape, [1,1,16,32], and dtype float32, the tensor should have 512 values but has 93;19;closed;2018-11-12T15:50:31Z;2021-03-05T11:15:15Z;Hi! I'm having trouble making the library work on production. Everything works perfect locally.Using angular 4. Here is the code:`const detections: Array<any> = await faceapi.detectAllFaces(this.video, new faceapi.TinyFaceDetectorOptions({ inputSize: 128, scoreThreshold: 0.4 })),`> ERROR Error: Uncaught (in promise): Error: Based on the provided shape, [1,1,16,32], and dtype float32, the tensor should have 512 values but has 93Error: Based on the provided shape, [1,1,16,32], and dtype float32, the tensor should have 512 values but has 93    at c (scripts.208b17eda93ff84a86fa.bundle.js:1)    at new t (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Function.t.make (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Ht (scripts.208b17eda93ff84a86fa.bundle.js:1)    at o (scripts.208b17eda93ff84a86fa.bundle.js:1)    at $a (scripts.208b17eda93ff84a86fa.bundle.js:1)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at c (scripts.208b17eda93ff84a86fa.bundle.js:1)    at new t (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Function.t.make (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Ht (scripts.208b17eda93ff84a86fa.bundle.js:1)    at o (scripts.208b17eda93ff84a86fa.bundle.js:1)    at $a (scripts.208b17eda93ff84a86fa.bundle.js:1)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at S (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at polyfills.b05dc6a29a1cfeb080d6.bundle.js:1    at a (main.424b2f298a047510607b.bundle.js:1)    at t.invoke (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at Object.onInvoke (main.424b2f298a047510607b.bundle.js:1)    at t.invoke (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at e.run (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at polyfills.b05dc6a29a1cfeb080d6.bundle.js:1    at t.invokeTask (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at Object.onInvokeTask (main.424b2f298a047510607b.bundle.js:1)$ @ main.424b2f298a047510607b.bundle.js:1t.handleError @ main.424b2f298a047510607b.bundle.js:1next @ main.424b2f298a047510607b.bundle.js:1e.object.i @ main.424b2f298a047510607b.bundle.js:1e.__tryOrUnsub @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e._next @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e.emit @ main.424b2f298a047510607b.bundle.js:1(anonymous) @ main.424b2f298a047510607b.bundle.js:1t.invoke @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1e.run @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1t.runOutsideAngular @ main.424b2f298a047510607b.bundle.js:1onHandleError @ main.424b2f298a047510607b.bundle.js:1t.handleError @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1e.runGuarded @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1t @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1n.microtaskDrainDone @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1d @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1Also the video is 351x360 pixels;"['Not sure why the code snippet shown above should make any difference in production.Looking at the error message your issue most likely resides in loading a model. Check that the model files are served correctly in your production environment.=====', 'Thanks! That solved it!=====', '@psiservices-azubizarreta Can you tell me how to solve? I have a same error. Thank you !=====', ""@psiservices-azubizarreta Hello ! I hava a same problem as you met, and it's very peremptorily, so。。Can you tell us the way to solve this situation plz ? my very thanks!====="", 'a bit stuck here too how did you solve it on production?=====', ""For me, this problem caused by the uploading process. I'm using filezilla and it's solved by change the transfer type to binary.Sorry for commenting on closed issue.====="", '@atmosuwiryo this helped!=====', 'I managed to solve it serving model files from a [cdn](https://gitcdn.xyz/)Use https://gitcdn.xyz/repo/justadudewhohacks/face-api.js/master/weights/ as `MODEL_URL`=====', 'That address seems invalid...=====', '@aryehrein , well it tends to hibernate occasionally. You have to wake it up by pasting the link to `weights` folder manually on https://gitcdn.xyz/=====', 'It was actually a different problem - the json files managed to get loaded, but the non extension files (e.g.  ""age_gender_model-shard1"") weren\'t. So what I did - I used my downloaded ""weights"" folder (and not the cdn), and for each model I :1. added a fictional "".shard"" extension for the non extension file.2. opened the corresponding ""..._manifest.json"" file and changed the ""path"" property to the new file name including the "".shard"" extension.For example:1. /age_gender_model-shard1 renamed to /age_gender_model-shard1.shard2. opened age_gender_model-weights_manifest.json, and changed `""paths"":[""age_gender_model-shard1""]`to:`""paths"":[""age_gender_model-shard1.shard""]`Pay attention that some manifest.json files have 2 paths=====', '@aryehrein It tried that (adding extension in non-extension files)  and also changing the name in .json files but it is still not working !!=====', '@aryehrein i am getting this => Error: Based on the provided shape, [1,1,128,256], the tensor should have 32768 values but has 12137 (i am using the face_landmark_68_model). =====', 'Please help face-api.min.js:1 Uncaught (in promise) Error: Based on the provided shape, [25], the tensor should have 25 values but has 21=====', '> 1. .shardThis solution worked for me. Thanks!But this strange why it needs an extension for production on iOS. =====', 'Maybe it will help somebody:The error occurred when I started my local server with --spa, when removed it error gone=====', '@aryehrein Changing the file names worked for me! (adding an extension to -shard1 files). Thanks for the tip!! I hope it will help more people.I would say that in my case I was developing an Ionic 5 (angular) app with face-api. It worked with ionic when testing with ""ionic serve"" in Chrome browser. It failed (and I need to apply this fix) when I created the app and tested in a real android device.=====', 'Added .bin file extension to tiny_face_detector tfjs model. Else the file is not being served in production mode.Changed the file `""face_detector_model-shard1""` to `""tiny_face_detector_model-shard1.bin"" `and updating the path in  detector_model-weights_manifest.json file like below`""paths"":[""tiny_face_detector_model-shard1.bin""] `=====', '> Added .bin file extension to tiny_face_detector tfjs model. Else the file is not being served in production mode.> > Changed the file `""face_detector_model-shard1""` to `""tiny_face_detector_model-shard1.bin"" `and updating the path in detector_model-weights_manifest.json file like below> `""paths"":[""tiny_face_detector_model-shard1.bin""] `Thanks! that\'s the solutionsetting .bin extension makes Content-Type: application/octet-stream for response=====']"
https://github.com/justadudewhohacks/face-api.js/issues/130;Error at faceapi.drawDetection(canvas, boxesWithText).;11;closed;2018-11-08T01:31:35Z;2020-07-13T11:26:17Z;"I am trying to implement the use case list on below sitehttps://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740i am able to do face detection but getting issue in face recognition with some investigation it looks like boxesWithText is giving empty output which is causing drawDetection to fail.Error Uncaught (in promise) TypeError: Cannot read property ‘x’ of undefined at VM1409 face-api.min.js:1 at Array.forEach (<anonymous>) at Object.t.drawDetection (VM1409 face-api.min.js:1) at run2 (VM1410 index.js:75)JS code``` $(document).ready(function() {       run1()})async function run1() {    const MODELS = ""http://localhost:8000/Desktop/FaceID/Face%20Detection%20with%20webcam/models"", // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    // try to access users webcam and stream the images  // to the video element const videoEl = document.getElementById('inputVideo')  navigator.getUserMedia(    { video: {} },    stream => videoEl.srcObject = stream,    err => console.error(err))}async function run2() {    const mtcnnResults = await faceapi.ssdMobilenetv1(document.getElementById('inputVideo'))overlay.width = 500overlay.height = 400const detectionsForSize = mtcnnResults.map(det => det.forSize(500, 400))faceapi.drawDetection(overlay, detectionsForSize, { withScore: true })    const input = document.getElementById('inputVideo')const fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()        const labels = ['sheldon','ravish']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `http://localhost:8000/Desktop/${label}.png`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it's landmarks and face descriptor    const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }        const faceDescriptors = [fullFaceDescription.descriptor]   // console.log(label)    return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)  }))// 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance) //console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))const boxesWithText = results.map((bestMatch, i) => {  const box = fullFaceDescriptions[i].detection.box  const text = bestMatch.toString()  const boxWithText = new faceapi.BoxWithText(box, text)})faceapi.drawDetection(overlay, boxesWithText)}async function onPlay(videoEl) {    run2()    setTimeout(() => onPlay(videoEl))} ```";"['You forgot the return statement in your map function:``` javascriptconst boxesWithText = results.map((bestMatch, i) => {  const box = fullFaceDescriptions[i].detection.box  const text = bestMatch.toString()  const boxWithText = new faceapi.BoxWithText(box, text)  return boxWithText})```Fixed the gist...=====', 'Hi, I am attempting to get this example from the blogpost at itnext.io running as well, but have a few issues. Does anyone have the full source code (HTML + JS) for this, so I can see what I am missing?=====', ""const video = document.getElementById('video')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri('/models'),  faceapi.nets.faceLandmark68Net.loadFromUri('/models'),  faceapi.nets.faceRecognitionNet.loadFromUri('/models'),  faceapi.nets.faceExpressionNet.loadFromUri('/models'),  faceapi.nets.ssdMobilenetv1.loadFromUri('/models')]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} },    stream => video.srcObject = stream,    err => console.error(err)  )}video.addEventListener('play', async () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const labeledFaceDescriptors = await loadLabeledImages()  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)  const displaySize = { width: video.width, height: video.height }  faceapi.matchDimensions(canvas, displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors(),    const resizedDetections = faceapi.resizeResults(detections, displaySize)      canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)    if(resizedDetections.length>0){      const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))      results.forEach((result, i) => {               const box = resizedDetections[i].detection.box        const drawBox = new faceapi.draw.DrawBox(box, { label: result.label.toString() })        drawBox.draw(canvas)      })        }      // faceapi.draw.drawDetections(canvas, resizedDetections)    // faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)    // faceapi.draw.drawFaceExpressions(canvas, resizedDetections)  }, 100)})function loadLabeledImages() {   const labels = ['Black Widow', 'Captain America', 'Captain Marvel', 'Hawkeye', 'Jim Rhodes', 'Thor', 'Tony Stark']  return Promise.all(    labels.map(async label => {      const descriptions = []      for (let i = 1, i <= 1, i++) {        const img = await faceapi.fetchImage(`images/${label}.png`)        const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        descriptions.push(detections.descriptor)        debugger,      }      return new faceapi.LabeledFaceDescriptors(label, descriptions)    })  )}====="", 'while running my code,this error shows,2141script.js:32 Uncaught (in promise) TypeError: faceapi.draw.AgeAndGender is not a function    at script.js:32please help me to solve this=====', 'There is no faceapi.draw.AgeAndGender. Try [this](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/ageAndGenderRecognition.html#L160-L169).=====', 'yeah bro its working.......please tell where  to i learn and understand  the complete face-api,please suggest to work well with face-api=====', 'script.js:66 Uncaught (in promise) ReferenceError: getFaceDetectorOptions is not defined    at updateReferenceImageResults (script.js:66)    at uploadImage (script.js:50)this is my error=====', ""//const video = document.getElementById('video')const input = document.getElementById('myImg')  Promise.all([ faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),   faceapi.nets.faceLandmark68Net.loadFromUri('/models'),   faceapi.nets.faceRecognitionNet.loadFromUri('/models'),   faceapi.nets.faceExpressionNet.loadFromUri('/models'),  faceapi.nets.ageGenderNet.loadFromUri('/models')])//.then(startVideo)//  function startVideo() {//    navigator.getUserMedia(//      { video: {} },//      stream => video.srcObject = stream,//      err => console.error(err)//    )//  }// .then(load)// async function load(){//   const imgFile = document.getElementById('myFileUpload').files[0]//   // create an HTMLImageElement from a Blob//   const img = await faceapi.bufferToImage(imgFile)//   document.getElementById('myImg').src = img.src//   const canvas = faceapi.createCanvasFromMedia(document.getElementById('myImg').src)//   const displaySize = { width: input.width, height: input.height }//   faceapi.matchDimensions(canvas, displaySize)// }// async function uploadImage() { //   const detections1 = await faceapi.detectSingleFace(input).withFaceLandmarks()//   const resizedResults = faceapi.resizeResults(detections1, displaySize)//   canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)//   const detectionsWithLandmarks = await faceapi.detectSingleFace(input).withFaceLandmarks()//   faceapi.draw.drawFaceLandmarks(canvas, resizedResults)//   console.log(detections1)// }let faceMatcher=null, async function uploadImage(e) {  const imgFile = document.getElementById('myFileUpload').files[0]  // create an HTMLImageElement from a Blob  const img = await faceapi.bufferToImage(imgFile)  document.getElementById('myImg').src = img.src  updateReferenceImageResults()}async function uploadQueryImage(e) {  const imgFile = document.getElementById('queryImgUploadInput').files[0]  // create an HTMLImageElement from a Blob  const img = await faceapi.bufferToImage(imgFile)  document.getElementById('queryImg').src = img.src  updateQueryImageResults()}async function updateReferenceImageResults() {  const inputImgEl = document.getElementById('#myImg')  const canvas = document.getElementById('#refImgOverlay')  const fullFaceDescriptions = await faceapi    .detectAllFaces(inputImgEl, getFaceDetectorOptions())    .withFaceLandmarks()    .withFaceDescriptors()  if (!fullFaceDescriptions.length) {    return  }  // create FaceMatcher with automatically assigned labels  // from the detection results for the reference image  faceMatcher = new faceapi.FaceMatcher(fullFaceDescriptions)  faceapi.matchDimensions(canvas, inputImgEl)  // resize detection and landmarks in case displayed image is smaller than  // original size  const resizedResults = faceapi.resizeResults(fullFaceDescriptions, inputImgEl)  // draw boxes with the corresponding label as text  const labels = faceMatcher.labeledDescriptors    .map(ld => ld.label)  resizedResults.forEach(({ detection, descriptor }) => {    const label = faceMatcher.findBestMatch(descriptor).toString()    const options = { label }    const drawBox = new faceapi.draw.DrawBox(detection.box, options)    drawBox.draw(canvas)  })}async function updateQueryImageResults() {  if (!faceMatcher) {    return  }  const inputImgEl = document.getElementById('#queryImg')  const canvas = document.getElementById('#queryImgOverlay')  const results = await faceapi    .detectAllFaces(inputImgEl, getFaceDetectorOptions())    .withFaceLandmarks()    .withFaceDescriptors()  faceapi.matchDimensions(canvas, inputImgEl)  // resize detection and landmarks in case displayed image is smaller than  // original size  const resizedResults = faceapi.resizeResults(results, inputImgEl)  resizedResults.forEach(({ detection, descriptor }) => {    const label = faceMatcher.findBestMatch(descriptor).toString()    const options = { label }    const drawBox = new faceapi.draw.DrawBox(detection.box, options)    drawBox.draw(canvas)  })}// video.addEventListener('play', () => {//   const canvas = faceapi.createCanvasFromMedia(video)//   document.body.append(canvas)//   const displaySize = { width: video.width, height: video.height }//   faceapi.matchDimensions(canvas, displaySize)//   setInterval(async () => {//     const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions().withAgeAndGender()//     const resizedDetections = faceapi.resizeResults(detections, displaySize)//     canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)//     faceapi.draw.drawDetections(canvas, resizedDetections)//     faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)//     faceapi.draw.drawFaceExpressions(canvas, resizedDetections)//     resizedDetections.forEach(result=>{//       const{age,gender,genderProbability}=result//       new faceapi.draw.DrawTextField([//         `${faceapi.round(age,0)} years`,//     `${gender}(${faceapi.round(genderProbability)})`],//     result.detection.box.bottomRight).draw(canvas)//     })    // resizedDetections.forEach(res=>{    //   const{age}=res    //   new faceapi.draw.DrawTextField([    //     `${faceapi.round(age,0)} years`],    //     res.detection.box.bottomLeft).draw(canvas)    //   })        // faceapi.draw.drawAgeAndGender(canvas, resizedDetections) // }, 1000)//})====="", 'there are many comment line so,sorry for thisplease give the solution guys=====', '> You forgot the return statement in your map function:> > ```js> const boxesWithText = results.map((bestMatch, i) => {>   const box = fullFaceDescriptions[i].detection.box>   const text = bestMatch.toString()>   const boxWithText = new faceapi.BoxWithText(box, text)>   return boxWithText> })> ```> > Fixed the gist..its not the proper solution,i have the same issuedrawDetection is not a function..please give solution its frustrating=====', 'can you help my ?? thats wrong in my code ?its can detect face, write score but without recognition >(label name)```  <script>    let forwardTimes = []    let withFaceLandmarks = false    let withBoxes = true    function onChangeWithFaceLandmarks(e) {      withFaceLandmarks = $(e.target).prop(\'checked\')    }    function onChangeHideBoundingBoxes(e) {      withBoxes = !$(e.target).prop(\'checked\')    }    function updateTimeStats(timeInMs) {      forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)      const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length      $(\'#time\').val(`${Math.round(avgTimeInMs)} ms`)      $(\'#fps\').val(`${faceapi.utils.round(1000 / avgTimeInMs)}`)    }    async function onPlay(videoEl) {      if(!videoEl.currentTime || videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())        return setTimeout(() => onPlay(videoEl))      const options = getFaceDetectorOptions()      const ts = Date.now()      const drawBoxes = withBoxes      const drawLandmarks = withFaceLandmarks      let task = faceapi.detectAllFaces(videoEl, options)      task = withFaceLandmarks ? task.withFaceLandmarks() : task      const results = await task      updateTimeStats(Date.now() - ts)      const canvas = $(\'#overlay\').get(0)      const dims = faceapi.matchDimensions(canvas, videoEl, true)      const resizedResults = faceapi.resizeResults(results, dims)      if (drawBoxes) {        faceapi.draw.drawDetections(canvas, resizedResults)      }      if (drawLandmarks) {        faceapi.draw.drawFaceLandmarks(canvas, resizedResults)      }      setTimeout(() => onPlay(videoEl))    }    async function run() {      // load face detection and face landmark models      await changeFaceDetector(TINY_FACE_DETECTOR)\t      await faceapi.loadSsdMobilenetv1Model(\'/\')\t\t      await faceapi.loadFaceRecognitionModel(\'/\')      await faceapi.loadFaceLandmarkModel(\'/\')      changeInputSize(416)      // start processing frames const labels = [\'1\',\'2\']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `picture/${label}.JPG`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it\'s landmarks and face descriptor    const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }        const faceDescriptors = [fullFaceDescription.descriptor]    console.log(label)    return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)  }))const input = document.getElementById(\'inputVideo\')const fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()      // 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance) console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))\tresults.forEach((bestMatch, i) => {\tconst box = fullFaceDescriptions[i].detection.box\tconst text = bestMatch.toString()\tconst drawBox = new faceapi.draw.DrawBox(box, { label: text })\tconsole.log(""last"")\t})//  results        onPlay($(\'#inputVideo\').get(0))}\t    function updateResults() {}    $(document).ready(function() {      renderNavBar(\'#navbar\', \'video_face_tracking\')      initFaceDetectionControls()      run()    })            </script>```=====']"
https://github.com/justadudewhohacks/face-api.js/issues/129;Webcam_face_tracking doesn't detect face when it is run locally in firefox browser on laptop.;10;closed;2018-11-06T08:59:30Z;2019-03-27T14:05:41Z;I am using 63.0.1 firefox browser on windows 10. This works https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking on both chrome & firefox.but when it is run locally by npm start. It is opening the camera, but not detecting face in firefox. It works fine in chrome. And no logs too.;"['Hi dude, any way to resolve this?=====', 'I have the same question and wonder how to resolve it.=====', 'It seems like the program stops at this line`      const result = withFaceLandmarks        ? await faceDetectionTask.withFaceLandmarks()        : await faceDetectionTask`=====', 'yes=====', 'I think there is an issue with tf.fromPixels on firefox sometimes, when the processing loop starts too soon. Not sure why this is though.=====', '@vbh25, did you find a solution to that problem?=====', 'calling videoEl.play() in videoEl.onloadedmetadata function solves it.=====', ""> calling videoEl.play() in videoEl.onloadedmetadata function solves it.I don't find videoEl.onloadedmetadata function. Which file is the function in?====="", 'In fact, as @veetechh pointed out, moving onPlay to onloadedmetadata instead of onplay in the video element seems to solve it: `<video onloadedmetadata=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>`.Will fix this in the examples.=====', 'Checked in the fixed examples, closing here.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/128;Failed to Compile fragment Shader;2;closed;2018-11-06T05:05:23Z;2018-11-11T19:52:13Z;"I am getting the following error suddenly since yesterday (was working fine till Nov 1) when I try to detect face using tiny-yolo. I had downloaded latest face-api.min.js on Oct 30th and it worked for a day and somehow started getting this error since yesterday.![image](https://user-images.githubusercontent.com/38608333/48043981-726be480-e1af-11e8-8d34-bd15d0189af5.png)Can you pls help on the same. I am including tfjs using  the following link.<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""> </script>";['Seems like an issue with tf latest maybe? Try to use the tfjs-core version, that face-api.js uses, which is 0.13.2 currently. I noticed some issues with 0.13.9 + WebGL backend.=====', 'Thank you.You are right. It was an issue with tfjs as things work fine when I switchto tfjs 0.13.2.On Tue, Nov 6, 2018 at 2:03 PM justadudewhohacks <notifications@github.com>wrote:> Seems like an issue with tf latest maybe? Try to use the tfjs-core> version, that face-api.js uses, which is 0.13.2 currently. I noticed some> issues with 0.13.9 + WebGL backend.>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/128#issuecomment-436171165>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/Ak0dzURDszpL8o5YgFvTMYk4dW6CKXA_ks5usUllgaJpZM4YPwzZ>> .>=====']
https://github.com/justadudewhohacks/face-api.js/issues/126;Error about Float32Array ;8;closed;2018-11-02T22:09:54Z;2018-11-07T08:47:59Z;HiWhen I open faceAndLandmarkDetection.html and some other files, I get the following error:tf-core.esm.js:17 **Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4  at new Float32Array** (<anonymous>)    at o (tf-core.esm.js:17)    at decodeWeights (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at Object.<anonymous> (tf-core.esm.js:17)    at tf-core.esm.js:17    at Object.next (tf-core.esm.js:17)o @ tf-core.esm.js:17decodeWeights @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17i @ tf-core.esm.js:17async function (async)run @ faceAndLandmarkDetection.html:186(anonymous) @ faceAndLandmarkDetection.html:197j @ jquery-2.1.1.min.js:2fireWith @ jquery-2.1.1.min.js:2ready @ jquery-2.1.1.min.js:2I @ jquery-2.1.1.min.js:2Any comment?;"['Me too`Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4    at new Float32Array (<anonymous>)    at o (io_utils.ts:116)    at Object.decodeWeights (io_utils.ts:79)    at e.<anonymous> (frozen_model.ts:109)    at exports_regularizers.ts:47    at Object.next (exports_regularizers.ts:47)    at s (exports_regularizers.ts:47)`=====', ""The error message is a bit confusing, but it might simply be, that the url to the model shards is simply wrong. Check out the network tab and verify, that the requests don't result in 404s.====="", ""> > > The error message is a bit confusing, but it might simply be, that the url to the model shards is simply wrong. Check out the network tab and verify, that the requests don't result in 404s.Thanks, No message in the Network.You can try yourself on my website: URL====="", 'Everything is visible in my web now (goto url). You can see what I have in the web. I had to copy some folders to ""views"" folder to prevent some errors.=====', ""The first shard doesn't load correctly: `ssd_mobilenetv1_model-shard1 | 206 | fetch | tf-core.esm.js:17 | 1.0\xa0MB`. The size should be 4MB and not 1MB and the response code 206 is also suspicious to me. Maybe your file is corrupted or something?====="", 'Have I put this shard file in a right place?=====', ""> > > The first shard doesn't load correctly: `ssd_mobilenetv1_model-shard1 | 206 | fetch | tf-core.esm.js:17 | 1.0 MB`. The size should be 4MB and not 1MB and the response code 206 is also suspicious to me. Maybe your file is corrupted or something?It seems that something was corrupted . I uploaded again and it is working now!====="", 'Great!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/125;Use SSD_MobileNet_V1 model, cause box width and height must be positive numbers;2;closed;2018-11-02T09:43:14Z;2018-11-13T09:18:22Z;"Hello, I try to use face-api.js by follow this article[https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07)But when I run the code, the console output the error:## Error> face-api.js:1118 Uncaught (in promise) Error: Box.constructor - width (0.08514529466629028) and height (-0.15933257341384888) must be positive numbers>     at Function.Box.assertIsValidBox (face-api.js:1118)>     at Rect.Box (face-api.js:1103)>     at new Rect (face-api.js:1387)>     at face-api.js:4774>     at Array.map (<anonymous>)>     at SsdMobilenetv1.<anonymous> (face-api.js:4765)>     at step (face-api.js:317)>     at Object.next (face-api.js:298)>     at fulfilled (face-api.js:288)## EnvironmentChrome: 70.0.3538.77System: Windows10Node.js: v11.0.0express: v4.16.3## Code```var MODEL_URL = '/models'var canvas = $('#canvas')[0],var imgObj = new Image(),imgObj.src = ""/images/face-sample.jpg"",imgObj.onload = function(){        var ctx = canvas.getContext('2d'),        ctx.drawImage(this, 0, 0),    }async function start() {    var MODEL_URL = '/models'    await faceapi.loadSsdMobilenetv1Model(MODEL_URL)    await faceapi.loadFaceLandmarkModel(MODEL_URL)    await faceapi.loadFaceRecognitionModel(MODEL_URL)    console.log('Model loaded successed!')    var input = canvas    var fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()    var detectionsArray = fullFaceDescriptions.map(fd => fd.detection)    faceapi.drawDetection(canvas, detectionsArray, { withScore: true })    const landmarksArray = fullFaceDescriptions.map(fd => fd.landmarks)    faceapi.drawLandmarks(canvas, landmarksArray, { drawLines: true })    console.log(detectionsArray)}start()```## Sample Image![face-sample](https://user-images.githubusercontent.com/11308780/47908134-95547b00-dec7-11e8-9067-c4f46b22386e.jpg)## MessageWhen this code run in my friends' computer, it works. I'm confused about it.And When I try to play demo in this url [https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection](https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection), and I try to use SSD_MobileNet_V1 model, I got the same problem.";"[""Ahh yeah, that might be because you run the thing on an Intel GPU probably (please correct me if I am wrong with that assumption)? Intel GPUs seems to not be supported yet (https://github.com/tensorflow/tfjs/issues/510), thus the predicted bounding boxes are totally wrong resulting in this error.Nevertheless, it might be better to handle bounding boxes, that are out of image borders, although it doesn't help in your case, since the prediction is simply wrong.You could use the TinyFaceDetector, which should work with an Intel GPU.====="", ""> Ahh yeah, that might be because you run the thing on an Intel GPU probably (please correct me if I am wrong with that assumption)? Intel GPUs seems to not be supported yet ([tensorflow/tfjs#510](https://github.com/tensorflow/tfjs/issues/510)), thus the predicted bounding boxes are totally wrong resulting in this error.> > Nevertheless, it might be better to handle bounding boxes, that are out of image borders, although it doesn't help in your case, since the prediction is simply wrong.> > You could use the TinyFaceDetector, which should work with an Intel GPU.Okay, I found that TinyFaceDetector is work for me, thank you.(But it is not as accurate as SSD_MobileNet model XD)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/124;Face recognition using webcam;7;closed;2018-11-02T05:51:57Z;2019-12-27T06:46:22Z;Hi, How can we **recognise the images using a webcam**... I  am only able to detect the faces but not able to recognise them.Please help! ;"['Hi,Could you tell me what you are doing currently? If you take a look at the webcam example + the face recognition example, it should be clear, what you have to do.Also reading the [MTCNN article](https://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740) should help, which performs facial recogition on webcam images. Simply substitue the mtcnn, with the face detector of your choice.=====', 'okay thanks ,Yes I am trying to implement this.. I m using **webcamFaceTracking** module right now, I want to implement face recognition as well inside it.I will let u know if i would have any further queries.=====', 'Hey, I am not able to combine these two modules together. =====', ""What's the issue?====="", 'I have checked it again.. And now its working. Thanks !=====', 'Hello I would be glad if I can have your webcam face recognition code=====', 'I am having intense difficulties combining them.  Any help?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/120;getBox error on 0.15.0;3;closed;2018-10-30T02:55:06Z;2018-11-04T02:04:07Z;### Calling allFaces with SsdMobilenetv1 is giving me a getBox error.This code:```jsconst faceImage = await faceapi.fetchImage('https://i.imgur.com/GzBMITw.jpg')const faceDescript = await faceapi.allFacesSsdMobilenetv1(faceImage)```resulting in:![image](https://user-images.githubusercontent.com/997157/47692693-a732d700-dbc4-11e8-9d81-588766fb61fd.png)I have the code (in React) available upon request.  Please let me know if this issue makes sense or if I should do more.;['Apparently I forgot to clean the build folder before building and publishing, because the error originates from an old file that has been removed.=====', 'Should be resolved in 0.15.1.=====', 'it is=====']
https://github.com/justadudewhohacks/face-api.js/issues/116;Reference Error on TinyFaceDetectorOptions (and other options);2;closed;2018-10-27T20:30:53Z;2018-10-28T13:53:52Z;"I'm attempting to implement your new changes, but when I try to use: `const img1Description = await faceapi.detectSingleFace(""s1"", new TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor()`I get the following error:`ReferenceError: TinyFaceDetectorOptions is not defined`I've confirmed I've got the latest JavaScript files and have tried it across all browsers with the same result. I also get the same error for MtcnnOptions and SsdMobilenetv1Options. When I don't provide the options, the call is successful. What am I missing?";['I think it should be clear, that everything is accessible via faceapi and not on the global scope:`new faceapi.TinyFaceDetectorOptions()`.Sorry if the readme was confusing, I will adjust the snippets.=====', 'I figured it was something simple. That should have occurred to me. Thanks!=====']
https://github.com/justadudewhohacks/face-api.js/issues/113;Face not being detected when image contains face.;8;closed;2018-10-26T12:50:12Z;2020-09-07T06:09:57Z;"I'm trying to use your module with Electron in a renderer process.I'm trying to retrieve the face descriptor of this image:![woman](https://user-images.githubusercontent.com/21309909/47566105-7e1df800-d8df-11e8-8ae5-3614ee37e350.jpg)Which should work but doesn't. When I run the following code snippet, I get `undefined`:```javascriptvar input = document.getElementById(""myImg""), // Image of womanconsole.log(faceapi), // Just to check if I have the right module.(async () => {    const MODELS = ""./models"", // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    const fullFaceDescriptions = await faceapi // I tried the ""detectAllFaces(input)"" method too, with no success    .detectSingleFace(input)    .withFaceLandmarks()    .withFaceDescriptor()    console.log(fullFaceDescriptions), // Returns ""undefined""})(),```When I try to run the following code snippet, I get an empty array:```javascriptvar input = document.getElementById(""myImg""), // Image of womanconsole.log(faceapi), // Just to check if I have the right module.(async () => {    const MODELS = ""./models"", // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    const fullFaceDescriptions = await faceapi // I tried the ""detectAllFaces(input)"" method too, with no success    .detectAllFaces(input)    .withFaceLandmarks()    .withFaceDescriptors()    if (!fullFaceDescriptors.length) { // This condition is satisfied.        console.log(fullFaceDescriptors), // Returns an empty array.        return,    }    console.log(fullFaceDescriptors), // Code doesn't even reach this point.})(),```My HTML file:```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">    <title>FAJS</title>    <style>    </style></head><body>    <img id=""myImg"" src=""woman.jpg"" alt="""">    <script src=""./node_modules/face-api.js/dist/face-api.min.js""></script>    <script src=""index.js""></script></body></html>```Please help as I really need this to work for a project of mine. ThanksP.S: Thanks for making this module. It's great!EDIT: I've been looking for 2 days now and haven't been able to find a solution due to which I had to open this issue. My apologies if this has a very easy fix and wasted your time.";"['![foo](https://user-images.githubusercontent.com/31125521/47568097-7f2b4580-d930-11e8-8f68-ffeb6c7a5040.png)If you have an Intel GPU, use the new TinyFaceDetector, which should work on an Intel GPU instead of SSDMobilenetv1.Intel Support is still an open topic for tfjs: https://github.com/tensorflow/tfjs/issues/510=====', ""I don't even have a GPU and my computer is very slow...Does that mean that I won't be able to use this module?====="", ""Oh well, then what kind of cpu? If you don't have a GPU then you can run everything on the CPU.Are there any errors thrown by tfjs, when trying to register a backend?====="", ""Oh boy, this is embarrassing.I have an Intel Core i3 with 4 GB of RAM and no, I don't see any errors by tfjs anywhere.====="", '@Frixoe did you manage to figure out the issue? Would be interested, what the issue was.=====', ""I am having a similar problem on a Samsung slate 7 with a i5 Processor 2467M,  Intel® HD Graphics 3000. I'm getting undefined when trying to compute a descriptor , when clearly the image contains a face. How do I debug this ? I've tried TinyFaceDetector and SSD , both return undefined.====="", '@justadudewhohacks I guess the issue is, electron is integrating node env with the application. Hence face-api.js is considering it as node environment and running node specific code=====', ""@justadudewhohacks Sorry for the super late reply but I don't exactly remember what was causing the issue. I ended up fixing it somehow anyways.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/110;Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array;2;closed;2018-10-25T15:20:05Z;2018-10-28T11:01:21Z;"Hi Vincent,You've done a good job with your new update.I have an array like this:```js[  0: {     className: ""unique-id-1""    descriptors: [      0: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ],      1: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ],    ]  },  1: {     className: ""unique-id-2""    descriptors: [      0: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ],      1: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ],    ]  },]```I will do facial recognition using these values. That's why I applied this method:```jsconst labeledDescriptors = [  new faceapi.LabeledFaceDescriptors(    vm.trainDescriptorsByClass[0].className,    vm.trainDescriptorsByClass[0].descriptors  ),  new faceapi.LabeledFaceDescriptors(    vm.trainDescriptorsByClass[1].className,    vm.trainDescriptorsByClass[1].descriptors  )],const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors),const fullFaceDescriptions = await faceapi    .detectAllFaces(vm.yuz_degiskenleri.videoEl, vm.yuz_degiskenleri.options)    .withFaceLandmarks()    .withFaceDescriptors(),fullFaceDescriptions.forEach(({ detection, landmarks, descriptor }, index) => {    console.log({ detection, landmarks, descriptor }, index),    const bestMatch = faceMatcher.findBestMatch(descriptor),    console.log(bestMatch),    /*vm.yuz_degiskenleri.en_iyi_eslesen_yuz = getBestMatch(vm.trainDescriptorsB    console.log(vm.yuz_degiskenleri.en_iyi_eslesen_yuz.className),*/}),```I get the following error:```Uncaught (in promise) Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array    at new t (face-api.min.js:1)    at Vue.yuz_tanima_calistir (9nPmZuxCNCQxMzQkTWsxNlRYa3pKREU9JDE4OTk5OTkwMDAwMDI1Y97wNB:1)    at setTimeout (9nPmZuxCNCQxMzQkTWsxNlRYa3pKREU9JDE4OTk5OTkwMDAwMDI1Y97wNB:1)```Where am I doing wrong?";['Hi,`vm.trainDescriptorsByClass[0].descriptors.map(desc => new Float32Array(desc))` should make it? Actually a plain array of numbers should also be ok, but as it is implemented right now the FaceMatcher expects descriptors to be Float32Arrays. =====', 'It worked! Thanks Vincent...=====']
https://github.com/justadudewhohacks/face-api.js/issues/108;RGB or Grayscale in order to improve accuracy?;2;closed;2018-10-23T04:44:35Z;2018-10-23T17:53:44Z;At first, many thanks for your support and dedication to the project.I'm just wondering, in order to improve accuracy, do we need to convert the media into gray-scale or just leave it at RGB before computeFaceDescriptor ?As I can see that you've saved media as gray-scale in the example folder.;"['Hi,The example images just happen to be gray scale, RGB images should in general give better results. If there was a need to convert them, I would have provided the necessary utility for that.=====', ""Thank you for your clarification. I've tested the grey-scale method and the results were not much different from RGB.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/106;Missing Age and Gender detection;25;closed;2018-10-19T18:28:40Z;2020-03-15T15:17:18Z;Please add age and gender detection in lib;"['Sounds like a reasonable extension. Any links to repos with pretrained models would be helpful.=====', ""> Sounds like a reasonable extension. Any links to repos with pretrained models would be helpful.You can take a look at this https://github.com/bharathvaj1995/gender-detection-tensorflowjsand the pretrained here https://s3.ap-south-1.amazonaws.com/arunponnusamy/pre-trained-weights/gender_detection.model However, I'm using this works https://github.com/oarriaga/face_classification and the results so far is pretty good. I would have provided the codes for that.====="", 'How could I perform transfer learning and train, over which models files, to add gender / age recognition. If I had an already trained model, how could I ""plug"" it into the API?=====', 'Not sure if I understood correctly.If you want to train your own model with transfer learning, I would actually try to use the face landmark model and replace the fully connected layer with an own one that outputs male / female.Regarding the second question: What kind of model do you have? A tensorflow.js web model? If it is a plain tensorflow model you can probably convert it to a web model using the tfjs-converter tool.=====', ""Sorry, I'm new to TensorFlow. I'd need the outputs of face classificationto include gender and approximate age. I currently don't know how to do it,not even in a way that would result in a model compatible with this API.On Thu, Nov 29, 2018, 6:00 PM justadudewhohacks <notifications@github.comwrote:> Not sure if I understood correctly.>> If you want to train your own model with transfer learning, I would> actually try to use the face landmark model and replace the fully connected> layer with an own one that outputs male / female.>> Regarding the second question: What kind of model do you have? A> tensorflow.js web model? If it is a plain tensorflow model you can probably> convert it to a web model using the tfjs-converter tool.>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/106#issuecomment-442991296>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AmUwmR5cAzKmjR2T48aQ2qAnYBzMkKxdks5u0ErngaJpZM4XxM-b>> .>====="", 'Sorry again. If I had a couple of TensoFlow.js Web models for gender / age classification of faces, could I ""pipeline"" them with each detected face from this API, maybe using single image recognition (not object detection), augmenting the face output predictions with these attributes?=====', 'Yes, I mean the input of such a model would probably be the image of a face right? So you basically use face-api.js to detect and extract faces from an image and pass these into your network. Does that answer your question?=====', ""Yes, and thanks! I've just saw it was an enhancement request and tought itcould be part of the API itself. Again, sorry for my lack of knowledge.On Thu, Nov 29, 2018, 6:39 PM justadudewhohacks <notifications@github.comwrote:> Yes, I mean the input of such a model would probably be the image of a> face right? So you basically use face-api.js to detect and extract faces> from an image and pass these into your network. Does that answer your> question?>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/106#issuecomment-443002806>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AmUwmYEz6894ojLr39cYViA3zpLN7D3Mks5u0FP1gaJpZM4XxM-b>> .>====="", 'Some datasets:https://susanqq.github.io/UTKFace/https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/=====', 'Thanks! It will be of great help.On Sat, Dec 1, 2018, 1:45 PM justadudewhohacks <notifications@github.comwrote:> Some datasets:> https://susanqq.github.io/UTKFace/> https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/106#issuecomment-443439468>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AmUwmfuTQHfq6zVPIyp-RBAwBuEb5NpYks5u0rIpgaJpZM4XxM-b>> .>=====', 'Yeah, age and gender recognition is not implemented yet, I am collecting some resources at the moment.=====', '+1 from me, gender detection would be a great addition=====', ""Age, gender, ethnicity recognition is next on my list. Fortunately there is lot's of data available. Can't promise any ETA yet.====="", ""That's great!====="", 'A dataset from IBM https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/#access=====', 'Hi,Thank you so much for the library, great work! I also find the age and gender recognition quite interesting and had seen that you had already started the implementation in a branch. Can you already give an estimate when you will release it?=====', ""I am working on it a little bit more actively currently. Can't give any estimations yet when to I will release it, sorry. I have some first working versions, but I am constantly improving the model accuracy, trying to get close to state of the art.====="", ""> I am working on it a little bit more actively currently. Can't give any estimations yet when to I will release it, sorry. I have some first working versions, but I am constantly improving the model accuracy, trying to get close to state of the art.First off - amazing work on this ,) is it possible to play around with your current implementation? do you have it in a branch that we can access?====="", 'Age estimation and gender prediction is now implemented.=====', ""That's great news! ====="", 'script.js:26 Uncaught (in promise) TypeError: faceapi.detectAllFaces(...).withFaceLandmarks(...).withFaceExpressions(...).withAgeGender is not a functionI got this error message while running,please help me out=====', 'it should be withAgeAndGender and not withAgeGender=====', 'thank you and for drawfaceapi.draw.drawAgeAndGender is not a function=====', 'hello where do i found the code for recognizing two image without node and using javascript=====', '@thirukumars hello, i\'m getting same error, ""faceapi.draw.drawAgeAndGender is not a function""did you solve that?=====']"
https://github.com/justadudewhohacks/face-api.js/issues/105;WebGL Warning when using this API;2;closed;2018-10-17T19:08:16Z;2018-10-23T17:46:24Z;"Hello there,I'm quite new to using Javascript and this API, and having read the tutorials and examples multiple times I'm having trouble getting this API to work. When running certain methods such as faceapi.tinyYolov2, the code fails and gives the following error:`Error: WebGL warning: texImage2D: Element is write-only, thus cannot be uploaded. (source: face-api.js: line 23: col 132152)`I also get this error (same location) when running the method faceapi.locateFaces (and possibly during some more methods).   This is my code:```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>Face Recognition Test</title>    <script src=""../../modulesExtern/face-api/dist/face-api.js""></script></head><body><div id=""container""><img id=""image"" src=""https://cdn-images-1.medium.com/max/1000/1*5XojINtKrkHFT3nOx6oW1g.jpeg""><canvas id=""myCanvas"" width=""200"" height=""100""></canvas><script type=module>    const MODEL_URL = '../../modulesExtern/face-api/weights',    let scoreThreshold = 0.5,    let sizeType = 'lg',    async function main() {        await faceapi.loadTinyYolov2Model(MODEL_URL),        const inputImgEl = document.getElementById(""image""),        const { width, height } = inputImgEl,        const canvas = document.getElementById(""myCanvas""),        canvas.width = width,        canvas.height = height,        const forwardParams = {            inputSize: sizeType,            scoreThreshold        },        const detections = await faceapi.tinyYolov2(inputImgEl, forwardParams)        // faceapi.drawDetection('myCanvas', detections.map(det => det.forSize(width, height)))    }    main(),</script></div></body></html>```Does anyone know what I'm doing wrong and/or how to solve it?";"['`<img id=""image"" src=""https://cdn-images-1.medium.com/max/1000/1*5XojINtKrkHFT3nOx6oW1g.jpeg"">`You\'re trying to load an image from an external source, which is a security problem. The way I got around that was to convert the remote image to a data uri and use that as the src of the image tag.=====', 'This indeed was the issue. Thank you very much for your help! :) =====']"
https://github.com/justadudewhohacks/face-api.js/issues/104;Error on release version;6;closed;2018-10-17T13:22:38Z;2018-10-18T06:19:00Z;Dear when using the local version (debug version) the plugin works finewhen building a release version we get an error![error](https://user-images.githubusercontent.com/3799066/47089120-bb6fef00-d228-11e8-93fd-6f19918b5285.png)tho we installed an local certificate for ssl to run https since i thought it was a problem from the httpsregards;"['Looks like your loading of the JSON is failing. Are you sure the path to the json files for your release version is correct?=====', 'dear friend,we didn’t upload any json, just on debug mode it works fine on release it doesn’t work it has nothing to do with json could be a react version ? or a node version on release doing a problem ?On Oct 17, 2018, at 10:14 PM, ScottDellinger <notifications@github.com<mailto:notifications@github.com>> wrote:Looks like your loading of the JSON is failing. Are you sure the path to the json files for your release version is correct?—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/104#issuecomment-430753834>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ADn4GuPbMcrxRg8HEgKneK0EUw5GBpV4ks5ul4GzgaJpZM4Xj599>.=====', 'Not sure what debug and release mode means for you, but this error message comes from the model loading function, meaning in your release setup the uri to your model files are different.=====', 'okay thank you so much tomorrow i will check iti’m using reactwhen building a release version (the version to be publish on a server) it looks like it’s changing something in the path i will check it back tomorrow and get back to youthank you once againhave a nice eveningOn Oct 17, 2018, at 11:51 PM, justadudewhohacks <notifications@github.com<mailto:notifications@github.com>> wrote:Not sure what debug and release mode means for you, but this error message comes from the model loading function, meaning in your release setup the uri to your model files are different.—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/104#issuecomment-430784429>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ADn4GlzQRh5GlMs4smuE9RQrDNVol1cgks5ul5g0gaJpZM4Xj599>.=====', 'Alright, if in doubt, simply open the network tab in the dev tools of your browser and you should find out the uri of the request when loading the model.=====', 'thank you guys, yes on release version it\'s loading the path directly from localhost (react) not from the folder where the published website isi will check the file and edit it it\'s better to use ""./path"" so we won\'t have the errorthank you again=====']"
https://github.com/justadudewhohacks/face-api.js/issues/103;How to use my model in face-api to recognize face;4;closed;2018-10-15T08:43:58Z;2018-10-18T01:07:49Z;The face-api.js throw error when I load my model. I found my model different with face-api's model by checked my model json .How to use my model in face-api to recognize face?;"['Which model are you using? What kind of error is thrown?=====', ""The error is ' expected weightMap[conv32_down/conv/filters] to be a Tensor4D, instead have undefined'.There is a picture for my code.![default](https://user-images.githubusercontent.com/39442460/46986537-88dbce80-d122-11e8-8fb4-e87f1866ea47.png)My model has trained to use face recognition.My model's data structure didn't same with face-api's model.It look like no some parameters in my model.![default](https://user-images.githubusercontent.com/39442460/46987068-19b3a980-d125-11e8-92a9-cae8f7f0499c.png)====="", 'You can not simply load your own models, this library only allows you to load the provided models. If you want to load own models, use tfjs.=====', 'Thanks,I tried other palne to finish it.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/102;faceapi.FaceLandmark68TinyNet() bounding boxes?;5;closed;2018-10-09T06:26:04Z;2018-10-12T05:58:32Z;I just want to say that I really appreciate the work you have put into **face-api.js**. I am having a bit of trouble getting things working but I do have a reasonably easy version of faceapi.FaceLandmark68TinyNet() running [here](https://hpssjellis.github.io/beginner-tensorflowjs-examples-in-javascript/advanced-keras/face/44-face-knn-classifier.html) using the TFJS [knn-classifier](https://github.com/tensorflow/tfjs-models/tree/master/knn-classifier) for face recognition. I would like to make my model a bit more accurate by using and showing bounding boxes. From your examples I am not sure if I can stay with the FaceLandmark68TinyNet or if I have to switch to one of the other models (SSD Mobilenet, Tiny Yolo or MTCNN) to get and use the bounding boxes?;"[""Hmm I think I don't quite understand. Usually you have to detect the bounding boxes before landmark detection, because the landmark net expects the input image to contain roughly only the facial region.The type of face detector to use depends on your use case. If SSD works for you, stick with it, since it is the most accurate currently. If want to track faces in realtime, for example from a webcam use Tiny Yolo. Also soon I will push a new face detecion model, which is faster, smaller and more accurate than the current Tiny Yolo, but not quite as accurate as SSD, though.Edit.: You can also use FaceLandmark68Net instead of the tiny net, which has higher accuracy, especially on side ways turned faces.====="", 'I will try your new model when you push it. Presently I am having lots of success with this small amount of code. It sounds like it is detecting the bounding boxes I just have to figure out how to layer the boxes onto the video. HTML element: (not shown here some basic HTML5 code to activate the webcam)```<video  id=""myVideo"" width=""300"" height=""200"" style=""border: 1px solid #ddd,""></video>```Javascript:```   net = new faceapi.FaceLandmark68TinyNet()                                                                                  await net.load(\'models/face_landmark_68_tiny_model-weights_manifest.json\')                                                                                              const img2 = tf.fromPixels(myVideo),                                                                                       const myLandmarks = await net.detectLandmarks(img2)    ```I think that before running net.detectLandmarks(img2) I need to align the faces within the bounding boxes, but I am having difficulties figuring out how to do that. Most of the example code does not seem to work for me.=====', ""`FaceLandmark68TinyNet` and `FaceLandmark68Net` don't predict bounding boxes first, they predict landmark position straight from the input image. In your case that probably works, because the frames of your video (you are probably using your webcam?) are mostly solely showing the face. Stepping back from the camera a bit, you will probably see that the landmarks are not predicted correctly anymore, that's why you usually detect the bounding box first using one of the face detectors.The library also has utility functions to draw bounding boxes onto a canvas. If you take a look at the examples, I am laying absolutely positioned canvases on top of the html img and video elements.====="", ""Thanks. I am getting it slowly, the layered canvas makes sense and I do have bounding boxes running using  this code:```      result2 = await faceapi.tinyYolov2(videoEl, forwardParams)      faceapi.drawDetection('overlay', result2.map(det => det.forSize(width, height)))```Just going a little slow on using the above information to resize the following code: ` const myLandmarks = await net.detectLandmarks(img2) `Not sure how to map the new width and height to the above code.P.S. Your example code is great I just can't get all of them working. You don't have any live demos anywhere do you?====="", 'My code is really messy and for some reason I had to use 4 canvases, but I got it working so I can close this issue.  See live demo [here](https://hpssjellis.github.io/beginner-tensorflowjs-examples-in-javascript/advanced-keras/face/backups44-face-drawI.html)  I will be making it better but that demo will work for a while.Once again. Thanks for getting faceapi.js working with tensorflowjs=====']"
https://github.com/justadudewhohacks/face-api.js/issues/100;Distance often quite low, regardless of faces - How to improve accuracy?;5;closed;2018-10-05T01:44:10Z;2018-10-05T18:23:04Z;I've been doing some tests today and am using the following images:![image](https://user-images.githubusercontent.com/16200100/46511671-1f4bee00-c80d-11e8-9a77-f9d455b955a8.png)The bottom 4 images are all the same person and the top 4 are different people. Comparing the bottom, third from the left with the first top image (the female) gets  0.4439...All of them are below 0.6. Many are 0.41 to 0.50.Do you have any suggestions on how to improve the accuracy?;"['Additionally, despite the bottom 4 being the same person, there is a stronger match between them with the images of other people in many cases.=====', 'I get a distance of `0.8425371094302093` compraing the top left image with the bottom third.Can you show me what you are doing (code snippet). Are you performing face detection or pass the whole image into the recognition net?Are you using an Intel GPU?=====', 'Thanks for the prompt reply!It doesn\'t seem to matter what machine I test on... iPhone 6 in Safari reports almost the same (like .01 difference here and there) as Chrome on my Windows 10 machine with an Nvidia card and Firefox on my Pixel 2.I\'m passing the whole photo in. Here\'s the snippet:`$(document).on(""click"", "".btn"", function () {                var _ = $(this),                Compare(_.data(""img1""), _.data(""img2"")),            }),`        `async function Compare(img1, img2) {            await faceapi.loadFaceRecognitionModel(""/Content/FaceAPIModels""),            const descriptor1 = await faceapi.computeFaceDescriptor(img1),            const descriptor2 = await faceapi.computeFaceDescriptor(img2),            const distance = faceapi.euclideanDistance(descriptor1, descriptor2),            alert(distance),        }`And here\'s the button tag for the image comparison: `<input type=""button"" value=""Compare S1 and O3"" class=""greenbutton btn"" data-img1=""s1"" data-img2=""o3"" />`If you have suggestions or a different code sample I should try, that would be fantastic.=====', 'You have to perform face detection first, dont pass the entire image to computeFaceDescriptor, that wont give you a correct face vector. Therefore you can use faceapi.allFaces as in the face recognition examples and as stated in the readme and introduction article.=====', ""Thanks... that makes sense. Got it working now. It's super quick, once the models/etc are loaded!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/99;FR: Custom builds with selected models;4;closed;2018-10-01T21:23:31Z;2018-10-04T12:09:34Z;Thanks for creating this lib, it's a best starting point to do image recognition with JS as to me. Extensive README with links, plus working demo examples - priceless!However, when you're down with experiments and want to have a workable solution 400kb lib size seems a lot. I know face-api.js is built on top of tensorflow.js core + various utilities + models. So one answer for users who need optimisation is to start using tfl.js itself. But if that solution is too low level maybe it could be a good idea to have a pre-built versions or custom build params (like here: https://lodash.com/custom-builds) covering only selected models. With that a user could choose between variety and total bundle size.;"[""Hi,Sorry if I don't quite understand. tfjs-core really is the only dependency of this package and the minified size of tfjs-core already happens to be ~320kb as it seems. So I am not sure if it is possible to decrease the bundle size much further.> So one answer for users who need optimisation is to start using tfl.js itself.What is tfl.js?I am not sure if trying to minimize the bundle size does make sense, since you are loading model files, which are much larger in size anyways.====="", ""Sorry, I was just meaning tensorflow.js.My point was that if I want to use face-api.js and use *only one* model I would love to have there size of a library to decrease - yes I'm not using other models and utilities related to them.====="", 'I see what you mean. However, I think you will at most save a few kilobytes with this approach. Again, not sure if it is worth the effort.=====', ""Ok thanks for the details. I didn't have time to go through all the code to understand it's modularity. If living out specific model related parts doesn't make the bundle smaller then of course it's not relevant.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/96;Don't understand await keyword;4;closed;2018-09-22T14:56:30Z;2018-10-23T17:46:42Z;Hey,I have a question on the `await` keyword. In your examples, you say like : ```jsconst detections = await faceapi.ssdMobilenetv1(myImg, minConfidence, maxResults),```But I have this error : ![synthax_error](https://user-images.githubusercontent.com/35373357/45918509-583ca800-be88-11e8-9044-9ea73ff43ebb.png)Is it the new syntax of JavaScript or am I missing something?ThanksJoseph;"['Hi @johhnry ,`await` can not be used in regular functions, put it in a `async` function. `async` and `await` are javascript ES7 features, you can understand how to use it via [this post](https://javascript.info/async-await).=====', 'I understood how it works.Now I have a problem with loading the model.Here is my code : ```jsconst net = new faceapi.Mtcnn(),let img,function preload(){    img = loadImage(""img.jpg""),}function setup(){    canvas = createCanvas(1200,630),    canvas.id(\'canvas\'),    image(img,0,0),        async function detect(){        console.log(""Detecting...""),        const forwardParams = {            maxNumScales: 10,            scaleFactor: 0.709,            scoreThresholds: [0.6, 0.7, 0.7],            minFaceSize: 20        }                  const results = await faceapi.mtcnn(document.getElementById(\'canvas\'), forwardParams),        console.log(results),    }    async function loadNet(){        console.log(""Loading...""),        await net.load(\'models/mtcnn_model-weights_manifest.json\'),    }    loadNet().then(() => {        console.log(""Net loaded""),        detect(),    }),    noLoop(),}```I am using another library called p5.js but it doesn\'t matter.I have this : ![image](https://user-images.githubusercontent.com/35373357/45927122-8d9bd100-bf2e-11e8-9f99-9b502931875e.png)I tried all the solutions to load a model (creating an instance with the json file, using faceapi.loadMtcnnModel ) but I can\'t load the model.Thanks again=====', ""If you are creating a new instance like this: `const net = new faceapi.Mtcnn(),`, then your detection code should be  ` await net.forward(document.getElementById('canvas'), forwardParams)` instead of `await faceapi.mtcnn(document.getElementById('canvas'), forwardParams),`, otherwise you are using the global mtcnn instance. If you want to use `faceapi.mtcnn` you would have to load the mtcnn model like this: `await faceapi.loadMtcnnModel('models/mtcnn_model-weights_manifest.json')` instead of `await net.load('models/mtcnn_model-weights_manifest.json'),`.====="", 'Thank you so mush for your answer, it works very well ! :+1: Open Source is so much appreciated :)Just a little warning : ![image](https://user-images.githubusercontent.com/35373357/46111641-0da09000-c1e8-11e8-8c92-9f9e21403748.png)=====']"
https://github.com/justadudewhohacks/face-api.js/issues/95;Cannot read property 'reshape' of undefined;2;closed;2018-09-20T08:23:29Z;2018-11-13T09:04:22Z;Hi @justadudewhohacks,I follow the steps here https://github.com/justadudewhohacks/face-api.js#usage-face-detection-yolohere is my code:```javascriptconst inputImgEl = $('#output_image').get(0)const forwardParams = {    inputSize: sizeType,    scoreThreshold}const detections = await faceapi.tinyYolov2(inputImgEl, forwardParams)```but it has this error:![image](https://user-images.githubusercontent.com/10130649/45805329-14636a80-bcf1-11e8-948e-72fa186dc071.png);['Could you provide me with some code to reproduce the issue? What are the dimensions of your inputImgEl and which sizeType did you specify?=====', 'Closing because of inactivity.=====']
https://github.com/justadudewhohacks/face-api.js/issues/92;bounding boxs coordinates;7;closed;2018-09-13T08:49:53Z;2018-10-25T20:09:34Z;Hi does anyone can output the bounding boxes coordinates of the **result**?https://github.com/justadudewhohacks/face-api.js/blob/fdfedbecf64193b4c92ad680bf0f83a60830479b/examples/views/tinyYolov2FaceDetectionWebcam.html#L106Thanks!;"['Not quite sure what you mean by that. `result` is an array of bounding boxes.=====', 'Hi I followed the steps of [here](https://github.com/justadudewhohacks/face-api.js#shortcut-functions) and modified the code to below:```jsconst result = await faceapi.tinyYolov2(videoEl, forwardParams)const result0 = result[0]console.log(result0.detection)```but in browser console:![image](https://user-images.githubusercontent.com/10130649/45491003-2f872500-b79b-11e8-8c23-c5b1c321cc45.png)Does any problem here?Thank you very much!=====', 'Sorry I solved by my self!I got the bounding boxes coordinates using `result[0].box` it returns `Rect\xa0{x: 238.43092550152085, y: 147.76242547896555, width: 229.86379377029357, height: 278.7513614399462}` =====', ""Hello, thanks for this api and I'm still learning javascript, I have some questions.Could you explain how **faceapi.extractFaces()** works? What's its input format and output type?I want to extract every face in the video, and I wrote these code:```javascriptconst result = await faceapi.tinyYolov2(videoEl, forwardParams)if (result.length != 0) {     result.forEach(function(element) {        const faceImages = await faceapi.extractFaces(videoEl, element.box)    }),}```Does it seem reasonable?Thank you very much!====="", '> /**>  * Extracts the image regions containing the detected faces.>  *>  * @param input The image that face detection has been performed on.>  * @param detections The face detection results or face bounding boxes for that image.>  * @returns The Canvases of the corresponding image region for each detected face.>  */> export async function extractFaces(>   input: TNetInput,>   detections: Array<FaceDetection | Rect>> ): Promise<HTMLCanvasElement[]>Implemented [here](https://github.com/justadudewhohacks/face-api.js/blob/master/src/dom/extractFaces.ts).So yes it looks fine to me.=====', '@justadudewhohacks thanks for the reply!but when I call the **faceapi.extractFaces()** , it shows![image](https://user-images.githubusercontent.com/10130649/45737874-88364200-bc21-11e8-8018-1db24ad4f3ed.png)my code : ```javascriptconst result = await faceapi.tinyYolov2(videoEl, forwardParams)if (result.length != 0) {     result.forEach(async function(element) {        const faceImages = await faceapi.extractFaces(videoEl, element.box)    }),}```=====', '`extractFaces `expects an array of detections not a single element as stated in the signature, thus -> `faceapi.extractFaces(videoEl, [element.box])`=====']"
https://github.com/justadudewhohacks/face-api.js/issues/90;Preloading of the MTCNN model is very slow.;8;closed;2018-09-12T14:44:01Z;2021-09-14T05:38:38Z;"Hello,I'm using VueJS. When you open a Modal, these codes are working, which I created from your sample code. The value of ""console.time"" is quite high.**Code:**```JSconsole.time(""fullFaceDescriptions""),vm.yuz_degisken_objesi.fullFaceDescriptions = (await faceapi.allFacesMtcnn(vm.yuz_degisken_objesi.videoEl, vm.yuz_degisken_objesi.mtcnnParams))    .map(fd => fd.forSize(vm.yuz_degisken_objesi.hw.width, vm.yuz_degisken_objesi.hw.height)),console.timeEnd(""fullFaceDescriptions""),console.log(""*""),```fullFaceDescriptions: **5500.3720703125ms***fullFaceDescriptions: **372.788818359375ms***fullFaceDescriptions: **257.229736328125ms***fullFaceDescriptions: **180.324951171875ms***fullFaceDescriptions: **222.89404296875ms***fullFaceDescriptions: **207.50927734375ms***How can I make it more stable?";"['More stable or faster? Faster inference time of the mtcnn detector can be achieved by increasing the minFaceSize parameter for example, which will skip the detection of smaller faces.=====', 'Preloading? What can be done for this?fullFaceDescriptions: **5500.3720703125ms**=====', 'Nothing really that I know of, seems like the first time a tensor of a certain shape is uploaded to the GPU there is an initial delay, which they refer to as ""warmup"" in the mobilenet example. The MTCNN uploads tensors of N + 2 different shapes (N in stage 1, can be adjusted by the `maxNumScales` forward parameter and 1 each in stages 2 and 3). This unfortunately causes the first forward pass of MTCNN to be much longer on certain machines.I am not sure where this inital delay comes from (maybe due to allocating textures of certain sizes on the GPU, just a wild guess, I am not that familar with WebGL for GPGPU). If it concerns you, maybe you could ask at tfjs for help.PS: I would also be interested in the answer to that.=====', ""Here's a [comment](https://github.com/tensorflow/tfjs-examples/blob/02e330e43d7234bc4002a836586141812ec08ee7/mobilenet/index.js#L35) from a tfjs-example: > Warmup the model. This isn't necessary, but makes the first prediction faster.This warmup works on `computeFaceDescriptor`, but not works on `mtcnn` or `allFacesMtcnn` on my project. Maybe you can make a predict first after the model is loaded.====="", ""I've been fighting with this for a week or so now... I'm currently looking into web workers as a way to do an initial prediction without tying up the main UI thread so that my actual predictions later on are lightning-fast. I'll let you know if I make any progress.====="", ""Alas... no such luck. I think I'd have to understand tfjs much better than I do to convert it in such a way to get it to work in a web worker. I have gotten it so far as to attempt to load the models, but the registration of the backend failed. Some Googling tells me that webgl isn't supported in Web Workers, but cpu backend also fails to register.====="", 'Before you are putting too much work into this, I will soon publish some new changes, including a new tiny face detector, which is much faster than ssd and mtcnn and produces much more stable detection results than mtcnn.I think at that point there will be no real reason anymore to use the mtcnn, unless there is a way to fix the warmup issue.=====', '@justadudewhohacks Awesome! I look forward to this.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/89;Issue with parcel build;3;closed;2018-09-12T14:21:34Z;2018-09-13T15:15:54Z;"Trying to bundle face-api.js for an SPA, similar setup to this:https://github.com/tensorflow/tfjs-examples/blob/master/addition-rnn/package.json#L17We're trying to do this:```javascriptimport * as faceapi from 'face-api.js',```Getting this error on the Parcel build:```~/workspace/project/node_modules/face-api.js/build/tinyYolov2/TinyYolov2.js:1:25: Cannot resolve dependency 'tslib'> 1 | import * as tslib_1 from ""tslib"",    |                          ^  2 | import { TinyYolov2 as TinyYolov2Base } from 'tfjs-tiny-yolov2',  3 | import { FaceDetection } from '../classes',  4 | import { BOX_ANCHORS, BOX_ANCHORS_SEPARABLE, DEFAULT_MODEL_NAME, DEFAULT_MODEL_NAME_SEPARABLE_CONV, IOU_THRESHOLD, MEAN_RGB_SEPARABLE, } from './const',```I looked at the package.json, and it seems like `tslib` should be in `dependencies`, not `devDependencies`. We're working around currently by doing this:```javascriptimport * as faceapi from 'face-api.js/dist/face-api'```Seems hacky. The other option is setting `main` to the dist file.";"['Probably because the build output is still using es6 imports, I think the output should be commonjs again, not sure atm why I changed that.=====', ""@justadudewhohacks Not sure why ES6 modules would cause this issue... The issue is being able to refer to `node_modules/tslib`. It's in `devDependencies`, which means that it will never be installed when we are consuming face-api.js.Tensorflow.js uses ES6 modules, and it builds fine. See `@tensorflow/tf-js` import here:https://github.com/tensorflow/tfjs/blob/master/src/index.ts#L18See here how the `@tensorflow` modules are in `dependencies`?https://github.com/tensorflow/tfjs/blob/master/package.json#L62From what I understand, TD import helpers will emit common functions to utilize `tslib` instead of duplicating functions like `__extends`, with the expectation that `tslib` will be installed as a dependency. So `tslib` defines `__extends` once and all references to it are rewritten  to call `tslib`. Doesn't necessarily cause issues in SPA or server projects when you put it into `devDependencies`, but for node modules, I think it has to be in `dependencies`.If you're still not convinced, look at the `tslib` readme. Notice the `npm install --save tslib` command, not `--save-dev`:https://github.com/Microsoft/tslib/blob/master/README.md#npmMaybe this is an oversight but that seems doubtful.What would be the drawback of including `tslib` in `dependencies`? Am I missing something?====="", ""> @justadudewhohacks Not sure why ES6 modules would cause this issue... The issue is being able to refer to node_modules/tslib. It's in devDependencies, which means that it will never be installed when we are consuming face-api.js.Yep you are absolutely right. Sorry for the confusion here.Besides moving tslib from devDependencies to dependencies, I think the output should still be commonjs instead of es6, but that's another story.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/88;Fail to provide models as as static assets;7;closed;2018-09-11T06:43:43Z;2019-12-03T15:52:33Z;"I've read through the post from https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07It mentioned that ""The model files can simply be provided as static assets in your web app"". So I've done two different approach and failed.1. Integration with vue.js: In this approach, then i got the following error,Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0    at eval (loadWeightMap.js?3bcc:12)    at step (tslib.es6.js?9ab4:97)    at Object.eval [as next] (tslib.es6.js?9ab4:78)    at fulfilled (tslib.es6.js?9ab4:68)2. Native approach:  A different error,loadWeightMap.js:12 Fetch API cannot load file:///Users/ian.chu/demo-face-api/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ loadWeightMap.js:12It seems to me that models can't be provided as static assets, but served by the web host. Could anyone correct me or show me how it can be done, I will be very grateful. Thanks.";"['Hi,1: The issue might be due to, how assets are served in vue.js (PS: I am not familar with vue.js), e.g. the uri might simply be wrong. Open the network panel and verify, that the uri of the fetch request is correct.2: I think it should be selfexplaining, that a browser can not fetch from your filesystem.=====', '@Chuiantw1212Unexpected token < in JSON at position 0 - this issued occurred to me when I was loading all model files at the same time. Use babel-polyfill to implement async and await or use Promise function. Everything loads up, But then I got this error  Error: Constructing tensor of shape (4608) should match the length of values (876). The same code worked when using reactjs but do not know why not on vuejs. =====', 'As I already pointed out the root of the issues with loading the models lies in requesting them from a wrong route. Investigating the outgoing request in the network panel should solve it.=====', 'I am have the same problem using Vue.  I am storing the weights in ./assets.  I am wondering if there needs to be a webpack loader added to load the weights.  How did you fix this problem?=====', '> I\'ve read through the post from https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07> > It mentioned that ""The model files can simply be provided as static assets in your web app"".> So I\'ve done two different approach and failed.> > 1. Integration with vue.js: In this approach, then i got the following error,>    Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0>    at eval (loadWeightMap.js?3bcc:12)>    at step (tslib.es6.js?9ab4:97)>    at Object.eval [as next] (tslib.es6.js?9ab4:78)>    at fulfilled (tslib.es6.js?9ab4:68)> 2. Native approach:  A different error,>    loadWeightMap.js:12 Fetch API cannot load file:///Users/ian.chu/demo-face-api/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.>    (anonymous) @ loadWeightMap.js:12> > It seems to me that models can\'t be provided as static assets, but served by the web host.> Could anyone correct me or show me how it can be done, I will be very grateful. Thanks.Did you find any solution? getting the same errors in both the ways.=====', 'simply put the models into the ./public and not the ./src/assets directory of your vue project=====', 'seams reasonable =====']"
https://github.com/justadudewhohacks/face-api.js/issues/86;euclidean distance ;5;closed;2018-09-07T16:05:45Z;2019-07-16T18:23:23Z;Euclidean distance never goes down below 0.4. What is the ideal number of picture of one person to train the model. Can we implement svm or multiclass classification here ?;"['0.4 is actually pretty low, your decision threshold should be 0.6. There is no ideal number really, usually one image of the face in a frontal pose is enough.You are not forced to use euclidean distance, you can implement any kind of classifier that you want and feed the face descriptor into that classifier.=====', 'O.4 is the distance therefore confidence is 1-0.4 = 0.6, so i am saying that why this confidence not going beyond 0.6 or 0.65 even with the 5 pictures of same person ? Since i have implemented face recognition in Python using dlib which is giving confidence upto 0.98. Also there is quit differences between dlibs face encoding of python based dlib model and this face-api.js based dlib model although they are of same dimensions which is 128Thanks=====', ""`confidence is 1-0.4 = 0.6` I don't think you can directly translate the distance metric to confidence score like that.Are you using an intel GPU? The model should be the same as the dlib one, maybe there are some minor difference due to weight quantization. Actually I didn't see that large of difference between the python model and the model exposed here, maybe one would have to investigate this closer with a larger benchmark.====="", ""@justadudewhohacks > confidence is 1-0.4 = 0.6 I don't think you can directly translate the distance metric to confidence score like that.If this is not the best way to calculate confidence score (%) in recognition, what would be the best way?====="", 'There is not really a way to calculate a confidence score. The lower the distance the more similar two faces are. A distance lower than 0.6 means, that the faces are very likely from the same person.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/81;Same code, different results (0.11.0/0.12.0);3;closed;2018-09-02T14:31:37Z;2019-01-17T09:31:32Z;Hi,I was locating faces with SSD Mobile Net (faceapi.locateFaces) because with other nets (MTCNN, Tiny Yolo v2) the result was unstable. Now I updated the library from 0.11.0 to 0.12.0 version and same code produces different results. At 0.11.0 version faceapi.locateFaces was stable but with 0.12.0 version is unstable (like MTCNN, Tiny Yolo v2 at 0.11.0).Greetings and thanks for the repository.;"[""Hmm, I couldn't notice any difference regarding ssd mobilenet . Can you upload a picture to show me what kind of differences you are seeing?The only real difference between 0.11.0 and 0.12.0 is, that I upgraded tfjs-core from 0.11.9 to 0.12.14.====="", 'yep, the problems come with tfs-core >= 0.12.0. With unstable I want to say multiple detection in same region and continuous interruptions while detecting faces.=====', 'If this is still an issue with the recent version feel free to reopen, giving atleast one example showing the issue.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/76;How to use quantized as Float32Arrays;4;closed;2018-08-26T12:43:09Z;2018-11-13T09:01:31Z;I noticed that the weight files are missing since the model weights have been quantized,Does this mean that we now should use the shards if we want to load the weights as a Float32Arrays?How would the following code then work?```// using fetchconst res = await fetch('/models/face_detection_model.weights')const weights = new Float32Array(await res.arrayBuffer())net.load(weights)// using axiosconst res = await axios.get('/models/face_detection_model.weights', { responseType: 'arraybuffer' })const weights = new Float32Array(res.data)net.load(weights)```;"['You cannot pass quantized weights as Float32Arrays to net.load, because dequantization requires meta data, such as `scale `and `min` for each tensor. Why do you want to load quantized weights as a Float32Array in the first place?=====', 'Because the uncompressed weights have been removed in this repo, so I though that if I want to use http/axios then that would be the way to go.How would I then use this like that?=====', ""If you want to load quantized weights simply use net.load('uri'). This downloads the weights using fetch and then dequantizes the weights (this step is just a call to tfjs-core actually).May I ask, why you want to use axios for that?By the way, if for some reason you still want to use the weights without quantization, you can still download them from [face-api.js-models](https://github.com/justadudewhohacks/face-api.js-models/tree/master/uncompressed).====="", 'Closing here now, since question should be answered.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/74;getBox() Not a Function;8;closed;2018-08-22T18:54:59Z;2019-08-01T02:22:57Z;I've tried the recent and last version of the face-api.js and get the same error with both.Uncaught (in promise) TypeError: det.getBox is not a function    at face-api.js:1438    at Array.forEach (<anonymous>)    at Object.drawDetection (face-api.js:1437)    at getFace (facetest.php:41)(anonymous) @ face-api.js:1438drawDetection @ face-api.js:1437getFace @ facetest.php:41async function (async)getFace @ facetest.php:23(anonymous) @ facetest.php:20j @ jquery-3.2.1.min.js:2k @ jquery-3.2.1.min.js:2setTimeout (async)(anonymous) @ jquery-3.2.1.min.js:2i @ jquery-3.2.1.min.js:2fireWith @ jquery-3.2.1.min.js:2fire @ jquery-3.2.1.min.js:2i @ jquery-3.2.1.min.js:2fireWith @ jquery-3.2.1.min.js:2ready @ jquery-3.2.1.min.js:2S @ jquery-3.2.1.min.js:3Any help would be appreciated.;"['Could you show me your code, looks like you are passing something to drawDetections, which is not an instance of FaceDetection.=====', '`<html><head><script src=""https://code.jquery.com/jquery-3.2.1.min.js""></script><!-- <script src=""tensor.js""></script> --><script src=""face-api.js""></script><script src=""commons.js""></script><style>.face {   position: absolute,   border: 2px solid #FFF,}</style></head><body>\t<img id=""picture"" style=""""src=""facetest.jpg"">\t<canvas id=""overlay""></canvas></body><script>\t$(document).ready(function() {\t\tvar image = document.getElementById(\'picture\'),\t\tgetFace(),\t}),\tasync function getFace() {\t\tawait faceapi.loadFaceDetectionModel(\'models/weights\')\t\tawait faceapi.loadFaceLandmarkModel(\'models/weights\')\t\tawait faceapi.loadFaceRecognitionModel(\'models/weights\')\t\tconst myImg = document.getElementById(\'picture\'),\t\tconst minConfidence = 0.8\t\tconst maxResults = 10\t\tconst detections = await faceapi.allFaces(myImg, minConfidence)\t\t\t\t\t\t\t// resize the detected boxes in case your displayed image has a different size then the original\t\t//const detectionsForSize = detections.map(det => det.forSize(myImg.width, myImg.height))\t\t//console.log(detectionsForSize),\t\tconst canvas = document.getElementById(\'overlay\')\t\tcanvas.width = myImg.width\t\tcanvas.height = myImg.height\t\t\t\tfaceapi.drawDetection(canvas, detections, {withScore: true}),\t\t\t}</script></html>`I\'ve tried including tensorflow, but when I do I get the following error as well.I cant use anything outside of straight .js (in terms of a framework) for the given project and your code seemed to be a perfect fit for what we need to do, (detect faces, I\'d attempt to pull the faces afterwards to be send via API to a 3rd party).(ERROR w/ TENSOR.js included)tf-core.esm.js:17 Uncaught (in promise) Error: Argument \'tensors[0]\' passed to \'concat\' must be a Tensor, but got object.    at assert (tf-core.esm.js:17)    at assertArgumentIsTensor (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at n (tf-core.esm.js:17)    at assertArgumentsAreTensors (tf-core.esm.js:17)    at e.concat (tf-core.esm.js:17)    at tf-core.esm.js:17    at e.tidy (tf-core.esm.js:17)    at n.value (tf-core.esm.js:17)=====', 'I am getting information when I console.log out the original allFaces detection.  So I know its seeing the faces (tho only 4 of the 5 in my test image).=====', 'And ignore anything commented out - I was trying to run it down and left them  Just noticed myself=====', 'Think I may have figured this out.  I appologize, seems it was me not understanding the information.  I retreived the detection from the object and send that to the drawDetection and I got a result as expected.=====', 'Ah yeah you have to get the FaceDetection objects out of the result of allFaces. If anything is still unclear, try to take a look at the what allFaces returns [here](https://github.com/justadudewhohacks/face-api.js#shortcut-functions-for-full-face-description).=====', 'script2.js:38 Uncaught (in promise) TypeError: faceapi.drawDetections is not a function    at onPlay (script2.js:38)I have this error in browser face_detection code=====', '.anyone reply ,,give the solution=====']"
https://github.com/justadudewhohacks/face-api.js/issues/71;Error: Constructing tensor of shape (4608) should match the length of values (876);6;closed;2018-08-17T11:53:43Z;2018-08-26T12:22:18Z;Hi, Getting this error on loading the models.;"['Can you paste a snippet of your model loading code? You are passing a the contents of a wrong .weights file to the load function of your model.=====', 'Iam using vuejs```<template>  <v-layout row wrap>    <v-flex xs12 sm6 offset-sm3>      <v-card>        <v-card-media>        <img id=""img1"" src=""../assets/AE.jpg"" /></v-card-media>        <v-card-title primary-title>          <div>            <h3 class=""headline mb-0"">Img-1</h3>          </div>        </v-card-title>      </v-card>    </v-flex>    <v-flex xs12 sm6 offset-sm3>      <v-card>        <v-card-media>          <img id=""img2"" src=""../assets/AE.jpg"" />        </v-card-media>        <v-card-title primary-title>          <div>            <h3 class=""headline mb-0"">Img-2</h3>          </div>        </v-card-title>      </v-card>    </v-flex>    <v-flex xs12>      <v-btn color=""primary"" @click=""recognize"">        click      </v-btn>    </v-flex>  </v-layout></template><script>import * as faceapi from \'face-api.js\',const MODEL_URL = \'/models\',export default {  name: \'HelloWorld\',  data () {    return {      msg: \'Face detection and recognition\',      minConfidence : 0.8,      maxResults : 10    }  },  mounted(){    this.initialize(),  },  methods: {    initialize: async function(){        await faceapi.loadFaceDetectionModel(MODEL_URL)        await faceapi.loadFaceLandmarkModel(MODEL_URL)        await faceapi.loadFaceRecognitionModel(MODEL_URL)        let myImg1 = document.getElementById(\'img1\')        console.log(myImg1)        const fullFaceDescriptions = await faceapi.allFaces(myImg1, this.minConfidence)        fullFaceDescription.forEach((fd, i) => {          faceapi.drawDetection(myImg1, fd.detection, { withScore: true })        })    },    recognize: function(){}  }}</script>```=====', '""/models"" contain all the files from this https://github.com/justadudewhohacks/face-api.js/tree/master/weights=====', '@justadudewhohacks =====', ""I am guessing loading the shard files failed. Have a look at the network tab, to verify that the requests for the shard files don't result in a 404.Or you are hosting the wrong files along with the manifest or some shard files are missing. Can you append a screenshot of the contents of your /models dir.====="", '@justadudewhohacks  all files are loading successfully. I used reactjs instead of vuejs and same code loaded all model files without any error. Thank you for reply.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/68;Support Loading models from external sources;3;closed;2018-08-10T18:22:28Z;2018-08-13T12:10:40Z;The Current Implementation does not support loading of models from sources other than the current domain. This could be a useful feature as it could allow for loading of models from cdns or directly from a cloud object storage instance. I have implemented the required change [here](https://github.com/bmaguireibm/face-api.js), let me know if this is desired feature and I can create a PR.P.S.Thanks for the excellent library.;"['Hi,Sure that would be awesome, if you could create a PR for that. Thank you!=====', ""Great. I've created the PR [here](https://github.com/justadudewhohacks/face-api.js/pull/69).====="", 'Your changes are published in version 0.10.1.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/67;Uncaught (in promise) TypeError: faceapi.allFacesMtcnn(...).map is not a function;9;closed;2018-08-10T06:01:46Z;2019-07-31T07:02:17Z;"I'm using VueJS. How can I fix these errors?**Code** (VueJS)```onPlay: async function(videoEl) {                if(videoEl.paused || videoEl.ended || !modelLoaded)                    return false,                let minFaceSize = 200,                const { width, height } = faceapi.getMediaDimensions(videoEl),                const canvas = document.getElementById('inputVideo'),                canvas.width = width,                canvas.height = height,                const mtcnnParams = {                    minFaceSize                },                const fullFaceDescriptions = (faceapi.allFacesMtcnn(videoEl, mtcnnParams)).map(fd => fd.forSize(width, height)),                fullFaceDescriptions.forEach(({ detection, landmarks, descriptor }) => {                    faceapi.drawDetection('overlay', [detection], { withScore: false }),                    faceapi.drawLandmarks('overlay', landmarks.forSize(width, height), { lineWidth: 4, color: 'red' }),                    const bestMatch = getBestMatch(trainDescriptorsByClass, descriptor),                    const text = `${bestMatch.distance < maxDistance ? bestMatch.className : 'unkown'} (${bestMatch.distance})`,                    const { x, y, height: boxHeight } = detection.getBox(),                    faceapi.drawText(                    canvas.getContext('2d'),                    x,                    y + boxHeight,                    text,                    Object.assign(faceapi.getDefaultDrawOptions(), { color: 'red', fontSize: 16 })                    )                }),                setTimeout(() => onPlay(videoEl), 150),            },            run: async function() {                await faceapi.loadMtcnnModel(""/img/face-api/weights""),                await faceapi.loadFaceRecognitionModel(""/img/face-api/weights/""),                trainDescriptorsByClass = initTrainDescriptorsByClass(faceapi.recognitionNet),                modelLoaded = true,                const videoEl = document.getElementById('inputVideo'),                navigator.getUserMedia(                    { video: {} },                    stream => videoEl.srcObject = stream,                    err => console.error(err)                ),                this.onPlay(videoEl),            }```**Code** (HTML)```<div style=""position: relative"" class=""margin"">        <video ref=""inputVideo"" onload=""onPlay(this)"" id=""inputVideo"" autoplay=""true"" muted></video>        <canvas id=""overlay"" /></div>```";"['Hi,`faceapi.allFacesMtcnn` returns a Promise. You forgot the await keyword: ` (await faceapi.allFacesMtcnn(videoEl, mtcnnParams)).map(...)`.=====', '@justadudewhohacks thanks, it\'s working. But there is another problem.Uncaught (in promise)> Event\xa0{isTrusted: true, type: ""error"", target: null, currentTarget: null, eventPhase: 0,\xa0…}I guess, I forgot one more place.![photofacefun_com_1533893018](https://user-images.githubusercontent.com/18267195/43950167-4232b21e-9c98-11e8-8379-e99d38fc2619.jpg)=====', 'This could be caused by anything. Could you show me the line, where this error is thrown.=====', ""I solved this. The path to the Images folder isn't correct.Last question, How can i train recognition? I took 10 images (from web camera). But, distance value changes between 0.6 - 0.7.====="", 'You could use multiple images to compute reference descriptors and then compute the average of the euclidean distances between all reference descriptors and a input descriptor.Usually, you should already get quite good results using a single image only. Can you share the image/s you are using as reference and the images of the faces, you are trying to recognize.=====', 'Ok. Thanks for help.=====', 'script2.js:17 Uncaught (in promise) TypeError: fd.forSize is not a function    at script2.js:17    at Array.map (<anonymous>)    at onPlay (script2.js:17)=====', ' <video onplay=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>    <!-- <video  id=""inputVideo"" autoplay muted></video>  -->    <canvas id=""overlay"" />this is html code=====', 'and the script is$(document).ready(function() {    run()  })       async function onPlay(videoEl) {                      if(videoEl.paused || videoEl.ended || !modelLoaded)                          return false,                      let minFaceSize = 200,                      const { width, height } = faceapi.getMediaDimensions(videoEl),                      const canvas = document.getElementById(\'inputVideo\'),                      canvas.width = width,                      canvas.height = height,                      const mtcnnParams = {                          minFaceSize                      },                      const fullFaceDescriptions =  (await faceapi.allFacesMtcnn(videoEl, mtcnnParams)).map(fd => fd.forSize(width, height)),                    fullFaceDescriptions.forEach(({ detection, landmarks, descriptor }) => {                        faceapi.drawDetection(\'overlay\', [detection], { withScore: false }),                        faceapi.drawLandmarks(\'overlay\', landmarks.forSize(width, height), { lineWidth: 4, color: \'red\' }),                        // const bestMatch = getBestMatch(trainDescriptorsByClass, descriptor),                        // const text = `${bestMatch.distance < maxDistance ? bestMatch.className : \'unkown\'} (${bestMatch.distance})`,                        // const { x, y, height: boxHeight } = detection.getBox(),                        faceapi.drawText(                        canvas.getContext(\'2d\'),                        x,                        y + boxHeight,                        text,                        Object.assign(faceapi.getDefaultDrawOptions(), { color: \'red\', fontSize: 16 })                        )                    }),                    setTimeout(() => onPlay(videoEl), 150),                },                    async function run() {                                          await faceapi.loadMtcnnModel(""/models""),                                          await faceapi.loadFaceRecognitionModel(""/models""),                                        //   trainDescriptorsByClass = initTrainDescriptorsByClass(faceapi.recognitionNet),                                          modelLoaded = true,                                          const videoEl = document.getElementById(\'inputVideo\'),                                          navigator.getUserMedia(                                              { video: {} },                                              stream => videoEl.srcObject = stream,                                              err => console.error(err)                                          ),                                          this.onPlay(videoEl),                                      }=====']"
https://github.com/justadudewhohacks/face-api.js/issues/66;Uncaught (in promise) Error: Error in slice4D while running MtcnnFaceRecognitionWebcam;6;closed;2018-08-08T08:19:48Z;2018-08-13T12:14:30Z;Hello,At first, the library and your tutorial are amazing. I am currently implementing a real time face recognition from webcam with nodeJS and face-api and there was an error comes up. It happens few minutes after the streaming appeared or sometimes right after the webcam stream appeared. While it's working, it detect and recognize face quite good, but it would stop detecting and recognizing after some times (the webcam stream still works).Below is the error message taken from console window from Chrome 68 after it stop detecting or recognizing.> Uncaught (in promise) Error: Error in slice4D: begin[1] + size[1] (519) would overflow input.shape[1] (480)                                                                                                                    tf-core.esm.js:17 >     at assert (tf-core.esm.js:17)>     at assertParamsValid (tf-core.esm.js:17)>     at e.slice (tf-core.esm.js:17)>     at tf-core.esm.js:17>     at e.tidy (tf-core.esm.js:17)>     at n.value (tf-core.esm.js:17)>     at extractFaceTensors.ts:43>     at Array.map (<anonymous>)>     at extractFaceTensors.ts:42>     at e.tidy (tf-core.esm.js:17)Then I tried to run your examples directly and same thing keeps happening.> (519)number here changed each time error appeared, it seemed to depends on the duration of time it can run.Whenever I reload the page, it repeats the error after some another duration of time.Am I doing wrong somewhere? (noob in JS)Thank you.;"['Hmm maybe a rounding issue at the border of the image, not sure. I will try to reproduce it. Did you adjust the parameters or does the error also occur when running the example as is?=====', 'I only changed minFaceSize from 200 to 150. And the error also occurs with the example as well. UPDATE:As I did some playing around with the minFaceSize param (increasing and decreasing) and tried to move my face closer and further to the webcam, it seemed that when my face is getting too close, the error would occur immediately. If not, it still occurs after a minute.Is there anyway that we can get rid of this?Because each time the error appear, I have to reload the page to start detecting/recognizing again.=====', ""@justadudewhohacks confirmed, I'm getting this overflow error too. It happens when I run MTCNN with `minFaceSize: 200` and when I put my face too close so that the whole face isn't seen. To replicate, run forwardpass/allfacesmtcnn with webcam videoel as input and go very close to far a couple of times.![image](https://user-images.githubusercontent.com/3739702/43987694-8148bee8-9d57-11e8-9095-938039eacb05.png) @TuanHungVU1202 you could wrap your forward pass with `try...catch` block and just log the errors. Worked for my purposes. :) ====="", ""@beatobongco Thanks. I made it with your temp solution. But I still hope we can fix this permanently because it's too laggy for my purpose.====="", 'I could reproduce the issue, it was due to the bounding boxes reaching over the image borders. This should now be fixed.=====', '@justadudewhohacks  perfect, just tested and everything works like a charm.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/65;Using landmark model and don't have the expected behavior;1;closed;2018-08-07T16:38:40Z;2018-08-07T17:13:41Z;![image](https://user-images.githubusercontent.com/16566338/43789700-f7142d74-9a46-11e8-836e-0d0f9eacca7b.png)What I'm doing wrong? I just change the example to landmark model and it´s apparently not working correctly. I'm running it on Windows/Firefox and Windows/Chrome.;['You have to feed the landmark model an image of a face, e.g. just the extracted image from the bounding box, not the entire image.=====']
https://github.com/justadudewhohacks/face-api.js/issues/64; Compute and compare the descriptors of two face images ;6;closed;2018-08-03T10:16:42Z;2021-07-02T14:43:11Z;Hi thank you a lot for your efforts, but i need some helps in the face recognition : my goal is to compare two face  images for that i compute the descriptor o each image and the euclidean Distance the same thing in the examples when i enter the same person and the same image it gives me 'match' but when  i enter for example different photographs of the same person the result is 'no match'  i really need to compare different photographs of the same person ;"['Can you append some screenshots for your example, that is not working? Also some code snippets would be helpful. Are you performing face detection and alignment?=====', 'yes of course !first i detect the faces in the images than i compute their descriptors . this an example :![capture1](https://user-images.githubusercontent.com/38425517/43686560-e7c25fae-98bf-11e8-93c3-ef4e63bb3bfd.PNG) and for the code i use this : ``<script>     const mtcnnParams = { maxNumScales: 10,scaleFactor: 0.709,minFaceSize: 20,scoreThresholds: [0.6, 0.7, 0.7],}         const results = await faceapi.mtcnn(inputImgEl, mtcnnParams)       console.log(results)       if(results==\'\' ){ console.log(""no face detected try again !!""), return true,}       if (results) {          results.forEach(({ faceDetection,faceLandmarks }) => {          faceapi.drawDetection(\'overlay\', faceDetection.forSize(width, height))          faceapi.drawLandmarks(\'overlay\', faceLandmarks.forSize(width, height), { lineWidth: 4, color: \'red\' })        })          const descriptor1 = await faceapi.computeFaceDescriptor(inputImgEl)          console.log(descriptor1)          const descriptor2 = await faceapi.computeFaceDescriptor(inputImgEl1)          console.log(descriptor2)          const distance = faceapi.euclideanDistance(descriptor1, descriptor2)          console.log(distance)          alert(""test"")             if (distance < 0.62)              console.log(\'match\')           else              console.log(\'no match\')      } //fin de if(results)    //}        } //fin de run()           $(document).ready(function() {            run()    })</script>``=====', ""Hmm it says match, or am I missing something. Also are you computing the face bounding box for the second image as well? `descriptor1 = await faceapi.computeFaceDescriptor(inputImgEl)` looks like you are passing the entire image and not the extracted face image.You can also use `faceapi.allFaceMtcnn` to get the bounding boxes + descriptors, so you don't have to do everything on your own.====="", ""Hi again , you are right it's my fault you are right when i use `computeFaceDescriptor`  i pass the entire image that why it's give me error in matching !!!! thanks a lot :+1: ====="", 'I have two base64 image and both image has a single face i want to compare both faces. can any one help men=====', 'Hi,  I am new in tensorflow js. I am working detect and recognized images between realtime web cam and stored in db.Can any one Please suggest me from where i can start in react js.I intgreated this **@tensorflow-models/blazeface** modulei am able to detect the faces in real time web cam.i do not under stand how to convert image in tensor. now i am not able to understand what i do after that. I am very thankfull if any one help me.=====']"
https://github.com/justadudewhohacks/face-api.js/issues/61;Conflict when using Tensorflow.js with face-api.js;5;closed;2018-07-27T14:58:11Z;2018-09-06T19:15:55Z;I'm working on a prototype where I need to use tensorflow.js (load the models), there seems to be a conflict between the namespace with your version (assume you're just using core) and the official. I need access to, at least, loadModel which is not available in your module. I've tried importing both but only the first one assigns itself to the globally. ;"[""Yes face-api.js only uses tfjs-core. What do you mean by namespace collision? This module doesn't export tf on the global scope, so there shouldn't be any collisions.====="", 'Importing tensor-flow like this creating error in namespace and causing problems. **(Noob in JS)**Can you have solution to import only once.```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0""> </script>```=====', ""I see what the issue is, apparently tf can't register the WebGL backend twice.Anyways, I wouldn't include both scripts like that anyways. Just install both modules via npm and bundle them together.====="", 'Now I see what the actual issue was here.In version 0.12.0 I bumped the tfjs-core version to latest (0.12.14), which includes the fix (last PR referenced in [this](https://github.com/tensorflow/tfjs/issues/109) issue), which allows tfjs to reuse a backend instead of registering it again, if it has already been registered from another script. If there are still issues, feel free to reopen.=====', '@joshnewnham  did you ever find a solution to this?I\'m running into a similar problem where:1) I\'d like to load and predict using a TF-JS model I trained -- using https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest2) I\'d like to detect faces -- using a different TensorFlow built-in to FaceApiDepending on the order that I use these lines, only the 1st called module with work:    <script src=""modules/tfjs.js""></script>    <script src=""./node_modules/face-api.js/dist/face-api.js""></script>=====']"
https://github.com/justadudewhohacks/face-api.js/issues/57;Improving the face alignment;5;closed;2018-07-19T21:47:05Z;2018-07-24T17:14:59Z;Currently I get some faces misaligned, the issue seems to be more with the ROI of the detector. Not completely sure in the end if its a detection or alignment model issue![image](https://user-images.githubusercontent.com/590231/42971559-17537c32-8bad-11e8-92fd-6b7a0d1e2a44.png)With changing the detection ROI we can get good results![image](https://user-images.githubusercontent.com/590231/42971618-481461ce-8bad-11e8-84c0-a0fd1d77bfb2.png)I'm looking how to get overall improvement up, maybe running mtcnn and SSD side by side. I'd rather do a bit slower detection then having users fix the ROI manualy;"['The boxes drawn, are the bounding boxes returned from SSD? The 68 point landmark model we are using currently is not that accurate and could be improved, either by training an own model or looking for a pretrained model that grants better accuracy. At the moment the accuracy is sufficient to get a roughly aligned bounding box for the net computing the face descriptors.=====', ""First one is the SSD one the other is manual resizing to get good resultsSo I ended up messing around with bounding boxes. The alignment model can get good results but its very dependent on a good bounding box. I had much worse results with faces that are angled more than 75 degree, none of them had proper alignment set. It seems that alignment model likes the bounding box to start above eyes and a squared face. I'm currently testing making wider squared bounding boxes since ones generated by SSD are narrow.Going to test mtcnn later====="", ""Anyways, I would rather have a model, that is agnostic to the face detector and which can predict the 68 face landmarks more precisely, independent of the bounding box.I played around with retraining the landmark model a while ago actually and have some intermediate models, which seem to perform better already, but I didn't investigate further yet or validated them on a larger data set.Does deep fakes require 68 face landmark points or simply a aligned bounding box by the way?====="", ""Current deep fake projects use a 68 face landmark to determine the face cutout, you need your input face to be the closest as possible to the one you used in training to get the best quality output. Here is an example, training input, faces are determined by a function based on landmarks![image](https://user-images.githubusercontent.com/590231/43036700-9e051c68-8d06-11e8-8738-ea2890808dd4.png)Also in post processing  you do a landmark cutout of the new outer part of the face to remove the background. That's something I'm missing from my site until I can get an openCVjs compilation with drawing.Even the slight misalignment can be seen as flickers or jitters in a video, here's an example without manual fixing http://www.machine.tube/v/87 and this one doesn't even have hard align faces====="", ""I managed to figure it out. Since a face rotated to 90 or more won't get aligned anyways the box orientations don't matter. Lowering the bounding box to 15%-12% of the height and making it a box shape improved the aligner accuracy by a lotGoing to close the issue since it seems this is the best as it gets=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/56;Landmark Detection for a person counter?;4;closed;2018-07-17T20:52:53Z;2018-07-19T06:38:47Z;I would love any advice in the right direction :)I have detection running well and I don't need face recognition, but I'm looking for a way to determine if this is still the same person being detected as per previous frames, or is this now a completely different person.Essentially I want to increase a variable every time a new person looks at the camera.;"[""Wouldn't it be sufficient to track the bounding boxes? Across frames the position of the bounding boxes shouldn't change that much, so you could try to associate them with their previous positions.Determining whether a person looks at a camera could be more tricky, as you would need some kind of eye gaze detection.====="", 'That makes sense, ill work from that perspective and track the box until its out of vision and call that ""one person"". Thanks !=====', 'Is it crazy to say that you could run the facerecognition euclideanDistance on a list of previous face detection\'s (say all the last 1 minute).  If you get a low 0.6 or 0.5 threshold, you know you have seen that person recently and no need to increment the ""unique person view"" counter?=====', ""It's not crazy, just keep the face descriptors you calculated for previous detections and compare them to the descriptors from the bounding boxes you are currently tracking.A distance < 0.6 in general means, it's a match. The net has basically been trained to learn that threshold.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/55;How to speed it up after porting to node.js;14;closed;2018-07-16T11:35:58Z;2021-02-16T07:41:29Z;I ported it to node.js to compare faces, but the operation was slow when extracting 128-dimensional feature vectors. I replaced the latest @ tensorflow/ tfjs and used require('@tensorflow/tfjs-node'), but it is not accelerated, it is still very slow, each method call below will wait 5-20 seconds, but in the browser can be faster, what can I do to speed it up?            return tidy(function() {                var batchTensor = input.toBatchTensor(150, true),                var normalized = normalize(batchTensor),                var out = convDown(normalized, params.conv32_down),                out = maxPool(out, 3, 2, 'valid'),                out = residual(out, params.conv32_1),                out = residual(out, params.conv32_2),                out = residual(out, params.conv32_3),                out = residualDown(out, params.conv64_down),                out = residual(out, params.conv64_1),                out = residual(out, params.conv64_2),                out = residual(out, params.conv64_3),                out = residualDown(out, params.conv128_down),                out = residual(out, params.conv128_1),                out = residual(out, params.conv128_2),                out = residualDown(out, params.conv256_down),                out = residual(out, params.conv256_1),                out = residual(out, params.conv256_2),                out = residualDown(out, params.conv256_down_out),                var globalAvg = out.mean([1, 2]),                var fullyConnected = matMul(globalAvg, params.fc),                return fullyConnected,            }),;"['Unfortunately, I have not played around with tfjs-node yet. But as I said in another issue already, unless you can get tfjs-node to run on your native gpu (I am not sure what the current state of tfjs-node is in that regards), I would suspect it to be very slow.Edit.: If you are only interested in the resnet for computing face descriptors, you can give [face-recognition.js](https://github.com/justadudewhohacks/face-recognition.js), which exposes nodejs bindings to the dlib implementation of that net. Compiling with openblas, you should be able to get it down to realtime speeds, depending on your CPU.=====', 'I used the @tensorflow/tfjs-node to perform the multi-person detection of the posenet from the browser to the node.js, and the time to draw one frame was raised from about 2 seconds to 170-180 milliseconds.=====', 'So running posenet in nodejs was faster or slower compared to browser?=====', ""The posenet test speed node.js is 9-10 times higher than the browser. Just now, the faceapi euclideanDistance is speeded up by Node.js. The test results are as follows:    \tvar begin = new Date().getTime(),    \tconst descriptor1 = await faceapi.computeFaceDescriptor(face1)//225px*225px    \tconst descriptor2 = await faceapi.computeFaceDescriptor(face2)//252px*252px    \tconst distance = faceapi.euclideanDistance(descriptor1, descriptor2)    \tconsole.log('distance='+distance),    \tif (distance < 0.6)    \t  console.log('match')    \telse    \t  console.log('no match')    \tvar end = new Date().getTime(),    \tconsole.log(end - begin),browser：distance=0.46667880224688196match2628node.js:distance=0.4666788574536108match1136Although there is no speed improvement of posenet 9-10 times, but there is also a 2x improvement, only @tensorflow/tfjs-node is used for CPU acceleration.Also I modified your embedded @tensorflow/tfjs because I tried to upgrade the embedded @tensorflow/tfjs multiple times and the result could not be accelerated by @tensorflow/tfjs-node.====="", 'Interesting, did you set your backend in the browser to CPU or WebGL? 2628 ms for 2 forward passes through the recognition net is very slow.=====', ""Thanks for your reminder, it is true that my browser does not have Open WebGL. What I want to say is that if you don't turn on acceleration in node.js, the result will be like this:node.js:distance=0.4666789108041152match432349I haven't used your face-recognition.js yet, test it tomorrow, thank you for sharing such a good project on github.====="", 'I see, thanks for the hint. Maybe I will also try to tinker around with tfjs-node a bit soon.=====', 'heya , trying to port it aswell , land007 can you please help me a little bit?=====', ""I played around with face-api.js + node.js a bit now and actually it seems that you don't need to port anything. You simply have to declare HTMLImageElement, HTMLCanvasElement and HTMLVideoElement in your node environment, for example `global.HTMLImageElement = class {}`.I will look more into this and try to get better nodejs support into face-api.js soon, but right now you should already be able to use the package with nodejs.====="", '@zobis2 If you prefer, you can use ""docker kill tfjs-html, docker rm tfjs-html, docker run -it --privileged --name tfjs-html -v ~/tfjs-html:/node -p 8080:80 land007/tfjs-html:latest"" to pull the tensorflow.js image of node.js, which provides tfjs, canvas for node.js , ws, xmlhttprequest, node-fetch, mjpeg-server and other support, as justadudewhohacks said to simulate these objects in node.js, you can quickly port face-api.js to the node.js environment.=====', 'I am closing this now, since face-api.js now supports nodejs. Importing tfjs-node, you should be able to get pretty fast processing even on the CPU.=====', 'Ok, I will test your new program and hope it will satisfy the video processing.=====', ""I'm also experiencing slowness problem when using faceRecognition on api-faces. I use windows and my tests are taking around three seconds to identify the face.I already installed openblas on windows, but it looks like faces-api.js does not recognize it.Sorry, you are requested to use face-recognition.js and this repository is saying This package is pretty much obsolete. I recommend you to switch to face-api.js.Please help me, I'm loving this api.====="", 'https://github.com/land007/face-kit=====']"
https://github.com/justadudewhohacks/face-api.js/issues/53;MTCNN;1;closed;2018-07-16T05:44:33Z;2018-07-16T12:53:46Z;Is it possible to use allFaces function with MTCNN?;"[""I was still working on that, but it's available now in version 0.9.0. (faceapi.allFacesMtcnn, also have a look at the mtcnn face recognition examples)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/52;Investigating in YOLO (v2 or v3) for face detection;2;closed;2018-07-14T15:38:56Z;2018-08-26T12:23:14Z;It might be worth it to also investigate in yolo v2 or v3 as an alternative face detector. From the looks I would tend to yolo v2, as it is more leaned towards achieving high fps. Furthermore, there seem to be some already trained models for face detection with yolo v2 out there, which would make it easier to get started.;['Just merged an experimental version of tiny yolo v2 for face detection. I think this one could see a huge performance speedup by replacing the conv2ds with depthwise seperable convolutions, but therefore the model has to be retrained.=====', 'Currently training the tiny yolo v2 model using depthwise separable convolutions from scratch. Speed up over regular convolutions is huge. The current model already works pretty well for face tracking and even runs on mobile chrome. First version to play around with can be tested here: https://yolosep.herokuapp.com/ =====']
https://github.com/justadudewhohacks/face-api.js/issues/50;SSD to tensorflowjs convertor;4;closed;2018-07-14T11:50:40Z;2019-05-15T07:10:21Z;Hey Vincent, Can you enlighten me on how you converted the weights provided in https://github.com/yeephycho/tensorflow-face-detection to tfjs. I tried this but hit unsupported ops issue.                            tensorflowjs_converter --input_format=tf_frozen_model  --output_node_names='concat,concat_1' frozen_inference_graph_face.pb outputUnsupported Ops in the modelWhere, TopKV2, Assert, NonMaxSuppression;"[""Hi,I didn't use the tensorflow.js converter, because at that time way more ops were not supported than the ones you mentioned. I extracted them with some own scripts.====="", ""You are a rock star. Being able to get SSD to work when there are certain ops that aren't supported. It would be great if you can share them with the bigger community.  I am not sure how comfortable you would be to share the scripts. Can totally understand if you couldn't====="", 'Thanks, but it might actually be easier than you think. You could simply load your graphdef with tf + python and then save the weight values to binary files with numpy and get their names and shape from the graph, to construct the manifest.json file.I already commented on the [tfjs ssd_mobilenet issue](https://github.com/tensorflow/tfjs/issues/188), with how you could approach converting an ssd model to tfjs.I will close this issue here, as it is probably more appropriate to discuss this @tfjs.=====', 'Thank you for your response=====']"
https://github.com/justadudewhohacks/face-api.js/issues/48;Trying to use detected faces from face-api.js for face-recognition.js;1;closed;2018-07-13T10:27:41Z;2018-07-13T12:20:05Z;I am trying to use the detected faces from face-api.js for the recognition of face-recognition.js but they don't use the same dimensions.Face-api faces can be for example: 67x94 pxFace-recognition always expects a face of: 150x150 pxIs it possible to scale up the detection box (in face-api) to 150x150 px and how? Or should I take another approach? ;['Figured it out, I should really pay more attention to the examples and think about the math behind them before asking questions, my bad.=====']
https://github.com/justadudewhohacks/face-api.js/issues/46;How to resize the aligned face images to a constant fixed size?;8;closed;2018-07-12T08:04:41Z;2020-06-23T13:46:59Z;Hi I am trying to display the list of detected and aligned face images in a scroll div in html5. I am able to detect and append into the div dynamically however the size of the resultant face images are all different. At which function do I change the width and height so let's say they will all be 160x160.```const input = await faceapi.toNetInput(videoEl)        const locations = await faceapi.locateFaces(input, minConfidence)        //const resized_locations = locations.map(det => det.forSize(160, 160))        const faceImages = await faceapi.extractFaces(input.inputs[0], locations)        // detect landmarks and get the aligned face image bounding boxes        const alignedFaceBoxes = await Promise.all(faceImages.map(            async (faceCanvas, i) => {            const faceLandmarks = await faceapi.detectLandmarks(faceCanvas)            return faceLandmarks.align(locations[i])            }        ))        const alignedFaceImages = await faceapi.extractFaces(input.inputs[0], alignedFaceBoxes)                // free memory for input tensors        input.dispose()        //$('#facesContainer').empty()        faceImages.forEach(async (faceCanvas, i) => {            $('#facesContainer').append(alignedFaceImages[i])            percentage = percentage + 5,        })```These are the script i used for detect and append. I tried resizing the faceCanvas, or looping through #faceContainer and resize them, or mapping the alignedFaceImages to 160x160 size but all to no avail. Any assistance is much appreciated. Thank you!;"['Okay so I managed to resize the canvases to 160x160 by doing ```alignedFaceImages.forEach(photo => {            photo.width = 160            photo.height = 160        })```however the output canvases are blank. Do I need to use map() function. If so how do I do it? Sorry I am quite new into javascript html5 programming=====', 'Solved this on my own.=====', 'Hi @stephenxp04 ,Could you tell us, how you solved your problem? Other people might face the same issue and could benefit from your solution.The way I would approach resizing is either:1. As you did, extract the canvases with `faceapi.extractFaces` and resize them by creating a new canvas and using ctx.drawImage given the target sizes, to redraw the canvas at a new scale.2. To get the face tensors with `faceapi.extractFaceTensors` and then resize the tensor with `tf.image.resizeBiliniar` and extract the canvas again using `faceapi.imageTensorToCanvas`.=====', 'I did the html5 canvas method. Since individual alignedFaceImages element is a HTML canvas just create a new canvas children with size I wanted, then draw the alignedFaceImages[i] into the new canvas context inside the forEach loop.```var res = document.createElement(""canvas"")            res.width = 160            res.height = 160            res_con = res.getContext(""2d"")            res_con.drawImage(alignedFaceImages[i], 0, 0, 160, 160)```i know the approach is rather simple not sure about the optimization since this is just POC.Not familiar with the tensor method though, it is possible to use tf function within js?=====', ""Yep, that's the solution I would go for as well.> Not familiar with the tensor method though, it is possible to use tf function within js?Yes tfjs core is exported by the module via `faceapi.tf`.====="", ""is that possible to resize the results of `extractFaces()` to match the dimensions of VideoElement?VideoElement's dimensions are modified for the web page irrespective of its original dimensions.The result dimensions should match with the dimensions mentioned in web page.Could find the method `faceapi.resizeResults()` that works for detections and landmarks.Need similar method for results of `extractFaces()`====="", '> 2\\. faceapi.extractFaceTensorsUnfortunately extractFaceTensors is expected a 3D or 4D model, not canvas image. Please suggest.UnhandledPromiseRejectionWarning: Error: extractFaceTensors - expected image tensor to be 3D or 4D=====', '> I did the html5 canvas method. Since individual alignedFaceImages element is a HTML canvas just create a new canvas children with size I wanted, then draw the alignedFaceImages[i] into the new canvas context inside the forEach loop.> > ```> var res = document.createElement(""canvas"")>             res.width = 160>             res.height = 160>             res_con = res.getContext(""2d"")>             res_con.drawImage(alignedFaceImages[i], 0, 0, 160, 160)> ```> > i know the approach is rather simple not sure about the optimization since this is just POC.> > Not familiar with the tensor method though, it is possible to use tf function within js?Just in case if somebody looking for a canvas without ""stretching"" the image, you can do x,y offsets. My solution is like belowfaceImages.forEach((face)=>{    var res = canvas.createCanvas(200, 200)    let h = face.height,    let w = face.width,    let a = 0,    let b = 0,    if (h <= 200 && w <= 200) {      a = (200 - w) / 2,      b = (200 - h) / 2,      res.getContext(""2d"").drawImage(face, a, b, w, h)    } else {      res.getContext(""2d"").drawImage(face, 0, 0, 200, 200)    } //save or display canvas})=====']"
https://github.com/justadudewhohacks/face-api.js/issues/45;External image sources;10;closed;2018-07-10T08:46:38Z;2021-07-30T14:05:43Z;"Hi,First of all thank you for your work, it's excellent and very simple to use.I have a small problem, when I use the face recognition, the api doesn't seem to work with external image sources. I would like to work with a data base and I'm receiving this error message : ""Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.""Do you know how can I overcome this problem ?";"['Hmm, can you show some example code of what you are doing? The error message indicates that whatever you are trying to pass into the net is either not a tensor or a valid media element or the media content is not loaded yet.=====', 'html : ```<div class=""footballer_images_container"">     <img class=""it_will_work_for_this_image"" src=""public/amy1.png"" alt="""">     <img class=""it_will_not_work_for_this_image"" src=""https://upload.wikimedia.org/wikipedia/commons/e/e4/Karim_Benzema_2018.jpg"" alt="""">   </div>```js`     const $imgToCompare = footballerImageContainer.querySelectorAll(""img"")`        //compare image    const compareImage = async ()=>{        //load models        await faceapi.loadModels(MODEL_URL)        //vector the custom image         const descriptor1 = await faceapi.computeFaceDescriptor($customImage)               //compare all the images to the custom image        $imgToCompare.forEach(img => {            //compare both image            faceSimilarities(img, descriptor1)          }),    }    //compare faces    const faceSimilarities = async (image, descriptor1)=>{        //vector the images to compare        const descriptor2 = await faceapi.computeFaceDescriptor(image)        //compare the ressemblance of the faces        const distance = faceapi.euclideanDistance(descriptor1, descriptor2)        //transform the distance in %        const similarityPercentage = Math.round(((1 - distance) * 100) * 100) / 100         // console.log(similarityPercentage + ""%"")         //put the value in the array        imgScore.push(similarityPercentage)                //if it\'s the last image        if($imgToCompare[$imgToCompare.length - 1] === image){            compareScore()                    }    }So this will calculate the euclidian distance between a custom image, and all the image in the ""footballer_images_container"" div. When it\'s finished, it will pick the image with the most similarities with the function compareScore(). The problem is it will work for ""public/amy1.png"" because the source is in local but not for ""https://upload.wikimedia.org/wikipedia/commons/e/e4/Karim_Benzema_2018.jpg"" because the source is external.I hope i\'m clear.=====', 'You may get blocked by the browser because of the CORS on the external site=====', 'Yeah @seranus is right, that simply wont work. Look at the example how I implemented external images fetching, I proxy the request via an express server=====', 'Ok thank you !=====', 'Facing similar problem for External Video sources.any idea?=====', '@sathyamoorthyrr have you found a solution for videos? =====', ""@lpsBetty have you found solution, any idea? I'm have same problem====="", '> @sathyamoorthyrr have you found a solution for videos?@lpsBetty I think I got settled with an internal video, as per this, https://github.com/sathyarr/video-face-emotion-detection-analysis/blob/master/README.md#how-to-runI have lost the context altogether. been so long worked on this.=====', '@IgorGomesFATEC yes it worked with the ""newer"" npm package: https://www.npmjs.com/package/@vladmandic/face-api=====']"
https://github.com/justadudewhohacks/face-api.js/issues/44;Face API can support and using in server-side?;4;closed;2018-07-09T02:40:13Z;2018-07-16T02:25:34Z;I have some questions, is it possible to use `face-api` in server side? for example: I need use it to process some image in db. If it is, possible, could it work correctly without environment config like on browser?Use it like other package? And now, I can not setup it in server-side! :<```javascriptimport faceApi from 'face-api'// faceApi is undefined```Thanks!;"[""If you are using typescript: `import * as faceapi from 'face-api'` since it has no default export.The neural network implementations could probably also run on server side, but the package as is right now probably won't work with nodejs, as it uses quite some stuff from the dom api. For face recognition with nodejs you can also give [face-recognition.js](https://github.com/justadudewhohacks/face-recognition.js) a try.====="", '@justadudewhohacks what is `dom api` using in package now? we can convert it and run on nodejs? If you want I can help you convert it to nodejs because face detect some using in server.thanks for answers!=====', 'Probably possible to convert it to nodejs. If you want to investigate in that, sure. However, I am not sure what the state of tfjs + node is atm. If the code is running on the cpu in node, it would probably be very slow.=====', 'thanks @justadudewhohacks , I will learn more about tfjs 👍 =====']"
https://github.com/justadudewhohacks/face-api.js/issues/42;Implementing mtcnn for face detection and 5 point landmarks;13;closed;2018-07-06T08:33:44Z;2018-09-26T19:29:18Z;The face detection and alignment approach of [mtcnn](https://github.com/kpzhang93/MTCNN_face_detection_alignment) from this [paper](https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf) seems to be a promising alternative for real time face detection + face alignment. Next thing on my TODO list.;"['Sounds exciting. Just to understand well, the aim here is to get a faster, ""lighter"" way for faces detection right ?I\'m not sure if it helps but I\'ve found this tf implementation : https://github.com/AITTSMD/MTCNN-Tensorflow, I guess you have to adapt in order to keep the tfjs compatibility.Thanks for your awesome work.=====', 'Exactly that is the purpose. Plus mtcnn detects 5 point face landmarks along the way, which is enough for face alignment, so you could also skip the 68 point face landmark detection for face recognition.Thanks for sharing the link, I already played around with this repo and a few others. The results look very promising and I am already into porting this architecture to tfjs. Hopefully I can show some results soon ,)=====', 'Sounds pretty promising, any ETA? :) The current face detection is not suited for real-time detection (got 1 second on my laptop) so I am currently trying/using other alternatives. Looking forward to it!  =====', 'Turns out, that reimplementing the mtcnn architecture with tfjs is much trickier than I thought. However a few days ago I managed to get a first working solution. I still have to identify some performance bottlenecks and do some fine tuning. But first results show a significant speedup to the current SSD implementation. With mtcnn you also have some parameters allowing to get better overall performance. Atleast on my machine I can get realtime performance on the video face detection example with the current state of the implemenation.Im currently working on the [mtcnn](https://github.com/justadudewhohacks/face-api.js/tree/mtcnn) branch. I will try to make it available as soon as possible.=====', ""That sounds good ! Just curious about it, why did you choose to have a try with mtcnn. I've also read some good results with YOLOv3 (https://pjreddie.com/darknet/yolo) but I'm not able to say which one is the fastest algorithm.====="", ""Yolo v3 also seems to be a good option to investigate. I just read that yolov3 uses quite some layer, whose implementation is specific to darknet. Furthermore, I don't think there is any yolo v3 model trained specifically for face detection yet, meaning we would have to train it from scratch. Yolo v2 is certainly an option, but while it may be faster then the already implemented SSD face detector it will probably not be as accurate.I chose mtcnn since it seemed to be very lightweight, e.g. it is fast, has a small model size (only 2mb without any quantization) seems to be highly configurable for your needs by tuning paramaters.====="", ""I have also tested the current face detection on my mobile (S8+), it seems to take around 3 seconds to detect a single face from a 384*216 photo.I am currently looking into alternatives such as: pico.js (https://tkv.io/posts/picojs-intro/), it's fast but I am not sure if it works with face-recognition.js.Anyway, looking forward to your results.====="", 'Yea I think mobile is a specific case, we wont simply get realtime for a mobile device that easily. Maybe its better for face detection on mobile to use simpler solutions such as haar features or viola jones instead of a cnn.=====', ""Finally it's done. The Mtcnn has some weird warmup time, but for live face detection by video / webcam I roughly see an average speedup somewhere between 2 - 4x on my main machine as well as a huge speedup on my laptop.With that being said I am closing here. @akofman , I would like to keep up the YOLO discussion in another issue however, which might be worth some further investigations in future as well. Also @thexiroy I would be interested in your findings (performance wise) regarding pico.js or any other tool that would possibly be most suitable for face detection on mobile. ====="", ""Congrats 👏 👏 ! I've just tested it with a video and it's really faster. On my MBP Core i7 I have no lag anymore, I didn't measure it precisely but the difference is impressive.Unfortunately I didn't manage to test it on my old Nexus 5 phone because of a 'Failed to compile fragment shader' error ... I'm also curious to compare it with picojs. ====="", 'Great to hear! Let me know if this issue persists, I thought I fixed all those shader compilation errors, which most of the time occured to me, when the images have been scaled down too much.=====', '@justadudewhohacks  Did you train mtcnn from scratch or use the released models trained by others, I find the performance of your mtcnn model is not very stable=====', ""The weights are taken from the official repo, as stated in the README, I didn't retrain the model.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/39;run example failed on windows 10;3;closed;2018-07-03T09:38:51Z;2018-07-10T08:05:39Z;[npm-debug.log](https://github.com/justadudewhohacks/face-api.js/files/2158626/npm-debug.log)I cloned the repository, and run the example as the tutorial. but got error, I'm on Windows 10.;"[""The debug log doesn't help. Please tell me what you did and what kind of error was thrown.====="", 'Below are all what I do and error message.git clone https://github.com/justadudewhohacks/face-api.js.gitcd face-api.jscd examplesnpm inpm startapp.post(\'/fetch_external_image\', async (req, res) => {                                        ^SyntaxError: Unexpected token (    at Object.exports.runInThisContext (vm.js:53:16)    at Module._compile (module.js:513:28)    at Object.Module._extensions..js (module.js:550:10)    at Module.load (module.js:458:32)    at tryModuleLoad (module.js:417:12)    at Function.Module._load (module.js:409:3)    at Module.runMain (module.js:575:10)    at run (node.js:348:7)    at startup (node.js:140:9)    at node.js:463:3npm ERR! Windows_NT 10.0.17134npm ERR! argv ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js"" ""start""npm ERR! node v6.2.2npm ERR! npm  v3.9.5npm ERR! code ELIFECYCLEnpm ERR! @ start: `node server.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @ start script \'node server.js\'.npm ERR! Make sure you have the latest version of node.js and npm installed.npm ERR! If you do, this is most likely a problem with the  package,npm ERR! not with npm itself.npm ERR! Tell the author that this fails on your system:npm ERR!     node server.jsnpm ERR! You can get information on how to open an issue for this project with:npm ERR!     npm bugsnpm ERR! Or if that isn\'t available, you can get their info via:npm ERR!     npm owner lsnpm ERR! There is likely additional logging output above.npm ERR! Please include the following file with any support request:npm ERR!     D:\\Learnings\\face-api.js\\examples\\npm-debug.log=====', ""`node v6.2.2` does not support async await out of the box. You can either upgrade your node version, or rewrite that function using Promises (or alternatively comment it out, but then you won't be able to fetch images from an external url).=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/36;Error: Tensor is disposed;6;closed;2018-07-02T19:38:13Z;2018-07-22T16:38:20Z;Hi @justadudewhohacks ,I'm getting the following after performing the below steps:1. load models using `faceapi.loadModels('assets/models/') `(this is successful)2. invoke `faceapi.allFaces(input, this.minConfidence).then(..` (where `input` is the id to a image element.  This fails with below error).> core.js:1449 ERROR Error: Uncaught (in promise): Error: Tensor is disposed.Error: Tensor is disposed.    at t.throwIfDisposed (face-api.min.js:1)    at t.asType (face-api.min.js:1)    at t.toFloat (face-api.min.js:1)    at face-api.min.js:1    at Array.map (<anonymous>)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at rc (face-api.min.js:1)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at t.throwIfDisposed (face-api.min.js:1)    at t.asType (face-api.min.js:1)    at t.toFloat (face-api.min.js:1)    at face-api.min.js:1    at Array.map (<anonymous>)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at rc (face-api.min.js:1)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at c (polyfills.js:3)    at c (polyfills.js:3)    at polyfills.js:3    at t.invokeTask (polyfills.js:3)    at Object.onInvokeTask (core.js:4751)    at t.invokeTask (polyfills.js:3)    at r.runTask (polyfills.js:3)    at o (polyfills.js:3)    at e.invokeTask (polyfills.js:3)    at i.isUsingGlobalCallback.invoke (polyfills.js:3)Did I forget to do sth? Sorry, not familiar with how tensors work so maybe its a stupid question.Thanks;"['Just realized this.I was using `tfjs` for another library in my app. After removing it, `faceapi.allFaces` works!What could be wrong?> <script src=""https://unpkg.com/@tensorflow/tfjs"" defer></script>=====', 'So the error occurs when you are additionally including the tfjs script to face-api.js? Does it work with tfjs-core?Not sure if there are issues, when you are using tfjs additionally, because someone seemed to have issues with this in #9. In case tfjs-core is enough for your application, you can also access the bundled version via `faceapi.tf.`=====', '> So the error occurs when you are additionally including the tfjs script to face-api.js? Does it work with tfjs-core?yes and yes> you can also access the bundled version via faceapi.tfCan you please point me to how to use it? Sorry tf noob speaking :)=====', '> yes and yesSo it works with tfjs-core but not with tfjs?> Can you please point me to how to use it? Sorry tf noob speaking :)Well not quite sure what you mean. You can simply use tf via `faceapi.tf`. You could simply bind tf to the window object for convenience: `window.tf = faceapi.tf` and then use tf as you would normally.=====', ""> So it works with tfjs-core but not with tfjs?Correct> You could simply bind tf to the window object for convenience: window.tf = faceapi.tf and then use tf as you would normally.I see. I'll try that. Closing. Thanks for the prompt responses.====="", '> So it works with tfjs-core but not with tfjs?@justadudewhohacks  No. It doesnot work with both (tfjs & tfjs-core). It throws ""Tensor disposed Error"" for tfjs-core.![image](https://user-images.githubusercontent.com/16360498/43047855-82badb32-8dfb-11e8-811a-58da23b956ff.png)=====']"
https://github.com/justadudewhohacks/face-api.js/issues/32;Improve speed  ;9;closed;2018-06-29T19:56:15Z;2021-11-16T13:46:59Z;"Hello,in the beginning, I would like to say that this library is beautiful, and I'm really impressed by how good it works. I mean how precise. I have an issue with the speed of face and landmark detection. I feel that with this quality of precision it's hard to make it faster, but I'm really interested in slightly better speed of detecting. I'm using this library with the stream from my camera, and I get very low results (sth around 1 FPS in face detection)So Is there any known factor or something that I could change which will increase the speed of face detecting? I'm aware that the precision probably will decrease but I don't care much about this.This is my code for detecting face landmarks using camera stream:That's my code for drawing landmarks on camera stream```<!DOCTYPE html><html><head>    <script src=""face-api.js""></script>    <script src=""commons.js""></script>    <link rel=""stylesheet"" href=""styles.css"">    <link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css"">    <script type=""text/javascript"" src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>    <script src=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js""></script></head><body><video id=""video"" autoplay width=""480"" height=""360""></video><canvas id=""overlay"" width=""480"" height=""360""></canvas><script>    const video = document.getElementById('video'),    const canvas = document.getElementById('overlay')    canvas.width = 480,    canvas.height = 360,    navigator.mediaDevices.getUserMedia({audio: true, video: true}).then(        stream => {            video.srcObject = stream,            video.play(),            video.muted = true,            run()        }    ).catch(e => console.warn(e)),    const minConfidence = 0.6    const maxResults = 1    async function run() {        await faceapi.loadFaceLandmarkModel('/'),        await faceapi.loadFaceDetectionModel('/'),        requestAnimationFrame(processFrame)    }    async function processFrame() {                const detections = await faceapi.locateFaces(video, minConfidence, maxResults)        const faceTensors = await faceapi.extractFaceTensors(video, detections)        let landmarksByFace = await Promise.all(faceTensors.map(t => faceapi.detectLandmarks(t)))        faceTensors.forEach(t => t.dispose())        if(landmarksByFace.length > 0){            landmarksByFace = landmarksByFace.map((landmarks, i) => {                const box = detections[i].forSize(480, 360).getBox()                return landmarks.forSize(box.width, box.height).shift(box.x, box.y)            })            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)            faceapi.drawLandmarks(canvas, landmarksByFace[0], {drawLines: true})        }        requestAnimationFrame(processFrame)    }</script></body></html>```and this code is only for detecting face (1 FPS on chrome)```  navigator.mediaDevices.getUserMedia({audio: true, video: true}).then(        stream => {            video.srcObject = stream,            video.play(),            video.muted = true,            run()        }    ).catch(e => console.warn(e)),    const minConfidence = 0.6    const maxResults = 1    async function run() {        await faceapi.loadFaceLandmarkModel('/'),        await faceapi.loadFaceDetectionModel('/'),        requestAnimationFrame(processFrame)    }    async function processFrame() {        const detections = await faceapi.locateFaces(video, minConfidence, maxResults)        const detectionsForSize = detections.map(det => det.forSize(480, 360)        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)        faceapi.drawDetection(canvas, detectionsForSize)        requestAnimationFrame(processFrame)    }```";"[""Hi,I know, face detection is currently the bottleneck. Probably the main reason for that is that the net works on 512x512 sized images. I was planning on providing an option to make it alternatively run on 256x256 sized images, first measurements turned out to provide a speedup of 4x - 6x, which atleast on my machine was achieving realtime speed.However, this change requires to train an additional model on top of the current model. I am currently working on an api to make the nets conveniently trainable, such that I can hopefully refine some of the models.Apart from that, looking at your code I assume you are trying to get the face landmark positions of a users face from a webcam? Probably it is also possible to pass the frames directly into the landmark net if you only have a single face shown, but you would probably have to play some tricks to get a rough estimation of the bounding box, or atleast crop the frame to a square. Not sure if that's gonna work out sufficiently well though.Atleast the forwarding time of the face landmark net should be much faster. On my machine it's about 60 - 80 fps, even on my shitty laptop it runs at about 30fps.====="", 'Super, so what should I do to pass frames directly into the landmark net? It will be very good if I could reach 30 FPS 👍 =====', 'I just released v0.8.0, which implements MTCNN as an alternative face detector. I have played around with live webcam detection on my laptop a bit and I can achieve about realtime performance with MTCNN, in case you want to give it a try. You can also find a MTCNN webcam example [here](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/views/mtcnnFaceDetectionWebcam.html).=====', ""Wow, it's really better than my previous experiments 👍 It's a pity, that the precision isn't so good, but fps are great. Well, It's not ideal face tracking, but I see a big progress.Looking forward to hearing about another version with improvements. Kind regards ====="", 'If you only want to detect a single face (webcam), I suggest you try to create your own mobilenet that has as output:\xa0\xa0\xa0 - face: sigmoid [classificacion]\xa0\xa0\xa0 - bounding-box: 4values, linear [regression]You can use a loss combined with the detection of the face by using a binary_crossentropy and bounding-box using mean_square_error.If not, you can opt for already existing methods like: viola & jones or this (it is an algorithm similar to viola & jones but replaces the haar features with a comparison between two pixels): https://github.com/tehnokv/picojsKeep in mind that you will lose accuracy, but you will earn a lot of FPS.=====', ""The problem is, that mobilenet itself is already slower than mtcnn, atleast from what I measured from PC, laptop and mobile.But it's probably faster than SSD, if you only have to predict a single bounding box, sure.====="", ""I will soon publish a new face detector (tiny yolo v2, using depthwise separable convolutions). This one is much faster than MTCNN on mobile (I can get processing times around ~100ms / 4 - 5 fps on mobile android) and seems to be about as fast as MTCNN on my desktop and laptop, but it's much more stable.You can already try it out under the link I posted here: #52. I will close this issue here.====="", 'Is there any way to improve speed and stabilization ?Trying 0.15 version, webfacetracking example with tiny face detector, 128 input size.=====', ""I'm using other ML models with fcae.api, the video in my webCam is lagging a bit  when I add face.api in my project, any way to smoothen the video that the user see's in their laptop?Thanks!=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/26;Matching names to faces. ;1;closed;2018-06-28T00:28:29Z;2018-08-26T12:23:54Z;I am trying to understand how face-api matches names to faces. Most of it seems to occur at the function  [getBestMatch ](https://github.com/justadudewhohacks/face-api.js/blob/5dc807c1a52a8089867785be48798dbb359df8f8/examples/public/commons.js#L60-L77)```function getBestMatch(descriptorsByClass, queryDescriptor) {  function computeMeanDistance(descriptorsOfClass) {    return faceapi.round(      descriptorsOfClass        .map(d => faceapi.euclideanDistance(d, queryDescriptor))        .reduce((d1, d2) => d1 + d2, 0)          / (descriptorsOfClass.length || 1)      )  }  return descriptorsByClass    .map(      ({ descriptors, className }) => ({        distance: computeMeanDistance(descriptors),        className      })    )    .reduce((best, curr) => best.distance < curr.distance ? best : curr)}```However when I look at other functions in the common.js file. It looks like values for comparison are calculated by loading each character into the model and using those values. (If that is correct, I feel like those values for the main characters could be saved in a json object file) Can someone explain the process for making these matches. I am fairly sure non of this data is actually saved in the models.My minimalist demo of just bounding boxes is [here](http://rocksetta.com/tensorflowjs/tf-examples/face-api/index.html) and an attempt to match characters is [here](http://rocksetta.com/tensorflowjs/tf-examples/face-api/next-index.html) (Crashes on the array.reduce calculation since one of my arrays is empty)As my demos are just single page webpages just right click to view the source. P.S. helps to have the console open as it gives some idea of the process. Note: my bounding boxes are working on my windows and ubuntu laptops, but strangely are too high on my windows10 desktop.;['Hi,There are no trained models saved for the face recognition part. Basically, in the face recognition examples is an example image loaded for each character and the face descriptor is computed and stored in an array (reference descriptors). Note, that the images loaded here are already aligned face images, thus you can pass them straight into the FaceRecognitionNet.Now, in the example [detectAndRecognizeFaces](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/views/detectAndRecognizeFaces.html), once you select an input image, all faces are detected in that image and their face descriptors are computed. What we do now is to match these face descriptors (query descriptors) to the reference descriptors, by computing the euclidean distance between two descriptors and simply find the one with the lowest distance, to identify the person with the help of the reference data.I would recommend you to also read my introduction, where we actually do the same thing: https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07.One thing that might be confusing in the example is, that you can have multiple images / descriptors for each class in your reference data instead of only 1 descriptor, which should make it more robust. Now, whats done in getBestMatch, is to compute the distance of the query descriptor to all reference descriptors for a class and simply return the mean value of the distances.Hope that clarifies it.=====']
https://github.com/justadudewhohacks/face-api.js/issues/22;Browser support / compatibility;1;closed;2018-06-26T17:26:42Z;2019-05-09T00:14:14Z;Would it be possible to list the browsers which support this package? For example can this work server-side on node or is it primarily client side focused?;"[""Sure. Actually it should run anywhere, where tensorflow.js runs. It's primarily focused for clientside applications. Haven't tried to run this in node. Although it should be possible, I think tfjs-core stuff cannot be run on the native gpu and running this on the CPU would be pretty damn slow.Also, for face recognition in nodejs, I have a similar package, [face-recognition.js](https://github.com/justadudewhohacks/face-recognition.js).=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/21;Face Detection Video doesn't work;4;closed;2018-06-26T11:20:11Z;2018-07-04T06:14:16Z;I checked the Face Detection Video but it doesn't seem to work, there is an error message in the console panel, please see attached screenshot:![2018-06-26_18-18-35](https://user-images.githubusercontent.com/6857382/41908248-8666c72a-796d-11e8-879c-d72816475acb.png)Thanks,;"['Oops, I think that happens, when the video is loaded before the model is done loading. I will provide a fix for that example.=====', 'Should be fixed now.=====', ""@justadudewhohacks I don't think it is still working properly. Although, there is no error in console. But, once the video starts, it recognizes first face and draws the rectangle. After that, as soon as person1(Loenard) moves out of frame, that rectangle sticks to the screen and never moves. PFA the screen shot.![issue21_face-api](https://user-images.githubusercontent.com/25663886/42147636-5cdd6570-7dec-11e8-8422-278fdef93ad4.png)![issue21_face-api2](https://user-images.githubusercontent.com/25663886/42147638-607fbf84-7dec-11e8-881a-a7412c0de43c.png)Is it due to performance issue?P.S. I have not cloned the repo. I just downloaded the archive from github by clickng download zip button.====="", ""Yes, that's simply because the face detector doesn't run in realtime on your machine.=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/17;Facial expressions / sentiment recognition;7;closed;2018-06-23T09:22:21Z;2019-01-17T09:31:57Z;Beautiful work with the Face API 🙌😍I was thinking... would it be possible to extend the API to extract sentiment from the detected faces or it'll be beyond the scope of the project? ;"[""Hi,I think that is a beautiful idea and certainly a possible extension in the future. Would you build that upon face landmark extraction? Atleast that's what I could imagine, to train a model, which performs sentiment analysis based on the 68 face landmark positions.Do you have anything in mind? If you know any project that implements sentiment analysis from facial expressions or some data sets, that could be useful for training a model, feel free to share ,)====="", 'That would be a great approach and can create efficient emotion detection models vs current approaches,  Here are couple of datasets,https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challengehttps://www.kaggle.com/c/emotion-detection-from-facial-expressions=====', 'Thanks for sharing the links  :)=====', 'You can take a look at this project https://github.com/bharathvaj1995/gender-detection-tensorflowjs=====', ""@startupgurukul do you by any chance have access to the dataset of your first link? Looks like I can't download it.====="", ""> @startupgurukul do you by any chance have access to the dataset of your first link? Looks like I can't download it.You can download it from here https://drive.google.com/file/d/1HJladquPEkD0a7i2FJxhyZPv1I1TCj_p/view?usp=sharing====="", 'Thank you!=====']"
https://github.com/justadudewhohacks/face-api.js/issues/14;5-point landmark detector;2;closed;2018-06-21T03:36:21Z;2018-06-21T20:50:41Z;It would be great to port the dlib 5-point landmark detector to this framework.https://github.com/davisking/dlib-models/blob/master/shape_predictor_5_face_landmarks.dat.bz2Right now, to perform the entire recognition pipeline on a new photo, it requires:1. Detection net: 22MB2. 68-point landmark net: 29MB3. Recognition net: 22MBOr around 73MB total, making it a bit unwieldy for most public-facing applications.The dlib 5-point landmark net is only 5MB, and just as useful for alignment as the 68-point landmark net. This would bring the total from 73MB down to 49MB. In theory, it might be possible to use quantization too as described in #11 bringing the overall size closer to 12MB.;"['Hi Kyle,I am not sure if it is trivial to port the dlib 5 point face landmark model to tfjs. As far as I know it is not CNN based (correct me if I am wrong). But in theory I think it should be enough to come up with a simple CNN for 5 point face landmarks, like the 68 point face landmark CNN, just with less conv layers/ conv params, to reduce the overall size.Regarding the quantization, I am already working on it. The quantized 68 point landmark model is 7MB. The face detection and recognition weights will be ~5MB each. For running the full pipeline, this will be ~17MB.If everything works out, I will get this done today.=====', ""Sorry, you're right about the 5 point detector being non-CNN. I missed this.I supposed in theory it would be possibly to prune the last layer of the 68-landmark model to only produce 5 outputs instead of 68, but without knowing the exact architecture of the 68-landmark model it's hard to say how much that would actually save.Super exciting to have the models be smaller! I have something I'm building now that relies on that. I'm going to close this issue since it was sort of misguided :)=====""]"
https://github.com/justadudewhohacks/face-api.js/issues/11;Are the weights with quantization?;8;closed;2018-06-17T20:54:23Z;2018-06-23T10:39:27Z;I was looking at the weight file sizes, they seem the same size like from original repos. Would be nice if the size could be reduced with quantization;"[""Hi,You are right, the weights are not quantized. I am not familar yet with how to run inference with a quantized model and whether it's possible with tfjs. But it would be awesome if we could reduce the model sizes that way. I will dig into it.====="", 'The process of quantization is just changing your weights from float32 to uint8 so you get a 4 times size decrease. I usually do it trough the converter=====', 'I know that you can quantize the weights using bazel, but do the weights simply get dequantized once you load them again?I read somewhere that the ops in the network have to be aware of the quantized weights to run inference, but I might be wrong here.In the first case, that should hopefully be easy to implement.=====', ""I'm not sure I never did manual quantization. https://github.com/tensorflow/tfjs-converter/blob/master/python/tensorflowjs/quantization_test.pyFrom the looks of it there could be a default scaling based on type====="", ""Yep seems like you are right. Looking at the [weight loader](https://github.com/tensorflow/tfjs-core/blob/master/src/io/weights_loader.ts) it's a simple scaling operation to dequantize the weights.Awesome! I will try to get this running soon, decreasing the model size from 28mb to 7mb looks promising.====="", '**Update**: So I managed to quantize the weights for the face detection and the face landmark model. Currently the changes are available on [this](https://github.com/justadudewhohacks/face-api.js/tree/quantize-weights) branch.Apparently quantizing the face recognition model is not as straight forward, as it originally was not a tensorflow model. The issue here is that simply quantizing all weights will make the model unusable, in a way that it returns wrong outputs. Right now, it seems that leaving the weights for the conv64 layers uncompressed and quantize the rest does work out however.Long story short: I am still working on it.=====', 'And here it is :)> model weights have been quantized, to reduce the model size by ~75%:> - face detection model: 21.7 MB -> 5.4 MB> - face recognition model: 28.7 MB -> 7.0 MB> - face landmark model: 21.9 MB -> 6.2 MB> > plus model weights are sharded in chunks of 4 MB to allow them to be cached in the browser=====', 'Thanks, will check it out=====']"
https://github.com/justadudewhohacks/face-api.js/issues/9;Add a configuration with external tf-js;2;closed;2018-06-15T15:02:05Z;2018-06-16T14:12:56Z;My issue is i would like to use this code as an external package with my other tfjs models.Currently you can only have one tensorflowjs script instance any other that gets loaded after the first one breaks.;"['Hmm i didnt try that yet, what would be required to make that work? Can you not bundle this package together with you own?In case it helps, tf core is also exported via facepi.tf=====', ""I'm going to close it, managed to do it by changing the types to any and removing the tf reference=====""]"
https://github.com/martinjm97/ENNUI/issues/81;Update the tutorial video to also show new features (CIFAR, new layers, export to julia);1;open;2019-02-24T17:56:51Z;2020-01-16T04:14:00Z;;['Get Hendrik or Strang to do this?=====']
https://github.com/martinjm97/ENNUI/issues/73;Fast Tab Loading;1;open;2019-02-24T04:14:34Z;2020-01-16T04:16:24Z;Have the tabs load as fast as possible by not computing the data with tfjs until after the tab has loaded.;['Switch to a lighter weight plot library?=====']
https://github.com/martinjm97/ENNUI/issues/71;Export trained model to keras/tf;4;open;2019-02-22T14:25:50Z;2019-02-22T18:45:05Z;;"['This will likely involve generating copatible json.=====', 'I think all we need to do is save the model and then provide skeleton code for loading it=====', ""I don't think we need to provide skeleton code, but otherwise it's annoying.====="", 'True, we can just export the saved model files=====']"
https://github.com/martinjm97/ENNUI/issues/70;Save model checkpoints and graph data for comparing two architectures;1;open;2019-02-20T21:52:15Z;2020-01-16T04:17:10Z;;['This might mean abandoning the tensorflow plotting, as we will want to plot loss and accuracy from multiple models on one graph for comparison.=====']
https://github.com/martinjm97/ENNUI/issues/64;Add weight initialization as a parameter to layers;3;open;2019-02-18T18:17:44Z;2019-03-05T22:45:01Z;tf.initializers offers several initializers we can use. Maybe give a dropdown rather than an text field for this one. We could offer zeros, ones, uniform random, and Xavier (implementable with tf.initializers.varianceScaling(scale=2).This would be used for the Obedience Training story.;"['This would be a dropdown in the parameters section?=====', 'As in a < select > instead of an < input >=====', '@martinjm97 would we want an ""advanced"" parameters checkbox somewhere which would reveal stuff like this? Or should we stick with no toggles in our UI? I just don\'t want there to be too many parameters. =====']"
https://github.com/martinjm97/ENNUI/issues/29;Add back in option to downsample data;1;open;2019-01-27T21:40:03Z;2020-01-16T04:17:37Z;Add in the option to downsample data so slower computers are more able to run app.;['on parameters of input layer.=====']
https://github.com/martinjm97/ENNUI/issues/2;Improved parameter entry for layers ;1;open;2019-01-07T17:38:41Z;2020-01-16T04:16:40Z;When you click on a layer the parameters that show up on the right have to be parenthesized properly.;['With the new UI for layers, make the parameter entries over the canvas, on select.=====']
https://github.com/martinjm97/ENNUI/issues/130;"Keep Saying ""Loading MNIST dataset""";6;closed;2020-10-20T21:15:04Z;2020-10-21T00:18:57Z;"When I enter the webpage: https://math.mit.edu/ennui/, the message ""Loading MNIST dataset"" keeps being on the top, and nothing changes for a long time. Is there anything wrong with the system?";"['Yes are in the middle of fixing this. It is a problem with the hosting.=====', 'Thanks for the quick reply! Please let me know when it is fixed=====', ""Should be all fixed now! Let me know and I'll close this issue.====="", 'How long it is supposed to take? Unfortunately, it is still there when I open the webpage, for about 2mins already=====', 'Oh it works now! =====', 'Great! Closing this issue. Let us know what you end up using this for! We always love to see people using Ennui 😃 =====']"
https://github.com/martinjm97/ENNUI/issues/124;[Question] Use of the created network in a Master's Thesis?;1;closed;2020-04-29T07:48:27Z;2020-04-29T15:36:18Z;Hello!We are in the process of writing our Master's Thesis and would like use this tool to visualize a network we are using. What we would like to do is to simply screenshot the created network and include it in our thesis while explaining the concept of neural networks.Being unfamiliar with licenses, we would simply like to ask if this is possible with the MIT license you are using for this repo?;"[""Hi @Fooughhy,Yup, no license issues here. Suggestion: maybe try editing the HTML with inspect element and change the background to be white. Feel free to cite the repo or the website. Maybe some of the explanations in the education tab will be of some use. I'm delighted to hear you're using Ennui and good luck with your thesis! I'd enjoy seeing the final product.=====""]"
https://github.com/martinjm97/ENNUI/issues/109;ResNets fail on CIFAR ;1;closed;2019-05-26T20:27:02Z;2019-05-26T21:52:20Z;Dimension errors. ;['Fixed by adding an extra conv layer to get the number of filters standardized for the add.=====']
https://github.com/martinjm97/ENNUI/issues/108;Regularization for conv layers breaks with copy link;1;closed;2019-05-26T19:12:09Z;2019-05-26T20:26:19Z;;['@Zackh1998 @martinjm97 help with this? =====']
https://github.com/martinjm97/ENNUI/issues/99;Add a 'Copied to Your Clipboard' notification when clicking the copy link button;5;closed;2019-03-07T20:47:49Z;2019-03-11T21:44:09Z;;"['How would you want this to show up? =====', ""Ideally as a pop up in the same areas the errors show up, but that doesn't seem right since errors show up there. No idea. Maybe right next to the link button?====="", 'I read a post on quora that convinced me to do nothing. The main point was that a repeatable and computationally cheap operation with a single purpose need not have user feedback=====', ""I also couldn't think of a good place to put it====="", ""In that case, I'll close the issue=====""]"
https://github.com/martinjm97/ENNUI/issues/98;Weird Split model architecture performs bad;7;closed;2019-03-06T21:33:23Z;2019-05-04T14:26:04Z;Performs bad on tfjs but good on python.;"['![image](https://user-images.githubusercontent.com/8813660/53915254-96401980-402d-11e9-8e72-214510a0abea.png)This is random across classes when trained in tfjs, but great in python...=====', ""@Zackh1998  Did you refresh page and try again? Could've been a webgl lost context.====="", 'I just tried this model and it works just fine on tfjs. This was definitely a WebGL lost context issue.=====', ""I tried multiple times... it's still broken https://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22,%22children_ids%22:%5B2,7%5D,%22parent_ids%22:%5B%5D,%22params%22:%7B%22dataset%22:%22mnist%22%7D,%22id%22:0,%22xPosition%22:100,%22yPosition%22:399%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B3%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22filters%22:10,%22kernelSize%22:%5B5,5%5D,%22strides%22:%5B2,2%5D,%22activation%22:%22relu%22%7D,%22id%22:2,%22xPosition%22:250,%22yPosition%22:319%7D,%7B%22layer_name%22:%22Dense%22,%22children_ids%22:%5B6%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22units%22:30%7D,%22id%22:7,%22xPosition%22:285,%22yPosition%22:564%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B5%5D,%22parent_ids%22:%5B2%5D,%22params%22:%7B%7D,%22id%22:3,%22xPosition%22:571,%22yPosition%22:319%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B5%5D,%22parent_ids%22:%5B7%5D,%22params%22:%7B%7D,%22id%22:6,%22xPosition%22:527,%22yPosition%22:535%7D,%7B%22layer_name%22:%22Concatenate%22,%22children_ids%22:%5B1%5D,%22parent_ids%22:%5B6,3%5D,%22params%22:%7B%7D,%22id%22:5,%22xPosition%22:765,%22yPosition%22:444%7D,%7B%22layer_name%22:%22Output%22,%22children_ids%22:%5B%5D,%22parent_ids%22:%5B5%5D,%22params%22:%7B%7D,%22id%22:1,%22xPosition%22:900,%22yPosition%22:399%7D%5D,%22hyperparameters%22:%7B%22learningRate%22:0.1,%22batchSize%22:64,%22optimizer_id%22:%22defaultOptimizer%22,%22epochs%22:6,%22loss_id%22:%22defaultLoss%22%7D%7D====="", 'Not a bug for me at all. ![Screenshot from 2019-03-09 16-09-54](https://user-images.githubusercontent.com/7140467/54077423-f9c48400-4285-11e9-8215-1945aecc5f0b.png)=====', 'Still not working for you?=====', 'Makes absolutely no sense, is impossible to debug, and only shows up on my computer.=====']"
https://github.com/martinjm97/ENNUI/issues/90;Prevent bad connections at build time;1;closed;2019-03-05T22:01:00Z;2019-05-26T23:56:56Z;Right now we prevent bad connections during tfjs model compile time, but we could do some of this during build time, like preventing the user from putting multiple inputs into a dense (without a flatten). A red X over the dashed guide wire (or the dashed line becoming red) or something of that sort might work well to indicate to the user that is invalid. Then if they try to make it a wire, it can either give an error, or just not do anything and select the new layer (like selecting output then a dense currently does);['Implemented as part of displaying layer dimensions. This makes a model compile on every layer click (and therefore every layer connection).=====']
https://github.com/martinjm97/ENNUI/issues/88;Make initial position layers are spawned on proportional to svg size;1;closed;2019-03-05T06:45:19Z;2019-03-28T01:14:38Z;;['Close? @martinjm97 does the recent pushes regarding canvas sizing and boundaries fix this?=====']
https://github.com/martinjm97/ENNUI/issues/87;New generated layers are off canvas;1;closed;2019-03-05T05:56:13Z;2019-03-05T05:56:47Z;too high up;['Changed initial position of all activationlayers to be shifted down=====']
https://github.com/martinjm97/ENNUI/issues/82;Add batchnorm export to julia for now it errors.;1;closed;2019-02-24T21:59:09Z;2019-02-27T15:27:52Z;;['Implemented and working=====']
https://github.com/martinjm97/ENNUI/issues/65;If a parameter for a layer is a decimal, the python generation does not take anything after decimal point;2;closed;2019-02-18T19:31:06Z;2019-02-19T16:42:01Z;You can see this behavior in the newLayers branch;['You can also see this behavior by editing Conv filters to 10.1. The python code will show 10 instead.=====', 'Solution is in the newLayers branch=====']
https://github.com/martinjm97/ENNUI/issues/59;Activations are never removed from layers when deleted while attached;1;closed;2019-02-17T21:22:34Z;2019-02-24T22:35:41Z;;"[""The issue was that when activations were being added in model_templates, the layer was not also being added to that activation, this deleting the activation didn't update the corresponding layer.=====""]"
https://github.com/martinjm97/ENNUI/issues/57;Add Batch Norm, Dropout, Flatten, and Concatenate;2;closed;2019-02-17T01:25:41Z;2019-02-20T17:47:04Z;Batch norm will need to have an activation option;['Merge the addMenu branch afterward=====', 'This is all in the newLayers branch=====']
https://github.com/martinjm97/ENNUI/issues/53;when window is resized, right side of network is cut off;3;closed;2019-02-09T10:01:00Z;2020-01-17T22:49:26Z;;"['similarly the svg area is artificially small when the left menu is collapsed=====', ""Maybe we should trigger a refresh on resize? The only issue is if there is a resize while training it probably shouldn't refresh.====="", 'Can this be closed now?=====']"
https://github.com/martinjm97/ENNUI/issues/46;at 1080p with bookmarks bar conv parameters are cut off at the lower right corner;1;closed;2019-02-05T06:49:35Z;2019-02-09T22:18:40Z;;['I made all the menu categories collapsible, this way, one can see all the parameters by minimizing the export options for example=====']
https://github.com/martinjm97/ENNUI/issues/44;Investigate: Stupid architectures are still getting really good results;1;closed;2019-02-05T05:20:30Z;2019-02-05T06:00:40Z;I tried the architecture of input straight to output and that got 85% validation accuracy. That seems fishy to me...;['If a single layer network actually can get 85% on MNIST, that is pretty crazy=====']
https://github.com/martinjm97/ENNUI/issues/43;Better structure the cloning mechanism - No extra hidden divs;4;closed;2019-02-05T05:17:16Z;2019-02-24T04:20:12Z;This implies making the layer parameters an actual member of Layer and adding hooks with the parameter boxes.;['Or not making instances of the class directly correspond to rendered objects. Also, cloning should be separated from adding in Concate + Flatten layers.=====', 'Logic is now separated.=====', 'The way that cloning currently works is that a clone points to the same parameter box as the original. This is bizarre, so this should be changed by making a parameters member of Layer, as mentioned above.=====', 'Will be irrelevant once we merge in the flatten and concatenate layers=====']
https://github.com/martinjm97/ENNUI/issues/36;After selecting layer, dragging corresponding activation deletes it.;1;closed;2019-02-01T16:34:07Z;2019-02-03T02:03:37Z;This only happens for the very first layer you select.;['Semi duplicate of Issue #34, but you dont need to click the black dot=====']
https://github.com/martinjm97/ENNUI/issues/33;Don't modify original graph when training.;1;closed;2019-01-30T04:21:01Z;2019-02-05T05:14:31Z;;['Also errors if there are floating elements=====']
https://github.com/martinjm97/ENNUI/issues/32;Add name of layers to parameters section;1;closed;2019-01-30T04:12:34Z;2019-02-24T04:25:07Z;;['Should only be done for layers that have weights. =====']
https://github.com/martinjm97/ENNUI/issues/28;Add another template;1;closed;2019-01-27T21:38:34Z;2019-02-04T03:43:09Z;Add a template with non-sequential model (Maybe add a second template of a model that performs very badly on MNIST);['One that performs quite well and is non-sequential=====']
https://github.com/martinjm97/ENNUI/issues/27;Add another dataset (CIFAR-10 maybe);2;closed;2019-01-27T21:36:14Z;2019-03-06T21:24:01Z;;['fixing class labels also=====', 'Close this?=====']
https://github.com/martinjm97/ENNUI/issues/22;Shuffle data every time menu is clicked in Visualization tab;1;closed;2019-01-23T01:11:54Z;2019-02-24T04:35:43Z;;['decided not important=====']
https://github.com/martinjm97/ENNUI/issues/20;Add classes to visualization tab and split based on incorrect/correct predictions;1;closed;2019-01-22T02:20:19Z;2019-01-23T01:10:53Z;;"[""Don't yet split based on prediction, but it may be confusing to users.=====""]"
https://github.com/martinjm97/ENNUI/issues/11;Surround layers with a bounding box, Corner for dragging, Arrows out when dragging otherwise;2;closed;2019-01-07T17:51:22Z;2019-05-04T14:24:56Z;;"['Maybe, but probably not also names=====', ""No bounding box or corner dragging. It's better the way it is now.=====""]"
https://github.com/thekevinscott/ml-classifier-ui/issues/6;Model Error;8;closed;2019-05-14T06:10:57Z;2019-05-31T16:25:24Z; start(), async function start() {const model = await tf.loadLayersModel('model.json'),const example =  tf.browser.fromPixels(document.getElementById('test')),  // for exampleconst prediction = model.predict(example),    }errors.ts:48 Uncaught (in promise) Error: Error when checking : expected flatten_Flatten1_input to have 4 dimension(s), but got array with shape [640,960,3]    at new e (errors.ts:48)    at Dd (training.ts:311)    at e.predict (training.ts:1073)    at e.predict (models.ts:765)    at start ((index):31);"['It looks like you may need to resize your image. Can you provide a code sandbox link?=====', 'Yes , I did follow your article for resizing the image , here is my code thanks for helping out. [test.zip](https://github.com/thekevinscott/ml-classifier-ui/files/3193631/test.zip)=====', ""You are loading the wrong model. You're loading your own model and trying to use that for activations, you should be using mobilenet.Try replacing `loadTruncatedMobileNet` with the following:```async function loadTruncatedMobileNet() {    const pretrainedModelURL = 'https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_0.25_224/model.json',    const mobilenet = await tf.loadLayersModel(pretrainedModelURL),    const layer = mobilenet.getLayer('conv_pw_13_relu'),    return tf.model({inputs: mobilenet.inputs, outputs: layer.output}),  }  async function loadModel() {    return await tf.loadLayersModel('ml-classifier-Super_Galaxy-Super_Snow.json'),  }  async function start() {      truncatedMobileNet = await loadTruncatedMobileNet(),      model = await loadModel(),...```You can call `model.summary()` to inspect the shape of your model. You'll see that it expects a different shape.====="", 'The model was generated by your ml classifier=====', 'It still needs to process the images through mobilenet for the initial activation. The embeddings mobilenet produces are the things you want to pass to your model. =====', 'OK , many thanks!=====', ""Sure! Let me know if you run into any problems.As an FYI, you _can_ create a single model that combines the pretrained model with your own layers. I don't have any code showing how to do this, but high level you'd want to construct a sliced model from mobilenet, make sure the existing layers are frozen (aka, ensure they won't be trainable), and then concatenate your own unfrozen layers onto the model.I prefer the approach shown here, where the pretrained model exists separately from your model, because it gives more flexibility on the UX side to process data. For instance, you can activate the images through mobilenet when the user uploads them, instead of having to do it all in one big step.====="", 'Yes understand, again big thanks for your help, I am just into tensor flow your work really helped me out, keep it up !=====']"
https://github.com/thekevinscott/ml-classifier-ui/issues/4;How to get started?;1;closed;2018-11-10T21:24:35Z;2018-11-24T17:16:44Z;I am familiar with html/js/css, but not react.I can't figure out how to get started. The Quick start section did not really help me. Any link/additional info how i can use this code for generating a html page?Sam;"[""Hi @SamuelReinfelder,I set up a codesandbox here that demonstrates it in action:https://codesandbox.io/s/218po5mzxnIt is written in React so you will need to use the component within the context of a React application.If you're not using React, I'd recommend checking out the standalone piece, [`ml-classifier`](http://github.com/thekevinscott/ml-classifier), which has no associated UI and is just the model and training. You'd need to provide your own mechanism for uploading images and displaying the results but all of the ML work would be handled for you.And if you'd like to fork a version in Vue or another JS framework, that would be very welcome!=====""]"
https://github.com/thekevinscott/ml-classifier-ui/issues/3;How to add a single image for prediction?;3;closed;2018-09-16T10:29:56Z;2019-05-17T17:19:14Z;I have downloaded the trained model . When i import it into my code i get few errorUncaught (in promise) Error: Based on the provided shape, [12544,100], and dtype float32, the tensor should have 1254400 values but has 0Do you have any sample code for loading a pretrained model and run prediction just for 1 image;['Hi @technoartista, can you share the code or a repo with the bug?=====', 'Same problem here, if you can provide us a sample code of predicting image would be great! Thanks =====', 'Check out https://thekevinscott.com/image-classification-with-javascript/ =====']
https://github.com/BeTomorrow/ReImproveJS/issues/24;Train on a dataset?;1;open;2020-05-25T00:58:04Z;2021-10-14T19:53:26Z;Hello, thanks for putting all your work out there :100:Is it possible to train on a recorded data set? Specifically from something like a csv.txt?;['This defeats the point of a reinforcement network, which is try and figure out solutions and rules by itself.=====']
https://github.com/BeTomorrow/ReImproveJS/issues/18;Question: Why are Convolutional Networks not Recommended?;1;open;2019-10-01T08:12:14Z;2019-10-07T16:24:03Z;On the MIT lecture e.g. they have an example of a DQN with convolutional layers.https://youtu.be/i6Mi2_QM3rA?t=1531Or here: https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6bWould be interested if this is a problem in general or just with this implementation of DQN. I couldn't find anywhere that it is a problem.;['It is not a problem, it is just that I started developing it but I had not enough time to fully test and finish it. Some things were working but actually need to be revamped etc . So for now the feature is not functional.=====']
https://github.com/BeTomorrow/ReImproveJS/issues/17;How to save and load the model?;1;open;2019-09-03T09:02:59Z;2021-02-18T13:09:53Z;"I tried saving the model using tensorflow's model.save().But when I tried using it after loading, I got an error:""this.model.randomOutput is not a function""Please guide on how we can save and load the model to continue training.";['@Pravez Any chance you could answer this?=====']
https://github.com/BeTomorrow/ReImproveJS/issues/16;confused about using model;1;open;2019-07-12T07:19:46Z;2019-09-17T15:17:08Z;"Hello! Really nice library, thank you!I am bit confused, I've trained model with good results, saved it to file and now I would like to use with `model.predict` in ""plain"" tensorflow.js or even something like `agent.predict`. How can I do that? I am new to tensorflow.js, tried multiple options, always getting errors that shape is different than expected on `tensor2d` contruction. Thank you for your help.";['Hi ! Sorry for the delay.For what I understood, you want to use the model trained with ReimproveJS in a plain tensorflow.js environment, without the library.The problem must be the size of your input. Indeed the neural network in ReimproveJS is taking as input the current state plus a specified quantity of previous states and previous taken actions. By default it is set to 1. So for instance if your state is of size 5 and your actions is of size 1, then the size of the input will be of 5+5+1, so 11.Hope this helps=====']
https://github.com/BeTomorrow/ReImproveJS/issues/14;saving the model;4;open;2019-04-24T15:37:45Z;2020-07-11T18:24:39Z;I see a loadFromFile() method but I'm not sure how to save the model. Is there an easy way to do that?;"[""```jsmodel = await Model.loadFromFile('indexeddb://my-model')// ...await model.export('my-model', 'indexeddb')```====="", 'if in nodejs you need to hack```jsimport { NodeFileSystem } from ""@tensorflow/tfjs-node/dist/io/file_system""//...await model.model.save(new NodeFileSystem(\'./model))//...```=====', ""@Baael how do you load it, if I run it in node I get:`ReferenceError: fetch is not defined`and this can be skipped if the model is loaded, right:```        // Now we initialize our model, and start adding layers        this.model = new ReImprove.Model.FromNetwork(this.network, this.options.modelFitConfig),        // Finally compile the model, we also exactly use tfjs's optimizers and loss functions        // (So feel free to choose one among tfjs's)        this.model.compile({loss: 'meanSquaredError', optimizer: 'sgd'})```====="", ""how do you do this for the browser?I'm getting this: ![image](https://user-images.githubusercontent.com/199018/87231088-2ad52d00-c382-11ea-85ce-1da7311821c0.png)![image](https://user-images.githubusercontent.com/199018/87231097-404a5700-c382-11ea-8f08-8529767e0bf8.png)=====""]"
https://github.com/BeTomorrow/ReImproveJS/issues/11;How to execute actions (output);7;open;2018-10-13T08:10:48Z;2020-09-30T18:42:04Z;Hi,I am still new to Reinforcement learning, I remember [Reinforce JS](https://github.com/karpathy/reinforcejs) did have this act() function, which takes as input the state variables and outputs an action. How can we act upon the output of the RL agent in ReImproveJS?thanks in advance!;"['In Reinforcement learning the ""only"" way you have to act on the agent is through the rewards system. You have to balance well between positive or negative rewards in order to make your agent learn. For instance if you want your agent to go to the right, you will likely put a positive reward on its increasing x position, and a negative reward on its decreasing x position.ReImproveJS does ""everything"" for you, meaning you just have to call in the step() function either `academy.addRewardToAgent(agent, -1.0)` or `academy.addRewardToAgent(agent, 1.0)` according to what your agent was supposed to do.Let me know if it was you understood well :)=====', ""Hello Pravez, thanks for replying! I am still a bit confused to be honest. I will demonstrate to clarify my goal, here is the test case:http://jsfiddle.net/ydaqhpwL/3/There is a controllable player and a robot (red, the RL agent). The player can be moved using updownleftright.The  learning objective is to follow the player.The reward scheme is calculated according to the distance (in calculateReward function).I was thinking that the current setup would be something that could work, but after 10-15s the game freezes for a bit and afterwards the robot drifts away. I used the actionsBuffer (line 137) previously to make the robot (red) move, but I'm not sure if that makes sense anymore. I did have some sensible result in ReinforceJS in a similar test case, where the objective, simulation world and reward scheme is similar:https://codepen.io/Samid737/pen/opmvaRHere, in line 85, one of the four possible actions is chosen by the agent.P.S: I would gladly help out with creating examples (using Phaser JS or other frameworks) if that is on the roadmap. Thanks again in advance.====="", 'Indeed the example with ReinforceJS works really nicely !Thank you for your example, I think it will perfectly suit as an example for ReimproveJS... !First of all you have some ""errors"" in your code that might change some of the data the agent is learning on.line 127```Javascriptvar s = [player.y,player.y,robot.x, robot.y], ```I think you wanted ```Javascriptvar s = [player.x,player.y,robot.x, robot.y], ```line 185```Javascript var dist = Phaser.Math.Distance.Between(player.x,robot.x,player.y,robot.y), ```According to phaser.js it should probably more be like```Javascript var dist = Phaser.Math.Distance.Between(player.x,player.y,robot.x,robot.y), ```Also, the step() function from the academy already returns the action the agent took. So you just have to call `step().get(agent)` to get the action. No need to go deep directly into the agent\'s data.ReImproveJS, because built on top of TensorflowJS has a different way of learning than ReinforceJS.Indeed, it continuously do WebGL calls (and so, your GPU), and needs some more ""time"" to do backpropagation (that\'s why the `step` function is async). I designed so the learning phase in order to reduce a bit the impact of this time.Your agent has X learning sessions of Y steps. At the end of these Y steps, it will do its learning on the entire session it recorded. At the end of the X sessions, it will only do inference. According to your use case, you should do many very little sessions. Also, your agent really cares (I think) about its future rewards, so your gamma should also be really high.Another thing, your agent is training on parts of his memory, randomly selected. If its memory is too big, and its learning is too much made of bad decisions, it will statistically select more bad decisions to learn on and will indefinitely stay on it. A way to improve it would be to drastically reduce its size, so that with time the bad decisions are replaced with good ones, etc ...And, little detail (I think it\'s my documentation that is wrong, so mb) but it\'s `lessonLength` and not `lessonsLength`.Here is the configuration I would have used :+1: ```Javascriptconst teacherConfig = {    lessonsQuantity: 10000,                      lessonLength: 20,                        lessonsWithRandom: 0,                  // We do not care about full random sessions    epsilon: 0.5,                            // Maybe a higher random rate at the beginning ?    epsilonDecay: 0.995,                       epsilonMin: 0.05,    gamma: 0.9                            },const agentConfig = {    model: model,                          // Our model corresponding to the agent    agentConfig: {        memorySize: 1000,                      // The size of the agent\'s memory (Q-Learning)        batchSize: 128,                        // How many tensors will be given to the network when fit        temporalWindow: temporalWindow         // The temporal window giving previous inputs & actions    }},```Here is the link to the modified jsfiddle : http://jsfiddle.net/ydaqhpwL/7/This said, it does not work either :disappointed: . I will try to investigate further quickly and keep you informed.Thank you really much anyway for your interest and work :+1: =====', 'Indeed the example with ReinforceJS works really nicely !Thank you for your example, I think it will perfectly suit as an example for ReimproveJS... !First of all you have some ""errors"" in your code that might change some of the data the agent is learning on.line 127```Javascriptvar s = [player.y,player.y,robot.x, robot.y], ```I think you wanted ```Javascriptvar s = [player.x,player.y,robot.x, robot.y], ```line 185```Javascript var dist = Phaser.Math.Distance.Between(player.x,robot.x,player.y,robot.y), ```According to phaser.js it should probably more be like```Javascript var dist = Phaser.Math.Distance.Between(player.x,player.y,robot.x,robot.y), ```Also, the step() function from the academy already returns the action the agent took. So you just have to call `step().get(agent)` to get the action. No need to go deep directly into the agent\'s data.ReImproveJS, because built on top of TensorflowJS has a different way of learning than ReinforceJS.Indeed, it continuously do WebGL calls (and so, your GPU), and needs some more ""time"" to do backpropagation (that\'s why the `step` function is async). I designed so the learning phase in order to reduce a bit the impact of this time.Your agent has X learning sessions of Y steps. At the end of these Y steps, it will do its learning on the entire session it recorded. At the end of the X sessions, it will only do inference. According to your use case, you should do many very little sessions. Also, your agent really cares (I think) about its future rewards, so your gamma should also be really high.Another thing, your agent is training on parts of his memory, randomly selected. If its memory is too big, and its learning is too much made of bad decisions, it will statistically select more bad decisions to learn on and will indefinitely stay on it. A way to improve it would be to drastically reduce its size, so that with time the bad decisions are replaced with good ones, etc ...And, little detail (I think it\'s my documentation that is wrong, so mb) but it\'s `lessonLength` and not `lessonsLength`.Here is the configuration I would have used :+1: ```Javascriptconst teacherConfig = {    lessonsQuantity: 10000,                      lessonLength: 20,                        lessonsWithRandom: 0,                  // We do not care about full random sessions    epsilon: 0.5,                            // Maybe a higher random rate at the beginning ?    epsilonDecay: 0.995,                       epsilonMin: 0.05,    gamma: 0.9                            },const agentConfig = {    model: model,                          // Our model corresponding to the agent    agentConfig: {        memorySize: 1000,                      // The size of the agent\'s memory (Q-Learning)        batchSize: 128,                        // How many tensors will be given to the network when fit        temporalWindow: temporalWindow         // The temporal window giving previous inputs & actions    }},```Here is the link to the modified jsfiddle : http://jsfiddle.net/ydaqhpwL/7/This said, it does not work either :disappointed: . I will try to investigate further quickly and keep you informed.Thank you really much anyway for your interest and work :+1: =====', ""Thanks Pravez, I did overlook the x's and y's indeed. Yes, RL is extremely interesting, especially when seeing actual results/use cases, Phaser really helps in this imo. I do need to dive into the theory... It seems like the async operations don't do very well in update(), but I can't really tell whats causing the hickups. Maybe @photonstorm has a clue ,p?====="", 'I have been interested in this project for some time now as  I plan on using it within my Screeps environment. I finally had time to sit down with it and worked out an example. For those interested, I created a gist with a more or less complete example:https://gist.github.com/RGBKnights/756b5f51465cc22d0ca39205979ad2a1=====', ""Thank you for your time ! I'll add this exemple to the README. I will certainly create a new one based on yours when i'll have enough time.=====""]"
https://github.com/BeTomorrow/ReImproveJS/issues/10;Roadmap/Todos for Convolutional learning;1;open;2018-10-06T10:53:33Z;2018-10-08T09:28:47Z;First of all I want to thank you for providing this library to the public!What are your plans currently regarding finishing the implementation for convolutions? And is there like a Todo-List that others could look at to contribute?cheers & keep up the good work!;['Hi,I am really grateful for your interest. Unfortunately I cannot continue my work on it today, even if I plan to continue it later. If you feel brave enough to continue the work I will publish as soon as possible a roadmap in order for anyone to continue this project.Thank you,=====']
https://github.com/BeTomorrow/ReImproveJS/issues/9;Need more informations about lesson;3;open;2018-08-22T23:59:38Z;2019-01-03T01:30:23Z;Hi,Thank you for your job.I'm working for fun with your lib, but I don't clearly understand the concept of lesson (lessonsQuantity, lessonsLength, and lessonsWithRandom) and how to use lessons callback.Imagine I'm trying to train 50 agents to play to Flappy bird. How to setup my lessons please ?Thank you,Jeremie.;"['Hello and thank you.The idea of the lesson is that your agent is like a student in an academy. It has a teacher which regulates its learning. When your game is updating itself, the teacher is managing the agent so that he can learn properly at a certain ""rate"". Each lesson is finally hours of teaching (lessonsLength) where at the end the agent has ""homework"" (model training).For instance with (_lessonsQuantity_ = 50, _lessonsLength_ = 1000 and _lessonsWithRandom_ = 10), the teacher will regulate the agent so that it will ""learn"" during 50 lessons (meaning, will do 50x backpropagation to update model\'s weights), each lesson will be 1000 frames length (so after your world updated 1000 times, the current lesson ends, then the agent does some learning, then a new lesson starts), and during the 10 first lessons the agent will take completely random decisions (exploration/exploitation problem, with random you ensure you explored approximately every possible state of your world).Your computer will have a hard time if you are trying to train 50 different agents at the same time, do not forget that this library is at its beginnings, without the optimal algorithms and running in **Javascript**, in your browser. It would be a good idea to prefer genetic algorithms (which I hope will be implemented later in this library).Let me know if this helped=====', 'Hello,First, Thanks for your work !I\'m not an expert in reinforcement learning.I\'m actually trying to use your library correctly.I\'m wondering how do you handle simulation ending when you don\'t have fixed ""update/step"".For example (Flappy bird),  config => lessonLength: 1000  Your bird make a bad action after 10 update and the game end.How do you tell the teacher that the lesson is finish, and force him to train the agent ?Should I continue replaying several games until the number of step reach the lessonLength ?Thank youClement=====', 'Thanks for making this available, looks excellent. Is there any simple examples available online where you have implemented this? This concept of an academy and teachers etc. is intriguing, but the formulation is unique so it would help to have a concrete example where all the elements are interacting.=====']"
https://github.com/BeTomorrow/ReImproveJS/issues/23;expected dense_Dense1_input to have shape [null,12] but got array with shape [1,14];1;closed;2020-04-08T13:03:33Z;2020-04-08T19:43:39Z;I am getting this error when running ReimproveJs```Uncaught (in promise) Error: Error when checking : expected dense_Dense1_input to have shape [null,12] but got array with shape [1,14].    at new ValueError (reimprove.js:1)    at checkInputData (reimprove.js:1)    at Model.predict (reimprove.js:1)    at Sequential.predict (reimprove.js:1)    at Model.predict (reimprove.js:1)    at reimprove.js:1    at Object.Tracking.tidy (reimprove.js:1)    at Agent.createTrainingDataFromMemento (reimprove.js:1)    at reimprove.js:1    at Array.map (<anonymous>)```This error is not happening directly but after 5 seconds of it working. its called by academy.stepMy part, only added inputs:```let inputs = [speed,turn,sensor1Dist,sensor2Dist,sensor3Dist],          // Need to give a number[] of your inputs for one teacher.        let result = await academy.step([               // Let the magic operate ...        {teacherName: teacher, agentsInput: inputs}        ]),``` ;['Defined wrong inputSize=====']
https://github.com/BeTomorrow/ReImproveJS/issues/6;Unknown Loss in example Code;1;closed;2018-07-19T20:56:29Z;2018-08-23T08:19:48Z;The Loss function in your example code does not exist anymore:You should change this line:```// Finally compile the model, we also exactly use tfjs's optimizers and loss functions// (So feel free to choose one among tfjs's)model.compile({loss: 'crossEntropy', optimizer: 'sgd'})```to something like`model.compile({loss: 'meanSquaredError', optimizer: 'sgd'})`;['Indeed, done and thank you !=====']
https://github.com/BeTomorrow/ReImproveJS/issues/3;Is the library in a usable/robust state?;2;closed;2018-06-19T21:25:43Z;2018-06-25T08:25:24Z;If I wanted to train a feedforward network agent with ~100 inputs, 8 outputs, and a hidden layer of 512 or so, can I use a DQN from this library to do it and expect it to work out okay?Does the DQN have any of the bells and whistles such as dueling DQN, prioritized experience replay, etc.?;"['Hi,Currently this library implements the ""most simple"" form of the DQN. It is absolutely possible to use it with your inputs/outputs/layers, but it will be only the Q function with only the classical experience replay. So It will work out okay but without wonderful results, because of the need of better algorithms. Double Learning et Prioritized Experience Replay will be implemented in the future, but it needs some time ,)=====', 'Thanks for the answer, keep up the good work! =====']"
https://github.com/BeTomorrow/ReImproveJS/issues/2;Example/Documentation update [TFJS expected axis NaN for dense1];4;closed;2018-06-10T06:47:49Z;2018-06-11T13:48:47Z;"It seems that the model must goes inside the Agent config:`model.compile({loss: 'categoricalCrossentropy', optimizer: 'sgd'})const agentConfig = {    model: model,    memorySize: 5000,    batchSize: 128,    temporalWindow: temporalWindow},``await academy.step([        {teacherName: teacher, inputs: inputs}    ]),`The parameters in the interface AcademyStepInput are ""teacherName"" and ""agentsInput"".";"['Indeed, thank you for noticing it ! Corrected.=====', ""The model should be like this:`const agentConfig = {\tmodel: model,\tagentConfig: {\t\tmemorySize: 1000,\t\tbatchSize: 10,\t\ttemporalWindow: temporalWindow\t}},`Also, the academy.step should be:`await academy.step([\t\t\t\t{ teacherName: teacher, agentsInput: inputs }\t\t\t])`I'm also debugging another issue with tensorflow.js but I haven't figured out why it's happen.Error: Input 0 is incompatible with layer dense_Dense1: expected axis NaN of input shape to have value====="", 'Thank you once more, I forgot the second part of the correction...Can you provide an example to reproduce your bug ?=====', 'Please ignore the error, it was caused by another third part code overriding the Object.prototype.clone.At some point it was changing the type of the objects used by ts.js and failing in the assert.Thanks for this awesome framework 👍 =====']"
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/6;TinyYolov2Trainable;2;open;2019-03-27T19:37:01Z;2019-04-01T17:06:13Z;Hi @justadudewhohacks How are you?Do you have any plans of adding TinyYolov2Trainable to this repo?Would be cool :)Thank you;"[""I would rather not do so, since face-api.js does not have any dependency on TinyYolov2Trainable. What's your incentive behind that, why not install tfjs-tiny-yolov2?====="", 'I just thought that this repo is a dependency for the tfjs-tiny-yolov2 and it seems like you have moved everything except trainable from yolov2 in to this one, it seems a little redundant since both of them have yolov2.  It was just a thought, i think you are trying to keep it slim for the face-api.I have a separate question, have you tried training using nodejs? i run in to a problem of loading pictures in since backward function expects HTMLImageElement | HTMLCanvasElement. Any advice on how i could load pictures? Thank you=====']"
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3;'Illegal Constructor' exception thrown when used in Electron renderer process;8;open;2018-12-08T00:03:45Z;2019-03-01T06:28:36Z;When using this as part of the face-api.js package, I get an 'Illegal Constructor' exception when trying to do a face detection in the renderer process of an Electron app (e.g. `faceapi.detectSingleFace()`). Looks like face detection is attempting to create a canvas element by calling `new HTMLCanvasElement()` instead of the required `document.createElement('canvas')`.The root of the issue appears to be in src/env/initialize.ts, where `function createCanvasElement` attempts to create a new HTMLCanvasElement directly.I'm able to work around it by overriding `createCanvasElements` (and `createImageElement`), e.g.```const faceapi = require('face-api.js'),faceapi.env.monkeyPatch({    createCanvasElement: () => document.createElement('canvas'),    createImageElement: () => document.createElement('img')}),```;"['I see the same issue is filed here: https://github.com/justadudewhohacks/face-api.js/issues/157=====', ""Yes, I think we should definitely export the initializer funcitons from [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts). This way the user can simply fix the environment himself in case his working enivronment is an unusual deviation of the nodejs or browser enivronment.As for the electron renderer thread it would be optimal to fix the isNodejs() check, such that it doesn't break in the standard nodejs environment. Maybe checking for the browser environment first in [initializeEnvironment](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts#L5) would already fix this for electron.What do you think?====="", ""I think giving the user the option to specify environment would be best, essentially what monkeyPatch does but in a less seemingly-hackish way. :)I took a look at the environment setup in my electron app. `isBrowser` would return true since all of the browser elements are present. But `initializeBrowserEnv` doesn't give the option having `readFile` available, which would be present in electron apps as well.====="", 'Hello @andrewwalters and @justadudewhohacks I am also using electron but in my case isNodejs returns true and isBrowser also returns true.I get the following error when I call detectAllFaces:Error: pixels passed to tf.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement or ImageData, but was Canvas=====', ""I think the renderer process environment should now get initialized correctly, if you want to give face-api.js v0.16.2 a try.I also exported createBrowserEnv and createNodejsEnv, so in case there are still issues with incorrect initialization one can easily fix this by `faceapi.env.setEnv(faceapi.env.createBrowserEnv())` for example.Regarding readFile in the renderer process, as far as I am aware, you have to require fs via electron remote in the renderer process right? I also exported a createFileSystem helper, which lets you monkeyPatch all the fs related envs as follows:``` javascriptconst fs = remote.require('fs')faceapi.env.monkeyPatch(faceapi.env.createFileSystem(fs))```====="", ""The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process, in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment, and it worked:```const fs = require('fs'),faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(), faceapi.env.createFileSystem(fs))),```====="", 'Interesting, ok.=====', ""> The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process, in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment, and it worked:> > ```> const fs = require('fs'),> faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(), faceapi.env.createFileSystem(fs))),> ```This worked ! Awesome. Thanks ! :)=====""]"
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/10;Can you please use explicit tf-core dependency versioning that matches face-api;1;closed;2019-10-31T02:00:10Z;2020-03-21T13:04:45Z;It forces me to use a skipLibCheck and clutters my dependency tree since it will almost always download a higher version of tf-core then required by face-apiThis issue is a direct result of that https://github.com/justadudewhohacks/face-api.js/issues/415Is there any reason why setting an explicit version wouldn't be a good idea?;"[""> Is there any reason why setting an explicit version wouldn't be a good idea?The only real reason is that we would enforce other code bases using tfjs-image-recognition-base to use the same tfjs-core version as face-api.jsTo be honest, I do not understand, why your package manager does not install the tfjs-core version that is explicitly set in the package.json of face-api.js. Are you using yarn or npm?=====""]"
https://github.com/ntedgi/node-efficientnet/issues/59;Efficientnet trained with noisystudent ;4;open;2021-07-28T13:32:00Z;2021-07-30T06:29:10Z;**Is your feature request related to a problem? Please describe.**[NoisyStudent](https://github.com/google-research/noisystudent) is a training method that achieves better results.**Describe the solution you'd like**It would be cool if you could also offer NoisyStudent variants of EfficientNet ;"['thanks, @marcelklehr  for open this issue try follow this repo https://github.com/ntedgi/efficientnet-tensorflowjs-binariesit contains  node-efficientnet tensor flow model there is a simple example on how to transfer tf-modelafter you finish transfer you can load it here in the same model =====', 'Would you be willing to accept a PR that adds NoisyStudent checkpoints?=====', 'Definitely and if you need help I would be happy to help=====', 'Before you start modifying the code base try to load your model with this example ‘const path = require(""path""),const {  EfficientNetCheckPointFactory,  EfficientNetCheckPoint} = require(""node-efficientnet""),const model = await EfficientNetCheckPointFactory.create(  EfficientNetCheckPoint.B7,  {    localModelRootDirectory: path.join(__dirname, ""local_model"")  }),const path2image = ""..."",const topResults = 5,const result = await model.inference(path2image, {  topK: topResults,  locale: ""zh""}),’=====']"
https://github.com/ntedgi/node-efficientnet/issues/54;Landing page: add component with recent images that been predict;2;open;2021-04-10T17:09:51Z;2021-05-04T05:28:39Z;Server side save recent 3-5 images with there prediction.Expose end point to fetch recent predictions client side add component showing recent predictions ;['Hello, good day. Can I please work on this ?=====', 'go for it let me know if you need help =====']
https://github.com/ntedgi/node-efficientnet/issues/36;update playground ui add all available files format ;1;open;2021-03-28T08:29:44Z;2021-04-09T03:59:32Z;"**ui changes:**<img width=""397"" alt=""Screen Shot 2021-03-28 at 11 26 59"" src=""https://user-images.githubusercontent.com/31243793/112746621-9a468780-8fb8-11eb-95b3-938203caf965.png"">under here add a new line:""Supported file types : JPG, PNG ,GIF,SVG,HEIC,WEBP""add available files format to file uploader component ";['This task block by #35 =====']
https://github.com/ntedgi/node-efficientnet/issues/60;This is not how you deal with logits;2;closed;2021-08-05T17:40:08Z;2021-08-07T09:02:59Z;**Describe the bug**This algorithm produces the wrong outputs: https://github.com/ntedgi/node-efficientnet/blob/main/src/EfficientNetResult.ts#L26**To Reproduce**Steps to reproduce the behavior:1. Set topK = 32. Let code predict something3. All three labels are approximately 33%**Expected behavior**Get original model probabilities.**Screenshots**No.**Additional context**Google's mobilenet implementation illustrates how to deal with logits: https://github.com/tensorflow/tfjs-models/tree/master/mobilenet;"[""which EfficientNetCheckPoint did you use?can you attach your implementation  please ?or use `npm run example `and change `topK` and attchce outputthis is the output I am getting for the `samples` dir with topK=3`[  { label: 'honeycomb', precision: 39.57828750014812 },  { label: 'matchstick', precision: 30.304513760285996 },  { label: 'candle, taper, wax light', precision: 30.117198739565886 }][  {    label: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',    precision: 56.48649911640021  },  { label: 'French bulldog', precision: 24.4534070988847 },  { label: 'soccer ball', precision: 19.060093784715082 }][  { label: 'sports car, sport car', precision: 88.49501012956064 },  {    label: 'racer, race car, racing car',    precision: 7.0412236667543775  },  { label: 'car wheel', precision: 4.463766203684982 }] `====="", 'Ah, this may be due to me using Efficientnet v2 which churns out logits.=====']"
https://github.com/ntedgi/node-efficientnet/issues/27; Configurable download address;2;closed;2021-03-24T10:38:34Z;2021-03-25T15:21:06Z;**Is your feature request related to a problem? Please describe.**You know, downloading github assets in mainland China is particularly slow~**Describe the solution you'd like**Maybe, allow to specify local model dir? Such as:```jsEfficientNetCheckPointFactory.create(EfficientNetCheckPoint.B7, {  localModelRootDir: path.join(__dirname, 'tf-model'),}),```**Describe alternatives you've considered****Additional context**Add any other context or screenshots about the feature request here.;['should be enabled as part of #6  =====', '#28 =====']
https://github.com/ntedgi/node-efficientnet/issues/4;create an example classify image from stream on http server;1;closed;2021-01-10T06:10:20Z;2021-01-14T13:55:06Z;;"[""```const express = require('express'),const bodyParser = require('body-parser'),const multer = require('multer'),const port = process.env.PORT || 3000,const app = express(),const router = express.Router(),const upload = multer({    limits: {        fileSize: 4 * 1024 * 1024,    }}),router.get('/', async function (req, res) {    await res.render('index'),}),router.post('/upload', upload.single('image'), async function (req, res) {// use model here }),app.use(bodyParser.json()),app.use(bodyParser.urlencoded({extended: true})),app.use(express.static('public')),app.set('view engine', 'ejs'),app.listen(port, function () {    console.log('Server is running on PORT',port),}),```=====""]"
https://github.com/ntedgi/node-efficientnet/issues/3;create example of classify images on local dir ;2;closed;2021-01-10T06:09:43Z;2021-02-28T17:48:33Z;;"[""```jsconst {\tEfficientNetCheckPointFactory,\tEfficientNetCheckPoint,} = require('node-efficientnet'),const images = ['car.jpg', 'panda.jpg'],const imageDir = './samples',EfficientNetCheckPointFactory.create(EfficientNetCheckPoint.B2)\t.then((model) => {\t\timages.forEach((image) => {\t\t\tmodel.inference(`${imageDir}/${image}`).then((result) => {\t\t\t\tconsole.log(result.result),\t\t\t}),\t\t}),\t})\t.catch((e) => {\t\tconsole.error(e),\t}),```something like this?====="", 'I want to create a single page web-site for documentation i need to work on the boiler platting then i add all the examples thanks!=====']"
https://github.com/Volcomix/virtual-background/issues/30;Does not work on ipad;2;open;2021-10-27T12:42:20Z;2021-11-04T16:35:59Z;The applied on my website. However, it does not work on ipad. I have check your demo, it does not work on ipad too. Could you please help me to figure out the problem;"[""It would help if you manage to get any error from the JavaScript console, but I suspect that the issue is due to a version of Safari which doesn't support webgl2.You can find some discussions about this here: https://github.com/Volcomix/virtual-background/issues/23#issuecomment-881311562.You could handle this case by catching the error and fallback to the canvas pipeline.====="", ""Referencing a new comment I've into issue #23 (which is closed):https://github.com/Volcomix/virtual-background/issues/23#issuecomment-959753509=====""]"
https://github.com/Volcomix/virtual-background/issues/28;Installing / adding to project;1;open;2021-10-18T14:31:04Z;2021-10-20T07:45:02Z;First of all @Volcomix , thanks for this awesome lib and demo!My main questions are in terms of implementation - I've forked over the project and started integrating it with mine, as my end goal is to incorporate some of this BG removal functionality in my project. I haven't had any experience with docker before, so my question is if its required to set up a docker project with the TFLite models in order to get this to work?Also, in general - what would be the best way to integrate this into an existing project (vs cloning and running locally as a separate one).Thanks!;"['Hi @royherma and thank you!As of today there is no easy way to take a whole ""pipeline"", integrate it into an existing project and keep it in sync with this demo. At some point I was considering providing some features in an NPM package but I\'m not sure how to provide an API which:* is not too generic so that adding custom processing would be easy enough* is not too specific so that it fits more than a single use case and is customizable enough* doesn\'t overlap too much with other existing libraries (e.g. Mediapipe and TFJS which is now able to handle TFLite models in alpha)That being said, you could have different options depending on your needs:* You can copy the [public/tflite](https://github.com/Volcomix/virtual-background/tree/main/public/tflite) directory and consume the js files directly in your project. You can find some details about its interface in [src/core/hooks/useTFLite.ts](https://github.com/Volcomix/virtual-background/blob/main/src/core/hooks/useTFLite.ts). Some software/projects are already doing this and frequently update their copy of this directory from this repo. I\'m trying to keep it up to date against tensorflow repository.  Then for now, you need to implement/copy the pipelines and to maintain them manually.* You can take a look at [Mediapipe](https://google.github.io/mediapipe/solutions/selfie_segmentation#javascript-solution-api) which provides a simple API to consume the 256x144 of ML Kit model. I don\'t have much experience with Mediapipe but as far as I understand, it should be quite easy to create a processing pipeline with their tool and to integrate into your app.* You can try using the [TFLite API of TFJS](https://js.tensorflow.org/api_tflite/0.0.1-alpha.4/) directly with GMeet model. You\'ll still also need to implement the post-processing by yourself.I could be willing to reconsider providing an NPM package but I\'ll need some help to define what should go inside and the API.=====']"
https://github.com/Volcomix/virtual-background/issues/27;Any way to do this via webworkers ? ;2;open;2021-10-07T17:43:28Z;2021-10-12T10:18:04Z;Lately, I have been experimenting with putting this pipeline on a web-worker to make the experience better.I have been struggling to send the `tflite` model to the `worker.ts` file. Sending the canvas to the `worker.ts` was troublesome too but handled it using offscreen canvas. I also believe putting it on web-worker would fix the issue of Video pausing when browser is in the background #25 The pipeline I have been trying this with is `canvas2d` using the meet model ;['I am digging to this solution too. =====', '@abhisht51  Were you able to switch the canvas2d into the worker yet ?, I am having troubles in converting it. Can you please help ?!=====']
https://github.com/Volcomix/virtual-background/issues/25;Video pausing when browser is in the background;4;open;2021-07-21T17:07:42Z;2021-11-01T10:37:36Z;I have implemented this my app, but when the browser/tab goes into the background, the video is pausing. How I can overcome this? Thanks.;"['The video is not supposed to be paused but this might be symptomatic of rendering timers being throttled in background tabs, which could introduce a lag effect (https://developers.google.com/web/updates/2017/03/background_tabs#background_timer_alignment).As a workaround, you could try to play an almost inaudible sound with Web Audio API.=====', ""I can confirm that playing nearly silent audio is currently the best option--it prevents throttling (unless you're using requestAnimationFrame).Unfortunately, this only works in Chromium browsers (not in Firefox or Safari). As a result, I typically only offer canvas-based rendering in Chromium browsers. ====="", 'Quite a bummer. The workaround is not working for me yet. Also was hoping to get this working in Safari.=====', 'I solved the problem by using setTimeout instead of requestAnimationFrame.=====']"
https://github.com/Volcomix/virtual-background/issues/24;question about light wrapping;1;open;2021-07-19T07:53:54Z;2021-07-25T09:16:01Z;"Hi @Volcomix , thx for this nice work.I'm interested in the light wrapping function of the post process pipeline and try to rewrite the following function it in python.https://github.com/Volcomix/virtual-background/blob/8530c56ce419618a3679ca379e76bfd1518a35f5/src/pipelines/webgl2/backgroundImageStage.ts#L68-L78however, the output result is different with the output of your original code...I paste my python implementation here```def screen(a, b):    return 1.0 - (1.0 - a) * (1.0 - b)def linear_dodge(a, b):    return a + bdef clamp(x, lowerlimit, upperlimit):    if x < lowerlimit:        x = lowerlimit    if x > upperlimit:        x = upperlimit    return xdef smooth_step(edge0, edge1, x):    # Scale, bias and saturate x to 0..1 range    x = clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0)    # Evaluate polynomial    return x * x * (3 - 2 * x)def light_wrapping(segmentation_mask, image, background, cov_x=0.6, cov_y=0.8, wrap_cof=0.3, blend_mode='linear'):    light_wrap_mask = 1. - np.maximum(0, (segmentation_mask - cov_y))/(1 - cov_y)    light_wrap = wrap_cof * light_wrap_mask[:, :, np.newaxis] * background    if blend_mode == 'linear':        frame_color = linear_dodge(image, light_wrap)    elif blend_mode == 'screen':        frame_color = screen(image, light_wrap)    smooth = lambda i: smooth_step(cov_x, cov_y, i)    vectorized_smooth = np.vectorize(smooth)    person_mask = vectorized_smooth(segmentation_mask)    return person_mask, frame_colordef combind_frond_back_ground(person_mask, image, background):    person_mask = person_mask[:, :, np.newaxis]    output_image = image * mix_value + background * (1.0 - mix_value)    output_image = output_image.astype(np.uint8)    return output_imagedef main():    image_src = ""path to input image""    image = cv2.imread(image_src)   # (720, 1280, 3)    background_src = ""path to background image""    background = cv2.imread(background_src)  # assuming image and background have the same size (720, 1280, 3)    segmentation_mask = ""segmentation result from model""  # (720, 1280)    person_mask, frame_color = light_wrapping(segmentation_mask, image, background)    output_frame = combind_frond_back_ground(person_mask, frame_color, background)    return output_frame```I feel something might be wrong in smooth_step, but I cannot find more information except for this [link](https://en.wikipedia.org/wiki/Smoothstep). Could you help me check if there is an obvious error in this python code please?Thx in advance~";"[""Hi @carter54, I don't see anything obvious by reading the code. `smooth_step` also looks fine as it seems consistent with [OpenGL documentation](https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/smoothstep.xhtml).It might help if you could post the resulting output and maybe some intermediate outputs like the masks, the output before smooth_step and few others.=====""]"
https://github.com/Volcomix/virtual-background/issues/21;New landscape selfie segmentation model;8;open;2021-06-11T11:23:43Z;2021-07-05T16:19:49Z;Google released a new landscape model of the selfie segmentation:https://google.github.io/mediapipe/solutions/selfie_segmentationhttps://github.com/google/mediapipe/tree/master/mediapipe/modules/selfie_segmentationHow does this compares to the mlkit model in this project?Does selfie_segmentation.js use WebAssembly+SIMD or WebGL2?Should we use @mediapipe/selfie_segmentation/selfie_segmentation.js or is the model in this project more performant?;"[""Thank you for the excellent repo! Your post-processing pipeline is spectacular!I've also been interested in using the MediaPipe sefie segmentation model. I forked this project and tried the following1. Force this repo to use the [mediapipe landscape model](https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation@0.1/selfie_segmentation_landscape.tflite)2.  Update `SegmentationConfig.inputResolution` as well as `InputResolution`--I tried both `144x256` (as noted in [the docs](https://google.github.io/mediapipe/solutions/selfie_segmentation#models)) and `256x144` just to be sureIn all cases, the output video feed is blurry.![Screen Shot 2021-06-28 at 11 40 47 AM](https://user-images.githubusercontent.com/5660076/123680239-b4854100-d805-11eb-9767-128e3e14d365.png)When I saw that the output image is completely blurred, I wanted to check that the generated segmentation mask contained non-zero values. After checking `tflite.HEAPF32` just before it gets passed to `gl.texSubImage2D` in `loadSegmentationStage::render`, I was able to confirm that the provided segmentation mask does include non-zero values.With this in mind, I'm uncertain what else needs to be done get this new model working correctly within this repo. Any input is appreciated.Thank you!====="", ""Maybe we shouldn't resize the input when using this model?The selfie_segmentation model docs says:> Segmentation automatically resizes the input image to the desired tensor dimension before feeding it into the ML models.Can you please share your code?====="", ""Unfortunately, using the camera's native resolution as `inputResolution` (in my case 640x480) is not working.[Here's the code](https://github.com/jpodwys/virtual-background/tree/media-pipe) (note it's in the `media-pipe` branch of my fork). My fork strips out react and all non-background blur code.The files you'll be interested in are* `TFLite.ts`: Imports the MediaPipe landscape model* `blur.ts`: Sets `inputResolution`====="", ""I also tried replacing part of the pipeline with part of Media Pipe's [sample segmentation app](https://google.github.io/mediapipe/solutions/selfie_segmentation#javascript-solution-api) just so I could get the segmentation mask directly from their code. I then fed the generated mask into the pipeline in this repo, but I didn't have any luck there either.====="", ""Hi @jpodwys, thank you for experimenting with this model. Just an intuition without investigating the model file, have you tried replacing [this line](https://github.com/jpodwys/virtual-background/blob/b406cf948eb1ba42b6be9a72668b873112ea9655/src/pipelines/webgl2/webgl2Pipeline.ts#L101) by calling `buildLoadSegmentationStage` instead of `buildSoftmaxStage`? I'm wondering if the softmax is part of the model already.====="", ""Thank you for the suggestion, you were right!So the following works1. Swap to MediaPipe's model (I'm using [the landscape model](https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation@0.1/selfie_segmentation_landscape.tflite))2. Change `inputResolution` to `256x144` (**not** `144x256` as noted in the docs)3. Replace `webgl2Pipeline`'s call to `buildSoftmaxStage` with `buildLoadSegmentationStage`![Screen Shot 2021-06-29 at 10 39 10 AM](https://user-images.githubusercontent.com/5660076/123835836-405ea200-d8c6-11eb-8052-19597e449b7a.png)The face of a happy dev :)====="", 'Hi @jpodwys, did you check how does the landscape model performance differ from ML Kit?=====', ""My fork uses the landscape MediaPipe model so you can compare this repo's [live demo](https://volcomix.github.io/virtual-background) to my fork's [live demo](https://jpodwys.github.io/virtual-background/).I also made a [live demo](https://jpodwys.github.io/media-pipe/) that skips the post-processing pipeline and blurs directly via canvas. It has better performance (fps) but less impressive post-processing (there's a halo effect around humans). That said, the canvas-only blur implementation looks surprisingly good in this demo because, as Volcomix pointed out, the MediaPipe team built softmax directly into their model. =====""]"
https://github.com/Volcomix/virtual-background/issues/20;Error: Could not compile shader: ERROR: 0:3: '#' : invalid character ERROR: 0:3: '?' : syntax error;1;open;2021-06-09T03:30:01Z;2021-06-12T10:42:27Z;After upgrade chrome to version 91.0.4472.77, I got an compile error. How to fix this error?  thanks! Error: Could not compile shader: ERROR: 0:3: '#' : invalid character ERROR: 0:3: '?' : syntax error;"[""Please could you share the report from `chrome://gpu`?I'm running chrome v91.0.4472.101 without issue so maybe could you also try to upgrade it just in case.=====""]"
https://github.com/Volcomix/virtual-background/issues/13;Better performance in bigger video Width ;3;open;2021-05-27T15:30:54Z;2021-11-30T12:42:36Z;Very useful integration. Congrats on your implementation. My question is: Is there a way to increase the performance in bigger video size. I saw that right now it uses 640px video size. When you resize the video to 1980px the performance drops drastically. So is there a way to bypass this issue . Thanks in advance.;['Yeah, even the 1280×720 video of this demo get a performance drop when resizing through WebGL. I tried to mitigate this by downloading the pixels asynchronously after resizing on GPU but uploading video textures to GPU with WebGL is also pretty slow. Maybe there is a way to improve it by mixing 2D Canvas for resizing and WebGL for post-processing only.=====', '[Commercial sdk](https://medium.com/vectorly/building-a-more-efficient-background-segmentation-model-than-google-74ecd17392d5)  that improves the performance. Not sure how hard to implement.=====', 'Can this improve performance for large videos with the WebGL2 pipeline?1. Add a new sourceCanvas with the size of the mask.2. Resize by copying from sourcePlayback.html to sourceCanvas with a 2d context.3. Use sourceCanvas instead of sourcePlayback.html when calling gl.texImage2D here:4. https://github.com/Volcomix/virtual-background/blob/main/src/pipelines/webgl2/webgl2Pipeline.ts#L170=====']
https://github.com/Volcomix/virtual-background/issues/26;Demo not running on Safari 14.1.12;2;closed;2021-10-07T14:41:19Z;2021-10-12T08:54:02Z;Page starts to load UI then blanks.Error in the console is: TypeError: null is not an object (evaluating 'm.VERTEX_SHADER');"['Hi @slpn1,Indeed this demo relies on WebGL 2.0 which is supported on Safari 14.1 only after enabling it in the ""Experimental Features"" developer menu.It is supported by default starting from Safari 15 (https://caniuse.com/webgl2).=====', 'Thank you, and amazing work on this project!=====']"
https://github.com/Volcomix/virtual-background/issues/23;Working on Safari Browser;5;closed;2021-07-15T12:42:06Z;2021-11-03T17:45:47Z;"@Volcomix Thank you for nice work . I have a question ""Whether this web app is supported on the Safari Browser?"" . I have tested on the safari browser but it is not working . Thank you ";"['Hey @NaeemKhan333, thanks for asking 😄 This app relies on `webgl2` which needs to be enabled in Safari ""Experimental Features"" developer menu starting from v10.1 (https://caniuse.com/webgl2). SIMD is not supported though.=====', '@Volcomix Thank you for reply , if I change webgl2 to webgl will it support the safari browser or not.=====', ""The GLSL shaders are not compatible with webgl so it won't work. Maybe could you just fallback to the canvas implementation on safari rather than loading the webgl2 pipeline by default: https://github.com/Volcomix/virtual-background/blob/8530c56ce419618a3679ca379e76bfd1518a35f5/src/App.tsx#L35We could detect this case the same way we detect unsupported SIMD, by trying to create a webgl2 context and by disabling the webl2 options if it is not available (plus fallback to canvas).====="", 'Thank you =====', ""Did anyone succeed in running the playground (https://volcomix.github.io/virtual-background) or the package in general on iPhone iOS 15.1 / 14.8 / 14.7/ etc?What is the recommended implementation for cases where WEBGL2 isn't available?> The GLSL shaders are not compatible with webgl so it won't work. Maybe could you just fallback to the canvas implementation on safari rather than loading the webgl2 pipeline by default:> > https://github.com/Volcomix/virtual-background/blob/8530c56ce419618a3679ca379e76bfd1518a35f5/src/App.tsx#L35> > We could detect this case the same way we detect unsupported SIMD, by trying to create a webgl2 context and by disabling the webl2 options if it is not available (plus fallback to canvas).=====""]"
https://github.com/Volcomix/virtual-background/issues/22;TFLite WASM build failing after XNNPACK update;2;closed;2021-07-14T10:24:10Z;2021-08-14T11:34:40Z;"[XNNPACK has recently been updated](https://github.com/google/XNNPACK/commit/ee029b28ad3cf2908f869d789de396f6a83a68d9) with the latest versions of `wasm_simd128.h` intrinsics.As those intrinsics are not yet included in [the version of LLVM used by emsdk](https://github.com/llvm/llvm-project/blob/llvmorg-12.0.0/clang/lib/Headers/wasm_simd128.h), TFLite WASM build is failing.tfjs doesn't have this issue yet when building the wasm backend because it relies on an [old version of XNNPACK](https://github.com/tensorflow/tfjs/blob/9f0d88a4b7db807a9074720d865ef8d95efc5a2f/WORKSPACE#L96) but I guess it should face the same issue as soon as XNNPACK version will be updated.Here are some details about [the error we have](https://github.com/Volcomix/virtual-background/runs/3063327177):```[177 / 1,157] Compiling XNNPACK/src/f32-avgpool/9p8x-minmax-wasm-c1.c, 2s processwrapper-sandbox ... (2 actions running)ERROR: /github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/external/XNNPACK/BUILD.bazel:4672:19: C++ compilation of rule '@XNNPACK//:wasm_ukernels' failed (Exit 1): emcc.sh failed: error executing command external/emsdk/emscripten_toolchain/emcc.sh '--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot' -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG ... (remaining 66 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandbox emcc.sh failed: error executing command external/emsdk/emscripten_toolchain/emcc.sh '--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot' -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG ... (remaining 66 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandboxexternal/XNNPACK/src/x32-pad/wasmsimd.c:32:24: error: implicit declaration of function 'wasm_v128_load32_splat' is invalid in C99 [-Werror,-Wimplicit-function-declaration]  const v128_t vfill = wasm_v128_load32_splat(fill_value),                       ^external/XNNPACK/src/x32-pad/wasmsimd.c:32:24: note: did you mean 'wasm_v16x8_load_splat'?external/emscripten_bin_linux/lib/clang/13.0.0/include/wasm_simd128.h:70:1: note: 'wasm_v16x8_load_splat' declared herewasm_v16x8_load_splat(const void *__mem) {^external/XNNPACK/src/x32-pad/wasmsimd.c:32:16: error: initializing 'const v128_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'  const v128_t vfill = wasm_v128_load32_splat(fill_value),               ^       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~2 errors generated.emcc: error: '/github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/sandbox/processwrapper-sandbox/1173/execroot/__main__/external/emscripten_bin_linux/bin/clang -DEMSCRIPTEN -fignore-exceptions -mllvm -combiner-global-alias-analysis=false -mllvm -enable-emscripten-sjlj -mllvm -disable-lsr -Xclang -iwithsysroot/include/SDL -target wasm32-unknown-emscripten -D__EMSCRIPTEN_major__=2 -D__EMSCRIPTEN_minor__=0 -D__EMSCRIPTEN_tiny__=14 -D_LIBCPP_ABI_VERSION=2 -Dunix -D__unix -D__unix__ -flegacy-pass-manager -Werror=implicit-function-declaration --sysroot=/github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/external/emscripten_bin_linux/emscripten/cache/sysroot -Xclang -iwithsysroot/include/compat --sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG -fomit-frame-pointer -O3 -Wall -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/wasm-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/wasm-opt/bin/external/FP16 -iquote external/FXdiv -iquote bazel-out/wasm-opt/bin/external/FXdiv -iquote external/pthreadpool -iquote bazel-out/wasm-opt/bin/external/pthreadpool -Ibazel-out/wasm-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/wasm-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/wasm-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -isystem external/XNNPACK/include -isystem bazel-out/wasm-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/wasm-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/wasm-opt/bin/external/FP16/include -isystem external/FXdiv/include -isystem bazel-out/wasm-opt/bin/external/FXdiv/include -isystem external/pthreadpool/include -isystem bazel-out/wasm-opt/bin/external/pthreadpool/include -Wno-error=unused-function -msimd128 -Iinclude -Isrc -std=c99 -O2 -iwithsysroot/include/c++/v1 -iwithsysroot/include/compat -iwithsysroot/include -isystem external/emscripten_bin_linux/lib/clang/13.0.0/include -MD -MF bazel-out/wasm-opt/bin/external/XNNPACK/_objs/wasm_ukernels/1/wasmsimd.d -c -Wno-builtin-macro-redefined -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" -Werror external/XNNPACK/src/x32-pad/wasmsimd.c -o bazel-out/wasm-opt/bin/external/XNNPACK/_objs/wasm_ukernels/1/wasmsimd.o' failed (1)Target //:tflite-simd failed to buildUse --verbose_failures to see the command lines of failed build steps.INFO: Elapsed time: 59.927s, Critical Path: 8.59sINFO: 52 processes: 6 internal, 46 processwrapper-sandbox.FAILED: Build did NOT complete successfullyFAILED: Build did NOT complete successfully```Issue created in XNNPACK repository: https://github.com/google/XNNPACK/issues/1630";['It should definitely build under emsdk 2.0.25 as 2.0.25 comes with llvm 13 which has the necessary intrinsics.I used CMake instead of bazel and provided emscripten through other means because that works better in our case, and it built just fine.The fact that you linked llvm 12 above means that a pretty old emsdk is used for some reason. Emscripten follows tip of the tree llvm.=====', 'Indeed I followed the instructions without changing anything instead of specifying a more recent archive of emsdk. The bazel scripts only load the latest SDK prior to the specified archive.Fixed in this commit: https://github.com/Volcomix/virtual-background/commit/6bffb3e53d0614d6e9afc4e1430767654dc689f0Thank you for your help.=====']
https://github.com/Volcomix/virtual-background/issues/19;Change blur intensity in the WebGL2 pipeline;4;closed;2021-06-01T06:24:07Z;2021-06-12T10:43:01Z;How can I decrease the blur intensity in the WebGL2 pipeline? Even manually by changing the code.Non of the controls in the demo seems to affect it.What does the coverage parameter do? It seems the only parameter passed to BackgroundBlurStage but I don't see any effect in the result when changing it.;"['There is currently no control to change the blur intensity. You can change it by editing any (or both) of these lines:* [Number of blur passes](https://github.com/Volcomix/virtual-background/blob/021e3d8701480648d638e9074d2f77ae362cb6d9/src/pipelines/webgl2/backgroundBlurStage.ts#L166)* [Input image scale for linear sampling](https://github.com/Volcomix/virtual-background/blob/021e3d8701480648d638e9074d2f77ae362cb6d9/src/pipelines/webgl2/backgroundBlurStage.ts#L98)The coverage parameter allows us to control the transition between the background and the foreground by changing the left edge and right edge of the [smoothstep function](https://en.wikipedia.org/wiki/Smoothstep).=====', ""Thank you for the explanation.Do you use separable filters for the weighted blur?Do you change the weight based on the circle-of-confusion (CoC) radii like in the [Google Meet blog](https://ai.googleblog.com/2020/10/background-features-in-google-meet.html)?I'm new to shaders and the algorithms and trying keep track.====="", ""I do use separable filter. The vertical pass is [here](https://github.com/Volcomix/virtual-background/blob/021e3d8701480648d638e9074d2f77ae362cb6d9/src/pipelines/webgl2/backgroundBlurStage.ts#L169) and horizontal pass [here](https://github.com/Volcomix/virtual-background/blob/021e3d8701480648d638e9074d2f77ae362cb6d9/src/pipelines/webgl2/backgroundBlurStage.ts#L177). You can find some explanation in [this article](https://software.intel.com/content/www/us/en/develop/blogs/an-investigation-of-fast-real-time-gpu-based-image-blur-algorithms.html).I'm changing the weight based on the segmentation values (you can find an example [here](https://github.com/Volcomix/virtual-background/blob/021e3d8701480648d638e9074d2f77ae362cb6d9/src/pipelines/webgl2/backgroundBlurStage.ts#L88)). This avoids the person pixels to be blurred along with the background ones but I didn't really understand if this is what Google described in their blog when mentioning the CoC or if we could apply an additional effect :shrug:.====="", 'Thanks=====']"
https://github.com/Volcomix/virtual-background/issues/18;Check if simd is supported;2;closed;2021-05-30T16:40:47Z;2021-05-30T17:08:18Z;A function that check if simd is supported. Based on [wasm-check](https://github.com/MaxGraey/wasm-check/blob/master/lib/index.js).Can be used [here](https://github.com/Volcomix/virtual-background/blob/main/src/core/hooks/useTFLite.ts#L31) instead of try/catch.```javascriptprivate simdSupported(): boolean {  const u32 = (...bytes: number[]) => Uint32Array.of(0x6D736100, 1, ...bytes),  const u32a = (...bytes: number[]) => u32(1610679297, 33751040, ...bytes, 40239360, 259),  let wasm = u32a(84344833, 6357249, 17369600, 4259847, 186257917, 1845758464),  const buffer = wasm.buffer,  if (WebAssembly.validate(buffer)) {    try {      new WebAssembly.Instance(new WebAssembly.Module(buffer)).exports['0'](),      return true,    }    catch (_a) {      return false,    }  }  return false,}```;"['Thanks for pointing this out. There is also the one from [GoogleChromeLabs](https://github.com/GoogleChromeLabs/wasm-feature-detect) which looks great.[TF.js](https://github.com/tensorflow/tfjs/blob/e642f856dd289088610378d5042f19dc54c05601/tfjs-backend-wasm/src/flags_wasm.ts#L30) is using a copy of it:```tsWebAssembly.validate(new Uint8Array([  0, 97, 115, 109, 1, 0, 0, 0, 1,  4, 1,   96, 0,  0, 3,  2, 1,  0,   10,  9, 1, 7, 0, 65, 0, 253, 15, 26, 11]))```=====', ""This looks cleaner. I'll update my code. Thanks.=====""]"
https://github.com/Volcomix/virtual-background/issues/16;Wrong path for tflite.wasm;12;closed;2021-05-30T05:09:42Z;2021-06-12T11:14:45Z;"I'm trying to add the virtual background to an angular 11 project and getting an error when trying to load the tflite model:```GET http://localhost:4200/tflite.wasm 404 (Not Found)tflite.js:9 wasm streaming compile failed: TypeError: Failed to execute 'compile' on 'WebAssembly': HTTP status code is not oktflite.js:9 falling back to ArrayBuffer instantiationGET http://localhost:4200/tflite.wasm 404 (Not Found)failed to asynchronously prepare wasm: RuntimeError: abort(both async and sync fetching of the wasm failed). Build with -s ASSERTIONS=1 for more info.```This is how I import tflite.js:```javascriptimport createTFLiteModule from './tflite/tflite',import createTFLiteSIMDModule from './tflite/tflite-simd',```The path to tflite.wasm is hardcoded in tflite/tflite.js. Is there a config option to set the path without modifying tflite.js?```javascriptvar wasmBinaryFile=""tflite.wasm""```";"[""I hope there is a way to import wasm module using ES modules like you did but I haven't found the right way to do it. So what I did in this demo was to put `tflite.js` and `tflite.wasm` in the public directory (aka `assets` in Angular), manually added the required `<script>` tags in [`index.html`](https://github.com/Volcomix/virtual-background/blob/c367b96ba0fc2213a9ff262dde06e352754b5288/public/index.html#L46) and manually declared [`createTFLiteModule`](https://github.com/Volcomix/virtual-background/blob/c367b96ba0fc2213a9ff262dde06e352754b5288/src/core/hooks/useTFLite.ts#L7) for TypeScript to be happy when calling it without any associated `import`.====="", ""I didn't find a way to set the path yet. Maybe we can add a global variable like TFLITE_URL and TFLITE_SIMD_URL that the js file will use if exists. Not sure if relevant.====="", ""Actually `tflite.js` and `tflite.wasm` are generated by [emscripten](https://emscripten.org/) and the glue code in the js file is overwritten each time I build tflite in wasm. I don't think that would be a good idea to edit the file manually so unless there is an existing option in emscripten to specify the wasm file URL (which I can't find), I don't see any other alternative than importing those files from outside of the bundled code.====="", 'Relative path should work when loading tflite.js in a script tag because the glue code use [document.currentScript.url](https://developer.mozilla.org/en-US/docs/Web/API/Document/currentScript). See https://github.com/emscripten-core/emscripten/pull/5368```var _scriptDir = typeof document !== \'undefined\' && document.currentScript ? document.currentScript.src : undefined,```Angular dev server generate the html dynamically with <script src=""app.js""></script>. That\'s why document.currentScript.url points to the root path.I think on dev we can add a proxy that handles the wasm files specifically. Maybe with ""ng serve --proxy-config proxy.conf.json"".On production, as long as the .js and wasm files will be in the same path it should work because of document.currentScript.url.=====', 'Maybe we need```-s EXPORT_ES6=1 will turn the JavaScript code into an ES6 module with a default export that works with any bundler. Also requires -s MODULARIZE=1 to be set.```=====', 'Running a build with this option (https://github.com/Volcomix/virtual-background/commit/cdad50c0f67c3537854c1b9ea6b8bef498b55fcc): https://github.com/Volcomix/virtual-background/actions/runs/890576517=====', ""There is also [USE_ES6_IMPORT_META=1](https://github.com/emscripten-core/emscripten/blob/main/src/settings.js#L1166):// Use the ES6 Module relative import feature 'import.meta.url'// to auto-detect WASM Module path.// It might not be supported on old browsers / toolchainsvar USE_ES6_IMPORT_META = 1,====="", ""I updated tflite js files in [this branch](https://github.com/Volcomix/virtual-background/tree/emscripten-es6-export). React is yelling at me when trying to import them as ES6 modules:![image](https://user-images.githubusercontent.com/7324857/120114117-ead78e00-c17d-11eb-9163-4db3e29a0313.png)But maybe could you have more luck with Angular.USE_ES6_IMPORT_META is enabled by default so I'm gonna run a new build with this option disabled.====="", ""I'm getting a similar error with Angular 11. EXPORT_ES6=1 with import.model.url is probably the solution but react and angular tooling are not compatible with import.model yet.====="", ""I still prefer the automatic import.model.url solution but we can also pass custom [locateFile function](https://emscripten.org/docs/api_reference/module.html#Module.locateFile):```javascriptconst createdTFLiteSIMD = await createTFLiteModule({locateFile: () => 'path/to//tflite.wasm'}),```====="", 'Still an error with `""-s USE_ES6_IMPORT_META=0""`:![image](https://user-images.githubusercontent.com/7324857/121773236-ce265780-cb7a-11eb-813e-1f61045b2b27.png)=====', 'Not sure about complication flags but we can use locateFile  to dynamically specify the model location.This is from the [Selfie Segmentation example](https://google.github.io/mediapipe/solutions/selfie_segmentation#javascript-solution-api):```javascriptconst selfieSegmentation = new SelfieSegmentation({locateFile: (file) => {  return `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}`,}}),```=====']"
https://github.com/Volcomix/virtual-background/issues/15;Can't resolve 'fs' and 'path';4;closed;2021-05-30T05:04:33Z;2021-05-30T14:38:48Z;"I'm trying to add the virtual background to an angular 11 app. When calling 'ng serve' I'm getting the following errors:```bashError: ./src/tflite/tflite.jsModule not found: Error: Can't resolve 'fs' in 'src/tflite'Error: ./src/tflite/tflite.jsModule not found: Error: Can't resolve 'path' in 'src/tflite'```Adding this to package.json resolve the issue. Is there a way to fix the errors without changing package.json or other config files?```json""browser"": {    ""fs"": false,    ""path"": false},";"[""I think you could try to set a specific [target environment](https://github.com/emscripten-core/emscripten/blob/087ca39beeb6203ac838925f4a3e9c67623fb2fc/src/settings.js#L626) in tflite [`BUILD`](https://github.com/Volcomix/virtual-background/blob/main/tflite/BUILD) file:```-s ENVIRONMENT='web'```====="", ""I'll try to build the model with the ENVIRONMENT. Shouldn't it be the default from this project?====="", 'Could be the default indeed, it would probably reduce the js file size.=====', 'Works great. Thanks=====']"
https://github.com/Volcomix/virtual-background/issues/14;Green screen integration ;1;closed;2021-05-27T15:32:13Z;2021-05-30T07:41:16Z;Is there in plan to add a green screen integration with webgl ?Thank you.;"[""I'm not sure what would be the goal of handling green screens. The purpose of this demo is to show that we can replace the background without relying on physical backdrop. So basically if you have a green screen, it should work with this demo out of the box.If what you have in mind is rather to replace the background with green pixels, this could be done easily by using a green background image or by replacing the background replacement shader with one that just put green pixels instead. I don't plan to add this feature in the demo as I don't see any use case where putting green pixels would be the real end goal.=====""]"
https://github.com/Volcomix/virtual-background/issues/12;Regression: SIMD support;4;closed;2021-05-20T19:12:51Z;2021-05-20T21:51:27Z;Thanks for sharing this amazing project!I just noticed that SIMD support seems to be broken now. See this `console.warn` messages![image](https://user-images.githubusercontent.com/4052045/119035571-8e49c700-b964-11eb-847d-e645bee175e6.png)Not sure if this is related to recent commits updating the tflite version numbers?;"[""oh, is this a flag that needs to be enabled on the browser side? The SIMD option was available for me on your demo site yesterday, but disappeared today (I didn't touch any flags). Very weird...EDIT: I just turned on the chrome devflag and SIMD worked for me again. Leaving this open in case you have some insights into the puzzle. Otherwise, feel free to close.====="", 'Hi @jiangts, thanks a lot for reporting this issue. Indeed Chrome origin trial expired yesterday (May the 19th). I renewed the origin trial in ebf239f55ede070a55b76b86dd667b518dccf448 to fix the issue. SIMD should be back!=====', ""Wow, you are amazing! Very impressed that you were able to compile tflite to wasm and load these models. I got a bit into emscripten compilation learning from https://github.com/ffmpegwasm/ffmpeg.wasm and I've learned a bit more from this repo (not so familiar with `bazel` yet though).====="", ""Thanks a lot, I'm glad that you found this project useful!Learning bazel is not very smooth and I also struggled a little bit in the beginning. Building tflite was also tricky, especially because some key configurations are kind of hidden and are kept outside of tensorflow repos. Happy to share them!=====""]"
https://github.com/Volcomix/virtual-background/issues/11;BodyPix tflite model support;12;closed;2021-05-18T22:48:51Z;2021-05-30T09:03:00Z;@Volcomix , your work is really impressive! Thank you so much for this work.I saw this comment from @PINTO0309 in this issue https://github.com/Volcomix/virtual-background/issues/2#issuecomment-829730723> I don't know if it will be useful for you, but I have converted and quantized it for various frameworks and committed it to my repository.> > **TFLite Float32/Float16/INT8, TFJS, TF-TRT, ONNX, CoreML, OpenVINO IR FP32/FP16, Myriad Inference Blob**> > https://github.com/PINTO0309/PINTO_model_zoo> https://github.com/PINTO0309/PINTO_model_zoo/tree/main/109_Selfie_SegmentationAnd I'm able to easily play with the different models. Thank you both for your hardwork.I noticed @PINTO0309 also has bodypix tflite models [here](https://github.com/PINTO0309/PINTO_model_zoo/tree/main/035_BodyPix) but they don't seem to work with @Volcomix pipeline. Getting some logs I noticed the output image has the following:```jsconsole.log({  inputHeight: this._tflite._getInputHeight(),  inputWidth: this._tflite._getInputWidth(),  inputChannelCount: this._tflite._getInputChannelCount(),  outputHeight: this._tflite._getOutputHeight(),  outputWidth: this._tflite._getOutputWidth(),  outputChannelCount: this._tflite._getOutputChannelCount(),}),// Outputs an image with 10x8 resolution, 17 channels```As you can see, the output seems wrong. Any advice on what to adjust? My intention is I want to run bodypix model in wasm hoping to gain more performance than the tfjs one. Is this something you can help with? Thank you in advance!;"[""Hey @charliesantos, thanks for reaching out. Indeed @PINTO0309 is doing an awesome work on these model conversions!BodyPix models won't work with the pipelines of this demo without some tweaks. Especially because of the output which need to be decoded:* https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/single_person/decode_single_pose.ts* https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/single_person/util.tsNot sure how easy it would be, I'll try to investigate a little bit.====="", 'Thank you @Volcomix . It would be nice to at least use bodypix in wasm for the virtual background use case like the ones you have in this repo for google meet, ML Kit, and bodypix tfjs. No need to detect different poses.=====', ""I [created a branch](https://github.com/Volcomix/virtual-background/tree/bodypix-tflite) to experiment on BodyPix tflite models. I understand that the 4th output is supposed to hold the segmentation but I don't really understand how to work with its 20x15 resolution, even after looking into TF.js source code.Any hint from @PINTO0309 maybe? Did you get any working example in Python with the float16 quantized model?![image](https://user-images.githubusercontent.com/7324857/119266735-3a511380-bbec-11eb-9d5a-cd78de508ba8.png)The inference time is good but I guess this one is for the worst accuracy. Not sure how promising it is. The output is unusable for now:![image](https://user-images.githubusercontent.com/7324857/119266815-88fead80-bbec-11eb-8ed6-a9d8365b5a46.png)I'm not sure I will invest a lot more time on it so any help would be welcome.====="", 'I have not written any sample code, but the following repository may be helpful.https://github.com/hegman12/body_pix_tfliteThe last time I converted the model was a long time ago (about a year ago), so I will try to convert it again with the latest knowledge.=====', ""Awesome! Thanks for the update I'm gonna take a look.====="", '@Volcomix The conversion took only five minutes. But what kind of resolution model do you want? I can change it as much as I want. The figure below shows the MobileNetV2 240x320 resolution model. **`[n, h, w, c] = [1, 240, 320, 3]`**![Screenshot 2021-05-24 11:39:32](https://user-images.githubusercontent.com/33194443/119289300-4b019800-bc85-11eb-8450-d21ea0747dcf.png)I think we need to scale the height and width of the output according to the width of the stride.![Screenshot 2021-05-24 11:46:50](https://user-images.githubusercontent.com/33194443/119289598-de3acd80-bc85-11eb-8883-e65fcc6b0135.png)![Screenshot 2021-05-24 11:47:03](https://user-images.githubusercontent.com/33194443/119289608-e09d2780-bc85-11eb-97ed-fdda40870a5c.png)=====', 'All the models were re-transformed and totally replaced with the same model structure of TensorFlow.js.**https://github.com/PINTO0309/PINTO_model_zoo/tree/main/035_BodyPix**=====', 'Thank you @PINTO0309 @Volcomix you both are amazing!=====', 'I found a better sample.**https://github.com/google-coral/project-bodypix**![segmentation](https://user-images.githubusercontent.com/33194443/119413219-2c081200-bd28-11eb-87f8-8d6a43aa8635.gif)=====', 'Here is the one that was easiest to understand.**https://github.com/de-code/python-tf-bodypix**=====', ""Thank you so much @PINTO0309. I can see that the new model output still has a resolution of 15x20 so I need to figure out how to map this on a 240x320 image. Hopefully I'll find the answer in the samples you found for us 🤞.I'm gonna be pretty busy during the week so please don't worry if I answer with a little bit of delay.====="", ""After loading the new tflite models and trying with the fastest one (mobilenet050, stride16, 240x320, float16 quantization), the performance seems equivalent/worse with tflite on wasm than with tfjs on webgl. Moreover the tfjs one works on a higher resolution:![image](https://user-images.githubusercontent.com/7324857/120098221-e3d85d80-c134-11eb-81b1-8715d147463e.png)![image](https://user-images.githubusercontent.com/7324857/120098238-f9e61e00-c134-11eb-997a-9983d0b3b419.png)After checking other projects code and the tfjs implementation of BodyPix more closely, I would have to handle padding and scaling implied by the strides which is implemented using tensorflow in all the reference projects but would require a lot of extra work in this demo. As the performance results are not that good with the tflite model of BodyPix, I don't wish to invest more time on this specific experiment.I'm very sorry PINTO for making you spend time on it without ending.P.S.: There is a work in progress in TF.js to handle tflite models. Maybe is there a chance that BodyPix would work on it when it will be ready: https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite=====""]"
https://github.com/Volcomix/virtual-background/issues/10;v4l2 loopback sink;2;closed;2021-05-15T23:42:53Z;2021-05-16T11:17:00Z;Hello, is it possible to feed the output to other application as a virtual camera (subj). On linux all video conf software lacks virtual background.;"[""Hi, I'm not sure to be aware of every detail but I think you will need a TURN server capable of receiving WebRTC streams and to relay them to v4l2 loopback device.I guess this should be possible but I don't know enough TURN servers to implement it in this demo and it would need to provide server or local software to install which I'd like to avoid in order to keep this project as a static website.====="", ""Ok, it's far from the goals of the project. Thanks for explaining.=====""]"
https://github.com/Volcomix/virtual-background/issues/9;Incorrect response MIME type. Expected 'application/wasm';6;closed;2021-05-11T09:53:47Z;2021-05-30T07:30:45Z;Hello.Nice work of yours! I've used some of your code in [my demo](https://github.com/drlight17/background-process-demo) in research purposes. Tested it and your demo on my old Linux Mint notebook and these errors appear when webgl processing is trying to start:`wasm streaming compile failed: TypeError: Failed to execute 'compile' on 'WebAssembly': Incorrect response MIME type. Expected 'application/wasm'.`and then as a result:`Uncaught (in promise) TypeError: Cannot read property 'VERTEX_SHADER' of null`OS is:```# lsb_release -aNo LSB modules are available.Distributor ID:	LinuxMintDescription:	Linux Mint 19.3 TriciaRelease:	19.3Codename:	tricia```Browser is:```# apt show chromium-browserPackage: chromium-browserVersion: 90.0.4430.93-0ubuntu0.18.04.1```chrome://gpu contains:```Graphics Feature StatusCanvas: Hardware acceleratedCompositing: Hardware acceleratedMultiple Raster Threads: DisabledOut-of-process Rasterization: Hardware acceleratedOpenGL: EnabledRasterization: Hardware accelerated on all pagesSkia Renderer: EnabledVideo Decode: Hardware acceleratedVulkan: DisabledWebGL: Hardware acceleratedWebGL2: Hardware accelerated```Does my notebook have some hw or sw incompatibles with wasm?Thanks.;"[""Hello @drlight17 and thanks!Are you serving the app with create-react-app dev server (aka webpack-dev-server)? This should be the case if you run `yarn start` with this demo project.The dev server is supposed to set the right MIME type for wasm files in response headers. If you are using another web server, I guess there are some config to add.Otherwise, this error can also happen when the wasm file is not found by the dev server, in which case it returns the `index.html` file instead (which obviously won't have the wasm MIME type). Have you checked the response body in the network tab of your browser devtools?====="", ""> Hello @drlight17 and thanks!> Are you serving the app with create-react-app dev server (aka webpack-dev-server)? This should be the case if you run `yarn start` with this demo project.No, this happen even if I try to open your hosted demo at the link https://volcomix.github.io/virtual-background/> The dev server is supposed to set the right MIME type for wasm files in response headers. If you are using another web server, I guess there are some config to add.> > Otherwise, this error can also happen when the wasm file is not found by the dev server, in which case it returns the `index.html` file instead (which obviously won't have the wasm MIME type). Have you checked the response body in the network tab of your browser devtools?No, but I'll check and tell you a little later.====="", 'I\'ve checked network tab, see screenshot:![image](https://user-images.githubusercontent.com/37434652/118232538-64426180-b499-11eb-90ce-a666c2d85705.png)Something is really wrong. What is the ""bad magic number""?=====', 'Looks like this is the expected response when opening wasm file in this tab. So I guess you received the right file.Would you mind sharing also a screenshot of the response headers along with an extract of your browser console?=====', ""A lot of static file servers don't yet recognize the `.wasm` extension, so you need to give them a bit of help.If using `express` on Node.js, just add this line:```javascriptexpress.static.mime.define({'application/wasm': ['wasm']}),```====="", ""I don't see anything wrong with your hardware or software @drlight17. I tried installing Chromium to check potential incompatibilities and both wasm and SIMD seem working. Chromium version was 91 though. Maybe you could try updating yours but I don't have other ideas for now.Unless you have more insights regarding response headers and error details from the browser console, I'm gonna close the issue. Thanks.![image](https://user-images.githubusercontent.com/7324857/120064469-9abcd680-c06c-11eb-9705-cd76e9d04fbd.png)=====""]"
https://github.com/Volcomix/virtual-background/issues/8;Failure to Build a particular version (2.5.0-rc3);2;closed;2021-05-06T23:16:39Z;2021-05-12T17:38:53Z;"Hi,I am trying to build version 2.5.0-rc3 and Docker file looks like this```FROM tensorflow/tensorflow:develRUN git clone https://github.com/google/mediapipe.git /mediapipe_srcRUN git -C /tensorflow_src checkout refs/tags/v2.5.0-rc3 -b v2.5.0-rc3COPY entrypoint.sh /entrypoint.shENTRYPOINT [""/entrypoint.sh""]```And added the following line in `entrypoint.sh````sed -i '/"":tvos_arm64"": COMMON_SRCS + MACH_SRCS + MACH_ARM_SRCS,/a "":emscripten_wasm"": COMMON_SRCS + EMSCRIPTEN_SRCS,' /tensorflow_src/third_party/cpuinfo/BUILD.bazel```But I am getting this error:```ERROR: Analysis of target '//:tflite' failed, build aborted: /root/.cache/bazel/_bazel_root/00533da95dbcbd49bca844bdfd8d02a4/external/XNNPACK/BUILD.bazel:4433:26: Configurable attribute ""deps"" doesn't match this configuration (would a default condition help?).Conditions checked: @XNNPACK//:linux_k8 @XNNPACK//:linux_arm @XNNPACK//:linux_armeabi @XNNPACK//:linux_armhf @XNNPACK//:linux_armv7a @XNNPACK//:linux_aarch64 @XNNPACK//:macos_x86_64 @XNNPACK//:macos_arm64 @XNNPACK//:windows_x86_64_clang @XNNPACK//:windows_x86_64_mingw @XNNPACK//:windows_x86_64_msys @XNNPACK//:windows_x86_64 @XNNPACK//:android_armv7 @XNNPACK//:android_arm64 @XNNPACK//:android_x86 @XNNPACK//:android_x86_64 @XNNPACK//:ios_armv7 @XNNPACK//:ios_arm64 @XNNPACK//:ios_arm64e @XNNPACK//:ios_x86 @XNNPACK//:ios_x86_64 @XNNPACK//:watchos_armv7k @XNNPACK//:watchos_arm64_32 @XNNPACK//:watchos_x86 @XNNPACK//:watchos_x86_64 @XNNPACK//:tvos_arm64 @XNNPACK//:tvos_x86_64 @XNNPACK//:emscripten_wasm @XNNPACK//:emscripten_wasmsimdINFO: Elapsed time: 73.625sINFO: 0 processes.FAILED: Build did NOT complete successfully (74 packages loaded, 1523 targets configured)    Fetching @nodejs_linux_amd64, fetching 12s    Fetching @emscripten_npm_linux, Restarting. 10s    Fetching ...8d02a4/external/nodejs_linux_amd64/bin/nodejs, Extracting /root/.cache/bazel/_bazel_root/00533da95dbcbd49bca844bdfd8d02a4/external/nodejs_linux_amd64/bin/nodejs/temp1554428458845878867/node-v12.13.0-linux-x64.tar.xz 9serror Command failed with exit code 1.````Can you please let me know what I am missing here?Thanks,Dipankar.";['Hi @dipankadas, it looks like `master` branch has not been rebased yet in `v2.5.0-rc3` tag of tensorflow. Hence besides the `sed`, you also need to revert from the remote emscripten toolchain to the old local one.You can find the required changes in this commit: 381c82b7c5441d12be5edf093f6e2a6273504d39=====', 'Thanks for your help. Using the local emscripten toolchain worked. Closing the ticket.=====']
https://github.com/Volcomix/virtual-background/issues/6;Mediapipe webassembly;2;closed;2021-05-03T10:41:02Z;2021-05-30T07:30:28Z;First of all thank you for the efforts you put on to this project. I have gained insights on Tflite-wasm-simd from your project.I am aware that this question might not be totally relevant to this project, but have you ever tried compiling Mediapipe to WASM from their entire source?;"[""Thank you for encouraging this work, I'm glad to hear the project was useful to you. I haven't tried compiling Mediapipe to WASM but that could be an interesting challenge.Not sure I would be able to try it anytime soon but I'll keep the issue open to take a look in the coming weeks.====="", ""I'm closing the issue because I don't think I'll work on it anytime soon and if I do, it would be in another project. Thanks.=====""]"
https://github.com/Volcomix/virtual-background/issues/3;Build failure;1;closed;2021-04-21T23:23:09Z;2021-04-22T07:41:04Z;"Hi,When trying to build the wasm binaries I am seeing the following errors and the build fails when I run `yarn build:tflite:all````INFO: Build option --crosstool_top has changed, discarding analysis cache.Analyzing: target //:tflite (1 packages loaded, 0 targets configured)Analyzing: target //:tflite (22 packages loaded, 120 targets configured)Analyzing: target //:tflite (60 packages loaded, 1195 targets configured)Analyzing: target //:tflite (72 packages loaded, 2053 targets configured)ERROR: Traceback (most recent call last):	File ""/root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/cpuinfo/BUILD.bazel"", line 131, column 27, in <toplevel>		"":emscripten_wasm"": COMMON_SRCS + EMSCRIPTEN_SRCS,Error: Duplicated key "":emscripten_wasm"" when creating dictionaryERROR: /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/org_tensorflow/tensorflow/lite/kernels/BUILD:335:11: no such target '@cpuinfo//:cpuinfo_with_unstripped_include_path': target 'cpuinfo_with_unstripped_include_path' not declared in package '' defined by /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/cpuinfo/BUILD.bazel and referenced by '@org_tensorflow//tensorflow/lite/kernels:cpu_backend_context'INFO: Repository emscripten_bin_linux instantiated at:  /tflite_src/WORKSPACE:56:22: in <toplevel>  /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/emsdk/emscripten_deps.bzl:28:21: in emscripten_depsRepository rule http_archive defined at:  /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>ERROR: Analysis of target '//:tflite' failed, build aborted: Analysis failedINFO: Elapsed time: 5.551sINFO: 0 processes.FAILED: Build did NOT complete successfully (77 packages loaded, 2200 targets configured)FAILED: Build did NOT complete successfully (77 packages loaded, 2200 targets configured)```Can you please help?Thanks,Dipankar.";"[""Hi @dipankadas, have you managed to fix the issue?I can't reproduce it locally but this error seems to be related to https://github.com/tensorflow/tensorflow/commit/d993cf4d99ce150b14916fdcc1f182c25f856ec7, which makes me remove a sed from the Dockerfile (https://github.com/Volcomix/virtual-background/commit/7b4c16eb89a768bf734bbf8d20b3180e5cd31a41#diff-377eaeb5506b9fb7e3100b820802c5a2b6f144b284f07825335761520f673ac0).Recreating the image and the container or just reverting the `/tensorflow_src/third_party/cpuinfo/BUILD.bazel` file in the container should fix the issue (sorry for the Tensorflow update process which is not automated very well yet).=====""]"
https://github.com/Volcomix/virtual-background/issues/2;ML Kit Selfie segmentation model;4;closed;2021-04-19T09:47:47Z;2021-04-30T01:16:04Z;Hi @Volcomix, I am really impressed with your work here! I have been following the work that has been done by multiple parties on this topic but have gotten stuck on the licensing issue surrounding the Google Meet model, until I came across this:https://developers.google.com/ml-kit/vision/selfie-segmentationThis seems very similar to Google Meet's model at first glance, and when taking a look at the [model card](https://developers.google.com/ml-kit/images/vision/selfie-segmentation/selfie-model-card.pdf), it actually is licensed under Apache 2.0!Doing some further digging I was able to extract the .tflite file from the Android package without issue:[selfiesegmentation_mlkit-256x256-2021_01_19-v1215.f16.tflite.zip](https://github.com/Volcomix/virtual-background/files/6334898/selfiesegmentation_mlkit-256x256-2021_01_19-v1215.f16.tflite.zip)After that I did an inspection of both the Google Meet model you have in the repo and this new one using https://netron.app/:Google Meet![segm_full_v679 tflite](https://user-images.githubusercontent.com/17837362/115215590-f1490380-a103-11eb-8cdf-9a8c3c4ad85f.png)ML Kit Selfie segmentation![selfiesegmentation_mlkit-256x256-2021_01_19-v1215 f16 tflite](https://user-images.githubusercontent.com/17837362/115215564-ebebb900-a103-11eb-8ecb-807e70f88e60.png)As you can see their structures are practically identical, the only difference I see is the input and output sizes, the new model using 256x256.I have tried cloning your repo and loading this model with it by putting it in the public folder with the other models and changing the path used to load it, it seems to load correctly but there is no output visible except a very faint, perhaps stretched outline of the person detected. This leads me to believe using this model with your implementation should be possible, perhaps requiring some adjustment due to the different input/output resolutions (memory offset?), curious to hear your thoughts on this 😄.;"[""Hi @RemarkableGuy, thank you for the kind words and for pointing out the MLKit Selfie segmentation model.Indeed when you open Google Meet segmentation model in a text editor, you can see something like this:```��\x01��(location //research/aimatter/nnets/tools/conversion/keras2tflite:keras2tflite) --name=keras2tflite_selfiesegmentation_web_256x144-2020_10_05-v679.tflite.generated  (location selfiesegmentation_web_256x144-2020_10_05-v679.hdf5) blaze-out/k8-opt/genfiles/research/aimatter/nnets/models/selfiesegmentation_web_256x144_2020_10_05_v679/keras2tflite_selfiesegmentation_web_256x144-2020_10_05-v679.tflite.generated --remove_softmax���.���selfiesegmentation_web_256x144_2020_10_05_v679������n,��\x04���```This makes me think that MLKit Selfie and Google Meet segmentation models have been generated by exporting tflite files from the same Keras model. If by any chance this Keras model is available somewhere and released under Apache 2.0 licence, we should be able to export it as well on our own and to use it. Unfortunately I tried to find it few weeks (or months) ago without any luck. Maybe we could try again nowadays.I'm pretty confident that we can use the Selfie segmentation model in this repo with few adjustments. I will start a new branch to experiment on this (probably this coming weekend).In anyways Google Meet model with resolution 256x144 takes around 2x more time than the 160x96 one to infer the segmentation, so I guess the 256x256 one from MLKit Selfie segmentation will be even slower. This should still be fine for desktop browsers though.====="", ""Hi @Volcomix, no problem! I came to the same conclusion about the origin of these models and indeed it would be very interesting if we had access to that. Very nice to hear you are interested in creating a branch for this! I'm looking forward to comparing the model's performance characteristics with BodyPix for example, even equal performance to BodyPix but with an improved mask definition would be very valuable to me.====="", 'Hi @RemarkableGuy, I added ML Kit Selfie Segmentation in the demo. I also added [some documentation in the readme](https://github.com/Volcomix/virtual-background#ml-kit-selfie-segmentation). The PR closed the issue but please feel free to give any feedback either in this issue or by creating new ones. Cheers!=====', ""I don't know if it will be useful for you, but I have converted and quantized it for various frameworks and committed it to my repository.**TFLite Float32/Float16/INT8, TFJS, TF-TRT, ONNX, CoreML, OpenVINO IR FP32/FP16, Myriad Inference Blob**https://github.com/PINTO0309/PINTO_model_zoohttps://github.com/PINTO0309/PINTO_model_zoo/tree/main/109_Selfie_Segmentation=====""]"
https://github.com/Volcomix/virtual-background/issues/1;TFLite fails to build in the Docker container;5;closed;2021-03-11T21:25:43Z;2021-03-12T21:00:31Z;Hi, I am trying to run your demo on my Mac and when I run `yarn build:tflite:all` and bazel tries to build tflite, it fails with this error :```$ docker exec -w /tflite_src tflite bazel build --config=wasm -c opt :tfliteExtracting Bazel installation...Starting local Bazel server and connecting to it...Loading:Loading: 0 packages loadedERROR: error loading package '': cannot load '@org_tensorflow//tensorflow:workspace.bzl': no such fileINFO: Elapsed time: 4.869sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)FAILED: Build did NOT complete successfully (0 packages loaded)error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```I would guess I have to change something in the `/tflite_src/WORKSPACE` file but I don't know how bazel works yet. I will try to fix the issue and will comment again if I find how to get rid of the error.;"[""Hi @RaphaelRoyerRivard, thanks for reporting that. I juste took a look at tensorflow repo and it looks like `tensorflow/workspace.bzl` is gone from the master branch.I'll investigate as soon as possible (most probably this weekend) to figure out how it has been replaced.As a workaround, maybe could you try checking out the tag v2.4.1 of tensorflow within the container, manually apply the seds from the dockerfile and try again.I don't have a computer so I can't be more specific right now but I will come back to you as soon as possible.====="", 'Thanks a lot for your answer, I will try that.=====', 'I deleted the container and replaced the following line in Dockerfile```RUN git -C /tensorflow_src pull```with```RUN git -C /tensorflow_src fetchRUN git -C /tensorflow_src checkout 85c8b2a817f95a3e979ecd1ed95bff1dc1335cff```which is the 2.4.1 tag of Tensorflow and it built successfully :)=====', 'Nice thank you for the feedback!=====', '3d1593d should fix the issue.Let me know if you still have troubles to build TFLite (if you decide to switch back to tensorflow master :wink:).=====']"
https://github.com/thibo73800/metacar/issues/37;Stop the agent instead of changing lane;2;open;2020-06-11T02:42:03Z;2020-06-13T10:19:50Z;How to stop the agent whenever it's path is blocked or let it just simply follow other vehicles instead of changing lanes? I tried various things but nothing is reflecting while training.;"['Hi, you want the agent to learn a behavior to follow other vehicles rather than changing lanes ? =====', ""Yes, I want the agent to stop whenever any other vehicle comes on its way or simply follow the other vehicles without undergoing any collision. So basically I just want to introduce a braking part in it.The agent should accelerate whenever the path is free and at the same time it should reduce the speed or it should stop whenever it's path is blocked but it should never change lane.I have tried various things ranging from changing the control motion to changing reward function but still nothing seems to work out.=====""]"
https://github.com/thibo73800/metacar/issues/31;Import metacar environment to python;2;open;2019-01-25T12:25:30Z;2019-06-08T15:02:39Z;Hello, I'm working on a python project with Reinforcement Learning, and I found the Metacar project very useful..The thing is that I'm still new to this domain, and I don't know javascript.So how would I import this environment to my Python IDE? Any help would be great;['I think the proper way would be: Wrapping the Javascript into some Python code. At the same time making it compatible to OpenAI Gym.=====', 'Current WIP: https://github.com/AI-Guru/gym-metacar=====']
https://github.com/thibo73800/metacar/issues/27;Local copies of npm libs;1;open;2018-07-24T08:11:23Z;2018-09-09T18:04:50Z;Hi, if Pixi and TFJS were locally installed it would allow offline testing.;['There is no problem with TFJS, however for some reason Metacar does not know how to import Pixi.js from node modules. You need explicit script tag in html.=====']
https://github.com/infinitered/nsfwjs/issues/569;False positive on manga;1;open;2021-12-13T07:37:12Z;2021-12-15T19:14:57Z;## OverviewModel used: Inception v3. MobileNet models does not experience this issue.![ar0ltbti00h71](https://user-images.githubusercontent.com/32661241/145770552-f7b8c11d-7d73-43d7-a04f-97b3315c6eee.jpg)![proof](https://user-images.githubusercontent.com/32661241/145770838-b8b0fada-d9d0-437a-bbb9-88a021906cd1.png);"[""I'm thinking about making a voting layer that allows you to combine multiple models and get higher accuracy.=====""]"
https://github.com/infinitered/nsfwjs/issues/563;Demo predictions and Node JS App predictions are not the same result;2;open;2021-11-22T15:53:40Z;2021-11-22T16:42:16Z;"nsfwjs.com (load model: inceptionv3: ['/model/', { size: 299 }]) image result:```Identified as PornPorn - 48.19%Neutral - 40.21%Sexy - 10.87%Hentai - 0.61%Drawing - 0.12%```NodeJS app (load model: inceptionv3: ['/model/', { size: 299 }]) same image result:```[    {        ""className"": ""Neutral"",        ""probability"": 0.6607071757316589    },    {        ""className"": ""Sexy"",        ""probability"": 0.18770809471607208    },    {        ""className"": ""Porn"",        ""probability"": 0.14371605217456818    },    {        ""className"": ""Hentai"",        ""probability"": 0.004498853348195553    },    {        ""className"": ""Drawing"",        ""probability"": 0.0033697152975946665    }]```My Code:```const express = require('express')const multer = require('multer')const jpeg = require('jpeg-js')const tf = require('@tensorflow/tfjs-node')const nsfw = require('nsfwjs')const app = express()const upload = multer()let _modelconst convert = async (img) => {  // Decoded image in UInt8 Byte array  const image = await jpeg.decode(img, true)  const numChannels = 3  const numPixels = image.width * image.height  const values = new Int32Array(numPixels * numChannels)  for (let i = 0, i < numPixels, i++)    for (let c = 0, c < numChannels, ++c)      values[i * numChannels + c] = image.data[i * 4 + c]  return tf.tensor3d(values, [image.height, image.width, numChannels], 'int32')}app.post('/nsfw', upload.single(""image""), async (req, res) => {  if (!req.file)    res.status(400).send(""Missing image multipart/form-data"")  else {    const image = await convert(req.file.buffer)    const predictions = await _model.classify(image)    image.dispose()    res.json(predictions)  }})const load_model = async () => {  _model = await nsfw.load('https://nsfwjs.com/model/', {size: 299})}// Keep the model in memory, make sure it's loaded only onceload_model().then(() => app.listen(8080))```I tried the model files locally, same result.";['Have you tried tfjs-node to decode the images?Just for info.  Which one of the two was correct?  I assume the website was correct, and the node was wrong?=====', 'https://github.com/infinitered/nsfwjs/discussions/540=====']
https://github.com/infinitered/nsfwjs/issues/558;Cannot run in browser extensions;3;open;2021-11-18T04:06:23Z;2021-11-22T16:51:11Z;Manifest V3 for chromium based browsers require extensions have no remote-executed code (unsafe-eval must be disabled), nsfwjs uses new Function() 9 separate times, making the library unusable, since loading a function from a String is considered unsafe-eval.Is there a workaround, if anyone around her has used it with chromium-based apps? Would it be worth it for me to just fork & edit it myself?Isaac![156c8b4394cbfbf13802182d0c9a84a9](https://user-images.githubusercontent.com/65869106/142350142-cf5f4206-c818-4640-9514-f9d33b46eada.png);"[""For an extension have you tried this fix?  https://stackoverflow.com/questions/26242682/unsafe-eval-on-chrome-extensionFor the changes you're proposing, what do those look like?====="", ""Those changes don't work. Unfortunately google is deprecating manifest v3, so unsafe-eval will not be a possible choice for chrome webstore developers come ~January 2022. After doing a bit of looking around, I can't find the offending code in the source on this repo, which leads me to believe it's being created by the minifier?https://unpkg.com/nsfwjs@2.3.0/dist/nsfwjs.min.jsNot sure what changes I'm proposing, but these are the recurring elements that make it unable to run in browser:```eval()Function()     // typically new Function()setTimeout()   // with non-callable argumentsetInterval()  // with non-callable argumentsetImmediate()execScript()```I looked around and there are a plethora of Chrome apps using your library (It's a sick library, by the way), and they also will not be able to create new ones using it after Jan '22![image](https://user-images.githubusercontent.com/65869106/142751336-7920966d-aafe-4929-9dcc-c7d2bc9ef1ab.png)====="", ""Thanks so much!!!  I appreciate the kudos.   @isaackogan - I could use your help right quick!My guess is the issue is because my TypeScript is set to be supported as far back as ES5.https://github.com/infinitered/nsfwjs/blob/master/tsconfig.json#L10Would you mind pulling the code down, and trying different targets (https://www.typescriptlang.org/tsconfig#target) to see if one of these will remove the offending code?IF - that's the issue, I can release a second version on NPM that isn't as backwards compatible, but would work well on browser extensions.---Side note:  How do you find a list of chrome apps that are using NSFWJS?   I'd love to see that list!=====""]"
https://github.com/infinitered/nsfwjs/issues/554;Make @nsfw-filter/gif-frames optional;1;open;2021-10-23T21:00:27Z;2021-12-15T23:07:47Z;"@nsfw-filter/gif-frames is not only big, but it also depends on cwise-compiler which uses unsafe functions (""new Function"").  Ideally it could be made optional--not everyone using the nsfwjs needs/wants gif support. ";"[""This is a great idea.  It will take some architecting to make it practical for gif/no-gif - but I like where you're at with this.  It also opens the door for a separation between NSFWJS and video frames, too.=====""]"
https://github.com/infinitered/nsfwjs/issues/546;Not working in React native;3;open;2021-09-20T07:29:33Z;2021-11-30T07:24:16Z;I installed nsfwjs in react native. after installation without importing and using app runs but after:import * as nsfwjs from 'nsfwjs',and await nsfwjs.load()This error is shown.`Error: Unable to resolve module path from /Users/muhammadowais/workspace/datingapp/node_modules/get-pixels-frame-info-update/dom-pixels.js: path could not be found within the project or in these directories:  node_modules/get-pixels-frame-info-update/node_modules  node_modules  ../../node_modulesIf you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*  1 | 'use strict'  2 |> 3 | var path          = require('path')    |                              ^  4 | var ndarray       = require('ndarray')  5 | var GifReader     = require('omggif').GifReader  6 | var pack          = require('ndarray-pack')`- [1 ] Then I just installed :`npm install path`Error: Unable to resolve module assert from /Users/muhammadowais/workspace/datingapp/node_modules/gif-encoder/lib/GIFEncoder.js: assert could not be found within the project or in these directories:  node_modules/gif-encoder/node_modules  node_modules  ../../node_modulesIf you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*   9 | */  10 |> 11 | var assert = require('assert'),     |                       ^  12 | var EventEmitter = require('events').EventEmitter,  13 | var ReadableStream = require('readable-stream'),  14 | var util = require('util'),    at ModuleResolver.resolveDependency (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/node-haste/DependencyGraph/ModuleResolution.js:234:15)    at DependencyGraph.resolveDependency (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/node-haste/DependencyGraph.js:413:43)    at Object.resolve (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/lib/transformHelpers.js:317:42)    at resolve (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:629:33)    at /Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:645:26    at Array.reduce (<anonymous>)    at resolveDependencies (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:644:33)    at /Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:329:33    at Generator.next (<anonymous>)    at asyncGeneratorStep (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:137:24)- [2 ] then I installed:`npm install assert`This error poped.error: Error: Unable to resolve module stream from /Users/muhammadowais/workspace/datingapp/node_modules/through/index.js: stream could not be found within the project or in these directories:  node_modules  ../../node_modulesIf you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*> 1 | var Stream = require('stream')    |                       ^  2 |  3 | // through  4 | //    at ModuleResolver.resolveDependency (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/node-haste/DependencyGraph/ModuleResolution.js:234:15)    at DependencyGraph.resolveDependency (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/node-haste/DependencyGraph.js:413:43)    at Object.resolve (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/lib/transformHelpers.js:317:42)    at resolve (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:629:33)    at /Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:645:26    at Array.reduce (<anonymous>)    at resolveDependencies (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:644:33)    at /Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:329:33    at Generator.next (<anonymous>)    at asyncGeneratorStep (/Users/muhammadowais/workspace/datingapp/node_modules/@react-native-community/cli/node_modules/metro/src/DeltaBundler/traverseDependencies.js:137:24)- [3 ] Then installed:`npm install stream`error: Error: Unable to resolve module events from /Users/muhammadowais/workspace/datingapp/node_modules/gif-encoder/lib/GIFEncoder.js: events could not be found within the project or in these directories:  node_modules/gif-encoder/node_modules  node_modules  ../../node_modulesIf you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*  10 |  11 | var assert = require('assert'),> 12 | var EventEmitter = require('events').EventEmitter,     |                             ^  13 | var ReadableStream = require('readable-stream'),  14 | var util = require('util'),- [4 ] Intsalled: `npm install events`Error:error: Error: Unable to resolve module zlib from /Users/muhammadowais/workspace/datingapp/node_modules/pngjs-nozlib/lib/parser-async.js: zlib could not be found within the project or in these directories:  node_modules  ../../node_modulesIf you are sure the module exists, try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*  2 |  3 | var util = require('util'),> 4 | var zlib = require('zlib'),    |                     ^  5 | var ChunkStream = require('./chunkstream'),  6 | var FilterAsync = require('./filter-parse-async'),  7 | var Parser = require('./parser'),- [5 ] installed:`npm install zlib`Error:error: Error: Unable to resolve module ./zlib_bindings from /Users/muhammadowais/workspace/datingapp/node_modules/zlib/lib/zlib.js: None of these files exist:  * node_modules/zlib/lib/zlib_bindings(.native|.ios.js|.native.js|.js|.ios.json|.native.json|.json|.ios.ts|.native.ts|.ts|.ios.tsx|.native.tsx|.tsx|.ios.svg|.native.svg|.svg|.ios.bin|.native.bin|.bin)  * node_modules/zlib/lib/zlib_bindings/index(.native|.ios.js|.native.js|.js|.ios.json|.native.json|.json|.ios.ts|.native.ts|.ts|.ios.tsx|.native.tsx|.tsx|.ios.svg|.native.svg|.svg|.ios.bin|.native.bin|.bin)> 1 | module.exports = require('./zlib_bindings'),    |                           ^  2 |NOW I tried installing zlib and adding zlib_bindings in metro.config.js but no lock. there id file at `'./zlib_bindings'` I checked in node_module;['any update ?=====', 'Did you follow this blog post? I had to shim some things in order to get everything working properly on React Native: https://shift.infinite.red/nsfw-js-for-react-native-a37c9ba45fe9=====', '@kevinvangelder  yes I followed this blog I was stuck in the start (even just installing libraries) and found no support so I left it. :)=====']
https://github.com/infinitered/nsfwjs/issues/525;Unexpected errors are thrown if image width and height is not specified;5;open;2021-06-26T15:43:38Z;2021-06-26T20:17:23Z;When you try to run `model.classify(image)`, where `image` is of type `HTMLImageElement`, if the dimensions (width and height) of the image are not specified, the function throws unexpected errors, one of which is as follows:```DOMException: Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.```This is resolved if you make a new `Image` object with a width and height that you specify (`200` in the following example), and you set `newImg.crossOrigin` to `anonymous`. A similar code snippet is also used in the chrome extension [NSFW Filter](https://github.com/nsfw-filter/nsfw-filter/), yet there is no mention of it on the README of this repo.https://github.com/nsfw-filter/nsfw-filter/blob/2abc417c4a840ed53a7b0cf060c3867d54265779/src/background/Queue/LoadingQueue.ts#L42-L50```ts// `image` is the original `HTMLImageElement` that is supposed to be classified// `newImage` is `image` but with width and height specified (to 200, 200 in this case)const newImg = new Image(200, 200),newImg.crossOrigin = 'anonymous',newImg.src = image.src,model.classify(newImg)```Without the above modification, the image classification is inconsistent, i.e, it fails sometimes and succeeds sometimes for the same exact image (I've tested this on Google Images). It might be because the images contain too many pixels, I was not able to figure out why the error occurred randomly, but adding the above snippet fixed it. There should probably be a mention of this on the README of the repo or a fix that resolves the issue entirely.;"[""Interesting.  A couple thoughts come to mind.The most likely issue is that there's a security issue where the `src` of the image is cross-origin.  By using the `Image` tag, you're effectively making a local copy and getting around `tainted canvases` issues.That doesn't explain how it sometimes works and sometimes doesn't though.   That sounds more like a race-condition.   So maybe you're calling `classify` before the image is fully loaded?  I'd have to see the code to be sure.====="", '@thebongy could you please post the code snippet here?=====', ""I'll paste the relevant part of the code here.We basically had a content script in a chrome extension which did the following:```typescriptlet model: nsfwjs.NSFWJS,nsfwjs.load('<redacted private url>').then((loaded) => {  model = loaded,}),window.onload = async () => {   const images = [...document.querySelectorAll('img')],    for (const image in images) {      try {        console.log(await filterImage(images[image], 0.5)),        await sleep(50),      } catch (err) {        console.log(err),        // Blank      }    }}export default async function filterImage(  image: HTMLImageElement, threshold: Number,): Promise<FilterResult> {  const width = image.clientWidth,  if (width < 50) {    return {      filter: false,      reason: 'The image is too small',    },  }  const newImg = new Image(200, 200),  newImg.crossOrigin = 'anonymous',  newImg.src = image.src,  const result = await scanImage(newImg),  const highest = result[0],  if (highest.className === 'Neutral' || highest.className === 'Drawing') {    return {      filter: false,      reason: 'The image is neutral.',    },  }  if (highest.probability < threshold) {    return {      filter: false,      reason: 'The image is below the given threshold.',    },  }  return {    filter: true,    reason: `The image has ${highest.className} content which is over the configured threshold.`,  },}async function scanImage(element: HTMLImageElement) {  while (!model) {    // eslint-disable-next-line no-await-in-loop    await sleep(1000),  }  return model.classify(element),}```What we observed was that the same code, when the lines```typescriptconst newImg = new Image(200, 200),  newImg.crossOrigin = 'anonymous',  newImg.src = image.src,```weren't present, and we just directly processed the input image, we would sometimes get the error posted originally on this issue (Tainted canvases may not be loaded.) . But constructing a new smaller intermediate image always made the library work.====="", ""This seems to be a bug with TFJShttps://github.com/tensorflow/tfjs/issues/322It's also been identified here: https://stackoverflow.com/questions/57662313/requested-texture-size-0x0-is-invalid-error-when-i-am-loading-image-in-browseThis didn't used to be a bug.However, the error you're getting seems to actually be a crossDomain error.   Because you're loading an image from a different website.  So you'll need to make a local copy.You can test all 4 possible situations with this codesandbox I created here: * switch between `img1` and `img2` for local and remote. * switch line 14 and 15 for image resize on and offhttps://codesandbox.io/s/nsfwjscrossoriginnsizecheck-epu3w?file=/src/index.js====="", ""I see, thank you for verifying!For now the  intermediate image solution works for us, we'll update on this thread if we find out anything new about the issue :)=====""]"
https://github.com/infinitered/nsfwjs/issues/522;Cannot load model from file;5;open;2021-06-21T04:41:22Z;2021-07-03T21:53:55Z;I am trying to load a model from local directory using `const model = await nsfwjs.load('file://./model/'),`and it gives me error saying `TypeError: Only HTTP(S) protocols are supported`. What can I do?;"[""just use path in load. In your case relative path:const model = await nsfwjs.load('./model/'),====="", 'I tried that but it gives another error`TypeError: Only absolute URLs are supported`=====', ""@kavinplays - please create a small repo that reproduces this issue, and I'll find a fix.====="", 'Dear **[@GantMan](https://github.com/GantMan)**, would you kindly share for community full repo of your nsfwjs.com, because not all users here have advanced skills in tensorflow=====', '@intbw - NSFWJS.com is available here:  https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo=====']"
https://github.com/infinitered/nsfwjs/issues/521;NSFWjs not working in lambda;5;open;2021-06-21T04:23:05Z;2021-07-16T12:09:58Z;"I tried using nsfwjs to detect images upload to s3 as safe or not but it is not working in lambda, I tried increasing memory to 10GB. But it still timed out :( Log: ```START RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851e Version: $LATEST2021-06-21T03:34:53.247Z    18f3c1c7-3341-4973-b35c-b638cadd851e    WARN============================Hi there :wave:. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'), (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================END RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851eREPORT RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851e    Duration: 30033.57 ms    Billed Duration: 30000 ms    Memory Size: 10240 MB    Max Memory Used: 121 MB    Init Duration: 606.54 ms2021-06-21T03:35:23.277Z 18f3c1c7-3341-4973-b35c-b638cadd851e Task timed out after 30.03 seconds```Code (for local testing):```//  const aws = require('aws-sdk'),const tf = require('@tensorflow/tfjs'),const sizeOf = require('image-size'),const jpeg = require('jpeg-js'),const nsfwjs = require('nsfwjs')const axios = require('axios')//const s3 = new aws.S3({ apiVersion: '2006-03-01' }),//exports.handler = async (event, context) => {async function abc(){  const model = await nsfwjs.load(),  const readImage = buf => {    var imgprodata = sizeOf(buf)    if(imgprodata.type != ""JPEG"" && imgprodata.type != ""JPG"" && imgprodata.type != ""jpeg""&& imgprodata.type != ""jpg""){      let opt = {        height: imgprodata.height,        width: imgprodata.width,        data: buf,      }      const pixels = jpeg.encode(opt,100)      return pixels    } else {    const pixels = jpeg.decode(buf, true)    return pixels    }  }    const imageByteArray = (image, numChannels) => {    const pixels = image.data    const numPixels = image.width * image.height,    const values = new Int32Array(numPixels * numChannels),      for (let i = 0, i < numPixels, i++) {      for (let channel = 0, channel < numChannels, ++channel) {        values[i * numChannels + channel] = pixels[i * 4 + channel],      }    }      return values  }    const imageToInput = (image, numChannels) => {    const values = imageByteArray(image, numChannels)    const outShape = [image.height, image.width, numChannels],    const input = tf.tensor3d(values, outShape, 'int32'),      return input,  }    // const key = event.Records[0].s3.object.key,  const key = '7c1e158e-26a8-44a1-b219-bf85741e4754'  var pic = await axios.get(`https://public-usercontent.dogegram.xyz/storage/${key}`, {    responseType: 'arraybuffer'  }),  var pix = await readImage(pic.data),  var img = await imageToInput(pix, 3)    const predictions = await model.classify(img)  console.log(predictions)  return 'done'  }//}abc()```For testing:```npm init -ynpm i @tensorflow/tfjs image-size jpeg-js nsfwjs axios```";"[""While the jpeg-js should work, there's a better way to read images in node now.I haven't updated the docs here on NSFWJS but you could use things like https://js.tensorflow.org/api_node/3.7.0/#node.decodeJpegI cover this in my book.   Please give tfjs-node a go, and let me know if that solves your issues.====="", ""Tfjs-node is too large to fit in lambda's size limit :(====="", 'Tagging the issue here:  https://github.com/tensorflow/tfjs/issues/2817=====', '@hrichiksite - have you seen this?  https://stackoverflow.com/questions/59899650/running-tensorflow-js-tfjs-node-on-aws-lambda-node-js=====', ""> @hrichiksite - have you seen this? https://stackoverflow.com/questions/59899650/running-tensorflow-js-tfjs-node-on-aws-lambda-node-jsHi, I have seen this but I didn't get how to load NPM modules from a different DIR, I had tried to find but no success :(Actually, The Issue here is not Size anymore but the fact that it is timing out even 60 secs. =====""]"
https://github.com/infinitered/nsfwjs/issues/513;False Positive Drawing;3;open;2021-05-07T18:58:01Z;2021-05-07T19:13:08Z;![DvYY8qbXgAAMi62.jpeg](https://user-images.githubusercontent.com/50463727/117496130-e51fcb80-af76-11eb-82be-37411380d1fb.jpeg);"['Model Used is the latest release from the nsfw model repo=====', 'Prediction ResultsLabel: HentaiProbability: 70%=====', ""Thanks for sharing.  We'll add this to the data.=====""]"
https://github.com/infinitered/nsfwjs/issues/512;Cannot load latest (v1.1) nsfw_model from local file;8;open;2021-05-07T15:34:51Z;2021-06-26T15:32:16Z;"I currently have the latest nsfw_model found at https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 and am trying to load it as a local file like so:```jsconst model = await nsfw.load('file://' + __dirname + '/mobilenet_v2_140_224/web_model_quantized/'),```However this results in the following error:```(node:28129) UnhandledPromiseRejectionWarning: Error: layer: Improper config format: (LARGE JSON OBJECT HERE).'className' and 'config' must set.```I tried the solution found in #397, specifically [this comment](https://github.com/infinitered/nsfwjs/issues/397#issuecomment-702773846), however when running `npm install @tensorflow/tfjs@^1.7.4` I get:```npm ERR! npm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! Found: @tensorflow/tfjs@1.7.4npm ERR! node_modules/@tensorflow/tfjsnpm ERR!   @tensorflow/tfjs@""^1.7.4"" from the root projectnpm ERR! npm ERR! Could not resolve dependency:npm ERR! peer @tensorflow/tfjs@""^3.1.0"" from nsfwjs@2.4.0npm ERR! node_modules/nsfwjsnpm ERR!   nsfwjs@""^2.4.0"" from the root projectnpm ERR! npm ERR! Fix the upstream dependency conflict, or retrynpm ERR! this command with --force, or --legacy-peer-depsnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.npm ERR! npm ERR! See /home/jon/.npm/eresolve-report.txt for a full report.npm ERR! A complete log of this run can be found in:npm ERR!     /home/jon/.npm/_logs/2021-05-07T15_30_17_117Z-debug.logcode ERESOLVE```Running `npm install @tensorflow/tfjs@^1.7.4 --force` works to install that version but then the `load` function fails to load a local file, saying only absolute URLs are supported@tensorflow/tfjs-node: ^3.6.1nsfwjs: ^2.4.0";"[""Thanks for the detailed ticket. Can you make a small demo-repo and I'll pull that down and debug the issue?  That's he most effective way for me to construct a fix====="", 'I seem to have found the issue while making the demo repo (which honestly was only going to be like 3 lines of code)I was following the ""NodeJS App"" example here https://github.com/infinitered/nsfwjs#node-js-app which only says to pass in the path to the model folderHowever with the latest nsfw_model it seems to need you to pass in `{ type: \'graph\' }` into the options? This seems to have loaded the model correctly and it does seem to classify correctly stillMaybe this should be better explained in the README and examples? It wasn\'t immediately obvious that I needed to pass in this option to get the local files to work, especially since when looking at the source of this module it _doesn\'t_ pass that option...?=====', ""Ahhh, which model are you using?  The 93% accurate one?Please feel free to contribute back to the docs.  It's easy to forget to update them, and your experience from the outside is valuable to others who come in and use the docs.====="", '> I seem to have found the issue while making the demo repo (which honestly was only going to be like 3 lines of code)> > I was following the ""NodeJS App"" example here https://github.com/infinitered/nsfwjs#node-js-app which only says to pass in the path to the model folder> > However with the latest nsfw_model it seems to need you to pass in `{ type: \'graph\' }` into the options? This seems to have loaded the model correctly and it does seem to classify correctly still> > Maybe this should be better explained in the README and examples? It wasn\'t immediately obvious that I needed to pass in this option to get the local files to work, especially since when looking at the source of this module it _doesn\'t_ pass that option...?Hi RedDuck, I tried the following:`_model = await nsfw.load(\'.\\models\\2020-03-04 - nsfw_mobilenet_v2_140_224\\mobilenet_v2_140_224\\web_model\', ""{ type: \'graph\' }"")`But I get this error:`Cannot create property \'size\' on string \'{ type: \'graph\' }\'`How did you pass in the `{ type: \'graph\' }` option?Thanks,Fidel=====', ""Oh, never mind :)This worked:`_model = await nsfw.load('file://' + __dirname + '/models/2020-03-04 - nsfw_mobilenet_v2_140_224/mobilenet_v2_140_224/web_model/', { type: 'graph', })`====="", ""> Please feel free to contribute back to the docs. It's easy to forget to update them, and your experience from the outside is valuable to others who come in and use the docs.Sure, I can do that. How would the best way to do that be? Just make a PR for the readme to add `{ type: 'graph' }`?====="", 'Yup!  And any additional context.=====', 'Just checking in on this ticket.  =====']"
https://github.com/infinitered/nsfwjs/issues/499;Question;5;open;2021-04-20T04:29:39Z;2021-06-20T08:48:02Z;So let's say I wanted to use this with a discord bot that checks a url for nsfw content. Can I do that with this, and how could I do it?;"[""You'd have to have your bot pull the HTML/CSS and parse for JPG/PNG/GIF files.   Once you parse the files you would loop through them and use that information to report.====="", ""You're not the first person to ask about this, perhaps I should make this a feature of the library?  Just thinking on this.  Perhaps it's a useful enough skill.====="", ""Yeah, I have a command that screenshots whatever site you want, and if it's NSFW it returns `This site is NSFW`, but it doesn't check for images, just URLs====="", ""Currently, to use this you'd have to parse the HTML/CSS.  I'll mark this ticket as a feature request and revisit it next Hacktoberfest.====="", 'I\'ve just added this to my discord bot as well. This is how you can do it yourself.```const tf = require(""@tensorflow/tfjs-node"")const nsfwjs = require(""nsfwjs"")const fetch = require(""node-fetch"")// Attachment has to contain image url, height and widthasync function classifyImage(attachment) {const model = await nsfwjs.load()const imageBuffer = await fetch(attachment.url).then((res) => res.buffer()),const imageTensor = tf.node.decodeImage(imageBuffer)return await model.classify(imageTensor)}```This is a bit different from my implementation, but i think it should work. =====']"
https://github.com/infinitered/nsfwjs/issues/497;Censoring Specific Areas;5;open;2021-04-15T10:05:29Z;2021-05-04T16:52:58Z;"As the advancement of data segmentation continues, it seems that NSFW censoring certain area becomes increasingly easy.It now only takes a sample dataset, WITHOUT segmentation data, to roughly detect sexual organs and other ""things"".- https://people.csail.mit.edu/mrub/ObjectDiscovery/- https://www.researchgate.net/publication/325052999_Object_Discovery_and_Cosegmentation_Based_on_Dense_Correspondences- https://www.researchgate.net/publication/320350075_Unsupervised_Image_Co-segmentation_via_Guidance_of_Simple_ImagesThis would be really beneficial to blur out things.";"['We began work on something for this.  It is unfortunately a lot a lot of work.   Can you identify what the need for such a feature might be?=====', 'Censorship for movie family-friendliness or documentary ethics compliance purposes.=====', ""That makes sense.   To be honest, the amount of work needed to make that would be substantial.  Would this be a model you'd be willing to pay for?====="", 'I would potentially @GantMan.  Could you provide a ballpark figure?=====', ""We charge 6k USD a week of work.  I honestly don't know how long it would take.  We have 2k labeled object detection images right now, but we'd need to double that number. =====""]"
https://github.com/infinitered/nsfwjs/issues/493;example/manualtesting doesn't work;3;open;2021-04-03T06:12:09Z;2021-04-03T19:38:55Z;I get this error when running it: tfjs@1.7.4:2 Uncaught (in promise) TypeError: t is not a function    at tfjs@1.7.4:2    at tfjs@1.7.4:2    at t.scopedRun (tfjs@1.7.4:2)    at t.tidy (tfjs@1.7.4:2)    at h (tfjs@1.7.4:2)    at tfjs@1.7.4:2    at t.scopedRun (tfjs@1.7.4:2)    at t.runKernelFunc (tfjs@1.7.4:2)    at t.runKernel (tfjs@1.7.4:2)    at slice_ (nsfwjs.min.js:7147)(anonymous) @ tfjs@1.7.4:2;"['@YegorZaremba - can you take a look at this?=====', ""Hi @GantMan  I'll check it tommorowBTW, we have tfjs v 3.x.x in package.json https://github.com/infinitered/nsfwjs/blob/master/package.json#L27====="", 'Correct.  The project seems to work fine with TFJS 3.x =====']"
https://github.com/infinitered/nsfwjs/issues/491;Create a voting classifier ensemble for more advanced accuracy;1;open;2021-03-30T04:18:22Z;2021-06-26T01:17:16Z;Currently, we have two models.  The more accurate one that has type 2 errors, and the slightly less accurate one that has far more type 1 errorsWe can combine them with a voting classifier and get higher accuracy.https://medium.com/@sanchitamangale12/voting-classifier-1be10db6d7a5Thoughts?;['This can be accomplished with the simple JavaScript to weight one model over another, too.https://tinyurl.com/yjv3ohz4=====']
https://github.com/infinitered/nsfwjs/issues/489;trying to implement nsfwjs model in flutter web but getting same number from predicted array ;2;open;2021-03-28T03:25:26Z;2021-04-10T07:42:51Z;"label : help needed i am trying to use this nsfwjs model in flutter web by taking reference from this repogithub link : https://github.com/AseemWangoo/experiments_with_webtree/master/lib/mlin this example the  mobilenet model is working fine,so i tried to implement this code to nsfwjs model  added these script tag      <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.3.0/dist/tf.min.js""></script>      <script src=""https://unpkg.com/nsfwjs@2.3.0/dist/nsfwjs.min.js""></script>the closest i got from the below javascript code is`async function nsfwImageFilter() {const img = document.getElementById('img'),let result = [],const model = await tf.loadLayersModel('assets/model.json'),    const pixel = tf.browser.fromPixels(img)    const resized = tf.image.resizeBilinear(pixel,[299,299])    const casted = resized.cast('int32')    const expanded = casted.expandDims(0)const predictions= await model.predict(expanded).data(),for(var i = 0, i < predictions.length , i++){        console.log(predictions[i])        result.push(predictions[i]),    } return result,}`output : [2.982787221427266e-14, 2.053509825745257e-30, 1, 0, 0]pls help me understand where i am doing it wrong , because for every image the 3rd number of this array is always 1 or ~1";"[""That's strange that it keeps thinking the right model is the 3rd option.Which model are you using?  The 299x299 I see.   Have you tried the 224x224 model?Also, why are you bringing in NSFWJS if you're writing the model.predict code yourself?====="", 'iam sorry for not writing a reply for 11 days,but nsfwjs tflite model is working great,so i moved my project from flutter web to flutter android & ios applicationsthis is the error iam getting when i use 224*224 model```Error: Error: layer: Improper config format: {""node"":[{""name"":""input"",""op"":""Placeholder"",""attr"":{""shape"":{""shape"":{""dim"":[{""size"":""-1""},{""size"":""224""},{""size"":""224""},{""size"":""3""}]}},""dtype"":{""type"":""DTFLOAT""}}}...\'className\' and \'config\' must set.```=====']"
https://github.com/infinitered/nsfwjs/issues/485;[Question] Nodejs server side prediction;9;open;2021-03-18T12:52:38Z;2021-08-05T12:12:00Z;Hi, is there a way to perform prediction on server side with nodejs, without converting the local image as in the [demo](https://github.com/infinitered/nsfwjs/blob/master/example/node_demo/server.js#L13) code? I would like to directly use `fs` to read my local file and pass it into the model, is it possible? I supposed that this would largely improve the performance and reduce the running time.;"['I think images have to be converted to a tensor before they can be evaluated against the model. =====', 'Got it!Would it be better to use jpeg over png for prediction? Or any other image format you may suggest?=====', ""The fastest way to do this conversion would be to use the TFJS node lib.   I haven't updated the demo code bc of time restrictions, but I am releasing a book where I explain how to do this quite clearly:https://amzn.to/3dR3vpY====="", ""I have the same question : I'd like to perform a prediction on serverside with nodejs (firebase cloud functions).Is there anyway to do that with this package or is it not possible at all ?====="", 'It is completely possible.I guess I could improve the docs here.  But it would be awesome if someone contributed the change.I explain NodeJS image to tensor directly in my book.   But if no one can do it, I can see about setting aside some time at some point.=====', ""@kevinvangelder let me know if this is something you'd like to do.  I will, of course, help.====="", ""hi, bumping this :)I recently discovered this module and I'm interested to use it server-side====="", ""Sorry Korobaka, I'll have to find some time and add to the docs. I'll try to find some time this weekend.====="", ""> Sorry Korobaka, I'll have to find some time and add to the docs. I'll try to find some time this weekend.np! take your time :)=====""]"
https://github.com/infinitered/nsfwjs/issues/478;False positive-parrot images;4;open;2021-02-16T06:55:06Z;2021-02-16T17:24:37Z;When using Mobilenet V2 4.2mb Model, the following images show false positives[Parrot courtship dance](https://i.imgur.com/5QDpmTG.png) - Identified as Porn - 61.18%[Parrot sleeping](https://i.imgur.com/cdsTryl.png) - Identified as Porn - 59.64%[Parrot eating](https://i.imgur.com/LtR0GFZ.png) - Identified as Porn - 68.27%[Parrot moving at high speed](https://i.imgur.com/6cCgtie.png) - Identified as Porn - 95.78%[Banana](https://i.imgur.com/E4OAJri.png) - Identified as Porn - 95.21%[Lovebird](https://i.imgur.com/pkEW2xe.png) - Identified as Porn - 61.15%[Parrot's claws](https://i.imgur.com/gBPDw93.png) - Identified as Porn - 69.66%Seems to have a strange false positives rate for parrots?;"['A few additions:[Parrot calling](https://i.imgur.com/czro6qC.png) - Identified as Porn - 70.59%[Overlooking a parrot](https://i.imgur.com/bTvAEvF.png) - Identified as Porn - 91.37%[Parrot](https://i.imgur.com/7QBFg1R.png) - Identified as Porn - 69.18%[Parrot shakes its head](https://i.imgur.com/DaygdF6.png) - Identified as Porn - 51.20%[More Parrot](https://i.imgur.com/oFxJvSa.png) - Identified as Porn - 70.51%=====', 'That is strange!   How does the 90MB model perform?=====', ""> That is strange! How does the 90MB model perform?![chrome_mc5UthqZJD](https://user-images.githubusercontent.com/45941794/108090236-5b6a0b80-70b5-11eb-9dc0-35a1fdb22319.png)The situation doesn't look promising eitherI'll get more pictures to test it out====="", ""What a crazy result!   We'll have to add parrot pictures to the training data next round.=====""]"
https://github.com/infinitered/nsfwjs/issues/476;why numchannels is defined as 3, why use Int32Array;2;open;2021-02-15T06:13:55Z;2021-02-17T13:15:40Z;1. why numchannels is defined as 3, why use Int32Array ??2. what is the meaning of this line and why there is a constant value 4 ?3. whats is req.file.buffer ?;"['Which file are you looking at?  I assume a demo?=====', ""on this code for node js @GantMan const express = require('express')const multer = require('multer')const jpeg = require('jpeg-js')const tf = require('@tensorflow/tfjs-node')const nsfw = require('nsfwjs')const app = express()const upload = multer()let _modelconst convert = async (img) => {  // Decoded image in UInt8 Byte array  const image = await jpeg.decode(img, true)  const numChannels = 3  const numPixels = image.width * image.height  const values = new Int32Array(numPixels * numChannels)  for (let i = 0, i < numPixels, i++)    for (let c = 0, c < numChannels, ++c)      values[i * numChannels + c] = image.data[i * 4 + c]  return tf.tensor3d(values, [image.height, image.width, numChannels], 'int32')}app.post('/nsfw', upload.single('image'), async (req, res) => {  if (!req.file) res.status(400).send('Missing image multipart/form-data')  else {    const image = await convert(req.file.buffer)    const predictions = await _model.classify(image)    image.dispose()    res.json(predictions)  }})const load_model = async () => {  _model = await nsfw.load()}// Keep the model in memory, make sure it's loaded only onceload_model().then(() => app.listen(8080))=====""]"
https://github.com/infinitered/nsfwjs/issues/470;False results on 5/5 images/gifs?;2;open;2021-02-03T16:50:06Z;2021-02-13T10:04:37Z;testing images: http://d.snipi.sk/screenshot-nsfwjs.com-2021.02.03-17_40_54.jpgindetified as clean.. whole gif was toplesshttp://d.snipi.sk/screenshot-nsfwjs.com-2021.02.03-17_43_00.jpgindentified as neutral, nude model pose 99%http://d.snipi.sk/screenshot-nsfwjs.com-2021.02.03-17_43_38.jpgwhole gif was nude, identified only 1 framehttp://d.snipi.sk/screenshot-nsfwjs.com-2021.02.03-17_45_09.jpgidentified as porn, but no nudity at all, only (maybe) identify as sexy cuz redhead :D http://d.snipi.sk/screenshot-nsfwjs.com-2021.02.03-17_49_21.jpgidentified as hentai... no at all... ;"[""Have you tried this with the the 90% accuracy model?  I tested the top one and it caught it.I'd love to see which model you have selected.====="", ""> Have you tried this with the the 90% accuracy model? I tested the top one and it caught it.> > I'd love to see which model you have selected.as u see on screenshots, its tested on example provided with coder=====""]"
https://github.com/infinitered/nsfwjs/issues/466;How to use tensorflow from the source? I'm a beginner;2;open;2021-01-20T10:35:10Z;2021-01-23T20:51:40Z;Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2- This is the message that appears in the log file;"['no worries!  So this is for your node server?Are you using tfjs-node?  or tfjs-node-gpu?=====', ""yes, i'm using tfjs-node. I made a whatsapp bot that detects these images=====""]"
https://github.com/infinitered/nsfwjs/issues/461;minimal demo not working properly, only detect first image correctly and after that showing same result for each image;3;open;2021-01-12T09:37:27Z;2021-01-13T20:39:38Z;"I am using the minimal demo. It is working properly if I refresh the web page after checking each image and showing the correct result for each image. But if I don't refresh the page and just check multiple images using image gallery then it evaluates only the first image correctly and after that shows the same result for each image.          `const img = new Image(),           img.crossOrigin = ""anonymous"",                     img.src =imgpath,           var porn_rate = 0,           var sexy_rate = 0,           var drawing_rate = 0,                 nsfwjs.load().then((model) => {                model.classify(img).then((predictions) => {	              console.log(""Predictions"", predictions),	              predictions.forEach(function (arrayItem) {		      var x = arrayItem.className,		      var y = arrayItem.probability,		      		      if(x=='Porn'){		        y = y*100,		        porn_rate = y,		      }		      else if(x=='Sexy'){		        y = y*100,		        sexy_rate = y,	              }	              else if(x=='Drawing'){		        y = y*100,		        drawing_rate = y,	              }	          }),	      	          console.log(""porn rate: ""+porn_rate),              console.log(""sexy rate: ""+sexy_rate),              console.log(""Drawing rate: ""+drawing_rate),      	  if(porn_rate>40 || sexy_rate>40 ){           	  alert(""It seems that you have uploaded a nude picture :-(""),       	  }			}),    }),`I have checked all issues here but could not get any issue similar to mine. Thanks.";"['Taking a look at your code, you might need to wait for the `image.onload` event to fire before accessing the image.How are you calling this more than once?  This code looks very ""one time"" to me.=====', 'https://user-images.githubusercontent.com/29511058/104450715-569bce80-55c2-11eb-9222-5baad7f5a72a.mp4@GantMan  Please have a look at this video. I hope it will help you to understand the issue. so there is no loop actually but when I select the image each time from the gallery (you saw in the video) it changes the image name. I have checked by using the console and the image name is changing each time I select a new image. The rest of the code is the same you saw above. Please guide me where I am wrong.```document.getElementById(\'setimagename\'+pointer).value  = idBRzero,var imgpath = \'<?=sUPLOAD?>\'+idBRzero,//console.log(imgpath),const img = new Image(),img.crossOrigin = ""anonymous"",img.src =imgpath,......same as above code.......```=====', 'There\'s a small chance that the image is ""tearing,"" meaning that you\'re sending it to the model before it\'s finished loading.  Usually this will cause an issue with the first classification, though and even error, so that might not be the issue.  Can you load the image and await the onload event? my _guess_ is that your first image loads faster than the NSFWJS model, so it works the first time.  Once the model is loaded, each subsequent classification happens before the image is fully ready.```          const img= new Image(),          img.crossOrigin = \'anonymous\',          img.src = imgpath,          img.onload = () => {            model.classify(img).then((predictions) => {              ...            }          }```=====']"
https://github.com/infinitered/nsfwjs/issues/459;False positive drawing;13;open;2021-01-07T13:18:53Z;2021-01-12T16:32:30Z;![SPOILER_Matou_shinji-1-67-1.png](https://user-images.githubusercontent.com/50463727/103897058-3f0ca380-50f3-11eb-9578-8fad802bdb3f.png);"[""The ai classify's this as hentai(Latest mobilenet model)====="", ""Which model did you use?    I'm dragging the image to NSFWJS.com and can't reproduce with the 90% mode or the 93% model.![2](https://user-images.githubusercontent.com/997157/103913570-4bcad080-50ce-11eb-989b-40278c4ec53d.jpg)![1](https://user-images.githubusercontent.com/997157/103913574-4c636700-50ce-11eb-966d-63b200faf26e.JPG)====="", 'I used the 93% oneI set the jpg quality of images to 60 before scanning them using nsfwjs=====', 'Can you provide the image you used?And yes, false positives will happen.=====', '![SPOILER_Matou_shinji-1-67-2.jpg](https://user-images.githubusercontent.com/50463727/103941650-75681400-512f-11eb-8b88-8b594031607c.jpg)=====', ""for whatever reason I don't have this false positive on the website while I do on my codemaybe its because I use tf prod mode====="", ""From my testings, I have noticed that this issue is with the latest released model from the nsfwjs_model repowhen I used a previous release it didn't have this false positive====="", ""@TechnikEmpire - would know what's up.https://github.com/TechnikEmpire====="", ""@LINKdiscordd link directly to the model you pulled and I'll check it out to see if I can repro.====="", 'Just to be clear this could very well be a false positive. Latest models were trained by me with a vastly different data set than previous models. So imo such ""bugs"" can be expected, if they\'re defined simply by comparing against previous models. There will always be a margin of error. Even whether we get 70% or 99% ""accuracy"", those numbers only apply to the dataset we trained and tested on, not the billions of images in existence.I\'m still happy to verify this though if you link to it. I\'m asking for a link so there\'s no error in verifying. =====', 'https://github.com/GantMan/nsfw_model/releases/download/1.2.0/mobilenet_v2_140_224.1.zipIts the model from the web model folder which has this false positive=====', 'I can confirm that this is just a false positive. =====', 'With relatively low confidence I might add, but still a false positive. 57% confidence.=====']"
https://github.com/infinitered/nsfwjs/issues/455;Using Image;1;open;2021-01-03T11:10:18Z;2021-01-04T19:36:43Z;Hey there!I am using your package for Discord bot using [discord.js](https://discord.js.org)I do the following thing```js  msg.attachments.forEach(async(Attachment) => {          let lol = await canvas.loadImage(Attachment.url)    })```But I get error -```(node:7766) UnhandledPromiseRejectionWarning: Error: pixels passed to tf.browser.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData in browser, or OffscreenCanvas, ImageData in webworker or {data: Uint32Array, width: number, height: number}, but was Image```what can I do else except loading image??;"[""If you're looking to load from a URL, you have to first load the image into an image object.Take a look at:  https://github.com/infinitered/nsfwjs/blob/master/example/minimal_demo/index.htmlHowever, I'm considering adjusting the code so that if a string is provided, perhaps I can do this for you automatically?Which do you prefer?  Would you want to pass an image, or do you want me to automatically handle URLs and access images for you?   The problem with the second option, is that it might need to be specific to URLs vs local URIs like `file://`=====""]"
https://github.com/infinitered/nsfwjs/issues/451;Quantized model doesn't detect porn/hentai;5;open;2020-12-18T18:20:31Z;2021-01-05T01:27:58Z;I am using the latest release of the nsfw detection model and when I tried the quantized version it just didn't detect any nsfw images at all while the normal model did;"[""Can you specify what size/link to the model you're using?  Maybe email me the images that it is failing on?Are you sure this is for all NSFW images?  Or maybe just a few from your testing?====="", 'Can you provide the failing issues?=====', 'This is for all nsfw imagesalso its the x244 size oneI have put the  models that I use in my own repo called nsfwjs_model=====', 'I just google image searched for pornographic and sexual images.   Then dragged them onto https://nsfwjs.com/ and they all were detected.=====', 'But those models are not the quantum versionsMy testings failed with quantum versions of the latest mobilenet x244 model=====']"
https://github.com/infinitered/nsfwjs/issues/435;bad memory leak;6;open;2020-11-07T08:48:07Z;2021-02-23T15:52:11Z;Hello, I'm having a bad memory leak in production after implementing nsfwjs. I'm talking gigs of memory.```jsconst axios = require('axios')const tf = require('@tensorflow/tfjs-node'),tf.enableProdMode(),const nsfw = require('nsfwjs'),const modelFunc = async () => await nsfw.load(),let model,setInterval(() => {    tf.disposeVariables()console.log(tf.memory()) // { unreliable: true,  numTensors: 49, numDataBuffers: 49, numBytes: 532830292 } and keeps increasing}, 5000)async function scan(url) {    if (!model) {        console.log('no model, loading.....')        model = await modelFunc(),    }    const pic = await axios.get(url, {        responseType: 'arraybuffer',    }),    tf.engine().startScope(),    const image = await tf.node.decodeImage(pic.data, 3)    const predictions = await model.classify(image)    image.dispose()    tf.engine().endScope(),    return predictions,}for (const nsfwURL of [...(alex000kim / nsfw_data_scraper_chunk_ofurls)]) { //a chunk of nsfw image urls    scan(nsfwURL).then(e => console.log(e))}```I sometimes get `TypeError: Cannot read property 'backend' of undefined`, _(@tensorflow/tfjs-core/dist/tf-core.node.js:3280:31)_Either way, the memory is never free'd.What am i doing wrong?;"[""It's a bit confusing the way this is constructed, but I see you're doing a lot of good things.The thing I'm confused by, is why are you `tf.disposeVariables()` on an interval?   Does that cause the model to have to reload?I'm not familiar with startScope/endScope.  Are you sure you should be managing the global engine like that? Do you have an open source GitHub repo you can link me to, so I can pull down the code and run it and see if I can fix it?====="", '@GantMan It\'s actually a worker that exposes the ""scan"" function, the loop part is just to show how I\'m supplying the urls.I used tf.disposeVariables() in an interval just to see the effect it would have on memory and also the results of `tf.memory()`,I had tf.memory() return a VERY high numTensors number (over 500). tf.disposeVariables() reduced it and only few are returned now (around 50), The memory leak however wasn\'t fixed.`<image>.dispose()` ( or `tf.dispose(image)` ) had absolutely no effect sadly.As for `tf.engine().startScope()` and `tf.engine().endScope()`, I noticed online:> The way to clean any unused tensors in async code is to wrap the code that creates them between a startScope() and an endScope() call.I do admit, the two might be the cause of the error I had mentioned aboveEither way, the memory leak was never resolved.The code shown above is enough to actually reproduce this behavior on node=====', ""Hrmmm, TBH, I'm not seeing the source.  Perhaps the model is getting created over and over?   You could try `model.model.dispose()`.  But I'll have to create a downloadable runnable debuggable demo to find it.   I don't think it's the library, but I could be wrong, of course!  Strange stuff indeed.If you have a small, open source, ready to run demo I'll pull it down and throw an hour at it.If I were you, I'd comment out line by line and watch tensor memory with a log.====="", 'Was this resolved?=====', '<img width=""447"" alt=""Screen Shot 2021-02-23 at 8 34 56 AM"" src=""https://user-images.githubusercontent.com/32657584/108858660-0563ff00-75b2-11eb-9571-f7bad6696c1d.png"">I have same problem with this too. The `image.dispose()` seems doesn\'t work.=====', 'Can I see some code?  Are you using ts-node?  =====']"
https://github.com/infinitered/nsfwjs/issues/434;Supported File Types;4;open;2020-10-30T19:25:38Z;2020-11-01T16:29:26Z;Is there a list of file types that can be scanned? For instance not just traditional image files, but also file types like PDFs, docs, txt, etc.;"[""Canvas elements/JPG/PNG/GIFs for right now.I do like the idea of being able to scan PDFs/docs etc, but you'd have to find a way to extract images from those filetypes and run their images through one by one.What were you looking for it to catch in a txt file?====="", ""@GantMan I suppose it doesn't make sense to extract and scan images in a .txt file, as that's not possible. The main issue I'm trying to solve is ensuring individuals aren't storing inappropriate images in other types of files than the ones you've mentioned. File types like PDFs, word docs, PowerPoints, etc.====="", '@GantMan thinking through extracting images from the file... would the process be first identifying images in a document, then classifying the image?=====', 'For PDFs* https://stackoverflow.com/questions/18680261/extract-images-from-pdf-file-with-javascript* https://www.npmjs.com/package/pdf-extractor=====']"
https://github.com/infinitered/nsfwjs/issues/431;Prettify library for gif filtering;1;open;2020-10-25T20:21:01Z;2020-10-25T21:07:51Z;Related #401I just drop my thoughts and ideas so I wouldn't forget itRecently we started to use https://github.com/nsfw-filter/gif-frames for gif filtering because of speed and fps flexibility. It's pretty fast, but not for [NSFW-Filter](https://github.com/nsfw-filter/nsfw-filter) :smile: So, I suggest:1. Unite 3 libs:- https://github.com/nsfw-filter/gif-frames (3 y.o lib \ MIT License \ ~100 lines of code)- https://github.com/nsfw-filter/get-pixels (7 y.o lib \  MIT License \ ~100 lines of code)- https://github.com/nsfw-filter/save-pixels (8 y.o lib \  MIT License \ ~100 lines of code)into one library specific for our case. Not sure about Licenses, but we use it as open source, I hope we are fine :D2. Remove redundant code of new library (jpg, png stuff)3. Re-write in JS ES6+(remove `var` and etc)4. Add unit-tests and benchmarks5. Check for performance, event loop blocking and memory leaksEstimates: ~21-34 hrs (fibonacci)Probably I'll take it in the next couple months;"[""One of the things worth noting, if you're decoding pixels/encoding pixels for node, I've been using the provided Node API - https://js.tensorflow.org/api_node/2.6.0/ which is built in.=====""]"
https://github.com/infinitered/nsfwjs/issues/430;Supporting model load from indexeddb;2;open;2020-10-24T21:42:36Z;2020-10-25T03:30:21Z;"https://github.com/infinitered/nsfwjs/blob/ebcd41c46087a3f42c6577f96acc53d7a934b068/src/index.ts#L68Hello, it seems, although not explicit I can save the model to different schemas by referencing the underlying ""model"" attribute in the model returned by `nsfwjs.load()` e.g. `nsfwjs.load(path).then(function (newModel) {        console.log(""path"", path),        if(newModel) {           newModel.model.save('indexeddb://' + SOME_KEY),        }       }).catch(error => {        console.error('onRejected function called: ' + error.message),      })`However, I am unable to then reload the model from say an `indexeddb` location using:nsfwjs.load('indexeddb://' + SOME_KEY) as the line above in the code just checks for a string, assumes it's a url or relative path, rather than another schema location and them appends 'model.js'.I think tf.js supports loading and saving to other schemas, it'd be cool if NSFWJS followed suit ...";['Agreed.  I think the best plan of action would be URL unless it detects something like `indexeddb:` or even `localstorage:`Would you be interested in providing the PR with updated docs/tests/code?  =====', 'reference for whomever provides the code:   https://www.tensorflow.org/js/guide/save_load=====']
https://github.com/infinitered/nsfwjs/issues/428;wasm?;1;open;2020-10-23T15:43:04Z;2020-10-23T15:50:53Z;Any plans on packing into wasm?;"[""You can set the backend for TFJS to WASM and this should work.Here's a blog post on how to do so with create react app:  https://shift.infinite.red/adding-tensorflow-js-wasm-backend-in-create-react-app-f57f5baab736IF you'd like to add a WASM example to the site and a test to assure WASM is properly supported, that would be an epic contribution!  If for some reason this doesn't work with WASM, I'd love to fix that.=====""]"
https://github.com/infinitered/nsfwjs/issues/376;python;1;open;2020-07-20T03:18:28Z;2020-07-20T19:03:59Z;Can you directly provide a python callable interface? JS won't work. Thank you;['This should help:  https://github.com/GantMan/nsfw_model=====']
https://github.com/infinitered/nsfwjs/issues/370;Can I save the model to a browser locally and run it?;10;open;2020-07-08T07:01:09Z;2020-07-16T19:29:17Z;Hi,I am trying to use nsfwjs with TensorFlow JS to build an app. I am pretty new to this so I was wondering how I can use the model locally. i.e I need to be able to download the model files to my project folder and use it by importing it into a JS file. I see that I can use the model that you have hosted in S3. How can I go about without such hosting?I want to prevent loading the model every time the user uses my app. Instead I plan to store the model on his device locally. That way, it needs to load the model only the first time it is used. Please help!Thanks!;"['Yup!   You can save it locally, I actually prefer that.   ALSO the browser will cache it!See this section on how to tell the library where the model is located:  https://github.com/infinitered/nsfwjs#load-the-model=====', 'So like, I can just download the file there and add it to my project folder and just provide the url to the model?I have just installed tfjs and nsfwjs using npm. So I should just import the files and it would work fine?Sorry, I am a noob at this, I could not find any solution anywhere else=====', 'No problem!  It can be confusing :)When you say ""locally"" do you mean you\'re copying the files to your server, or do you mean you\'re making a web page on your desktop?You can just put these files in a folder, and point it there:  https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/quant_mid=====', 'I am using it to build a chrome extension. So it would be in my desktop and not on any server. I will try this out and check if it is working!Thank you so much!=====', 'No problem!  Put the files in a models folder and don\'t forget to set `""web_accessible_resources"": [""images/*"", ""models/*""],`in your manifest.json so the script can access them.=====', 'I will try that!=====', 'Yes! It seems to work! Thanks!=====', 'Would it be okay if I mention you guys in a LinkedIn post. I would like to share my project and I would like tag you guys. Could I do it?=====', 'yup!!!=====', 'https://www.linkedin.com/posts/navendup_navendu-pottekkatnsfw-filter-activity-6689531508220411905-lUw2=====']"
https://github.com/infinitered/nsfwjs/issues/342;Compiling model;3;open;2020-05-16T21:33:47Z;2020-05-28T14:44:27Z;Hey,Thank you for all the efforts on this library.So I downloaded the models from https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 and was able to use them with nsfwjs using `graph` option.I just wanted to know how can I compile these models from graph to image (299x299)? I know I can just download from your S3 bucket but I don't want to increase your network costs since I will be deploying the same to cluster of servers so I was looking for an optimum solution that could help me.Also, I just noticed that you've deployed a new model (mobilenetMid) with similar accuracy but decreased size, any tips on how to compile new model for backend (nodejs) use?;"[""Hi!  I'm not sure I understand the question.   > I just wanted to know how can I compile these models from graph to image (299x299)? I know I can just download from your S3 bucket but I don't want to increase your network costs since I will be deploying the same to cluster of servers so I was looking for an optimum solution that could help me.Can you elaborate?====="", ""Basically, I am using nsfwjs with `type: graph`:`nsfwjs.load('/path/to/different/model/', { type: 'graph' })`This process takes upto 150MB/worker in my nodejs application which is a lot tbh.I just want to know how can I load model using `size: 299x299` option. I am hoping that this will significantly reduce the memory usage.I do understand that `size` option won't work with https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 models directly and I might need to convert these models before making it work so I just wanted to know how can I convert these models from graph to image?====="", ""The size option depends on the model you load.    You won't be able to use the graph with 299, because it was trained on 224 data.That is a lot of space.  I wonder what's really taking up that process space.   Have you experimented on loading smaller models?You can create a 48kb model using this page I made:  https://rps-tfjs.netlify.com/I'd be interested in seeing the process memory for that.=====""]"
https://github.com/infinitered/nsfwjs/issues/324;Confused with the models - false positives;28;open;2020-05-01T11:51:46Z;2021-01-03T13:19:34Z;"Hello,I'm using these model files:https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/model( I think it's the same as https://s3.amazonaws.com/nsfwdetector/min_nsfwjs.zip )and I'm having several false positives.100% non-adult face pictures get flagged as 90%+ pornI tested also with the non-min model: https://s3.amazonaws.com/nsfwdetector/nsfwjs.zipBut, from the few examples I tried, it didn't change much at all, and some were even worse.I see that in this issue: https://github.com/infinitered/nsfwjs/issues/276You added some new ""DAG 93"" model.Is this one supposed to have better accuracy?What model is the best, and what should I use for best accuracy, please?What is the difference between using the model provided in README (first link above),and using the DAG one with { type: ""graph"" } ?Are there pros and cons?Thank you very much.";"['The new one is supposed to have better accuracy.Use the DAG 93.  Can you provide some false positives here?   I can elaborate on the pros/cons, but I want to see if it fixes your issue first.And sorry, I think that s3 link is outdated.=====', 'Hello,Thank you.I\'ve just updated to the latest version of NSFWJS, and then loaded the DAG 93 model.Tested against a number of false positives, and the ""porn"" % decreased to much more acceptable values for sure.However, I tested a few images that are 100% porn and they would give 99% on the older model, but with the DAG93 they give 72.95% for one, 56.37% for another and 34.13% for the other, which doesn\'t make sense. There\'s a 4th image that I tested that it\'s not porn, but a semi-naked lady, and that one gave 89.48%.With my examples, the new model just doesn\'t seem accurate at all.Happy to give you examples, please give me your email so I can send privately to you.=====', ""Our latest model was provided by @TechnikEmpire so I'm tagging em here.My email is Gant and the domain is infinite.red====="", '## **NSFW Post!!!**My post is NSFW because I\'m going to use some real talk here to get to the nitty gritty details of what I did differently.Alright so with the very latest model that I trained, there was a significant change.## **Changes to Training Data**I used the same training data that @GantMan used in older models, but I decided to clean it up. What I did was I loaded Yahoo Open NSFW model and I deleted all images from the `porn` category that had a classification score of `10%` or less.I then used Yahoo model to move all images out of the `sexy` category into the `porn` folder that had a score of `60%` or more.This translated into several thousand files for each category. There were thousands of files in the `sexy` category that were full blown pornography. Images of sexual penetration, just plain snapshots of porn movies, full body nudity, close up of genitalia. That\'s not `sexy`, that\'s porn.There were probably a couple thousand images in the `porn` folder of bukakke. As humans we recognize what this is, but really it\'s just a closeup of a face. To a neural network it\'s just a face, and Yahoo\'s Open NSFW model confirmed this by classifying them as non-pornographic. Those images were blown away, so I\'m really surprised that plain faces are coming up as porn.There were also a couple thousand images of extremely obscure pornography. I took a peek and some of them I was like ""this is a completely benign image"", then I\'d see a 3x3 pixel area of a penis hanging out of someone\'s pants. This is going to throw off the neural network, at least for training purposes. Those images were blown away.The `sexy` category should now only really trigger when there\'s provocative images. i.e. someone rolling around on a boat in a thong and bra. Nudity should come up in the pornography category.## **Remarks About Changing Accuracy**With regards to the scores for categories changing, this isn\'t necessarily a bad thing. The way this model is to be used is that the highest scored class wins, and that\'s how neural networks like this are scored as well. Top-1 and Top-5 accuracy. Even if a porn image is split up like so:```Porn: 21%Sexy: 20%Neutral: 20%Drawing: 20%Hentai: 19%```The model is still accurate. It\'s not even necessarily indicative of a problem in the neural network.## **Conclusion**What I believe happened here is twofold. First, I believe I\'ve overfit the model by over-training it. I didn\'t notice this before, but your remarks made me take a second glance and it seems to be a bit overfit.If you look at the posted Tensorflow output on [this issue](https://github.com/GantMan/nsfw_model/pull/56#issue-410906105), you can see that train loss is decreasing while validation loss is increasing in the final train iteration.Second, one-byte quantization would exacerbate this issue.I think I\'ll run a new training session and get a new model published with one less iteration because it seems it was that last iteration that pushed the model over the edge.Thanks for bringing this up!=====', ""Oh yeah I forgot, I also started the training session with training just the final softmax layer, then fine tuning for 5 consecutive sessions, then 2 more iterations with a highly reduced LR. I'll just train it the way I did the previous model and report back.====="", ""@GantMan - just sent the email. Please let me know if you didn't receive.Thanks @TechnikEmpire - please keep me updated!====="", ""I've started training so sometime this evening I'll do a PR on the model repo.====="", ""I got the email @ghnp5  thanks so much.@TechnikEmpire - thanks for checking for overfitting!  I'll look forward to your update.====="", 'Hey - any news? :)Thanks!=====', ""Yeah I've retrained, I'll check your submissions against the new model before I publish.I also got side tracked cause I'm not happy with this 93% ceiling fine tuning so I'm training mobilenet v3 large from scratch.====="", ""Great work  guys!!!```Porn: 21%Sexy: 20%Neutral: 20%Drawing: 20%Hentai: 19%```I am fairly new at this and I don't know what a good confidence score for each of the categories is.I have used the saved_model.tflite  found https://github.com/GantMan/nsfw_model/releases/tag/1.1.0  on Android/IOS with impressive results. However, I still don't know how to choose a good confidence score.====="", 'Hello,Any news about this? :)Thanks!=====', ""@ghnp5 Yeah sorry for going MIA, I got tied up with a bunch of stuff. I have trained new models, I'm just coordinating reviewing your submission against them.====="", ""@ghnp5 If there's an urgency, you can simply revert back to this model:https://github.com/GantMan/nsfw_model/releases/download/1.1.0/nsfw_mobilenet_v2_140_224.zip====="", ""I'm super excited to see your new model @TechnikEmpire !   Lots of people use NSFWJS and I'd love for your advanced model to be the de facto standard.====="", ""> Great work guys!!!> > ```> Porn: 21%> Sexy: 20%> Neutral: 20%> Drawing: 20%> Hentai: 19%> ```> > I am fairly new at this and I don't know what a good confidence score for each of the categories is.> I have used the saved_model.tflite found https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 on Android/IOS with impressive results. However, I still don't know how to choose a good confidence score.Those scores were just an example. You just take the highest valued class and accept that. Don't get into thresholding the values. Simply take the neural network's prediction at face value.====="", ""Here are the confidences on a new model for your submission @ghnp5 :For unsafe submissions:```BAD:  F: 80.39.jpg Confidence: 0.95885BAD:  F: 93.20.jpg Confidence: 0.998286BAD:  F: 93.79.jpg Confidence: 0.993065BAD:  F: 98.26.jpg Confidence: 0.671015BAD:  F: 99.07.jpg Confidence: 0.718538BAD:  F: 99.23.jpg Confidence: 0.998276BAD:  F: 99.24.jpg Confidence: 0.596281```For safe submissions:```BAD:  F: 70.99.jpg Confidence: 0.428323GOOD:  F: 96.52.jpg Confidence: 0.908221```As you know, the file `70.99.jpg` is black and white. This will throw off any neural network, which is why colorization before inference is an extensive topic with various techniques.Here's what happens to the scoring when I colorize that image:```BAD:  F: 70.99.jpg Class: 1 Confidence: 0.428323GOOD:  F: 96.52.jpg Class: 2 Confidence: 0.908221GOOD:  F: 70.99-Color.png Class: 2 Confidence: 0.827742```For future reference, it's very difficult to gauge any network with a few images. This is why we split out 10% or more of our total data set for validation. According to that split ratio (against tens of thousands of images), Tensorflow tells us that the latest model is ~92% accurate. While this is a really good accuracy, note that 8% between where we're at and perfection (100%) is quite the chasm, so you're definitely going to see false positives and false negatives.I'll be uploading the newly trained model for @GantMan to my attached PR here:https://github.com/GantMan/nsfw_model/pull/60I just need to convert it to tfjs first.====="", 'New model is attached to that linked PR. For greater clarity, my experimentation (the scores given above) are based on a FP16 version run through OpenCV::DNN, not the web or quantized web versions of the model.=====', 'Hi, I\'m fairly new to neural nets and this library, I tried using your model and I keep on getting the same error ""nsfwjs.min.js:1 Uncaught (in promise) Error: layer: Improper config format:"".Do you know what I\'m doing wrong? I have tried loading it without the {type: ""graph""}  parameter and the other models have worked, Thanks!Here\'s how I\'m trying to load the model:```this.nsfwjs.load(this.modelURL, {type: ""graph""})```EDIT: I\'ve just realised I was using an outdated version of nsfwJS, Thanks for the model!=====', ""I'm also new to the entire subject of preventing nsfw images uploaded by users entering my app without any sort of check for porn or violence. Now, I've read through the blog post here https://shift.infinite.red/avoid-nightmares-nsfw-js-ab7b176978b1 this github readme page as well as https://github.com/GantMan/nsfw_modelHowever, I'm still in the fog on what model file is right for me and where to get it from? What's the difference among the various model files eg normal vs graph? Could someone maybe add two or three lines to the readme page because I reckon that's something people new to the subject are struggling to understand in general?! 😃====="", ""@evdama I'm still using this one:https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelIn my experience, this one works the best.The thing is that you cannot 100% rely on any of the models.What we do is that if it detects 70%+, we show a popup to the user and ask them to confirm that this is not inappropriate, or otherwise don't proceed.====="", ""ha! I was just afk making a tasty ☕️  and already got two great answeres 👍 (those guys must be in lockdown as well so... 😂)@TechnikEmpire Please, like you'd speak to a very motivated but unexperienced puppy, where do I get those files and how do I use them? Is it the entire .zip I found or just one of contained files or the entire collection of shards 🤔 ? And once I have the right file/model, I'd just put it inside my Sapper's `public/` folder and reference it with the `.load()` right? @GantMan Can you add a line or two to the readme so that puppies know what to do with regards to model files (and which one is the right one for a certain use case e.g. images vs videos)? 🤓====="", 'Someone answered here, but seems the reply is gone. They were saying that the model I\'m using is very outdated and that there\'s a new one with 98% accuracy.Where can I find it?The README of this repository points to the model I\'m using, in the ""Host your own model"" section:https://github.com/infinitered/nsfwjs#host-your-own-model=====', ""> Someone answered here, but seems the reply is gone. They were saying that the model I'm using is very outdated and that there's a new one with 98% accuracy.Yup, it was @TechnikEmpire so I assume he'll come back with an even better answer...====="", ""> > > @evdama I'm still using this one:> > https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/model> > In my experience, this one works the best.> > The thing is that you cannot 100% rely on any of the models.> What we do is that if it detects 70%+, we show a popup to the user and ask them to confirm that this is not inappropriate, or otherwise don't proceed.I use this model in my chrome extension, from what I remember it was the best however it looks like it's trained on professional porn and not more _home-made_ as where my chrome extension is made for omegle, there are many people with bad lighting or blurry cameras and it doesn't detect them very well sadly====="", ""The site at https://nsfwjs.com/ uses the 93% accuracy model, which I'm pretty sure I tried in the past and got worse results than I get with the model I'm currently using.====="", ""I posted something and then thought better of it. I am the author of a closed-source program that uses such models and there are dirtbags that follow me on github and steal my oss so I'm doubly inclined not to help anyone.However, I was the guy making big overhauls to this repo and for some reason @GantMan stopped merging my pr's.https://github.com/TechnikEmpire/nsfw_modelHas stats for every kind of model I trained. Dunno if my site links are gone or not. But basically you just need to manually clean up the categories and the use the new tf2 api that leans on tfhub that I integrated and you'll hit much better numbers.====="", 'Sorry not ""this"" repo, the model repo that drives this project.=====']"
https://github.com/infinitered/nsfwjs/issues/305;classifyGif() - server side;2;open;2020-04-19T02:51:14Z;2020-04-20T07:51:57Z;"Hello,Thank you very much for implementing NSFWJS, which works very well!I use this as a Node.JS microservice app, doing Server-Side validation of photos uploaded to my website.Works well for JPG images. The Node app retrieves an image from a URL, converts to buffer, runs tf.tensor3d(), then classify().However, I am not sure how to classify GIF images, server side.From looking at your code, it appears that the ""gif"" parameter must be a DOM img element.Is it possible to classify GIF images server side?I suppose I can manually go through every frame of the GIF and run classify(), but if this could be natively implemented in NSFWJS, that would be perfect!Have a great day. Thank you.";"[""I haven't run into the need to do this server-side just yet.   I'd love to get a pull-request and merge the code so NSFWJS does this, would you be willing to solve it and send the code?====="", ""Unfortunately, I won't have the time to do a PR, but this is the way I got around it myself, using gifFrames:\t\t\tgifFrames({\t\t\t\turl: dataBuffer,\t\t\t\tframes: 'all',\t\t\t\toutputType: 'jpg',\t\t\t\tcumulative: true\t\t\t},\t\t\tasync (err, frameData) => {\t\t\t\tif (err) {\t\t\t\t\tthrow err,\t\t\t\t}\t\t\t\tframeData.forEach(function (frame) {\t\t\t\t\tconst jpgStream = frame.getImage(),\t\t\t\t\t... convert and classify, and add to an array\t\t\t\t}),\t\t\t}),=====""]"
https://github.com/infinitered/nsfwjs/issues/271;Buffer support [assistance];5;open;2020-03-17T13:41:46Z;2020-09-03T16:47:49Z;"Hello, I need to **fetch** images and pass them to `.classify()`.The images i'll be fetching have **.png** format.```const tf = require('@tensorflow/tfjs-node'),const fetch = require(""node-fetch""),const nsfw = require('nsfwjs'),const model = await nsfw.load(),let r = await fetch(IMAGE)let buffer = await r.buffer(),const readImage = tf.node.decodeImage(buffer)const predictions = await model.classify(readImage)return predictions```I used https://js.tensorflow.org/api_node/1.7.0/#node.decodePngBecause it's stated that **// Image must be in tf.tensor3d format**.**readImage** does return:```Tensor {  kept: false,  isDisposedInternal: false,  shape: [ 128, 128, 4 ],  dtype: 'int32',  size: 65536,  strides: [ 512, 4 ],  dataId: {},  id: 4402,  rankType: '3',  scopeId: 2430}```but **model.classify(readImage)** returns:**Error: Size(200704) must match the product of shape 1,224,224,3**What am I doing wrong?";"[""Hiya!The problem might be this hardcoded 3https://github.com/infinitered/nsfwjs/blob/master/src/index.ts#L129But to be sure I'd need to run your code to be 100 sure.Can you try modifying this 3 to a 4 locally and see if your code passes.  If so, we can see about fixing a dynamic alpha channel in the code.If this ISN'T the problem, I'd like you to make a small public repo so I can hunt down the bug.====="", '@GantManI did try to change it to 4 but I recieved `Error when checking : expected input_1 to have shape [null,224,224,3] but got array with shape [1,224,224,4].`Changed it in https://github.com/infinitered/nsfwjs/blob/master/src/index.ts#L74 as well and recieved the same errorUhm, do I just upload the code I posted above into a repo? =====', ""You don't have to.  I didn't know if you had a nice example setup.  Like specific with URL so I could just pull down the code and try to debug.  If not, I could create one.====="", '@GantMan This is literally what I\'m using :D```const nsfw = require(\'nsfwjs\'),    tf = require(\'@tensorflow/tfjs-node\'),    fetch = require(""node-fetch""),async function predict(imageURL) {    const model = await nsfw.load(),    let imageFetched = await fetch(imageURL)    let imageBuffer = await imageFetched.buffer(),    const readImage = tf.node.decodeImage(imageBuffer)    const predictions = await model.classify(readImage)    return predictions}await predict(\'https://4al52k24l8r51wpym5i46ltd-wpengine.netdna-ssl.com/wp-content/uploads/sites/2/2020/02/GettyImages-1199242002-1-1024x576.jpg\')```Node: 12nsfwjs: 2.1.0@tensorflow/tfjs-node: 1.7.0=====', ""Hi @lateiskate and @GantMan, I know is a little bit late response but I had the same problem so the way that solved (I don't know if is the best solution) is using a lib [sharp](https://sharp.pixelplumbing.com) to convert the png buffer to jpg.I think the problem is when use **tf.node.decoImage(buffer)** that will return **tf.Tensor3D** or **tf.Tensor4D** but the method **infer()** use **tf.Tensor3D** so I tried to cast **decodeImage()** as **Tensor3D** but didn't work.=====""]"
https://github.com/infinitered/nsfwjs/issues/258;sliding frame;3;open;2020-02-21T00:26:54Z;2020-04-24T03:37:31Z;I'd love to be able to add support for a sliding frame.  I know I could do this outside the library, by cropping the image multiple times, or potentially at the buffer level.  My gut says the performance would be significantly improved if I pushed it down into tensorflow though (is that possible?  Is https://js.tensorflow.org/api/latest/#signal.frame the right way to do it?).I'm willing to put some leg-work in--but I'd really appreciate some guidance or if anyone has resources describing how to do it with a tensorflow.js implementation.;"[""You're right!  I think Signal frame is the way to go.What do you foresee as the API?You call a frame based function and then pass the params for the frames?   Would it then average the frames?  Or give a report back like the GIF implementation?I think sliding frame would be an excellent addition!====="", 'Well--with my limited understanding--I\'d want to be able to define the window--it\'s size and how it moves.  I have no idea if this is the ideal way to handle this...  but passing in an array of ""sliding window definitions"" like so:``` [{ width: 300, height: 300, moveX: 250, moveY: 250}, { width: 800, height: 800, moveX: 600, moveY: 600}]```width - defines the width of the windowheight - defines the height of the windowmoveX - determines how far the window is moved in the x axis when it ""slides"" overmoveY - determines how far the window is moved in the y axis when it ""slides"" downI think an array would be ideal because if you\'re dealing with large resolution images you could pick up on small and large portions of the screen as ""nsfw"".  You could make the array optional and generate it if not provided based off the input image\'s width/height.I\'ll need some more time to research how to tie in Signal Frame--if you have any thoughts please share.One thing I\'m not sure about--how do I go from a tf.Tensor3D (which is what img is in `infer`), to then tf.Tensor1D which is what `tf.signal.frame` uses?  It looks like `tf.split` *might* be what I want.Also--why is img 3D instead of 2D?=====', ""Images are 3D bc widthxheightxRGBI'll try to circle back on sliding frame at some point.=====""]"
https://github.com/infinitered/nsfwjs/issues/173;Create iOS and Android app icons for NSFWJS;4;open;2019-09-26T16:00:43Z;2019-10-01T12:39:49Z;We'll be using NSFWJS in a React Native app, so we'll need the appropriate icons.iOS: https://developer.apple.com/design/human-interface-guidelines/ios/icons-and-images/app-icon/Android: https://developer.android.com/studio/write/image-asset-studio;"['I can make that icon for you. Both iOS and Android with Legacy and Adaptive versions. Do you have any suggestions? Should I use the favicon of your site or official logo?=====', ""The favicon is good.   If you have any design skills, maybe we can make the logo mix with the favicon?   I'll take anything passable :)====="", ""@dotvhs - You can probably send your icons here:  https://github.com/infinitered/nsfwjs-mobile/issues/3We'll be hosting the React Native project there.====="", ""@GantMan I'm mostly designer, I can help making a new logo :)=====""]"
https://github.com/infinitered/nsfwjs/issues/101;How to classify video?;4;open;2019-05-01T16:30:59Z;2021-12-02T11:18:30Z;Hello. I have a video uploader. I want to classify more than first video frame. Is it possible?Thanks.;"['Hey @Vista1nik, just throwing some idea in the air, might it be possible to convert the video to a gif and then use the module? This might take a long time though (converting + classifying).Another idea might be taking some of the frames (every second?) and checking them. Clearly not perfect=====', ""Take a look at the code I started in ticket #39 .  I'm still hoping someone will take this on:https://github.com/infinitered/nsfwjs/issues/39====="", 'Or can it be like first convert video to image, then image to Gif? Because direct Video to Gif maybe a little tough than the image to Gif=====', 'you can cover video to image. like ffmpeg4 or canvas=====']"
https://github.com/infinitered/nsfwjs/issues/94;5-6 seconds - or more, before model load and client (user) can use the page - how to improve?;34;open;2019-04-23T14:41:50Z;2020-05-21T01:02:46Z;I'm currently testing this interesting nsfwjs module on a live server.I activate this code:pdModel = await nsfwjs.load('model/'),via a call to an async function triggered by the onLoad event. The 6 model files are fetched from the disk and is using approximately 960ms to load (I see this using Chrome inspector - Network tab).After this I have to wait approx. 5-6 seconds before I get any response from the page. From what happens in the Memory tab in Inspector, it looks like the nsfwjs load is using this time to do something. After the 5-6 seconds. I can select an image. When that is done, this code (in another async function):var img = new Image(),img.src = document.getElementById('img_url').src,var predictions = await pdModel.classify(img),console.log('Predictions', predictions),is using about 1,5 - 2 seconds to finish. This is acceptable time for me.This time usage of about 1.5 -2 seconds - is consistent.But the usage of 5-6 seconds nsfwjs use to do something - before anything else can happen in the page (e.g. select an image to analyze), is quite a long time - for an end user which want to do something as soon as possible. So... is there a way to speed up the time nsfwjs uses to finish this part: pdModel = await nsfwjs.load('model/'),and whatever it does before the page is ready for handling user triggered events in the page?Could the preparation work done by nsfwjs (these first 5-6 seconds) maybe be split up in 2 parts, and then do half the job - onLoad, and then the second part - which is triggered to happen when an image actually is selected by the user? ;"['Try updating your version of Tensorflow maybe?=====', ""I'm using this:https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.4Is there a newer version available?====="", 'You should definitely keep track of what CDN packages are available https://cdnjs.com/libraries/tensorflow=====', 'Thanks for that information. Now tested with latest tfjs  1.1.0. Same results. Any idea on how to not make the client (user) wait ""so long"" (5-6 seconds) before he can interact with the page (see suggestion in the end of thread start)?=====', ""Hrmmm. I really don't know. Is your code open source? I can take a look if so. ====="", 'Thanks for that.**You can test it from here**Explanation is in the page:[nude-test-await](http://monsanto.watch/nsfwjs/nude-test-await.html)But this should be no surprice. The same delay can be experienced on this demo page: https://nsfwjs.com/Same delay there - even if the model files are fetched from disk.Here is the relevant information on how the page was set up: `<!-- Load TensorFlow.js. This is required -->\t<script src=""nsfwjs/tfjs@1.1.0""></script>\t<!-- Load the NSFWJS library. -->\t<script src=""nsfwjs/bundle.js""></script>const nsfwjs = require(\'nsfwjs\'),var pdModel,async function loadAwait() {\tpdModel = await nsfwjs.load(\'model/\'),}async function analyzeImageAwait(){\t\t\t\tif (pdModel){\t\t\tvar img = new Image(),\t\timg.src = document.getElementById(\'img_url\').src,\t\tvar predictions = await pdModel.classify(img),\t\tconsole.log(\'Predictions\', predictions),        }}`in the body element I use this: onload=""loadAwait()""on the server I have these sub directories with the following files:model/files: group1-shard1of6group1-shard2of6group1-shard3of6group1-shard4of6group1-shard5of6group1-shard6of6model.jsonnsfwjs/files:bundle.jstf.min.js.maptfjs@1.1.0Except for this the main file only contain an input file field where I can select an image on the computer, and some javascript code which copy the relevant file information to the img.src with id img_url - like this:`function img_pathUrl(input){     $(\'#img_url\')[0].src = (window.URL ? URL : webkitURL).createObjectURL(input.files[0]),     setTimeout(analyzeImageAwait, 1000),}`But as allready mentioned the delay of 5-6 seconds happens before I can click the file button which activate the img_pathUrl function above.=====', ""So this is going to sound strange.  Neither NSFWJS or your page have this delay for me.  When I load your page, the first time, it downloads the model and then I was immediately able to access the UI.The second time I access the page, it reconfirms the cache for a few seconds, but the very second your loader goes away, I'm able to click.Perhaps it's a browser/computer difference?  Would you be willing to google/run some other TensorflowJS examples and see if this happens across the board for you?Perhaps it's when the model is loaded into memory?   Instant for some, slower on other computers?====="", 'What is the load time in seconds (which shows in blue text in page after loader hides) when the model files loads from disk, when you use this page?[nude-test-await](http://www.monsanto.watch/nsfwjs/nude-test-await.html)=====', '### Cold start![image](https://user-images.githubusercontent.com/997157/56745759-906ad880-6740-11e9-98d6-af08b96bb523.png)### Second page load![image](https://user-images.githubusercontent.com/997157/56745812-ad071080-6740-11e9-9047-5873db71173d.png)For me, the spinner shows me that the UI would be inaccessible.  But the second it goes away I can click.### Side noteAs far as I can tell, this is verifying local cache and loading the model into memory.   I might train a smaller model soon and see if I can get it nearly as accurate.  If so, that will be helpful with these kinds of times.  As you can guess, training models can take a long time.=====', 'My point is that the load time takes a lot of time - and that this might make it awkward to use in a production environment. In my web application which is still under development, the user will want to interact rather quickly with the UI, not wait 5 - 20 seconds before he can select an image, or do something else in the page.The thing is, there is a huge difference between time to fetch the model files from disk, which on my computer takes only 400 - 600 milliseconds, and the total time the load stuff actually work (as long as the loader spins). After the loader spinner stops spinning (after load process finish) the page is accessible also on my computer.My question is related to if there is a way to make the load time (except for the 400 - 600 milliseconds for the files to be read from disk cache) shorter, or if possible to divide this work into e.g. two parts. If the latter (two parts) - then one part could be activated when user selects the image, and the other part when he submit the form. This way - the wait, could be non-problematic for end user - in my opinion. I say could because the time seems to vary between browsers (and platforms, e.g. mobile), and even between different page loads in the same browser.Another way to speed things up, could be to NOT analyze drawing and neutral. For my use it would be enough to test for a numeric value for Porn, Sexy and Hentai content. Is it possible in some way to force the module to a) not load the model files for Drawing and Neutral, and b) not test for these values?Or what about this idea. Open a new hidden browser window via Javascript, then load e.g. a similar html page [as this page](http://monsanto.watch/nsfwjs/nude-test-await.html), and then load the model there without blocking usage of the actual page. Then when the user click submit, or select an image in the actual page, the img src is copied into javascript in the tab with the model - then do the analyzing there, and return the answer back to the actual page.Today I also tested the page, I gave you the link to, on my Samsung Galaxy Note II - on a Wi-Fi connection. It took 121 seconds to finish the first load, and 181 seconds to finish the second load.  When selecting an image - and waiting for a couple of minutes - no predictions where output. Need to do more tests here - since I have no clue why it was so slow on second load, and why no predictions where output (UPDATE: the day after I tested this again. It actually works, but needed some more time to finish - see my next comment below).In Internet Explorer (version: 11.0.9600.17728) the model does not load. The error is from the tfjs@1.1.0 file, where debugger report this error: ""\'Symbol\' is undefined"", where the debugger highlighted this code: ![Bug-in-IE](https://user-images.githubusercontent.com/436558/56764638-7dd6ba80-67a5-11e9-909e-494209adab03.jpg)I also observe the \'Symbol\' error in IE, mentioned above - when testing the of [NSFWJS demo page](https://nsfwjs.com/) in IE.=====', 'GantManThere is something called Web Workers in Javascript. Is it possible to load the model in a web worker, and then make the model load behind the scenes, so to speak, so the user does not have to wait until the model has loaded before the user can start to interact with the web page.When I have this in the worker.js`importScripts(\'tfjs@1.1.0.js\'),  importScripts(\'bundle.js\'),`Then I get these errors in the console:`Uncaught Error: Could not find a global object    at engine.ts:829    at engine.ts:818    at engine.ts:836    at tfjs@1.1.0.js:2    at tfjs@1.1.0.js:2    at worker.js:13`If I click the ""at engine.ts:829, it reports error at this line:`throw new Error(\'Could not find a global object\'),`So, do you know if it is possible to run the nsfwjs module from within a web worker?More about web workers here: [Multithreading Java script](https://medium.com/techtrument/multithreading-javascript-46156179cf9a)=====', ""Personally, I haven't worked with Web Workers just yet, but some searching found this discussion which I think should be noted:  https://github.com/tensorflow/tfjs/issues/102====="", ""> Personally, I haven't worked with Web Workers just yet, but some searching found this discussion which I think should be noted: [tensorflow/tfjs#102](https://github.com/tensorflow/tfjs/issues/102)Thanks for that information, just what I needed. Have tested it, and Web Workers do work with NSFWJS (in Opera and Chrome). Which mean that the model can load in a background process, while the client can start interacting with the user interface of the web page immediately.**But could you please tell me** if it is possible in some way to not use the model for Drawing and Neutral. That is, just download (load) the model files for analyzing image for Porn, Sexy and Hentai? Thereby save time by not having to download the two mentioned model files (Drawing and Neutral) to the client, and hopefully awoid that the model.classify() use time on testing for Drawing and Neutral. **An even better solution**  would be to create separate Web Worker for analyzing if Porn, Sexy, Hentai (and the others if one want that: Drawing and Neutral). The first Web Worker could initiate the other Web Workers after the nsfwjs is created, but before loading the model for testing for Porn. This way the other workers would fetch e.g. bundle.js and tfjs@1.1.0.js from cache (second worker would then fetch model file(s) for anlyzing Sexy, the third worker would fetch file(s) for analyzing Hentai (and so on). I believe such an approach would speed up the load time drastically on mobile devices, as well as speeding up load time in general on other devices - since each worker is working in it's own separate process at the same time. The process of analyzing frames (images) from a video, should also be a lot faster - using Web Workers.**If this can be done, when can we see new model files and new other files on github - which makes it possible to load e.g. separate model for Porn, and analyze just for Porn (and same for the other types (e.g. Sexy, Hentai, etc) - in a separate Web Worker?****For this to work in the Web Worker, the Web Worker must support Offscreen Canvas.** Not all web browsers support this yet, but it would be easy to test in a web worker js file to report if the feature is supported on the client browser, and if not - do not use use web workers. Here one can see which browsers (versions) [support Offscreen Canvas](https://developer.mozilla.org/en-US/docs/Web/API/OffscreenCanvas/OffscreenCanvas#Browser_compatibility).I mentioned in my previous comment above, that I could not make the NSFWJS work in my mobile (Galaxy Note II) using Chrome. That was not correct. Did a new test yesterday, and found that the image analyzing did work on the mobile device. I did not wait long enough the first day I tried this. But it took about 3 minutes and 30 seconds just to analyze the image (get the predictions). Approximately the same time it took to load the model.====="", ""Unfortunately the model is all in one and would not be made smaller by removing classifications. However!  I'm nearing the end of training a smaller and nearly as accurate model. Expect that soon. It should speed everything upRe: https://github.com/GantMan/nsfw_model/issues/20#issuecomment-487354103====="", 'Look forward to test that. But, if you trained a model to recognize only Porn, Sexy and Hentai images, or just Porn, would that model (the files) not be smaller than a model trained to recognize (predict) Porn, Sexy, Hentai, Drawing and Neutral?=====', ""So the model is architected first and only the final layer is affected in size by number of possibilities.  Now, fewer possibilities could be supported by a smaller model architecture, but that would require building a smaller model and retraining.  Aka lots of time. It's hard to remove those parts without throwing the whole model out of balance. As far as I know it's not even a practice.  But it should be! That would be cool. Anyhow, best case is to get this new smaller model I'm making and do a conditional on the predictions. I fully believe model optimization will become a very real thing in the next 5 to 10 years. Just not a simple solution.  ====="", 'Is there noe way to run it non-blocking? Which means it should run async in background with a callback when it finishes...I have the same problem. I want to create a chrome extension. But waiting on every site 5-6 seconds is just annoying.=====', 'danieldaeschle:You can load the model and analyze the images in a non blocking mode. Just create the NSFWJS and load the model in a web worker - then it will run in a separate thread on the client. I have tested this, and it works in latest versions of Chrome and Opera. This way the web page UI is not blocked by the loading procedure of NSFWJS. @GantMan , could you please confirm if the suggestion below is technically possible:After figuring out that I can load the NSFWJS module in a web worker, my next thought was if it was possible to load the data from the different model files using separate Web workers, and just send the loaded data (array, object) or whatever is loaded in a web worker, to the main web worker (the one you want to use for analyzing the images). This way the data from the model files could be loaded in parallell - and could probably minimize the total load time - which would be especially beneficial if the client is on a mobile device, but should be faster on all devices. Note: It is possible to send e.g. arrays and objects from a worker back to main script (page) and from that script to another worker. =====', '@bongobongo like i already said, i want to create a chrome extension which does not support web workers. i just tried it :( =====', '> @bongobongo like i already said, i want to create a chrome extension which does not support web workers. i just tried it :(@danieldaeschle , have you looked into this:https://bugs.chromium.org/p/chromium/issues/detail?id=357664#c3=====', 'I checked both. Nothing works. They are 5 years old...bongobongo <notifications@github.com> schrieb am Di., 30. Apr. 2019, 17:36:> @bongobongo <https://github.com/bongobongo> like i already said, i want> to create a chrome extension which does not support web workers. i just> tried it :(>> @danieldaeschle <https://github.com/danieldaeschle> , have you looked> into this:> https://bugs.chromium.org/p/chromium/issues/detail?id=357664#c3>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488001585>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHKTWHAKXDBRIT67JTJT5JLPTBRQNANCNFSM4HHYUUNA>> .>=====', ""@danieldaeschle , do you know that you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker? Like this - place it first in the Web Worker JavaScript file: `if (typeof OffscreenCanvas !== 'undefined') {\t// currently tested and works in my Chrome and Opera\t    self.document = {        createElement: () => {            return new OffscreenCanvas(640, 480),        }    },    self.window = self,    self.screen = {        width: 640,        height: 480    },    self.HTMLVideoElement = function() {},    self.HTMLImageElement = function() {},    self.HTMLCanvasElement = OffscreenCanvas,}`====="", ""Thank you, i'll try this.bongobongo <notifications@github.com> schrieb am Di., 30. Apr. 2019, 17:46:> @danieldaeschle <https://github.com/danieldaeschle> , do you know that> you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker?> Like this - place it first in the Web Worker JavaScript file:>> `if (typeof OffscreenCanvas !== 'undefined') {> // currently tested and works in my Chrome and Opera>> self.document = {>     createElement: () => {>         return new OffscreenCanvas(640, 480),>     }> },> self.window = self,> self.screen = {>     width: 640,>     height: 480> },> self.HTMLVideoElement = function() {},> self.HTMLImageElement = function() {},> self.HTMLCanvasElement = OffscreenCanvas,>> }`>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488005415>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHKTWHCTQVYJE3R2BGOUHPLPTBSWDANCNFSM4HHYUUNA>> .>====="", ""I tried it with cpu instead of gpu which does not require a canvas. Thatalso didn't work.Daniel Däschle <daniel.daeschle@gmail.com> schrieb am Di., 30. Apr. 2019,17:48:> Thank you, i'll try this.>> bongobongo <notifications@github.com> schrieb am Di., 30. Apr. 2019,> 17:46:>>> @danieldaeschle <https://github.com/danieldaeschle> , do you know that>> you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker?>> Like this - place it first in the Web Worker JavaScript file:>>>> `if (typeof OffscreenCanvas !== 'undefined') {>> // currently tested and works in my Chrome and Opera>>>> self.document = {>>     createElement: () => {>>         return new OffscreenCanvas(640, 480),>>     }>> },>> self.window = self,>> self.screen = {>>     width: 640,>>     height: 480>> },>> self.HTMLVideoElement = function() {},>> self.HTMLImageElement = function() {},>> self.HTMLCanvasElement = OffscreenCanvas,>>>> }`>>>> —>> You are receiving this because you were mentioned.>> Reply to this email directly, view it on GitHub>> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488005415>,>> or mute the thread>> <https://github.com/notifications/unsubscribe-auth/AHKTWHCTQVYJE3R2BGOUHPLPTBSWDANCNFSM4HHYUUNA>>> .>>>====="", '@danieldaeschle - did u have any progress on making the chrome extension? I have just started looking building one myself, and then saw ur comment above saying u were trying it. =====', ""I'd be interested in seeing how this is performing on the latest TensorFlow.jsI know they've been working on the blocking UI problem.Additionally, I've really shrunk the model, so once it's loaded into memory, it should be super quick!!!1====="", 'Is there any progress in solving the blocking UI issue?=====', ""I've found using the small model doesn't even appear to cause any complication.   I haven't tried the larger model in a while.====="", 'Which small model? How long does it take to initialize?=====', 'By default https://nsfwjs.com/  uses the small model.  You can change it to the larger model in the dropdown.  I have a new small-medium sized model that is more accurate, as well.=====']"
https://github.com/infinitered/nsfwjs/issues/93;Allowing for pre gzip the 6 model files;1;open;2019-04-21T23:24:20Z;2019-11-07T23:25:50Z;"The 6 model files are quite large. If the nsfwjs are to be used by many clients, there might be others out there who see the need to pre compress the model files - and just serve the pre compressed files to the client. This way saving bandwidth and CPU usage on the server - as well as the user has quicker access to the web page.Found a solution on how to serve pre compressed files from Apache here:https://gist.github.com/bhollis/2200790So there obviously is no need to change anything in model.json file to make this work, since it all can be solved in a .htaccess file.Originally this was formed as a question, since I believed there had to be some changes to the model.json file to make this work. But there is no need for that. I'm not an expert in Apache and code in .htaccess - but the information below has been tested on a live server (web hotel), and does work (you just have to get the new lines correct though - they got messed up by some reason when displayed here).Here is how I did it:First I compressed the 6 files in the model directory (the files starting with ""group1-"", using 7-zip.So the original file named group1-shard1of6 ended up with this filename: group1-shard1of6.gzAfter compressing the 6 files, deleted the 6 corresponding uncompressed files, and added a .htaccess file (in same directory where I have the model files) with the content below (new lines is removed by the editor here, so they have to be fixed if using this code):`Options +SymLinksIfOwnerMatchRewriteEngine OnAddEncoding gzip .gzExpiresActive OnExpiresDefault ""access plus 2 months""RewriteCond %{HTTP:Accept-encoding} gzipRewriteCond %{REQUEST_FILENAME} !-fRewriteRule ^(.*)1of6 group1-shard1of6.gz [QSA,L]RewriteRule ^(.*)2of6 group1-shard2of6.gz [QSA,L]RewriteRule ^(.*)3of6 group1-shard3of6.gz [QSA,L]RewriteRule ^(.*)4of6 group1-shard4of6.gz [QSA,L]RewriteRule ^(.*)5of6 group1-shard5of6.gz [QSA,L]RewriteRule ^(.*)6of6 group1-shard6of6.gz [QSA,L]<Files *.gz>ForceType text/javascript</Files>`**************With and without gzip compression for the first model file:group1-shard1of6        = 4096 KBgroup1-shard1of6.gz   = 2698 KBJust by compressing the 5 largest files the client will ""save"" approximately 5 * 1400 KB =  7000 KB in the download process.";['There is a newer version of the model that is substantially smaller while still maintaining 90% accuracy which might be useful for you.=====']
https://github.com/infinitered/nsfwjs/issues/88;Some new ideas;8;open;2019-04-18T08:40:48Z;2020-01-21T08:12:25Z;1. Able to detect what regions are NSFW within the image2. Able to detect video, GIF, APNG and WebM.;"[""So Donald, we currently support GIF.   There's a ticket to support Video which @fvonhoven has claimed.I like the idea of detecting regions.  Is that something you'd be interested in adding?====="", '@GantMan I am interested but I am too new at this. Might want to keep this issue open for others to see.=====', 'All good.  How good at JavaScript are you?   That’s most of it. =====', '@GantMan I nearly finished codecademy, so not that good.Also does the dataset mark the regions of which they are NSFW?=====', ""Currently it does not mark regions. Maybe soon I'll try to make something do that. ====="", '@GantMan Maybe take a look at https://github.com/facebookresearch/detectron2 and see how they do things?=====', 'Apparently https://github.com/halcy/DeepDanbooruActivationMaps/blob/master/DeepDanbooru-ActivationMaps-Censorship.ipynb already does partial image censoring for Hentai. Hmmm=====', 'So here is the proposal I laid out for anime vs hentai https://github.com/KichangKim/DeepDanbooru/issues/5=====']"
https://github.com/infinitered/nsfwjs/issues/39;Run on every frame in Video;16;open;2019-03-04T20:57:22Z;2020-11-26T18:29:04Z;Much like the discussion over in #38 There's been a request to run on every frame in a video.It's possible to do in JS like so:  https://jsfiddle.net/bmartel/3h98gsvk/11/Maybe a syntax like `model.classifyVideo(video)` or `model.classifyAny(img|gif|video)`;"[""I'll take this one!====="", 'I would be interested in one. THough, for video I am curious about performance. (Did you benchmark frames per second? In my case, on a 8-year-old Macbook Pro, it takes ~10s for image classification.)=====', 'I imagine this would take a while client-side.  However, I\'d like to start at (just do it) and then work on doing it fast.  There could be a ""random scan"" which checks every 10 or 100 frames.  Just TOL.=====', 'Depending on the hardware on the client side, the speed could be increased drastically by creating several instances of the nsfwjs - using Web Workers. Then one could classify multiple frames in parallell. The number of workers that could work efficiently is probably defined by frame size (image size), CPU and memory. I have tested Web Workers against the current version of NSFWJS, and it works in the latest versions of Chrome and Opera. I did it because my web clients need to interact with the page UI immediately, and not wait 5 - 20 seconds (or more, e.g. mobile) for the model to load. Runing NSFWJS in a Web Worker - made it so that both the loading and classifying of images was done in a process separated from what happens in javascript in the main page. More about it here: https://github.com/infinitered/nsfwjs/issues/94#issuecomment-487306805=====', 'I like this idea.=====', 'How about using keyframe or scene detection to better select what frames to analyze?=====', 'I really like that.  Is that doable in JS?=====', ""I'll try to implement it====="", 'Hi @YegorZaremba Any update on this?=====', ""@uzaysan Yeap, we can use same interface for `classifyVideo` method as we use for `classifyGif` https://github.com/infinitered/nsfwjs/pull/401/files#diff-f41e9d04a45c83f3b6f6e630f10117feL194Add guard that checks is video fully buffered, if not - throw an error. Also for nodejs usage, we should accept `Buffers` you can see how it works for `gif` https://github.com/infinitered/nsfwjs/pull/401/files#diff-f41e9d04a45c83f3b6f6e630f10117feR206```js  async classifyVideo(    video: HTMLVideoElement,    config: classifyConfig = { topk: 5, fps: 25 }  ): Promise<Array<Array<predictionClassType>>> {    const totalFrames: number = Math.floor(video.duration * config.fps)    const interval: number = video.duration / totalFrames    // @NOTE Messy, but after refactoring please test with 25+ fps    const acceptedFrames: number[] = []    for (let i = 0, i < totalFrames, i++) {      const frame = Math.floor((i * interval) * 100) / 100      acceptedFrames.push(frame)    }    const canvas = createCanvas(video.offsetWidth, video.offsetHeight)    const context = canvas.getContext('2d')    const arrayOfClasses: predictionClassType[][]  = []    for (let i = 0, i < acceptedFrames.length, i++) {      video.currentTime = acceptedFrames[i]      context.drawImage(video, 0, 0, canvas.width, canvas.height),      const image = await loadImage(canvas.toDataURL()) // new Image() onload onerror      // @ts-ignore      const classes = await this.classify(image, config.topk),      arrayOfClasses.push(classes)    }    return arrayOfClasses  }```**Problems**```jsconst image = await loadImage(canvas.toDataURL())```For most browser using this line of code will throw an error like https://stackoverflow.com/questions/35244215/html5-video-screenshot-via-canvas-using-cors/35245146I have no idea how to fix it, we can implement this logic as is, and if we have that error, just throw new NSFWError(error), but as led dev of @nsfw-filter is not solved my problems at allUPD1, I'll return to this issue when I have free time and announce to everyone on this issue====="", ""@YegorZaremba Thanks for quick reply. I'm using this library in server side(NodeJs). But when I pass video buffer, It throws an unsuported Image error.`Expected image (BMP, JPEG, PNG, or GIF), but got unsupported image type`I'm using it like this:  ```const model = await nsfwlib.load(),  const video = await tf.node.decodeImage(videoBuffer),  const predictions = await model.classify(video),```Also I'm using version 2.2.0What am I doing wrong? Can you please help me?====="", '> But when I pass video buffer, It throws an unsupported Image error.@uzaysan  This feature is not implemented yet, you can slice your video to images by yourself and predict these image, just google some lib `mp4 to img node.js`, I hope sth exists in the npm=====', 'So I should classify video frames one by one. Am I right? But Also I can use library bu converting video to gif? Can you confirm?Also when will video classification be available? Do you have an estimate date? I saw in some issues, you associated this feature to v2.3.0. When will that version come out.Thanks.=====', 'yeap, great ideaSo if you want use classifyGif pls install `@nsfw-filter/nsfwjs` because this feature(`classifyGif`) has the status ""Work in progress"" (just some fixes for nsfw.com) but we use this package in @nsfw-filter. It works pretty slow for Buffers in Nodejs (but it works :D)=====', 'Thank you I will try=====', 'Hi there!Offscreen canvas could help get it more perfomant with web workers, but https://caniuse.com/offscreencanvas :(https://developers.google.com/web/updates/2018/08/offscreen-canvas=====']"
https://github.com/infinitered/nsfwjs/issues/565;Question on how to add more classifications on this model;5;closed;2021-12-01T13:25:56Z;2021-12-13T22:19:44Z;"Hi, can I sort of ""upgrade"" this model by adding one more category of classification? If so, how could I do it?Thank you!";"[""Great question.To take a trained model and train more data on top of it, you can do transfer learning.  I actually cover how to do transfer learning in my book, in chapter 11 I think (https://infinite.red/learn-tensorflowjs).   However, it's not a straightforward process.  It takes time and skill.It really depends on the data, and then you have to test it.   What categories are you thinking of adding?====="", ""> Great question.> > To take a trained model and train more data on top of it, you can do transfer learning. I actually cover how to do transfer learning in my book, in chapter 11 I think (https://infinite.red/learn-tensorflowjs). However, it's not a straightforward process. It takes time and skill.> > It really depends on the data, and then you have to test it. What categories are you thinking of adding?gun detection====="", ""I would start a separate model and train it on all kinds of weapons.  I bet there are datasets out there to help get you started!I'd recommend grabbing a few courses or books on training models and then use NSFWJS as a good example on how to glue the model into a JavaScript API.====="", ""> I would start a separate model and train it on all kinds of weapons. I bet there are datasets out there to help get you started!> > I'd recommend grabbing a few courses or books on training models and then use NSFWJS as a good example on how to glue the model into a JavaScript API.Thanks for the attention and replies. I'll take a look on it for sure. Appreciate it.====="", ""you're very welcome!=====""]"
https://github.com/infinitered/nsfwjs/issues/564;Where is the latest model?;1;closed;2021-11-30T08:10:28Z;2021-12-16T20:54:19Z;Hi!Where I find the latest models?This models updated 2-3 year ago: https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public;"[""There hasn't been any significant updates to the models to re-release them.  Most of the updates have been to the code.There might be a mobilent v3 model at some point.  For now, we're looking to add in a voting classifier to help merge existing models into a higher accuracy option, but none of this is under active development beyond support for updates.=====""]"
https://github.com/infinitered/nsfwjs/issues/560;False Positive: women eating banana;1;closed;2021-11-19T08:18:26Z;2021-11-19T16:01:44Z;Hi Gant, NSFW JS will fail with pictures of women eating banana!just a doing a simple google search: https://www.google.com/search?q=woman+eating+banana... and some examples:https://image.shutterstock.com/image-photo/young-woman-eating-banana-260nw-3106254.jpghttps://thumb.mp-farm.com/89598622/preview.jpghttps://st2.depositphotos.com/1012146/6473/i/950/depositphotos_64738027-stock-photo-girl-eating-a-banana.jpghttps://image.shutterstock.com/image-photo/woman-eating-banana-260nw-514825333.jpgas what I know, eating a (real) banana is not porn or nudity!;"[""I'll add these to the training data.  Thanks for the find.=====""]"
https://github.com/infinitered/nsfwjs/issues/551;Hosted model not working;2;closed;2021-10-20T18:35:51Z;2021-10-20T20:16:08Z;Hello. I'm trying to host the files  in the link https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelin my own domain, but when I try to load the model it doesn't work. Can you upload the model files that are used in the index.js file? https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/thank you!;"[""That's strange it doesn't work.   I believe the default model is here: https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/quant_nsfw_mobilenet - that other folder is the larger model which requires you set the size to 299 via `const model = nsfwjs.load('/path/to/different/model/', { size: 299 })`====="", ""@GantMan thanks for the reply. Just solved the problem looking into another issue:`async loadNSFWModel() {      tf.enableProdMode()      this.nsfwModel = await nsfwjs.load(        'backend/images/model/mobilenet_v2_140_224/web_model/',        { type: 'graph' }      )    },`had to load the web_model=====""]"
https://github.com/infinitered/nsfwjs/issues/538;returning extreme large value;3;closed;2021-08-26T20:39:17Z;2021-08-26T22:24:49Z;image: https://cdn.discordapp.com/attachments/721821364255719504/880540077850247198/unknown.pngreturnings:```js[  { className: 'Neutral', probability: 0.9999781847000122 },  { className: 'Drawing', probability: 0.00002110443529090844 },  { className: 'Porn', probability: 4.6671684117427503e-7 },  { className: 'Hentai', probability: 2.755132015863637e-7 },  { className: 'Sexy', probability: 2.7379114087011658e-8 }]```;"[""Ahhh, the issue is that it's represented using scientific notation.   See the `e-7`? and `e-8`? that actuall makes the number VERY small, like  0.000000432https://calculator.name/scientific-notation-to-decimal/4.32e-7This is a common notation and shouldn't be an issue for finding max values.====="", 'oh. i didnt know that. but still thank you=====', 'If you have any issues with it being in this form, please update me.  =====']"
https://github.com/infinitered/nsfwjs/issues/534;classifyGif not working with buffer?;1;closed;2021-07-21T00:41:56Z;2021-08-10T06:24:15Z;```const imageDataBuffer = Buffer.from(Array.from(data.imageData.data))        if (!Buffer.isBuffer(imageDataBuffer)) {          console.log(`imagedata convert to buffer failed: ${typeof imageDataBuffer}`)        }        // TODO: path must be a string returned        const framePredictions = await this._model.classifyGif(imageDataBuffer)````data.imageData` is the ImageData (https://developer.mozilla.org/en-US/docs/Web/API/ImageData) and `data.imageData.data` is the https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8ClampedArray so in above code imageDataBuffer is the Buffer object which I'm expecting it to work with `classifyGif` function. But with that I'm getting this error in the Browser (NOTE: I'm not using this server-side)![image](https://user-images.githubusercontent.com/787164/126412622-78699efd-a4bb-4da0-a179-4c1c41cc586f.png)any advice would be appreciated. Thanks.btw, I'm using web worker, so I'm sending imagedata to the web worker for processing since web worker don't have access to the DOM objects.;['oops. this might be a webpack build issue. https://github.com/infinitered/nsfwjs/issues/273 similar to this.=====']
https://github.com/infinitered/nsfwjs/issues/524;NSFW model is loaded over `HTTP` by default, instead of `HTTPS`;13;closed;2021-06-26T14:06:31Z;2021-06-26T19:32:49Z;https://github.com/infinitered/nsfwjs/blob/5981af701f650ff57f802e3f2bd980dae9bb63c8/src/index.ts#L30This does not work on HTTPS websites due to `mixed content` errors.;"[""I'm a bit torn here:1.  It would be best if you just pull down the model and host it locally.  Rather than depend on the cloudfront version.2.  What would be a good API?Thinking out loud:`load````jsawait nsfwjs.load('/path/to/model/directory/')````load cloudfront model````jsawait nsfwjs.infiniteRedHosted()// AND this for HTTPS?await nsfwjs.infiniteRedHosted({secure: true})```Infinite Red is fine hosting the model for free, but I think sometimes people have no clue they are depending on it.  Maybe a secondary function that is explicit is smart.====="", ""I don't think there is any reason to load it over `http` instead of `https` (https is a better default in any case)On another note, why can't we host the model on github itself, and serve it statically from `https://raw.githubusercontent.com/` ?====="", ""Good point.Would you like to do the PR to make the default https for cloudfront?   Basically a PR adding an 's'?As for github vs cloudfront, there's definitely a speed difference, but that's a good suggestion.====="", ""I've noticed that the CloudFront URL also throws a CORS error when you fix `http` to `https`.```Access to fetch at 'https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json' from origin 'https://www.google.com' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.```However, I think the CORS issue won't happen if you host the same thing on `https://raw.githubusercontent.com/`.So, adding an 's' won't really fix it on browsers (it will work in `node` though, it probably does already).====="", 'My CORS knowledge is a bit shaky.I tested the HTTPS here:https://www.test-cors.org/#?client_method=GET&client_credentials=false&server_url=https%3A%2F%2Fd1zv2aa70wpiur.cloudfront.net%2Ftfjs_quant_nsfw_mobilenet%2Fmodel.json&server_enable=true&server_status=200&server_credentials=false&server_tabs=remoteIt seems to load with no issue.  Maybe I need more context?   =====', 'If you use `nsfwjs` in a chrome extension, you\'ll need to load it over an origin that\'s not `d1zv2aa70wpiur.cloudfront.net`. For example, if you run the extension on `google.com`, it will try to fetch `model.json` on Google, which is not the same origin as `d1zv2aa70wpiur.cloudfront.net`, so the S3 servers will throw a CORS error.You can simulate the same thing by running a simple `fetch` in your browser\'s console on `google.com`.```jsfetch(\'https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json\').then((response) => console.log(response)),```![image](https://user-images.githubusercontent.com/42958812/123518999-4a19b700-d6c6-11eb-925c-86572208939c.png)You can fix this by modifying the permissions on the S3 bucket. Once you select your bucket, go to the `Permissions` tab and scroll down to the `Cross-origin resource sharing (CORS)` section. Here you can enter a JSON to define the behaviour you want. Here\'s an example that allows all origins.```[    {        ""AllowedHeaders"": [            ""*""        ],        ""AllowedMethods"": [            ""GET""        ],        ""AllowedOrigins"": [            ""*""        ],        ""ExposeHeaders"": []    }]```![image](https://user-images.githubusercontent.com/42958812/123519102-e5129100-d6c6-11eb-9ecc-83057df45206.png)You can also use [this](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors-troubleshooting.html) for reference.If you run a fetch similar to the one above but from `raw.githubusercontent.com`, it does not throw a CORS error.```jsfetch(\'https://raw.githubusercontent.com/infinitered/nsfwjs/master/src/nsfwjs.ts\').then((response) => console.log(response)),```So if you feel that changing the S3 configuration is a hassle, then you could host the model on Github. I agree that it would be slower, but it would be free (I don\'t know if it\'s free for you to host it on CloudFront/S3, but I assume it\'s not).=====', ""OK, I want you to file all GitHub tickets from now on :D  This was perfect.1.  I updated the permissions on the bucket2.  I tested with the example you gave (from Google) and it now works.I'm going to add you as a contributor to the project for this insight.   I might still move the code to GitHub, but I want to do that in a separate ticket.====="", 'You\'ve been added.   I\'ll also add the ""code"" tag if you want to do the PR to switch to https![image](https://user-images.githubusercontent.com/997157/123519783-f0d46800-d672-11eb-8e1e-fcb2b726b37d.png)=====', ""Thanks for adding me as a contributor @GantMan ! I've made a PR with the requested change, it will be functional once the CORS issue is fixed. https://github.com/infinitered/nsfwjs/pull/526====="", 'Please verify the CORS issue is fixed (I verified it did for me), and then you can close this ticket if successful.=====', ""Hey @GantMan , I just tried it, I seem to be getting a CORS error still.![image](https://user-images.githubusercontent.com/42958812/123521865-f6639980-d6d6-11eb-8fa7-ded232ace9d1.png)I pasted this:```jsfetch('https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json').then((response) => console.log(response)),```====="", ""I had to do some extra steps: https://stackoverflow.com/questions/12358173/correct-s3-cloudfront-cors-configurationThis means there's a cache that can take a little while.  Try again and let me know.![image](https://user-images.githubusercontent.com/997157/123522692-8298a100-d684-11eb-86c9-4f3bba9c1b3b.png)====="", ""@GantMan, it is working now. I think it was a caching issue. I'll close this ticket, thanks for your support!=====""]"
https://github.com/infinitered/nsfwjs/issues/523;Can webp image be detected ?;8;closed;2021-06-26T13:37:50Z;2021-07-02T04:08:28Z;Someone can help me, how to use nsfwjs detect webp image, thanks;"[""Hiya @ldnvnbl - yes it does work with webp!   I noticed I forgot to add that option on NSFWJS.com - so I'll be adding it now.====="", ""I've updated the site.  For posterity, if anyone wants to check using a webp file, just throw https://file-examples-com.github.io/uploads/2020/03/file_example_WEBP_50kB.webpinto NSFWJS.com====="", ""Does that answer your question?  On web you'd do it the same way you detect any other image.Are you needing to detect on a node server?====="", '@GantMan I detect on node server=====', ""You'll need to use something like this: https://www.npmjs.com/package/webp-converter====="", '@GantMan my original image is webp, you means I should convert it to webp and then detect it ?I want direct detect webp image, but my code is not work, can you give me a demo code that can detect webp image directly with node js (express api service).  thank you very very much.=====', 'woops.  I meant convert it to a different image then detect.If you want it built into TFJS, you should open a ticket asking for node webp to tensor here:  https://github.com/tensorflow/tfjs/issues=====', '> woops. I meant convert it to a different image then detect.> > If you want it built into TFJS, you should open a ticket asking for node webp to tensor here: https://github.com/tensorflow/tfjs/issuesgot it, thank you very much, you are so nice=====']"
https://github.com/infinitered/nsfwjs/issues/520;Many False Positives;5;closed;2021-06-19T19:24:14Z;2021-06-26T15:42:06Z;"Hello. today i tried to implement nsfwjs into my discord bot, but im getting many false positives. This is my first time using tenserflow.My problem is that the model marks almost everything as ""Drawing"" with 30-70% Probability. At first i didn't load a model but tried loading all of these models after seeing the results were wrong:https://github.com/GantMan/nsfw_model/releases/tag/1.1.0   (when using this model i also tried setting its type to ""graph"", but the false results continued )https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelas well as one more model i cannot find the link of.All of these models gave very similar false positives.This is my code ```const nsfwjs = require('nsfwjs'),const tf = require('@tensorflow/tfjs-node'), // Tried with @tensorflow/tfjs and @tensorflow/tfjs-node-gpu as wellconst fetch = require('node-fetch'),tf.enableProdMode(), // Read to do this here https://github.com/infinitered/nsfwjs#production but tried running without doing // it as wellclass NSFWHandler {  model = null,  constructor() {    this.#loadModel(),  }  async #loadModel() {    return (this.model = await nsfwjs.load()),  }  async #formatData(attachment = null) {    if (!attachment?.url || !attachment?.width || !attachment?.height) return,    // Fetch image from url and save as buffer    const imageBuffer = await fetch(attachment.url).then((res) => res.buffer()),    // Im passing this in this format because a error notified me i can pass data in this format    return { data: imageBuffer, height: attachment.height, width: attachment.width },  }  async classifyImage(attachment) {    if (!this.model) return console.log('NO MODEL'),    const formattedData = await this.#formatData(attachment),    const classification = await this.model.classify(formattedData),    return { ...classification, imageHash },  }}module.exports = NSFWHandler,```I think the data format i pass in could be problematic, but im new to tenserflow and this was the only way it worked.As a test i compared the results of 10 pictures both nsfw and sfw between my implementation and https://nsfwjs.com/ results with the site always scoring perfectly.My implementation results were: Drawing: 80%Website results for the same picture were: Sexy: 60%";"[""ahhh, I see you're using node.   I agree, it's an issue with how you're passing the images into the the model.Can you try converting the images to a 3D Tensor and then passing them in to classify?   BTW, if you're new, I have lots of sample code for converting images to Node.js tensors in my book source code:  https://github.com/GantMan/learn-tfjs====="", ""Thanks. Seems to work way better, but the results are still not as confident as the results i get from the website. Does nsfwjs by default use a different model than the site, and if yes could i get it linked if it's public?====="", 'The website has 3 different models![image](https://user-images.githubusercontent.com/997157/122687805-fb5bb100-d1dd-11eb-9a55-0ba03231c55a.png)You can choose whichever model you want most and grab it here for local hosting:  https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public=====', ""Since this has been sitting for a week, I'm going to close this issue, let me know if you've got the model you want or have any further questions.====="", 'Sorry forgot to close it. And yes it works perfectly now. Thanks.=====']"
https://github.com/infinitered/nsfwjs/issues/517;False Positive Neutral;2;closed;2021-06-14T07:46:05Z;2021-06-14T08:17:06Z;"![52f81f94-4a5f-43cf-8a25-7d6b8ba53fd8](https://user-images.githubusercontent.com/23108292/121856660-6a786780-ccfd-11eb-9b01-320c36d9498e.jpg)**Dependencies:**""@tensorflow/tfjs"": ""^3.7.0""""nsfwjs"": ""^2.4.0""**Prediction Results**className: ""Drawing""probability: 0.9339360594749451";"['Model config`{  ""format"": ""graph-model"",  ""generatedBy"": ""2.2.0"",  ""convertedBy"": ""TensorFlow.js Converter v1.7.4r1""}`=====', 'Now i using ""Inception V3 Model"", and get better results.=====']"
https://github.com/infinitered/nsfwjs/issues/468;Question: what kind of this string;2;closed;2021-01-23T01:47:51Z;2021-01-23T04:46:44Z;sir, ty for this beautiful modules, i wanna ask a biti've using nodejs and return:```json[  { className: 'Hentai', probability: 0.998166024684906 },  { className: 'Porn', probability: 0.0016352280508726835 },  { className: 'Drawing', probability: 0.00019833433907479048 },  { className: 'Sexy', probability: 4.824803454539506e-7 },  { className: 'Neutral', probability: 5.4902471902096295e-8 }]```**0.998166024684906** what kind of this one? how can i convert `probability` value into percent instead or make it more readable?  i tried it decimal into percent but the result is totality wrong. thankyou so much;['You can multiply probability by 100 to get percent.  Then you can round it.=====', 'very cool gantman=====']
https://github.com/infinitered/nsfwjs/issues/464;UnhandledPromiseRejectionWarning: FetchError: request to https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json failed, reason: self signed certificate in certificate chain;6;closed;2021-01-19T11:32:08Z;2021-02-13T14:23:26Z;(node:19904) UnhandledPromiseRejectionWarning: FetchError: request to https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json failed, reason: self signed certificate in certificate chain    at ClientRequest.<anonymous> (D:\Learning\demo\node_modules\node-fetch\lib\index.js:1461:11)    at ClientRequest.emit (events.js:315:20)    at TLSSocket.socketErrorListener (_http_client.js:469:9)    at TLSSocket.emit (events.js:315:20)    at emitErrorNT (internal/streams/destroy.js:106:8)    at emitErrorCloseNT (internal/streams/destroy.js:74:3)    at processTicksAndRejections (internal/process/task_queues.js:80:21)(Use `node --trace-warnings ...` to show where the warning was created)(node:19904) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:19904) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.;"['Is it working now?  It seems https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json is loading fine=====', ""Hi Infinitered/Nsfwjs,Thanks for response, Still i am getting the same issue for both the node jscode which is present in the documentation. Few days ago i have tried thesame code it was running but now it's not running(node:19896) UnhandledPromiseRejectionWarning: FetchError: request tohttps://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.jsonfailed, reason: self signed certificate in certificate chain    at ClientRequest.<anonymous>(D:\\Learning\\demo\\node_modules\\node-fetch\\lib\\index.js:1461:11)    at ClientRequest.emit (events.js:315:20)    at TLSSocket.socketErrorListener (_http_client.js:469:9)    at TLSSocket.emit (events.js:315:20)    at emitErrorNT (internal/streams/destroy.js:106:8)    at emitErrorCloseNT (internal/streams/destroy.js:74:3)    at processTicksAndRejections (internal/process/task_queues.js:80:21)(Use `node --trace-warnings ...` to show where the warning was created)(node:19896) UnhandledPromiseRejectionWarning: Unhandled promise rejection.This error originated either by throwing inside of an async functionwithouta catch block, or by rejecting a promise which was not handled with.catch(). To terminate the node process on unhandled promise rejection, usethe CLI flag `--unhandled-rejections=strict` (seehttps://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejectionid: 1)(node:19896) [DEP0018] DeprecationWarning: Unhandled promise rejections aredeprecated. In the future, promise rejections that are not handled willterminate the Node.js process with a non-zero exit code.[image: image.png]Thanks,AnishOn Tue, Jan 19, 2021 at 10:42 PM Gant Laborde <notifications@github.com>wrote:> Is it working now? It seems> https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json> is loading fine>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/infinitered/nsfwjs/issues/464#issuecomment-762987393>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AMMDDO3HMFKB3ETHRNUCJXTS2W4OPANCNFSM4WIUNTHA>> .>====="", 'it seems like your internet change IP of aws CDN, try use vpn or other network=====', 'The master branch has modified the default URL.=====', 'Correct, the new URL is a CDN and much faster.=====', ""Thanks guys, i found the solution basically antivirus was blocking to access the aws url from local. after making disable it's working fine=====""]"
https://github.com/infinitered/nsfwjs/issues/457;[React] Cannot call a class as a function error in production build;8;closed;2021-01-04T18:52:38Z;2021-01-04T22:22:39Z;GitHub Repo: https://github.com/amanvishnani/firegramLive URL: https://firegram-aman.web.app/I get the following error on the production build however not on the Development build. Not sure what is causing this issue.![image](https://user-images.githubusercontent.com/8425802/103568346-c8299d80-4e9b-11eb-926c-55c2b2de8383.png)Can someone please help me out what's the issue here?;"['@amanvishnani - is your GitHub repo private?=====', '@GantMan sorry about that, just made it public. Try now.=====', ""Couple of notes:Don't use `var` here.   It willl hoist and possibly cause issues with your useStatehttps://github.com/amanvishnani/firegram/blob/a9ac785a6ea2af9d78cd692bfde72b0bb882a007/src/hooks/useNsfw.js#L12Try to stay away from using `var` anytime you can, instead use `let`.Also, I'm not sure if my hook knowledge is limited, but what's stopping your second useEffect from firing before your first useEffect is complete?Just a few things I noticed, but I'm not sure why it's calling a class.====="", '@GantMan the dependency array in the second useEffect tells it to only run when either `model` or `imageElement` is changed.=====', ""But doesn't that run on the first time setting it?  My useEffect knowledge is always sticky.   Hah, I usually have to fiddle for 10 minutes to get it right :D   Is the issue the model code 100%?  Have you tried switching NSFWJS out with any old object?====="", 'Yes, although it is called for the initial values which are `null`, the if-condition will not run the code for the first time. I have tried with the v2.2.0 only. Did not try old models or previous versions. Not sure what is causing the issue when the build changes from Dev-build to Prod-build.=====', 'I just found this:   https://github.com/tensorflow/tfjs/issues/3384Take a look at the webpack.confi.js adjustment they recommend.=====', 'Downgrading tfjs to 1.7.4 worked for me. Thanks.=====']"
https://github.com/infinitered/nsfwjs/issues/436;Need help with model.classifyGif();6;closed;2020-11-09T21:21:14Z;2020-11-10T08:20:52Z;"Hi, I'm building a server on node.js to handle NSFW gifs. However, I've run into some error messages and I'm not sure how to resolve them.(node:4365) UnhandledPromiseRejectionWarning: Error: ""url"" option is required.    at Object.gifFrames [as default] (/home/runner/HandmadeSeparateProjections/node_modules/@nsfw-filter/gif-frames/gif-frames.js:45:12)    at NSFWJS.<anonymous> (/home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:204:60)    at step (/home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:52:23)    at Object.next (/home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:33:53)    at /home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:27:71    at new Promise (<anonymous>)    at __awaiter (/home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:23:12)    at NSFWJS.classifyGif (/home/runner/HandmadeSeparateProjections/node_modules/nsfwjs/dist/index.js:193:16)    at Client.<anonymous> (/home/runner/HandmadeSeparateProjections/index.js:1800:38)So here's what I have done:```const pic = await axios.get(linkToGif, {              responseType: 'arraybuffer',})     // Get the GIF in the form of a buffer (encoded in uint8)const image = await tf.node.decodeGif(pic.data)     //Decode the GIF using tensorflowconst prediction = await model.classifyGif(image),     //Classify the GIF (This is where the error is thrown)```Can anyone please help me out? Is there anywhere I can find sample code to GIF classification? Thank you in advance.";"[""Pinging @YegorZaremba  -  But I'll see about taking a look next week if he doesn't get a chance to look at this.I should def include a node sample for gifs in the examples folder.====="", 'That sounds great. Thank you for your quick response!=====', 'Hi @edsumpena Here is test for `classifyGif` https://github.com/infinitered/nsfwjs/blob/master/__tests__/classifyGif.ts#L43According to error https://github.com/nsfw-filter/gif-frames/blob/master/gif-frames.js#L45, It looks like your `url: gif` is empty https://github.com/infinitered/nsfwjs/blob/master/src/index.ts#L208could you pls write result of this for me```const image = await tf.node.decodeGif(pic.data)  console.log(Buffer.isBuffer(image))```Related: https://github.com/infinitered/nsfwjs/issues/431=====', 'Hi @YegorZaremba, I have run the log command. It returns ""false"". I\'m assuming that it should return ""true""?=====', ""@edsumpena I think you should skip decoding gif by tenserflow and pass buffer as is. We will improve this step for Node.js https://github.com/infinitered/nsfwjs/issues/431#issuecomment-716212203This code also runs in browsers. Browsers don't have some Node.js stuff. I plan to do separate logic in this lib for browsers and Node.js in the future====="", 'I have run the code without tf.node.decodeGif(pic.data) and it works. Thank you for your help!=====']"
https://github.com/infinitered/nsfwjs/issues/397;Error when using a custom model;16;closed;2020-08-15T12:56:06Z;2021-01-04T19:43:28Z;"> ""classname"" and ""config"" must be set.";"[""I've emailed Gantman, and please take a look at it.====="", 'Can you provide a repo with your custom model so I can debug?=====', '> Can you provide a repo with your custom model so I can debug?I was using your newest model.> https://github.com/GantMan/nsfw_model/releases/tag/1.2.0My code was like this:```jsconst axios = require(\'axios\') //you can use any http clientconst tf = require(\'@tensorflow/tfjs-node\')const nsfw = require(\'nsfwjs\')async function fn() {  const pic = await axios.get(link_image, {responseType: \'arraybuffer\'})  const model = await nsfw.load(""file://anti-nsfw-bot/project-model/"")  const image = await tf.node.decodeImage(pic.data,3)  const predictions = await model.classify(image)  image.dispose()  console.log(predictions)}fn()```=====', 'What version of TensorFlow js?=====', '@tensorflow/tfjs-node: 2.3.0@tensorflow/tfjs: 2.3.0=====', 'I believe that\'s the issue.  I have it on my TODO to support the latest version of TFJS, but they made some breaking changes when they jumped from 1.x to 2.x and I need to come back and update it.Try ""@tensorflow/tfjs"": ""^1.7.4""=====', 'Which one? tfjs or tfjs-node?Also I tried to downgrade it, and it said version not found (404)=====', 'tfjs =====', '![image](https://user-images.githubusercontent.com/33544674/90414713-b4df8680-e0e2-11ea-8f6b-a0e725d7add2.png)=====', 'Should I use above than 1.7.4 ?=====', ""**Update:**I uninstall tfjs (not the node one)I downgraded my node to v12.13.0Now I only used tfjs-node v2.1.0I don't know if I stuck at here.**Only HTTP(S) protocols are supported**Don't know why but I'm stuck here.====="", 'Are you sure you cleaned out web TFJS?https://stackoverflow.com/questions/60137483/tensorflow-node-js-typeerror-only-https-protocols-are-supported=====', 'Yes, I only have tfjs-node.=====', ""@conver4yI've just had the same problemuninstall @tensorflow/tfjs-node and @tensorflow/tfjsand then npm install @tensorflow/tfjs-nodenpm install @tensorflow/tfjs@^1.7.4EXACTLY IN THIS ORDERidk why, but when I install  @tensorflow/tfjs@^1.7.4 first, it crashes with the same error you had![image](https://user-images.githubusercontent.com/44163887/94936092-778a5900-04d6-11eb-8330-b714aaf491f4.png)====="", ""> @conver4y> I've just had the same problem> uninstall @tensorflow/tfjs-node and @tensorflow/tfjs> and then> npm install @tensorflow/tfjs-node> npm install @tensorflow/tfjs@^1.7.4> EXACTLY IN THIS ORDER> idk why, but when I install @tensorflow/tfjs@^1.7.4 first, it crashes with the same error you had> ![image](https://user-images.githubusercontent.com/44163887/94936092-778a5900-04d6-11eb-8330-b714aaf491f4.png)exactly the same dude :)====="", ""I'll go ahead and close this issue.  If you can think of a proper place to warn others of this issue with install order, please PR the docs.=====""]"
https://github.com/infinitered/nsfwjs/issues/388;"Memory leaking that can't be fixed using ""img.dispose()""";5;closed;2020-08-04T11:30:43Z;2020-12-11T16:34:25Z;"I'm currently having memory issues when classifying images. Here is my code:```jstf.enableProdMode(),async function classify(url){    var res = await axios.get(url, {        responseType: ""arraybuffer""    }),    if(!res || !res.data) return null,     tf.engine().startScope(),    var model = await nsfw.load(""file://model/"", { size: 299 }),    var img = await tf.node.decodeImage(res.data, 3),    var classes = await model.classify(img),    // Attempted to use "".dispose()"" but didn't make that much of a difference    img.dispose(),    var reviewed = {        sexy: {},        porn: {},        hentai: {}    },    classes.forEach(async(c) => {        if(c.className == ""Sexy"") reviewed.sexy = { name: ""explicit"", pr: c.probability },        if(c.className == ""Porn"") reviewed.porn = { name: ""pornography"", pr: c.probability },        if(c.className == ""Hentai"") reviewed.hentai = { name: ""hentai"", pr: c.probability },    }),    tf.disposeVariables(),    tf.engine().endScope(),    return reviewed,}```After around 80 classifications, memory usage has already passed 1.8GB:![image](https://user-images.githubusercontent.com/65198941/89288367-6e534c00-d64d-11ea-8eee-e45b09b2bb45.png)![image](https://user-images.githubusercontent.com/65198941/89288400-7f9c5880-d64d-11ea-9598-13b83871ecfa.png)I have also attempted to use:```js tf.engine().startScope(),// Classification code tf.engine().endScope(),```But it made no difference.Anyone know how I could possibly fix this?";"['I have managed to control the problem a little. I have used child processes to free the memory as soon as classification is finished.=====', ""Let me know what you use to solve this.  I haven't had that problem yet, but I want to see if it's specific to a version or something I can fix in code on my side.====="", 'It isn\'t really a fix but it at least allows memory to be freed as soon as classification finishes.**index.js**```jsvar { fork } = require(\'child_process\'),var forked = fork(\'./compute.js\'),forked.send({ url: toClassify }),forked.on(\'message\', async out => {    console.log(out),    if(out.err) return console.warn(""Classification failed.""),    // continue using output}),```**compute.js**```jsvar tf = require(""@tensorflow/tfjs-node""),var nsfw = require(""nsfwjs""),var axios = require(""axios""),tf.enableProdMode(),process.on(\'message\', async msg => {    var res = await run(msg.url),    process.send(res),    process.exit(),}),async function run(url){    var res = await axios.get(url, {        responseType: ""arraybuffer""    })    .catch(async(err) => {        process.send({ err: true }),        process.exit(),    }),    if(!res || !res.data){        process.send({ err: true }),        process.exit(),    }    tf.engine().startScope(),    var model = await nsfw.load(""file://model/"", { size: 299 }),    var img = await tf.node.decodeImage(res.data, 3),    var classes = await model.classify(img),        img.dispose(),        var reviewed = {        sexy: {},        porn: {},        hentai: {},        err: false    },        classes.forEach(async(c) => {        if(c.className == ""Sexy"") reviewed.sexy = { name: ""explicit"", pr: c.probability },        if(c.className == ""Porn"") reviewed.porn = { name: ""pornography"", pr: c.probability },        if(c.className == ""Hentai"") reviewed.hentai = { name: ""hentai"", pr: c.probability },    }),        tf.disposeVariables(),    tf.engine().endScope(),    return reviewed,}```=====', 'Can you dispose the model as well please?=====', 'I have tried using `tf.dispose(model)` but it makes little to no difference.=====']"
https://github.com/infinitered/nsfwjs/issues/383;NSFW Filter is released!;3;closed;2020-07-30T17:10:13Z;2021-01-04T20:20:28Z;Hi Team,We have released version 1.0.0 of NSFW Filter which was developed using nsfwjs. Thank you for your help in answering our queries. Now we need to get contributors to the project to improve it and add more features. If you could, give us a shout out in social media to reach more people.@GantMan Thanks for your help!Link: https://github.com/navendu-pottekkat/nsfw-filter;['https://twitter.com/FunMachineLearn/status/1288900451140808704=====', '@GantMan That is awesome!=====', 'Closing this ticket!   Really happy to see this great work.=====']
https://github.com/infinitered/nsfwjs/issues/375;Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.;2;closed;2020-07-20T02:50:40Z;2020-07-24T00:01:29Z;"Package works for me in development, but not on a production server. This is the error message I got and I don't know much more about it than what it says.`Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.`Here is my code. It is basically the simple example on the readme but with a couple of small changes:```<script src=""https://unpkg.com/@tensorflow/tfjs@1.2.8"" type=""text/javascript""></script><script src=""https://unpkg.com/nsfwjs@2.1.0"" type=""text/javascript""></script><script>    const img = new Image()    // image is hosted on the same domain as this page, so no crossDomain attribute is required    img.src = ""{{$url}}""    // Load the model.    nsfwjs.load(        'my/path/to/model/directory',        { size: 299 }    ).then(model => {        // Classify the image.        model.classify(img).then(predictions => {             // changed from console log to document write            document.write(JSON.stringify(predictions)),        })    })</script><body></body>```";"['It sounds to me like you might have CORS or some other cross origin protection in place.Try setting `img.crossOrigin = ""anonymous"",`=====', 'Looks like you were right on that one. I had too many problems with CORS settings, especially when handling images from unknown sites. So switching to a Node.js configuration made it a lot more predictable=====']"
https://github.com/infinitered/nsfwjs/issues/350;Response.arraybuffer is not a function, and other errors;1;closed;2020-06-03T22:03:21Z;2020-06-04T00:31:34Z;I have been trying to implement this to work with my discord bot. at first i kept getting Response.arraybuffer is not a function i later fixed that then a length error which i also fixed my current errors are Unsupported file type supports (insert file types here) and (node:37) UnhandledPromiseRejectionWarning: Error: values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray here is my current code https://pastebin.com/VwWF9rt2 I have started from the example with axios then tweaked it to fix errors. currently i am stuck ;['Fixed, had to use buffer instead of arraybuffer=====']
https://github.com/infinitered/nsfwjs/issues/338;Question: React Native Support?;2;closed;2020-05-11T18:45:23Z;2020-05-13T18:15:55Z;This package support React Native?;['Yup!   It takes some dancing, but here ya go:  https://github.com/infinitered/nsfwjs#react-nativeand code here: https://github.com/infinitered/nsfwjs-mobile=====', 'Thanks=====']
https://github.com/infinitered/nsfwjs/issues/336;Error: Size(200704) must match the product of shape 1,224,224,3;5;closed;2020-05-11T11:30:25Z;2020-05-29T07:48:03Z;"Hi.I'm using the module together with NodeJS, and when I run the `model.classify(image),` function it only successfully executes for a certain portion of images (roughly 50%)Sometimes, I get the error:```mdError: Size(200704) must match the product of shape 1,224,224,3```I have no control over what size the images are posted in (as the user sends them in whatever size), I can only resize them at max. ~~From my observations, it doesn't work for square or square-ish images, the rest seems to be ok, although I will continue my testings.~~ Edit: random images do that, not only square or square-ishI'm using the default model: `nsfwjs.load(),`My whole code:```jsconst photoUrl = ""domain.com/photo.png"", /* Note: this is only a placeholder */const pic = await axios.get(photoUrl, { responseType: `arraybuffer` }),let model = await nsfwjs.load(),let image = await tf.node.decodeImage(pic.data),let predictions = model.classify(image),/* Do something with predictions */```(It's almost directly copied from the readme files on Github & npm)";"[""That's strange.  It should resize the images for you.Can you assure you're using a new-ish version of TFJS?  I'm not an expert on the Node side, as that code was written by the community.   But the concepts seem simple enough.====="", ""I'm using the latest version of both tfjs & nsfwjs module.====="", ""Please create an open source repo of your example, I'll pull down the code exactly as you have it, and provide an image that fails in the demo.This will assure I solve the exact problem as I dig in.   I'd describe my steps, but they are going to be very ML specific.   I should have an answer of what's technically breaking and if I can fix it.====="", ""Can't, I don't fully own it.====="", 'I had the same issue today. I went through the react native files for [https://github.com/infinitered/nsfwjs-mobile/blob/master/App.js](NSFW-JS) and found a function called imagetotensor. I used the code from that to convert my array buffer to a 3d tensor, and it seems to work pretty well for me. I am getting slight issues, but I think they might just be false positives. If anyone sees any issue with the below code please let me know :)```javascriptvar sizeOf = require(\'buffer-image-size\'), //npm install buffer-image-sizeconst pic = await axios.get(photoUrl, { responseType: ""arraybuffer"" }),let model = await nsfwjs.load(),var dimensions = sizeOf(pic.data),const buffer = new Uint8Array(dimensions.width * dimensions.height * 3),let offset = 0, // offset into original datafor (let i = 0, i < buffer.length, i += 3) {    buffer[i] = pic.data[offset],    buffer[i + 1] = pic.data[offset + 1],    buffer[i + 2] = pic.data[offset + 2],    offset += 4,}const image = tf.tensor3d(buffer, [dimensions.height, dimensions.width, 3]),let predictions = model.classify(image),```=====']"
https://github.com/infinitered/nsfwjs/issues/332;Error: Cannot find module '@tensorflow/tfjs' and other error;2;closed;2020-05-05T14:00:58Z;2020-05-08T20:49:50Z;```Node: v13.12.0@tensorflow/tfjs-node: v1.7.4@tensorflow/tfjs: v1.7.4nsfwjs: 2.2.0```I tried to run NSFWJS on NodeJS,I copied the test code,```jsconst axios = require('axios') //you can use any http clientconst tf = require('@tensorflow/tfjs-node')const nsfw = require('nsfwjs')async function fn() {  const pic = await axios.get(`https://www.mankeynews.com/wp-content/uploads/2019/11/Anime.jpg`, {    responseType: 'arraybuffer',  })  const model = await nsfw.load() // To load a local model, nsfw.load('file://./path/to/model/')  // Image must be in tf.tensor3d format  // you can convert image to tf.tensor3d with tf.node.decodeImage(Uint8Array,channels)  const image = await tf.node.decodeImage(pic.data,3)  const predictions = await model.classify(image)  image.dispose() // Tensor memory must be managed explicitly (it is not sufficient to let a tf.Tensor go out of scope for its memory to be released).  console.log(predictions)}fn()```I installed the required dependencies (nsfwjs, axios and @tensorflow/tfjs-node)I ran the code, but it throws the following error```Cannot find module '@tensorflow/tfjs'Require stack:- %directory%\node_modules\@tensorflow\tfjs-node\dist\index.js- %directory%\nsfw.js```I installed `@tensorflow/tfjs`, but it returned another error```Unable to find the specified module.\\?\%path%\node_modules\@tensorflow\tfjs-node\lib\napi-v5\tfjs_binding.node```;"['Take a look at this ticket:  https://github.com/tensorflow/tfjs/issues/2046=====', ""Ok, I'll take a look at it=====""]"
https://github.com/infinitered/nsfwjs/issues/331;Error: FileReader.readAsArrayBuffer is not implemented;1;closed;2020-05-04T19:52:30Z;2020-05-04T20:33:17Z;Hi @GantMan @kevinvangelder ,I tried this code in a new react native project but I was getting this error. I used react-native 0.61.5 and react 16.9.0 to produce this error.[Unhandled promise rejection: Error: FileReader.readAsArrayBuffer is not implemented]- node_modules/react-native/Libraries/Blob/FileReader.js:84:20 in readAsArrayBuffer- node_modules/react-native/Libraries/vendor/core/whatwg-fetch.js:220:29 in readBlobAsArrayBuffer;['Closing in favor of https://github.com/infinitered/nsfwjs-mobile/issues/9=====']
https://github.com/infinitered/nsfwjs/issues/308;Please sepcify how to use the model in windows and proper guide;3;closed;2020-04-21T12:24:21Z;2020-04-30T03:06:05Z;i had been trying to figuring out how to run the model can you tell how to do this beacuse i am not familiar from js ;['What are you familiar with?  So I know how to explain.=====', 'i am familiar with python background so if you could tell how to deploy this project from scratch would be a great help coz i find this model performance in your demo very impressive.Also can this model be deployed in an android app and what is the size of that model.=====', 'Take a look here:  https://github.com/GantMan/nsfw_model=====']
https://github.com/infinitered/nsfwjs/issues/291;Load the retrained model;1;closed;2020-04-03T08:34:24Z;2020-04-30T03:58:32Z;"> I assume you were trying to classify on node?> > Have you looked at how to convert a PNG to a tensor? https://stackoverflow.com/questions/53231699/converting-png-to-tensor-tensorflow-js----------------------@GantMan Thank you very much for your reply, this issue has been resolved.I am now experiencing another problem. I trained a batch of images with **nsfw_model / train_all_model.sh** (using https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4) and then I used tensorflowjs- The converter converted from **keras** or **keras_saved_model** to **tfjs_layer_model** and succeeded. But when I reference the model in nsfwjs-node, I get an error message:```2020-04-02 15:45:40,085 ERROR 70582 nodejs.unhandledRejectionError: Unknown layer: KerasLayer. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().    at new ValueError (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/errors.js:68:28)    at Object.deserializeKerasObject (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js:260:19)    at Object.deserialize (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js:29:28)    at Sequential.fromConfig (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:958:41)    at Object.deserializeKerasObject (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js:294:29)    at Object.deserialize (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js:29:28)    at /Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:290:45    at step (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:54:23)    at Object.next (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:35:53)    at fulfilled (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:26:58)name: ""unhandledRejectionError""pid: 70582hostname: bogon```How should I train the models that nsfwjs-node can use?Looking forward to your reply, thank you again!_Originally posted by @wunamesst in https://github.com/infinitered/nsfwjs/issues/274#issuecomment-607691294_";"[""Here's a ton of code you can use to train on whatever data you like:  https://github.com/GantMan/nsfw_model=====""]"
https://github.com/infinitered/nsfwjs/issues/282;Memory Leaking;2;closed;2020-03-29T10:33:29Z;2020-04-24T07:47:36Z;Hey, every image that I classify it, the more memory consumes it.Is there any way to reduce it like deleting cache or something? It's really weird.;"['You must mean on server?See this item:  https://github.com/infinitered/nsfwjs/issues/49=====', 'Yup! #49 is definitely the fix for this. I was having the same issue.Whatever object you send to "".classify()"", just run "".dispose()"" after.=====']"
https://github.com/infinitered/nsfwjs/issues/278;tf.enableProdMode;3;closed;2020-03-27T03:30:36Z;2020-03-27T07:25:02Z;Any reason why the library doesn't enable prod mode?  Should this be left up to the developer?  If so, should documentation point it out?https://www.tensorflow.org/js/guide/platform_environment#flags;"[""That's an excellent question.I didn't want to assume prod mode, because it's not something you can turn on and back off.  Enabling debug mode doesn't disable prod mode.  Even though it seems like those are switches, they both just turn on and not off.  (reference https://github.com/tensorflow/tfjs/issues/1972)So I leave this to the site owner.====="", ""BTW, if you'd like to contribute to the documentation stating to enable prod mode, I'd accept that PR!====="", ""Would you like to expose this through nsfwjs's apis, or make people import TF directly?=====""]"
https://github.com/infinitered/nsfwjs/issues/276;Higher accuracy model;11;closed;2020-03-26T08:57:09Z;2020-05-01T02:03:13Z;Does anyone have success in training the higher accuracy model(s)?;"['The 93% accuracy is held back by the strange line between sexy/porn.I dare say if you combined those two, the accuracy would go up significantly.=====', ""I have 250GB of different images and I am experiencing a lot of false positives: a) text banners rarely flagged as drawings,b) decent / neutral photos flagged as porn.I had to code some tricky logical prediction normalizer to decrease these cases, but it doesn't help that much. It seems I need to train my own model ))====="", ""Just to verify, can you provide some of those false positives here?   Along with whatever model you're using?  We have several published versions now.====="", 'I have the same problem. I have a lot of pictures with only the upper body, but because of the hot weather, many people wear very few clothes or no clothes. These are not elegant, but this type of photos are recognized as neutral So I want to retrain the model. Any other suggestions? Thank you!=====', '@wunamesst - please take a look at the new 6.1 MB model and let me know how it performshttps://nsfwjs.com/=====', ""@GantMan https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/quant_mid  this model doesn't work with nodejs, gives this error: https://pastebin.com/Me0PGUFp====="", '**Hey @qwertyforce - so here\'s the deal and maybe you can help me fix our example?  It\'s not complicated, but this will come across as wordy.**That new model is a graph model.   More specifically it\'s a DAG.DAGs are fast/portable, but cannot be used for inference.   The 2.6MB model is a ""Layers"" model.   Which is slightly slower on loading/perf but can be disected for inference/transfer learning.To tell TensorFlow.js which model it\'s working with, you use `loadLayersModel` or `loadGraphModel`.Originally NSFWJS only did Layers models, until I did this update:  https://github.com/infinitered/nsfwjs/commit/1368d6dd1e2be83e257621124cba508fefc3e07fCan you please point your code at Master on GitHub, and try loading your model with the options set to `{ type: ""graph"" }` and then it should work.If it does!!!  I\'ll take in any PR you might have to help document this AND release a new version so you don\'t have to point at GitHub.Sorry, but the code was only updated last night!  Hah.=====', '@GantMan It\'s not working :(file://./model/ = https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelfile://./model_new/ = https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/quant_midfile://./model/ - workingfile://./model_new/ - not workingHere is my example```const axios = require(\'axios\'),const tf=require(\'@tensorflow/tfjs-node\')const nsfw = require(""nsfwjs""),async function fn() {  let pic= await axios.get(`https://i.imgur.com/bFDWhkK.jpg`,{responseType: \'arraybuffer\' })  // const model = await nsfw.load(\'file://./model/\',{size: 299}),  const model = await nsfw.load(\'file://./model_new/\',{ type: ""graph"" }),  const image = tf.node.decodeImage(pic.data),  const predictions = await  model.classify(image),  console.log(predictions)}fn()````=====', ""And you're 100% sure you're pointed at Master for NSFWJS?What version of tfjs-node do you have?  The latest?====="", '@GantMan  I am so sorry... I have just built it from source and everything is working! =====', 'awesome!  Good!=====']"
https://github.com/infinitered/nsfwjs/issues/274;How to classify PNG?;2;closed;2020-03-23T02:31:22Z;2020-04-02T08:12:04Z;Hello, I tried to classify a png image in nodejs, but it throws an exception: Size (357604) must match the product of 1,299,299,3 shapes. How can this be resolved?;"['I assume you were trying to classify on node?Have you looked at how to convert a PNG to a tensor?  https://stackoverflow.com/questions/53231699/converting-png-to-tensor-tensorflow-js=====', '> I assume you were trying to classify on node?> > Have you looked at how to convert a PNG to a tensor? https://stackoverflow.com/questions/53231699/converting-png-to-tensor-tensorflow-js----------------------@GantMan Thank you very much for your reply, this issue has been resolved.I am now experiencing another problem. I trained a batch of images with **nsfw_model / train_all_model.sh** (using https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4) and then I used tensorflowjs- The converter converted from **keras** or **keras_saved_model** to **tfjs_layer_model** and succeeded. But when I reference the model in nsfwjs-node, I get an error message:```2020-04-02 15:45:40,085 ERROR 70582 nodejs.unhandledRejectionError: Unknown layer: KerasLayer. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().    at new ValueError (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/errors.js:68:28)    at Object.deserializeKerasObject (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js:260:19)    at Object.deserialize (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js:29:28)    at Sequential.fromConfig (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:958:41)    at Object.deserializeKerasObject (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js:294:29)    at Object.deserialize (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js:29:28)    at /Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:290:45    at step (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:54:23)    at Object.next (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:35:53)    at fulfilled (/Users/wuguofeng/www/nodejs/classify/node_modules/nsfwjs-node/node_modules/@tensorflow/tfjs-layers/dist/models.js:26:58)name: ""unhandledRejectionError""pid: 70582hostname: bogon```How should I train the models that nsfwjs-node can use?Looking forward to your reply, thank you again!=====']"
https://github.com/infinitered/nsfwjs/issues/261;Pre-trained model for Keras?;1;closed;2020-02-27T14:19:44Z;2020-02-27T14:31:25Z;We're looking to try the model with Keras in Python. The problem is that the current pre-trained model is in the `tfjs_graph_model` format and there doesn't seem to be a way to convert it for Keras. Is there a chance for releasing a Keras version?;"[""Oops, sometimes it's useful to read the [FAQ](https://github.com/infinitered/nsfwjs/wiki/FAQ:-NSFW-JS) ,)=====""]"
https://github.com/infinitered/nsfwjs/issues/253;Question: Must the supplied image be 224 x 224?;2;closed;2020-02-14T20:50:04Z;2020-02-15T20:32:19Z;(or 299 x 299 with the bigger model?)If so, how would you suggest classifying an image with a different ratio?;['NSFWJS will automatically resize the input image to be square for you.No code should be needed on your side.=====', 'Awesome. Thanks=====']
https://github.com/infinitered/nsfwjs/issues/219;Using s3 bucket works but when I switch to local model I get an error;2;closed;2019-12-05T01:02:14Z;2019-12-05T19:04:46Z;"```Error: Error when checking : expected input_1 to have shape [null,299,299,3] but got array with shape [1,224,224,3].    at new t (tf-layers.esm.js?271e:17)    at checkInputData (tf-layers.esm.js?271e:17)    at t.predict (tf-layers.esm.js?271e:17)    at eval (index.js?9368:92)    at eval (tf-core.esm.js?45ef:17)    at t.scopedRun (tf-core.esm.js?45ef:17)    at t.tidy (tf-core.esm.js?45ef:17)    at Module.je (tf-core.esm.js?45ef:17)    at NSFWJS.eval (index.js?9368:91)    at step (index.js?9368:33)``````html<template>  <div>    <img ref=""img"" src=""@/assets/img.jpg"">  </div></template>``````javascript<script>import * as nsfwjs from ""nsfwjs"",export default {  async mounted () {    // const model = await nsfwjs.load(),    // ""/static/model/"" is mapped as ""/model/"" using Nuxt.js    const model = await nsfwjs.load(""/model/""),    const predictions = await model.classify(this.$refs.img),    console.log(""Predictions: "", predictions),  }},</script>```";"[""I see the issue.  You're using the larger model, right?The larger model takes slightly larger images (299 pixels), so you can tell it you're using the more advanced model like so: `const model = nsfwjs.load('/path/to/different/model/', {size: 299})`The 299 tells it to expect 299x299 tensors on the model.  Having 2 models with different sizes is the issue.  I have a note in the docs, but it's easy to miss.I hope this fixes it!====="", 'It fixed the problem. Thanks.=====']"
https://github.com/infinitered/nsfwjs/issues/196;Where is the FAQ?;2;closed;2019-11-06T05:43:20Z;2019-11-06T06:29:43Z;https://github.com/infinitered/nsfwjs/wiki/FAQ redirects to home page;"[""@jamonholmgren - is wiki turned off for our org?   I had a WIKI setup and now it's gone.  Not only is it gone, I can't seem to find the wikipage link!   I can find one in my personal GitHub.   So I'm at a bit of a loss of where the wiki went.It held some pretty critical info.====="", ""UPDATE:  It was turned off.... by whom I have no clue!I turned it back on in settings.   ![image](https://user-images.githubusercontent.com/997157/68273490-32427a80-002c-11ea-8abd-763f534d4c5e.png)Sorry @mifi - looks like it's back now, and still contained all the important data.=====""]"
https://github.com/infinitered/nsfwjs/issues/172;Pre-trained model link is dead;1;closed;2019-09-18T16:32:28Z;2019-09-25T23:25:37Z;When downloading the model through the s3 at https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/ The following error code is thrown:```<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/</Key><RequestId>8ECD5C58025A96EC</RequestId><HostId>0wNKLqrKG2SyXYZ4d//i3otFu2uOq+OE7xEQkACROa/HRY/+vfIINBlAqG3oYATZbUVxtFoz44A=</HostId></Error>```;['add model.jsonhttps://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json=====']
https://github.com/infinitered/nsfwjs/issues/167;How to use in nodejs(koa2)?;1;closed;2019-09-16T07:40:43Z;2019-09-18T02:01:57Z;;['https://github.com/infinitered/nsfwjs/wiki/FAQ:-NSFW-JS#can-i-use-this-model-in-backend-node-with-tfjs-node=====']
https://github.com/infinitered/nsfwjs/issues/139;Using without third-party server?;5;closed;2019-07-19T00:38:12Z;2019-07-19T14:50:36Z;Is there any way to use this offline aka do not use from your S3 server? Maybe there is a way to download the existing trained data on your server which can be used at offline. Why? It could be complicated due DSGVO and the less there are third party servers, the less complicated it gets. 😄 or am I missing here?;"['Yes! You can host the files locally on your site, computer, cache etc.  You can download them and point the code to load models from anywhere.See here:  https://github.com/infinitered/nsfwjs#load-the-model=====', 'Thanks for fast reply! Sound good. Where can I download the existing trained data? 👏 =====', 'The large one 93% accurate can be found here:  https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelThe smaller one 90% accurate but 1/10th the size here:https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/quant_nsfw_mobilenetThese are the two folder I use for NSFWJS.com - neither uses our S3=====', 'Great, thank you so much 😄 One last question, do I only need the .json file or also this ""group1_shardXof6"" too?=====', ""Ah it does. Ok, got everything what I have. Thanks for the project! 👏 I'm going to have fun experiments with them.=====""]"
https://github.com/infinitered/nsfwjs/issues/128;Classifying Image data?;2;closed;2019-06-28T19:49:14Z;2019-07-03T17:32:00Z;I was curious about the capabilities of NSFWJS, when it came to classifying Image data, instead of an actual element. In the **Parameters** of **ClassifyImage**, it states: > Tensor, Image data, Image element, video element, or canvas element to checkSo does that mean I could potentially pass an images base64 data to NSFWJS, and it would render the results, or does it have to be the actual image element itself?I'm just curious about the applications of this, and how it could be used server side.;"[""Basically, I get to use the awesomeness of Tensorflow JS here.Whatever TFJS can convert, NSFWJS can read.   The Image data portion is the web ImageData (https://developer.mozilla.org/en-US/docs/Web/API/ImageData)Here's some good info on reading base64 encoding:  https://github.com/tensorflow/tfjs/issues/1485====="", 'Awesome, I figured it could take the image data, I was mainly looking for a confirmation of it. Thanks!=====']"
https://github.com/infinitered/nsfwjs/issues/119;Invalid mode.json file;3;closed;2019-06-10T12:26:40Z;2021-01-04T20:21:39Z;To host model on our own server in readme.md you've provided link to https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelI was trying to host these on my server and was facing issues but working absolutely fine with the default files then figured out that `model.json` from `demo` folder is different from the one you're using as default here `https://s3.amazonaws.com/ir_public/nsfwjscdn/TFJS_nsfw_mobilenet/tfjs_quant_nsfw_mobilenet/model.json`happy to send pull request, if you can confirm that is the issue.;"['Also in the paths section of the hosted `model.json` you only have one path which is `group1-shard1of1` but in github in demo folder there are 6 paths```""paths"": [""group1-shard1of6"",""group1-shard2of6"",""group1-shard3of6"",""group1-shard4of6"",""group1-shard5of6"",""group1-shard6of6""],```which one is valid and which one gives more accurate results?=====', ""I've created two models now.One that is very small that is 90% accurate, and one that is 23MB large, but is 93% accurate.You can see that I've added this to the main site:![image](https://user-images.githubusercontent.com/997157/60608313-c651bc00-9d84-11e9-925a-0c863ea2a888.png)Any updates to the readme/docs would be greatly appreciated!!!====="", 'у меня всё работает=====']"
https://github.com/infinitered/nsfwjs/issues/118;React Native Support;3;closed;2019-06-09T16:09:11Z;2019-10-08T22:19:51Z;How hard will be to use or port this to React Native? Right now, the content analyzed is based on a DOM node. But maybe there's a way to pass an image/video URL or a base64 to analyze? ;"[""### I'd love for this to work with React NativeThe big issue right now is that TFJS doesn't work with RN.   You COULD however, use a webview (hah!) and take the results and report them back to RN!I've heard from Google they hope to support RN at some point, but that will be a while.====="", '@mariano-meitre Good news! https://twitter.com/tafsiri/status/1169312345358815233=====', 'There are some rough edges, but it is now possible to use NSFWJS in a React Native app. https://shift.infinite.red/nsfw-js-for-react-native-a37c9ba45fe9=====']"
https://github.com/infinitered/nsfwjs/issues/85;Where is the javascript code to put on the web-server?;8;closed;2019-04-15T16:46:25Z;2019-04-21T02:37:45Z;"About the nsfwjs we can read the following:""A simple JavaScript library to help you quickly identify unseemly images, all in the client's browser.""So, where is the actual javascript that we can upload to our web-server, and then use on the client side, to ""identify unseemly images""?It would be nice if that particular javascript was easily accessible from this github page. That way it would be a lot quicker to implement in various web applications.I have tested the demo page, which is to be found here: https://nsfwjs.com/Have tested it with several images which contain e.g. various sexy and porn content. It works like expected. So far so good. But the javascript code used on that demo page, is not usable in my web-application.So where is the actual javascript code that I can implement in my web-application?Does this module require node.js to work?What did I miss?";"['Hi @bongobongo The Readme has a section titled ""How to use the module"", which is located here:https://github.com/infinitered/nsfwjs#how-to-use-the-moduleThe readme then goes on to describe the API of the module.  Can you try these, and if they are not useful for you, could you elaborate on how you are limited?  =====', 'One more note bongo!  I see you saying ""web server"" so if you\'re hosting with node (server side), you can use this library there as well.  See the FAQ:  https://github.com/infinitered/nsfwjs/wiki/FAQ:-NSFW-JS=====', 'Okay, but if nsfwjs is just plain client side javascript, then I do not understand why the javascript files is not available here as it should be used on the client side. Do I need node.js to make this work or not?No node.js is available on the web-servers I use.Just using this line client side: import * as nsfwjs from \'nsfwjs\',gives this error in javascript: ""Unexpected token *""=====', ""ok good just client side JavaScript.No need for node if you're hosting the files for client JS.Is your JavaScript es6+?  Are you looking to use script tags?  It sounds to me like you need a non NPM example.====="", 'Here, I\'ve made the simplest possible example:  Please let me know if this helps.https://github.com/infinitered/nsfwjs/blob/master/example/minimal_demo/index.html```html<!-- Load TensorFlow.js. This is required --><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.4""></script><!-- Load the NSFWJS library. --><script src=""https://s3.amazonaws.com/ir_public/nsfwjscdn/bundle.js""></script><script>  const nsfwjs = require(\'nsfwjs\')  const img = new Image()  img.crossOrigin = \'anonymous\'  // some image here  img.src = \'https://i.imgur.com/Kwxetau.jpg\'  // Load the model.  nsfwjs.load().then(model => {    // Classify the image.    model.classify(img).then(predictions => {      console.log(\'Predictions\', predictions)    })  })</script><pre>Checkout console.log output for results!</pre>```=====', 'This is getting closer to something I can use. The code you provided do work, but:What I need to do now is to be able to call model.classify in a separate function. So I tried this:const nsfwjs = require(\'nsfwjs\')const model = nsfwjs.load(),Then in function analyzeImage I put this code:const img = new Image(),img.crossOrigin = \'anonymous\',img.src = \'http://localhost/atest-code/geocode/pixlab/images/35.jpg\',predictions = model.classify(img),console.log(\'Predictions: \', predictions),WHAT happens now is that I click a button which run the analyzeImage function, I get this error:""model.classify is not a function""So how do I use the ""model.classify(img)"" from within a function like this?I would not like to load the model each time an image is classified, if possible to avoid. =====', 'So the issue looks like scope.  One way to keep a reference to a variable is to fasten it on your global window object. Like“window.model = nsfwjs.load()”And you can later access it with window.model=====', ""Thanks a lot for your help. It's working fine now.=====""]"
https://github.com/infinitered/nsfwjs/issues/79;Nice Feature Idea - Scrub GIF;8;closed;2019-04-04T22:18:37Z;2019-04-19T19:00:36Z;It would be pretty cool if when you scubbed the victory bar for a GIF, the associated GIF frame was shown.  This can happen by having an option passed in for a reference to the SuperGIF and then using `move_to(index)`.  If you don't want the supergif, it's set to NULL in memory like it is currently.I'd love to see why a particular frame is misclassified!;"['@xilaraux - sorry I took your last ticket, would you want this one?=====', ""@GantMan don't be sorry :) I can handle it, but it will take a while for me.====="", ""Awesome! Off the top of my head here's a simple implementation:* [`classifyGif`](https://github.com/infinitered/nsfwjs/blob/76948557feee91a7f0df097ac6f7a98968ef6485/src/index.ts#L153-L188) takes options.  One option is an object to store the SuperGif * When you use this option, [SuperGif is not set to null in `classifyGif`](https://github.com/infinitered/nsfwjs/blob/76948557feee91a7f0df097ac6f7a98968ef6485/src/index.ts#L183)* The web page now has a reference tot he SuperGif* [Victory Bar](https://formidable.com/open-source/victory/docs/victory-bar) Charts can have an `onHover` event described in their docs.  Add that to the existing [GifBar](https://github.com/infinitered/nsfwjs/blob/76948557feee91a7f0df097ac6f7a98968ef6485/example/nsfw_demo/src/components/GifBar.js#L48-L74)* The `onHover` calls the SuperGif [`move_to(i)`](https://github.com/buzzfeed/libgif-js) - I think plus 1 - check index off by one possibility here### TADAAAAAAA 🎉 ====="", ""You almost did it.I will implement it. :) I don't understand one thing. Should the frame be displayed next to [this](https://github.com/infinitered/nsfwjs/blob/76948557feee91a7f0df097ac6f7a98968ef6485/example/nsfw_demo/src/components/GifBar.js#L51) text or somewhat separately? ====="", 'Hahahaha.So I think if you call `GifReference.move_to(i)` the GIF they dropped will lock to that frame.  No need to show it anywhere.  Basically it will stop animating and move to that frame.  I think.... :)  Once you start experimenting it should be clear.  Then when you stop mousing over the graph, it would be cool if it went back to animating.=====', '@xilaraux - how goes it?=====', '@GantMan You can see the progress here - #83 I want to propose to name SuperGif object as `gifControl` and `setGitControl` and prop in config. What do you think? Also, I have doubts about the way I am setting this object and saving if to the React state, but spoiling the window is not an option neither.Looking forward to your review :) Sorry for the delay.=====', 'Looks great, I updated your contributor status in the readme:  https://github.com/infinitered/nsfwjs/commit/5413bd0e8541e3bf555a355712ef0959d3e0f04e=====']"
https://github.com/infinitered/nsfwjs/issues/68;Add CI;6;closed;2019-03-31T05:19:45Z;2019-04-04T17:19:03Z;There are tests.  Let's add CI for verification on PRs;['I can take it.=====', 'Awesome @xilaraux - looking forward to it.  Let me know what you need from me.=====', 'Need any help @xilaraux ?=====', '@GantMan I was a little bit busy, will complete it soon.Thx for asking. =====', '@GantMan Here it goes: https://github.com/infinitered/nsfwjs/pull/77=====', 'Merged and it looks good!  Will add you to the contributors now.=====']
https://github.com/infinitered/nsfwjs/issues/63;nodejs execution;1;closed;2019-03-27T14:49:36Z;2019-03-27T17:18:40Z;Hello! Is possibile to use with nodejs in a backend environment? Because i have seen only client-side javascript example.Thank you;"[""Hiya!  Someone seems to have done this in #25 Please let me know if it works for you.  I'm hoping to add a Wiki section on this, and your experience would be most helpful. Gonna close this ticket, so update us over at #25 if you can 👍 =====""]"
https://github.com/infinitered/nsfwjs/issues/62;error TS2348;6;closed;2019-03-27T00:32:03Z;2019-04-05T01:32:24Z;any idea?```shellC:\Users\Jae Jin\Desktop\HUB\nsfwjs>yarn prepyarn run v1.15.2$ yarn && yarn build && cd example/nsfw_demo/ && yarn add ../../ && cd -[1/4] Resolving packages...success Already up-to-date.$ tsc --skipLibChecksrc/index.ts:103:47 - error TS2348: Value of type 'typeof Model' is not callable. Did you mean to include 'new'?103           this.intermediateModels[endpoint] = tf.Model({                                                  ~~~~~~~~~~104             inputs: this.model.inputs,    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~105             outputs: layer.output    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~106           })    ~~~~~~~~~~~~Found 1 error.```;"[""What version of Tensorflow do you have?  I might know what's happening.====="", 'Tensorflow/tfjs@0.15.3=====', 'Closed for now Answer for a week.=====', ""sorry @Kadantte -  I'll be upgrading the project to Tensorflow 1.x like in ticket #61 - It might fix your issue then.  I can't recreate this bug, so I'm hoping that fixes it.====="", 'PR to upgrade to TFJS 1.x is in #78 =====', ""Thank's you very much.=====""]"
https://github.com/infinitered/nsfwjs/issues/61;Support TensorflowJS 1.x;3;closed;2019-03-26T11:11:33Z;2019-04-04T23:49:03Z;Currently we are using Tensorflow 0.15.x, but we should support latest tensorflow.Update and fix to work with latest.;"['Will take it.Should it be moved to the latest version?=====', 'Sorry, @xilaraux - I just knocked this one out.  Great minds think alike!=====', ""I wouldn't mind a review!   https://github.com/infinitered/nsfwjs/pull/78=====""]"
https://github.com/infinitered/nsfwjs/issues/60;Tensorflow logo change;1;closed;2019-03-26T10:41:13Z;2019-04-04T14:25:35Z;![image](https://user-images.githubusercontent.com/997157/54992668-d855de80-4f8d-11e9-8535-b3c7894ec47b.png)Update brand logo on the demo site. Currently it's the old logo. ;['Adhere to brand guidelines.https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf=====']
https://github.com/infinitered/nsfwjs/issues/59;Example - classifyGif;5;closed;2019-03-21T14:44:14Z;2019-04-02T16:48:05Z;classifyGif was added in #57 Add code to demo app that will properly prompt user to wait while gif is being processed, and then show the user the output in a meaningful way.  I'm thinking graph, possibly with D3.;"['Presenting the gif classification can look like this:  https://observablehq.com/d/c70415332a1f4cb9Gif output can be converted to a readabe chart format with the following transform```javascriptlet results = {name: ""results"", children: []}data.map((frame, i) => {  const lastChild = results.children.slice(-1)[0]  const currentClass = frame[0].className  if (lastChild && currentClass == lastChild.name) {    // Add frame to entry    lastChild.children.push(      {""name"": ""frame "" + (i+1), ""value"": 1}    )  } else {    // Create new    results.children.push({      name: currentClass,      children: [        {""name"": ""frame "" + (i+1), ""value"": 1}      ]    })  }})```This comes out looking like so!![image](https://user-images.githubusercontent.com/997157/54861960-fe1a8380-4d00-11e9-9588-3f9c8b7d38a8.png)=====', ""So getting d3 working in React is a pain.Even though I have all the code for d3, I apparently can't get my branch to render the SVG.  I might abandon the demo of `classifyGif` and just document it.====="", 'The branch is gifGraph if someone wants to take a stab:https://github.com/infinitered/nsfwjs/tree/gifGraph=====', 'Thinking of switching to bar graph.=====', 'GIF is in demo now.  No graphs yet.=====']"
https://github.com/infinitered/nsfwjs/issues/50;tensorflow .pb model wanted;3;closed;2019-03-16T03:21:30Z;2019-06-05T06:00:11Z;I have found some nsfw's trained model or Demo, but yours perform best.Could you upload the trained .pb model to help me develop my java Demo?Thanks a lot.;"[""Hiya!  Yup!!!The training code and `.pb` + others are located here:https://github.com/GantMan/nsfw_modelThanks so much for your kind words.  Feel free to let me know how it works for you, as I haven't tested the .pb yet.====="", 'Closing because question is moved to https://github.com/infinitered/nsfwjs/wiki/FAQ:-NSFW-JS=====', ""> Hiya! Yup!!!> > The training code and `.pb` + others are located here:> https://github.com/GantMan/nsfw_model> > Thanks so much for your kind words. Feel free to let me know how it works for you, as I haven't tested the .pb yet.I have compared the results between pb and tensorflowjs , the pb model and keras model seems not working, much worse than the js demo=====""]"
https://github.com/infinitered/nsfwjs/issues/49;Memory issues;8;closed;2019-03-13T20:03:13Z;2020-04-24T18:49:53Z;"I'm noticing some serious memory issues when using NSFWJS with `tfjs-node`. The memory footprint increases over time. Here's some code to test and see what I mean:**./lib/nsfwjs-inputs.js**```jsconst tf = require('@tensorflow/tfjs-node')const fs = require('fs')const jpeg = require('jpeg-js')const fetch = require('node-fetch')const readImageFile = path => {	return new Promise((resolve, reject) => {		fs.readFile(path, (err, buf) => {			if (err) return reject(err)			const pixels = jpeg.decode(buf, true)			resolve(pixels)		})	})}const readImageUrl = async url => {	let res = await fetch(url)	let buf = await res.arrayBuffer()	const pixels = jpeg.decode(buf, true)	return pixels}const imageByteArray = (image, numChannels = 3) => {	const pixels = image.data	const numPixels = image.width * image.height	const values = new Int32Array(numPixels * numChannels)	for (let i = 0, i < numPixels, i++) {		for (let channel = 0, channel < numChannels, ++channel) {			values[i * numChannels + channel] = pixels[i * 4 + channel]		}	}	return values}const imageToInput = (image, numChannels = 3) => {	const values = imageByteArray(image, numChannels)	const outShape = [image.height, image.width, numChannels]	const input = tf.tensor3d(values, outShape, 'int32')	return input}const inputFromFile = async (path, numChannels = 3) => {	const image = await readImageFile(path)	const input = imageToInput(image, numChannels)	return input}const inputFromUrl = async (url, numChannels = 3) => {	const image = await readImageUrl(url)	const input = imageToInput(image, numChannels)	return input}module.exports = {	inputFromFile,	inputFromUrl,}```**./nsfwjs-resource-test.js**```jsprocess.title = 'NSFWJS Resource Test'const fs = require('fs')const nsfwjs = require('nsfwjs')const nsfwjsInputs = require('./lib/nsfwjs-inputs.js')const pathJoin = require('path').joinconst IMG_DIR = pathJoin(__dirname, 'test-imgs')try {	fs.accessSync(IMG_DIR)} catch (err) {	fs.mkdirSync(IMG_DIR)}async function main() {	let nsfw = await nsfwjs.load('file://./nsfwjs_model/')	let files = fs.readdirSync(IMG_DIR)	let outputFile = fs.createWriteStream('./analysis.txt')	let counter = 0	for (let file of files) {		const filepath = pathJoin(IMG_DIR, file)		let input = await nsfwjsInputs.inputFromFile(filepath)		let predictions = await nsfw.classify(input)		let predictionMap = predictions.reduce(			(map, prediction) => ({				...map,				[prediction.className]: prediction.probability,			}),			{}		)		outputFile.write(`${file} = ${JSON.stringify(predictionMap)}\n`, 'utf8')		console.log(++counter)	}}main().catch(err => console.error(err) && process.exit(1))```You'll need to have images in a `./test-imgs` directory ready for this test. After only 100 images, I see the ""NSFWJS Resource Test"" process at 1 GB of memory. I'm noticing 5 GB at only 500 images processed. The longer the program runs, the more memory is consumed.This leads me to believe there is a memory leak somewhere. Either it's in the `nsfwjs-inputs.js` file, tensorflow, or NSWFJ.";"[""Modifying the file to remove the classification code appears to not suffer memory leakage:```jsprocess.title = 'NSFWJS Resource Test'const fs = require('fs')const nsfwjs = require('nsfwjs')const nsfwjsInputs = require('./lib/nsfwjs-inputs.js')const pathJoin = require('path').joinconst IMG_DIR = pathJoin(__dirname, 'test-imgs')try {\tfs.accessSync(IMG_DIR)} catch (err) {\tfs.mkdirSync(IMG_DIR)}async function main() {\tlet nsfw = await nsfwjs.load('file://./nsfwjs_model/')\tlet files = fs.readdirSync(IMG_DIR)\tlet outputFile = fs.createWriteStream('./analysis.txt')\tlet counter = 0\tfor (let file of files) {\t\tconst filepath = pathJoin(IMG_DIR, file)\t\tlet input = await nsfwjsInputs.inputFromFile(filepath)\t\t/*let predictions = await nsfw.classify(input)\t\tlet predictionMap = predictions.reduce(\t\t\t(map, prediction) => ({\t\t\t\t...map,\t\t\t\t[prediction.className]: prediction.probability,\t\t\t}),\t\t\t{}\t\t)*/\t\t//outputFile.write(`${file} = ${JSON.stringify(predictionMap)}\\n`, 'utf8')\t\tconsole.log(++counter)\t}}main().catch(err => console.error(err) && process.exit(1))```So it has to be an issue with NSFWJS.====="", ""Hi @samholmes -  I am upgrading to TFJS 1.x here in this branch: https://github.com/infinitered/nsfwjs/pull/78Once merged, I'd love for you to check again and see if you're still having your issue.====="", 'I solved it by using `input.dispose()`.=====', '@samholmes could you shed some more light on input.dispose?  What is input?=====', ""@camhart - as you see above where he made some functions called `imageToInput` etc.?When you convert an image to a tensor it loads it into GPU.   To garbage collect, you need to call `.dispose` on the tensor.   So once the tensor has been used, you should dispose it.If you'd like to learn more about tensors, I suggest my course:https://academy.infinite.red/p/beginning-machine-learning-with-tensorflow-js====="", 'THANK YOUI had the same issueI disposed the input, and now the memory usage is stable :)=====', 'This!I really think this should be documented, as I was having memory leaks, having to restart the app a few times a day due to several alerts about RAM being almost full on the server!The fix is so simple, and definitely something that should be done anyway.Whatever object you send to "".classify()"", just run "".dispose()"" after.=====', ""I'd love to have a PR of where we should mention this in the docs!  Plz PR=====""]"
https://github.com/infinitered/nsfwjs/issues/38;Run on every frame inside a GIF;4;closed;2019-03-04T20:52:43Z;2019-03-21T13:53:05Z;Currently only one frame is selected.  Whatever was visible when the classification was run.Better to grab all frames from a gif:Use this library: https://github.com/buzzfeed/libgif-jsHere's an example of grabbing all frameshttps://jsfiddle.net/zpmgv7k4/The question is... How do we report this?# Discussion portionDo we report back the prediction on every frame?  Or do we average all the frames?  Both?;"['Hi! I found this project and sounds cool!Just clarifying, is the gif going to be combine together again as an animated GIF?=====', 'My thought, is that it would return an array of the classification of each frame of the GIF.So if a gif has 16 frames, it returns 16 classifications.  This might be perfect, for people to set their own threshold on how many ""dirty"" images make the whole GIF ""dirty"" etc.=====', '@emer7 you planning on jumping on this one?=====', 'Oh okay, yeah I would like to try to implement this feature=====']"
https://github.com/infinitered/nsfwjs/issues/32;video support;6;closed;2019-02-28T18:20:15Z;2019-03-20T21:32:12Z;Hi! Are you planning to release video support? thanks;"[""I'll keep working on it.  Video means a few things.* I could access the webcam and do live video realtime checking.* I could scan a video and express if it's safe or not. As of right now, you can pass a video element to the detector, but I believe it will only process the first frame of the video, not all of them.I hope to add both those features mentioned above.  Wish me luck!====="", ""ok, what i'm interested is second option.Thank you for your response and for your work!====="", ""I've created tickets #38 and #39 for the processing of a video.I'll use this ticekt for live webcam video updates.   Please subscribe the others as you wish.  this ticket will be closed when Webcam support is complete (meaning the example).====="", 'ok, thank you!=====', ""We have webcam working but it's insanely slow.  Not sure if I'll release this.====="", 'Nevermind, I figured out how to fix the speed issue.  Will release soon.=====']"
https://github.com/infinitered/nsfwjs/issues/31;Blocked ui;6;closed;2019-02-28T14:31:42Z;2019-04-30T09:35:59Z;I found ui been blocked for few seconds when execute ”nsfwjs.load('/path/to/model/directory/') “. How to optimize it?;"[""Good call, I noticed this, too.  This block seems to be built into Tensorflow, and is very likely the fetch call.  I really don't know what's causing it, but this would be a fantastic PR if someone could find/fix this issue.====="", 'I opened an issue at tensorflowjs repo.[tfjs#1373](https://github.com/tensorflow/tfjs/issues/1373)=====', ""PR #78 is ready, so when that's merged, we can test this a bit more.====="", 'From my testing, it looks like this is fixed in TFJS 1.x=====', 'Closing this, I feel 1.x was the fix.=====', ""I'm on tf 1.1.0 and it still blocks the UI. Does someone knows what i can do?=====""]"
https://github.com/infinitered/nsfwjs/issues/27;Training on Submitted Images;2;closed;2019-02-25T16:23:25Z;2019-03-27T17:26:58Z;IDK if this is already taking place but maybe you could use images submitted to help train the Neural Network?Also maybe there could also be a people category.;"[""Hey hey!  So the first things first.  If you have some categories that you're identifying as mistakes, you can contribute to the scraper.https://github.com/alexkimxyz/nsfw_data_scraperThere might be a torrent file of the training data soon.  From that torrent, there is room to clean the data and augment and organize it.====="", 'Ask me or https://twitter.com/KarthikRevanuru for the keyhttps://mega.nz/#!Q5RnDQYaLet me know if you make a modified training set that we could use!=====']"
https://github.com/infinitered/nsfwjs/issues/25;backend with @tensorflow/tfjs-node ?;9;closed;2019-02-24T22:10:40Z;2021-08-26T22:41:38Z;could it be possible to port the lib for backend use via tfjs-node and node canvas?;"[""Hey @AZOPCORP - so I've recently solved how to do this!  (I think)I made a test, and for that to work it had to run 100% in node.  This test runs a classification on the logo without accessing a browser.https://github.com/infinitered/nsfwjs/blob/master/__tests__/regressionCheck.tsI hope the setup code in this test works for you.   I'm sure it could be better wrapped in the actual lib though.   If you do use this lib on a backend, please contribute back any improvements 👍 ====="", ""Thanx for your awesome job!Below a working eg with tfjs-node based on your test:```jsconst tf =require('@tensorflow/tfjs-node')const load=require('./dist/index').loadconst fs = require('fs'),const jpeg = require('jpeg-js'),// Fix for JESTconst globalAny = globalglobalAny.fetch = require('node-fetch')const timeoutMS = 10000const NUMBER_OF_CHANNELS = 3const readImage = (path) => {  const buf = fs.readFileSync(path)  const pixels = jpeg.decode(buf, true)  return pixels}const imageByteArray = (image, numChannels) => {  const pixels = image.data  const numPixels = image.width * image.height,  const values = new Int32Array(numPixels * numChannels),  for (let i = 0, i < numPixels, i++) {    for (let channel = 0, channel < numChannels, ++channel) {      values[i * numChannels + channel] = pixels[i * 4 + channel],    }  }  return values}const imageToInput = (image, numChannels) => {  const values = imageByteArray(image, numChannels)  const outShape = [image.height, image.width, numChannels] ,  const input = tf.tensor3d(values, outShape, 'int32'),  return input}    (async()=>{        const model = await load('file://./model/')//moved model at root of folder  const logo = readImage(`./_art/nsfwjs_logo.jpg`)  const input = imageToInput(logo, NUMBER_OF_CHANNELS)  console.time('predict')  const predictions = await model.classify(input)  console.timeEnd('predict') console.log(predictions)    })()```====="", 'very cool! Maybe we should put an example in the demo folder? =====', 'Something like  a way to pass some options to configure lib using tfjs-node and tfjs-node-gpu would be much apreciated I think.  BTW i will make a backend demo as soon as i have the time.=====', '**AZOPCORP**, I am getting this Error: Request for file://.model/model.json failed due to error: TypeError: Only HTTP(S) protocols are supported.=====', 'Closing because question moved to https://github.com/infinitered/nsfwjs/wiki/FAQ:-NSFW-JS=====', ""For information, if you are still wondering how to run it on Node.js, this guy's code works https://github.com/mishazawa/numsWhen you do `npm install nsfwjs` and use the following it doesn't work immediately.```javascriptconst nsfwjs = require('nsfw/dist')```He basically forked the code to [`lib/nsfwjs/index.js`](https://github.com/mishazawa/nums/blob/master/lib/nsfwjs/index.js) and made minor changes to make it runnable. (his repo doesn't have the `classifyGif` function though)It would be nice and not too much work to have this published to npmjs.org.Similarly you can use [`@tensorflow-models/toxicity`](https://www.npmjs.com/package/@tensorflow-models/toxicity) pretty easily on server side using just ```javascriptconst toxicity = require('@tensorflow-models/toxicity')const sentences = ['I love C++']toxicity.load(0.9).then(model =>  model.classify(sentences).then(predictions => ...))```What I would like to have from a developer point of view is to be able to use it out of the box the same way,```javascriptconst image = ...nsfwjs.load().then(model =>    // Instead of nsfwjs.load('file://..../model/')   model.classify(image).then(predictions => ...))```====="", 'I wish he had contributed back with a Pull Request.   @mycaule - would you be willing to take a shot?If not, this is something I could get around to at some point.=====', ""Ok I will try to do this week, the workflow between [NPM](https://www.npmjs.com/package/nsfwjs) and your build might be something I can't test though.For the fetching of this line, `@tensorflow-models` managed that by hosting the files at `tfhub.dev`https://github.com/infinitered/nsfwjs/blob/04978560b221fa66f18b937c6c218e101cbc9145/src/index.ts#L21-L22it might save you some S3 costs.```javascriptconst mobilenet = require('@tensorflow-models/mobilenet')mobilenet.load().then(model => model.classify(...).then(predictions => ...)// Downloads a file from tfhub in the background// https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1/model.json?tfjs-format=file````See #224 =====""]"
https://github.com/infinitered/nsfwjs/issues/16;Bias in model;14;closed;2019-02-20T15:51:44Z;2019-03-05T17:53:55Z;There seems to be a bias of when an image contains a woman with _any_ skin showing, the model will mark those as porn, even when they clearly aren't.There's some examples in [this twitter post](https://twitter.com/_Nec/status/1098140532751118338), but I could reproduce it with different images too.;"['I am finding very similar issues as well:![image](https://user-images.githubusercontent.com/2205537/53109390-c6080100-3506-11e9-8680-3e1ada6dbb46.png)![image](https://user-images.githubusercontent.com/2205537/53109862-cead0700-3507-11e9-9553-f87c1944b0e5.png)=====', 'Agreed.  I have a plan to help fix this.  One thing to note is that faces near microphones might exacerbate this bias.  The original training data was provided by https://github.com/alexkimxyz/nsfw_data_scrapperThe input images seem to slightly off due to the subreddits that fueled the dataset. Hopefully, I can counterbalance this and contribute back to that original repo.=====', 'I tried going the opposite approach and seeing how it classified women who were predominantly covered up, I used two different types of women, two with women in traditional Muslim Garb, and one with traditional Quaker garb. I then tried out an image that (by my analysis) would be considered more ""sexy"", and the results were woefully incorrect.![collage](https://user-images.githubusercontent.com/11723093/53112657-c8218e00-350d-11e9-9f84-04f1f15effa0.png)As you can see, the predominantly covered group, are all flagged as porn with high certainty, despite being anything but. Though interestingly, they are also all flagged with a high ""Neutral"" rating as well.  VS this woman in a revealing, but technically not ""porn quality"" swimsuit.<img width=""314"" alt=""screen shot 2019-02-20 at 12 39 07 pm"" src=""https://user-images.githubusercontent.com/11723093/53112689-da033100-350d-11e9-89b7-5c919e99cf21.png"">This would be a good lesson for the ML Model, as you could use this result to test false positives, where if the ""Porn"" & ""Sexy"" categories are the two highest, it is most likely porn. Obviously, this would still allow for some false positives labeled as ""porn"" to fall through the cracks, but it\'s a step in the right direction. Similarly, if the two highest categories are ""Porn"" and ""Neutral"", then there is a good chance that it is also a false positive as well.=====', ""Great examples so far!  This perspective is very valuable.  I filed an issue on the data-repo.#### Side note:I've gotten lots of requests to share the dataset (for a bunch of reasons, not all for science).   But with some public curation of the training data, the model can significantly improve.  Adding a few hundred photos of false positives into the test/train will help.  As a call to action, I would love to crowsource-fix this problem.  Feel free to let me know you have a zip of false-positive images to add, or if you'd be interested in curation. ====="", ""> ... The original training data was provided by https://github.com/alexkimxyz/nsfw_data_scrapper@GantMan it'd be nice if you credited the original repo https://github.com/alexkimxyz/nsfw_data_scraper in both your medium blog post and this repo. Thanks.====="", ""You read my mind @alexkimxyz - you've been added to both! Sorry for the delay====="", 'Bearded man covered in snow:<img width=""544"" alt=""screen shot 2019-02-20 at 8 26 04 pm"" src=""https://user-images.githubusercontent.com/3649460/53138992-c677ba80-354d-11e9-8ef7-367460c99f48.png"">=====', 'My fingers ![微信图片_20190222112357.png](https://i.loli.net/2019/02/22/5c6f6b91cfcd2.png)=====', 'Hah, is there a subreddit of hands?  If so we can add it. =====', 'Some more examples here: https://twitter.com/JustTenDads/status/1098502194196697088 (Jeffrey goldblum)=====', 'RE: Jeff Goldblum![image](https://user-images.githubusercontent.com/997157/53252118-d5608900-3683-11e9-9d82-ad1f2f5234c4.png)🤣 =====', ""## MODEL BIAS UPDATE:**I'm working on a newly trained model.  This takes DAYS on my home computer.  This will take time**I appreciate everyone's feedback!  You'll be happy to know I'm grabbing some updated training photos to hopefully help fix this bias... except for Jeff Goldblum, that stays.   I'm going to lock this conversation for two reasons, which I hope are fair and clear.1.  I'm actively working on an update which will have new biases, so noted biases on the existing model won't help.  A new ticket can be created when the new model is in place.2. This model is trained on GIGS of data, so single examples actually don't help as much as providing an excellent subreddit to the data scraper repo.   The workflow is like so:### [NSFW Data Scraper](https://github.com/alexkimxyz/nsfw_data_scrapper) ➡️ [NSFW Model](https://github.com/gantman/nsfw_model)  ➡️  NSFW JS 🥳So if you find category bias, please contribute back to the data scraper, it's easy!  Those contributions will make it to the model, which will make it to NSFW JS.====="", ""I'm happy to announce after a lot of tweaking, adjusting, and hours of additional training.  I've increased the test set to reflect a broader array of images, AND increased accuracy to 93% on that larger dataset.   ## While model bias is unavoidable, and accuracy is below a human's potentialI'm very happy to continue working on improving NSFW JSThank you all for your feedback, and I hope you find the following results pleasing.![image](https://user-images.githubusercontent.com/997157/53823053-97424f80-3f36-11e9-9f69-7659991dae0d.png)### I consider this more accurate than the original classification![image](https://user-images.githubusercontent.com/997157/53823080-a88b5c00-3f36-11e9-8901-58d38831c178.png)![image](https://user-images.githubusercontent.com/997157/53823108-b5a84b00-3f36-11e9-96d3-e517696d5d35.png)![image](https://user-images.githubusercontent.com/997157/53823144-c658c100-3f36-11e9-8d4f-4bcb30b49e4a.png)![image](https://user-images.githubusercontent.com/997157/53823177-d53f7380-3f36-11e9-858c-a4f09b8822b0.png)![image](https://user-images.githubusercontent.com/997157/53823197-e4262600-3f36-11e9-8fe6-d0134629f5bb.png)![image](https://user-images.githubusercontent.com/997157/53823402-51d25200-3f37-11e9-8101-2c074bf1d0b2.png)### And unfortunately, 1 false positive.   Which is of-course to be expected.  93% accuracy is not nearly as good as a human.![image](https://user-images.githubusercontent.com/997157/53823494-83e3b400-3f37-11e9-9412-136af9d43a3e.png)If someone can suggest a large set of images of people singing into microphones, I'd love to add that to the training data.  Please supply zip files with hundreds of examples in appropriate folders, if you'd like to help!## ThanksI'd like to thank everyone who helped in the spirit of making something creative and useful.  Please keep in mind this is not my full-time job.  I build things like this as a passion to help the community.This model is about to be released.  If you have old results, please clear your cache and make sure you get the latest model.====="", ""## BONUS:   Jeff Goldblum is fixed... also I'm sad about this 😿![image](https://user-images.githubusercontent.com/997157/53824387-73343d80-3f39-11e9-9235-1261e69b75d2.png)![image](https://user-images.githubusercontent.com/997157/53824362-64e62180-3f39-11e9-9be0-411cca1122de.png)![image](https://user-images.githubusercontent.com/997157/53824415-83e4b380-3f39-11e9-84f9-e400d2d74e38.png)![image](https://user-images.githubusercontent.com/997157/53824446-9363fc80-3f39-11e9-9774-8eb3df3149c6.png)=====""]"
https://github.com/mexili/incogly/issues/120;add e2e encryption?;3;open;2021-07-19T14:11:14Z;2021-07-19T18:09:32Z;https://github.com/webrtc/samples/blob/gh-pages/src/content/insertable-streams/endtoend-encryption/js/main.jsLet me know what you think of this.If we use this, then we can't use peer js.;['I think this feature is a stretch feature and we should wait till peer js supports it by themselves. What say?=====', '@ZohebMOPO @raghavdhingra @shivaylamba =====', 'Like it will be wonderful to add e2e encryption rather than using standard encryption. Idk much about the cons of e2e. Like I mean it will be alright if we add e2e. Will there be any problem with the servers if we add `E2E`?=====']
https://github.com/mexili/incogly/issues/103;Add a feature to kick out the users!;3;open;2021-06-27T12:09:15Z;2021-07-01T17:34:35Z;Add a way to kick out people if causing issues.;['For kicking someone out of a room we need a user to be an admin(it can the person who started the meeting first or any algorithm we can implement).  =====', '@ZohebMOPO , we will be following a consensus mechanism where the majority can decide how to kick a person. And anyone can start a vote to kick out the person.Like how csgo implements it in their matches :D=====', 'Alright. Gotcha!=====']
https://github.com/mexili/incogly/issues/95;Stream behaving weirdly when the room is created by ~3 people.;6;open;2021-06-26T12:54:30Z;2021-06-28T15:37:14Z;I can't seem to debug this.Need to figure this out.;"[""Hey @sansyrox would love to work on the backend of this APP. Also can we've have a backend tag for the issues would really help me looking into specific issues.Thanks====="", 'Hey @nimit2801 ,This is the repo of the backend : https://github.com/mexili/incogly-backendWe have migrated away from a monolith architecture. You can have a look at the backend repo there.=====', ""Ohh, I don't see any issues there right now! Would watch it constantly for any issues!Thanks @sansyrox ====="", 'I library like Peerjs should fix this: https://glitch.com/edit/#!/peerjs-video?path=public%2Fmain.js%3A1%3A0We are only using one stun server and handling native connections. We should use something more off the shelf.@raghavdhingra what say?=====', ""@nimit2801 , if this works well, we will also be integrating peer js in our backend. Let us know if you'd be interested in that.====="", ""Sure, I'd look more into peer.js and would love to help.=====""]"
https://github.com/mexili/incogly/issues/89;Feature request: Audio Visualizer;13;open;2021-06-10T04:13:45Z;2021-07-23T08:34:39Z;**Is your feature request related to a problem? Please describe.**At the time of joining the client should be able to know that how loud his/her voice is. For that Meet has a voice visualizer.![image](https://user-images.githubusercontent.com/72716462/121461985-78607c80-c9cd-11eb-9ef9-ce55f5486685.jpg)In the left corner, there is the voice visualizer.**Describe the solution you'd like**There are two ways of doing it which I figured out. 1) to collect the decibel of the client in the backend and according to that, we will send a response to the client via visualizer. For that, we can use a package called [decibel-meter](https://www.npmjs.com/package/decibel-meter).2) A package that does it all like getting the sound and sending the amplitude on its own. The package is called [p5.js](https://p5js.org/reference/#/libraries/p5.sound).**Describe alternatives you've considered**Well, after googling many times, I found this [video](https://youtu.be/eEeUFB1iIDo) which can make you clear what I am trying to say.;"['Hello there!👋 Welcome to the project!🚀⚡Thank you and congrats🎉for opening your very first issue in this project. Please adhere to our [Contributing Guidelines](https://github.com/mexili/incogly/blob/develop/CONTRIBUTING.md).🙌 You may submit a PR if you like, make sure to follow our [Pull Request Template](https://github.com/mexili/incogly/blob/develop/.github/pull_request_template.md). If you want to report a bug🐞 please follow our [Issue Template](https://github.com/mexili/incogly/tree/develop/.github/ISSUE_TEMPLATE). Also make sure you include steps to reproduce it and be patient while we get back to you.😄Feel free to join our [Discord Community](https://discord.com/invite/RxttJFy2BM).💖 We have different channels for active discussions.✨ Hope you have a great time there!😄=====', '![ezgif com-gif-maker](https://user-images.githubusercontent.com/72716462/126140380-3a5c7836-67e1-4c2d-b123-3c4acf404f47.gif)@sansyrox @shivaylamba @raghavdhingra @midopooler``The Audio visualizer thing is working rn. This one is in a separate environment. I just wanna ask that in which part of the video call we should add the visualizer?``=====', '@ZohebMOPO , on the screen where you select the username=====', 'Also, once you integrate that, try using hooks instead of class components. Let us know if you face any issues.=====', 'Yeah sure. I am not in home rn. Will let you know about that :)=====', ""I will not be at my house for 2days because of the festival. Like I have changed them to functional components but haven't used the hooks properly. Sorry for that :(====="", 'No worries @ZohebMOPO . You can take your time.=====', '```import React, { Component } from ""react"",import AudioVisualiser from ""./Visualizer"",class AudioAnalyser extends Component {  constructor(props) {    super(props),    this.state = { audioData: new Uint8Array(0) },    this.tick = this.tick.bind(this),  }  componentDidMount() {    this.audioContext = new (window.AudioContext ||      window.webkitAudioContext)(),    this.analyser = this.audioContext.createAnalyser(),    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount),    this.source = this.audioContext.createMediaStreamSource(this.props.audio),    this.source.connect(this.analyser),    this.rafId = requestAnimationFrame(this.tick),  }  tick() {    this.analyser.getByteTimeDomainData(this.dataArray),    this.setState({ audioData: this.dataArray }),    this.rafId = requestAnimationFrame(this.tick),  }  componentWillUnmount() {    cancelAnimationFrame(this.rafId),    this.analyser.disconnect(),    this.source.disconnect(),  }  render() {    return <AudioVisualiser audioData={this.state.audioData} />,  }}export default AudioAnalyser,This was the class component, I am literally confused on how to use the DOM updates in useEffect.Functional component..... import React, { useState, useEffect } from ""react"",import Visualizerf from ""./AudioAnalyserf"",function AudioAnalyserf({ audio }) {  const [audioData, setaudioData] = useState(new Uint8Array(0)),  const audioContext = new window.AudioContext(),  const analyser = audioContext.createAnalyser(),  const dataArray = new Uint8Array(analyser.frequencyBinCount),  const source = audioContext.createMediaElementSource(audio),  source.connect(analyser),  const rafId = requestAnimationFrame(tick),  const tick = () => {    analyser.getByteTimeDomainData(dataArray),    setaudioData(dataArray),    rafId = requestAnimationFrame(this.tick),  },  const unMount = () => {    cancelAnimationFrame(this.rafId),    analyser.disconnect(),    source.disconnect(),  },  useEffect(() => {    // Idk what I am doing :((((  }, []),  return (    <div>      <Visualizerf audioData={audioData} />    </div>  ),}export default AudioAnalyserf,``=====', 'How do I make updates using `useEffect` @raghavdhingra =====', 'useEffect is a life cycle hook. It has a dependency list, over which the function call itself when ever any of the value from that list changes.For e.g. `const callBackFunction = () => {// do some changes}``const dependencyArray = [value, object, list, ...]``useEffect(callBackFunction, dependencyArray),`So, if any of the values changes within the list via state change, or any hook change, the callback function will call itself. Hence you can define the callback function as per the way you want =====', 'Class component:-```import AudioVisualiser from ""./Visualizer"",class AudioAnalyser extends Component {  constructor(props) {    super(props),    this.state = { audioData: new Uint8Array(0) },    this.tick = this.tick.bind(this),  }  componentDidMount() {    this.audioContext = new (window.AudioContext ||      window.webkitAudioContext)(),    this.analyser = this.audioContext.createAnalyser(),    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount),    this.source = this.audioContext.createMediaStreamSource(this.props.audio),    this.source.connect(this.analyser),    this.rafId = requestAnimationFrame(this.tick),  }  tick() {    this.analyser.getByteTimeDomainData(this.dataArray),    this.setState({ audioData: this.dataArray }),    this.rafId = requestAnimationFrame(this.tick),  }  componentWillUnmount() {    cancelAnimationFrame(this.rafId),    this.analyser.disconnect(),    this.source.disconnect(),  }  render() {    return <AudioVisualiser audioData={this.state.audioData} />,  }}export default AudioAnalyser,``````import React, { useState, useEffect } from ""react"",import Visualizerf from ""./AudioAnalyserf"",function AudioAnalyserf({ audio }) {  const [audioData, setaudioData] = useState(new Uint8Array(0)),  const audioContext = new window.AudioContext(),  const analyser = audioContext.createAnalyser(),  const dataArray = new Uint8Array(analyser.frequencyBinCount),  const source = audioContext.createMediaElementSource(audio),  source.connect(analyser),  const rafId = requestAnimationFrame(tick),  const tick = () => {    analyser.getByteTimeDomainData(dataArray),    setaudioData(dataArray),    rafId = requestAnimationFrame(tick),  },  const unMount = () => {    cancelAnimationFrame(rafId),    analyser.disconnect(),    source.disconnect(),  },  useEffect(() => {    return unMount,  }, []),  return (    <div>      <Visualizerf audioData={audioData} />    </div>  ),}export default AudioAnalyserf,```so it will look like this?=====', ""@raghavdhingra Sorry for the late reply. Schools are going offline and I am kinda stuck in that thing cuz I didn't do anything the whole summer haha. Anyways, will the functional component look like this?====="", 'I am hella confused about converting this to functional components. I am used to class components cuz my first programming lang was Java. I am not sure when I will come out of this learning curve :(=====']"
https://github.com/mexili/incogly/issues/87;Feature request : Adding custom backgrounds for video using Selfie Segmentation MediaPipe Solution;7;open;2021-06-07T01:55:19Z;2021-06-07T08:14:52Z;**Is your feature request related to a problem? Please describe.**No it is a new feature request. **Describe the solution you'd like**Using the newly announced MediaPipe Selfie Segmentation Solution for Web to allow dynamic background change during live video.  ( Refer : https://google.github.io/mediapipe/solutions/selfie_segmentation.html ) **Describe alternatives you've considered**The https://github.com/tensorflow/tfjs-models/tree/master/body-pix Bodypix model can also do body segmentation. But the Selfie Segmentation model is lighter. **Approach to be followed (optional)**- Use the CDN link or NPM package for MediaPipe. - Define the model type ( General or Landscape ) - Define the canvas element and Video capture device - Load the model and send the canvas as the input element - Wait for the segmentation mask to be generated - Choose between different types of backgrounds **Additional context**- Refer to https://google.github.io/mediapipe/solutions/selfie_segmentation.html- https://google.github.io/mediapipe/images/selfie_segmentation_web.mp4;['Hello there!👋 Welcome to the project!🚀⚡Thank you and congrats🎉for opening your very first issue in this project. Please adhere to our [Contributing Guidelines](https://github.com/mexili/incogly/blob/develop/CONTRIBUTING.md).🙌 You may submit a PR if you like, make sure to follow our [Pull Request Template](https://github.com/mexili/incogly/blob/develop/.github/pull_request_template.md). If you want to report a bug🐞 please follow our [Issue Template](https://github.com/mexili/incogly/tree/develop/.github/ISSUE_TEMPLATE). Also make sure you include steps to reproduce it and be patient while we get back to you.😄Feel free to join our [Discord Community](https://discord.com/invite/RxttJFy2BM).💖 We have different channels for active discussions.✨ Hope you have a great time there!😄=====', '@sansyrox @midopooler would like to discuss about this implementation with you all=====', '@shivaylamba , adding custom backgrounds is not a part of the current roadmap. We would like to improve the facemesh detection feature first. Custom backgrounds is definitely a good feature to consider for future roadmaps, however, if you or @midopooler be able to help us with implementing that, we would really appreciate it.=====', '@shivaylamba , we can discuss this on our weekly community call on Sunday.=====', 'Sure @sansyrox =====', 'Can you share details of the weekly community call?=====', 'They usually take place on Sunday around 5pm. I will share the link with you later this week.=====']
https://github.com/mexili/incogly/issues/85;Eject and convert to webpack;3;open;2021-06-06T12:46:08Z;2021-06-07T14:07:22Z;**Is your feature request related to a problem? Please describe.**CRA has a lot of limitations regarding the config structure and file directory. Eject all the files and configure them for Webpack**Describe the solution you'd like**`yarn eject`**Describe alternatives you've considered**use parcel maybe?;"['Can I give a try to this , assign me =====', ""@Jassi10000 , this feature is not a part of the current roadmap. We will try to implement it when we complete the refactor. This is just open for discussions in the future. You can help us with other issues, if you'd like to.====="", 'Ohh okayy , will see other issuesThanks for telling=====']"
https://github.com/mexili/incogly/issues/82;Create a global state store and decide an architecture;4;open;2021-06-06T12:06:12Z;2021-06-13T11:31:27Z;**Is your feature request related to a problem? Please describe.**A clear and concise description of what the problem is.**Describe the solution you'd like**A clear and concise description of what you want to happen.**Describe alternatives you've considered**A clear and concise description of any alternative solutions or features you've considered.**Approach to be followed (optional)**A clear and concise description of approach to be followed.**Additional context**Add any other context or screenshots about the feature request here.;"['@ZohebMOPO @raghavdhingra , I have added a design doc here: https://github.com/mexili/incogly/pull/90 .Let me know what do you think of it?=====', 'So @sansyrox looks like we are gonna use Next.js. We also have the server files inside it. Should we take the server files in another directory so that the files will be easy to navigate through?=====', ""> So @sansyrox looks like we are gonna use Next.js. Zoheb, we will be using react only. Nextjs and webrtc don't work very well from our experience.====="", '@shivaylamba =====']"
https://github.com/mexili/incogly/issues/34;Add a light mode to the app;12;open;2020-12-09T07:48:24Z;2021-02-02T09:54:26Z;**Is your feature request related to a problem? Please describe.**Right now the app is just present in dark mode and not light mode which is required for some people. We just need to decide a light colorscheme of the app and then create a global toggle.**Describe the solution you'd like**Just propose a light colorscheme for now and we can integrate the feature collaboratively.**Approach to be followed (optional)**A white/ish background is a must for the light mode**Additional context**Add any other context or screenshots about the feature request here.;"['Dark mode: https://dribbble.com/shots/12263615-Landing-Page-WatchAppLight Mode: https://dribbble.com/shots/14046353-Website-for-BlueReceipt-on-Awwwards=====', 'Please let me take care of this=====', 'Hey @mamadou-diallo , sure ❇️Can you start by creating a Figma/Invision file and sharing the design with us?=====', 'The light mode in the above comment just a reference of the colour schema, you are free to use your imagination and you can see the app at https://incogly.ml/=====', '@sansyrox as requested here is a figma implementation. https://www.figma.com/proto/mKDWlGeK2dqmxxfbJPJh64/Untitled?node-id=17%3A3&scaling=min-zoom I was thinking that I could create a simple link/icon at the botton that would toggle on/on off the dark/light mode. Let me know what you think :) Thanks ! =====', '@mamadou-diallo , the button at the bottom really looks like a great idea 🔥 For the design, I was thinking more of a white background and white detailing. Can you please do that?Also, can you share a collaborator link maybe? So, we can work together on the design.=====', '@sansyrox - https://www.figma.com/file/mKDWlGeK2dqmxxfbJPJh64/Untitled?node-id=0%3A1 here you go :) =====', 'Thanks @mamadou-diallo ! I will have a look at it tomorrow morning!! =====', 'Also, can you join our discord community for easier collab? https://discord.gg/ZHDd59yE=====', ""hey nice design, @mamadou-diallo. Let me know if you aren't working on the code, I could start then.====="", 'Hey @pranay-simejia .You can start working on it. Please chakra ui to implement this design. If possibleAnd we are looking to implement this design only for now.![Screenshot 2021-02-02 at 3 23 43 PM](https://user-images.githubusercontent.com/29942790/106583030-a4c14380-656a-11eb-9688-658e651da45b.png)=====', 'Hey @pranay-simejia .You can start working on it. Please chakra ui to implement this design. If possibleAnd we are looking to implement this design only for now.![Screenshot 2021-02-02 at 3 23 43 PM](https://user-images.githubusercontent.com/29942790/106583030-a4c14380-656a-11eb-9688-658e651da45b.png)=====']"
https://github.com/mexili/incogly/issues/18;Migrate the Class components to functional components;19;open;2020-12-04T12:00:31Z;2021-08-29T13:04:36Z;Now the class components are soon to be deprecated, try porting them to functional components.;"['Can you assign me this issue? What are the things need to know ?=====', ""There are class components in the app, you just need to convert them to functional components.Ensuring you don't break anything.====="", '@kibi11 , you can go ahead. Sorry forgot to assign this issue to you.=====', 'Can you assign this to me? I would like to look into it.=====', '@sayna3311 , we should wait for 2/3 days to allow @kibi11 to work on it.=====', '@sansyrox Can I work on it now?=====', 'Yes @sayna3311 . Go ahead!! 🔥 =====', 'https://github.com/mexili/incogly/pull/43Changed the NotFound.js into Functional component and solved the ""GO TO HOME"" button issue. =====', 'Can I work on this?=====', 'Hi @MohsinAliSoomro .Go ahead!=====', 'is this issue still open? i would like to work on it.=====', 'Hi @pranay-simejia , yes it is still open. Please go ahead.=====', 'working on it =====', 'Tried but app not working now.=====', 'Hi @ranveersequeira , please try to migrate the components in the `major_refactor` branch=====', ""The main branch has been flaky ever since its inception, hence we're moving away from it====="", 'branch name: `major_refactor`=====', '@sansyrox any updates on this issue?=====', 'Can I work on this issue as it seems the issue has not been resolved yet?=====']"
https://github.com/mexili/incogly/issues/8;Add Eye Tracking;6;open;2020-11-24T08:25:26Z;2021-02-02T16:24:57Z;### IdeaImplement eye tracking with the face-api and display it on the avatar to eliminate possibility of cheatingCc @ayushjainrksh ;['I want to work on this feature, Can anyone guide me in this @sansyrox @ayushjainrksh =====', 'Hi @carrycooldude , Thank you for volunteering. We are using this API https://github.com/justadudewhohacks/face-api.js/And right now we are unable to do the eye-tracking logic. it would be helpful if you can add the feature.=====', 'Can I use tensorflow.js and use their model face-mesh I think maybe can implement that=====', 'Will you be able to create an abstraction for us like the one currently used? Using tensorflow looks like a better approach.What do you think @QEDK =====', '> > > Will you be able to create an abstraction for us like the one currently used? Using tensorflow looks like a better approach.> > What do you think @QEDKTFJS would fare better, yep.=====', 'Let me try on TFJS=====']
https://github.com/mexili/incogly/issues/7;Add a way to auto generate an SSL certificate. ;6;open;2020-11-24T08:23:47Z;2020-12-04T12:22:10Z;This app only runs on HTTPS.Use SSL to deploy this app;"['can you please assign this issue to me? @sansyrox =====', 'Can you please assign it to me? @sansyrox =====', 'Yes sure @dijodaiju7 . Go ahead!=====', 'Can I know where to deploy this app.On Fri, 4 Dec 2020, 4:15 pm Sanskar Jethi, <notifications@github.com> wrote:> Yes sure @dijodaiju7 <https://github.com/dijodaiju7> . Go ahead!>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/mexili/incogly/issues/7#issuecomment-738712678>, or> unsubscribe> <https://github.com/notifications/unsubscribe-auth/AOL6NETE3FH7PSRTG76MH63STC4VPANCNFSM4UARP7YA>> .>=====', ""@dijodaiju7 , since the app requires an ssl certificate for connections on the same network. We want to create a way to generate a new SSL certificate locally, if it doesn't already exist.You can just create a custom script from here: https://stackoverflow.com/questions/21397809/create-a-trusted-self-signed-ssl-cert-for-localhost-for-use-with-express-node====="", ""Thanks, I'll check it out.On Fri, 4 Dec 2020, 5:37 pm Sanskar Jethi, <notifications@github.com> wrote:> @dijodaiju7 <https://github.com/dijodaiju7> , since the app requires an> ssl certificate for connections on the same network. We want to create a> way to generate a new SSL certificate locally, if it doesn't already exist.>> You can just create a custom script from here:> https://stackoverflow.com/questions/21397809/create-a-trusted-self-signed-ssl-cert-for-localhost-for-use-with-express-node>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/mexili/incogly/issues/7#issuecomment-738748461>, or> unsubscribe> <https://github.com/notifications/unsubscribe-auth/AOL6NEQ2PCDUIO7OAHPBS33STDGGVANCNFSM4UARP7YA>> .>=====""]"
https://github.com/mexili/incogly/issues/1;Audio modulation and encoding in a video call #2;1;open;2020-11-24T07:35:46Z;2020-12-17T15:56:28Z;We need an ability to obfuscate audio that we are sending across the stream.cc https://github.com/incog-ly/incog-ly/issues/2;['https://www.toptal.com/webassembly/webassembly-rust-tutorial-web-audio?utm_campaign=Toptal%20Engineering%20Blog&utm_medium=email&_hsmi=103382885&_hsenc=p2ANqtz-9ngfhkcwxaX0F82IomAI3X2fiS9j0hWvMumC-HZTS5gGsHhTjBW3aYzA-Nmr6g9BpYyWovdZwwpE50w5m7TcKozg-tAQ&utm_content=103382885&utm_source=hs_email=====']
https://github.com/mexili/incogly/issues/101;Landing page gif broken;14;closed;2021-06-27T11:59:30Z;2021-07-01T15:15:44Z;The GIF file on the Landing page is broken and is showing a blur image. Need a quick fix.;"['@sansyrox, @midopooler  Which one should we go for?![demo1](https://user-images.githubusercontent.com/72716462/123901795-49fd0e00-d989-11eb-9d9a-5c8940313920.JPG)![demo 2](https://user-images.githubusercontent.com/72716462/123901802-4ec1c200-d989-11eb-81a5-6c36088b75ac.JPG)=====', 'P.S. These gifs are in motion but while taking a screenshot it is static.=====', ""@ZohebMOPO , you can put these gifs in the comments for us to view. Also, can you try looking for gifs that are maybe more colorful? They don't have to be related to code and can just be related to collaboration.====="", 'Alr,[1](https://www.careerguide.com/career/wp-content/uploads/2020/04/collaboration-clipart-animated-gif-2.gif)[2](https://cdn.dribbble.com/users/4341791/screenshots/8574595/media/3cbca4ba607b111c4d4702f210062a62.gif)[3](https://static.wixstatic.com/media/570bfa_55217a8ab97d4cf1a206fe2ca2f5e2cf~mv2.gif) <- This one is my favorite.=====', '@ZohebMOPO , all of these look good. If you can change the background to transparent ,then you can make a PR with any gif that you like 😄 =====', 'Ummm. Third one looks more like a dating website GIF lol=====', 'Haha.What about [this](https://static.wixstatic.com/media/ab8e83_ed69dcb2b5b341e99ddfc77323599475~mv2.gif) and [this](https://cdn.dribbble.com/users/37380/screenshots/10883216/zoom-party4_12fps.gif) one.=====', 'https://cdn.dribbble.com/users/37380/screenshots/10883216/zoom-party4_12fps.gif this one looks nice=====', 'You can also have a look at these : https://cdn.dribbble.com/users/37380/screenshots/10883216/zoom-party4_12fps.gif=====', 'Sure=====', ""But the problem is it's occupying too much space on the home page====="", 'What do you mean?=====', ""nvm, that was a problem with `<iframe>` tag. It's fixed. I am making a PR then.====="", 'Fixed here: https://github.com/mexili/incogly/pull/105=====']"
https://github.com/mexili/incogly/issues/91;Check why the the app is making us log out on chat send?;6;closed;2021-06-14T17:22:02Z;2021-06-26T12:48:02Z;**Describe the bug**The app is ending the call when the user sends the chat in the socket stream.**To Reproduce**Steps to reproduce the behavior:1.  Join the call2.  Try to send the message3.  App will log you out**Expected behavior**The app should successfully send the app to the backend**Actual behavior**Crashing at the moment;"[""![bug](https://user-images.githubusercontent.com/72716462/121991094-99a1de00-cdbc-11eb-9576-c1350341aed3.JPG)I am trying to send messages it's not logging me out. The messages are not showing up in the chat cause there is no Database. But it is not logging me out. I guess the problem was that time.====="", '@ZohebMOPO , you need to have 2 people in the socket room.=====', ""![No bug](https://user-images.githubusercontent.com/72716462/122021006-e7314180-cde2-11eb-94e8-1f0f587ccc0e.JPG)No, it is not @sansyrox and there is a problem that the next person is not showing after he/she joins. I figured it out. The problem that if your webcam is used in one place then it couldn't be used in another place. If we host that in the cloud and another person joins with another device, it will show I guess.====="", ""@ZohebMOPO > I figured it out. The problem that if your webcam is used in one place then it couldn't be used in another place.This is not the case. This was working well locally before the refactor. You can join from two tabs without incognito mode and this should work well.(in an ideal scenario)Read my comments here: https://github.com/mexili/incogly/issues/92#issuecomment-860943842I have a feeling that these two might be connected.====="", ""I literally changed the browser and did that so that there will be no conflicts. Idk then what's happening.P.S. I checked the thing in my normal browser first it was working fine and then I did that in incognito and screenshotted that so that I can hide my normal chrome unnecessary tabs.====="", 'Fixed in the latest release. GG 🔥 =====']"
https://github.com/mexili/incogly/issues/69;Add a github logo on the landing page to navigate user to the github repo;8;closed;2021-02-14T07:15:16Z;2021-02-16T14:15:03Z;**Is your feature request related to a problem? Please describe.**The user would want to view the code repository and having a footer icon in the anding page would navigate the user to the codebase**Additional context**No additional context;['If this means that click the logo would direct it to the github repo of Incogly , then I want to definitely try this out , assign me thisAlso pls mention the tags as well=====', '@lopeselio @Jassi10000 , you can add this issue on this PR https://github.com/mexili/incogly/pull/67 and the major_refactor branch. =====', '@sansyrox  can I work on this issue. This would be my first opensource commit and it looks like a great issue for a begineer=====', 'Okay, are we supposed to create a new landing page for the project and before making other changes.  =====', '@sansyrox Can I work on this issue ? I am a total beginner. Thanks=====', '@trishnakalita660 @lopeselio @vikas-099 , this issue is not an active issue at the moment. Please try to solve the issues on this PR #67 https://github.com/mexili/incogly/pull/67=====', 'Okay cool =====', 'Okay no problem=====']
https://github.com/mexili/incogly/issues/28;Create a Landing Page;17;closed;2020-12-07T07:27:21Z;2021-07-01T15:16:15Z;Right now the Landing Page is a little old and the fonts are not very crisp on different screens.You can create a new landing page as per your imagination.Requirements:1. Get the mockup approved2. Make the PR.We can split the PR into 2 issues and can allot the points as required.;"['@sansyrox can i work on this issue sir?=====', '@akshay1027 , we have already created the dark mode one. Will be interested in creating the light mode version?=====', '![](https://cdn.discordapp.com/attachments/780403248360062990/786881607624818708/Screenshot_2020-12-11_at_2.36.30_PM.png)Here is a dark mode design. Lmk if you can propose a light mode one.=====', ""ohh, that's cool, i will try to propose the light mode then 🙌🏻! @sansyrox ====="", 'I have made a pull request for the lightmode version. review it and let me know what else i have to change. Also i have not changed the navbar for lightmode, if you give me permission, i would like to to that as well. @sansyrox =====', '@akshay1027 , please go ahead!=====', 'Is the issue still open ?=====', 'Hi @lopeselio ,Yes, it is still open!=====', 'We have to implement the design in the comment: https://github.com/mexili/incogly/issues/28#issuecomment-745115912using scss and chakraui preferably, but you are free to choose whatever you like. =====', 'If you just want to propose a design, we require a light version of the darkmode proposed above.=====', '> If you just want to propose a design, we require a light version of the darkmode proposed above.I am in for that=====', 'Hey @sansyrox, I developed this light version can you check it. It is just UI design.![Incogly - light](https://user-images.githubusercontent.com/45565999/108593058-68c61500-7397-11eb-83d4-dfc629f1b032.png)=====', '@RutvikJ77 , this looks great! Do you want to help with coding it out as well?=====', '> @RutvikJ77 , this looks great! Do you want to help with coding it out as well?If it is in react, it will take some time for me to set it up since it will be a new tech stack additionally I will need to develop through scss right?=====', '@RutvikJ77 , yes. Take a look at this: https://chakra-ui.com/getting-startedIt makes using react much simpler for creating is.Also, can you share the design files in a PR, so I can allot you your points.=====', 'Cool! I did go through Chakra UI seems easy to develop. I will give it my try. Okay! I will share the background image and UI for the homepage.=====', 'We have a good landing page now. Closing this!=====']"
https://github.com/mexili/incogly/issues/19;Add the option of adding more than one member;4;closed;2020-12-04T12:11:12Z;2021-07-01T15:17:06Z;Currently, the video call happens only between 2 people. We need to add a way to allow more than 2.;"[""Heyy @sansyrox , I want to try this out Btw I have currently requested to work on issue #69 , after that I want to work on this , so am requesting to assign it to me beforehand , so that this doesn't get crowdedBtw pls add the tags as well====="", 'Hey @Jassi10000 , this feature is already implemented in the PR, I proposed here https://github.com/mexili/incogly/pull/67This PR has some issues that require some help. You can try contributing to those. And as soon as that PR gets merged, we will have more issues like this one.=====', '> Hey @Jassi10000 , this feature is already implemented in the PR, I proposed here #67> > This PR has some issues that require some help. You can try contributing to those. And as soon as that PR gets merged, we will have more issues like this one.Means am late , anyways , can you pls tell what are the issues that require help , I can try to help the team to make it a better project =====', 'We support multiple people now. Closing this!=====']"
https://github.com/mexili/incogly/issues/17;Add CONTRIBUTING.md;5;closed;2020-12-03T13:05:31Z;2020-12-04T17:57:40Z;### Add ```CONTRIBUTING.md``` FileAs this project is a part of an Open Source Program ```NJACK Winter of Code``` I would recommend to have a contribution guidelines file (```CONTRIBUTING.md```) to assist beginners to get started with open source contribution.Points to be covered in the guideline :- How to pull your first request.- How to create issue.- Some general rules to keep in mind while contributing to an open source project.- Links of some tutorials which will help beginners to learn the basics of git and github.;['Sure, I would be happy to add a CONTRIBUTING file.=====', 'Sure! Go ahead @Pranav016 =====', '@sansyrox @Pranav016 Actually I was already working on this issue. I have even raised PR #20 regarding it.=====', 'Økay @atarax665 . From next time please claim the issue before working to avoid any botch ups.=====', '> Økay @atarax665 . From next time please claim the issue before working to avoid any botch ups.Yeah, sorry for the inconvenience.=====']
https://github.com/mexili/incogly/issues/14;App doesn't work in cross platform and crashes when using my laptop;6;closed;2020-11-24T08:30:32Z;2020-12-04T12:35:26Z;Issue is unique to my laptop and with other peopleNeed to check if this is with my network or just the laptop;"['can you please assign this issue to me? @sansyrox =====', 'How do you plan to debug this?This issue is unique to me.=====', 'Can you describe some details and error messages?=====', ""@mohdorusaid , that is the strange thing. It doesn't give any errors just doesn't connect to a session.This bug is unique to me. I would have to fix it myself :(. I would suggest y'all work on a different bug maybe #6 is good alternative to this one. ====="", ""Sure I'll look around:)====="", 'Fixed Now.Wierd Issue with the Nginx proxy settings.=====']"
https://github.com/tensorspace-team/tensorspace/issues/238;Use with model trained in TFJS?;1;open;2020-06-01T05:34:36Z;2021-11-23T12:27:49Z;Hello, How would I use TensorSpace with a model trained in TFJS? I'm trying to find why it is so difficult to train. Is it possible to skip the loading of pretrained model part? https://github.com/Y8Games/Y8-Instant-io-Game/blob/ai-gym/src/game.js#L32https://tensorspace.org/html/docs/preTfjs.html;"[""Have you found a solution to this? I'd really like something like```tensorspace_model.load(tfjs_model)```to skip all the loading and defining stuff, but I cannot find anything like that.=====""]"
https://github.com/tensorspace-team/tensorspace/issues/233;How will I insert our own picture? ;2;open;2019-08-07T00:50:21Z;2020-05-09T18:10:30Z;How will I insert our own picture? Convert the image to JSON format. When inserting, it can't be uploaded too much.;"[""Hi @alaosion2019,A cool aproach can be this:    First upload an image  in an image html element (I named it results )    let canvas = document.getElementById('canvas'),    let imageSnap = document.getElementById('results'),    let context = canvas.getContext( '2d' ),    context.drawImage(imageSnap,0,0, canvas.width, canvas.height),   // you can draw the image in a canvas and after to get the data from the image     let imgData = context.getImageData( 0, 0, canvas.width, canvas.height ),    context.putImageData(imgData,0,0),    let cameraImage = [],    let size = imgData.width * imgData.height *4, // get the size of the image (is multiplied by 4 because we have 4 channels  RGBA )    let maxR = 0,    let minR = 255,    let maxG = 0,    let minG = 255,    let maxB = 0,    let minB = 255,    // for normalization method we need to take the max and min of each channel  and ignore Alfa chanel    for (let i = 0, dx = 0, dx < size, i++, dx = i << 2) {        cameraImage.push( imgData.data[dx]),        if(imgData.data[dx] > maxR){            maxR= imgData.data[dx],        }        if(imgData.data[dx] < minR){            minR= imgData.data[dx],        }        cameraImage.push( imgData.data[dx+1]),        if(imgData.data[dx] > maxG){            maxG= imgData.data[dx],        }        if(imgData.data[dx] < minG){            minG= imgData.data[dx],        }        cameraImage.push( imgData.data[dx+2]),        if(imgData.data[dx] > maxB){            maxB= imgData.data[dx],        }        if(imgData.data[dx] < minB){            minB= imgData.data[dx],        }    }    // normalize the data  in order to fit in as an input in model.predict()    for(let j =0 , j< cameraImage.length,j+=3){        cameraImage[j] = 2*(cameraImage[j] - minR) / (maxR - minR)        cameraImage[j+1] = 2*(cameraImage[j+1] - minG) / (maxG - minG)        cameraImage[j+2] = 2*(cameraImage[j+2] - minB) / (maxB - minB)    }       // camera image is now an array with values betwwen -1 and 1 that can be interpreted by the network    model.predict(cameraImage, function (finalResult) {        generateInference(finalResult)    }),====="", ""Hi @cBogdan96,thank you for your proposal, i coded it in python and it's fine.Be carefull because there's errors in your code, each 'imgData.data[dx]' must correspond to the related color channel for R, B and G while in your code it always access the 'R' channel value.Lines 'if(imgData.data[dx] > maxG){' and 'if(imgData.data[dx] < minG){' for example must be 'imgData.data[**dx+1**]' and so on for the rest 'imgData.data[**dx+2**]' for the 'B' color value channel.We assume that we have image from imagenet here 'helico.jpg', it generates intermediate 'png' file (may be useless) and 'json' file with RGB values. The 'json' file can be used with tensorspace model predict. ```from PIL import Imageimport json # Normally select img_file_path image         image = Image.open('helico.jpg')       #to be used with alexnet trained model in tensorspace        new_image = image.resize((227,227))        new_image.save('helico.png')        rgb_image = new_image.convert('RGB')        rgb_value = list(rgb_image.getdata())        dest_image = []        maxR = 0        minR = 255        maxG = 0        minG = 255        maxB = 0        minB = 255        for rgb in rgb_value:            # Red            if rgb[0] > maxR:                 maxR= rgb[0]            if rgb[0] < minR:                minR= rgb[0]            # Green            if rgb[1] > maxG:                maxG= rgb[1]            if rgb[1] < minG:                minG= rgb[1]            # Blue            if rgb[2] > maxB:                maxB= rgb[2]            if rgb[2] < minB:                minB= rgb[2]        for rgb in rgb_value:            dest_image.append(2*(rgb[0] - minR) / (maxR - minR))            dest_image.append(2*(rgb[1] - minG) / (maxG - minG))            dest_image.append(2*(rgb[2] - minB) / (maxB - minB))        #normally use img_file_path        with open('helico.json', 'w') as outfile:            json.dump(dest_image, outfile)```I used it with tensorspace alexnet NN example and it works fine.=====""]"
https://github.com/tensorspace-team/tensorspace/issues/198;Faster rcnn support ;1;open;2019-01-31T05:24:05Z;2019-08-30T06:36:01Z;Hello.Have you any plan to support Faster Rcnn? I really want to play FRCNN with your project.;['Hello!I really want to play FRCNN with your project, too.=====']
https://github.com/tensorspace-team/tensorspace/issues/179;Support configure font style in scene;6;open;2019-01-10T05:00:56Z;2019-01-29T14:08:59Z;TensorSpace use `fonts/helvetiker_regular.typeface.json` as the basic font style for `output1d` layer text information. However, it does not support some text, for example, when input `中文`, the `fonts/helvetiker_regular.typeface.json` will show `??` (this case can be reproduced in three.js docs's playground).To handle this case, add a configuration in model, user can override the default `fonts/helvetiker_regular.typeface.json` with own fonts.For example:```// user loadFont file through AJAXlet ownFont = loadFont(),let model = new TSP.models.Sequential({    ...    font: ownFont,    ...}),```;"[""@syt123450 So I did a bit of digging on this after setting up a local instance of tensor (fixed a bug with missing dep aswell). I found the hard coded font and saw that there is an npm module for this specific font already (https://www.npmjs.com/package/three.regular.helvetiker). The rollup config didn't have common module and ES6 external resolver support so I added that into the build aswell. That at least trims down that giant line of json into one easy import. Still looking at the best way to pass through the option without rewriting too much function calls, but looks doable.====="", ""@Truemedia It's really a good news! I think this refactoring will highly improve the TensorSpace's source code readability. Could you send a PR related your refactoring?====="", '@BoTime Could you give some suggestions about rollup configuration?=====', ""@syt123450 Sure, would you be able to open a feature branch/develop branch for me to do a new request on so it doesn't go into master just yet.====="", 'Hi @Truemedia , I created a new issue #192 and create a new branch [refactorfont](https://github.com/tensorspace-team/tensorspace/tree/refactorfont). We can work under the new issue for refactoring, separate the work of refactor and new feature ~=====', ""@syt123450 Ok cool, I won't have much time to commit any code during week but weekend can probably box off both those branches with the new features=====""]"
https://github.com/tensorspace-team/tensorspace/issues/139;Add two finger rotation to control rotation;1;open;2018-11-29T08:21:55Z;2018-11-29T17:51:18Z;It's hard to control rotation in touchable screen, so may add two finger rotation to control rotation?;"[""@DIYer22 Thanks for your suggestion, now TensorSpace mainly use a three.js plugin [TrackballControl](https://github.com/mrdoob/three.js/blob/master/examples/js/controls/TrackballControls.js) to control events, it may has some limitation in touch device. It is possible to [registered new events](https://github.com/tensorspace-team/tensorspace/blob/master/src/scene/SceneInitializer.js#L157) for TensorSpace, which are not included in TrackballControl.Now, user can use two fingers to zoom in or out, translate model in the scene in the touch device, these touch events are provided by TrackballControl. Honestly, I don't have experience adding android's two finger rotation to website, and I am afraid I will break the TrackballControl's functionality QAQ.If you have any idea how to implement this amazing functionality, feel free to propose it to us or send us a PR ~=====""]"
https://github.com/tensorspace-team/tensorspace/issues/116;Enhancements;5;open;2018-11-14T00:27:06Z;2018-11-25T22:45:46Z;1. display weights (in addition to activations)2. display how activations/weights change during training. For example, I should be able to point to checkpoint directory where my model is saved every epoch (or every few iterations), and then hit 'play' button to see how a particular feature map or weights filter is changing as I move between epochs/iterations.3. display which inputs contribute the most to the activations, at every layer.These additional features would be very useful for debugging, for model compression, for identifying information flows (e.g. in a DenseNet), for saliency analysis, and many other things as well. ;['Nice suggestions! Thanks a lot.The first problem need to be solved is the API of `TensorFlow.js` is limited which is not powerful compare to `TensorFlow`. Feel hard to extract every relative weights, parameters and contributors in the models.Can this features be visualized by `TensorBoard`?=====', '> Can this features be visualized by `TensorBoard`?yes, I think so.=====', 'Our framework based on `TensorFlow.js`. So when it comes to the model part, it depends on the implements and APIs of `TensorFlow.js` because we need to run our model **in browser**. Our team will try to add this features.If you have some materials about this topic please refer them to me. Thanks=====']
https://github.com/tensorspace-team/tensorspace/issues/52;Add Unit test for tensorspace;5;open;2018-09-21T05:14:03Z;2019-01-21T23:12:42Z;Write unit test for utils, add test script under [test/unit](https://github.com/syt123450/tensorspace/tree/master/test/unit) folder.;['Headless Chromium for continuous integration testinghttps://medium.com/@metalex9/replace-phantomjs-with-headless-chromium-for-javascript-unit-testing-in-karma-59812e6f8ce4=====', 'Sample repo for using karma + headless chromium + jasmine in TravisCIhttps://github.com/BoTime/travisci-test=====', 'I Added a basic test case [here](https://github.com/tensorspace-team/tensorspace/blob/testcase/test/testcase.html) in `testcase` branch. @BoTime =====', 'https://github.com/BoTime/travisci-testhttps://github.com/BoTime/travisci-test/blob/master/test/e2e.spec.jsA working example to test Tensorspace based on your example. But your example is more like an integration test or e2e test. To run it ```shell# set environment to Travisexport CI=truenpm installnpm test# reset to environment to localunset CI```@syt123450 =====', 'https://blog.kentcdodds.com/write-tests-not-too-many-mostly-integration-5e8c7fff591c@syt123450 =====']
https://github.com/tensorspace-team/tensorspace/issues/16;Construct network examples;1;open;2018-09-12T04:16:28Z;2018-12-01T04:08:45Z;"More network examples build with Tensorspace, add interesting examples to ""examples"" folder:- [x] Resnet 50- [ ] Dense net- [x] Acgan generative model- [ ] pix2pix - Picture style transfer- [ ] Yolo v3-tiny- [x] Mobilenet v1- [x] Inceptionv3- [ ] SqueezeNet- [ ] ...";['ResNet50 added (#23). Need to be reviewed and completed.=====']
https://github.com/tensorspace-team/tensorspace/issues/14;Merge functions for Layers;1;open;2018-09-12T02:38:15Z;2018-11-29T11:59:00Z;Implement merge function for Tensorspace, support all merge function in keras including:- [x] Add3d- [x] Substract3d- [x] Multiply3d- [x] Average3d- [x] Maximum3d- [x] Concatenate3d- [ ] Dot3d- [x] Add2d- [x] Substract2d- [x] Multiply2d- [x] Average2d- [x] Maximum2d- [x] Concatenate2d- [ ] Dot2d- [x] Add1d- [x] Substract1d- [x] Multiply1d- [x] Average1d- [x] Maximum1d- [x] Concatenate1d- [ ] Dot1d;['User can use merge function like: TSP.add([layer1, layer2], config) to get a MergedLayer=====']
https://github.com/tensorspace-team/tensorspace/issues/229;ImportError: cannot import name 'convert_to_constants';3;closed;2019-04-13T11:49:46Z;2019-04-25T05:31:15Z;"在导入tensorflowjs的时候就会报这个错，在执行转换格式脚本的时候也是现实这个包导入有问题。>>> import tensorflowjs as tfjsTraceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""F:\anaconda\anaconda3\lib\site-packages\tensorflowjs-1.0.1-py3.6.egg\tensorflowjs\__init__.py"", line 21, in <module>    from tensorflowjs import converters  File ""F:\anaconda\anaconda3\lib\site-packages\tensorflowjs-1.0.1-py3.6.egg\tensorflowjs\converters\__init__.py"", line 24, in <module>    from tensorflowjs.converters.tf_saved_model_conversion_v2 import convert_tf_saved_model  File ""F:\anaconda\anaconda3\lib\site-packages\tensorflowjs-1.0.1-py3.6.egg\tensorflowjs\converters\tf_saved_model_conversion_v2.py"", line 29, in <module>    from tensorflow.python.framework import convert_to_constantsImportError: cannot import name 'convert_to_constants'";"[""I figured out that the problem here, was that **_convert_to_constants.py_** was missing from my python tensorflow framework directory.This problem can easily be solved, by copying the [convert_to_constants.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py) found on the official [web-page's framework directory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py), into your **Python36\\Lib\\site-packages\\tensorflow\\python\\framework** folder.In your case, you would probably have to copy the file to your **anaconda\\lib\\site-packages\\tensorflowjs-1.0.1-py3.6.egg\\tensorflowjs\\** folder.====="", '@hazily123 看起来像是 tensorflowjs 版本问题。对于模型预处理，可以试试最新发布的 TensorSpace 预处理工具 [TensorSpace-Converter](https://github.com/tensorspace-team/tensorspace-converter)（使用这个工具来预处理，理论上不会遇到以上提到的 python 包版本问题），可以极大简化预处理过程。=====', '我把文件放进去了文件夹，但是没有任何作用，还是会报那个错误。=====']"
https://github.com/tensorspace-team/tensorspace/issues/224;Demo - add TensorSpace to tfjs-vis;1;closed;2019-03-29T02:38:08Z;2019-04-02T06:09:46Z;"Add a demo to show how to add TensorSpace model to tfjs-visor. SS:<img width=""545"" alt=""Screen Shot 2019-03-28 at 7 36 34 PM"" src=""https://user-images.githubusercontent.com/7977100/55205659-f8a1bb00-5190-11e9-821c-533f6c120850.png"">";['* Example [link](https://github.com/tensorspace-team/tensorspace/tree/master/examples/render-in-tfjsvisor)* Example Screen Shot:![vis-demo](https://user-images.githubusercontent.com/7977100/55379805-075ed980-54d3-11e9-949f-4fa85d9a41fb.gif)=====']
https://github.com/tensorspace-team/tensorspace/issues/221;Using own images on YOLO model;3;closed;2019-03-21T20:17:19Z;2019-04-02T06:26:15Z;Hi, I have been trying to deployed YOLO model on my local web server and it successfully displayed the images within the examples. However, I would like to use my images for the model and I do not have the json version of the images. Is there anyway I can fit my images in the model for visualization purposes?Thank you so much.;"['Hi @trile83 , the .json data is the same as the prediction data in python, just in .json format.I use the following script to generate input data from .jpg image (make sure resize input image to 416 * 416 before applying the following script):```html<canvas id=""container"" width=""416"" height=""416""></canvas><script>var context = document.getElementById(\'container\').getContext(""2d""),var img = new Image(),img.onload = function () {  context.drawImage(img, 0, 0),  let dataArray = context.getImageData(0, 0, 416, 416).data,  let array = [],  for (let i = 0, i < dataArray.length, i++) {    if (i % 4 !== 3) {      array.push((dataArray[i] / 255).toFixed(4)),    }  }  console.log(array.toString()),},img.src = ""PATH/TO/IMAGE/predict_image.jpg"",</script>```If you have already prepared a dataset in python, you can reproduce the above function in Python to generate the JSON format input data .=====', 'Thank you so much for your help. I was able to use my image in the model using the script to process the image. However, I am new to python and I hope you can help me understand the function in python. I am planning to process an array of image and put into the model.=====', 'Hi @trile83 , for YOLOv2-tiny demo, each JSON data is an array [R, G, B, R, G, B ....., R, G, B], the array length is `3 * 416 * 416`. To reproduce it in Python, get out RGB of each point (416 * 416 in total) from input image, normalize the RGB value to 0-1, and then generate JSON array.As you have an array of images, you can apply this strategy to images one by one, and generate a folder of JSON files to be the input data of your model, or generate a matrix:[[R, G, B, R, G, B ....., R, G, B],[R, G, B, R, G, B ....., R, G, B],...[R, G, B, R, G, B ....., R, G, B]]To fully understand how YOLO pre-process input data and post-process network result, [YOLO website](https://pjreddie.com/darknet/yolo/) may help.=====']"
https://github.com/tensorspace-team/tensorspace/issues/219;[Question]: Loading Keras models into appropriate TSP layers automatically;2;closed;2019-03-11T18:28:46Z;2019-04-20T03:39:41Z;Hey, awesome project! Forgive me if this question is answered elsewhere.I noticed that the InceptionV3 example seems to manually specify the javascript definition of the Tensorspace models, even though I _think_ that the same information could be inferred given the `.json` file within that example.Am I wrong about this? I know there is an issue with some backends (tf.js?) not providing enough information for automated extraction and encoding into Tensorspace models, but was under the impression that Keras based models contained the necessary data.Have I messed up in my understanding of the `.json` keras model definitions?;"['Oh, this may just be solved by #213, when it closes :)=====', ""@Marviel Thanks for your suggestions!As you said,  tfjs model can provide some information for TensorSpace layers, and it is possible to get these information automatically, and I am designing the `auto-loader` to automatically create TensorSpace model with given pre-trained model.Theoretically, it is possible to get all information from tfjs layer model or graph model. As TensorSpace mainly used to visualize a pre-trained model, some layers will not take effect in model prediction process, such as, batchnorm, dropout, and so on, and these layers will not be visualized in TensorSpace. From my perspective, `auto-loader` needs to infer a sub-set of pre-trained model's topo structure, it is a little bit complex, and may take time to design and implement it.I am actively developing this feature, it will come in future version of TensorSpace~=====""]"
https://github.com/tensorspace-team/tensorspace/issues/217;Can not get input layer through getLayerByName();1;closed;2019-03-02T10:19:08Z;2019-03-02T10:26:56Z;model.getLayerByName() can not get input layer, to fix this bug, add extra check for input layer.;['Do not have this bug, the scene of accident QAQ=====']
https://github.com/tensorspace-team/tensorspace/issues/215;Stats error in LenetTraining example;2;closed;2019-02-23T10:22:46Z;2019-02-23T10:32:06Z;Stats error in [LenetTrainingExample](https://github.com/tensorspace-team/tensorspace/tree/master/examples/trainingLeNet), caused by stats usage change in SceneInitializer.;['Fixed by checking existence `Stats` type before `import` sentence.=====', '@BoTime The usage of Stats in sceneInitializer seems a little bit weird, maybe need further discussion.=====']
https://github.com/tensorspace-team/tensorspace/issues/214;Test and upgrade dependency version;1;closed;2019-02-23T07:26:34Z;2019-02-23T10:47:33Z;Test and make sure tensorspace work well with latest tfjs v1.0.0-alpha3 ;['loadFrozenModel now will trigger warning, caused by upgrade to 1.0.0-alpha3. Issue #212 will fix this warning.=====']
https://github.com/tensorspace-team/tensorspace/issues/212;Align loader API with new tfjs loaders;3;closed;2019-02-22T05:09:44Z;2019-02-24T11:23:19Z;* tf.loadFrozenModel() -> tf.loadGraphModel()* tf.loadModel() -> tf.loadLayersModel();['depend on #214 =====', 'Change:* tf.loadGraphModel() do not need to provide weightUrl=====', 'TODO:- [x] Change `tensorflow loader` API in TensorSpace, do not need to provide weightUrl- [x] Clear up examples using `tensorflow loader`=====']
https://github.com/tensorspace-team/tensorspace/issues/194;Create Angular Hello World;1;closed;2019-01-29T06:09:42Z;2019-02-11T06:49:06Z;Depends on #189 Create a hello world app that use Tensorspace with Angular.js ;['Resource:https://github.com/JohnnyDevNull/ng-three-template=====']
https://github.com/tensorspace-team/tensorspace/issues/189;Build tensorspace as CommonJS module;2;closed;2019-01-26T23:16:56Z;2019-02-07T04:04:18Z;### Goal:Use tensorspace in Node.js with `require()` syntax;"[""1. Refactor source code (add `import * as THREE from 'three'`)2. Two input files:- tensorspace.js -> tensorspace.cjs.js- tensorspace.dev.js -> tensorspace.dev.cjs.js====="", 'For threejs, added:```javascriptimport * as THREE from ""three"",```For tfjs, added:```javascriptimport * as tf from ""@tensorflow/tfjs"",```For tween.js, added:```javascriptimport * as TWEEN from ""@tweenjs/tween.js"",```For Stats.js, added:```javascriptimport Stats from ""stats.js/src/Stats"",```For trackballcontrol, added:```javascriptimport TrackballControls from ""three-trackballcontrols"",```=====']"
https://github.com/tensorspace-team/tensorspace/issues/184;Make easier to find in npm;3;closed;2019-01-24T15:14:21Z;2019-01-30T00:45:12Z;"Hi, just a small suggestion for improvement.Currently if you search for ""tensor flow"" or ""tensor space"", you won't find this package. If you add a keywords section to the package.json with those and relevant phrases people will find this library easier.";"['Thanks for your suggestions!=====', 'Can do a pull request if that ok? I have an idea in my head of the keywords and would like to contribute to future development if possible anyway.=====', ""@Truemedia PRs welcomed, looking forward to it! If you have any further interesting idea, free free to propose it and it's my pleasure working with you.=====""]"
https://github.com/tensorspace-team/tensorspace/issues/183;Simplify animation time configuration;1;closed;2019-01-22T16:25:15Z;2019-02-24T12:28:47Z;Previously, we use `animationTimeRatio` to control layer animation time. Developer can configure the `animationTimeRatio` for model and layers. The base animationTime is 2 seconds, the final animation time will be `base * animationTimeRatio_model * animationTimeRatio_layer`, however, it will be too complex and confusing.The latest strategy is to use `animeTime`, which is the real animation time, `animeTime` default to 2000 (which means 2 second). Developer can configure `animeTime` for model and layer, the model's `animeTime` will override default `animeTime`, the layer's `animeTime` will override model's `animeTime`.;['TODO:- [X] change api- [X] change usage in example- [x] change website docs=====']
https://github.com/tensorspace-team/tensorspace/issues/180;Add onProgress to loaders;3;closed;2019-01-12T07:31:25Z;2019-02-24T12:56:07Z;add onProgress callback function to show loading percentage.;['Waiting for `onProgress` feature in TensorFlow.js to be released.- [X] [tfjs-core#1485](https://github.com/tensorflow/tfjs-core/commit/7e30204d177320ff12160868616b514d4e5d82bd)- [X] [tfjs-layers#413](https://github.com/tensorflow/tfjs-layers/pull/413)- [X] [tfjs-converter#261](https://github.com/tensorflow/tfjs-converter/pull/261)=====', 'Depend on #212 & #214 =====', 'TODO:- [x] Add onProgress feature to [loader doc](https://tensorspace.org/html/docs/basicLoad.html) - [x] Add onProgress usage to playground examples=====']
https://github.com/tensorspace-team/tensorspace/issues/177;Failed to fetch local files when running examples;1;closed;2019-01-09T00:49:29Z;2019-01-18T12:10:33Z;- IssueSee screenshot below:![image](https://user-images.githubusercontent.com/25629006/50868376-11ee5480-1364-11e9-834c-d1b1d850a704.png)- Debugging:Tried to open this example from WebStorm and it works. But if I open it directly(File -> Open File) from the browser, the error above shows. So I am guessing fetch() api requires a static file server.;['TensorSpace must run in web environment. Directly open example files from browser will have errors.=====']
https://github.com/tensorspace-team/tensorspace/issues/175;Transfer gh-pages branch to new repo;1;closed;2019-01-07T21:55:22Z;2019-01-10T00:31:04Z;gh-pages host tensorspace official website. As there are some model files in gh-pages, it make tensorspace repository extremely large. Transfer gh-pages to a new repository `tensorspace-website`.;['Website now in [tensorspace-website](https://github.com/tensorspace-team/tensorspace-website).=====']
https://github.com/tensorspace-team/tensorspace/issues/170;Add predictDataShapes config in Model;1;closed;2018-12-15T03:57:10Z;2018-12-15T06:00:22Z;Add predictDataShapes config in model if model's input is dynamically, for example, inceptionv3 model in keras can accept dynamically input shape, if user want to visualize this model in TensorSpace, must config the predictDataShapes in TensorSpace Model.;['For a dynamically input shape model, for example, inceptionv3 from Keras, must provide input shape for prediction. Use TensorSpace Functional model like:```let model = new TSP.models.Model( modelContainer, {    inputs: [ input ],    outputs: [ predictions ],    predictDataShapes: [ [ 299, 299, 3 ] ]} ),```=====']
https://github.com/tensorspace-team/tensorspace/issues/160;Update documentation: shape of all layers;2;closed;2018-12-13T03:52:39Z;2018-12-17T00:16:01Z;Modify the documentation for shape configuration way.;['Conv and pooling move filter to the 3-dimension=====', 'All APIs support output shape constructor=====']
https://github.com/tensorspace-team/tensorspace/issues/156;TSP is not defined ReferenceError;3;closed;2018-12-11T09:17:48Z;2018-12-12T04:11:26Z;I'am new here. When i run the html from the examples/helloworld/helloworld.html with WebStorm, i got an error as follows:Uncaught SyntaxError: Unexpected token functionjQuery.Deferred exception: TSP is not defined ReferenceError: TSP is not defined    at HTMLDocument.<anonymous> (http://localhost:63342/tensorspace-master/examples/helloworld/helloworld.html:42:19)    at j (http://localhost:63342/tensorspace-master/examples/lib/jquery.min.js:2:29948)    at k (http://localhost:63342/tensorspace-master/examples/lib/jquery.min.js:2:30262) Uncaught ReferenceError: TSP is not defined  (anonymous function)	  j	  k	;"['Hi @jiaojiaole211, have you run `npm run build` to build the latest version of tensorspace.js?In `examples`, all html files include tensorspace.js from `build` folder```html<script src=""../../build/tensorspace.js""></script>```After clone the TensorSpace repo, you need to run `npm run build` to build the latest tensorspace.js from source code into the `build` folder.=====', '> > > Hi @jiaojiaole211, have you run `npm run build` to build the latest version of tensorspace.js?> > In `examples`, all html files include tensorspace.js from `build` folder> > ```> <script src=""../../build/tensorspace.js""></script>> ```> > After clone the TensorSpace repo, you need to run `npm run build` to build the latest tensorspace.js from source code into the `build` folder.@syt123450 hi，i didn\'t install npm. is there any other way to build tensorspace.js? Thank you very much=====', 'Hi @jiaojiaole211, you can replace the```<script src=""../../build/tensorspace.js""></script>```with ```<script src=""https://cdn.jsdelivr.net/npm/tensorspace@0.2.0/build/tensorspace.min.js""></script>```0.2.0 is the latest public version of TensorSpace, I think it can work well with most examples.=====']"
https://github.com/tensorspace-team/tensorspace/issues/155;Conv2d and Pooling2d shape config definition ;1;closed;2018-12-10T19:40:23Z;2018-12-11T19:23:26Z;Change Conv2d and Pooling2d shape config definition. Conv2d and Pooling2d have shape constructor, however, the shape definition in this constructor have different meaning with the upcoming shape feature ( #152 ).Change From:```javascriptTSP.layers.Conv2d( { filters : Int , shape : [ Int, Int ] } ),```Change To: ```javascriptTSP.layers.Conv2d( { shape : [ Int, Int, Int ] } ),```;"[""Conv2d and Pooling2d's constructor in next TensorSpace version:```TSP.layers.Conv2d( { shape : [ Int, Int, Int ] } ),``````TSP.layers.Pooling2d( { shape : [ Int, Int, Int ] } ),```=====""]"
https://github.com/tensorspace-team/tensorspace/issues/153;Export input layer in model preprocessing;2;closed;2018-12-09T17:52:52Z;2018-12-15T02:27:12Z;In old way, TensorSpace enforces to exclude input layers from multiple output list in model preprocessing. To make the preprocessing and TensorSpace predictor consistent in later development, in later version of TensorSpace, export all layers in preprocessing which will be presented in 3D scene including input layers.To support this idea, modify:- [x] predictor and renderer in source code- [ ] preprocessing tutorial- [ ] preprocessing sample code- [ ] models in examples;"['Since Keras does not support any input related ""Layer"" class, we can\'t encapsulate all inputs within the preprocessed model.=====', 'Deprecate this idea QAQ=====']"
https://github.com/tensorspace-team/tensorspace/issues/152;Create layer by shape;1;closed;2018-12-09T01:01:50Z;2018-12-13T01:20:01Z;"Make TensorSpace's layer can be created by directly given the shape. For example:```javascriptlet depthwiseConv2d = new TSP.DepthwiseConv2d({    shape: [28, 28, 32],    name: ""depthwise_1""}),```";['For 1d layer, use config:```{    shape: [ 28 ]}```For 2d layer, use config:```{    shape: [ 28, 28 ]}```For 3d layer, use config:```{    shape: [ 28, 28, 3 ]}```=====']
https://github.com/tensorspace-team/tensorspace/issues/150;Dense layer overlay in Model;1;closed;2018-12-08T22:06:22Z;2018-12-08T22:25:01Z;"When dense layer opened in functional model, the close button will overlay the left aggregation object, it seems that the 1d layer need a larger bounding box.<img width=""288"" alt=""2018-12-08 2 03 41"" src=""https://user-images.githubusercontent.com/7977100/49691231-559a3800-faf2-11e8-883b-0c3339aa9225.png"">";"['Fixed.<img width=""394"" alt=""2018-12-08 2 21 39"" src=""https://user-images.githubusercontent.com/7977100/49691359-0570a500-faf5-11e8-9734-234a83485a33.png"">=====']"
https://github.com/tensorspace-team/tensorspace/issues/148;Improve reset() method for Model;1;closed;2018-12-01T08:48:11Z;2018-12-01T09:18:24Z;reset() method for Model now can:* clear layer data* reset cameraImprove reset() method, when this method called, close layer if layer is open.;"['When reset() method in Model is called, three steps:1. reset camera,2. clear the layer visualization data,3. set all layers to ""initStatus"", ""close"" or ""open"".=====']"
https://github.com/tensorspace-team/tensorspace/issues/147;Test and upgrade three.js r99;1;closed;2018-12-01T07:49:20Z;2018-12-02T11:54:55Z;- [x] test r99 in TensorSpace- [x] upgrade examples lib- [x] upgrade website lib;['It has been test that TensorSpace can work well with three.js r99.=====']
https://github.com/tensorspace-team/tensorspace/issues/143;Add paging attribute for Input1d layer;1;closed;2018-11-29T13:22:38Z;2018-11-30T19:23:16Z;[Paging](https://tensorspace.org/html/docs/basicPaging.html) feature for Input1d attribute, for better visual effects.;['Can be used like:```let input = new TSP.layers.Input1d({      shape: [400],      paging: true,      segmentLength: 100}),```=====']
https://github.com/tensorspace-team/tensorspace/issues/142;Optimize relation lines;1;closed;2018-11-29T12:09:00Z;2018-11-30T18:29:23Z;Some relation line cases can be optimized, including:- [x] 1. merge function 1d, when merged layer is open.- [x] 2. concatenate3d miss one relation line when layer is open.- [x] 3. merge2d function and merged3d function, use curve line when layer is open, make sure the relation line will not overlap.- [x] 4. functional model's relation line, the relation line from last layer is curve now, which should be straight line.;"['For 4.Before fix:<img width=""500"" alt=""2018-11-29 8 54 23"" src=""https://user-images.githubusercontent.com/7977100/49238324-0a2ab000-f3b5-11e8-9a04-3f0c0dc6bc8d.png"">After fix: <img width=""500"" alt=""2018-11-29 8 54 33"" src=""https://user-images.githubusercontent.com/7977100/49238334-0eef6400-f3b5-11e8-9909-c2f3551a4b03.png"">=====']"
https://github.com/tensorspace-team/tensorspace/issues/138;Unsupported Ops in the model before optimization OneShotIterator, IteratorGetNext?;1;closed;2018-11-28T05:28:37Z;2019-04-20T03:03:33Z;Unsupported Ops in the model before optimization OneShotIterator, IteratorGetNext ?;"[""@yts19871111 Unfortunately, TensorSpace haven't reached pre-processing layer step. However, its really an interesting idea to add pre-processing layers to TensorSpace, if you have any idea which layers can be included, what these layers shall look like, feel free to propose it to us ~=====""]"
https://github.com/tensorspace-team/tensorspace/issues/136;Add Inceptionv3 demo to playground;1;closed;2018-11-27T11:54:45Z;2018-11-29T09:12:04Z;- [x] playground inceptionv3 model- [x] test images- [x] links modification for other playground pages;['Inceptionv3 demo in website playground: https://tensorspace.org/html/playground/inceptionv3.html=====']
https://github.com/tensorspace-team/tensorspace/issues/135;Add version attribute to TensorSpace;1;closed;2018-11-27T11:17:40Z;2018-12-08T22:08:37Z;User can know TensorSpace version from this attribute.;"['```javascript// ""0.2.0""console.log( TSP.version ),```=====']"
https://github.com/tensorspace-team/tensorspace/issues/133;Hello, how can I run this hello world locally?;1;closed;2018-11-24T05:04:13Z;2019-01-09T21:47:21Z;I have downloaded and installed node.jsGit your projectcd  E:\ynh\c7\tensorspace-master cnpm install tensorspacecnpm inpm run buildopen helloworld.html![image](https://user-images.githubusercontent.com/32252319/48964756-6d00fd80-efe9-11e8-9564-72b010c339a2.png);"['Have you `start a local web server to view the html file`? Directly use ""open"" command to open html, the url in browser may get a ""file://"" prefix. The browser will give limit privileges to files in this case.If you are using some IDE, for example WebStorm, its really easy to start a local server to run TensorSpace examples.=====']"
https://github.com/tensorspace-team/tensorspace/issues/126;Functional Model predict render bug;1;closed;2018-11-20T07:45:04Z;2018-11-20T07:57:49Z;Functional Model Input and Layer render have bug.;['Fixed during the process of building Inceptionv3 model.=====']
https://github.com/tensorspace-team/tensorspace/issues/121;visualize the predictions;7;closed;2018-11-17T08:57:14Z;2019-04-20T03:03:19Z;@CharlesLiuyx Hi, I got some problems here when I'm trying to visualize the output image with my model. I even couldn't get the prediction result. May you help me with this? And the following is the javascript problems:Uncaught TypeError: Cannot read property 'dataSync' of undefined    at Sequential.updateLayerPredictVis (tensorspace.js:2548)    at Sequential.updateVis (tensorspace.js:2512)    at Sequential.predict (tensorspace.js:2269)    at Object.success (unet_sequence.html?_ijt=26969st38nf3fi6d731vk69l6f:150)    at i (jquery.min.js:2)    at Object.fireWith [as resolveWith] (jquery.min.js:2)    at A (jquery.min.js:4)    at XMLHttpRequest.<anonymous> (jquery.min.js:4);"['It seems that your pre-trained model do not compatible with TensorSpace, have you preprocessed(about TensorSpace preprocess: https://tensorspace.org/html/docs/preIntro.html) your pre-trained model properly?Can you provide more code information, such as github repo or codepen, maybe with more information, I can help position problem more precisely.=====', '@syt123450 thanks a lot! It worked! I did forget to preprocess the model and everything goes well after I did this! You guys really did an amazing job!=====', '@NextGuido You are welcome! Hope you enjoy TensorSpace~=====', '@syt123450 Hi, I tried adding a self-designed layer with keras using Lambda, but I cannot visualize the model once again! Can you solve this problem?Uncaught (in promise) Error: Unknown layer: Lambda. This may be due to one of the following reasons:1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().    at new t (errors.ts:48)    at deserializeKerasObject (generic_utils.ts:202)    at deserialize (serialization.ts:27)    at i (container.ts:1329)    at t.fromConfig (container.ts:1355)    at deserializeKerasObject (generic_utils.ts:235)    at deserialize (serialization.ts:27)    at models.ts:270    at index.ts:79    at Object.next (index.ts:79)=====', ""@NextGuido From the information you provided, it seems like you have the errors after model conversion. You probably need to register your custom layer before executing the converted tfjs model. If that's the issue, you can check the repo from official TensorFlow.js: https://github.com/tensorflow/tfjs-examples/tree/master/custom-layerIf you think that's still not the error source, can you provide more details about the error? 1. Which stage do you get the error? At preprocessing, building model or loading model?2. Can you provide more details about your custom layer? Like how to build it?And it will be helpful if you can provide some sample codes or links that we can have a look.====="", ""@zchholmes Oh, I just wrote the code with keras using a custom layer and use tensorflowjs_converter to convert the model. When I did everthing as before, I met the error. I think you are right for registering my custom layer before executing the converted tfjs model. But I'm not familiar with the tfjs and javascript code. So, is there an easy way solve this? ====="", ""@NextGuido At this point, we really can't help too much. Since TensorSpace renders the model in web browser, which is required to consume a web acceptable model format - tensorspace.js model format. For now, there's no way to walk around, especially for the case involves custom layer, which requires extra description and clarification on the layer you customized.=====""]"
https://github.com/tensorspace-team/tensorspace/issues/119;Network with skip connections;4;closed;2018-11-16T20:09:11Z;2019-01-09T21:47:44Z;"Hello,I would like to build this network with tensorspace, is it possible ?![model](https://user-images.githubusercontent.com/5201978/48644628-d0b37700-e9a8-11e8-9234-c60a024e6b90.png)Here is the code I tried:```{% load static %}<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>TensorSpace - Saddle</title>    <meta name=""author"" content=""syt123450 / https://github.com/syt123450"">    <script src=""{% static ""lib/three.min.js"" %}""></script>    <script src=""{% static ""lib/tween.min.js"" %}""></script>    <script src=""{% static ""lib/tf.min.js"" %}""></script>    <script src=""{% static ""lib/TrackballControls.js"" %}""></script>    <script src=""{% static ""lib/tensorspace.min.js"" %}""></script>    <script src=""{% static ""lib/jquery.min.js"" %}""></script>    <style>        html, body {            margin: 0,            padding: 0,            width: 100%,            height: 100%,        }        #container {            width: 100%,            height: 100%,        }    </style></head><body><div id=""container""></div><script>    $(function() {		let data_path = ""{% static ""saddle/"" %}"",		let modelContainer = document.getElementById( ""container"" ),		let input_1 = new TSP.layers.Input1d({			shape: [10]		}),		let dense_1 = new TSP.layers.Dense({ units: 80 }),		dense_1.apply(input_1),		let dropout_1 = new TSP.layers.Layer1d( { shape: [80] } ),		dropout_1.apply(dense_1),		let dense_2 = new TSP.layers.Dense({ units: 5 }),		dense_2.apply(dropout_1),		let dropout_2 = new TSP.layers.Layer1d( { shape: [5] } ),		dropout_2.apply(dense_2),		let dropout_3 = new TSP.layers.Layer1d( { shape: [10] } ),		dropout_3.apply(input_1),		let dense_3 = new TSP.layers.Dense({ units: 160 }),		dense_3.apply(dropout_3),		let dropout_4 = new TSP.layers.Layer1d({ shape: [160] }),		dropout_4.apply(dense_3),		let dense = new TSP.layers.Dense({ units: 20 }),		dense.apply(input_1),		let dropout = new TSP.layers.Layer1d({ shape: [20]}),		dropout.apply(dense),		let concatenate = new TSP.layers.Layer1d( { shape: [185] } ),		concatenate.apply(dropout_4),		let dense_4 = new TSP.layers.Dense({ units: 1 }),		dense_4.apply(concatenate),		let model = new TSP.models.Model(modelContainer, {			inputs: [input_1],			outputs: [dense_4],		}),		console.log(""ici""),		model.load({			type: ""tfjs"",			url: data_path+""model/model.json"",			onComplete: function() {				console.log( ""\""Hello World!\"" from TensorSpace Loader."" ),			}		}),		model.init( function() {			$.ajax({				url: data_path+""data/s1.json"",				type: 'GET',				async: true,				dataType: 'json',				success: function (data) {					model.predict( data ),				}			}),		} ),    }),</script></body></html>```and here is the result I have![screenshot 2018-11-16 at 13 48 18](https://user-images.githubusercontent.com/5201978/48644738-2ab43c80-e9a9-11e8-8628-6be4bee0f4cd.png)Thank you for your help !";['TensorSpace can be used to create skip connection such as Inception block in Inception or residual block in ResNet. However, the merge function in TensorSpace only support for 3d layer (https://tensorspace.org/html/docs/mergeConcate.html) now.Temporarily, TensorSpace can not create skip connection for 1d layers(like dense layer), I am working it, and will add this new feature to TensorSpace as soon as possible.=====', '@syt123450 thanks ! that would be very helpful !=====', 'Hi @Deathn0t , the latest TensorSpace ( [ v0.2.0 ](https://github.com/tensorspace-team/tensorspace/releases/tag/v0.2) ) supports 1d concatenate operation. For your skip connection graph, I wrote a code sample in [ this repo ](https://github.com/syt123450/skip-connection-1d). In this sample, I generated a random model using TensorFlow.js ( [trainingModel.html](https://github.com/syt123450/skip-connection-1d/blob/master/trainingModel.html) ) and visualized it with TensorSpace ( [index.html](https://github.com/syt123450/skip-connection-1d/blob/master/index.html) ).However, as Dropout layer only take effect in model training process, I did not add it to TensorSpace model. If you want to add it to your TensorSpace model, just like your previous code, use Layer1d to replace the Dropout Layer.Hope these code snippets can help ~=====', 'Thank you @syt123450 ! I will try it soon !=====']
https://github.com/tensorspace-team/tensorspace/issues/118;Installation [From Scratch];3;closed;2018-11-16T13:18:27Z;2019-01-09T21:47:05Z;Hi I am pretty new here to play with tensorspace.js.  I am able to see the git and stuck to install in my desktop.My questions 1. do i have to install node.js and npm to run tensorspace.js?Could you direct me for a quick installation guide? Many thanks. ;['https://tensorspace.org/html/docs/index.html#downloadError page=====', '> https://tensorspace.org/html/docs/index.html#download> Error pageThanks for your report, I have fixed the broken link.The work download link is: https://tensorspace.org/index.html#download=====', 'If you just want to checkout TensorSpace examples locally, it is not required to use npm or node.js. However, if you want to have a look at latest TensorSpace library or try to add new features to TensorSpace, you may need to use npm and run `npm run build` or `npm run build-publish` to build latest TensorSpace, and get the build files in `build` folder.I hope these short guides can help you start your experience with TensorSpace ~=====']
https://github.com/tensorspace-team/tensorspace/issues/117;Live Loader for visualizing training process;4;closed;2018-11-15T00:57:27Z;2018-12-04T10:53:30Z;"User can use this loader to load a ""live model"" (not a pre-trained model), and visualize training process.Usage:```model.load({    type: ""live"",    modelHandler: reference_to_model}),```";['Example to show how to use TensorSpace to visualize the training process of LeNet https://github.com/tensorspace-team/tensorspace/tree/master/examples/trainingLeNet=====', 'doc and example gif required @CharlesLiuyx =====', 'Playground Online Demo:https://tensorspace.org/html/playground/trainingLeNet.html=====', 'Already uploaded .gif to team driver=====']
https://github.com/tensorspace-team/tensorspace/issues/114;Non-image;1;closed;2018-11-13T02:15:20Z;2019-01-09T21:46:54Z;Is the tool compatible with non-image data?;['Like the NLP model? We are still working on it( Try to add some plugin to support the NLP model, maybe seq2seq?). If you input a vector, matching with the structure of your model, tensorspace will work fine. But insight of this 3D visualization will be not so intuitive.=====']
https://github.com/tensorspace-team/tensorspace/issues/106;Add link to properties and methods;1;closed;2018-11-05T07:39:29Z;2018-11-06T10:41:52Z;inputShape `layer.js`outputShape `layer.js` neuralValue `layer.js`name `layer.js`layerType `layer.js`apply() `NativeLayer.js`  and  `MergedLayer.js` merge to be specific> Merge API need to be modifiedopenLayer() ➜ `NativeLayer1d 2d 3d` `MergedLayer 3d` divide by dimentioncloseLayer() ➜ `NativeLayer1d 2d 3d` `MergedLayer 3d` divide by dimention;['# 1D: `NativeLayer1d.js` `openLayer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer1d.js#L486``closeLyaer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer1d.js#L504`**Dense Flatten Activation1d Layer1d Input1d Output1d**# 2D: `NativeLayer2d.js``openLayer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer2d.js#L412``closeLyaer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer2d.js#L432`**Conv1d Cropping1d GlobalPooling1d Padding1d Pooling1d Reshape1d UpSampling1d Input2d Activation2d Layer2d**# 3D:  `NativeLayer3d.js``openLayer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer3d.js#L459``closeLyaer()`: `https://github.com/tensorspace-team/tensorspace/blob/38650f71af638527502fce69d9908a8228934b6f/src/layer/abstract/NativeLayer3d.js#L475` **Conv2d Conv2dTranspose Cropping2d GlobalPooling2d Padding2d Pooling2d Reshape2d UpSampling2d Input3d Activation3d Layer3d**=====']
https://github.com/tensorspace-team/tensorspace/issues/95;Yolov2-tiny vis problem in pooling layer;1;closed;2018-10-31T04:21:44Z;2018-11-01T03:31:35Z;;['Caused by model.=====']
https://github.com/tensorspace-team/tensorspace/issues/94;Add Layer API gif;1;closed;2018-10-31T03:45:49Z;2018-11-11T21:18:46Z;## Layer- [x] Conv2d- [x] Conv2d gif - [x] Conv1d - [ ] Conv1d gif- [x] Input1d - [ ] Input1d gif- [x] Input2d- [ ] Input2d gif- [x] Input3d- [x] Input3d gif- [x] Padding2d - [ ] Padding2d gif- [x] Padding1d- [ ] Padding1d gif- [x] Pooling2d - [ ] Pooling2d gif- [ ] Pooling1d - [ ] Pooling1d gif- [x] Dense - [ ] Dense gif- [x] Output1d - [ ] Output1d gif- [x] UpSampling2d- [ ] UpSampling2d gif- [x] UpSampling1d- [ ] UpSampling1d gif- [x] Reshape- [ ] Reshape gif- [x] Flatten - [ ] Flatten gif- [x] Cropping2d - [ ] Cropping2d gif- [x] Cropping1d- [ ] Cropping1d gif- [x] Conv2dTranspose- [ ] Conv2dTranspose gif- [x] YoloGrid - [ ] YoloGrid gif- [x] OutputDetection- [ ] OutputDetection gif- [x] GlobalPooling2d- [ ] GlobalPooling2d gif- [x] GlobalPooling1d- [ ] GlobalPooling1d gif- [x] Activation2d - [ ] Activation2d gif - [x] Activation1d- [ ] Activation1d gif- [x] Activation3d - [ ] Activation3d gif## Merge- [x]  Add- [ ]  Add gif- [x]  Average- [ ]  Average gif- [x]  Maximum- [ ]  Maximum gif- [x]  Multiply - [ ]  Multiply gif- [x]  Substract - [ ]  Substract gif- [x]  Concatenate - [ ]  Concatenate gif;['## 静态图- 左边收缩图 + 右边展开图 = 1070 * 450520px * 450px 每幅图 中间白色 30px## 动态图- 展开 800 * 450- 收缩 800 * 450=====']
https://github.com/tensorspace-team/tensorspace/issues/86;Layer Documentation Process;1;closed;2018-10-14T17:19:26Z;2018-10-20T00:57:03Z;- [x] Layer Introdcution [zh](https://github.com/syt123450/tensorspace/wiki/%5BLayer%5D-Introduction_zh) - [x] Layer Introdcution [en](https://github.com/syt123450/tensorspace/wiki/%5BLayer%5D-Introduction)- [x] [基本概念] 网络层 [zh](https://github.com/syt123450/tensorspace/wiki/%5B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%5D-%E7%BD%91%E7%BB%9C%E5%B1%82) - [x] Basic Concept Layer [en](https://github.com/syt123450/tensorspace/wiki/%5BBasic-Concepts%5D-Layer)- [x] [基本概念] 网络层度量 [zh](https://github.com/syt123450/tensorspace/wiki/%5B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%5D-%E7%BD%91%E7%BB%9C%E5%B1%82%E5%BA%A6%E9%87%8F) - [x] Layer Metrixs [en](https://github.com/syt123450/tensorspace/wiki/%5BLayer%5D-LayerMetrixs)- [x] [基本概念] 网络层维度 [zh](https://github.com/syt123450/tensorspace/wiki/%5B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%5D-%E7%BD%91%E7%BB%9C%E5%B1%82%E7%BB%B4%E5%BA%A6) - [x] Layer Dimension [en](https://github.com/syt123450/tensorspace/wiki/%5BLayer%5D-LayerDimension)- [x] [基本概念] 网络层颜色 [zh](https://github.com/syt123450/tensorspace/wiki/%5B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%5D-%E7%BD%91%E7%BB%9C%E5%B1%82%E9%A2%9C%E8%89%B2) - [x] Layer Color [en](https://github.com/syt123450/tensorspace/wiki/%5BLayer%5D-LayerColor);['Finish=====']
https://github.com/tensorspace-team/tensorspace/issues/76;API Documentation Process;2;closed;2018-10-08T19:56:38Z;2018-10-19T07:06:31Z;Complete the API docs## Model- [x] TSP.model.Sequential [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Sequential_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Sequential)- [x] TSP.model.Model [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Model_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Model)## Layer- [x] Conv2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv2d) ![#FFFF2E](https://placehold.it/15/FFFF2E/000000?text=+) - [x] Conv1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv1d)- [x] Input1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input1d)- [x] Input2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input2d) ![#EEEEEE](https://placehold.it/15/EEEEEE/000000?text=+)- [x] Input3d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input3d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Input3d)- [x] Padding2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_padding2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_padding2d) ![#6eb6ff](https://placehold.it/15/6eb6ff/000000?text=+)- [x] Padding1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_padding1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_padding1d)- [x] Pooling2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Pooling2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Pooling2d) ![#00ffff](https://placehold.it/15/00ffff/000000?text=+) - [x] Pooling1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Pooling1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Pooling1d)- [x] Dense [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Dense_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Dense) ![#00ff00](https://placehold.it/15/00ff00/000000?text=+)- [x] Output1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Output1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Output1d) ![#EEEEEE](https://placehold.it/15/EEEEEE/000000?text=+) - [x] UpSampling2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_UpSampling2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_UpSampling2d) ![#30e3ca](https://placehold.it/15/30e3ca/000000?text=+)- [x] UpSampling1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_UpSampling1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_UpSampling1d)- [x] Reshape [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape) ![#a287f4](https://placehold.it/15/a287f4/000000?text=+)Reshape1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape1d)Reshape2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Reshape2d)- [x] Flatten [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Flatten_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Flatten) ![#dfe2fe](https://placehold.it/15/dfe2fe/000000?text=+)- [x] Cropping2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Cropping2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Cropping2d) ![#cefc86](https://placehold.it/15/cefc86/000000?text=+)- [x] Cropping1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Cropping1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Cropping1d)- [x] Conv2dTranspose [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv2dTranspose_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Conv2dTranspose) ![#ff5722](https://placehold.it/15/ff5722/000000?text=+)- [x] YoloGrid [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_YoloGrid_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_YoloGrid)- [x] OutputDetection [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_OutputDetection_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_OutputDetection)- [x] GlobalPooling2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_GlobalPooling2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_GlobalPooling2d)  ![#0074e4](https://placehold.it/15/0074e4/000000?text=+)- [x] GlobalPooling1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_GlobalPooling1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_GlobalPooling1d)- [x] Layer2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer2d) ![#f08a5d](https://placehold.it/15/f08a5d/000000?text=+)- [x] Layer1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer1d)- [x] Layer3d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer3d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Layer3d)- [x] Activation2d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation2d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation2d) ![#fc5c9c](https://placehold.it/15/fc5c9c/000000?text=+)- [x] Activation1d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation1d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation1d)- [x] Activation3d [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation3d_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Activation3d)## Merge- [x]  Add [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Add_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Add)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+) - [x]  Average [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Average_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Average)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+)- [x]  Dot [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Dot_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Dot)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+)- [x]  Maximum [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Maximum_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Maximum)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+)- [x]  Multiply [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Multiply_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Multiply)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+)- [x]  Substract [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Substract_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Substract)  ![#e23e57](https://placehold.it/15/e23e57/000000?text=+)- [x]  Concatenate [zh](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Concatenate_zh) | [en](https://github.com/syt123450/tensorspace/wiki/API_Docs_Merge_Concatenate)  ![#f9a1bc](https://placehold.it/15/f9a1bc/000000?text=+);['API Docs directory:[API Problem list](https://github.com/syt123450/tensorspace/wiki/API_Problem_List)[Sequential API Configuration](https://github.com/syt123450/tensorspace/wiki/Sequential-API-Configuration)[Layer API Configuration](https://github.com/syt123450/tensorspace/wiki/Layer-API-Configuration)[load model API](https://github.com/syt123450/tensorspace/wiki/load-model-API)[Layer dimension](https://github.com/syt123450/tensorspace/wiki/Layer-Dimension)[readme prototype](https://github.com/syt123450/tensorspace/wiki/Readme-prototype)=====', '# Modify Process List![image](https://user-images.githubusercontent.com/21956621/46630831-397e1880-cafa-11e8-8dbf-b394b8dd2aa6.png)=====']
https://github.com/tensorspace-team/tensorspace/issues/73;Ref. Basic Concepts;1;closed;2018-10-07T04:05:56Z;2018-10-26T05:41:29Z;General Model Implementations:- [x] Model - [x] Layer- [x] Preprocessing- [x] Load model- [x] Predict- [x] Layer Metrics- [x] Layer DimensionsVisualization Features:- [x] CloseButton- [x] Layer Color- [x] Paging- [x] Stats;['Add more if necessary @CharlesLiuyx =====']
https://github.com/tensorspace-team/tensorspace/issues/64;Source code comments review;2;closed;2018-09-28T18:18:24Z;2018-11-11T11:51:13Z;Review the comments from sources code to ensure the clarity, the mean and the standard.- [ ] animation- [ ] assets- [ ] configure- [ ] elements- [ ] layer- [ ] loader- [ ] merge- [ ] predictor- [ ] scene- [ ] utils- [ ] vis-model- [ ] tensorspace.js;"['@syt123450  I put all sub-directories from ""src"", feel free to REMOVE any part that\'s not ready or not necessary for review.=====', 'Ready to be reviewed:- [ ] animation- [ ] assets- [ ] configure- [ ] elements- [x] layer    - [x] abstract    - [x] input    - [x] intermediate    - [x] output- [x] loader- [ ] merge- [x] predictor- [ ] scene- [ ] utils- [x] tsp-model- [ ] tensorspace.js@zchholmes =====']"
https://github.com/tensorspace-team/tensorspace/issues/63;HTML example clean up;1;closed;2018-09-28T18:13:57Z;2018-10-06T08:29:42Z;Based on the standard from Three.js examples (https://github.com/mrdoob/three.js/tree/master/examples),clean up and reformat the existed example codes and comments.;['forget to clean up ResNet-50=====']
https://github.com/tensorspace-team/tensorspace/issues/54;Fix dense clear bug;1;closed;2018-09-21T11:07:24Z;2018-09-21T12:17:28Z;If dense layer is opening, call clear will have bug.;['Fixed, caused by repeatedly setting layer status.=====']
https://github.com/tensorspace-team/tensorspace/issues/46;Configure for opacity;1;closed;2018-09-18T09:34:14Z;2018-09-21T12:10:27Z;The default opacity is 0.3 as Constant, support configure for opacity in layer and model.;"['use now can use configuration ""minOpacity"" [0.3, 1] in model and layers to configuration min opacity.=====']"
https://github.com/tensorspace-team/tensorspace/issues/39;Add control for layer animation ratio;1;closed;2018-09-17T08:23:04Z;2018-09-18T08:18:45Z;Now the animation time for layer animation is 2 seconds, this animation time may be too short or too long in some cases. Let user can configuration animation time in model configuration or in each layer configuration.;['animationTimeRatio configure in model and layer to control animation time.=====']
https://github.com/tensorspace-team/tensorspace/issues/36;Configure for tensorspace background color;1;closed;2018-09-16T06:30:43Z;2018-09-16T08:57:27Z;User can configure for tensorspace background color through model configuration.;"['Now user can config background color of the scene, the effect will be like this:<img width=""809"" alt=""2018-09-16 1 55 13"" src=""https://user-images.githubusercontent.com/7977100/45594696-d6162600-b953-11e8-960a-0b6eee0d5ae3.png"">=====']"
https://github.com/tensorspace-team/tensorspace/issues/35;Yolo v3 tiny example;3;closed;2018-09-16T06:22:27Z;2018-11-06T12:35:19Z;Add example for yolo v3 tiny network.;['Resource:https://github.com/mystic123/tensorflow-yolo-v3https://github.com/qqwweee/keras-yolo3=====', 'transform yolov3-tiny cfg to tensorflow model and use tensorspace to visualize it.=====', 'duplicate #16 =====']
https://github.com/tensorspace-team/tensorspace/issues/34;Return predict result to user;2;closed;2018-09-16T05:15:39Z;2018-09-16T10:42:47Z;When user doing predict in tensorspace, tensorspace would pass predict result to user.;['two steps:- [x] refactor prediction process with predictor- [x] add callback for predictor=====', 'Now user can get predict result from callback, use like this:```javascriptmodel.predict(data, function(predictResult) {    console.log(predictResult),}),```=====']
https://github.com/tensorspace-team/tensorspace/issues/33;Callback for loader;1;closed;2018-09-16T05:00:06Z;2018-09-16T06:11:17Z;Add onComplete callback for loader. User can trigger its own function when load model complete.;"['Now loader has onComplete callback, user can use loader like this:```model.load({\ttype: ""tfjs"",        url: \'./lenetModel/mnist.json\',        onComplete: function() {\t\tconsole.log(""Complete load model.""),        }}),```=====']"
https://github.com/tensorspace-team/tensorspace/issues/30;Fix hover error log;1;closed;2018-09-15T13:30:19Z;2018-09-16T06:44:20Z;Hover on layer sometimes show error log, may be caused by last layer's status.;"['Caused by set isOpen status too late, and for layer1d, add a ""isTransition"" status.=====']"
https://github.com/tensorspace-team/tensorspace/issues/28;Configuration for Stat.js;1;closed;2018-09-15T13:27:24Z;2018-09-16T08:38:42Z;User can configure for Stat.js. By this configuration, tensorspace can be with less dependence.;"['User now can configure whether to use Stats.js for tensorspace model like this:```let model = new TSP.model.Sequential(container, {\tlayerInitStatus: ""close"",\taggregationStrategy: ""max"",        layerShape: ""square"",        textSystem: ""enable"",        relationSystem: ""enable"",        stats: true,}),```=====']"
https://github.com/tensorspace-team/tensorspace/issues/24;Default color for tensorspace layers;1;closed;2018-09-15T06:23:10Z;2018-10-30T05:58:54Z;Choose default color for layers, and documentation to explain it.;['After choose default color, modify files:https://github.com/syt123450/tensorspace/blob/master/src/configure/ModelConfiguration.js=====']
https://github.com/tensorspace-team/tensorspace/issues/23;ResNet50 model;1;closed;2018-09-14T02:58:52Z;2018-09-16T09:39:46Z;- [x] Preprocess ResNet50 model- [x] Apply TensorSpace API to preprocessed model;['Model source: ~~https://www.kaggle.com/keras/resnet50/home~~keras.applications.resnet50.ResNet50 (https://keras.io/applications/)Model structure ref: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006=====']
https://github.com/tensorspace-team/tensorspace/issues/18;Convert yolo output to Frame format for image;3;closed;2018-09-12T06:03:39Z;2018-10-03T22:55:27Z;;['- [x] Get `const` contains the data and figure out the data structure- [x] Get out the data by layers- [x] Figure out how to calculate the parameters[Google Doc Reference](https://docs.google.com/document/d/1UC-Fm2ERewIBCroIpDknt8Ye4X68YZK9S-OlUC8bKLc/edit)- [x] Convert the 425 inputs to get data with above format=====', 'Convert data format:inputData: javascript array length = 525outputData: javascript array, x, y is the top left corner of the rectangleassume image size is 416 * 416 to calculate the x, y, width, height``` [{\tx: 0,\ty: 0,\twidth: 100,\theight: 150}, {\tx: 250,\ty: 50,\twidth: 100,\theight: 50}, {\tx: 100,\ty: 10,\twidth: 50,\theight: 100}, {\tx: 200,\ty: 200,\twidth: 100,\theight: 200}, {\tx: 250,\ty: 300,\twidth: 100,\theight: 80}],```=====', '- [ ] Implement the IOU- [ ] Implement the NMS =====']
https://github.com/tensorspace-team/tensorspace/issues/17;Dense layer optimization;2;closed;2018-09-12T05:47:40Z;2018-09-18T07:43:33Z;If there is too many units in dense layer, the dense layer will be too long, we need to figure out a strategy to optimize it.;['Use pagination strategy to organize dense output.=====', 'same as #3 =====']
https://github.com/tensorspace-team/tensorspace/issues/15;Add functional layer;1;closed;2018-09-12T03:41:05Z;2018-09-12T05:38:38Z;Add function layer1d, layer2d, layer3d. Users can use these these functional layer to build any kind of layer they like.- [x] layer1d- [x] layer2d- [x] layer3d;['The usage of function layer:new TSP.Layer3d({shape: [20, 20, 6]})new TSP.Layer2d({shape: [20, 6]})new TSP.Layer1d({shape: [20]})As users can not configure the relation for functional layer, the functional layer will not show relation lines.=====']
https://github.com/tensorspace-team/tensorspace/issues/12;Build tensorflowjs model to get the input and output of ACGAN;3;closed;2018-09-11T17:49:55Z;2018-09-30T15:54:58Z;Load the model from `Google Drive` . Figure out how to make the input and output work properlyTo be done before 09/14/2018;['Transfer the model using tfjs[Reference](https://github.com/syt123450/tensorspace/wiki/keras-preprocessing-tutorial)=====', '~Reconstruct the model to fit the one-line model design pattern~Can not make it work unless we could support the 2 inputs model structure=====', '(1) Read the source code, figure out how model.predict works.(2) Try to modify the 2 inputs to 1 and let one input as a parameter.=====']
https://github.com/tensorspace-team/tensorspace/issues/10;LoadModel Percentage;2;closed;2018-09-11T16:49:58Z;2018-09-15T17:02:41Z;We want to show the process while we are loading a large model.;"[""I've asked the question in the tensorflow discussion group:https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!topic/tfjs/Tu6UMbWJADAFrom the source code of weights_loader, I can't find any clue on any callback() for loading models.https://github.com/tensorflow/tfjs-core/blob/master/src/io/weights_loader.ts====="", 'Replied within 10 minutes......""This is on our TODO list. To stay updated about the progress, you can cc yourself on the GitHub issue: https://github.com/tensorflow/tfjs/issues/336""Close now, could be reopened once the feature is introduecd=====']"
https://github.com/tensorspace-team/tensorspace/issues/9;Npm registeration for tensorspace;1;closed;2018-09-11T15:19:11Z;2018-09-13T04:20:46Z;Account and npm, release tensorspace 0.0.1;['Done.https://www.npmjs.com/package/tensorspace=====']
https://github.com/tensorspace-team/tensorspace/issues/8;Auto layer creation for tfjs model;2;closed;2018-09-11T09:22:32Z;2018-11-06T12:36:25Z;Automatically read layer configuration from tfjs model, create and add tensorspace layers for model.In this way, users only need to create Sequential model, do not need to create and configure the layers.;['Blocked by #5=====', '> Blocked by #5As layer detection may not support by dependence, our strategy is to let user to configure the layer name and layer type.=====']
https://github.com/tensorspace-team/tensorspace/issues/6;Close button configuration;2;closed;2018-09-11T01:06:40Z;2018-09-15T13:18:39Z;Add the feature which can change the config of the close button.Include but not limit on enable or not, displayable or not etc.;"['Close button config:- [x] display or not- [x] size ratio=====', 'User now can configure close button like this:```let layer2 = new TSP.layers.Conv2d({\tkernelSize: 2,\tfilters: 4,\tstrides: 1,\tpadding: ""same"",\tcloseButton: {\t\tratio: 1,\t\tdisplay: true\t}}),```=====']"
https://github.com/tensorspace-team/tensorspace/issues/5;Find a way to detect layer type/class from tfjs model;3;closed;2018-09-11T01:04:27Z;2018-11-04T12:39:28Z;From the loaded tfjs model (json+weights), we'd better to find the way to catch the layer types.Now we can catch other configurations, such as output_shape, filter, stride etc.If we can find the layer type/class, we can keep the process on the auto-generation from a target tfjs model for applying tsp API.;"['So far, it seems it\'s mission impossible.Since the JavaScript has not supported ""class"" yet, which means all we can get is the names of object, constructer and prototype. However:1. layer name is customized. It could be anything that user put in,2. constructer name for all layer is called ""t"", (><)3. all layers are from the same prototype.=====', 'The only possible way for now is to determine the configurations (shapes/filter/size etc.) of the input and output tensors, then based on some evaluation to get a best ""guess"".=====', 'Could be separated as a tool which would be implemented by Python in the future.=====']"
https://github.com/tensorspace-team/tensorspace/issues/4;Implement new function for auto-fitting shapes of different layers;1;closed;2018-09-11T00:59:33Z;2018-09-17T16:27:10Z;To implement a function which can auto-fitting the given number of feature map centers.It should follow the strategy like:1. sqrt(), if it's a perfect squre. e.g. if 1024, return 32x32,2. rectangle which as close as a square. e.g. if 24, return 4x6 instead of 3x8,3. squre/rectangle with less empty slot. e.g. if 47, return 7x7 with 2 empty slots,;['Note:4. If the ratio of width (long side) and height (short side) is over 2 (2:1), then the function returns a square shape like 3.=====']
https://github.com/tensorspace-team/tensorspace/issues/3;Coordinate shape of Dense layer;1;closed;2018-09-11T00:52:25Z;2018-09-20T09:52:38Z;Try to change the single linear structure of the Dense layer to other compatible or other visual-friendly shape.It is used to avoid the case like when we have a dense layer 4096, it is difficult to see the model compare with the length of dense layer. Camera and other auto-fitting feature may be affected.;['Use pagination strategy to organize dense output.=====']
https://github.com/hiukim/mind-ar-js/issues/90;about miniprogram;6;open;2021-12-10T05:21:06Z;2021-12-22T15:25:58Z;can make it support wechat miniprogram?;"['Do you know whether wechat miniprogram can start webcam? @zero16832 =====', 'I mean that can make support with wechat?=====', ""I understand your question. I'm just wondering if wechat miniprogram can start webcam====="", 'Yes,it can start webcam.=====', '@hiukim =====', ""@zero16832 Actually I don't know very well about wechat miniprogram. Last time I checked (long time ago), I remembered webcam cannot be started. Maybe it's changed.What's the easiest way to test it nowadays?Is there a specific error message you are receiving?=====""]"
https://github.com/hiukim/mind-ar-js/issues/88;image-tracking and Intereactive video;5;open;2021-12-07T06:12:03Z;2021-12-23T09:47:56Z;This is a great code repository! In addition, can you provide the code for the effect shown in the figure below? That would be greatly appreciated.![image](https://user-images.githubusercontent.com/19623177/144975999-604f310b-02b4-4109-a888-3ffd2ecb2041.png);"['@hiukim I would also like to ask, can I use some cv algorithms(deep learning) for detection?=====', '@wduo I\'m not sure I understand the questions.What effect are you referring to?and what do you mean by using some ""cv"" algorithms for detection?=====', 'Hi, @hiukim The effect I am talking about refers to: pop-up card, pop-up 3D model, pop-up video (click to play this video), pop-up the 4 icons below. I only found html sample codes for pop-up cards and pop-up 3D models. Can you provide the complete code for the effect shown in the above gif animation? Thank you so much.=====', '@wduo ""view-source:https://hiukim.github.io/mind-ar-js-doc/samples/advanced.html""=====', '@hiukim Great job! very interesting. In addition, I am a computer vision algorithm engineer, can we have some cooperation?=====']"
https://github.com/hiukim/mind-ar-js/issues/87;How to trigger targetFound again after unpause or stop?;1;open;2021-12-06T05:15:30Z;2021-12-09T15:46:49Z;expect: video feeding -> **targetFound** -> pause -> [environment remains unchanged]-> unpause -> **targetFound**current situation:After unpause, the targetFound never triggered until i move the target outside the camera to trigger targetLost event.attempt:1. i called `arSystem.stop` and `arSystem.start()`，the `.mind` file was reloaded, but the targetFound won't be triggered.1. i used` jQuery.trigger` to trigger `targetFound` from outside after unpause, didn't work....;"[""Your use case seems pretty tricky. I'm not sure a `targetFound` event firing after `unpause` should be the expected behaviour.=====""]"
https://github.com/hiukim/mind-ar-js/issues/69;[feature request] AR Camera GYRO;5;open;2021-11-07T15:23:45Z;2021-11-11T03:07:16Z;With the gyroscope alone, it's possible to do something [like this](https://www.youtube.com/watch?v=8etGUUtLoJY&ab_channel=MakakaGames%3AMadewithUnity). I have no idea how. But I suppose it works by placing the objects around the camera, and use the gyroscope to keep them on a more or less fixed position. Can mind-ar-js implement this feature?;"['with gyroscope alone, seems hard to stick to the ground.  will at least need some kind of plane detection and tracking maybe?=====', ""No, I confirmed with the author that's it's just the gyroscope. There is no slam or anything. You can see that the objects drift a bit.  It doesn't detect the ground or anything. The idea is that you just place the objects around the camera and use the gyro to keep them in place as best as possible.Still, it's good enough for a number of use cases. Take a look at [this game](https://assetstore.unity.com/packages/templates/systems/ar-camera-lite-ar-engine-gyroscope-accelerometer-augmented-reali-191073).====="", '@hiukim Do you have an idea how to do it? Do you use Unity?=====', ""@marcusx2 That's very interesting.  But it seems like it can detect the floor, judging from the shadows?====="", ""It doesn't detect the floor, the shadows is just an effect. There is no floor detection at all. Nothing. It's just the gyroscope.=====""]"
https://github.com/hiukim/mind-ar-js/issues/65;"Define hidding time of a-entity after ""targetLost""";3;open;2021-10-22T02:47:18Z;2021-11-23T12:44:20Z;"Hello guys, before starting the text I would like to congratulate you for the excellent work developing the library. Congratulations!I set up an augmented reality timeline that has around 44 bookmarks with another application (OpenSpace3d), the problem came with registering the app in the playstore and the lack of interest from users to download them, so I decided to reprogram it now with mindAR. Just to show off I leave you a picture :)![image](https://user-images.githubusercontent.com/67477259/138384180-3d7d2d2e-0a96-4493-a09e-34205fe46636.png)Returning to the topic of the thread. Would you like to see if it is possible to define the time it takes for the a-entity to disappear once the target is lost?![image](https://user-images.githubusercontent.com/67477259/138384552-6d30622d-490b-4809-8c25-2d04bc55ea38.png)clickMe.addEventListener(""targetLost"") takes about 8 seconds after the marker is no longer in front of the camera to log it to the console. I don't know if it is a tolerance that can be defined in the a-scene attribute or if I could make a direct modification to the core of the library to short that tolerance time.Greets, AngelPS. Sorry for the funny english.PS. You can check the web version of that linetime at https://rmn2.com/graffiti-gdl/recorriendo-el-trazo/";"['> > > Hello guys, before starting the text I would like to congratulate you for the excellent work developing the library. Congratulations!> > I set up an augmented reality timeline that has around 44 bookmarks with another application (OpenSpace3d), the problem came with registering the app in the playstore and the lack of interest from users to download them, so I decided to reprogram it now with mindAR. Just to show off I leave you a picture :)> > ![image](https://user-images.githubusercontent.com/67477259/138384180-3d7d2d2e-0a96-4493-a09e-34205fe46636.png)> > Returning to the topic of the thread. Would you like to see if it is possible to define the time it takes for the a-entity to disappear once the target is lost?> > ![image](https://user-images.githubusercontent.com/67477259/138384552-6d30622d-490b-4809-8c25-2d04bc55ea38.png)> > clickMe.addEventListener(""targetLost"") takes about 8 seconds after the marker is no longer in front of the camera to log it to the console. I don\'t know if it is a tolerance that can be defined in the a-scene attribute or if I could make a direct modification to the core of the library to short that tolerance time.> > Greets, Angel> > PS. Sorry for the funny english. PS. You can check the web version of that linetime at https://rmn2.com/graffiti-gdl/recorriendo-el-trazo/Good job man, I would like to know about this as well...Eres de Guadalajara!?you seem to have been able to get video to work in different targets, how were you able to accomplish this using web, hosted on private host? I have this code and it\'s not working (I wonder if it\'s because I am using a video on top of a background image...?)`<a-scene mindar-image=""imageTargetSrc: ./SSS-target.mind,"" color-space=""sRGB"" renderer=""colorManagement: true, physicallyCorrectLights"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: `false"">``<a-camera` position=""0 0 0"" look-controls=""enabled: false"">`</a-camera>``<a-assets>` `<video` id=""#vid"" `src=""https://www.v4verastegui.com/mindar-project/scottmillersss.mp4""></video>``</a-assets>``<a-entity` id=""my_target"" mindar-image-target=""targetIndex: 0"">`<a-video` src=""#vid"" position=""0 0 .05"" width="".96"" height="".54"" rotation=""0 0 0"" `></a-video>``<a-image` id=""background"" src=""topshapeandbackground.png"" opaciy=""0.5"" position=""0 0 0"" height=""1.3145"" width=""1"" rotation=""0 0 `0""></a-image>``</a-entity>``<script>` // detect target found const myTarget = document.getElementById(""my_target""), const vid = document.getElementById(""vid""), myTarget.addEventListener(""targetFound"", event => { console.log(""target found""), vid.play(), vid.muted = false, }), // detect target lost myTarget.addEventListener(""targetLost"", event => { console.log(""target lost""), vid.muted = true, vid.pause(), }), </script> ```</a-scene>``Post Data: Man podrias pasarme tu contacto, quisiera poder consultar contigo como hacer ciertos arreglos, apenas estoy aprendiendo sobre A-Frame y se muy poco de programacion.=====', 'I think the example you have at https://www.v4verastegui.com/mindar-project have some issues.This is the basic html structure for mindar to work like a charm with multiple targets.MindAr dependencies`    <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />    <meta name=""apple-mobile-web-app-capable"" content=""yes"">    <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@1.0.0/dist/mindar-image.prod.js""></script>    <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>    <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@1.0.0/dist/mindar-image-aframe.prod.js""></script>`Open a-scene tag, and reference the mindset you can compile here -> https://hiukim.github.io/mind-ar-js-doc/tools/compile/` <a-scene mindar-image=""imageTargetSrc: ./mindset/foo.mind,"" color-space=""sRGB"" renderer=""colorManagement: true, physicallyCorrectLights"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"">`You open a-assets container (tag) and add all the video, image, sound or 3dmodels assets you want to display. Remember each asset have an ID selecter, they are unique, you can only use each once.`<a-assets>                <video crossorigin=""anonymous"" preload=""metadata"" id=""idFoo1"" src=""./videos/idFoo1.mp4""></video>                <video crossorigin=""anonymous"" preload=""metadata"" id=""idFoo2"" src=""./videos/idFoo2.mp4""></video> </a-assets>`After closing  a-assets tag you define a-camera attributes, its important to ad rayOrigien: mouse and objets: .clickable, those supposed to play and stop video.`<a-camera              position=""0 0 0""              look-controls=""enabled: false""              cursor=""fuse: false, rayOrigin: mouse,""              raycaster=""far: 10000, objects: .clickable""            >            </a-camera>`After using the a-camera tag you link each image on the mindset with the assets you define before in a-assets container. Mindset is an array an start with 0 position, so the first image in the imagecompiler will be mindar-image-target=""targetIndex: 0"" and so on.`<a-video mindar-image-target=""targetIndex: 0""            class=""clickable""             src=""#idFoo0""              height=""1""            width=""1"" >            </a-video>            <a-video mindar-image-target=""targetIndex: 1""            class=""clickable""             src=""#idFoo1""              height=""1""            width=""1"" >            </a-video>`After you link all your AR assets to the position of your images in the Mindset file. You are almost done. Just have to close <a-scene> tag. And add this JS code that goes trough all .clickable class <a-video mindar-image-target=""targetIndex: n""> and add and EventListeners to targetFound and targetLost`const buttons = document.querySelectorAll("".clickable""),buttons.forEach(        clickMe => {              clickMe.addEventListener(""targetFound"", event => {        console.log (event),        var idVideo = (event.target.attributes[""src""].value),        var video = document.querySelector(idVideo),        video.play(),        event.target.classList.add(""act"")    }),    clickMe.addEventListener(""targetLost"", event => {        //console.log (event),        var idVideo = (event.target.attributes[""src""].value),        var video = document.querySelector(idVideo),        video.pause(),        event.target.classList.remove(""act"")    }),        }),`I don\'t know about your specific example but the source attribute on video seems to have a mistake and the url itself throw 500 internal error. So mindar can\'t find your video file.PS. Escríbeme a: abundis [en] rmn2.com=====', ""Has anyone solved this problem?I think that when there is no image, the scene should immediately disappear. it just doesn't look pretty.please tell me how can i change targetlost delay=====""]"
https://github.com/hiukim/mind-ar-js/issues/64;Video not showing, not playing;4;open;2021-10-20T03:43:11Z;2021-10-24T20:13:14Z;"Hi, I've done everything to try to make a video appear and play efficiently, I was able to see a little bit of it but only when the video was just too big to be completely shown.What is the right way to code it?      <a-scene mindar-image=""imageTargetSrc: ./SSS-target.mind"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"" color-space=""RGB"" renderer=""colorManagement: true, physicallyCorrectLights"">``<a-camera position=""0 0 0"" look-controls=""enabled: false""`<a-cursor></a-cursor></a-camera>``<a-light type=""ambient"" intensity=""1""></a-light>``<a-assets><video id=""SSS-video""        preload=""auto""        autoplay loop=""true"" crossOrigin=""anonymous"" muted        src=""https://www.v4verastegui.com/mindar-project/Scottmillersss.mp4"" controls>      </video></a-assets>``<a-entity mindar-image-target=""targetIndex: `0"">`<a-video src=""#SSS-video""  position=""0 0 .05"" width="".96"" height="".54"" rotation=""0 0 0"" preload=""auto"" `></a-video>`<a-image id=""background"" src=""topshapeandbackground.png"" opaciy=""0.5"" position=""0 0 0"" height=""1.3145"" width=""1"" rotation=""0 0 0""></a-image>``</a-entity></a-scene></body></html>";"['Does it work without MindAR? I mean as a regular aframe application?=====', 'Here is an example of how I got it working ( Code block kept breaking sorry ) :)<a-scene mindar-image=""imageTargetSrc: /my-marker.mind"" color-space=""sRGB"" renderer=""colorManagement: true, physicallyCorrectLights"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"">\t<a-assets>\t\t<video preload=""auto"" id=""vid"" response-type=""arraybuffer"" loop=""false"" crossorigin webkit-playsinline playsinline controls muted>\t\t\t<source src=""/my-video.mp4"" type=""video/mp4, codecs=""avc1.42E01E, mp4a.40.2"">\t\t\tYour browser does not support Or Else Please Click Reset Button.\t\t</video>\t</a-assets>\t<a-camera position=""0 0 0"" look-controls=""enabled: false""></a-camera>\t<a-entity id=""my_target"" mindar-image-target=""targetIndex: 0"">\t\t<a-video src=""#vid"" position=""0 0 0.5"" rotation=""0 0 0"" width=""1"" height=""1""></a-video>\t</a-entity></a-scene><script>\t// detect target found\tconst myTarget = document.getElementById(""my_target""),\tconst vid = document.getElementById(""vid""),\tmyTarget.addEventListener(""targetFound"", event => {\t\tconsole.log(""target found""),\t\tvid.play(),\t\tvid.muted = false,\t}),\t// detect target lost\tmyTarget.addEventListener(""targetLost"", event => {\t\tconsole.log(""target lost""),\t\tvid.muted = true,\t\tvid.pause(),\t}),</script>=====', '> > > Here is an example of how I got it working ( Code block kept breaking sorry ) :)> > ```> <a-camera position=""0 0 0"" look-controls=""enabled: false""></a-camera>> > <a-entity id=""my_target"" mindar-image-target=""targetIndex: 0"">> \t<a-video src=""#vid"" position=""0 0 0.5"" rotation=""0 0 0"" width=""1"" height=""1""></a-video>> </a-entity>> ```> > <script> // detect target found const myTarget = document.getElementById(""my_target""), const vid = document.getElementById(""vid""), myTarget.addEventListener(""targetFound"", event => { console.log(""target found""), vid.play(), vid.muted = false, }), // detect target lost myTarget.addEventListener(""targetLost"", event => { console.log(""target lost""), vid.muted = true, vid.pause(), }), </script>it\'s still not working for me, what If I am using a video on top of an image? this is my code now that I tried to use your script`<a-scene` mindar-image=""imageTargetSrc: ./SSS-target.mind"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"" color-space=""RGB"" renderer=""colorManagement: true, `physicallyCorrectLights"">`<a-camera position=""0 0 0"" look-controls=""enabled: `false"">``</a-camera><a-assets><video` id=""#vid"" `src=""scottmillersss.mp4""></video></a-assets>``<a-entity` id=""my_target"" mindar-image-target=""targetIndex: `0"">``<a-video` src=""#vid"" position=""0 0 .05"" width="".96"" height="".54"" rotation=""0 0 0"" `></a-video>``<a-image` id=""background"" src=""topshapeandbackground.png"" opaciy=""0.5"" position=""0 0 0"" height=""1.3145"" width=""1"" rotation=""0 0 `0""></a-image>``</a-entity>``<script>` // detect target found const myTarget = document.getElementById(""my_target""), const vid = document.getElementById(""vid""), myTarget.addEventListener(""targetFound"", event => { console.log(""target found""), vid.play(), vid.muted = false, }), // detect target lost myTarget.addEventListener(""targetLost"", event => { console.log(""target lost""), vid.muted = true, vid.pause(), }), </script> `</a-scene>` =====', '> > > Does it work without MindAR? I mean as a regular aframe application?Some have been able to have their videos shown on just A-Frame, Three.js Ar.js but many say that autoplay parameter deactivates the video on AR experiences on webAR, strangely even though I took off the autoplay parameter from the video line, it still not showing, I am going to try with a WebM format and see if it works, I am also using a video in front of an image. If it works next thing would be trying to set a video inside of a 3D box with a paralax effect, is there any recommended lines to use to create that effect?=====']"
https://github.com/hiukim/mind-ar-js/issues/63;ES6 module support;3;open;2021-10-18T21:32:38Z;2021-10-18T22:54:50Z;"Anyone has experience building library with es6 module? The goal is to allow module import for MindAR, which will be similar to importing three.js,  e.g.```<script type=""module"">import MindAR from './mindar-image.esm.js',</script>```Apparently, I'm having a hard time doing that. I feel like it shouldn't be too difficult. Currently, we are only using a simple webpack config: https://github.com/hiukim/mind-ar-js/blob/master/webpack.config.prod.cjsAnyone can advice what are the necessary changes?";"[""I've been importing it like this in my tests:`import 'mind-ar/dist/mindar-image.prod.js',`Don't know much about webpack though.====="", ""In the `output` field you should add an entry `libraryTarget: 'umd',` in this way webpack will build for the major standards (module too).====="", ""@ThorstenBux Yes, right now, I'm simply assigning it to the window object, e.g. `window.MINDAR`, but it isn't being exported as a module. @kalwalt I believe I've tried that without much luck. =====""]"
https://github.com/hiukim/mind-ar-js/issues/60;Showing Animated GIF ;1;open;2021-10-13T06:38:55Z;2021-10-15T17:10:04Z;Hi,Is there any way to show an animated gif in MindAR? I am searching for this for long time and I found https://github.com/mayognaise/aframe-gif-shader, but is not compatible with mind ar. Please help me;"[""Hey @sandeepkronline ,The repo you linked hasn't been updated in over 3 years. In that time, A-frame has progressed several versions and it's unlikely that they are compatible with the version of A-frame you're likely using. For more info [read this stackoverflow thread](https://stackoverflow.com/questions/61852705/how-do-i-place-animated-gif-in-a-frame). You have a few options. You could learn how to author a GLSL shader using widely available resources online. Alternatively, consider using a video texture instead and just looping it. Finally, if we're going to assume that your project requires A-frame, you could split your GIF animation into a series of PNG frames and swap between them programmatically.Cheers=====""]"
https://github.com/hiukim/mind-ar-js/issues/59;can not open camera in android edge browser ;2;open;2021-10-12T11:50:19Z;2021-12-13T02:49:00Z;can not open camera in android edge browser . app version is 96.0.1043.0 and its ok in old version of edge.;"[""Hey @chenleicrm ,Is there any specific error message you're getting? It'll be much easier pin-pointing the issue if you could post the code that is causing the error. The necessary API's are supported in Edge so I'm leaning towards some misconfiguration. ====="", 'just open the demo link **https://hiukim.github.io/mind-ar-js-doc/samples/basic.html**  in edge browser, already get the camera permission, but can not open the camera and the whole screen is white -- =====']"
https://github.com/hiukim/mind-ar-js/issues/55;feature request: marker based ar;7;open;2021-10-05T21:49:18Z;2021-12-23T10:07:03Z;Something [like this](https://ar-js-org.github.io/AR.js-Docs/marker-based/).;"['@hiukim do you intend to add all the features present on arjs? That would be nice, to make mindarjs a complete alternative solution to arjs.=====', '@marcusx2 Just wondering do people still use this kind of markers given that there is natural image tracking?At this moment, I personally have no plan to work on this type of ""marker based"". But I\'m open to it. If a lot of people think it\'s useful, or if anyone interested to implement it, I\'m happy to discuss.=====', ""@hiukim Yes, people use it. Think of it as normal image tracking, but it's a generic image that goes on top of several other images. It's used when you want to activate AR content with any image. A big qrcode is placed on top of the image(imagine there are many image targets) and is used for activation and tracking at the same time. So instead of tracking several different image targets, you have just one qr code.Anyhow, I'd love to see mindarjs as a complete replacement for arjs.What about [this feature request](https://github.com/hiukim/mind-ar-js/issues/54)?====="", ""@marcusx2 NFT (Natural Feature Tracking) is capable of tracking arbitrary images. You can upload your ArUco marker, QR code, or other marker to the [compiler tool](https://hiukim.github.io/mind-ar-js-doc/tools/compile) and achieve pretty much the same results. I'd, personally, be much more interested in having 'Plugins' for MindAR that could switch between backends - say instead of the FREAK detector/descriptor we can swap it out for ORB(v1/v2/v3). Even more interesting is if @hiukim has explored some recent advancements in employing a stronger ML presence in the library, for example [this paper](https://arxiv.org/abs/1906.02539) where the authors compute the homography parameters beyond just a 4-point bounding box. Could be useful for world-tracking / image-tracking by computing the homography between two adjacent frames and reconstructing a camera pose.====="", 'The problem is that normal image tracking is much less stable especially on longer distances. If you have a multitude of markers that you want to use to create content at the same time NFT is rather slow. Markers can usually be read over a longer distance.=====', ""So then let's have marker based ar as well! Hahaha====="", ""In my opinion, I agree with @Blackclaws and markers still have their own sense for performances and tracking stability, even if Mind-arjs image tracking is already great.Anyway, I do not know if this is the best feature where to carry efforts: I mean, what are the most used type of AR today?- Markerless (aka plane tracking)- Image tracking (already on mindarjs)- Location based? (could be ported from AR.js)- Marker basedI think (personally) that markers are a subset of image tracking, with some enhancements but also bigger limitations.If I'd have to choose, I will not go for markers as first next thing to add but rather on Markerless. Just my opinion looking at the current market situations and other paid libraries like 8thWall.=====""]"
https://github.com/hiukim/mind-ar-js/issues/54;feature request: Location Based AR;16;open;2021-10-05T21:46:50Z;2021-12-23T10:01:46Z;Something like [this](https://ar-js-org.github.io/AR.js-Docs/location-based/).;"[""I personally have never worked on location based AR. but I'm open to this. Is there particular anything you think ar.js need to improve on location based AR?====="", 'AR.js has done a okay job putting Aframe entity pinned to a latlng. But adding any position tag would override the latlng. I’m interested to try something else out, too=====', '`Is there particular anything you think ar.js need to improve on location based AR?`No, not really. My idea here is to have your library be a complete replacement for arjs. If I someday need image tracking and locationar, I can just use your library instead of mixing with arjs. Your image tracking is already better than theirs by a mile.=====', ""@hiukim Come on, please implement geolocation ar. You can ignore [this feature](https://github.com/hiukim/mind-ar-js/issues/55) request because as @akhrarovsaid points out, it can already be done with image tracking. But geolocation is another story. It's the only feature mindar doesn't have when comparing with arjs. Pretty please implement this. In fact the time has come where I need geolocationar and image tracking on the same project. I hope it's not much of a pain to use mindarjs with arjs for this. Otherwise I'll just stick with arjs, even though your mindarjs is better!====="", ""@marcusx2 I guess I could try to look into it.  but I've never used that before and I don't really have much idea about it.I guess AR.js use gps location and gyroscope? anything else?since there is no ground tracking in ar.js, so the augmented objects is just floating in the air somehow? Is it very unstable though?====="", ""That's all as far as I know. You can read more about it [here](https://ar-js-org.github.io/AR.js-Docs/location-based/).  You can try this [codepen](https://codepen.io/nicolocarpignoli/pen/MWwzyVP).====="", ""@hiukim if you're familiar with three.js there's a three.js implementation without A-Frame that could be a good reference. https://github.com/AR-js-org/AR.js/tree/dev/three.js====="", ""@raywu Nice! yes, I'm familiar with three.js. Thanks for the reference!====="", '@hiukim Is there an eta for this feature? Just curious no pressure haha.=====', '@marcusx2 sorry, was busy these days.  I guess I will try to look into it on December. =====', 'Hi @hiukim , are you looking into this feature? No pressure haha, just want to know if I can expect it soon. Thanks!=====', '@hiukim ? ,_,=====', ""@marcusx2 sorry about the late reply. Actually, I've taken a little look into the location AR in ar.js. At this moment, I have a general idea on how it works. However, I don't have much idea on what's lacking there and what to improve. Unlike image tracking, I have been using it a lot and I understand deeply what's lacking so I have strong opinion on how to improve it.In contrast, I feel like I'm not very knowledgeable in location AR. Other than just copying the code here and update the API to make it more consistent with mindAR, I don't have much idea on what else to do.====="", ""Hi @hiukim , if you can just add the same location AR that is already present in arjs that would be great. So I don't need to mix arjs with mindarjs when image tracking and location ar are both needed.====="", ""@marcusx2 I just released a new version with the support of three.js integration now instead of AFRAME. here is an example: https://hiukim.github.io/mind-ar-js-doc/more-examples/threejs-imageWith this new version, I think it's easy to import both libraries. I haven't tried it, but by looking at this example: https://github.com/AR-js-org/AR.js/blob/dev/three.js/location-based/example/index.htmlI can see that with the previous AFRAME version, things could get messy. but I think it's kind of straight forward now. The only thing is that there might be different versions of THREE.js====="", ""Hi everybody,I think @nickw1 might help you @hiukim on porting location based of AR.js into your project.As for now, though, AR.js has location based only on the A-Frame side.@hiukim in location based everything is estimated based on gyroscope data and gps data. It is not very precise, but works okay for distant objects. Even if I started the feature myself, Nick is far more expert than me in general and has worked a lot with that in the past months, so he is willing to, he can probably help.I can help you anyway, but I feel like I can't do much more - in fact, all I was knowing I already put it on the docs and on the code comments, just to prepare for this moments :)=====""]"
https://github.com/hiukim/mind-ar-js/issues/38;Tutorial on how to make object stays visible even when the image target is lost;6;open;2021-06-15T21:02:37Z;2021-10-17T21:11:33Z;"Considering that mind-ar has a huge potential and that use case is very usual, I'd like to help you create such tutorial/documentation page.Do you already have in mind what changes are required to do so?You mentioned that ""If you want to do something like this, you can approach the problem by using the non-aframe library build, then include and modify the above aframe.js script"", but it wasn't that clear to jump into the code and do it.Any guidelines?";"['@pedrorocha-net Sure, that would be nice! Appreciate the help.Sorry, that part of documentation is kind of sketchy. I wasn\'t prepared to expose the `Controller` class in the first place. I try to elaborate a bit:First of all, if you use the build without aframe, i.e. `https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@0.4.2/dist/mindar.prod-min.js`.  The only difference is that the `aframe` extension wasn\'t included. and the aframe extension is this file `https://github.com/hiukim/mind-ar-js/blob/master/src/aframe.js`. In the other words, you need to add a modified version of this file as part of your application code.There are five imports at the top```const {Controller} = require(\'./controller\'),const {UI} = require(\'./ui/ui.js\'),require(""aframe""),require(""aframe-extras""),const Stats = require(""stats-js""),```You can include the official`aframe` and `aframe-extras` sources from cdn. `stats-js` is not really necessary, but you can still include the officlal one from cdn too. If you don\'t need the stats, just remove the relevant codes. replace `Controller` with `window.MINDAR.Controller`. In the other words,You should copy that aframe.js file and change to something like this```const Controller = window.MINDAR.Controller,// require(""aframe""),// require(""aframe-extras""),// const Stats = require(""stats-js""),// const {UI} = require(\'./ui/ui.js\'),....```Then in your html page, should be something like this```<head>   <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@0.4.2/dist/mindar.prod-min.js""></script>    <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>   <script src={{AFRAME_EXTRA_LIKE_ABOVE}}</script>   <script src={{STAT_JS_LIKE_ABOVE}}</script>   <script src=""./custom-aframe.js""></script>  </head>```So that leaves `UI`, which turns out to be kind of complicated. They are the default UI, like scanning screen. If you don\'t need those, you can also remove it and create your custom UI, just comment out the relevant codes. If you want to keep it, then you need to port those in. They are not that complicated, but still take a little time I guess. The source is here `https://github.com/hiukim/mind-ar-js/blob/master/src/ui/ui.js`If you managed to do those, then the app should be running like the original production build. Then you can start tweaking the aframe extension to suit specific needs.Sorry, I know it\'s not straight forward. This is really not I originally expect people to do. but since people are asking how to achieve something the library wasn\'t supporting yet, I can only come up with this ugly solution for the moment.=====', '@pedrorocha-net Actually there is another easier way.  You can just clone the repo, and modify the `aframe.js` directly and make a custom build. But that means you are branching out from the master.I guess depending on your intention. If you want to improve the library, and make the changes generic to common cases, then I can merge your update into master too.=====', 'Hi @pedrorocha-net @hiukim , it would be great if you can develop this option for everyone.   I mean, as an option in the library, so we all can use this option.  Thank you guys.  =====', ""Thanks for the guidelines @hiukim ! I'll work on top of your repo, for sure, so that anyone can benefit from that.I'll update here when I have something.====="", '@pedrorocha-net Did you manage to fix this functionality?=====', 'Hi, this is all new to me, I am trying to work AR.js for unity package but my issue is that it only works with Market Target, do I have to install this package instead or have both and hopefully unity rewrites everything automatically? :) I apologize if I sound very ignorant, I am very ignorant in this subject, thanks for the help.=====']"
https://github.com/hiukim/mind-ar-js/issues/23;Scanning screen does not come back after targetLost-event;2;open;2021-04-16T09:33:56Z;2021-04-21T07:24:06Z;The scanning screen provides clear gui-feedback to the user that scanning is going on. As such, it is very helpful. However, once a target-match has been found, it won't come back.Is there any way of making it show up again after a targetLost-event?;"[""@HeimMatthias Actually, I couldn't decide which one should be the default behaviour when I implemented that. Should the scanning screen only be shown the first time or not.Will it be very annoying if the scanning screen show up every time when the tracking is lost like momentarily for a second or so?Love to hear some feedback.====="", 'I think the most practical for users of the library would be to have a function with which to enable the scanning screen again.I can see that it would be annoying if the screen came back every time the target is lost for a second, but the default behaviour will be different from project to project (and maybe even from target to target).For example, in my sample project, I have targets that trigger a video to play - if I loose the target , for a fraction of a second, I clearly don\'t want the additional overhead for the scanning screen to to come back before the video continues. On the other hand, I also use targets that simply trigger a slide-in frame with additional links, and in this case I don\'t mind if the scanning screen comes back immediately, as the user may simply use the scanner to go to the next target to load a different information screen.So, while it would be good to have a boolean loading option to change the default behaviour of the screen coming back or not, in my case it would be most useful to be able to do something like this programatically:````exampleTarget.addEventListener(""targetLost"", event => {      setTimeout(function() {          if (!arSystem.targetIsBeingTrackedAtTheMoment()) arSystem.showScanningScreen(),      }, 2000),    }),````(sorry, the function names are woefully inadequate, but I think you get the idea).=====']"
https://github.com/hiukim/mind-ar-js/issues/93;Aframe particles system component does not work with MindAR? ;5;closed;2021-12-20T16:52:04Z;2021-12-20T19:26:16Z;I'm having issues getting any sort of particle system to be incorporated with MindAR, the particle do not seem to visible no matter the values I incorporate. Anyone else also getting this issue? This is the particle system I am using to get particle to appear: [https://www.npmjs.com/package/aframe-particle-system-component](https://www.npmjs.com/package/aframe-particle-system-component);"['If you\'re using aframe 1.2.0, the latest release of https://www.npmjs.com/package/aframe-particle-system-component is not compatible with the THREE version included in aframe 1.2.0.An update was merged in October 2021 in the master branch https://github.com/IdeaSpaceVR/aframe-particle-system-component that is compatible with aframe 1.2.0, but is not yet released. You can use the master build like this:```<script src=""https://cdn.jsdelivr.net/gh/IdeaSpaceVR/aframe-particle-system-component@master/dist/aframe-particle-system-component.min.js""></script>```=====', '@vincentfretin thank you for the clarificationI\'ve added `<script src=""https://cdn.jsdelivr.net/gh/IdeaSpaceVR/aframe-particle-system-component@master/dist/aframe-particle-system-component.min.js""></script>`and also incorporated the preset`        <a-entity position=""0 2.25 -15"" particle-system=""preset: snow""></a-entity>`However I\'m still unable to see any particles being emitted within my marker scene. I\'ve changed the position, size and texture. Have you incorporated particles before? If so do you have a basic example of this with MindAR marker? ```<head>  <meta charset=""utf-8"">  <meta name=""apple-mobile-web-app-capable"" content=""yes"">  <!--Need this for iOS-->  <meta name=""viewport"" content=""width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0"">  <!--Need this for Android-->  <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@1.0.0/dist/mindar-image.prod.js""></script>  <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>  <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@1.0.0/dist/mindar-image-aframe.prod.js""></script>  <script    src=""https://cdn.jsdelivr.net/gh/IdeaSpaceVR/aframe-particle-system-component@master/dist/aframe-particle-system-component.min.js""></script>  <script src=""script.js""></script>  <link rel=""stylesheet"" href=""styles.css""></head>``````<a-scene mindar-image=""uiScanning: #example-scanning-overlay, imageTargetSrc:businesscard_qrmarker_5.mind""    vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"" loading-screen=""enabled: false"">    <!--Asset Management-->    <a-assets>    </a-assets>    <!--Camera-->    <a-camera position=""0 0 0"" look-controls=""enabled: false"" cursor=""fuse: false, rayOrigin: mouse,""      raycaster=""far: 10000,objects: .clickable""></a-camera>    <!--Main Scene Area-->    <a-entity id=""marker-target"" mindar-image-target=""targetIndex: 0"">      <a-entity position=""0 2.25 -15"" particle-system=""preset: snow""></a-entity>    </a-entity>  </a-scene>```Thank you for the help regardless!=====', ""We have the snow particles on the networked-aframe exampleshttps://github.com/networked-aframe/networked-aframeDoes this work directly in `<a-scene>` ?If you don't see the snow only when you put the particle-system as a child of a marker, then I can't help you further, I never used mind-ar-js.====="", ""I'm such a idiot, thank you @vincentfretin I got it working! I put the the particle entity outside on of the marker section and it started working! ====="", ""Ok, I'm glad it works for you. I thought you wanted the snow only after having scanned the marker or something like that. If you want such thing, if there is an event emitted when a marker is scanned (I don't know the lib, so I don't know if there is one), you can probably add the particule-system dynamically with javascript.You can close the issue and mark as answered the discussion if it's ok for you.=====""]"
https://github.com/hiukim/mind-ar-js/issues/56;feature request: using mind-ar with native threejs (without a-frame);11;closed;2021-10-06T07:53:48Z;2021-12-11T09:48:59Z;Hi Hiukim,could you maybe provide an example on how to use mind-ar with just vanilla threejs. That would be awesome. Thank you;"['@ThorstenBux Yes, totally. I have been planning on doing this for a long time. Just separated aframe extension from the core in the latest release 1.0.0.Will try to write up something.=====', 'Awesome, thank you 😊Get Outlook for iOS<https://aka.ms/o0ukef>________________________________From: HiuKim Yuen ***@***.***>Sent: Thursday, October 7, 2021 7:23:33 AMTo: hiukim/mind-ar-js ***@***.***>Cc: Thorsten Bux ***@***.***>, Mention ***@***.***>Subject: Re: [hiukim/mind-ar-js] feature request: using mind-ar with native threejs (without a-frame) (#56)@ThorstenBux<https://apac01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FThorstenBux&data=04%7C01%7C%7Cc431d08afcbc423bdb6508d988f6682a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637691414180266223%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2F66LQmvjuL6ddVBd3BkaZOIge%2BwLKfUoYDrCf%2BaL1Pg%3D&reserved=0>Yes, totally. I have been planning on doing this for a long time. Just separated aframe extension from the core in the latest release 1.0.0.Will try to write up something.—You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub<https://apac01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fhiukim%2Fmind-ar-js%2Fissues%2F56%23issuecomment-936844638&data=04%7C01%7C%7Cc431d08afcbc423bdb6508d988f6682a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637691414180276180%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=2J9899fTbVeoFrVp5Y9qRvow1oiBLdJBrBYPZLsKTfw%3D&reserved=0>, or unsubscribe<https://apac01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAD765PEZUTJNDUYZSPPLJB3UFSH2LANCNFSM5FNZPKJQ&data=04%7C01%7C%7Cc431d08afcbc423bdb6508d988f6682a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637691414180286141%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=1zh0duOKsnfi83FBDTbbFc6AaaM0KGsyi6RRUoiLc0Q%3D&reserved=0>.=====', ""Hey @ThorstenBux ,I recently integrated ThreeJS + React + MindAR using a Context Provider. I did this, however, before the last major release. The code is essentially just a rewrite of the aframe wrapper in the src. Let me know if you'd like to know more - I'd be happy to run you through the process.Cheers====="", ""Hi @akhrarovsaid ,That is awesome. Yes please let me know how you did it. I got somewhere with my approach but always have an offset of the virtual content on the marker that I can't fix.Thank you 🙏 ====="", 'Hi @hiukim, have you had a chance to look into this?=====', ""@ThorstenBux Yes, I'm working on it, but probably need a bit more time.  Need to make the API right before officially releasing something, otherwise it will be hard to maintain in the long run, or break backward compatibility.====="", 'Yes true, I highly appreciate it. Let me know if you have some dev-repo or anything somewhere. Happy to contribute.=====', 'Great project! Interested in this feature as well.=====', ""Hey guys,Thanks for your patience! If you're still looking for a way to use MindAR with Three, I've made a small repo that you can look at [here](https://github.com/akhrarovsaid/react-three-mindar-demo). It uses Typescript and works with a single marker. Should be relatively straight forward to lift state or add multi-nft support. It effectively just uses useEffect hooks to init MindAR then start the experience. Let me know if you have any questions / issues. @ThorstenBux @gjscope====="", ""@akhrarovsaid, thank you very much for that. I've created a PR to your repo which fixes issues with iOS devices. Other than that it looks very promising. I'll integrate it into my use case and see how it goes. Many thanks again, I very much appreciate it.====="", '@ThorstenBux @akhrarovsaid threejs integration is added in latest release. You can check out this installation guide: https://hiukim.github.io/mind-ar-js-doc/installationand a quick example:https://hiukim.github.io/mind-ar-js-doc/more-examples/threejs-image=====']"
https://github.com/hiukim/mind-ar-js/issues/51;1.0.0 doesn't work with aframe-animation-timeline-component?;1;closed;2021-10-05T11:33:03Z;2021-10-05T12:36:20Z;I developed an AR experience using version 0.4.2 together with the aframe-animation-timeline-component. After updating to 1.0.0 everything stopped working. I don't get the loading spinner animation and no errors in the console. Did something change?;"['Discovered that the ""mindar"" attribute on the a-scene has changed name to ""mindar-image"". Maybe some upgrade information would be good?=====']"
https://github.com/hiukim/mind-ar-js/issues/49;Any way to improve fps and tracker follow speed?;7;closed;2021-09-30T06:54:48Z;2021-11-16T21:17:32Z;Hi,I'm using mind-ar with an a-video as asset with no additional overlay or interactivity. Image target specs:656x656 892KB sizeVideo asset specs:9 seconds video lengthmp4 format320x32030 fps643KB sizeThe image tracker is spot-on but somewhat slow to follow the image target and have low fps except in iphone which have a good fps.I've tested on devices:Asus Zenfone Max Pro M2 (< 15 Fps) Chrome BrowserXiaomi Redmi 8 (< 15 Fps) Chrome BrowserSamsung Galaxy A71 (< 20 Fps) Samsung Internet BrowserSamsung Galaxy A7 (< 10 Fps) Chrome BrowserSamsung Galaxy Tab 8 (< 10 Fps) Chrome BrowserApple Iphone 7 plus (< 45 Fps) Safari BrowserIs there any way to improve fps? and tracker follow speed?Thank you.;"[""I'm working on an improved version that has increased performance, hopefully can release soon.====="", 'Okay. Thank you for the awesome library, man!=====', ""> I'm working on an improved version that has increased performance, hopefully can release soon.Hi.. Could you please inform a tentative date for new version? ====="", ""> I'm working on an improved version that has increased performance, hopefully can release soon.just use another routine instead of https://github.com/hiukim/mind-ar-js/blob/ec5828889687fb8af760d45d728e35c348ba2107/src/image-target/controller.js#L239 linearInterpolation is very expensive. Using a OneEuroFilter may help a lot. :slightly_smiling_face: ====="", 'Hi @hiukim, can you give some clues on how you are improving tracker speed? Any chance that we can give some help with some of the work. Aweomse library :)=====', ""@sumyiren I'm currently not working on improving tracker speed. But definitely would love some help in this area! Do you have computer vision background?====="", ""Ah right, thought you were working on something, misread the above thread.I'm not unfortunately. But have some software eng experience, free time, and interest in this area!=====""]"
https://github.com/hiukim/mind-ar-js/issues/47;Transparent images;5;closed;2021-08-27T10:56:39Z;2021-08-29T15:47:02Z;Hey, @hiukim how do images, that are mostly transparent backgrounds, work on mind-ar? On JsArtoolkit5 the background becomes black and then there are issues tracking the image on white or bright backgrounds.;"[""@Carnaux that's an interesting question. I don't have a certain answer top of my head, coz I haven't really tried that. The first step is to convert to grayscale, so depends on the pixel value of the conversion.Is there any particular reason you want to track image with transparent background?  The image will still have some solid background later anyway depends on whether you place it?====="", '> The image will still have some solid background later anyway depends on whether you place it?Exactly that, background color independent marker, so I can use in any surface unless Mind-Ar already does that. =====', 'My recommendation would then be to crop the marker such that none of the transparency is in the cropped version. Then you track the cropped version instead and use the original (uncropped) target image as you intended. =====', ""I think @akhrarovsaid is right. that's also what I would do if cropping is possible.I know there are cases where cropping is not possible. For example, your target is in circular shape, or your transparency background is in an inside area. I think the simple answer to the original question is that the library didn't handle that. It will just turn them into a single color (maybe white or black, I'm not certain).====="", ""Yeah, that should solve and I tried that too, but in my case, most images fall back to what @hiukim said. Depending on the tracking algorithm I think gray would be better. I'm currently using black on the NFT-Marker-Creator and that is part of the issue.I have this [proposal](https://github.com/webarkit/jsartoolkit5/issues/13) on JsArtoolkit5, I will test it today and maybe it can be applied to Mind-Ar too. If you are interested I will keep you updated on the matter. Thanks, guys!=====""]"
https://github.com/hiukim/mind-ar-js/issues/37;Mesh distortion on a curved marker;2;closed;2021-06-14T09:28:46Z;2021-06-15T18:50:14Z;"Hi @hiukim, we've been trying to replicate the curved images tracking feature offered by the 8th wall with the mind ar library.We tried using a curved plane and curved image primitives offered by a-frame, but in both cases, the edges of the mesh appear to distort when the device is moved wrt the marker. <img width=""137"" alt=""Screenshot 2021-06-13 at 11 46 45 PM"" src=""https://user-images.githubusercontent.com/17825870/121868774-01095080-cd1f-11eb-8d34-047f555d123e.png"">Here a [video recording](https://drive.google.com/file/d/1U9LXllVHIqRdhMwxDMKLjAJos3yf69zq/view?usp=sharing) of the same issue.I'm curious to know your thoughts on the issue and any workaround you can suggest to overcome this? ";"[""@devhims Unfortunately, the library does not support curved markers at this point. I don't have good suggestions for this.  If you are only tracking it in a specific view (e.g. frontal view). maybe you can make a distorted image as the marker to somehow simulate a plane......====="", ""Unfortunately, we can't decide the view in advance. Anyway, thanks for the response @hiukim. I'll close this issue for now.=====""]"
https://github.com/hiukim/mind-ar-js/issues/31;what do I need to do to make object3D.visible = true after loss of target picture?;3;closed;2021-05-21T06:32:53Z;2021-06-01T18:50:47Z;"I know that I need to add some code here:```exampleTarget.addEventListener(""targetLost"", event => {          console.log(""target lost""),}),```i tried to add this:         `  exampleTarget.addEventListener(""targetLost"", event => {          arSystem.el.object3D.visible = true,          console.log(""target lost""),}),`but it didnt work ";"[""@Ebeleh It's not `arSystem.el`. Probably get the `el` of the `exampleTarget`. I don't have the code right away, but that's the idea. Also, I'm not entirely sure this approach will work properly for the current production build. For sure, something like this can be done but just that this use case wasn't being considered before and I'm not sure the behaviour is expected in the current production release.  If that doesn't work, then will need to customize the source code and rebuild the library.====="", '> @Ebeleh It\'s not `arSystem.el`. Probably get the `el` of the `exampleTarget`. I don\'t have the code right away, but that\'s the idea.> > Also, I\'m not entirely sure this approach will work properly for the current production build. For sure, something like this can be done but just that this use case wasn\'t being considered before and I\'m not sure the behaviour is expected in the current production release. If that doesn\'t work, then will need to customize the source code and rebuild the library.I added the function to mindar.prod.js : ```AFRAME.registerSystem(""mindar-system"", {... object3DSetVisible: function(value){    console.log(""value visible set""),    this.el.object3D.visible = value,}, ...```and called it: `exampleTarget.addEventListener(""targetLost"", event => { arSystem.object3DSetVisible(true), }),`but it doesn\'t work...=====', ""I'll close this issue for now.The library doesn't officially support this behaviour at the moment. However, it's possible to customize the aframe wrapper to achieve that. I have updated the doc to include a section giving a high level guide on digging deeper to access the core API: https://hiukim.github.io/mind-ar-js-doc/core-api=====""]"
https://github.com/hiukim/mind-ar-js/issues/30;Keeping the object in front of camera;5;closed;2021-05-17T09:54:04Z;2021-06-01T18:51:04Z;"Hi, I'm new to mind-ar-js and Javascript. I have to create an AR experience where after having detected the target the AR experience starts, and I want it to continue even after the target is lost. Specifically, I have a video playing, and when I lose the target the audio keeps playing but the video is no longer visible, so I want the whole a-entity to be retained on screen. This is a snippet of the code I'm using. I'll be glad for any help!```AFRAME.registerComponent('mytarget-one', {        init: function () {          this.el.addEventListener('targetFound', event => {            console.log(""target found""),              setTimeout(() => {                                  setTimeout(() => {                    showInfo(),                  }, 100),              }, 100),          }),          this.el.addEventListener('targetLost', event => {            console.log(""target lost""),            //stopInfoOne(),          }),          this.el.emit('targetFound'),        }}),``````<a-scene mindar=""imageTargetSrc: multitargets.mind, showStats: false,"" embedded color-space=""sRGB"" renderer=""colorManagement: true, physicallyCorrectLights"" vr-mode-ui=""enabled: false"" device-orientation-permission-ui=""enabled: false"">               <a-assets>...</a-assets> <a-camera position=""0 0 0"" look-controls=""enabled: false"" cursor=""fuse: false, rayOrigin: mouse,"" raycaster=""far: 10000, objects: .clickable""></a-camera>``````<a-entity id=""mytarget-one"" mytarget-one mindar-image-target=""targetIndex: 1"">          <a-video id=""paintandquest-video-link_wear"" webkit-playsinline playsinline muted autoplay width=""1"" height=""0.552"" position=""0 0 0.2""  ></a-video>          <a-image id=""paintandquest-preview-button"" class=""clickable"" src=""#paintandquest-preview"" alpha-test=""0.5"" position=""0 0 0.25"" height=""0.552"" width=""1"">          </a-image>          <a-image id=""linkedin"" class=""clickable"" src=""#icon-profile"" position=""-0.42 -0.5 0"" height=""0.15"" width=""0.15""        ></a-image>        <a-image  id=""facebook"" class=""clickable"" src=""#icon-web"" alpha-test=""0.5"" position=""-0.14 -0.5 0"" height=""0.15"" width=""0.15""        ></a-image>        <a-image  id=""web"" class=""clickable"" src=""#icon-email""  position=""0.14 -0.5 0"" height=""0.15"" width=""0.15""        ></a-image>        <a-image  id=""contact"" class=""clickable"" src=""#icon-location""  position=""0.42 -0.5 0"" height=""0.15"" width=""0.15""        ></a-image>      </a-entity>```";"['Hi @MatHeartGaming in `targetLost` event listener you should set the **object3D.visible** to `true`, in this way the object will be always visible.=====', '> Hi @MatHeartGaming in `targetLost` event listener you should set the **object3D.visible** to `true`, in this way the object will be always visible.That doesn\'t seem to work. What should I set to visible? My a-entity? Should I do`document.querySelector(""mytarget-one"").setAttribute(""visible"", true),`is that what you were referring to?Thanks in advance.=====', '> > Hi @MatHeartGaming in `targetLost` event listener you should set the **object3D.visible** to `true`, in this way the object will be always visible.> > That doesn\'t seem to work. What should I set to visible? My a-entity? Should I do> `document.querySelector(""mytarget-one"").setAttribute(""visible"", true),`> is that what you were referring to?> Thanks in advance.did you find a solution?=====', '> Hi @MatHeartGaming in `targetLost` event listener you should set the **object3D.visible** to `true`, in this way the object will be always visible.where I need to add this code? =====', ""I'll close this issue for now.The library doesn't officially support this behaviour at the moment. However, it's possible to customize the aframe wrapper to achieve that. I have updated the doc to include a section giving a high level guide on digging deeper to access the core API: https://hiukim.github.io/mind-ar-js-doc/core-api=====""]"
https://github.com/hiukim/mind-ar-js/issues/26;keep on screen;1;closed;2021-05-01T12:05:04Z;2021-05-04T07:32:35Z;its it possible to keep object on screen after losing the marker ? and the object will set it self in the middle of the screen in front of the camera ! ;"[""@Khodour Unfortunately, the library doesn't support this behaviour at this point. One option I can think of is to utilize the `targetLost` event, and then display another static object at the center.   Then when `targetFound`, you switch back.The transition might be odd, I'm not sure. =====""]"
https://github.com/hiukim/mind-ar-js/issues/25;Camera is not really switched off after triggering arSystem.pause();2;closed;2021-04-17T23:39:31Z;2021-06-01T18:52:06Z;Calling arSystem.pause(false) switches off the camera feed in the a-frame, but the camera remains active. At least on iOS, the status-light remains switched on. For user-security reasons it would be good if it could be switched off completely during pause, or at least optionally. Especially as the camera is also a drain on energy consumption.;"['@HeimMatthias Yes, `the arSystem.pause(false)` function translate to `video.pause()`, which keeps the video so the light indicator will still be on.If you want to have it completely switched off, you can call `arSystem.stop()`, which should stop the video completely. and then you can call `arSystem.start()` again. There is still a bit of extra overhead when restart, but I think should be much faster than the first time.=====', ""Thanks, I'll try that.=====""]"
https://github.com/hiukim/mind-ar-js/issues/22;multi-targets crash mobile browser + many targets are problematic to compile targets.mind;6;closed;2021-04-11T17:18:41Z;2021-12-11T03:21:53Z;Hey(first of: wonderful project, came here from ar.js and I am amazed by the ease of setup and stability in tracking).I was thinking of using mind-ar-js to AR-enrich a book. This means ~50 targets have to be recognized by the app. I set up a test with only 6 targets and immediately Safari on iPhone X and Chrome on a Galaxy A5 crash. The multi-targets example app works on both devices - but it has only two targets. I managed to get it to load on my iPad Pro, but that is not what end users would use.Is there any way of getting 10+ targets to work?Moreover, is it necessary to put all the tracking targets into the same targets.mind-file? This seems to be the bottleneck here, as  even with only 6 targets it grew to 32MB. Moreover, they are difficult to compile and it would be hard to add or remove individual targets at a later point. One .mind-file per target would seem much easier to use and address in the app.Any feedback or help is highly appreciated.Best, Matthias;"[""~50 targets is probably too much for the library to handle unless we revamp to some new algorithms. But I guess 10 is possible with some optimization. What's the resolution of the images you use? Maybe reducing it could help.One workaround for applications that require many targets is to separate them into different pages (or maybe just different .mind file as you suggested). Instead of detecting all of them at the same time, just load up the one you needed. It probably requires some UX design to guide users to select. ====="", 'I\'ve now successfully deployed a test-app with 44 targets. The crash on the mobile phones was actually because I used self-closing tags for `a-entity`. (i.e. `<a-entity id=""target-0"" mindar-image-target=""targetIndex: 0""/>` instead of `<a-entity id=""target-0"" mindar-image-target=""targetIndex: 0""></a-entity>`). This didn\'t work and meant that these tags ended up being nested inside each other in the DOM. I assume that the more powerful iPad could just handle the nested event-chain, but the others couldn\'t.For the successful test, I had 44 images at 96dpi (each at about 600x800px), which seems to be just enough for target detection, but not enough for stable target-retention. It also won\'t find targets that contain text and graphics instead of photographs. The latter works only with higher resolution target-images.Unfortunately, the *Image Targets Compiler* does not compile the image set with the higher resolution images. I\'ve found previously that only the new Chromium based Edge can successfully compile larger image sets (who would have thought - curiously, it\'s also much faster than Chrome), but results are not consistent and crashes happen (with the same images) regularly but not always.In the `console.log` I get the following message:``` shellWebGL: CONTEXT_LOST_WEBGL: loseContext: context lost3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2 Uncaught Error: Failed to link vertex and fragment shaders.    at 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2    at iR.createProgram (3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2)    at 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2    at 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2    at hD.getAndSaveBinary (3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2)    at hD.runWebGLProgram (3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2)    at hD.compileAndRun (3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2)    at 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2    at 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2    at Rr.scopedRun (3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2)(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2createProgram @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2getAndSaveBinary @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2runWebGLProgram @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2compileAndRun @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2scopedRun @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2tidy @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2so @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2_applyFilter @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2detect @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2(anonym) @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2scopedRun @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2tidy @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2so @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2i @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2onmessage @ 3c6a9f77-4ea9-46a9-bb29-ffc4645e1db5:2hiukim.github.io/:1 WebGL: CONTEXT_LOST_WEBGL: loseContext: context lost```Not sure whether this is helpful. It looks to me like a browser-timeout. Maybe I\'ll check whether inactivity in the window makes a difference.However, it would be massively easier if it was possible to just drop individual pre-compiled `*.mind`-files into the compiler and it wouldn\'t have to do the entire work again, just process them into one bigger file. Alternatively, could you give me any advice on how I could go about uncompressing mind-files and putting them together myself?At the moment, I don\'t think performance on mobile would actually be an issue. The 44-target test, with a 36MB mind-file runs smoothly on all the devices I tested. It\'s the compilation of the targets that is the problem. =====', ""That's an interesting findings! I wouldn't expect it can cope with 44 targets, although there are still some practical issues as you mentioned. I'd love to see your application, if it is publicly accessible.For manipulating the .mind file. It's not provided in the API and any published tools, but it's actually not that hard if you are up for some little hacking. You can import the .mind file and look at the content with a few lines of code:https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/src/controller.js#L67-L69https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/src/compiler.js#L75-L77It's just a few array of compiled data, which you can manipulate and then export again.https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/src/compiler.js#L59====="", ""Thanks a lot. This has been helpful too. I have, for the moment resolved the issue by manually fine-tuning my ar-target images. I've noticed that both compiler crashes and browser crashes happen mostly on overly complex images. For testing purposes I intentionally deployed this on as many border-line recognizable cases as possible.My findings for target recognition are basically as follows (writing this down in case anyone finds this useful).- Photographs are recognized immediately and even when not zoomed in. They make by far the best targets. 75dpi is enough for stable tracking.- Vector images with clear details (but no overload of text) are recognized well, but need to almost fill the frame for recognition. Surprisingly, for those who - like me came here from ar.js - this means that marker-recognition is weaker than photo-recognition. I have not attempted one obvious use-case: similar patterns with differentiating details. For some graphics that seem clearly distinguishable to the eye, but might use somewhat muted colors, the only way to make the match can be to take an actual photo of the element with a camera phone and to use this for the target instead of a digital export of the graphic.- Color does not seem to be differentiated at all (unless I am mistaken). This has obvious technical reasons. But it means that color should not be a distinguishing factor for your markers.- Book pages, even when they have clearly distinguishing aspects such as colored boxes, title elements, etc. won't be recognized, no matter what. The only way to make these match is - I've found - to only use the title as the target image. The title will then produce a match, but only when scanned at close-up with the camera phone.All in all, this works very well, as long as you're aware of the limitations. I will eventually come back here and post the result of my test. I will try to post more clearly technical issues, as I work on this. ====="", 'the feature points collected by the camera and submitting it to the back-end for matching can increase the limit on the number of targetsIs the workload heavy?=====', 'Thank your for your informations!By merging single pre-compiled "".mind"" files into a big one, I was able to bypass the CONTEXT_LOST_WEBGL error and explore multitarget limits.=====']"
https://github.com/hiukim/mind-ar-js/issues/19;Basic example doesn't display on iPhone ;18;closed;2021-03-30T10:13:56Z;2021-12-11T03:21:28Z;"Hi,When I try to see the live demo of the basic example on my iPhone (6SPlus - iOS 14.4.1),I stay on the waiting animation and if I take a look at the debug console, I got <img width=""1215"" alt=""Capture d’écran 2021-03-30 à 12 06 33"" src=""https://user-images.githubusercontent.com/10347315/112972970-1ecc0e00-9151-11eb-8079-3251804fe2f0.png"">Any idea ?";"[""Probably something went wrong when running the webgl code.  Unfortunately, it's hard for me to debug without the device.If you are interested, I can give some pointers and see if we can fix it together. But it requires you running the development build and tweak some codes and locate the errors.====="", ""Yes, I think I can help you. I'm a developper, well, not a web one rather anative application one ,o)I have basic knowledge of debugging on my iOS device (using Safari).So tell me everything !o)Le mer. 31 mars 2021 à 01:57, HiuKim Yuen ***@***.***> aécrit :> Probably something went wrong when running the webgl code. Unfortunately,> it's hard for me to debug without the device.>> If you are interested, I can give some pointers and see if we can fix it> together. But it requires you running the development build and tweak some> codes and locate the errors.>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/hiukim/mind-ar-js/issues/19#issuecomment-810653105>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ACO6GM5S26UDRECNT5MTGWDTGJQONANCNFSM42BWMCZQ>> .>====="", ""@Brun0oO Thanks a lot. It would be greatly appreciated. First of all, you need to clone the repo and start development by running `npm run watch`. It will compile a development build of the library, i.e. `./dist-dev/mindar.js`. It will also watch for file changes and recompile continuously.Then we can start debugging. I have just created a new debug example in the repo. `./examples/debug.html`. It's a minimal example that try to detect a simple image. You can run it as static html page. If everything runs successfully, you should see some console logs like this. I guess the first step is to use your desktop browser to verify you can get something like this.![Screenshot 2021-03-31 at 11 04 01 AM](https://user-images.githubusercontent.com/459126/113190201-e55ad780-9210-11eb-8be3-e424eb2ec4e6.png)After that, we can start debugging with the problematic device. We just need to locate the exact line that is causing the error. As you can see from debug.html, the entry point is `./src/controller.js`. Just a quick tip here: It's almost certain that the issue is related to the webgl code. After you dive into the codebase, you will find that there are codes that look like:```userCode:`\tvoid main() {\t  ivec2 coords = getOutputCoords(),\t  int texR = coords[0],\t  int texC = coords[1],\t  vec2 uv = (vec2(texC, texR) + halfCR) / vec2(${width}.0, ${height}.0),\t  vec4 values = ${textureMethod}(A, uv),\t  setOutput((values.r + values.g + values.b) * 255.0 / 3.0),\t}      ````Basically, these code are compiled and run in gpu. The issue is that they failed to compile. Let's try to locate the error and we'll go from there.====="", 'I got some problems when I try to call the first ""npm install"" command in the main directory...  I understand the problem is installing ""node-sass@4.5.0"" on my computer (macbook pro with macOS Catalina). Here the complete log of ""npm install"" call (I truncated the long part which, in my opinion, does not give any relevant information):```0 info it worked if it ends with ok1 verbose cli [ \'/usr/local/bin/node\', \'/usr/local/bin/npm\', \'install\' ]2 info using npm@6.5.0-next.03 info using node@v11.6.04 verbose npm-session 70cb30c857581bab5 silly install runPreinstallTopLevelLifecycles6 silly preinstall mind-ar@0.3.27 info lifecycle mind-ar@0.3.2~preinstall: mind-ar@0.3.28 silly install loadCurrentTree9 silly install readLocalPackageData10 timing stage:loadCurrentTree Completed in 11ms11 silly install loadIdealTree12 silly install cloneCurrentTreeToIdealTree13 timing stage:loadIdealTree:cloneCurrentTree Completed in 1ms14 silly install loadShrinkwrap15 timing stage:loadIdealTree:loadShrinkwrap Completed in 249ms16 silly install loadAllDepsIntoIdealTree17 timing stage:loadIdealTree:loadAllDepsIntoIdealTree Completed in 351ms18 timing stage:loadIdealTree Completed in 715ms19 silly currentTree mind-ar@0.3.220 silly idealTree mind-ar@0.3.220 silly idealTree ├── @discoveryjs/json-ext@0.5.220 silly idealTree ├── @msgpack/msgpack@1.12.220 silly idealTree ├── @npmcli/move-file@1.1.220 silly idealTree ├── @tensorflow/tfjs-backend-cpu@2.8.620 silly idealTree ├── @tensorflow/tfjs-backend-webgl@2.8.620 silly idealTree ├── @tensorflow/tfjs-converter@2.8.6...9967 silly saveTree │ └── webpack-sources@2.2.09967 silly saveTree └─┬ worker-loader@2.0.09967 silly saveTree   ├─┬ loader-utils@1.4.09967 silly saveTree   │ └── json5@1.0.19967 silly saveTree   └─┬ schema-utils@0.4.79967 silly saveTree     ├── ajv-keywords@3.4.19967 silly saveTree     └── ajv@6.12.29968 verbose stack Error: node-sass@5.0.0 postinstall: `node scripts/build.js`9968 verbose stack Exit status 19968 verbose stack     at EventEmitter.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/index.js:301:16)9968 verbose stack     at EventEmitter.emit (events.js:188:13)9968 verbose stack     at ChildProcess.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/lib/spawn.js:55:14)9968 verbose stack     at ChildProcess.emit (events.js:188:13)9968 verbose stack     at maybeClose (internal/child_process.js:978:16)9968 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)9969 verbose pkgid node-sass@5.0.09970 verbose cwd /Users/bruno/GitHub/mind-ar-js9971 verbose Darwin 19.6.09972 verbose argv ""/usr/local/bin/node"" ""/usr/local/bin/npm"" ""install""9973 verbose node v11.6.09974 verbose npm  v6.5.0-next.09975 error code ELIFECYCLE9976 error errno 19977 error node-sass@5.0.0 postinstall: `node scripts/build.js`9977 error Exit status 19978 error Failed at the node-sass@5.0.0 postinstall script.9978 error This is probably not a problem with npm. There is likely additional logging output above.9979 verbose exit [ 1, true ]```I\'ve followed the advices of this post : https://stackoverflow.com/questions/48298361/npm-install-failed-at-the-node-sass4-5-0-postinstall-script without success.  Here the paste of the console log of ""npm install node-sass@4.5.0"" (once the previous advices has been applied) :```0 ✓ bruno@BRUNO ~/GitHub/mind-ar-js $ npm i node-sass@4.5.0npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142npm WARN deprecated har-validator@5.1.5: this library is no longer supported> node-sass@4.5.0 install /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass> node scripts/install.jsDownloading binary from https://github.com/sass/node-sass/releases/download/v4.5.0/darwin-x64-67_binding.nodeCannot download ""https://github.com/sass/node-sass/releases/download/v4.5.0/darwin-x64-67_binding.node"": HTTP error 404 Not FoundHint: If github.com is not accessible in your location      try setting a proxy via HTTP_PROXY, e.g.       export HTTP_PROXY=http://example.com:1234or configure npm proxy via      npm config set proxy http://example.com:8080> node-sass@4.5.0 postinstall /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass> node scripts/build.jsBuilding: /usr/local/bin/node /Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=gyp info it worked if it ends with okgyp verb cli [ \'/usr/local/bin/node\',gyp verb cli   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js\',gyp verb cli   \'rebuild\',gyp verb cli   \'--verbose\',gyp verb cli   \'--libsass_ext=\',gyp verb cli   \'--libsass_cflags=\',gyp verb cli   \'--libsass_ldflags=\',gyp verb cli   \'--libsass_library=\' ]gyp info using node-gyp@3.8.0gyp info using node@11.6.0 | darwin | x64gyp verb command rebuild []gyp verb command clean []gyp verb clean removing ""build"" directorygyp verb command configure []gyp verb check python checking for Python executable ""python2"" in the PATHgyp verb `which` succeeded python2 /usr/local/bin/python2gyp verb check python version `/usr/local/bin/python2 -c ""import sys, print ""2.7.15gyp verb check python version .%s.%s"" % sys.version_info[:3],""` returned: %jgyp verb get node dir no --target version specified, falling back to host node version: 11.6.0gyp verb command install [ \'11.6.0\' ]gyp verb install input version string ""11.6.0""gyp verb install installing version: 11.6.0gyp verb install --ensure was passed, so won\'t reinstall if already installedgyp verb install version is already installed, need to check ""installVersion""gyp verb got ""installVersion"" 9gyp verb needs ""installVersion"" 9gyp verb install version is goodgyp verb get node dir target node version installed: 11.6.0gyp verb build dir attempting to create ""build"" dir: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/buildgyp verb build dir ""build"" dir needed to be created? /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/buildgyp verb build/config.gypi creating config filegyp verb build/config.gypi writing out config file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/build/config.gypigyp verb config.gypi checking for gypi file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/config.gypigyp verb common.gypi checking for gypi file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/common.gypigyp verb gyp gyp format was not specified, forcing ""make""gyp info spawn /usr/local/bin/python2gyp info spawn args [ \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/gyp_main.py\',gyp info spawn args   \'binding.gyp\',gyp info spawn args   \'-f\',gyp info spawn args   \'make\',gyp info spawn args   \'-I\',gyp info spawn args   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/build/config.gypi\',gyp info spawn args   \'-I\',gyp info spawn args   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/addon.gypi\',gyp info spawn args   \'-I\',gyp info spawn args   \'/Users/bruno/.node-gyp/11.6.0/include/node/common.gypi\',gyp info spawn args   \'-Dlibrary=shared_library\',gyp info spawn args   \'-Dvisibility=default\',gyp info spawn args   \'-Dnode_root_dir=/Users/bruno/.node-gyp/11.6.0\',gyp info spawn args   \'-Dnode_gyp_dir=/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp\',gyp info spawn args   \'-Dnode_lib_file=/Users/bruno/.node-gyp/11.6.0/<(target_arch)/node.lib\',gyp info spawn args   \'-Dmodule_root_dir=/Users/bruno/GitHub/mind-ar-js/node_modules/node-sass\',gyp info spawn args   \'-Dnode_engine=v8\',gyp info spawn args   \'--depth=.\',gyp info spawn args   \'--no-parallel\',gyp info spawn args   \'--generator-output\',gyp info spawn args   \'build\',gyp info spawn args   \'-Goutput_dir=.\' ]ERROR:root:code for hash md5 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type md5ERROR:root:code for hash sha1 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha1ERROR:root:code for hash sha224 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha224ERROR:root:code for hash sha256 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha256ERROR:root:code for hash sha384 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha384ERROR:root:code for hash sha512 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha512Traceback (most recent call last):  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/gyp_main.py"", line 16, in <module>    sys.exit(gyp.script_main())  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 545, in script_main    return main(sys.argv[1:])  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 538, in main    return gyp_main(args)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 514, in gyp_main    options.duplicate_basename_check)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py"", line 98, in Load    generator.CalculateVariables(default_variables, params)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py"", line 79, in CalculateVariables    import gyp.generator.xcode as xcode_generator  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/generator/xcode.py"", line 7, in <module>    import gyp.xcodeproj_file  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/xcodeproj_file.py"", line 152, in <module>    _new_sha1 = hashlib.sha1AttributeError: \'module\' object has no attribute \'sha1\'gyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/lib/configure.js:345:16)gyp ERR! stack     at ChildProcess.emit (events.js:188:13)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)gyp ERR! System Darwin 19.6.0gyp ERR! command ""/usr/local/bin/node"" ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild"" ""--verbose"" ""--libsass_ext="" ""--libsass_cflags="" ""--libsass_ldflags="" ""--libsass_library=""gyp ERR! cwd /Users/bruno/GitHub/mind-ar-js/node_modules/node-sassgyp ERR! node -v v11.6.0gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok Build failed with error code: 1npm WARN sass-loader@11.0.1 requires a peer of fibers@>= 3.1.0 but none is installed. You must install peer dependencies yourself.npm WARN sass-loader@11.0.1 requires a peer of sass@^1.3.0 but none is installed. You must install peer dependencies yourself.npm WARN worker-loader@2.0.0 requires a peer of webpack@^3.0.0 || ^4.0.0-alpha.0 || ^4.0.0 but none is installed. You must install peer dependencies yourself.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! node-sass@4.5.0 postinstall: `node scripts/build.js`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the node-sass@4.5.0 postinstall script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /Users/bruno/.npm/_logs/2021-04-02T07_07_05_352Z-debug.log```ƪ(ړײ)\u200eƪ\u200b\u200b=====', 'Hurrah! I managed to install by updating node to v15.13.0, clearing the cache and calling again ""npm install"". I continue following your instructions...=====', ""On my desktop, I got this :![Capture d’écran 2021-04-02 à 17 03 57](https://user-images.githubusercontent.com/10347315/113427599-9ca04b80-93d5-11eb-9b95-91c751fb3ceb.png)On my iPhone, something strange appears, It's difficult to debug remotely with Safari. The debug window disappears quite often in the development menu on my desktop Safari, I don't know why yet. The rare times I was able to access the remote debug, I could see that mindar.js was not loaded in this specific case...====="", '@Brun0oO We are in the right track. Your desktop log is perfect. I remember that disappearing problem happens to me before. You can try this tool for remote debug: https://remotejs.com/=====', 'Using _remotejs_, on my iOS device, I got this :<img width=""1508"" alt=""Capture d’écran 2021-04-06 à 08 48 29"" src=""https://user-images.githubusercontent.com/10347315/113670492-7d473e00-96b5-11eb-855e-6c96247f816b.png"">=====', 'New attempt with useful (perhaps!) informations :<img width=""1531"" alt=""Capture d’écran 2021-04-06 à 15 34 07"" src=""https://user-images.githubusercontent.com/10347315/113719398-dc28a980-96ed-11eb-9fb2-bfac42c7bb0a.png"">=====', '> New attempt with useful (perhaps!) informations :> <img alt=""Capture d’écran 2021-04-06 à 15 34 07"" width=""1531"" src=""https://user-images.githubusercontent.com/10347315/113719398-dc28a980-96ed-11eb-9fb2-bfac42c7bb0a.png"">Just want to double check. Your iOS safari works on the example, right?  =====', ""Oups!, it works now, I don't understand what i did different!====="", 'Does it work on debug.html or the basic example? =====', 'I do not understand anything : now it doesn\'t work anymore on debug.htmlI tried on example1.html without success, here the screenshots :![IMG_D666F990FF79-1](https://user-images.githubusercontent.com/10347315/113781795-870e8700-9731-11eb-8dc5-a2c1196dbacd.jpeg)<img width=""1531"" alt=""Capture d’écran 2021-04-06 à 23 36 30"" src=""https://user-images.githubusercontent.com/10347315/113781855-9beb1a80-9731-11eb-9ed2-a9a1a6f2f6df.png"">I noticed I have the same error as for displaying debug.htmlbut I didn\'t dream (at least I think), I (only) saw the debug.html page with the camera display once.What is the expected behaviour of the debug.html page, should I see the camera if all goes well?=====', ""You should see an empty page in debug.html.  Just the console.log. Anyway, I think we were expecting error, so that's fine.Can you quickly try one thing? In the `src/image-target/input-loader.js` make it use the old method `loadInput` methodBasically, just rename `_loadInput` to `loadInput` and `loadInput` to `_loadInput`https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/src/image-target/input-loader.js#L29-L48====="", 'I\'ve reversed _loadInput and loadInput.I took the opportunity to add some traces (logs like "">0"", "">1"", ...)  :```javascript...class InputLoader {  constructor(width, height) {    console.log("">0""),    this.width = width,    this.height = height,    this.texShape = [height, width],    const context = document.createElement(\'canvas\').getContext(\'2d\'),    context.canvas.width = width,    context.canvas.height = height,    this.context = context,    this.program = this.buildProgram(width, height),    const backend = tf.backend(),    console.log("">1""),    console.log(backend),    this.tempPixelHandle = backend.makeTensorInfo(this.texShape, \'int32\'),    // warning!!!    // usage type should be TextureUsage.PIXELS, but tfjs didn\'t export this enum type, so we hard-coded 2 here     //   i.e. backend.texData.get(tempPixelHandle.dataId).usage = TextureUsage.PIXELS,    console.log("">2""),    backend.texData.get(this.tempPixelHandle.dataId).usage = 2,    console.log("">3""),  }  // old method  loadInput(input) {    console.log("">4""),    return tf.tidy(() => {      let inputImage = tf.browser.fromPixels(input),      inputImage = inputImage.mean(2),      return inputImage,    }),  }  // input is instance of HTMLVideoElement or HTMLImageElement  _loadInput(input) {    this.context.drawImage(input, 0, 0, this.width, this.height),    const backend = tf.backend(),    backend.gpgpu.uploadPixelDataToTexture(backend.getTexture(this.tempPixelHandle.dataId), this.context.canvas),    const res = backend.compileAndRun(this.program, [this.tempPixelHandle], \'float32\'),    //backend.disposeData(tempPixelHandle.dataId),    return res,  } ...```Here the results :<img width=""1487"" alt=""Capture d’écran 2021-04-07 à 10 33 12"" src=""https://user-images.githubusercontent.com/10347315/113836117-e483f180-978c-11eb-95f6-1d18465f7fe6.png"">I\'ve tried to inspect the backend variable but I did not succeed :* Using console.log(JSON.stringify(backend, null, 2), I got a ""JSON.stringify cannot serialize cyclic structures."" message.* Using console.dir(backend), I got ... nothing !It is as if the tensorflow backend was not yet initialized when the InputLoader constructor was called... Do we have to wait until the tensorflow engine is ready?I noticed that the loadInput function was not called when the exception is raised.=====', 'ah.. let\'s try comment out these part of code as well. we don\'t need them for the old loadInput ```/*this.program = this.buildProgram(width, height),    const backend = tf.backend(),    console.log("">1""),    console.log(backend),    this.tempPixelHandle = backend.makeTensorInfo(this.texShape, \'int32\'),    // warning!!!    // usage type should be TextureUsage.PIXELS, but tfjs didn\'t export this enum type, so we hard-coded 2 here     //   i.e. backend.texData.get(tempPixelHandle.dataId).usage = TextureUsage.PIXELS,    console.log("">2""),    backend.texData.get(this.tempPixelHandle.dataId).usage = 2,    console.log("">3""),*/```=====', 'We are going forward ,o) now, loadInput is called, new screenshot :<img width=""1531"" alt=""Capture d’écran 2021-04-07 à 20 11 44"" src=""https://user-images.githubusercontent.com/10347315/113914234-9ea34980-97dd-11eb-93d3-b74caf62fbbe.png"">=====', 'seems like the tensorflow webgl backend has failed to initialize, which is also indicated in the Warning. Probably need to dig deeper into that.=====']"
https://github.com/hiukim/mind-ar-js/issues/18;Compile Target Images;29;closed;2021-03-26T13:59:22Z;2021-09-03T09:41:11Z;Is there any way to compile images with NodeJS? Just to run command in console or whatever ;"[""Unfortunately, it can't be done at the moment. Part of the code is currently run in webgl, which requires a browser environment. ====="", ""Hi @hiukim! How i can compile the target image on my own server?I can't found an example of that.Thanks in advance!====="", ""@ultranano Unfortunately, it can't be done at the moment.Part of the code is currently run in webgl, which requires a browser environment.====="", 'I mean... I want to compile image target reference without pass to your site but use my own domain. It’s possible? How I can setup this?=====', ""There is an old (not maintained anymore) example here: https://github.com/hiukim/mind-ar-js/blob/master/examples/compile.html. But I think it's still working.The most important function is this one:https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/examples/compile.html#L90====="", 'thanks! i\'ll try today.BTW why there isn\'t a new example of compile target form like in the documentation?Tha compile form example work only with ""dist-dev"" or I can use also ""dist""?=====', 'I think you can use either `dist-dev` or `dist`=====', ""I just wasn't expecting there is a need to host a custom target compiler. What's the use case of it?I guess I can tidy that example up and put it in the doc.  ====="", ""> I just wasn't expecting there is a need to host a custom target compiler. What's the use case of it?> > I guess I can tidy that example up and put it in the doc.maybe u can give code for the image compiler? or the link where u got it====="", '@hiukim if you want we can develop a target compiler to host on our Netlify org account [webarkit](https://github.com/webarkit)=====', ""@Ebeleh @kalwalt The code is already in the repo. It's mainly the `./src/compiler.js`There is an example under `./examples/compile.html`. The code should still be working, but just that the UI might not be well polished. but shouldn't be too difficult to understand.I think the core function call is here:`https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/examples/compile.html#L96`Yes, feel free to develop your target compiler if you want.====="", ""> @Ebeleh @kalwalt> > The code is already in the repo. It's mainly the `./src/compiler.js`> > There is an example under `./examples/compile.html`. The code should still be working, but just that the UI might not be well polished. but shouldn't be too difficult to understand.> > I think the core function call is here:> `https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/examples/compile.html#L96`> > Yes, feel free to develop your target compiler if you want.thanks====="", 'Hi Hiukim,Thanks for the compiler https://github.com/hiukim/mind-ar-js/blob/master/examples/compile.html. About this line: <script src=""../dist-dev/mindar.js""></script>Since I cannot find dist-dev, I am referring the following from dist:1. mindar.prod.js2. mindar.prod-min.jsBut it is not working as expected. At least I cannot upload any image to the page or start the process (I\'m not bothered about the UI but the basic functionality seems not working).Can you please help?![image](https://user-images.githubusercontent.com/6933077/126062072-0054a092-65de-49c4-82ba-c33ca4c65f1f.png)=====', ""run `npm run watch` to build `dist-dev/mindar.js`  but mindar.prod.js is also okaywhat's the error?====="", ""The div for uploading image to start the conversion is not working.In the above screenshot, the div is apparently non-responsive to upload any image.I'm not the expert apparently - am I missing anything?====="", 'Hi,Any suggestion on this?=====', 'need to see the error.  Do you know how to open the console log?=====', 'Sure, I am testing it in here, you can too see the code and probably find the root cause: https://nft-webui.glitch.me![image](https://user-images.githubusercontent.com/6933077/126492243-8cc0f3ab-2518-49aa-848f-6755ce662c40.png)There are 2 things that could possibly create issue:1. Line 13: const compiler = new MINDAR.Compiler(),    **Uncaught ReferenceError: MINDAR is not defined** --> I\'m not sure how to import the **MINDAR.compiler()** since it is not getting imported from the following JS:    - mindar.prod.js    - mindar.prod-min.js    - compiler.js    - compiler.worker.prod.js    - compiler.worker.prod-min.js    2. I added a few lines of HTML tags inside ""div id=""dropzone"" class=""dropzone""></div>""   to make sure image can be uploaded and visible. But clicking on \'Start\' button not yielding any action     I added an alert inside JS function:     document.getElementById(""startButton"").addEventListener(""click"", function() {          alert(\'Hi\'),     ...}     this alert is not showing on click. Probably it is related to ""myDropzone"" files which it cannot refer to.Looking forward for your suggestions here.   ![image](https://user-images.githubusercontent.com/6933077/126492747-53893491-1b07-44a7-a4d3-932e19ed604b.png)=====', 'You only need to include `mindar.prod.js`. can remove others=====', 'Followed it. But still the console throws error at [ const compiler = new MINDAR.Compiler(), ] lineAfter uploading image, the start button does not respond - even after waiting for 10 min - download button does not yield .mind filePlease see the code in here: https://glitch.com/edit/#!/nft-webui=====', 'can you double check this link exists `https://nft-webui.glitch.me/mindar.prod.js`=====', 'You are absolutely right - the JS was not functioning due to Glitch auto-formattting file structure. After editing, it is working like a charm now. Thank you so much :-)Sorry for all the inconvenience though...=====', 'nice!=====', 'One more question: what is the basis of putting a scale of the compiled image between Scale 1 to 10?=====', 'Hi @hiukim Hope you are doing well.I have two questions for Mind AR:1. Can we interact with the 3D GLTF in here like rotate, zoom in/out etc.? (It can be done using AR.js for marker-based AR - using gesture-handler tag. But that tag does not work in here). Any suggestion how to make it work?2. Can we put audio as an asset-item and play it (onclick/autoplay) during AR show? (AR.js can do it actually).Since it is also based on AR.js - can you not enable/add the functionality in the latest builds?=====', ""@Swapratim It's not based on AR.js. It's two entirely different engine from ground up.Even said that, I think those two are totally possible. Unfortunately, I haven't tried any of these and I don't have a working example to show you.====="", '@hiukim Thanks. It totally makes sense.Do you have any timeline when it can be incorporated in Mind AR?=====', ""@Swapratim sorry, I don't have a timeline for that at the moment. When I have some free time, I could try it out.====="", '@hiukim I would really appreciate that. Thanks for the update.=====']"
https://github.com/hiukim/mind-ar-js/issues/16; problem playing a video;10;closed;2021-03-22T05:11:04Z;2021-04-10T19:25:43Z;"Hello, I am new to programming and augmented reality, I am studying your code a bit, but I find the problem that the video starts when loading the page and not when recognizing the augmented reality image, some example or advice where I modify this thank you very much.`<html>  <head>    <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />    <meta name=""apple-mobile-web-app-capable"" content=""yes"" />    <script src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@0.3.2/dist/mindar.prod.js""></script>    <script>      const showInfo = () => {        let y = 0,        const profileButton = document.querySelector(""#profile-button""),        const webButton = document.querySelector(""#web-button""),        const emailButton = document.querySelector(""#email-button""),        const locationButton = document.querySelector(""#location-button""),        const text = document.querySelector(""#text""),        profileButton.setAttribute(""visible"", true),        setTimeout(() => {          webButton.setAttribute(""visible"", true),        }, 300),        setTimeout(() => {          emailButton.setAttribute(""visible"", true),        }, 600),        setTimeout(() => {          locationButton.setAttribute(""visible"", true),        }, 900),        let currentTab = """",        webButton.addEventListener(""click"", function(evt) {          text.setAttribute(""value"", ""https://softmind.tech""),          currentTab = ""web"",        }),        emailButton.addEventListener(""click"", function(evt) {          text.setAttribute(""value"", ""hello@softmind.tech""),          currentTab = ""email"",        }),        profileButton.addEventListener(""click"", function(evt) {          text.setAttribute(""value"", ""AR, VR solutions and consultation""),          currentTab = ""profile"",        }),        locationButton.addEventListener(""click"", function(evt) {          console.log(""loc""),          text.setAttribute(""value"", ""Vancouver, Canada | Hong Kong""),          currentTab = ""location"",        }),        text.addEventListener(""click"", function(evt) {          if (currentTab === ""web"") {            window.location.href = ""https://softmind.tech"",          }        }),      },      const showPortfolio = done => {        const portfolio = document.querySelector(""#portfolio-panel""),        const portfolioLeftButton = document.querySelector(          ""#portfolio-left-button""        ),        const portfolioRightButton = document.querySelector(          ""#portfolio-right-button""        ),        const paintandquestPreviewButton = document.querySelector(          ""#paintandquest-preview-button""        ),        let y = 0,        let currentItem = 0,        portfolio.setAttribute(""visible"", true),        const showPortfolioItem = item => {          for (let i = 0, i <= 2, i++) {            document              .querySelector(""#portfolio-item"" + i)              .setAttribute(""visible"", i === item),          }        },        const id = setInterval(() => {          y += 0.008,          if (y >= 0.6) {            clearInterval(id),            portfolioLeftButton.setAttribute(""visible"", true),            portfolioRightButton.setAttribute(""visible"", true),            portfolioLeftButton.addEventListener(""click"", () => {              currentItem = (currentItem + 1) % 3,              showPortfolioItem(currentItem),            }),            portfolioRightButton.addEventListener(""click"", () => {              currentItem = (currentItem - 1 + 3) % 3,              showPortfolioItem(currentItem),            }),            paintandquestPreviewButton.addEventListener(""click"", () => {              paintandquestPreviewButton.setAttribute(""visible"", false),              const testVideo = document.createElement(""video""),              const canplayWebm = testVideo.canPlayType(                'video/webm, codecs=""vp8, vorbis""'              ),              if (canplayWebm == """") {                document                  .querySelector(""#paintandquest-video-link"")                  .setAttribute(""src"", ""#paintandquest-video-mp4""),                document.querySelector(""#paintandquest-video-mp4"").play(),              } else {                document                  .querySelector(""#paintandquest-video-link"")                  .setAttribute(""src"", ""#paintandquest-video-webm""),                document.querySelector(""#paintandquest-video-webm"").play(),              }            }),            setTimeout(() => {              done(),            }, 500),          }          portfolio.setAttribute(""position"", ""0 "" + y + "" -0.01""),        }, 10),      },      const showAvatar = onDone => {        const avatar = document.querySelector(""#avatar""),        let z = -0.3,        const id = setInterval(() => {          z += 0.008,          if (z >= 0.3) {            clearInterval(id),            onDone(),          }          avatar.setAttribute(""position"", ""0 -0.25 "" + z),        }, 10),      },      AFRAME.registerComponent(""mytarget"", {        init: function() {          this.el.addEventListener(""targetFound"", event => {            console.log(""target found""),            showAvatar(() => {              setTimeout(() => {                showPortfolio(() => {                  setTimeout(() => {                    showInfo(),                  }, 300),                }),              }, 300),            }),          }),          this.el.addEventListener(""targetLost"", event => {            console.log(""target found""),          }),          //this.el.emit('targetFound'),        }      }),    </script>    <style>      body {        margin: 0,      }      .example-container {        overflow: hidden,        position: absolute,        width: 100%,        height: 100%,      }      #example-scanning-overlay {        display: flex,        align-items: center,        justify-content: center,        position: absolute,        left: 0,        right: 0,        top: 0,        bottom: 0,        background: transparent,        z-index: 2,      }      @media (min-aspect-ratio: 1/1) {        #example-scanning-overlay .inner {          width: 50vh,          height: 50vh,        }      }      @media (max-aspect-ratio: 1/1) {        #example-scanning-overlay .inner {          width: 80vw,          height: 80vw,        }      }      #example-scanning-overlay .inner {        display: flex,        align-items: center,        justify-content: center,        position: relative,        background: linear-gradient(to right, white 10px, transparent 10px) 0 0,          linear-gradient(to right, white 10px, transparent 10px) 0 100%,            linear-gradient(to left, white 10px, transparent 10px) 100% 0,              linear-gradient(to left, white 10px, transparent 10px) 100% 100%,                linear-gradient(to bottom, white 10px, transparent 10px) 0 0,                  linear-gradient(to bottom, white 10px, transparent 10px) 100%                      0,                    linear-gradient(to top, white 10px, transparent 10px) 0 100%,                      linear-gradient(to top, white 10px, transparent 10px) 100%                        100%,        background-repeat: no-repeat,        background-size: 40px 40px,      }      #example-scanning-overlay.hidden {        display: none,      }      #example-scanning-overlay img {        opacity: 0.6,        width: 90%,        align-self: center,      }      #example-scanning-overlay .inner .scanline {        position: absolute,        width: 100%,        height: 10px,        background: white,        animation: move 2s linear infinite,      }      @keyframes move {        0%,        100% {          top: 0%,        }        50% {          top: calc(100% - 10px),        }      }    </style>  </head>  <body>    <div class=""example-container"">      <div id=""example-scanning-overlay"" class=""hidden"">        <div class=""inner"">          <img src=""./assets/card-example/card.png"" />          <div class=""scanline""></div>        </div>      </div>      <a-scene        mindar=""imageTargetSrc: https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@0.3.1/examples/assets/card-example/card.mind, showStats: false, uiScanning: #example-scanning-overlay,""        embedded        color-space=""sRGB""        renderer=""colorManagement: true, physicallyCorrectLights""        vr-mode-ui=""enabled: false""        device-orientation-permission-ui=""enabled: false""      >        <a-assets>          <img id=""card"" src=""./assets/card-example/card.png"" />          <img id=""icon-web"" src=""./assets/card-example/icons/web.png"" />          <img            id=""icon-location""            src=""./assets/card-example/icons/location.png""          />          <img            id=""icon-profile""            src=""./assets/card-example/icons/profile.png""          />          <img id=""icon-phone"" src=""./assets/card-example/icons/phone.png"" />          <img id=""icon-email"" src=""./assets/card-example/icons/email.png"" />          <img            id=""icon-play""            src=""https://cdn.glitch.com/b38eb9a2-8d3c-4e64-998d-6d0738b4c845%2Fplay.png?v=1616294545285""          />          <img id=""icon-left"" src=""./assets/card-example/icons/left.png"" />          <img id=""icon-right"" src=""./assets/card-example/icons/right.png"" />          <img            id=""paintandquest-preview""            src=""./assets/card-example/portfolio/paintandquest-preview.png""          />          <video            id=""paintandquest-video-mp4""            autoplay=""false""            loop=""true""            src=""https://cdn.glitch.com/d854003b-b32d-455a-98db-95fe418cab4c%2Fpaintandquest.mp4?v=1616389137177""          ></video>          <video            id=""paintandquest-video-webm""            autoplay=""false""            loop=""true""            src=""https://cdn.glitch.com/d854003b-b32d-455a-98db-95fe418cab4c%2Fpaintandquest.webm?v=1616389074156""          ></video>          <img            id=""coffeemachine-preview""            src=""./assets/card-example/portfolio/coffeemachine-preview.png""          />          <img            id=""peak-preview""            src=""./assets/card-example/portfolio/peak-preview.png""          />          <a-asset-item            id=""avatarModel""            src=""https://cdn.jsdelivr.net/gh/hiukim/mind-ar-js@0.3.1/examples/assets/card-example/softmind/scene.gltf""          ></a-asset-item>        </a-assets>        <a-camera          position=""0 0 0""          look-controls=""enabled: false""          cursor=""fuse: false, rayOrigin: mouse,""          raycaster=""far: 10000, objects: .clickable""        >        </a-camera>        <a-entity id=""mytarget"" mytarget mindar-image-target=""targetIndex: 0"">          <a-plane            src=""#card""            position=""0 0 0""            height=""0.552""            width=""1""            rotation=""0 0 0""          ></a-plane>          <a-entity visible=""false"" id=""portfolio-panel"" position=""0 0 -0.01"">            <a-text              value=""Portfolio""              color=""black""              align=""center""              width=""2""              position=""0 0.4 0""            ></a-text>            <a-entity id=""portfolio-item0"">              <a-video                id=""paintandquest-video-link""                webkit-playsinline                playsinline                width=""1""                height=""0.552""                position=""0 0 0""              ></a-video>              <a-image                id=""paintandquest-preview-button""                class=""clickable""                src=""#paintandquest-preview""                alpha-test=""0.5""                position=""0 0 0""                height=""0.552""                width=""1""              >              </a-image>            </a-entity>            <a-entity id=""portfolio-item1"" visible=""false"">              <a-image                class=""clickable""                src=""#coffeemachine-preview""                alpha-test=""0.5""                position=""0 0 0""                height=""0.552""                width=""1""              >              </a-image>            </a-entity>            <a-entity id=""portfolio-item2"" visible=""false"">              <a-image                class=""clickable""                src=""#peak-preview""                alpha-test=""0.5""                position=""0 0 0""                height=""0.552""                width=""1""              >              </a-image>            </a-entity>            <a-image              visible=""false""              id=""portfolio-left-button""              class=""clickable""              src=""#icon-left""              position=""-0.7 0 0""              height=""0.15""              width=""0.15""            ></a-image>            <a-image              visible=""false""              id=""portfolio-right-button""              class=""clickable""              src=""#icon-right""              position=""0.7 0 0""              height=""0.15""              width=""0.15""            ></a-image>          </a-entity>          <a-image            visible=""false""            id=""profile-button""            class=""clickable""            src=""#icon-profile""            position=""-0.42 -0.5 0""            height=""0.15""            width=""0.15""            animation=""property: scale, to: 1.2 1.2 1.2, dur: 1000, easing: easeInOutQuad, loop: true, dir: alternate""          ></a-image>          <a-image            visible=""false""            id=""web-button""            class=""clickable""            src=""#icon-web""            alpha-test=""0.5""            position=""-0.14 -0.5 0""            height=""0.15""            width=""0.15""            animation=""property: scale, to: 1.2 1.2 1.2, dur: 1000, easing: easeInOutQuad, loop: true, dir: alternate""          ></a-image>          <a-image            visible=""false""            id=""email-button""            class=""clickable""            src=""#icon-email""            position=""0.14 -0.5 0""            height=""0.15""            width=""0.15""            animation=""property: scale, to: 1.2 1.2 1.2, dur: 1000, easing: easeInOutQuad, loop: true, dir: alternate""          ></a-image>          <a-image            visible=""false""            id=""location-button""            class=""clickable""            src=""#icon-location""            position=""0.42 -0.5 0""            height=""0.15""            width=""0.15""            animation=""property: scale, to: 1.2 1.2 1.2, dur: 1000, easing: easeInOutQuad, loop: true, dir: alternate""          ></a-image>          <a-gltf-model            id=""avatar""            rotation=""0 0 0""            position=""0 -0.25 0""            scale=""0.004 0.004 0.004""            src=""#avatarModel""          ></a-gltf-model>          <a-text            id=""text""            class=""clickable""            value=""""            color=""black""            align=""center""            width=""2""            position=""0 -1 0""            geometry=""primitive:plane, height: 0.1, width: 2,""            material=""opacity: 0.5""          ></a-text>        </a-entity>      </a-scene>    </div>  </body></html>`";"[""@jhonrymat I'm not sure I understand what you mean. Can you provide screencaps?====="", 'just take the example of advanced.html and change the link of the video, for a video that has embedded audio, but when the page is opened the audio is played instantly without having clicked on the video=====', '@jhonrymat Can you perhaps create a minimal and runnable example?  That would help a lot.Better off, are you able to confirm the problem is caused by MindAR.  One good way to do that is remove MindAR and create a regular AFRAME scene with video, and make the ""click and play"" works.After that, put in the MindAR library, and see what\'s the difference. Then we will know what\'s causing the problem.=====', 'here an example in glitch.com[https://mighty-arrow-pyrite.glitch.me/](url)https://cdn.glitch.com/b38eb9a2-8d3c-4e64-998d-6d0738b4c845%2Fcard.png?v=1616288422890and here the codehttps://glitch.com/edit/#!/mighty-arrow-pyrite?path=index.html%3A1%3A0=====', ""I run your example, and I couldn't see any video.  So there is also no audio as well. ====="", ""Hello again, could you please explain how to play a video I still can't find how, and then proceed to close the thread, thank you very much====="", ""Now, I'm a bit confused about the problem you are having. Are you1) no able to play video, or 2) can play video, but the video started playing incorrectlyif 1)The interactive example you mentioned already provided an example of how to start a video. if 2)Can you provide a minimal and runnable example that I can replicate your issue. ====="", ""You just want to put a video on top of an image but I don't know how to break down your example, just to use the video, I appreciate your help. I tried this way and I did not succeed.https://glitch.com/edit/#!/mighty-arrow-pyrite?path=index.html%3A1%3A0====="", '@jhonrymat Your video doesn\'t play because the click event wasn\'t fired/captured. If you need to capture click event on aframe element, you need to setup the raycaster in camera, e.g. ``` <a-camera cursor=""fuse: false, rayOrigin: mouse,"" raycaster=""near: 10, far: 10000, objects: .clickable""></a-camera>        <a-cursor></a-cursor>      </a-camera>``````<a-video class=""clickable"" src=""#surfer"" height=""4.5"" width=""8"" position=""0 3 -9""        ><a-image          id=""videocontrols""          src=""#play""          position=""0 -3 0""          scale=""0.5 0.5 1""          play-pause        ></a-image>      </a-video>```=====', 'Excellent, thank you very much, it was what I needed, you are the best. I close thread.=====']"
https://github.com/hiukim/mind-ar-js/issues/13;image compiler tool not working;3;closed;2021-03-07T18:55:19Z;2021-03-07T19:17:17Z;Hi @hiukim, I've tried a couple of images before trying out the default `card.png` that is provided with the repo. In all the cases, the generated `.mind` file isn't getting recognized. I also noticed `2 KB` size difference in the `card.mind` that's included in the example and the one I generate with the same image. Here is one of my images: ![ww](https://user-images.githubusercontent.com/17825870/110251018-7dfc8f80-7fa4-11eb-9155-32ab9526027a.jpg);['@devhims Are you using the latest version of the library? Both for compile and recognize.  I just pushed an update yesterday. =====', 'Thanks for the prompt reply @hiukim. Yes, I was using the older version of the library. All sorted now. Loving the new UX upgrades.  ✌️=====', 'Nice!=====']
https://github.com/hiukim/mind-ar-js/issues/10;Issue trying new tensorflow version;7;closed;2021-01-17T20:04:48Z;2021-12-11T03:21:07Z;"I tested the new version with tensorflow, i get this error:```javascriptA-Frame Version: 1.0.4 (Date 2020-02-05, Commit #2b359246)mindar.prod.js:12858 three Version (https://github.com/supermedium/three.js): ^0.111.6mindar.prod.js:12858 WebVR Polyfill Version: ^0.10.10mindar.prod.js:12680 THREE.WebGLRenderer: WEBGL_depth_texture extension not supported.get @ mindar.prod.js:12680mindar.prod.js:12680 THREE.WebGLRenderer: OES_texture_float_linear extension not supported.get @ mindar.prod.js:12680mindar.prod.js:12658 video ready... <video autoplay muted playsinline style=​""position:​ absolute,​ top:​ 0px,​ left:​ -32px,​ z-index:​ -2,​ width:​ 384px,​ height:​ 512px,​"" width=​""480"" height=​""640"">​</video>​mindar.prod.js:10332 Could not get context for WebGL version 2mindar.prod.js:10348 1    2        precision highp float,3        precision highp int,4        precision highp sampler2D,5        varying vec2 resultUV,6        7        const vec2 halfCR = vec2(0.5, 0.5),8    9        struct ivec510       {11         int x,12         int y,13         int z,14         int w,15         int u,16       },mindar.prod.js:10348 Fragment shader compilation failed.mindar.prod.js:10348  17                                                                                                      mindar.prod.js:10348 18       struct ivec619       {20         int x,21         int y,22         int z,23         int w,24         int u,25         int v,26       },27   28       uniform float NAN,29       30         #define isnan(value) isnan_custom(value)31         bool isnan_custom(float val) {32           return (val > 0. || val < 1. || val == 0.) ? false : true,33         }34         bvec4 isnan_custom(vec4 val) {35           return bvec4(isnan(val.x), isnan(val.y), isnan(val.z), isnan(val.w)),36         }37       38       39         uniform float INFINITY,40   41         bool isinf(float val) {42           return abs(val) == INFINITY,43         }44         bvec4 isinf(vec4 val) {45           return equal(abs(val), vec4(INFINITY)),46         }47       48       49         int round(float value) {50           return int(floor(value + 0.5)),51         }52   53         ivec4 round(vec4 value) {54           return ivec4(floor(value + vec4(0.5))),55         }56       57   58       int imod(int x, int y) {59         return x - y * (x / y),60       }61   62       int idiv(int a, int b, float sign) {63         int res = a / b,64         int mod = imod(a, b),65         if (sign < 0. && mod != 0) {66           res -= 1,67         }68         return res,69       }70   71       //Based on the work of Dave Hoskins72       //https://www.shadertoy.com/view/4djSRW73       #define HASHSCALE1 443.897574       float random(float seed){75         vec2 p = resultUV * seed,76         vec3 p3  = fract(vec3(p.xyx) * HASHSCALE1),77         p3 += dot(p3, p3.yzx + 19.19),78         return fract((p3.x + p3.y) * p3.z),79       }80   81       82   vec2 uvFromFlat(int texNumR, int texNumC, int index) {83     int texR = index / texNumC,84     int texC = index - texR * texNumC,85     return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),86   }87   vec2 packedUVfrom1D(int texNumR, int texNumC, int index) {88     int texelIndex = index / 2,89     int texR = texelIndex / texNumC,90     int texC = texelIndex - texR * texNumC,91     return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),92   }93   94       95   vec2 packedUVfrom2D(int texelsInLogicalRow, int texNumR,96     int texNumC, int row, int col) {97     int texelIndex = (row / 2) * texelsInLogicalRow + (col / 2),98     int texR = texelIndex / texNumC,99     int texC = texelIndex - texR * texNumC,100    return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),101  }102  103      104  vec2 packedUVfrom3D(int texNumR, int texNumC,105      int texelsInBatch, int texelsInLogicalRow, int b,106      int row, int col) {107    int index = b * texelsInBatch + (row / 2) * texelsInLogicalRow + (col / 2),108    int texR = index / texNumC,109    int texC = index - texR * texNumC,110    return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR),111  }112  113    114  115      float sampleTexture(sampler2D textureSampler, vec2 uv) {116        return texture2D(textureSampler, uv).r,117      }118    119  120      void setOutput(vec4 val) {121        gl_FragColor = val,122      }123    124  uniform sampler2D A,125  uniform int offsetA,126  127      ivec3 getOutputCoords() {128        ivec2 resTexRC = ivec2(resultUV.yx *129                               vec2(160, 160)),130        int index = resTexRC.x * 160 + resTexRC.y,131  132        int b = index / 25500,133        index -= b * 25500,134  135        int r = 2 * (index / 510),136        int c = imod(index, 510) * 2,137  138        return ivec3(b, r, c),139      }140    141  142          143      float getA(int row, int col) {144        vec2 uv = (vec2(col, row) + halfCR) / vec2(1020.0, 100.0),145        return sampleTexture(A, uv),146      }147    148          float getA(int row, int col, int depth) {149            return getA(col, depth),150          }151        152      float getAAtOutCoords() {153        ivec3 coords = getOutputCoords(),154        155        return getA(coords.x, coords.y, coords.z),156      }157    158  159        ivec3 outCoordsFromFlatIndex(int index) {160          int r = index / 102000, index -= r * 102000,int c = index / 1020, int d = index - c * 1020,161          return ivec3(r, c, d),162        }163  164        void main() {165          ivec2 resTexRC = ivec2(resultUV.yx *166            vec2(160, 160)),167          int index = 4 * (resTexRC.x * 160 + resTexRC.y),168  169          vec4 result = vec4(0.),170  171          for (int i=0, i<4, i++) {172            int flatIndex = index + i,173            ivec3 rc = outCoordsFromFlatIndex(flatIndex),174            result[i] = getA(rc.x, rc.y, rc.z),175          }176  177          gl_FragColor = result,178        }179      mindar.prod.js:10348 Uncaught (in promise) Error: Failed to compile fragment shader.    at mf (mindar.prod.js:10348)    at dg.createProgram (mindar.prod.js:10911)    at mindar.prod.js:11440    at mindar.prod.js:11440    at vb.getAndSaveBinary (mindar.prod.js:11440)    at vb.runWebGLProgram (mindar.prod.js:11440)    at vb.decode (mindar.prod.js:11440)    at vb.getValuesFromTexture (mindar.prod.js:11440)    at vb.readSync (mindar.prod.js:11440)    at g.readSync (mindar.prod.js:4141)```Tested with a Wiko View, with Android 7.1.2. Chrome 87.0.4280.141.";"[""@kalwalt Sorry, I still couldn't replicate this error.Maybe can you try running any tensorflowjs example first? https://www.tensorflow.org/js/demos====="", ""> @kalwalt Sorry, I still couldn't replicate this error.> > Maybe can you try running any tensorflowjs example first? https://www.tensorflow.org/js/demosi will try again also with other devices and will test the examples you suggested :slightly_smiling_face: ====="", 'Thanks a lot! appreciated! =====', '@kalwalt Just bumped the tfjs version from 2.x to 3.6. Not sure it helps, but worth a try.=====', '> @kalwalt Just bumped the tfjs version from 2.x to 3.6. Not sure it helps, but worth a try.Thank you for the news! I will test It.=====', '@hiukim with my new Oppo A72 works fine all the examples! i will test also with my oldest device.=====', '@kalwalt good news! thanks for testing.=====']"
https://github.com/hiukim/mind-ar-js/issues/8;Replace gpu.js with tensorflowjs;1;closed;2020-12-11T13:25:52Z;2021-03-17T04:41:12Z;In view of having lots of unknown bugs from gpu.js, I'm trying to replace it with another library for the webgl part.I came up with this interesting idea of using tensorflow while discussing with my colleague.  There is a very solid foundation of of api that utilize webgl in tensorflow, so I decided to give it a try. Nope, we are not using it for Machine Learning. :DIf it works out, it will solve most of the issues. ;['Great news @hiukim ! 🙂👍🤸=====']
https://github.com/hiukim/mind-ar-js/issues/5;Error compiling fragment (GPU);17;closed;2020-09-22T18:09:45Z;2021-03-17T04:40:45Z;Hi @hiukim while i was testing your example https://hiukim.github.io/mind-ar-js/samples/example1.html i receive this error:```jsgpu-browser.js:14991 Uncaught (in promise) Error: Error compiling fragment shader: Fragment shader compilation failed.ERROR: 0:2: '' :  integer constant overflow ERROR: 0:67: '' :  integer constant overflow ERROR: 2 compilation errors.  No code generated.�    at WebGLKernel.build (gpu-browser.js:14991)    at WebGLKernel.run (gpu-browser.js:18496)    at shortcut (gpu-browser.js:18516)    at Tracker._combineImageList (tracker.js:334)    at new Tracker (tracker.js:38)    at eval (controller.js:84)```of course nothing can be tracked or displayed...Maybe my Android device is not supported? i tested with a Wiko View with Android 7.1.2 but i will test with another device. EDIT: tested on Chrome browser 86.04240.110;"['Whenever you see ""WebGLKernel"", it\'s from the underlying gpu.js. I have experienced different behaviours in different devices with this library. I have tested with all my available devices and fixed a lot of things. Unfortunately, it\'s really hard to debug without the actual devices.=====', '@hiukim `Whenever you see ""WebGLKernel"", it\'s from the underlying gpu.js.` i supposed this. Probably this is caused because my device (it is an Adreno CPU) does not support high precision float? I will investigate this.EDIT: the CPU is not an ADRENO, this is the GPU=====', ""@kalwalt  thx a lot.Please noted that I'm using a custom branch of gpu.js instead of their current master. They are basically the same, but have one line difference here:    https://github.com/hiukim/gpu.js/blob/1ab9497dfd05fdc9e62ea599adc6efb51abedbcd/src/backend/gl/kernel.js#L33It's to temporarily suppress an issue found in ios: https://github.com/gpujs/gpu.js/issues/632====="", 'ok understood, look this is the device specifications: https://www.devicespecifications.com/en/model-cpu/fa444577=====', ""I guess you can try running the gpu.js examples with your devices and see if they works first.gpu.js basically translate the js code into shader programs in webgl and reformat the result back. My experience is that most of the issues come from the js code failed to translate or the translated shader programs run differently in different devices.The proper fix is probably figuring out what's wrong and fixing the gpu.js. That's the final move and can fix anything, but hard to do.Sometimes I just do trial and error and see which lines of js code caused the problem, and rewrite it in some other ways hopefully to bypass the problems.====="", ""> I guess you can try running the gpu.js examples with your devices and see if they works first.> > gpu.js basically translate the js code into shader programs in webgl and reformat the result back. My experience is that most of the issues come from the js code failed to translate or the translated shader programs run differently in different devices.> > The proper fix is probably figuring out what's wrong and fixing the gpu.js. That's the final move and can fix anything, but hard to do.> > Sometimes I just do trial and error and see which lines of js code caused the problem, and rewrite it in some other ways hopefully to bypass the problems.If i have time i will try the gpu.js examples, just to understand which code run or not. I had also to debug the error, i will try when i have a bit of time. Thank you for the answer and support! :smile: ====="", 'no problem. whenever you have time. Feel free to let me know if you encountered any troubles.=====', 'same here, doesnt work on android 10 samsung s10e=====', '@subjectdenied which error do you receive? @hiukim i tried the gpu.js examples and i got the same result, i have the suspect that my Gpu is not supported. GPU: Adreno (TM) 308 with OpenGLES 3.0=====', 'it hangs at the ""loading"" for an example in example2, the error is about ""precision missing""Uncaught (in promise) Error: precision missing    at lookupKernelValueType (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:13365)    at Function.lookupKernelValueType (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:14602)    at WebGLKernel.setupConstants (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:14908)    at WebGLKernel.build (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:14944)    at WebGLKernel.run (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:18496)    at Detector.shortcut [as inputKernel] (webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:18516)    at Detector.detect (webpack:///./src/image-target/detectorGPU/detector.js?:149)    at Controller.dummyRun (webpack:///./src/controller.js?:100)    at NewSystem._startAR (webpack:///./src/aframe.js?:147)``=====', '@subjectdenied Can you try some examples from gpu.js? https://gpu.rocks/#/examples=====', 'i tried some, they seem to work fine, with the exception of ""video convolution"", where the first canvas shows only black on my phone. however there is no error in the consolei found two settings this might be related to in the gpu.js docs- tactic or kernel.setTactic(\'speed\' | \'balanced\' | \'precision\') New in V2!: Set the kernel\'s tactic for compilation. Allows for compilation to better fit how GPU.js is being used (internally uses lowp for \'speed\', mediump for \'balanced\', and highp for \'precision\'). Default is lowest resolution supported for output.- precision or kernel.setPrecision(\'unsigned\' | \'single\') New in V2!: \'single\' or \'unsigned\' - if \'single\' output texture uses float32 for each colour channel rather than 8=====', ""I don't really have much idea now. Most of the time when it involves gpu.js, I need to tweak the code line of by line and see what triggers the error. ====="", 'Testing in firefox. It doesn\'t works but i receive a bit different message:```jsunreachable code after return statementmindar.prod.js:9077:4Use of the motion sensor is deprecated. mindar.prod.js line 547 > eval:65126:11Use of the orientation sensor is deprecated. mindar.prod.js line 547 > eval:85127:11video ready... <video autoplay="""" muted="""" playsinline="""" style=""position: absolute, top:…0.667px, height: 518px,"" width=""640"" height=""480"">aframe.js:71:17unreachable code after return statementmindar.prod.js line 558 > eval:9077:4WebGL warning: drawArraysInstanced: Drawing to a destination rect smaller than the viewport rect. (This warning will only be given once) mindar.prod.js line 558 > eval:15136:8Uncaught (in promise) Error: Error compiling fragment shader: Fragment shader compilation failed.ERROR: 0:2: \'\' :  integer constant overflow ERROR: 1 compilation errors.  No code generated.    build webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:14991    run webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:18496    shortcut webpack:///./node_modules/gpu.js/dist/gpu-browser.js?:18516    detect webpack:///./src/image-target/detectorGPU/detector.js?:149    dummyRun webpack:///./src/controller.js?:100    Aframe 40mindar.prod.js line 558 > eval:14991:13Errore mapping di origine: Error: NetworkError when attempting to fetch resource.URL risorsa: webpack:///./node_modules/aframe/dist/aframe-master.js?URL mapping di origine: aframe-master.js.map```EDIT: tested on the same device (Wiko View) with Firefox 82.1.1=====', '@subjectdenied @kalwalt I have just finished a big revamp, removing GPU.js completely. Hopefully it will solve a lot of these problems.If you guys could try the example again with your devices (whatever devices you have, haha), that would be nice!https://hiukim.github.io/mind-ar-js/samples/example1.html=====', 'Dear @hiukim i will try it for sure! 🙂🎉😉=====', 'I tested the new version but i got an error see #10.=====']"
https://github.com/hiukim/mind-ar-js/issues/4;Feature Request: hints for supported browser;1;closed;2020-09-10T04:25:19Z;2021-03-17T04:40:35Z;Hello. This tool is great. But if someone opens the web in some browser not supported like in-app-browser, would there be any hints for reminding user to open the right browser to view the effect?;['@tomtomtong @hiukim I think that this is a good idea!=====']
https://github.com/hiukim/mind-ar-js/issues/3;Image target tool fault;10;closed;2020-09-09T17:30:50Z;2021-03-17T04:40:20Z;Your image target tool is not working. After uploading the photo nothing is happening after clicking on start button. I had wait for around 30 minutes nothing was happened;"['can you open the developer console and see any error messages?=====', 'Source map error: Error: NetworkError when attempting to fetch resource.Resource URL: webpack:///./node_modules/aframe/dist/aframe-master.js?Source Map URL: aframe-master.js.map=====', 'I think you can ignore that warning.=====', 'still not workingkindly plz recheck your web toolthe errors are-gpu-browser.js:14988 Uncaught Error: Error compiling vertex shader:     at WebGLKernel.build (gpu-browser.js:14988)    at WebGLKernel.run (gpu-browser.js:18496)    at Array.shortcut (gpu-browser.js:18516)    at Detector._applyPrune (detector.js:692)    at Detector.detectImageData (detector.js:232)    at _extractMatchingFeatures (compiler.worker.js:39)    at onmessage (compiler.worker.js:18)![image](https://user-images.githubusercontent.com/58983655/93017059-7060e100-f5e3-11ea-8acd-acc38da64ffb.png)=====', ""Unfortunately I don't have a window platform to fix it. If you have a mac or linux, maybe you can use those. Otherwise, you could also try using your ios/android phones/tablets to try.====="", '![image](https://user-images.githubusercontent.com/32410475/101906253-ac094800-3bde-11eb-8250-4a53a486d5e3.png)@hiukim having the same issue on my microsoft ege and firefox.=====', '@ragavendranbala Thanks for reporting. Please follow this issue for updates. https://github.com/hiukim/mind-ar-js/issues/8=====', '@ragavendranbala @ckbsbal I have just finished a big revamp, removing GPU.js completely. Hopefully it will solve a lot of these problems.If you guys could try again with your devices (whatever devices you have, haha), that would be nice!=====', 'I haven\'t been able to get the compile to work on Mac or iOS. Mac console::<img width=""1273"" alt=""image"" src=""https://user-images.githubusercontent.com/115180/106203385-eb84f700-6180-11eb-82f6-307c2a522b36.png"">iOS console<img width=""1279"" alt=""image"" src=""https://user-images.githubusercontent.com/115180/106203326-d14b1900-6180-11eb-9536-c7213dfacfdb.png"">=====', ""@patrickoshaughnessey the Mac one might be loading. What's the resolution of your image?=====""]"
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/34;Tensorflow and Tensorflowjs version;4;open;2020-09-11T15:05:46Z;2020-09-21T10:57:45Z;Hello @reiinakano,Thanks for your great repository. Can you tell me which version of Tensorflow and Tensorflowjs have you used?I want to convert Tensorflowjs model to TFLite model but I have some issue.Look forward to hearing from you soon,Thanks;"['@KienPM as I\'ve been working on a fork of this repo, I can answer that:This uses Tensorflowjs 1.0 (as you can notice in `package.json` as well `""@tensorflow/tfjs"": ""~1.0.0""`).What\'s the issue you\'re encountering with TFLite?=====', '@geni94 Thanks for your answer.I\'ve tried to convert TensorflowJS model to TFlite model according to [this answer](https://stackoverflow.com/questions/62544836/how-to-convert-from-tensorflow-js-json-model-into-tensorflow-savedmodel-or).When I run `tensorflowjs_converter --input_format""=tfjs_layers_model --output_format=keras tfjs_model.json hdf5_keras_model.hdf5` to convert from TensorflowJS model to Keras model with tensorflow 2.3.0, tensorflowjs 2.3.0, I got ""ValueError: Improper config format""When I run with tensorflow 1.15.0, tensorflowjs 1.7.4, I got ""KeyError: \'class_name\'""=====', 'I literally told you in the first comment that this library uses tensorflow 1.0, and yet you go ahead and try with 1.15, 1.7.4, 2.3.0, etc... The `package.json` specifies the version: `""@tensorflow/tfjs"": ""~1.0.0""`. I\'m not surprised you can\'t transform this model with TFLite, seeing that the versions do not match.=====', 'thank you!=====']"
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/30;distilled tf savedModel ;1;open;2020-06-06T11:33:20Z;2020-06-10T02:03:21Z;Hi @reiinakano, thanks for this example, we tried running the TFJS model in our electron application, it is a bit slower in the browser. We are planning to write a native node module with onnxruntime, just wondering is there a place we could find the original distilled TF model before the TFJS conversion, which might help us to test. Thanks!;"[""I don't think I have them lying around anymore but @khanhlvg might have them!=====""]"
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/32;How to change content size slider in local? Need bigger images;1;closed;2020-07-21T04:41:35Z;2020-07-21T04:43:08Z;What it says on the tin, I'd like for the image output to be larger than the slider allows for, so I'm trying to edit it, but the references in the main js file don't reveal anything so I'm at a bit of a loss. (My background is python and AI/ML, not nodejs.);['lol I IMMEDIATELY found it in the index.html file :P=====']
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/27;where is port 9966 configured?;2;closed;2019-09-06T03:43:32Z;2019-09-06T04:27:40Z;"I thought I knew yarn/node/javascript.  ¯\_(ツ)_/¯yarn run start  : checking package.json to see what is executed.package.json shows ""scripts"": {    ""start"": ""budo main.js:dist/bundle.js --live --host localhost"",main.js does not contain a reference to port 9966 directory dist is empty. bundle.js does not exist, yet my localhost is running fine.Where should I be looking?";"[""I don't know either, sorry. First place I would look is the budo docs, there might be a cli option to set the port number there. ====="", 'https://github.com/mattdesl/budo/blob/master/docs/command-line-usage.mdhttps://github.com/mattdesl/budo/blob/master/README.md#cliedited package.json as below     ""start"": ""budo main.js:dist/bundle.js --live --host localhost --port 5000"",thanks. turns out 9966 is a default port # in budo. I was looking to rework this into an progressive web app. Javascript is not my power. Any thoughts on how to structure or resources?also: just found this. the range of in browser options is impressive.https://bleedingedgepress.com/deep-learning-browser/https://github.com/backstopmedia/deep-learning-browser=====']"
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/24;Larger images;1;closed;2019-05-05T07:37:36Z;2019-05-05T07:44:08Z;"I've tried to increase the size of the source image (changed index.html) to produce larger stylised images but I can't seem to get beyond 1000x1000.  I get an error: ""GL ERROR :GL_OUT_OF_MEMORY : glBufferData: cannot allocate more than 1GB"".  Any suggestions?";"[""Your browser just ran out of memory. It *is* a resource-constrained environment after all. I don't think there's a solution for this other than using another implementation that does not run on the browser. I suggest the original Magenta implementation in Python https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization=====""]"
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/22;Sharing the original SavedModel;1;closed;2019-02-08T03:25:09Z;2019-02-08T15:27:19Z;Hi Nakano-san,Do you mind sharing the original float SavedModel prior to being converted to TensorFlow.js?I'm interested in converting the model to TF Lite and use on mobile. It'd be great if I can build on top of your work instead of rebuilding what you have done.Thanks.;['SavedModels for distilled style and original transformer:https://drive.google.com/drive/folders/15r48XYPRzBB49PGvnjN9RN3xHogF8u6-?usp=sharinghttps://drive.google.com/drive/folders/1-IIg2SotsYlaLJLpI7UTlkCSAZZhxnjD?usp=sharingThe distilled style model here has its variables multiplied by 10 to fix the float16 issue.Good luck with your project!=====']
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/6;How about migrate the model to the mobile？;1;closed;2018-11-30T04:04:05Z;2019-02-08T15:32:22Z;Great demo for arbitrary style transfer！The progress from fixed style images to totally free choices is really exciting. It seems that you are more likely to create a demo with js?  Inspired by this repo, I want to have a try with migrating the model to the mobile like Prisma, just for practice, no commercial use : ).  Since I am not familiar with the tensorflow.js apis, there are few questions:Is there any way to integrate the .js with android frame? Or the possible solution is taking use of the original model (before  combined with js) and trying to reduce it with apis like tensorflow lite?Looking for your help : );['Hi! You can check #22. It seems @khanhlvg will work on the same thing. Perhaps he could give you some tips since I have never done anything with TF Lite=====']
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/3;Dimensions of the model;1;closed;2018-11-20T00:47:59Z;2018-11-28T08:11:47Z;Hi!Great work!I just only wanted to tell you that we just recently ported this model to tfjs from [here](https://github.com/elleryqueenhomels/arbitrary_style_transfer)We haven't modified the use of the InceptionV3. After the parameters optimisation the total model (we don't expose it as two parts) is just only about 25MB in total and the model is able to run (like yours) if the browser can handle enough RAM creating the WebGL context  (usually limited to 512MB).So your model is 17MB in total and the model with inceptionV3 is just only 25MB.Of course that's not the REAL memory consumed by the model on inference.Again, great work!;"[""Oh, cool, thanks for letting me know! Maybe I'll try porting the inceptionmodel too.You should put up a tfjs demo so people can play around with it!On Tue, Nov 20, 2018, 9:48 AM DavidGOrtega <notifications@github.com wrote:> Hi!>> Great work!>> I just only wanted to tell you that we just recently ported this model to> tfjs from here> <https://github.com/elleryqueenhomels/arbitrary_style_transfer>>> We haven't modified the use of the InceptionV3. After the parameters> optimisation the total model (we don't expose it as two parts) is just only> about 25MB in total and the model is able to run (like yours) if the> browser can handle enough RAM creating the WebGL context (usually limited> to 512MB).>> So your model is 17MB in total and the model with inceptionV3 is just only> 25MB.>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/3>,> or mute the thread> <https://github.com/notifications/unsubscribe-auth/ARg1Vog6e8ezIoie7Ywfhni9ZPaSXtWeks5uw1E_gaJpZM4YqKhh>> .>=====""]"
https://github.com/infinitered/nsfwjs-mobile/issues/9;Error: FileReader.readAsArrayBuffer is not implemented;5;closed;2020-05-04T19:46:00Z;2020-09-29T19:01:39Z;Hi @GantMan @kevinvangelder ,I tried this code in a new react native project but I was getting this error. I used react-native 0.61.5 and react 16.9.0 to produce this error.[Unhandled promise rejection: Error: FileReader.readAsArrayBuffer is not implemented]- node_modules/react-native/Libraries/Blob/FileReader.js:84:20 in readAsArrayBuffer- node_modules/react-native/Libraries/vendor/core/whatwg-fetch.js:220:29 in readBlobAsArrayBuffer;"[""I feel like I had to write a shim for this and maybe I didn't do an upstream PR. Unfortunately I won't have access to the computer I built this project on for the next week so I'm not currently able to check. Can you try with react 16.9.0 and react-native 0.60.5 and see if you get a different result?====="", ""I'll keep watching this ticket.====="", '@JoonJoon123 I had some spare time so I tried running this with the default package versions and everything appears to be running correctly.=====', 'Hey @kevinvangelder ,I was able to successfully run the app on my iPhone. Thanks for the help.I will close this issue.=====', ""Hi!I'm having the same issue under Android 9: `Error: FileReader.readAsArrayBuffer is not implemented`.react: 16.13.1.react-native: 0.63.3.=====""]"
https://github.com/infinitered/nsfwjs-mobile/issues/4;Load Local Model;3;closed;2019-09-28T00:22:04Z;2019-10-23T23:55:49Z;Currently, the model is being fetched over the network every time the app is loaded. This both expensive and inconvenient.Depends on https://github.com/infinitered/nsfwjs/issues/170;['Hi, I was trying the example. Nice work by the way and thanks for sharing the code.Can the model (since to my understanding is pre-trained) be loaded from the assets file?=====', 'It can be for web projects but since it is trying to do a network fetch the existing system will not work for loading the model locally in React Native, which is why this links to the NSFWJS ticket this issue depends on. =====', 'Thanks! My perspective was for a React Native app. =====']
https://github.com/radi-cho/tfjs-firebase/issues/1;question: how do you configure and determine the layer with dense against the vocab?;2;closed;2018-10-11T09:01:10Z;2018-10-14T17:45:59Z;model.add(tf.layers.dense({ units: 2, inputShape: [vocabulary.length] })),model.add(tf.layers.dense({ units: 1, inputShape: [2] })),if i add new vocab how can i reconfigure the dense layer?;"['## Simple explanation of how the TF model & the vocab are configuredThe model is configured to work with vocabulary shaped like this:```[ string, string, string, string... n - string ]```Then when you\'re training or trying to get prediction there is a `fitData` method, which receives string as input and returns sequence of numbers which tensorflow can understand. For example:```Input: ""This app is really bad and the design ugly. It crashes and shows me a lot of ads. Just bad.""Vocabulary: [ ""bad"", ""ugly"", ""crashes"", ""slow"", ""ads"", ""good"", ""beautiful"", ""fast"", ""worth"" ]Output of the fitData method: [ 2, 1, 1, 0, 1, 0, 0, 0, 0 ]```The output is an array, similar to the vocab but contains numbers - how many times a word from the vocab is used. Then a model is trained like this:```x layer: [ [ fitData output ], [ ... ], [ ... ], ... ]y layer: [ [ number ], [ number ], ... ]```The `y` layer is a 2d tensor, with a single integer for each value. In my case I use zeros and ones.```0 - negative, 1 - positive```But you can also set it up with more complex values to get more than 2 results. That\'s helpful when you try to label something.## Your question, @kenken64 The first layer (`x`) is initialized by `tf.layers.dense({ units: 2, inputShape: [vocabulary.length] })`, with `inputShape` equial to `vocabulary.length`, because it contains _int_ value for each vocab item.> if i add new vocab how can i reconfigure the dense layer?- If you wanna change the vocab items but keep the current shape, you won\'t need any changes on the model or its layers.```javascript[ string, string, ... n strings ] // The length is not in significant importance, because the model automatically gets it via vocab.length```- If you think that the current shape won\'t fit your need, you should rethink the model. I can help you, but in order to keep the issues specific and helpful to others you can chat me on my e-mail rsg.group.here@gmail.com or join my Slackroom - sign up here http://rsg-slack.herokuapp.com/ and explaining your specific use case and ideas.## ResourcesFew sources which might be helpful to you and anyone else:- https://www.quora.com/What-is-the-bag-of-words-algorithm- https://en.wikipedia.org/wiki/Bag-of-words_model- https://machinelearningmastery.com/gentle-introduction-bag-words-model/- https://www.youtube.com/watch?v=hgQTIMrGwtg- https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts=====', 'Closing it as a resolved. @kenken64, If you still have questions, feel free to comment on the issue again and reopen it, or file a new one. 😃 😃 😃 Also my e-mail and the Slackroom are linked above. 👍 =====']"
https://github.com/amandeepmittal/mobilenet-tfjs-expo/issues/10;Dependency Issues;1;open;2021-02-06T23:30:15Z;2021-05-08T21:09:32Z;Hi! Will you be able to update node module dependencies? Thank you. ;['i updated https://github.com/Wellbrito29/mobilenet-tfjs-expo.git=====']
https://github.com/amandeepmittal/mobilenet-tfjs-expo/issues/1;Fetch from tensorflow js fails ;9;open;2019-11-25T02:10:55Z;2020-06-11T09:27:47Z;After selecting an image the fetch request from tfjs fails `Network request failed` `- node_modules\@tensorflow\tfjs-react-native\dist\platform_react_native.js:80:49 in onerror``- node_modules\event-target-shim\lib\event-target.js:172:43 in dispatchEvent``- ... 8 more stack frames from framework internals`;"[""As correctly noted from the article comments this is caused by the fetch request from tfjs which only supports network fetch requests and not local requests`const response = await fetch(imageAssetPath.uri, {}, { isBinary: true })` will fail `const response = await fetch('https://testimage.jpg', {}, { isBinary: true })` will succeed I will look into what can be done to fix this ====="", ""Hey @daniel-sudz this is a great. If you are able to fix this, let me know, and with your permission, I'd add the you use case as well as your solution in the post itself. I haven't played much with tfjs tbh, and am just exploring the use cases just like you. :)I'll let the issue open, until you decide to close it.====="", 'Hi, is there an update on this ? I ran into the same error=====', '@Hasherz96 I think TFJS now supports local requests in React Native. I will be updating the article/repo soon.=====', 'any updates on this?=====', '@amandeepmittal have you been able to sucessfully request local files through the fetch function with the latest version of TFJS?=====', ""Does anyone have any idea why it's working on an iOS simulator, but it crashes on an Android mobile device (not emulator)?====="", 'Hey @amandeepmittal, I have made some changes and opened a pull request that fixes the problem on android but I was not able to test it on ios (not having an ios device) maybe you can test it on ios and let me know if there is anything that needs to be fixed :smile: =====', 'Hey @sarthakpranesh thanks for the PR. Let me test on iOS and get back to you.=====']"
https://github.com/hugozanini/realtime-semantic-segmentation/issues/3;Fail to load trained model;2;open;2020-08-05T12:40:37Z;2020-10-01T00:45:38Z;When trying to load the pre-trained model, I get the following error:```Unhandled Promise Rejection: Error: Based on the provided shape, [576], the tensor should have 576 values but has 100```This is at this line:https://github.com/hugozanini/realtime-semantic-segmentation/blob/master/src/index.js#L24;['I want to work on this issue @hugozanini =====', 'Hey @carrycooldude feel free to work on that. Thanks for contributing to the project :) =====']
https://github.com/charliegerard/whereami.js/issues/3;UnhandledPromiseRejectionWarning;7;open;2021-04-21T19:03:07Z;2021-04-27T19:40:05Z;Hey Charlie, wonder if you can help with this....simon@Simons-Air whereami.js % node server.js -p     (node:10796) UnhandledPromiseRejectionWarning: TypeError: Cannot set property 'room' of undefined    at /Users/simon/Documents/Projects/whereami.js/predict.js:99:53    at Array.map (<anonymous>)    at predict (/Users/simon/Documents/Projects/whereami.js/predict.js:99:22)(Use `node --trace-warnings ...` to show where the warning was created)(node:10796) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:10796) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.Room command works fine.....simon@Simons-Air whereami.js % node server.js rooms  backroombathroomhallkitchenloungemainbedroomRecent installation of the repo. MacBook Air 2020 11.2.3 (BigSur)Any help would be appreciated. Thankyou;"[""Hey!I was able to reproduce this issue when using data recorded in an environment with different wifi networks from the one i'm currently in.Could you double check if the wifi network you are connected to when using the `predict` command is included in your training data networks? They should be found in the `whereamijs-data` folder generated when training locations 🙂 If it's not in there, you will probably need to delete the `whereamijs-data` folder and retrain locations.Please let me know if this is the issue so I can publish a fix for this 🙂 Thanks!!====="", ""That's AWESOME Charlie! I'll check it out shortly and let you know.Thankyou so much for responding to me. Was t sure if user error, whichwould make sense to me.Take care and thanks again.CheersSimonOn Fri, 23 Apr 2021, 4:04 pm Charlie, ***@***.***> wrote:> Hey!>> I was able to reproduce this issue when using data recorded in an> environment with different wifi networks from the one i'm currently in.>> Could you double check if the wifi network you are connected to when using> the predict command is included in your training data networks? They> should be found in the whereamijs-data folder generated when training> locations 🙂>> If it's not in there, you will probably need to delete the whereamijs-data> folder and retrain locations.>> Please let me know if this is the issue so I can publish a fix for this 🙂>> Thanks!!>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/charliegerard/whereami.js/issues/3#issuecomment-825721082>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ATZA6TJU2QD4K4CSOKTE6O3TKGD75ANCNFSM43K54IYQ>> .>====="", 'HEY Charlie,Just checked the folder and the rooms i trained where in there.[image: Screenshot 2021-04-23 at 18.13.29.png]I\'ll check the contents of the JSON docs, but with only 1 wifi network onhere, Im not sure.  I will double check.Ok, checked a couple of the JSON docs, and there are some spurious results.Some just show the wifi here, and others show some Ive not seen before.""The Hindley Empire cc:32:e5:e1:22:9e"":12,""VM325139940:0d:10:28:10:3f"":16,""SKYE8F7Aa0:bd:cd:15:e1:b5"":22,""Virgin Media 52:0d:10:28:10:39"":36,""VM325139940:0d:10:28:10:39"":36,""SKYE8F7A a0:bd:cd:15:e1:b2"":42,""SKYE8F7A20:47:ed:49:5e:53"":44,""SKYE8F7A 20:47:ed:49:5e:56"":56,""VodafoneConnect63328897b8:08:d7:bc:36:2a"":78},{""The Hindley Empire cc:32:e5:e1:22:9e"":12,""VM325139940:0d:10:28:10:3f"":16,""SKYE8F7A a0:bd:cd:15:e1:b5"":16,""Virgin Media52:0d:10:28:10:39"":36,""VM3251399 40:0d:10:28:10:39"":36,""SKYE8F7Aa0:bd:cd:15:e1:b2"":40,""SKYE8F7A 20:47:ed:49:5e:53"":46,""SKYE8F7A20:47:ed:49:5e:56"":56,""VodafoneConnect63328897 b8:08:d7:bc:36:2a"":88},{""TheHindley Empire cc:32:e5:e1:22:9e"":12,""VM3251399 40:0d:10:28:10:3f"":16,""SKYE8F7Aa0:bd:cd:15:e1:b5"":16,""Virgin Media 52:0d:10:28:10:39"":36,""VM325139940:0d:10:28:10:39"":36,""SKYE8F7A 20:47:ed:49:5e:53"":46,""SKYE8F7Aa0:bd:cd:15:e1:b2"":48,""SKYE8F7A 20:47:ed:49:5e:56"":66,""VodafoneConnect63328897b8:08:d7:bc:36:2a"":86},{""VM3251399 40:0d:10:28:10:3f"":16,""SKYE8F7Aa0:bd:cd:15:e1:b5"":16,""Virgin Media 52:0d:10:28:10:39"":36,""VM325139940:0d:10:28:10:39"":36,""SKYE8F7A 20:47:ed:49:5e:53"":40,""SKYE8F7Aa0:bd:cd:15:e1:b2"":48,""SKYE8F7A 20:47:ed:49:5e:56"":68,""VodafoneConnect63328897b8:08:d7:bc:36:2a"":90},{""VM3251399 40:0d:10:28:10:3f"":16,""SKYE8F7Aa0:bd:cd:15:e1:b5"":16,""Virgin Media 52:0d:10:28:10:39"":36,""VM325139940:0d:10:28:10:39"":36,""BTWi-fi 62:86:20:13:5f:61"":40,""VesiLud69f0:86:20:13:5f:60"":40,""SKYE8F7A a0:bd:cd:15:e1:b2"":44,""SKYE8F7A20:47:ed:49:5e:53"":48,""SKYE8F7A 20:47:ed:49:5e:56"":66,""VodafoneConnect63328897b8:08:d7:bc:36:2a"":88}]For reference, mine is VodafoneConnect63328897, which is waaaaay off to theright in the above line form the JSON.Does that help at all?CheersSimonOn Fri, Apr 23, 2021 at 5:25 PM Simon Hodgson ***@***.***>wrote:> That\'s AWESOME Charlie! I\'ll check it out shortly and let you know.>> Thankyou so much for responding to me. Was t sure if user error, which> would make sense to me.>> Take care and thanks again.> Cheers> Simon>>> On Fri, 23 Apr 2021, 4:04 pm Charlie, ***@***.***> wrote:>>> Hey!>>>> I was able to reproduce this issue when using data recorded in an>> environment with different wifi networks from the one i\'m currently in.>>>> Could you double check if the wifi network you are connected to when>> using the predict command is included in your training data networks?>> They should be found in the whereamijs-data folder generated when>> training locations 🙂>>>> If it\'s not in there, you will probably need to delete the>> whereamijs-data folder and retrain locations.>>>> Please let me know if this is the issue so I can publish a fix for this>> 🙂>>>> Thanks!!>>>> —>> You are receiving this because you authored the thread.>> Reply to this email directly, view it on GitHub>> <https://github.com/charliegerard/whereami.js/issues/3#issuecomment-825721082>,>> or unsubscribe>> <https://github.com/notifications/unsubscribe-auth/ATZA6TJU2QD4K4CSOKTE6O3TKGD75ANCNFSM43K54IYQ>>> .>>>=====', 'Looks like it is trying to connect to other wireless networks in the area which are secured.Just looked up other available networks and these are all there are around.CheersSimon=====', 'Deleted the whereamijs-data folder, retrained a single room, and predict worked just fine for that. Trained another 3 rooms which all worked fine, the 5th room returned the original error.Hope that helpsCheersSimon=====', ""Hey @imagineering-uk!Ok, I think I found the issue! 🙂 🎉  For the prediction to work, there has to be **at least 1 wifi network name that's common to all training files.**I think what happened is that when you tried with 3 rooms, it worked because they all contained at least 1 common network name, but when you trained a 5th room, maybe that file did not contain any network that could also be found in all other files. This could be due to spotty wifi, changing network you're connected to OR, what your case might be, while moving around your house, your network SSID `VodafoneConnect63328897` can be the same but the BSSID can change! In general, people won't realise it because it doesn't directly affect your connectivity, but as I use the BSSID as part of the network name in the data, my logic sees it as a different network.I didn't experience this issue before probably because I'm in a small apartment.I didn't want to only use the SSID alone because some people can call their networks the same name and this can result in wrong data fed to the machine learning algorithm and predict wrong results. 🤔 The only quick thing I can do for now is log an error message with some explanation when this case happens 🙂 There might be a way to have a more robust solution for this but right now I don't have the capacity to spend too much time on this.I pushed some changes and released v2.0.0 so now people should see an error message when this happens 🙂 ====="", ""Youre a superstar Charlie,Didnt want to make a big deal of it, and its not been a problem, justwanted to get your code working to a) prove that I could do that and b) seewhat might be possible with such a fine level of location detection.Its SUPER COOL, and I cant thankyou enough for sharing the code, and alsobeing super helpful in helping me out. Really appreciate your time.Take care and thankyou again!!!CheersSimonOn Tue, Apr 27, 2021 at 1:02 PM Charlie ***@***.***> wrote:> Hey @imagineering-uk <https://github.com/imagineering-uk>!>> Ok, I think I found the issue! 🙂 🎉 For the prediction to work, there> has to be *at least 1 wifi network name that's common to all training> files.*> I think what happened is that when you tried with 3 rooms, it worked> because they all contained at least 1 common network name, but when you> trained a 5th room, maybe that file did not contain any network that could> also be found in all other files. This could be due to spotty wifi,> changing network you're connected to OR, what your case might be, while> moving around your house, your network SSID VodafoneConnect63328897 can> be the same but the BSSID can change! In general, people won't realise it> because it doesn't directly affect your connectivity, but as I use the> BSSID as part of the network name in the data, my logic sees it as a> different network.>> I didn't experience this issue before probably because I'm in a small> apartment.>> I didn't want to only use the SSID alone because some people can call> their networks the same name and this can result in wrong data fed to the> machine learning algorithm and predict wrong results. 🤔>> The only quick thing I can do for now is log an error message with some> explanation when this case happens 🙂>> There might be a way to have a more robust solution for this but right now> I don't have the capacity to spend too much time on this.>> I pushed some changes and released v2.0.0 so now people should see an> error message when this happens 🙂>> —> You are receiving this because you were mentioned.> Reply to this email directly, view it on GitHub> <https://github.com/charliegerard/whereami.js/issues/3#issuecomment-827551743>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ATZA6TI3CPSIWWZTFOFRZD3TK2RT3ANCNFSM43K54IYQ>> .>=====""]"
https://github.com/cpury/lookie-lookie/issues/3;Replace CLMTracker with FaceMesh;1;open;2020-05-22T15:21:19Z;2020-05-25T20:02:26Z;You may want to consider replacing CLMTracker with [FaceMesh](https://github.com/tensorflow/tfjs-models/tree/master/facemesh), as we found it to be much more stable/accurate in WebGazer. Plus, it's also in tf.js. ;['Thanks for the tip! That looks very impressive. I will look into it when I get the time.=====']
https://github.com/cpury/lookie-lookie/issues/1;If I had trained a model use mine face, can it identify where others are looking?;1;open;2019-07-23T11:27:02Z;2019-07-24T07:52:26Z;;"[""Good question! You should try it :) But unfortunately, I don't think it will. Even if it learns a robust understanding of eye movements, the other person will have a different screen size, height, webcam, etc. So in the end, the predictions will probably be off.If you do try, please let me know what you find!=====""]"
https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/issues/2;Code doesn't run :-(;3;open;2018-09-19T14:15:48Z;2019-02-21T22:11:00Z;Hello,Thx for sharing everything  :-)I can install without problems.Clicking on 'Logic' ends with success.When I click on 'AAPL',I get the attached error.How can I get the code to run ?Thanks.![llsourcell](https://user-images.githubusercontent.com/5527990/45758973-2c6eb780-bc27-11e8-9d2a-ee66861d98fa.png);"[""Hi All!I'm @Thoughtscript - I wanted to apologize about the confusion this has caused! Yikes!First, let me be very clear that I'm not in any way affiliated with @llSourcell.This code was NOT PRODUCTION WORTHY and INCOMPLETE - as noted on the original repo listed here - it's a WORK IN PROGRESS.Actually, I'm not sure how it ended up in a video or attracting any interest whatsoever.The code was being completed for an article to be posted on the top-ranked consulting firm X-Team's blog. Unfortunately, the company decided to move in another direction with its writing and I've since moved over to Microsoft (though I remain affiliated with X-Team's great community).Every other repo listed under my @Thoughtscript account without a WIP flag should be fully functional (with any issues, if they exist, noted). This particular project (which you'll note bears my name on in the pictures), is actually no longer maintained.Any questions, feel free to send me an email at adam.gerard@gmail.com. Sorry for any confusion this has caused.Cheers!====="", ""PS - maybe try:1. Normalizing the values1. Correcting the shapes1. Correctly configuring the convolutional neural net1. Leveraging parallel processing and WebGL since that shit's gonna break on single-threaded JS====="", ""I tried to use the code but it didn't worked for me. So the closest working solution that I got was this one: https://towardsdatascience.com/stock-price-prediction-system-using-1d-cnn-with-tensorflow-js-machine-learning-easy-and-fun-fe5323e68ffb Hope that helps.=====""]"
https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/issues/1;Console error expected dense_Dense1 to have shape [,57,10];18;open;2018-07-14T21:18:26Z;2019-02-21T22:10:27Z;I've got an error trying to run your code,Uncaught (in promise) Error: Error when checking target: expected dense_Dense1 to have shape [,57,10], but got array with shape [1,1960,1].;"[""Same, I've tried changing a lot of parameters and modifying the data structure, but I can't get rid of it.The error appears in model.fit (and also when testing model.evaluate) at line 105 in 2b_cnn.js====="", ""I would love a thorough explanation for why this happens in this case in particular. I've been banging my head against it for a while now and the head is losing.====="", 'I implement this in Python and Js, same error with data shapes in Dense Layer.=====', ""glad I'm not the only one seeing this error.  ====="", ""I'm afraid it's my fault - I wrote the original (and as yet uncompleted) code! The CNN wasn't fully implemented yet!**NINJA EDIT - 9.30.18**: Very sorry all please see the comment added below. Also, forgive the excessive `!`'s. Thanks!====="", 'no fix ?=====', 'did anyone manage to figure this out... =====', ""There are major errors with the code. It is not in a functioning state and that makes me wonder it was even tested before the video was recorded? Firstly, **`data.highs` is mapped to epoch times** here:https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2a_cnn.js#L35then comes the reshaping part. Why not reshape it prior when the dates are being converted to epoch? https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L94That does not make sense.  ( https://js.tensorflow.org/api/0.12.5/#reshape )and then the part that left me speechless was this... https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L95Why would you map a price series that way? Not even sure what was the goal... Could someone enlighten me. I'm not sure how the reshaped list should look like. ====="", ""It wasn't tested, it's just an idea but that wasn't made very clear in the video.I've played around with it a lot and gotten it to a state where it works fairly well with a simple NN consisting only of dense layers. Next step is to get my head around formatting the data as matrices and implementing a CNN, but I'm learning as I'm going and some of the maths is hard.====="", 'http://oracle.gigil.berlin/=====', 'Same problems here!=====', 'Same here, kinda disappointment.=====', '@llSourcell Hi, do you notice this issue?=====', ""> There are major errors with the code. It is not in a functioning state and that makes me wonder it was even tested before the video was recorded?> > Firstly, **`data.highs` is mapped to epoch times** here:> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2a_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2a_cnn.js#L35)> > Line 35 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  dates.push(new Date(data[i]['Date'] + 'T00:00:00.000').getTime()) > then comes the reshaping part. Why not reshape it prior when the dates are being converted to epoch?> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2b_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L94)> > Line 94 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  tdates.reshape([1, 1960, 1]), > That does not make sense. ( https://js.tensorflow.org/api/0.12.5/#reshape )> > and then the part that left me speechless was this...> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2b_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L95)> > Line 95 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  thighs.reshape([1, 1960, 1]), { > Why would you map a price series that way? Not even sure what was the goal...> > Could someone enlighten me. I'm not sure how the reshaped list should look like.Hi All! I'm @Thoughtscript - I wanted to apologize about the confusion this has caused! Yikes! First, let me be very clear that I'm not in any way affiliated with @llSourcell.This code was **NOT PRODUCTION WORTHY** and **INCOMPLETE** - as noted on the original repo listed [here](https://github.com/Thoughtscript/x_team_tensorflow_js) - it's a **WORK IN PROGRESS**.Actually, I'm not sure how it ended up in a video or attracting any interest whatsoever. The code was being completed for an article to be posted on the top-ranked consulting firm [X-Team](https://x-team.com/blog/)'s blog. Unfortunately, the company decided to move in another direction with its writing and I've since moved over to Microsoft (though I remain affiliated with X-Team's great community).Every other repo listed under my @Thoughtscript account without a *WIP* flag should be fully functional (with any issues, if they exist, noted). This particular project (which you'll note bears my name on in the pictures), is actually no longer maintained. Any questions, feel free to send me an email at adam.gerard@gmail.com. Sorry for any confusion this has caused. Cheers!====="", ""PS - maybe try:1. Normalizing the values1. Correcting the shapes1. Correctly configuring the convolutional neural net1. Leveraging parallel processing and WebGL since that shit's gonna break on single-threaded JS====="", 'Hi @Thoughtscript , thanks for your kind explanation and mostly for noticing this issue!Now the big question is, what was the working demo which @llSourcell showed us in his Live stream.https://youtu.be/5Uw1iSwvHH8?t=47m53sAnyway, I think this is not a proper learning material after all.=====', ""> Anyway, I think this is not a proper learning material after all.exactly - this is generating a lot of confusion. for beginners this whole subject is hard to grasp and it is made even harder with all the other backend stuff going on.siraj makes stuff look really easy by skipping a lot of difficult and crucial steps and this repo here is the perfect example i wasn't even aware that this is not his code - but maybe i just didn't pay attention¯\\_(ツ)_/¯====="", ""I tried to use the code but it didn't worked for me. So the closest working solution that I got was this one: https://towardsdatascience.com/stock-price-prediction-system-using-1d-cnn-with-tensorflow-js-machine-learning-easy-and-fun-fe5323e68ffb Hope that helps.=====""]"
https://github.com/shaqian/tfjs-yolo/issues/4;Support Yolov4;1;open;2020-08-28T19:32:06Z;2020-10-28T08:00:23Z;Will you support yolov4?;['Hello, which library do you use to convert yolov4 model and how to use it in tensorflow.js Run on=====']
https://github.com/shaqian/tfjs-yolo/issues/3;l.loadModel is not a function - tfjs 1.1.2;2;closed;2019-06-14T02:01:49Z;2019-06-17T15:59:41Z;"```""@tensorflow/tfjs"": ""^1.1.2"",""tfjs-yolo"": ""^0.0.3"",```I get `l.loadModel is not a function`, same issue as https://github.com/ModelDepot/tfjs-yolo-tiny/pull/22. If I patch per that PR (use `loadLayersModel`) I get `myYolo.predict is not a function`. I actually tried pinning tfjs back to `^1.0.0`, but I get the same errors (strangely, since this project is pinned at that version..)";"[""I think it's fixed in https://github.com/shaqian/tfjs-yolo/pull/2. I just published the change in v0.0.4. ====="", 'confirmed fixed, thanks!=====']"
https://github.com/sanyuered/WeChat-MiniProgram-AR-TFJS/issues/2;Super cool! Will it work for other Tensorflow models?;1;closed;2021-03-16T00:59:57Z;2021-08-15T15:14:46Z;Hi!@sanyuered great work on this, I can't wait to test it out! Have you tried other models, like the bodypix tensorflow that extracts backgrounds?Where did you find the ressources for loading in the blazeface model  (or bodypix for that matter) - that step confuses me a little and I can see that you are self hosting? Can you explain this part?;"['The blazeface model is downloaded from TensorFlow Hub. Because some areas can not access the TensorFlow hub, so self hosting.  The TensorFlow models have been included in the folder ""/assets/blazeface_v1"" and  ""/assets/facemesh_v1"".=====']"
https://github.com/sanyuered/WeChat-MiniProgram-AR-TFJS/issues/1;Not working at all on Android.;1;closed;2020-04-01T05:29:51Z;2021-08-15T15:14:57Z;"I am using Andoird 10.When running the program on my mobile, it will remian  on the  ""warming up...""  status. When I removed the ""warmning up"" part, it will not detect any thing, I can only see :""onCamerFrame"" logged out. ";"['The program replaces ""face-api.js"" with ""face-landmarks-detection"". Please try the new program.=====']"
https://github.com/bensonruan/Face-Mask/issues/1;TypeError: t.toFloat is not a function;2;closed;2021-04-24T16:39:42Z;2021-05-08T16:02:07Z;I was trying to understand how Tensorflow wroks on web. As your instruction I cloned your project and opened it on my browser. I faced the issue.`Uncaught (in promise) TypeError: t.toFloat is not a function    c https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    tidy https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    scopedRun https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    tidy https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    kn https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    estimateFaces https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    a https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    a https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    r https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    r https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    estimateFaces https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    detectFaces https://localhost/face/js/face-mask.js:109    <anonymous> https://localhost/face/js/face-mask.js:77    promise callback* https://localhost/face/js/face-mask.js:75`;"['This issue can be solved by downgrading tensorflow version to 2.6.0Replace below CDN in Index.html```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@2.6.0""></script>```that should work=====', '> > > This issue can be solved by downgrading tensorflow version to 2.6.0> > Replace below CDN in Index.html> > ```> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js""></script>> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0""></script>> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@2.6.0""></script>> ```> > that should workThank you so much!=====']"
https://github.com/mwdchang/tfjs-gan/issues/3;GPU memory crash;1;open;2020-02-14T21:31:52Z;2020-02-20T20:35:19Z;Hi @mwdchang, thank you for the awesome repo!I modified your example to train with some black and white images of hand drawn curves, just for my own learning purposes.My images are 50x50, so they are a bit bigger than the MNIST images. I'm running into a problem where Chrome's GPU process crashes after a couple of training iterations.Looking at the Chrome task manager, I noticed that the GPU process retains memory and never garbage collects after training.Do you have any suggestions? I was thinking about caching the weights but I was not sure how to do that.Thanks again;['@freeman-g you might be able to alleviate some of the memory pressure by reducing the sizes of the hidden layers. Having said that, this was created as a proof-of-concept/toy-example and not meant for heavy-duty usage.=====']
https://github.com/mwdchang/tfjs-gan/issues/1;[Question]: Make DC GAN;1;open;2019-01-29T01:57:28Z;2019-01-30T02:22:19Z;"Hi man, great work. I'm trying to make a GAN or a DCGAN for learning. Taking from your project, should i just replace the gen and disReal and have my deep convolution models there? I'm following a python project but sometimes it's hard to ""port"" the ideas to tfjs.I want to input my own images for training and have it output a larger image. How big you would say it's feasible? 128, 256, more? It becomes computationally very expensive right?Thanks,Tiago Vasconcelos ";['In general yes - you would just replace the generator and discriminator. Training time/convergence are the issues in my opinion. GAN can be difficult to train. While I suppose there are no hard upper bounds in terms of image sizes, if you want to scale up you will mostly likely want a more sophisticated setup that (tries to) get around GAN pitfalls than the simple toy example I have here :)=====']
https://github.com/mwdchang/tfjs-gan/issues/2;Update to TFJS version 1.1.2;4;closed;2019-05-05T14:58:06Z;2019-07-15T03:58:23Z;Great example. I really appreciate the live githubPages pure javascript format.Getting zeroslike errors when updating to TFJS v1.1.2 any suggestions? Works fine on Version 0.15.3, but breaks at Version 1.0.0;"['https://github.com/tensorflow/tfjs/issues/1398#issuecomment-490806579=====', ""Thanks for the report/information! Will play around with upgrading options when there's some free time on the weekends.====="", 'https://github.com/tensorflow/tfjs/issues/1211#issuecomment-491829787=====', 'Bumped to 1.2.2=====']"
https://github.com/kevinisbest/FrontEnd-EmotionDetection/issues/1;EmotionModel.'Predict' of undefined at onPlay;2;closed;2019-10-23T06:22:37Z;2020-07-31T11:46:00Z;I am loading the TinyfaceDetectWebcam.html, where the camera is getting stuck and I could see the below issue on console.Uncaught (in promise) TypeError: Cannot read property 'predict' of undefined    at onPlay (TinyFaceDetectWebcam.html:165)It was working before, but i don't know why its not working now. Could be reference issue which I am not sure.![image](https://user-images.githubusercontent.com/15680777/67364135-9117e680-f58c-11e9-914f-af1f742dfde8.png)EmotionModel is undefined, sometimes not.I am not sure whether it is dependent on where we host because in local WampServer I don't have any issue but when I hosted on Remote Server and enabled openSSL for secure connection I see this issue.;"[""I have the same issue and I don't know how to solve it====="", 'Hi @OllyOli @shreeraj04,Sorry for the late reply, please pull the latest version and try again.I modified the paths of the model and the library.=====']"
