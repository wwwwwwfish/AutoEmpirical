{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fa862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm,trange\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from sklearn.metrics import accuracy_score\n",
    "import demjson3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4048fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartJSONExtractor:\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # \n",
    "        self.json_patterns = [\n",
    "            r'```json\\s*\\n(.*?)\\n```',  # \n",
    "            r'```\\s*json\\s*\\n(.*?)\\n```',  # \n",
    "            r'```\\s*\\n(.*?)\\n```',  # ```\n",
    "            r'```json(.*?)```',  # \n",
    "            r'```(.*?)```',  # \n",
    "            r'`(.*?)`',  # \n",
    "        ]\n",
    "        \n",
    "        # \n",
    "        self.object_patterns = [\n",
    "            r'(\\{(?:[^{}]|{[^{}]*})*\\})',  # \n",
    "            r'(\\[(?:[^\\[\\]]|\\[[^\\[\\]]*\\])*\\])',  # \n",
    "        ]\n",
    "    \n",
    "    def extract_json(self, text: str) -> Optional[Dict[Any, Any]]:\n",
    "\n",
    "        json_obj = self._extract_from_code_blocks(text)\n",
    "        if json_obj:\n",
    "            return json_obj\n",
    "\n",
    "        json_obj = self._extract_from_text(text)\n",
    "        if json_obj:\n",
    "            return json_obj\n",
    "        \n",
    "        json_obj = self._extract_with_fixes(text)\n",
    "        if json_obj:\n",
    "            return json_obj\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_from_code_blocks(self, text: str) -> Optional[Dict[Any, Any]]:\n",
    "\n",
    "        for pattern in self.json_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                json_obj = self._try_parse_json(match.strip())\n",
    "                if json_obj:\n",
    "                    return json_obj\n",
    "        return None\n",
    "    \n",
    "    def _extract_from_text(self, text: str) -> Optional[Dict[Any, Any]]:\n",
    "\n",
    "        for pattern in self.object_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                json_obj = self._try_parse_json(match.strip())\n",
    "                if json_obj:\n",
    "                    return json_obj\n",
    "        return None\n",
    "    \n",
    "    def _extract_with_fixes(self, text: str) -> Optional[Dict[Any, Any]]:\n",
    "\n",
    "        fixes = [\n",
    "            lambda x: x.replace('\"\"', '\"'),  # \n",
    "            lambda x: re.sub(r'(\\w+):', r'\"\\1\":', x),  # \n",
    "            lambda x: re.sub(r':\\s*([^\",\\[\\]{}]+)(?=\\s*[,}])', r': \"\\1\"', x),  # \n",
    "            lambda x: x.replace(\"'\", '\"'),  # \n",
    "            lambda x: re.sub(r',\\s*}', '}', x),  #\n",
    "            lambda x: re.sub(r',\\s*]', ']', x),  \n",
    "        ]\n",
    "        \n",
    "        for fix in fixes:\n",
    "            try:\n",
    "                fixed_text = fix(text)\n",
    "\n",
    "                json_obj = self._extract_from_code_blocks(fixed_text)\n",
    "                if json_obj:\n",
    "                    return json_obj\n",
    "                json_obj = self._extract_from_text(fixed_text)\n",
    "                if json_obj:\n",
    "                    return json_obj\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_parse_json(self, text: str) -> Optional[Dict[Any, Any]]:\n",
    "\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def extract_all_json(self, text: str) -> List[Dict[Any, Any]]:\n",
    "\n",
    "        json_objects = []\n",
    "        \n",
    "\n",
    "        for pattern in self.json_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                json_obj = self._try_parse_json(match.strip())\n",
    "                if json_obj:\n",
    "                    json_objects.append(json_obj)\n",
    "        \n",
    "\n",
    "        for pattern in self.object_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                json_obj = self._try_parse_json(match.strip())\n",
    "                if json_obj and json_obj not in json_objects:\n",
    "                    json_objects.append(json_obj)\n",
    "        \n",
    "        return json_objects\n",
    "\n",
    "def extract_categories(text):\n",
    "\n",
    "    pattern = r'\\[([A-Z](?:\\.\\d+)*)\\]'\n",
    "    \n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def extract_categories_plus(text):\n",
    "\n",
    "    categories = []\n",
    "\n",
    "    pattern1 = r'\\[([A-Z](?:\\.\\d+)*)\\]'\n",
    "    matches1 = re.findall(pattern1, text)\n",
    "    categories.extend(matches1)\n",
    "    \n",
    "    pattern2 = r'([A-Z](?:\\.\\d+)*)(?=\\[)'\n",
    "    matches2 = re.findall(pattern2, text)\n",
    "    categories.extend(matches2)\n",
    "\n",
    "    pattern3 = r'([A-Z](?:\\.\\d+)*)(?=\\s+(?![[\\]]))'\n",
    "    matches3 = re.findall(pattern3, text)\n",
    "\n",
    "    for match in matches3:\n",
    "\n",
    "        match_pos = text.find(match)\n",
    "        if match_pos != -1:\n",
    "            after_match = text[match_pos + len(match):match_pos + len(match) + 1]\n",
    "            if after_match != '[': \n",
    "                categories.append(match)\n",
    "    \n",
    "\n",
    "    seen = set()\n",
    "    unique_categories = []\n",
    "    for category in categories:\n",
    "        if category not in seen:\n",
    "            seen.add(category)\n",
    "            unique_categories.append(category)\n",
    "    \n",
    "    return unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4728ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 29266.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7120\n",
      "Processing deepseek-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 29623.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7500\n",
      "Processing claude-3-7-sonnet-20250219...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 31768.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7760\n",
      "Processing o3-2025-04-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 500/500 [00:00<00:00, 32222.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7560\n",
      "Processing gemini-2.5-flash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 32021.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## analyze the accuracy of filtering issues by LLM\n",
    "\n",
    "model_list = [\"gpt-4o-mini\", \"deepseek-chat\",\"claude-3-7-sonnet-20250219\",\"o3-2025-04-16\",\"gemini-2.5-flash\"]\n",
    "\n",
    "for model in model_list:\n",
    "    print(f'Processing {model}...')\n",
    "    df = pd.read_csv(f'./res/Filteration_{model}.csv')\n",
    "\n",
    "    pred_labels = []\n",
    "    # scan each data, if the Accept in LLM_classification,  then append 1 to pred_labels, otherwise append 0\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if 'Accept' in row['LLM_classification']:\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "\n",
    "    # take the 'label' column from df to a list, and caculate the accuracy of the pred_labels, wri\n",
    "    label_list = df['label'].tolist()\n",
    "    accuracy = accuracy_score(label_list, pred_labels)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e0de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing labeling results of gpt-4o-mini...\n",
      "Symptom correctness: {'0': 0.6746666666666666, '1': 0.5260869565217391, '2': 0.543859649122807, 'leaf': 0.5740223463687151}\n",
      "Root cause correctness: {'0': 0.5493333333333333, '1': 0.34782608695652173, '2': 'NaN', 'leaf': 0.33519553072625696}\n",
      "--------------------------------\n",
      "Processing labeling results of deepseek-chat...\n",
      "Symptom correctness: {'0': 0.7650918635170604, '1': 0.6345291479820628, '2': 0.6090909090909091, 'leaf': 0.6480446927374302}\n",
      "Root cause correctness: {'0': 0.6150627615062761, '1': 0.4393939393939394, '2': 'NaN', 'leaf': 0.4064245810055866}\n",
      "--------------------------------\n",
      "Processing labeling results of claude-3-7-sonnet-20250219...\n",
      "Symptom correctness: {'0': 0.7568681318681318, '1': 0.5995762711864406, '2': 0.7204301075268817, 'leaf': 0.6730205278592375}\n",
      "Root cause correctness: {'0': 0.6524926686217009, '1': 0.5070866141732283, '2': 'NaN', 'leaf': 0.47214076246334313}\n",
      "--------------------------------\n",
      "Processing labeling results of o3-2025-04-16...\n",
      "Symptom correctness: {'0': 0.7994186046511628, '1': 0.7627118644067796, '2': 0.8028169014084507, 'leaf': 0.7536656891495601}\n",
      "Root cause correctness: {'0': 0.6101190476190477, '1': 0.4805194805194805, '2': 'NaN', 'leaf': 0.46920821114369504}\n",
      "--------------------------------\n",
      "Processing labeling results of gemini-2.5-flash...\n",
      "Symptom correctness: {'0': 0.639943741209564, '1': 0.7209302325581395, '2': 0.6833333333333333, 'leaf': 0.6363636363636364}\n",
      "Root cause correctness: {'0': 0.5303703703703704, '1': 0.5240847784200385, '2': 'NaN', 'leaf': 0.4912023460410557}\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1255252/266911309.py:169: FutureWarning: Possible nested set at position 27\n",
      "  matches3 = re.findall(pattern3, text)\n"
     ]
    }
   ],
   "source": [
    "## analyze the accuracy of labeling bugs by LLM\n",
    "# extractor = SmartJSONExtractor()\n",
    "\n",
    "model_list = [\"gpt-4o-mini\", \"deepseek-chat\",\"claude-3-7-sonnet-20250219\",\"o3-2025-04-16\",\"gemini-2.5-flash\"]\n",
    "\n",
    "\n",
    "\n",
    "for model in model_list:\n",
    "    print(f\"Processing labeling results of {model}...\")\n",
    "\n",
    "    symptom_correctness = {\n",
    "    '0':[],\n",
    "    '1':[],\n",
    "    '2':[],\n",
    "    'leaf':[]\n",
    "    }\n",
    "\n",
    "    root_cause_correctness = {\n",
    "        '0':[],\n",
    "        '1':[],\n",
    "        '2':[],\n",
    "        'leaf':[]\n",
    "    }\n",
    "    df = pd.read_csv(f'./res/Labeling_{model}.csv')\n",
    "\n",
    "    symptom_pred_ids = []\n",
    "    root_cause_pred_ids = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        llm_cls = str(row['LLM_classification'])\n",
    "\n",
    "        split_idx = llm_cls.find('\"root_cause\"')\n",
    "        if split_idx == -1:\n",
    "\n",
    "            symptom_part = \"\"\n",
    "            root_cause_part = \"\"\n",
    "        else:\n",
    "            symptom_part = llm_cls[:split_idx]\n",
    "            root_cause_part = llm_cls[split_idx:]\n",
    "\n",
    "        if model == 'gemini-2.5-flash':\n",
    "            symptom_ids = extract_categories_plus(symptom_part)\n",
    "            root_cause_ids = extract_categories_plus(root_cause_part)\n",
    "        else:\n",
    "            symptom_ids = extract_categories(symptom_part)\n",
    "            root_cause_ids = extract_categories(root_cause_part)\n",
    "\n",
    "        symptom_ids_sorted = sorted(symptom_ids, key=len)\n",
    "        root_cause_ids_sorted = sorted(root_cause_ids, key=len)\n",
    "        symptom_pred_ids.append(symptom_ids_sorted)\n",
    "        root_cause_pred_ids.append(root_cause_ids_sorted)\n",
    "\n",
    "        gt_symptom_id = row['symptom_id']\n",
    "        gt_root_cause_id = row['root_causes_id']\n",
    "\n",
    "        level_symptom = gt_symptom_id.count('.')\n",
    "        level_root_cause = gt_root_cause_id.count('.')\n",
    "\n",
    "        for i in range(len(symptom_ids_sorted)):\n",
    "            cur_level = int(symptom_ids_sorted[i].count('.'))\n",
    "            if cur_level >  level_symptom:\n",
    "                break\n",
    "            if symptom_ids_sorted[i] in gt_symptom_id:\n",
    "                symptom_correctness[str(cur_level)].append(1)\n",
    "            else:\n",
    "                symptom_correctness[str(cur_level)].append(0)\n",
    "\n",
    "        for i in range(len(root_cause_ids_sorted)):\n",
    "            cur_level = root_cause_ids_sorted[i].count('.')\n",
    "            if cur_level >  level_root_cause:\n",
    "                break\n",
    "            if root_cause_ids_sorted[i] in gt_root_cause_id:\n",
    "                root_cause_correctness[str(cur_level)].append(1)\n",
    "            else:\n",
    "                root_cause_correctness[str(cur_level)].append(0)\n",
    "        \n",
    "        symptom_correctness['leaf'].append(symptom_correctness[str(level_symptom)][-1])\n",
    "        root_cause_correctness['leaf'].append(root_cause_correctness[str(level_root_cause)][-1])\n",
    "\n",
    "    for level in symptom_correctness.keys():\n",
    "        if len(symptom_correctness[level]) > 0:\n",
    "            symptom_correctness[level] = sum(symptom_correctness[level]) / len(symptom_correctness[level])\n",
    "        else:\n",
    "            symptom_correctness[level] = 'NaN'\n",
    "    for level in root_cause_correctness.keys():\n",
    "        if len(root_cause_correctness[level]) > 0:\n",
    "            root_cause_correctness[level] = sum(root_cause_correctness[level]) / len(root_cause_correctness[level])\n",
    "        else:\n",
    "            root_cause_correctness[level] = 'NaN'\n",
    "    print(f\"Symptom correctness: {symptom_correctness}\")\n",
    "    print(f\"Root cause correctness: {root_cause_correctness}\")\n",
    "    print(f\"--------------------------------\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
