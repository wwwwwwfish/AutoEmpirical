Faults,title,comments,state,created_at,updated_at,body,comments_content,sub-symptom,symptoms,root causes,sub-component,components of Tensorflow.js,fix,fix patterns,3-level architecture,development stages,LLM_classification,symptom_id,root_causes_id
https://github.com/justadudewhohacks/face-api.js/issues/550,it is possible to use backend tfjs-backend-wasm or webgl?,2,open,2020-02-12T16:36:38Z,2020-04-07T23:23:13Z,it is possible to use backend tfjs-backend-wasm or webgl?performance is much betterhttps://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm![Captura de pantalla 2020-02-12 a la(s) 13 34 59](https://user-images.githubusercontent.com/731936/74356052-7caa6480-4d9c-11ea-93d5-02d685f32fce.png),"[""The WebGL backend is utilized by default; I didn't try tfjs-backend-wasm yet; but if all the ops are supported by the backend yet then yes you should be able to use it as well.=====""; ""It looks like the `fill` op is a dependency for `tinyFaceDetector` and `tfjs-backend-wasm` doesn't yet support it:```Uncaught (in promise) Error: 'fill' not yet implemented or not found in the registry. Did you forget to import the kernel?```(I didn't forget to import the kernel 😄)List of available kernels: https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm/src/kernels=====""]",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change backend,changing backend,framework,Model loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/justadudewhohacks/face-api.js/issues/816,Minimizing memory and CPU consumption with Face API,1,open,2021-09-12T15:16:51Z,2021-09-13T02:32:37Z,I am using Face API to detect faces on a webcam. I call the detectSingleFace() function every 500ms. Everything works fine; but I can see that RAM and CPU usage is increasing. I realize that this is normal; finally face detection needs some resources; but I am asking if there is a way to optimize this. I am using the tiny models version. Maybe the ram consumption is related to storing the model in the browser memory; because it weighs a bit (if so; will cdn help something)?,"[""assuming you have no memory leaks in your code (check for `tf.engine().memory.numTensors` if it's growing after each frame); it's really not up to `face-api`; its up to browser to perform garbage collection  and modern browsers are lazy doing so as they value responsiveness more than anything  if youre using `webgl` backend; you can force gpu memory deallocation (which does slow things down) with `tf.ENV.set('WEBGL_DELETE_TEXTURE_THRESHOLD'; 0)`  another thing is that this `face-api` is using very old `tfjs` v1.x while latest is v3.9.0 - but if memory is your biggest concern; newer tfjs definitely doesn't use less memory  =====""]",Memory Leak,Poor Performance,Browser Incompatibility,Browser,Platform,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,D.2
https://github.com/justadudewhohacks/face-api.js/issues/793,Unhandled Rejection (Error): Kernel 'Identity' not registered for backend 'webgl',1,open,2021-05-10T18:48:59Z,2021-06-17T18:42:44Z,![image](https://user-images.githubusercontent.com/42316496/117709333-6cc33f80-b1ee-11eb-967c-7139bf02a17e.png),['Same for me : (====='],Reference Error,Crash,Unimplemented Operator,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/justadudewhohacks/face-api.js/issues/776,error api face in Motorola E5,8,open,2021-04-07T21:34:09Z,2021-04-13T02:51:07Z,![MicrosoftTeams-image (1)](https://user-images.githubusercontent.com/24438895/113937590-65f87580-97c7-11eb-9c2a-20360c0a5c74.png),"[""Insufficient GPU memory to compile GL shaders.  In general; if you don't have a dedicated GPU; use of `WebGL` backend is not recommended in any machine learning project - Use `WASM` backend instead.=====""; 'how can I use backend wasm in this case?====='; ""after `tfjs` has been loaded; but before `faceapi` has been used:```jsawait tf.setWasmPaths('./path-to-wasm-files/');await tf.setBackend('wasm');```and you do need to provide *.wasm files (part of `tfjs` package itself); typically in `node_modules/@tensorflow/tfjs-backend-wasm/dist` or download from any CDN  however; in this case quite old as `faceapi` uses `tfjs` 1.7.0 and `wasm` version must match  or use a newer port of `faceapi`: <https://github.com/vladmandic/face-api> that's compatible with tfjs 2.x and 3.x  also; recommended to enable `SIMD` as performance is higher by order of magnitude  for example; go to <chrome://flags> and enable `WebAssembly SIMD support`=====""; 'I\'m usingimport * as faceapi from ""components / lib / face-api.esm"";await faceapi.tf.setWasmPaths (""../ statics /"");      await faceapi.tf.setBackend (""wasm"");but the compilation is very slow; because the file weighs 3MB.I am using this face-api.esm because it was the one that solved the problem of Insufficient GPU memory to compile GL shaders.In general; if you don\'t have a dedicated GPU; use of WebGL backend is not recommended in any machine learning project - Use WASM backend instead.What other option do I have?====='; ""@deylyn if you're using `https://github.com/vladmandic/face-api`; please post issues there  also; size of js file has little to do with execution speed - but yes; it can be minimized to around 1.3mb if that means anything    since your device is really low on memory; make sure you're using `tinyFaceDetector` model; not `ssdMobilenetv1` - it's slightly less accurate; but far less memory demanding. notes on how to use it are in the docs.=====""; 'if i am using tinyFaceDetectorimport * as faceapi from ""components/lib/face-api.esm"";  var backendWasm = await this.backendWasm();    Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri(""/statics/models"")]);this.video.addEventListener(        ""play"";        function() {          self.canvasFace = document.getElementById(""c1"");          self.displaySize = {            width: self.videoWidth;            height: self.videoHeight          };          faceapi.matchDimensions(self.canvasFace; self.displaySize);          self.timerCallback();        };        false      ); timerCallback: function() {      if (this.video.paused || this.video.ended) {        return;      }      const box = { x: 75; y: 37.5; width: 150; height: 225 };      var lenghtx = box.x + box.width;      var lenghty = box.y + box.height;      // see DrawBoxOptions below      const drawOptions = {        lineWidth: 2;        boxColor: ""#23b2be""      };      const drawBox = new faceapi.draw.DrawBox(box; drawOptions);      drawBox.draw(this.canvasFace);      let inputSize = 128;      let scoreThresholds = 0.1;      this.id = setInterval(async () => {        let self = this;        const useTinyModel = true;        const detections = await faceapi.detectAllFaces(          self.video;          new faceapi.TinyFaceDetectorOptions({ inputSize; scoreThresholds })        );        // resize the detected boxes in case your displayed image has a different size than the original        const resizedDetections = faceapi.resizeResults(          detections;          self.displaySize        );        let boxArea = 35000;        if (self.$q.screen.xs) {          boxArea = 35000;        } else {          boxArea = 21000;        }        if (resizedDetections[0]) {          let anchBoundBox = resizedDetections[0].box[""width""] - 15;          let xBox = box.x - 35;          let altBoundBox = resizedDetections[0].box[""height""] - 15;          let yBox = box.y + 45;          if (            anchBoundBox + resizedDetections[0].box[""x""] > lenghtx ||            resizedDetections[0].box[""x""] < xBox ||            resizedDetections[0].box[""y""] < yBox ||            altBoundBox + resizedDetections[0].box[""y""] > lenghty          ) {            self.validCentered = false;            self.textAlert = ""Centre su rostro"";            self.paintBoxAlert(self.textAlert);            self.$emit(""detect-centered""; false);          } else {            self.validCentered = true;            self.$emit(""detect-centered""; true);          }          if (resizedDetections[0].box.area > boxArea) {            self.validProximity = true;            self.$emit(""detect-proximity""; true);          } else {            self.textAlert = ""Acerque su rostro"";            self.paintBoxAlert(self.textAlert);            self.validProximity = false;            self.$emit(""detect-proximity""; false);          }        } else {          self.validProximity = false;          self.validCentered = false;        }        if (self.validCentered && self.validProximity) {          self.canvasFace            .getContext(""2d"")            .clearRect(0; 0; self.canvasFace.width; 36);        }           }; 250);    }====='; ' async backendWasm() {      await faceapi.tf.setWasmPaths(""../statics/"");      await faceapi.tf.setBackend(""wasm"");   };====='; ""@deylyn like I said - if you're using https://github.com/vladmandic/face-api; please post issues therebut there are several major issues here:- you're creating new instance of model on each frame: `new faceapi.TinyFaceDetectorOptions({ inputSize; scoreThresholds })`    create it outside of the loop as a variable- your loop is a 250ms fixed loop which is a great way to create race conditions    don't use `setInterval`; refactor the code to call itself upon completion using `requestAnimationFrame`- reduce `scoreThreshold` to a reasonable value  low values mean that library has to do far more processing for all possible false-positives=====""]",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/771,Difference between running the algorithm on browser and nodeJS,6,open,2021-03-18T10:57:24Z,2021-04-26T18:38:53Z,"Node Version: 14.16.0Face-api.js : master clonedChrome version :  89.0.4389.90 (Official Build) (64-bit)I was running tests to see the difference between how the algorithm performs on the two platforms. For some reason; nodeJS is able to find images in the same image in which browser fails to find. An example of this would be : `George_W_Bush_0137.jpg` from the Labelled faces in the wild dataset. Shouldn't the encoding algorithm be platform agnostic. Why exactly does this happen?Standalone code to encode on browser:```<!DOCTYPE html><html><head>  <script src=""../../../dist/face-api.js""></script>  <script src=""../public/js/commons.js""></script>  <script src=""../public/js/faceDetectionControls.js""></script>  <!-- <link rel=""stylesheet"" href=""styles.css""> -->  <!-- <link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css""> -->  <script type=""text/javascript"" src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>  <!-- <script src=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js""></script> --></head><body>  <input type=""file"" id=""file"" name=""file""/><br><br>    <button id = ""run"" onclick=""run()"">Run</button><br><br>  <script>async function run(){    await faceapi.loadSsdMobilenetv1Model('../../../weights')    await faceapi.loadFaceLandmarkModel('../../../weights')    await faceapi.loadFaceRecognitionModel('../../../weights')    const inputElement = document.getElementById(""file"");    console.log(inputElement.files[0]);    const img = await faceapi.bufferToImage(inputElement.files[0]);    const encoding = await calculateEncoding(img);    console.log(encoding);}async function calculateEncoding(img){      //We want to detect the best face in each file      const result = await faceapi.detectSingleFace(img; getFaceDetectorOptions())      .withFaceLandmarks()      .withFaceDescriptor()      //only if face found      if(result != undefined)          return result.descriptor;      else         return undefined;}  </script></body></html>```NodeJS:```//encodes one image; path of which is suppliedconst fsPromises = require('fs/promises');const path = require('path');const process = require('process');const faceapi = require('face-api.js')const { canvas; faceDetectionNet; faceDetectionOptions } = require('./commons');async function run(){    //load models    await faceDetectionNet.loadFromDisk('../../weights')    await faceapi.nets.faceLandmark68Net.loadFromDisk('../../weights')    await faceapi.nets.faceRecognitionNet.loadFromDisk('../../weights')    const encoding = await calculateEncoding(process.argv[2]);    console.log(encoding);}async function calculateEncoding(imagePath){    const img = await canvas.loadImage(imagePath);        //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img; faceDetectionOptions)    .withFaceLandmarks()    .withFaceDescriptor()    //only if face found    if(result != undefined)        return result.descriptor;    else         return undefined;}run();        ```","[""what are actual detect options? in browser case; you're calling undocumented function `getFaceDetectorOptions` and in node case; you're importing from `commons` - neither is known.also; why the difference in loading (plus mix&match methods)? use `faceapi.nets.ssdMobilenetv1.load` and `faceapi.nets.ssdMobilenetv1.loadFromDisk`=====""; ""I've been using the examples provided. They point to the same code; but to make sure that wasn't the reason; here are the updated partsnodeJS:```    await faceapi.nets.ssdMobilenetv1.loadFromDisk('../../weights')    await faceapi.nets.faceLandmark68Net.loadFromDisk('../../weights')    await faceapi.nets.faceRecognitionNet.loadFromDisk('../../weights')-----    //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img; new faceapi.SsdMobilenetv1Options({ minConfidence }))    .withFaceLandmarks()    .withFaceDescriptor()```Browser:```    await faceapi.nets.ssdMobilenetv1.load('../../../weights')    await faceapi.nets.faceLandmark68Net.load('../../../weights')    await faceapi.nets.faceRecognitionNet.load('../../../weights')        //We want to detect the best face in each file    const result = await faceapi.detectSingleFace(img; new faceapi.SsdMobilenetv1Options({ minConfidence }))    .withFaceLandmarks()    .withFaceDescriptor()```The results are still the same. =====""; 'This made me really curious...I wrote a quick test and moved image loading and conversion to tensor out of FaceAPI to eliminate that part  And yes; `Browser` and `NodeJS` return different values  But in `Browser`; both `WebGL` and `WASM` return same values  So it\'s not a image loading and not backend specific  And it\'s not a different model as FaceAPI makes no distinction when running inference on browser vs node  So what\'s different? This is just a guess; but `tf.image.resizeBilinear` has different defaults in TFJS and TF  For example; property `alignCorners` is not specified by FaceAPI; so it\'s interpreted as `false` in TFJS and `true` in TF  (and there are probably more instances of similar cases)  It makes sense as when you look at result outputs; face bounding box is slightly shifted (by about half a pixel)And that\'s enough to create a small cascading differences in descriptor and elsewhereNow; if there is an actual reason to ""fix"" this;  I can do that in my newer fork <https://github.com/vladmandic/face-api>  Otherwise; this will stay as curiosity  ====='; 'Yes; has me baffled for a couple of days. I tried that. The flow doesn\'t even go in the conditional loop for the example image mentioned.```          if (imgTensor.shape[1] !== inputSize || imgTensor.shape[2] !== inputSize) {            imgTensor = tf.image.resizeBilinear(imgTensor; [inputSize; inputSize]; true);            console.log(""resizeBilinear alignCorners arg changed to true"");          } ```  But do check it on your end as I\'m not familiar with the codebase and there might be something that I\'m missing.   Don\'t you think fixing this or at least finding the root cause would be beneficial as it looks like the browser is underperforming. However; I\'ve only found two instances (yet) so concluding that browser always underperforms might be naive. Yes; I did look at your fork. Kudos on the work!. Should I open an issue in your fork; so the problem can be resolved there.    ====='; ""yes; i was wrong. but at least i managed to find it at the end :)seems its an actual bug in implementation of `tf.conv2d`  actual function in FaceAPI is `pointwiseConvLayer` which is literary first operation performed in `mobileNetV1` model implementation - so this tiny change cascades down to eventually create a difference you're seeing.see <https://github.com/tensorflow/tfjs/issues/4843> for details  there is one more difference between browser and nodejs and that's the initial jpg decoding which shifts pixel values by one (not coordinate offset; but rgb values itself). so simply adding `tf.add(input; 1)` to nodejs decode workflow solves it.=====""; '@mayankagarwal-cf just closing the loop...it seems that issue was `conv2d` implementation in old **TF1** binaries which were used by `tfjs-node`  `tfjs-node` **3.5.0** finally ships with TF2 binaries and this issue is resolved  =====']",Incorrect Functionality,Incorrect Functionality,Untimely Update,TF(CPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,B.3
https://github.com/justadudewhohacks/face-api.js/issues/764,Low Performance on Safari on iOS,3,open,2021-03-04T17:26:37Z,2021-03-05T15:21:42Z,Hello!My web page is using the face-api.js api to find the face expressions of human faces on mobile. On android all works fine; the video of the webcam is smooth; on Google Chrome and Firefox. On iOS using Chrome the video is smooth but on Safari is very laggy: it shows 2-3 frames and then stops and then other 2-3 frames and then stops and so on. Since we cannot use Chrome on iOS due to other problems; I wanna know if someone else has encountered the same problem and how did he solve it.Thank you all for the support.,"[""Safari is lacking full support for GL version 2; so if you're using `webgl` backend; it will be lacking.  Check if `wasm` backend is an option for you; although I don't know what's the state of SIMD support in Safari.=====""; ""I've managed to solve the issue reducing the inputSize from 512x512 to 128x128. Anyway; I will go to inform about it!Thanks @vladmandic =====""; '@lucaTriboli I have the same issue; face-api use GPU hardware; so I try to install a graphic card; and work fine. Check the system resource monitor when the app is running.=====']",Slow Execution,Poor Performance,Browser Incompatibility,Browser,Platform,change input size,Replace data Shape/type,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",B.1.1,D.2
https://github.com/justadudewhohacks/face-api.js/issues/734,Slow performance & Errors with 2 1080s,4,open,2020-12-04T20:04:59Z,2020-12-07T17:30:38Z,I am batch processing a few hundred images. Running detectAllFaces().withFaceLandmarks().withFaceDescriptors()Running with tfjs-node It takes about 200 secondsRunning with TFJS-node GPU it takes about 100 seconds...About 80 seconds in (image 1682/1745) I start to get buffer allocation errors. `2020-12-04 15:03:04.531673: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocating tensor with shape[1;64;64;256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`Was really hoping for more than double CPU performance. GPUs only reach about 20% utilization. Situation does not change with only one 1080 present in the system.Each image is called with:```images.forEach(image => {    const tensor = decodeImage(image)    const faces = await faceapi.detectAllFaces(tensor; detectFacesOptions).withFaceLandmarks().withFaceDescriptors()})```Any ideas?,"[""Can confirm that the performance is also terribly slow for browser as well. But that's the case at least if you use windows. Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces. ![fr-slow](https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gif)On linux it works quite alright; taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:https://bugzilla.mozilla.org/show_bug.cgi?id=1678652https://bugzilla.mozilla.org/show_bug.cgi?id=1679671In short; for firefox v83; there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU; which is rather slow. However; that's true for linux only for now. On windows it still uses webgl; and still rather slow (20-30 seconds) for any browser.I am not sure if that's the issue of face-api.js or tensorflow itself=====""; ""> Can confirm that the performance is also terribly slow for browser as well. But that's the case at least if you use windows.> Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces.> ![fr-slow](https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gif)> > On linux it works quite alright; taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:> https://bugzilla.mozilla.org/show_bug.cgi?id=1678652> https://bugzilla.mozilla.org/show_bug.cgi?id=1679671> > In short; for firefox v83; there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU; which is rather slow.> However; that's true for linux only for now. On windows it still uses webgl; and still rather slow (20-30 seconds) for any browser.> > I am not sure if that's the issue of face-api.js or tensorflow itselfFaces are recognized almost instantly. I mean milliseconds; its incredibly fast. I'm watching very subtle spikes in GPU 3D usage; and vRam is sitting at 1.5 GB after models are loaded.It seems to be an issue specifically with tfjs-node-gpu.Would be great if I could get GL to work with node out of the browser.**Also* I wrote a program that dispatches instances of faceAPI with their own tfjs-node backend. Running multiple threads quadrupled my performance when running solely on the CPU. However the moment I switch to tfjs-node-gpu I get the issue described here**https://github.com/tensorflow/tfjs/issues/4362=====""; ""To clarify; I'm getting significantly more performance (with multithreading) out of tfjs-node than tfjs-node-gpu. This certainly does not hold true in the browser.FWI I have 2 GTX 1080s; Windows 10=====""; ""On another note; to troubleshoot the issue; I found a fork of FaceAPI that runs the latest versions of TensorFlow. The exact same issues persist; Here is the issue I posted there:https://github.com/vladmandic/face-api/issues/20I also tried multithreading with the fork as described above. Again; significant performance boost from multithreading. Multithreading doesn't work on the GPU because of the issues with tjfs-node-gpu vram managment.=====""]",Out of Mermory,Poor Performance,Unknown,TF(GPU),Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B,E
https://github.com/justadudewhohacks/face-api.js/issues/541,Firefox 72.0.1 WebGL Errors and failed face detection,1,open,2020-01-31T02:03:30Z,2020-02-28T08:35:41Z,"## SummaryRunning Firefox 72.0.1; experiencing several WebGL errors in the console and face detection is failing. Same code ran in Chromium works great with no errors. I'm running Fedora 30 if it matters.## Additional Information**Screenshot**![image](https://user-images.githubusercontent.com/10566658/73506336-3f5cd480-43a3-11ea-886f-9d9a7121a14a.png)**Errors as Text**```Error: WebGL warning: readPixels: PIXEL_PACK_BUFFER must be null.Error: WebGL warning: readPixels: Buffer for `target` is null.```**OS Info** ```5.2.11-200.fc30.x86_64 #1 SMP Thu Aug 29 12:43:20 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux```**My Code**```  const foo = async () => {    const MODEL_URL = '/models'        console.log(""here"")    var result = faceapi.loadFaceLandmarkTinyModel(MODEL_URL)    await faceapi.loadTinyFaceDetectorModel('/models')    await faceapi.loadFaceLandmarkTinyModel('/models')    await result    console.log(result)    console.log(""i loaded it?"")    const input = document.getElementById('image')    const detections2 = await faceapi.detectAllFaces(input; new faceapi.TinyFaceDetectorOptions())    console.log(detections2)  }  foo()```Thank you for your hard work on this project!","[""Hmm; it's better to ask the tfjs team about issues with the WebGL backend; since I am not familar with the internals.=====""]",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.2
https://github.com/justadudewhohacks/face-api.js/issues/495,SSD Mobilenet v1: not detecting faces on some systems,3,open,2019-12-06T16:17:26Z,2019-12-16T15:40:08Z,I developed a demo code for face detection using face-api js library; below are the details of model I have used - **Model** : SSD Mobilenet v1**Function** : faceapi.detectAllFaces(canvasElement)Demo code is able to detect faces on other system; but it is not working on the systems mentioned below**System Hardware Info**:Laptop model : Lenovo Thinkpad L440CPU : Intel i5 - 4210MRAM : 8 GBGraphics : Intel HD Graphics 4600**System Software Info**:OS : Windows 7 ultimate 64-bitChrome version : 78.0.3904.108 (Official Build)(64-bit)Observations : - faceapi.detectAllFaces function returns no face but faces are present in the image coming from webcam- Intel HD Graphics driver stopped working and restarted in between few times.- Live Demo site with webcam face tracking is also not working properly with ssd mobilenet v1 model on this system,"['Hi @justadudewhohacks ;There seems an issue with the tfjs webgl backend; Face-api is detecting faces when we use CPU as tfjs backend.It is being fixed by tfjs team. But the current version of face-api is not supporting latest tfjs version.When are you planning to upgrade it?====='; 'This weekend.====='; ""Hi @justadudewhohacks;This Issue still persist with the latest version of face-api and tfjs.Code works properly with 'cpu' backend on tfjs but fails to give proper result on 'webgl' backend on tfjs on the system mentioned above.Any suggestions/solution?=====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/435,Using model SSD MobileNet v1 detects only 2 faces max,5,open,2019-10-11T03:32:27Z,2019-12-18T11:01:48Z,So; I have tested the API in multiple computers. All of them have really good hardware. On my desktop the picture bbt1.jpg will have all 4 faces detected. However; there are 2 computers that no matter if I clone the Face API with no modifications and try with 4 different browsers; the faces detected on the picture bbt1.jpg will only be Raj and Leonard only (2 faces max) this only occurs when using SSD MobileNet v1. If I run the same build on my phone this doesn't occur and all faces are detected. So; it seems that the problem is only on the computers. Any ideas about a similar issue?,"['Can you please give me the exact hardware information of the computers it is not working for (CPU; GPU; OS).There still seems to be some issues with Intel Graphics Cards / (CPUs?).Some reference issues: #43 #332 https://github.com/tensorflow/tfjs/issues/510====='; 'Here are the specs:CPU: Intel i7-4770 GPU: Intel HD Graphics 4600OS: Windows 10Both have the same====='; 'Hi @justadudewhohacks ;There seems an issue with the tfjs webgl backend; It is being fixed by tfjs team.But the current version of face-api is not supporting latest tfjs version.When are you planning to upgrade it?====='; 'Upgrade to the latest tfjs-core version will come this weekend.====='; ""Hi @justadudewhohacks;This Issue still persist with the latest version of face-api and tfjs.Code works properly with 'cpu' backend on tfjs but fails to give proper result on 'webgl' backend on tfjs on the system mentioned above with Intel GPU :Intel GPU:Intel HD Graphics 4600 + win 7 ultimateAny suggestions/solution?=====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/332,[Intel; IPhone 6S] Problem Processing face-api.js,4,open,2019-06-24T15:21:14Z,2019-10-12T11:27:42Z,I am building an application that uses face-api.js and it was working just fine until I started getting this error everytime I try to access the website```Uncaught (in promise) Error: Failed to link vertex and fragment shaders.    at Wr (face-api.min.js:1)    at t.createProgram (face-api.min.js:1)    at face-api.min.js:1    at face-api.min.js:1    at t.getAndSaveBinary (face-api.min.js:1)    at t.compileAndRun (face-api.min.js:1)    at t.conv2dWithIm2Row (face-api.min.js:1)    at t.conv2d (face-api.min.js:1)    at Zt.engine.runKernel.x (face-api.min.js:1)    at face-api.min.js:1```After that It just stops and doesnt load anything else or perform any another action; I am stuck at this point; I wonder if its because of my GPU; but I dont know. **Please Help me!Thanks.**,"['OK I realised that there might be a problem with Intel (R) Graphics 3000;  the problem is that the other issue featuring this error #327 got a solution which for me doesnt work any help would be appreciated====='; '> I am building an application that uses face-api.js and it was working just fine until I started getting this error everytime I try to access the websiteWhen exactly did it start throwing those errors? This could be an issue of your device not supporting the WebGL backend; running out of memory or some browser issue. Which browser + version are you running this on?====='; "" I am running the lastest Chrome Stable Version 75.0.3770.100; I also tested it in Microsoft Edge and Microsoft Edge Developer Version all of them fully updated and all of them threw the same error. Probably it's a problem with the WebGL Backend but I have no idea how to solve it It also doesnt work on my iPhone 6S running iOS 12 and iOS 13 (Uodated recently) but it may be due to another error in my code I dont know because I cant debug It so easily.=====""]",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/299,Degraded age detection performance of different systems,3,open,2019-05-22T05:41:13Z,2019-05-22T19:33:52Z,Hello;Recently I have integrated age & gender detection feature in my electron app. It is integrated and tested on laptop; and everything works perfect. Both age & gender prediction have very less error rate. But when I run the same application on another machine; the predictions are completely different than my machine. The error rate is very high for age detection. Gender detection works correctly.Below is the configuration of both the machines:-- My Machine (Works Perfect on this) --- DELL ( i3 processor; 8GB RAM; 2TB HDD; Intel HD Graphics; All drivers available)- Windows 10 64 bit- Electron 5.x- Node 10.15 latest LTS-- Target Machine ( Works Incorrrect )--- Lenovo ( i3 processor; 4 GB RAM; Intel HD Graphics; All drivers available)- Windows 7 64 bit- Electron 5.x- Node 10.15,"['Hey;There are still problems in the tfjs WebGL backend with both electron #262 #264 as well as Intel GPUs #43. The problem that you are seeing could be related to any of them or a combination thereof.Can you try to run the web age recognition example of this repo on your target machine? If this is not working properly outside of electron then it might be a precision issue with your Intel GPU. In any case; it still remains to be done to figure out the exact cause of these deviations from the expected results; e.g. which ops of tfjs-core are causing this; so that we can report more information to the tfjs team.====='; 'Well I tried the same using Chrome. I got same results. The result for my machine (that works perfectly) shows correct age (25 - 26 years).  and second machine (target) shows very less age (11 - 14 years).  I guess you can figure out which operation is taking it down.Secondly; I implemented face detection and recognition for live cams in electron and till now it is working good on staging deployment with 15-20 machines. Do you think it is a good idea to release my electron application with face recognition for 70 thousand computers around the country?Because; if this is a hardware issue ( precision issue of Intel GPU you mentioned ); what will be the success failure ratio of this feature for 70;000 machines? ====='; ""Hmm; hard to say. I am only aware of some precision issues on INTEL GPUs that remain using the WebGL backend of tfjs; although they did a lot of fixes there; as well as some issues with electron; but I didn't investigate further in the latter one.=====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/282,failed to link vertex and fragment shaders,9,open,2019-04-29T11:11:15Z,2021-02-19T03:29:04Z,"I import `""face-api.js"": {      ""version"": ""0.19.0"";      ""resolved"": ""https://registry.npmjs.org/face-api.js/-/face-api.js-0.19.0.tgz"";      ""integrity"": ""sha512-XbEIqqnLzlVIkvtLffFIl/Ltit3VAnJ1B3aFEc28pnkNVvqzJnBJbZVgbJ0JTvaafdSPJ5pFufC2t0KZJhwdpw=="";      ""requires"": {        ""@tensorflow/tfjs-core"": ""1.0.3"";        ""tfjs-image-recognition-base"": ""^0.5.1"";        ""tslib"": ""^1.9.3""      };      ""dependencies"": {        ""tslib"": {          ""version"": ""1.9.3"";          ""resolved"": ""https://registry.npmjs.org/tslib/-/tslib-1.9.3.tgz"";          ""integrity"": ""sha512-4krF8scpejhaOgqzBEcGM7yDIEfi0/8+8zDRZhNZZ2kjmHJ4hv3zCbQWxoJGz1iw5U0Jl0nma13xzHXcncMavQ==""        }      }    };`but I got an error in chrome 70 in windows; as follows:<img width=""487"" alt=""99"" src=""https://user-images.githubusercontent.com/4914876/56892535-859ea380-6ab2-11e9-9fd3-cb3accaa7224.png"">","['If can I update tensorflow to `""dependencies"": {    ""@tensorflow/tfjs-converter"": ""1.1.0"";    ""@tensorflow/tfjs-core"": ""1.1.0"";    ""@tensorflow/tfjs-data"": ""1.1.0"";    ""@tensorflow/tfjs-layers"": ""1.1.0""  }`====='; '![EADBE271B539976EDD5C697E318419FB](https://user-images.githubusercontent.com/4914876/56936442-051b8980-6b2a-11e9-9fc3-ce68d4c89101.png)chrome 73 also has same issue...![7B2F30A04C43DB51A733B2F78618E808](https://user-images.githubusercontent.com/4914876/56936510-51ff6000-6b2a-11e9-9cf9-46d619d60a14.jpg)How to solve it ? please help me ; thank you very much.====='; 'https://github.com/llSourcell/pose_estimation/issues/9give the answer; but I update to 1.0.4; still has the issue....sigh...====='; 'This either is a problem with your machine (I think the tfjs team has a sample page somewhere; where you can check the capabilites related to the WebGL backend); or the problem is due to unaligned package versions. face-api.js is currently using @tensorflow/tfjs-core 1.0.3; so you should use this version.====='; 'I got the same error. does you founded resolve to that? ====='; ""The error also occurs in our app; but only with chrome on older devices (8gb RAM; intel core i5 M 560 2.67 GHz). On the same devices it works with firefox.  On new devices it works with chrome as well as with firefox.![image](https://user-images.githubusercontent.com/13392042/59786015-94415400-92c6-11e9-9738-0b4ab9e3bbe5.png)Then; I set `tf.ENV.set('WEBGL_PACK'; false)` at the beginning of my app (from this [sstackoverflow-comment](https://stackoverflow.com/questions/55188619/ms-edge-script5022-failed-to-link-vertex-and-fragment-shaders)). On new devices it works again; on the older device the error `Box.constructor - expected box to be IBoundingBox | IRect; instead have ...` appears.![image](https://user-images.githubusercontent.com/13392042/59785982-7c69d000-92c6-11e9-8647-7e07740bd644.png)=====""; ""@PutziSan I got this error too. I resolved it by setting below specific inputSize:```jsnew faceapi.TinyFaceDetectorOptions({      inputSize: 256; // this line solves 'Box.constructor - expected box to be IBoundingBox | IRect; instead ...'      scoreThreshold: 0.5;    });```I believe blow inputSize also works:https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/tinyYolov2/TinyYolov2.ts#L27=====""; ""Hi; > @PutziSan> I got this error too. I resolved it by setting below specific inputSize:> > ```js> new faceapi.TinyFaceDetectorOptions({>       inputSize: 256; // this line solves 'Box.constructor - expected box to be IBoundingBox | IRect; instead ...'>       scoreThreshold: 0.5;>     });> ```> > I believe blow inputSize also works:> https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/tinyYolov2/TinyYolov2.ts#L27This didnt work for me. Is there any solution for Chrome ?=====""; 'I was having this error when running the html served in a node environment. Running it from traditional apache works fine.=====']",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Limits""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/278,Face expression probabilities random on chrome android,9,open,2019-04-26T08:31:17Z,2019-05-17T12:18:57Z,"After loading and detecting face from html video element; the probability distribution is correct for a few frames (happy 0.99; sad 0.0000343) etc.After a few processed frames; the probabilities jump all over the place (see screenshots below).The same code works fine when using it in chrome on my desktop machine (Nvidia GTX 1060)Android 8.0.0Chrome 73.0.3683.90Samsung Galaxy S7![probabilities](https://user-images.githubusercontent.com/16301487/56792792-b0ff6500-680a-11e9-9cdf-e6c362da217e.png)```const model = FaceApiFaceDetectorModel.TINY_FACE_DETECTOR;const modelBaseUrl = '/assets/tfmodels';const minConfidence = 0.5;const inputSize = 256;const scoreThreshold = 0.5;const minFaceSize = 200;await loadFaceDetectorModel(model; modelBaseUrl);await faceapi.loadFaceExpressionModel(modelBaseUrl);const options = getFaceDetectorOptions(model; minConfidence; inputSize; scoreThreshold; minFaceSize);const result = await faceapi.detectSingleFace(video; options).withFaceExpressions();```Might this be some kind of overflow?Edit: I tried it with the sample application (examples-browser) by updating the ""video face tracking"" example. The result is the same: Works on desktop; random values on mobile","[""That's odd; especially since these are not even valid probabilities anymore. Looks like something is going wrong in `tf.softmax` here.=====""; 'Maybe this is related: https://github.com/tensorflow/tfjs/issues/1488; although face-api does not do batch predictions; does it? The probabilities are correct on newer phones. Do you have any plans to upgrade your tfjs dependency? Might be this is already fixed.====='; 'The code that you posted does not do batch processing. Did you try to play around with enabling/disabling the specific features as @annxingyuan described in this issue? Does that help?====='; 'Yes; I tried all 3 suggestions. The predictions are ""better"" when setting `tf.ENV.set(\'WEBGL_PACK\'; false)` (no more huge numbers); but still there are negative values and values in the range from -100 to 100.====='; 'Ok. All I can do for now is upgrade tfjs-core to latest; which I am working on now.====='; ""Thank you; I'll let you know in this issue if that helped=====""; 'Ok I will have to postpone the upgrading; since tfjs-core 1.1.2 seems to not come with the platform specific check; which seems to be implemented in the current master. This breaks the unit tests; since the browser tests utilize a wrong fetch function.====='; 'Let me know if we can help====='; 'Update: seems to work now on chrome mobile 74.0.3729.136=====']",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/264,[electron] Face recognitions is not working properly in my project,22,open,2019-04-05T11:33:33Z,2020-03-18T13:53:45Z,I am doing a project in angular7 and electronjs. In my project I want to detect and recognise the face from given image. But I am facing a problem in face recognition it's not recognizing the face properly ![Screenshot from 2019-04-05 16-27-44](https://user-images.githubusercontent.com/34840376/55624393-505ca980-57c3-11e9-9c56-88534dfad676.png)But went I try with your example project face was recognizing properly.![Screenshot from 2019-04-05 16-29-46](https://user-images.githubusercontent.com/34840376/55624531-df69c180-57c3-11e9-8061-f6c99f90c572.png)![Screenshot from 2019-04-05 16-29-50](https://user-images.githubusercontent.com/34840376/55624532-e0025800-57c3-11e9-9f93-b2eef6c8b450.png),"['Hi !It is kind of hard to tell what is wrong in your project if we don\'t see your code. Could you add a code snippet with the involved code ?What do you want to achieve exaclty in first screenshot ? If I understand correctly; the first image is used to get face descriptor in order to populate a Face Matcher and you want to recognize this person in second image; am I right ?According to the few information we have; you are maybe not adding correctly face descriptor from first image to your face matcher.```_currentLabeledDescriptor = new faceapi.LabeledFaceDescriptors(""person1""; [_descriptorsFromFirstPicture])_localFaceMatcher  = new faceapi.FaceMatcher([_currentLabeledDescriptor])```Double check that _descriptorsFromFirstPicture is a Float32Array and that you have ""logical"" values (i;e; floats with a lot of digits and values in range [-1;1]).====='; 'yes; I want to recognise person from first image in Second image. Your App recognise it properly but my code is not recognising it.First Image Facedetection code( .ts ):    const firstFaceDescription = await faceapi      .detectSingleFace(img)      .withFaceLandmarks().withFaceDescriptor()    if (!isNullOrUndefined(firstFaceDescription))     {      const detectionsForSize = faceapi.resizeResults(firstFaceDescription;        { width: profileImgRef.width; height: profileImgRef.height })            const canvas = this.canvas1.nativeElement;      canvas.width = profileImgRef.width;      canvas.height = profileImgRef.height;      const detectionsArray = detectionsForSize.detection      const singleLabelDiscription = new faceapi.LabeledFaceDescriptors(""person1""; [firstFaceDescription.descriptor])      this.faceMatcher = new faceapi.FaceMatcher(singleLabelDiscription)      faceapi.drawDetection(canvas; detectionsArray; { withScore: true; withClassName: true })    }Second image detection code          const fullFaceDescriptions = await faceapi                      .detectAllFaces(img)                      .withFaceLandmarks().withFaceDescriptors()    if (!isNullOrUndefined(fullFaceDescriptions))     {      console.log(fullFaceDescriptions);            const detectionsForSize = faceapi.resizeResults(fullFaceDescriptions;         { width: groupImgRef.width; height: groupImgRef.height })            const canvas = this.canvas2.nativeElement;      canvas.width = groupImgRef.width;      canvas.height = groupImgRef.height;            const detectionsArray = detectionsForSize.map(fd => fd.detection)      // Face recognition      const results = fullFaceDescriptions.map(fd => this.faceMatcher.findBestMatch(fd.descriptor))      console.log(results)      const boxesWithText = results.map((bestMatch; i) => {        const box = detectionsArray[i].box        const text = bestMatch.toString()        const boxWithText = new faceapi.BoxWithText(box; text)        return boxWithText      })      faceapi.drawDetection(canvas; boxesWithText)    }![Screenshot from 2019-04-08 23-52-28](https://user-images.githubusercontent.com/34840376/55747322-9cc11700-5a59-11e9-8573-0ea1bfae61dd.png)One more question: How many face descriptor of each person will required for recognition that person====='; 'Since both distances are 0; odds are; that you are comparing the exact same image against eachother.> One more question: How many face descriptor of each person will required for recognition that personOne descriptor is sufficient.====='; ""@justadudewhohacks He is right. There's definitely a big problem with your library. I computed the face descriptors(as in your example) for DIFFERENT images of DIFFERENT( and i really mean different) people and guess what; in some of them i got the exact same face descriptor. But the strange thing is that this infamous descriptor is THE EXACT ONE showed by the indian man in the picture above(the picture shows only some of it but i'm sure 100% that the rest of it is equal to what i got) ; which is:[-0.036157943308353424;0.11451219767332077;0.08395102620124817;-0.015166843309998512;-0.06600701808929443;-0.042419757694005966;-0.0490785650908947;0.008687270805239677;0.14321738481521606;-0.025509962812066078;0.2366054803133011;0.001498898956924677;-0.23820878565311432;-0.046752505004405975;0.0028616711497306824;0.08502069860696793;-0.14962242543697357;-0.06591969728469849;-0.10743647068738937;-0.12046073377132416;0.033088553696870804;0.00343224941752851;0.018290642648935318;0.009096329100430012;-0.09878937155008316;-0.2763112187385559;-0.07352373749017715;-0.1335674375295639;0.08969920873641968;-0.19571422040462494;0.012862971052527428;0.021690549328923225;-0.13443519175052643;-0.05735789239406586;-0.02766452543437481;0.02025718241930008;0.0033780643716454506;-0.08854390680789948;0.15200215578079224;-0.03676857054233551;-0.14023277163505554;-0.053991273045539856;-0.010764091275632381;0.2646256685256958;0.18705308437347412;0.044662196189165115;0.00966082513332367;-0.043214693665504456;0.06518538296222687;-0.2766679525375366;0.02771242894232273;0.1624835729598999;0.02950565330684185;0.12462048977613449;0.06644287705421448;-0.1700247824192047;0.0408409908413887;0.08843832463026047;-0.1381165087223053;0.03919530287384987;0.0404399111866951;-0.09420391917228699;-0.036383725702762604;-0.11028037220239639;0.21431322395801544;0.06784548610448837;-0.08882343769073486;-0.12961089611053467;0.10797084122896194;-0.14936871826648712;-0.033338963985443115;0.11759535223245621;-0.11118640750646591;-0.16549569368362427;-0.22307536005973816;0.10256917774677277;0.3594265282154083;0.19990438222885132;-0.1451532542705536;0.036446064710617065;-0.1241573691368103;-0.01249675266444683;0.025015369057655334;0.023310799151659012;-0.06697430461645126;0.016636021435260773;-0.09167926013469696;0.05546090751886368;0.1520289182662964;-0.07664483040571213;0.02443789131939411;0.18324145674705505;-0.05131388455629349;0.03371473029255867;-0.0032856762409210205;0.01570814847946167;-0.08626414090394974;0.053548894822597504;-0.06702996790409088;0.01747414842247963;0.12703940272331238;-0.1353486031293869;0.027928803116083145;0.06764037907123566;-0.17105481028556824;0.10737797617912292;-0.004590324591845274;-0.020738571882247925;0.03764114901423454;0.005512760952115059;-0.06117083877325058;-0.036734696477651596;0.23441985249519348;-0.26262736320495605;0.24768288433551788;0.23473739624023438;0.03640888258814812;0.14327383041381836;0.039577461779117584;0.1486268937587738;-0.04878677800297737;-0.06605279445648193;-0.1222165897488594;-0.028063280507922173;0.012238509021699429;-0.014763789251446724;-0.04189039394259453;-0.003825186751782894]How can 2 different images of two different persons have the EXACT same descriptor ?!? Here's my code(i'm using electron so i need to monkey patch it otherwise i get an error on the 'new Canvas()' constructor)```var path = 'js/face-api.js/models';await faceapi.loadSsdMobilenetv1Model(path)await faceapi.loadFaceLandmarkModel(path)await faceapi.loadFaceRecognitionModel(path)faceapi.env.monkeyPatch({\tCanvas: HTMLCanvasElement;\tImage: HTMLImageElement;\tImageData: ImageData;\tVideo: HTMLVideoElement;\tcreateCanvasElement: () => document.createElement('canvas');\tcreateImageElement: () => document.createElement('img')});let img = new Image();async function loadImg() {      const faces = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors();      if(faces.length){          faces.forEach(function(face; i){              console.log(face.descriptor)         });      }}img.onload = loadImg;img.src = thumbAbsPath + '?t=' + Date.now();```=====""; ""It is hard to say what's going wrong without knowing your inputs. Can you share the images you are facing the issue with (reference and query image).=====""; '> Since both distances are 0; odds are; that you are comparing the exact same image against eachother.>Both image are not same. And I have notice that this three face description has value(Float32Array) exactly same. How? my codehttps://github.com/Rohit-B-Kadam/Memento/tree/master/src/app/face-detectImagehttps://github.com/Rohit-B-Kadam/Memento/tree/master/Dataone warning i have seen  tf-core.esm.js:17 performance warning: READ-usage buffer was read back without waiting on a fence. This caused a graphics pipeline stall.====='; '@justadudewhohacks i created a repository here: https://github.com/phoenixsue/face-api-issuesI included 4 images; all of which produce the infamous descriptor i showed above. Maybe this descriptor is used for initializing the array but then something happens and  instead of returning an error that\'s what we get. As for the code; i\' m using electron 4.1.0. I downloaded the zip file of your library from this repository and included in the index.html file like this: <script type=""text/javascript"" src=""js/face-api.js/dist/face-api.js""></script>The rest of the code is shown in my previous comment.I also checked Rohit-B-Kadam image(refImage.jpg) and yes; it produces the same infamous descriptor.Try to Please try to solve this problem otherwise your library is unusable====='; ""Hmmm; i tried the same images in a simple web page with only your library loaded;  no server; no electron; no nothing and i got what seems the right descriptors.  So i think that the problem is with electron. I' m confused...=====""; '@Rohit-B-Kadam @phoenixsue; the images you have shared work fine on a web application. Seems to be related to electron then. A similar issue with electron has been reported in #262. Which backend are you guys using on the web (webgl?; cpu?) and what backend in the electron app?@phoenixsue thanks for setting up an example. I will check it out on the weekend. If we figure out the ops; that are causing different results between web and electron we can certainly file this issue at tfjs.====='; '""Which backend are you guys using on the web (webgl?; cpu?) and what backend in the electron app?""I don\' t understand the question(i\'m an amateur programmer; you know). What do you mean by backend?Can you explain please?====='; 'You can check `faceapi.tf.getBackend()`; which will tell you whether the cpu or webgl backend is used. The webgl backend runs the ops on your gpu.====='; 'it says webgl. I have a good GPU(gtx 1060 3GB); so i think its good====='; ""@phoenixsue I tried out your example repo and indeed there is an issue with utilizing the webgl backend in electrons renderer thread. If I run the example on the CPU; e.g. by calling `faceapi.tf.setBackend('cpu')` the descriptors are calculated as expected.Now this is kind of a hairy situation. If we are going to file an issue at tfjs for that; we might have to figure out the exact cause of the issue; e.g. which ops are causing the divergence between the browser webgl backend and the webgl backend of the electron renderer thread. One approach would be to compare the outputs of each layer of the face recognition net. Since the face-recognition net is the only net utilizing regular convolutions; the issue might maybe reside in tf.conv2d.=====""; ""@Vincent Mühler Thank you for your effort. I checked too and the descriptors seem to be the right ones BUT it' s painfully SLOW on the CPU :((. I don' t know how to help you. I understand nothing about the details and machine learning in general. Why don' t you just show them the example you have worked with and tell them what the problem is? They could fix it in no time. No?=====""; 'i am using face-api for live browser recognisation using mtcnn;i follow the instruction in mtcnn documentation but when i try to run i got this error  Uncaught (in promise) TypeError: faceapi.drawDetection is not a function    at onPlay (script2.js:37)====='; '<!DOCTYPE html><html lang=""en""><head>  <meta charset=""UTF-8"">  <title>FaceDetect</title>    <script defer src=""face-api.min.js""></script>  <script defer src=""script2.js""></script>   <!-- <script src=""./js/commons.js""></script> -->  <!-- <script src=""./js/faceDetectionControls.js""></script> -->  <style>    body {      margin: 0;      padding: 0;      width: 100vw;      height: 100vh;      display: flex;      justify-content: center;      align-items: center;      background-color: azure;    }      </style></head><body>  <div style=""position: relative"" class=""margin"">    <video onplay=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>    <canvas id=""overlay"" />  </div></body></html>this my HTML====='; '[index.txt](https://github.com/justadudewhohacks/face-api.js/files/3455241/index.txt) this is html ====='; '[script.txt](https://github.com/justadudewhohacks/face-api.js/files/3455247/script.txt)this is the script file====='; 'any one please help with the solution====='; 'Maybe in electron is best to use the main thread to use TF and not the render thread.In main thread I think we can use GPU backend with an Nvidia GPU.What do you think @justadudewhohacks ?====='; 'Hi; I am experiencing an issue. When I Downloaded the files and ran it I got a blank white screen pls help====='; '@ImposibleScience  ""Hi ! Can you help me about a problem I give no information about ?""If you want help; open a new issue first. How is your problem related to current issue in any way ??Then; consider making a REAL description of your problem.=====']",Incorrect Functionality,Incorrect Functionality,Cross-platform App Framework Incompatibility,Desktop,Platform,change backend,changing backend,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.3
https://github.com/justadudewhohacks/face-api.js/issues/160,faceapi.detectAllFaces blocks the main thread for seconds on first call,4,open,2018-12-06T15:12:52Z,2018-12-09T18:54:05Z,I have been using this in a React project on a component's mount and it blocks completely the JS execution white it awaits for faceapi.detectAllFaces. This seems to be related with the warning that appears in the console Error: WebGL warning: getBufferSubData: Reading from a buffer with usage other than *_READ causes pipeline stalls. Copy through a STREAM_READ buffer.I could try and submit a PR; but I can't seem to find the actual calls or the buffers it uses; could you point me to it?,"['I never saw this warning before; but it comes from tfjs-core. I dont touch any WebGL in this library.The reason why the initial call takes longer; is due to the shaders being compiled; which is referred to as ""warmup time"".====='; ""Thanks for the quick reply. Is there any way we could split the warmup time into chunks and give JS time to breath; or maybe move the warmup to another thread? The problem is not that it takes long; it's that it hangs the browser. =====""; ""What I was commenting [here](https://github.com/justadudewhohacks/face-api.js/issues/87#issuecomment-432351672) might be an option we could try out:> Would probably be an enhancement to sprinkle some tf.nextFrame calls into to forward methods. But I would first recommend some closer investigation.If that doesn't help; then it's probably something; that has to be done by the WebGL engine of tfjs-core.=====""; 'Get the same ""warning""/notice in my browser console :/ =====']",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,call tf.nextFrame,call tf.nextFrame,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.2,D.4
https://github.com/justadudewhohacks/face-api.js/issues/151,[Android] Error on Live Demos: Failed to compile fragment shader.,6,3,2018-11-28T08:06:59Z,2021-11-10T11:49:45Z,I accessed the demo site (https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/) on my Android device; but the face detection didn't work.There was such an error.```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.createProgram (/face-api.js/0-e459655e1a7a45356a06.js:250)    at compileProgram (/face-api.js/0-e459655e1a7a45356a06.js:250)    at /face-api.js/0-e459655e1a7a45356a06.js:250    at e.getAndSaveBinary (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.compileAndRun (/face-api.js/0-e459655e1a7a45356a06.js:250)    at e.conv2d (/face-api.js/0-e459655e1a7a45356a06.js:250)    at ENV.engine.runKernel.x (/face-api.js/0-e459655e1a7a45356a06.js:250)    at /face-api.js/0-e459655e1a7a45356a06.js:250    at e.scopedRun (/face-api.js/0-e459655e1a7a45356a06.js:250)```My device:Android 6.0.1Chrome 70.0.3538.30SONY Xperia A4,"['Which face detector did you use; the default one; e.g. TinyFaceDetector?====='; '@justadudewhohacks I got the same error in all detector...- Tiny Face Detector- SSD Mobilenet v1- MTCNN====='; ""That's odd; may be some issue with the WebGL backend on your device. Did run any tfjs application on your Xperia before? If not you could try setting up a simple page running a conv2d operation; to see whether it is an issue with your device and if so report an issue at tfjs.=====""; ""This is a TFJS/device issue. I've hacked together a solution which works in my case (I'm also using face-api.js) here: https://github.com/tensorflow/tfjs/issues/952#issuecomment-468393905=====""; '![image](https://user-images.githubusercontent.com/45730502/110116513-9c645e80-7ddd-11eb-8167-5f4e77082a92.png)Hi; I\'m also facing the same issue on Samsung Galaxy J2.I developed an app in Cordova and VueJs. It\'s working fine on some of the phones.Even if it does not work; it\'s ok for me but it does not throw any error to any of called functions.It\'s blocking the flow; so I\'m using a timeout of 10sec to close the page.But some devices taking more than 10sec to load models; here it creates a problem for me.I\'m using this monkey path code:<img width=""983"" alt=""Screenshot 2021-03-05 at 6 08 15 PM"" src=""https://user-images.githubusercontent.com/45730502/110116631-c61d8580-7ddd-11eb-8242-7d6c9884ab69.png"">Not getting where it\'s failing and where to add try-catch.Does anyone have a solution for this?====='; 'having the same issue on Oppo A37; is there a patch/solution available to this issue?=====']",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/87,blocking browser hang issue,8,open,2018-09-09T11:12:07Z,2019-03-30T15:12:55Z,Checked demo link as well as ran the local setup - why it hangs the browser in a blocking way for the time being it is detecting/recognizing face?,"['Because for the time of forwarding an input through the net; the main thread is blocked.====='; 'Would probably be an enhancement to sprinkle some **tf.nextFrame** calls into to forward methods. But I would first recommend some closer investigation.====='; '@justadudewhohacks can those blocking operations be written node.js backend script?====='; ""I didn't quite get your question sorry. If your question is; whether you can use face-api.js with nodejs; then the answer is yes. See my comment in #55.=====""; '@justadudewhohacks  thanks dude - that answered my query ====='; 'maybe pass to web worker to do it ? 🤔====='; 'I tried to pass it to a web worker but had no success at all. If anyone manages to do it; I would be most interested.====='; 'i tried pass the job of load model to web worker do; but when getEnv() in tfjsImageRecognitionBase are fail because web worker cant access window object. Not sure anyway to bypass and direct tell the tfjsImageRecognitionBase is broswer env.=====']",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,call tf.nextFrame,call tf.nextFrame,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",B.1.2,D.4
https://github.com/justadudewhohacks/face-api.js/issues/63,Mobile Browser not working; Android 8.0; Chrome; React,4,open,2018-08-01T19:50:37Z,2018-08-07T15:40:09Z,Hi @justadudewhohacks I was trying the api on a Mobile browser; but is not working :worried: I tried it on a Android 8.0; Chrome; and I was using React.jsIf you want to try it too; here is the repo:https://github.com/BonnieMilian/face-api-webcam-react,"[""Just tested the examples on android with chrome. Seems that the SSD face detector is not working. Besides that; it should work fine; although it runs much slower on mobile. You could try MTCNN or Yolo (haven't tried Yolo on mobile yet) for face detection.=====""; 'I try it on iOS; Safari.And work fine; chrome is not supporting something; chrome on the pc neither work====='; ""MTCNN doesn't work for me on android 6.0.1 and mobile chrome 68.0.3440.85=====""; 'Related with this issue https://github.com/tensorflow/tensorflow/issues/18741Is chrome itself=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.2
https://github.com/justadudewhohacks/face-api.js/issues/43,[Intel GPU] - run the example but the result isn‘t right,15,open,2018-07-08T10:25:19Z,2019-05-01T02:14:51Z,I download the source code and npm the example like the  'readme.md' said; but the result isn't right.the detection draw on the canvas like the pic.why this happen?![qq 20180708182022](https://user-images.githubusercontent.com/979374/42418845-9deb796e-82db-11e8-8287-022253e17169.png),"[""What kind of gpu are you using? I tried to run this on a intel gpu once and got the exact same picture. Maybe tfjs doesn't support intel gpus.=====""; 'thank you . yes ; I use the intel hd gpu. maybe I should change the computer====='; 'I see; maybe we should also report this to the tfjs team at some point. Unfortunately I only have access to a intel gpu sporadically; otherwise I could try to debug step by step; which operations are causing to return inconsistent results.====='; 'I will try to do it. but I should know the consistent results first.====='; 'I had the exact same issue and fixed it by changing the default GPU for the browser to my Nvidia GPU. Nvidia has decided to disable the GPU by default for Chrome and Firefox and use the Intel HD GPU. Source: https://superuser.com/questions/645918/how-to-run-google-chrome-with-nvidia-card-optimus====='; '@marine008 @justadudewhohacks @thexiroy Recently; I had investigated some precision issues on Intel GPUs. I had tested this issue on different Intel GPUs; but could not reproduce it.Testing platforms were as below;Intel(R) UHD Graphics 630 + win10Intel(R) HD Graphics 530 + win 10Intel(R) HD Graphics 630 + win 10Intel(R) UHD Graphics 630 + Ubuntu 18.10Intel(R) HD Graphics 530 + Ubuntu 17.10Intel(R) HD Graphics 630 + Ubuntu 18.04I want to know which platform could reproduce this issue. Was an old GPU on which you produced this issue? Thank you.====='; '@xhcao Intel(R) HD Graphics 4600 + win10====='; ""Hi; I'm from the TensorFlow.js team (which face-api.js uses). Can someone let me know if you can reproduce these precision problems using the latest tf@1.0.2? Thank you!=====""; ""Latest version of face-api.js now runs on tfjs-core 1.0.3. The initial issue occured in the ssd mobilenet v1 face detector. One can simply verify if it's working now by running the face detection example; the ssd face detector is selected by default. So if any Intel GPU user would give that a try; that would be nice.=====""; ""Thanks! We recently got a Lenovo Yoga X1 Windows laptop with integrated Intel HD 520 GPU.I just cloned the face-api.js repo; ran the examples and couldn't reproduce the problem.=====""; 'Thanks. I could not reproduce this issue on Intel(R) HD Graphics 4600 + win 10 platform. Do you know the root-cause?====='; ""TF.js went through a lot of changes (packing; better memory layout/indexing) so it's hard to tell what exactly helped with numerical stability; but we do know that the `Haswell` chipset (Intel Graphics 4600) was the one that had numerical issues. It seems to me that we can close this issue.=====""; '@dsmilkov I get access to an Intel GPU next week; I will double check if everything works as expect and if so will close here.====='; '@dsmilkov I could verify; that the SSD mobilenet model; which was subject of this issue; now works as expected on an Intel gpu.I noticed some precision differences between amd and intel gpu though; which can be seen in the output of the landmark detection model:### AMD:![landmarks-amd-gpu](https://user-images.githubusercontent.com/31125521/56611749-e5103580-6612-11e9-8c5b-29b24cf4cae6.png)### Intel:![landmarks-intel-gpu](https://user-images.githubusercontent.com/31125521/56611750-e5a8cc00-6612-11e9-8508-fc21ce9b376e.png)Not sure if the info helps; but except of an intitial regular tf.conv2d the model is composed of depthwise seperable convolutions; followed by a fully connected layer (tf.matMul) at the end.====='; ""Thanks. That's good feedback. If you find some extra time; would love if you can diff the outputs after each internal operation (activations) between the amd and the intel gpu. I'm curious to see if the difference start to occur after a specific op; or they slowly drift over multiple different ops.=====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/15,Safari performance is poor,7,open,2018-06-21T21:04:48Z,2020-03-22T17:05:42Z,"There is a known bug with tensorflow.js on Safari that keeps it from performing accurately (on mobile and Desktop). The good news is it was just recently fixed https://github.com/tensorflow/tfjs-core/pull/978 The changes are merged into master but not released as a new version yet.Here's what the difference looks like in practice. Chrome:<img width=""575"" alt=""screen shot 2018-06-21 at 17 00 33"" src=""https://user-images.githubusercontent.com/157106/41745516-2bba6652-7575-11e8-89ad-5a1adf5a30bf.png"">Safari:<img width=""582"" alt=""screen shot 2018-06-21 at 17 00 27"" src=""https://user-images.githubusercontent.com/157106/41745524-3272a522-7575-11e8-9917-bf627155a79e.png"">","['I changed this dependency: `""@tensorflow/tfjs-core"": ""0.11.9""`And now it works on Desktop Safari; the results are almost exactly the same as Chrome:<img width=""590"" alt=""screen shot 2018-06-22 at 12 03 34"" src=""https://user-images.githubusercontent.com/157106/41786870-abeb50d0-7614-11e8-923f-f6cdae42d3d7.png"">Results on iOS Chrome and Safari are the same as well.Edit: sorry; I might have spoken too soon. Most of the time I run this demo on Safari and Chrome on mobile it crashes the page. When it does work; looking at the inspector I see:```[Error] Unhandled Promise Rejection: RangeError: Maximum call stack size exceeded.\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:995)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:1012)\tstep (face-api.js:294)\t(anonymous function) (face-api.js:268)\tinitializePromise```That last error repeats a few times.====='; ""Hmm okay; is that actually a tfjs issue and it's simply a matter of updating the tfjs-core version once they released a fix?=====""; ""yes; tfjs-core is now updated (version 0.11.9 has the fix for safari).however it looks like there's still a bug inside face-api.js that's keeping it from running correctly on iOS (that's the trace\xa0i pasted above).update: i just tested the latest release of the code (with the quantized weights) and 0.11.9; and this bug is still there. it worked the first time; but crashes the page after i reload.=====""; 'Hmm okay; maybe I get the chance to test it on safari some time.====='; 'Have you guys found any solution to make it work on ios mobile ?====='; ""I didn't; but if anyone has access to an IOS device + safari and can figure out; which ops cause issues on safari; we could report an issue at tfjs-core.=====""; 'Do you have any plans to merge to new tfjs wasm backend? The facemesh seems to be a good place to start?https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html=====']",Reference Error,Crash,Browser Incompatibility,Browser,Platform,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,D.2
https://github.com/justadudewhohacks/face-api.js/issues/15,Safari performance is poor,7,open,2018-06-21T21:04:48Z,2020-03-22T17:05:42Z,"There is a known bug with tensorflow.js on Safari that keeps it from performing accurately (on mobile and Desktop). The good news is it was just recently fixed https://github.com/tensorflow/tfjs-core/pull/978 The changes are merged into master but not released as a new version yet.Here's what the difference looks like in practice. Chrome:<img width=""575"" alt=""screen shot 2018-06-21 at 17 00 33"" src=""https://user-images.githubusercontent.com/157106/41745516-2bba6652-7575-11e8-89ad-5a1adf5a30bf.png"">Safari:<img width=""582"" alt=""screen shot 2018-06-21 at 17 00 27"" src=""https://user-images.githubusercontent.com/157106/41745524-3272a522-7575-11e8-9917-bf627155a79e.png"">","['I changed this dependency: `""@tensorflow/tfjs-core"": ""0.11.9""`And now it works on Desktop Safari; the results are almost exactly the same as Chrome:<img width=""590"" alt=""screen shot 2018-06-22 at 12 03 34"" src=""https://user-images.githubusercontent.com/157106/41786870-abeb50d0-7614-11e8-923f-f6cdae42d3d7.png"">Results on iOS Chrome and Safari are the same as well.Edit: sorry; I might have spoken too soon. Most of the time I run this demo on Safari and Chrome on mobile it crashes the page. When it does work; looking at the inspector I see:```[Error] Unhandled Promise Rejection: RangeError: Maximum call stack size exceeded.\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:995)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:1012)\tstep (face-api.js:294)\t(anonymous function) (face-api.js:268)\tinitializePromise```That last error repeats a few times.====='; ""Hmm okay; is that actually a tfjs issue and it's simply a matter of updating the tfjs-core version once they released a fix?=====""; ""yes; tfjs-core is now updated (version 0.11.9 has the fix for safari).however it looks like there's still a bug inside face-api.js that's keeping it from running correctly on iOS (that's the trace\xa0i pasted above).update: i just tested the latest release of the code (with the quantized weights) and 0.11.9; and this bug is still there. it worked the first time; but crashes the page after i reload.=====""; 'Hmm okay; maybe I get the chance to test it on safari some time.====='; 'Have you guys found any solution to make it work on ios mobile ?====='; ""I didn't; but if anyone has access to an IOS device + safari and can figure out; which ops cause issues on safari; we could report an issue at tfjs-core.=====""; 'Do you have any plans to merge to new tfjs wasm backend? The facemesh seems to be a good place to start?https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html=====']",Reference Error,Crash,Browser Incompatibility,Browser,Platform,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",D,D.2
https://github.com/justadudewhohacks/face-api.js/issues/15,Safari performance is poor,7,open,2018-06-21T21:04:48Z,2020-03-22T17:05:42Z,"There is a known bug with tensorflow.js on Safari that keeps it from performing accurately (on mobile and Desktop). The good news is it was just recently fixed https://github.com/tensorflow/tfjs-core/pull/978 The changes are merged into master but not released as a new version yet.Here's what the difference looks like in practice. Chrome:<img width=""575"" alt=""screen shot 2018-06-21 at 17 00 33"" src=""https://user-images.githubusercontent.com/157106/41745516-2bba6652-7575-11e8-89ad-5a1adf5a30bf.png"">Safari:<img width=""582"" alt=""screen shot 2018-06-21 at 17 00 27"" src=""https://user-images.githubusercontent.com/157106/41745524-3272a522-7575-11e8-9917-bf627155a79e.png"">","['I changed this dependency: `""@tensorflow/tfjs-core"": ""0.11.9""`And now it works on Desktop Safari; the results are almost exactly the same as Chrome:<img width=""590"" alt=""screen shot 2018-06-22 at 12 03 34"" src=""https://user-images.githubusercontent.com/157106/41786870-abeb50d0-7614-11e8-923f-f6cdae42d3d7.png"">Results on iOS Chrome and Safari are the same as well.Edit: sorry; I might have spoken too soon. Most of the time I run this demo on Safari and Chrome on mobile it crashes the page. When it does work; looking at the inspector I see:```[Error] Unhandled Promise Rejection: RangeError: Maximum call stack size exceeded.\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:995)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:1012)\tstep (face-api.js:294)\t(anonymous function) (face-api.js:268)\tinitializePromise```That last error repeats a few times.====='; ""Hmm okay; is that actually a tfjs issue and it's simply a matter of updating the tfjs-core version once they released a fix?=====""; ""yes; tfjs-core is now updated (version 0.11.9 has the fix for safari).however it looks like there's still a bug inside face-api.js that's keeping it from running correctly on iOS (that's the trace\xa0i pasted above).update: i just tested the latest release of the code (with the quantized weights) and 0.11.9; and this bug is still there. it worked the first time; but crashes the page after i reload.=====""; 'Hmm okay; maybe I get the chance to test it on safari some time.====='; 'Have you guys found any solution to make it work on ios mobile ?====='; ""I didn't; but if anyone has access to an IOS device + safari and can figure out; which ops cause issues on safari; we could report an issue at tfjs-core.=====""; 'Do you have any plans to merge to new tfjs wasm backend? The facemesh seems to be a good place to start?https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.2] Out of Memory""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,D.2
https://github.com/justadudewhohacks/face-api.js/issues/15,Safari performance is poor,7,open,2018-06-21T21:04:48Z,2020-03-22T17:05:42Z,"There is a known bug with tensorflow.js on Safari that keeps it from performing accurately (on mobile and Desktop). The good news is it was just recently fixed https://github.com/tensorflow/tfjs-core/pull/978 The changes are merged into master but not released as a new version yet.Here's what the difference looks like in practice. Chrome:<img width=""575"" alt=""screen shot 2018-06-21 at 17 00 33"" src=""https://user-images.githubusercontent.com/157106/41745516-2bba6652-7575-11e8-89ad-5a1adf5a30bf.png"">Safari:<img width=""582"" alt=""screen shot 2018-06-21 at 17 00 27"" src=""https://user-images.githubusercontent.com/157106/41745524-3272a522-7575-11e8-9917-bf627155a79e.png"">","['I changed this dependency: `""@tensorflow/tfjs-core"": ""0.11.9""`And now it works on Desktop Safari; the results are almost exactly the same as Chrome:<img width=""590"" alt=""screen shot 2018-06-22 at 12 03 34"" src=""https://user-images.githubusercontent.com/157106/41786870-abeb50d0-7614-11e8-923f-f6cdae42d3d7.png"">Results on iOS Chrome and Safari are the same as well.Edit: sorry; I might have spoken too soon. Most of the time I run this demo on Safari and Chrome on mobile it crashes the page. When it does work; looking at the inspector I see:```[Error] Unhandled Promise Rejection: RangeError: Maximum call stack size exceeded.\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating \'h.shape\')\t(anonymous function) (face-api.js:23:1953)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\treadSync (face-api.js:23:262888)\t(anonymous function) (face-api.js:23:263801)\t(anonymous function) (face-api.js:23:1889)\t(anonymous function) (face-api.js:23:921)\tinitializePromise\tPromise\t__awaiter (face-api.js:23:707)\tcompileAndRun (face-api.js:23:280738)\trunKernel (face-api.js:23:140660)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:806)\tforEach\t(anonymous function) (face-api.js:802)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:995)\ttidy (face-api.js:23:73989)\ttidy (face-api.js:23:73989)\t(anonymous function) (face-api.js:1012)\tstep (face-api.js:294)\t(anonymous function) (face-api.js:268)\tinitializePromise```That last error repeats a few times.====='; ""Hmm okay; is that actually a tfjs issue and it's simply a matter of updating the tfjs-core version once they released a fix?=====""; ""yes; tfjs-core is now updated (version 0.11.9 has the fix for safari).however it looks like there's still a bug inside face-api.js that's keeping it from running correctly on iOS (that's the trace\xa0i pasted above).update: i just tested the latest release of the code (with the quantized weights) and 0.11.9; and this bug is still there. it worked the first time; but crashes the page after i reload.=====""; 'Hmm okay; maybe I get the chance to test it on safari some time.====='; 'Have you guys found any solution to make it work on ios mobile ?====='; ""I didn't; but if anyone has access to an IOS device + safari and can figure out; which ops cause issues on safari; we could report an issue at tfjs-core.=====""; 'Do you have any plans to merge to new tfjs wasm backend? The facemesh seems to be a good place to start?https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.2] Out of Memory""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",D,D.2
https://github.com/justadudewhohacks/face-api.js/issues/799,Performance optimization,19,closed,2021-05-29T19:02:11Z,2021-09-02T17:52:39Z,It takes usually 30-40 Seconds to detect faces. Is there any way I can do optimization and get results in 10-15 seconds???,"['it never takes 30-40 sec to detect faces; it\'s pretty much sub 1sec.now; if *initial* detection takes long time; that is most likely due to your configurating using `webgl` backend and `tfjs` takes some time to ""warmup"" the model (compile `gles` shaders and upload weighs as textures to gpu). but after initial detection; anything afterwards is really fast.on the other hand; if you used `wasm` backend; warmup doesn\'t exist; but actual detection is quite a lot of slower than using `webgl` (but still well below 1sec threshold).====='; 'What backend would your recommend for CPU optimization if we wanted to do detectallface with expression and not gender and age====='; ""it's less of a question which models are execution; more of a question where are they executed.each backend has it's pros and cons:- tfjs-node-gpu:  - pro: absolutely fastest  - con: can run in backend only; really messy compatibility matrix with nvidia cuda- tfjs-backend-webgl:  - pro: fast if client has a discrete gpu  - con: slow if client does not have a discrete gpu; slow initial startup; startup causes ui blocking- tfjs-backend-wasm:  - pro: fast startup; does not require gpu  - con: client must load wasm binaries from a trusted source so cannot use cdn easily; only mediocre performance at best; requires enabling browser simd flag on each client or performance is poor=====""; ""I am using face-api on react and it's face-api older version I think which using  tensorflow 1 version. Should I change to @vladmandic/face-api then it can make performance improvements???? I have notice it shows very slow performance on mobile; mostly users will access this through mobile.=====""; 'no; update is for compatibility reasons. any performance improvements are minor (~5-10%).====='; 'what changes should I do to boost its performance. In my case it is taking 30-40 seconds; here is my node_modules for face_api```""@mapbox/node-pre-gyp"": ""^1.0.4"";""@tensorflow/tfjs-core"": ""1.7.0"";""@tensorflow/tfjs-node"": ""1.7.0"";""face-api.js"": ""^0.22.2"";""canvas"": ""2.6.1"";```====='; 'we are going back and forth with generic statements - boost performance from what? on what platform?  30-40sec noted in your original post is NOT detect time; i already wrote that.  be as specific as possible. list exact times for each step. list your actual configuration. version of packages alone says nothing.  then i may be able to help.  ====='; 'you still did not post exact times. nor did you post your configuration. i don\'t even know which backend are you using.i just tried using ""original sample"" link on my android based mobile phone and loading and warmup is ~2sec with any subsequent detection <0.2sec.and your site has soo many elements that face-api is lost between all the noise. not to mention that looking at minified code is not something i would do.i give up :(====='; 'My project backend is Nodejs with linux environment and frontend I have impliment on javascript. I am trying to get face LANDMARK .I don\'t know how to figure out time for each steps.. face-api works on tensiorflow v1 so it workis on olxder version of Node. I am using Node Version 8.          ```       ""@tensorflow/tfjs-core"": ""1.7.0"";       ""@tensorflow/tfjs-node"": ""1.7.0"";       ""canvas"": ""^2.7.0"";        ""face-api.js"": ""^0.22.2"";        faceapi.nets.ssdMobilenetv1.loadFromUri(path)        faceapi.nets.faceLandmark68Net.loadFromUri(path)        faceapi.nets.faceRecognitionNet.loadFromUri(path)        faceapi.nets.tinyYolov2.loadFromUri(path)       const mypic = document.getElementById(\'userimg\');       const detections = await faceapi.detectAllFaces(mypic).withFaceLandmarks()```         I am accessing original image which is user uploads for face landmark detecttion; normally it is 960x1280.        ====='; ""Well; I don't know why; but for me; the FIRST time; right after it opens my camera; it takes about 30 seconds to find the first face (even if I'm sitting still right in front of it); then; it works perfectly.Watching the logs; it's NOT related to loading resources...I pinpointed it to this peace of code:```console.log(1);faceapi.detectSingleFace(      video;      new faceapi.TinyFaceDetectorOptions({ minConfidence: 0.5 })    )      .withFaceLandmarks()    .then(...) // console.log(2);    .catch(...) // console.log(3);```I see the `1` in the log right away...then; about 30 seconds later I see the `2`.I call the function again when `then` is triggered...from this time on; it works smoothly.Tried it on a macbook pro in chrome and firefox.Interestingly enough...the samples/demos run as expected (really quickly).I'm rendering it on a react component but as far as my logs show; it's not being rendered or processed more times than it should.=====""; ""@felipenmoura What you're talking about is called **warmup** and it heavily depends on the backend you're using  For example; `WASM` has fast warmup; but slower inference (meaning first frame is faster; but then every other frame is not that great) while `WebGL` has much slower warmup (that is most likely what you're seeing); but then inference for each frame is faster than using `WASM` backend  This is common for every TensorFlow model; not specific to FaceAPI at all  On a side-note; don't create `Options` object each time; do it outside of the loop during component initialization and then re-use it. That doesn't have impact on 30sec delay you're seeing; but will have significant benefit to overall performance  =====""; 'Hm I see. Interesting; thanks.Is there a way I can speed up this first process? I mean; is there a way I can swap/decide between wasm and webgl?====='; ""@felipenmoura simply initialize tensorflow with appropriate backend before calling `face-api` and `face-api` will use whatever is set:```jsimport * as tf from '@tensorflow/tfjs';import '@tensorflow/tfjs-backend-webgl';tf.setBackend('webgl');await tf.ready();```or```jsimport * as tf from '@tensorflow/tfjs';import * as wasm from '@tensorflow/tfjs-backend-wasm';wasm.setWasmPaths('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.8.0/dist/');tf.setBackend('wasm');await tf.ready();```note that `tfjs` version and version of WASM binaries *must* match version used by `face-api`  (which in case of this original version is quite old; it's 1.7.0; definitely not latest 3.8.0)  or use an up-to-date port like <https://github.com/vladmandic/face-api>=====""; ""Awesome; thanks.I see some interesting parts of documentation over there; that I hadn't seen yet!I did NOT have tensorflow installed as dependency...does it mean it was actually using my GPU/CPU regular cycles to process it?I'm now trying to build it using the updated repo you linked; thanks.=====""; ""`FaceAPI` is built using TensorFlow/JS no matter what:- This version of `FaceAPI` has TFJS 1.7.0 embedded; that's why you don't have external dependencies- Newer version of `FaceAPI` has both embedded and non-embedded version and is based on TFJS 3.8.0  Now; TFJS can use different backends to execute actual ML operations:- CPU: ops are executed as interpreted JS code on CPU- WASM: ops are executed on CPU using compiled WASM library (thus the need to download WASM binary)- WebGL (Browser only): ops are executed on GPU using compile-on -the-fly GLSL code (thus there is a longer warmup period as GL code needs to be compiled for specific GPU and model weights uploaded to GPU as shaders)- TensorFlow (NodeJS only): ops are executed on CPU using compiled tensorflow library- TensorFlow-GPU (NodeJS only): ops are executed on GPU using compiled tensorflow library; but with help of CUDA libraries for GPU acceleration- WebGPU: New experimental GPU backend that will eventually replace WebGL=====""; 'Awesome; thanks for this complete reply.I managed to start using wasm and the warmup period is perceivably faster :)====='; 'Hey there...sorry for pinging here again; but it seems like there\'s something I\'m missing!I\'m trying to dynamically import the dependencies only if the user selects the ""selfie"" option.It works; but it never uses wasm!```js        Promise.all([          import(\'@tensorflow/tfjs\'); // dynamic import          import(\'@tensorflow/tfjs-backend-wasm\') // dynamic import        ]).then(async ([tf; wasm]) => {          window.tf = tf;          wasm = wasm;          window.wasm = wasm;          wasm.setWasmPaths(\'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.9.0/dist/\');          await tf.setBackend(\'wasm\');          await tf.ready();          await createScript(\'/face-detection-ia/face-api.min.js\'; scrId); // imports the script from /public          faceapi = window.faceapi; // tried this to see if anything would change...but nope          try {            await Promise.all([              faceapi.nets.tinyFaceDetector.loadFromUri(\'/face-detection-ia/models/\');              faceapi.nets.faceLandmark68Net.loadFromUri(\'/face-detection-ia/models/\');            ]); // this also loads all the models OK            console.log(\'DONE loading AI stuff\');            console.log(faceapi.tf.getBackend()); // webgl <<<<<----- !!!!!!!!!          } catch (error) {            console.error(\'ERROR loading AI stuff\'; error);            return;          }        });      }```It IS loading everything. It IS waiting for everything to be ready. It DOES NOT trigger any error...but it still uses webgl and has quite a poor performance and long warmup.Any idea what I might be missing here?> By the way...I had to do all that because I\'m using next.js and its SSR insists on trying to render it in the backend; requiring many other dependencies...I\'m trying to avoid that by ensuring it ONLY runs on client side.> By the way 2 ... My `@tensorflow/tfjs`\'s version is 3.9.0 as well as the `tfjs-backend-wasm`\'s version. Is this what you meant about matching versions?====='; ""> My @tensorflow/tfjs's version is 3.9.0 as well as the tfjs-backend-wasm's version. Is this what you meant about matching versions?No; that's smaller part of it. Important part is that `@tensorflow/tfjs-backend-wasm` and your wasm binary set by `wasm.setWasmPaths` are of the same version - which it looks like they are.> By the way...I had to do all that because I'm using next.js and its SSR insists on trying to render it in the backend;  > requiring many other dependencies...I'm trying to avoid that by ensuring it ONLY runs on client side.you need to tell `next.js` not to mess with dependencies  for example; this is `next.config.js` i've tested a while back:```jsmodule.exports = {  webpack: (config) => {    if (config.target === 'web') config.externals = [ 'fs'; 'os'; 'util' ];    return config;  };}```loading WASM modules via `next.js` is tricky at best.if you want to dynamically import wasm and not have SSR; i don't see where is that set?in the past i've done something like this:```jsconst wasm = dynamic(  () => import('@tensorflow/tfjs-backend-wasm');  { ssr: false })```and that should be in the component init; not calling `faceapi` immediately afterwards - how would you then use `faceapi` for the second time? load modules again? > but it still uses webgl and has quite a poor performance and long warmup.after `tf.ready()`; do a `console.log(tf.getBackend());` to see what's the actual backend usedbtw; this is no longer related to original issue - why not create a new one; 'how to use faceapi with wasm in next.js`and if you're using new `faceapi` (which it seems you are since you're loading wasm 3.9.0); open an issue in that git repository.=====""; 'Sure thing.Thanks; I just created the issue over there:https://github.com/vladmandic/face-api/issues/65=====']",Slow Execution,Poor Performance,WebGL Limits,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.1,D.4
https://github.com/justadudewhohacks/face-api.js/issues/499,TensorFlow Synchronous processing on Node.js,1,closed,2019-12-09T18:07:42Z,2020-01-28T16:14:37Z,Per Tensorflow's documentation; the node.js implementation is synchronous and takes (https://www.tensorflow.org/js/guide/nodejs - Production considerations).From looking at the code; it appears to me that no special action was taken in face-api to address this; meaning that the main thread will be blocking when running a face detection on Node JS. Am I correct in my understanding?(We're using face-api on a web server to detect faces in uploaded images).,"[""> From looking at the code; it appears to me that no special action was taken in face-api to address this; meaning that the main thread will be blocking when running a face detection on Node JS. Am I correct in my understanding?Correct; face-api.js doesn't touch any of the backend logic; it's only build on top of it.=====""]",Browser Hangs,Poor Performance,Dependency Error,Node.js,Platform,change platform,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.1.2,B.2
https://github.com/justadudewhohacks/face-api.js/issues/416,Laggy on mobile browsers,4,closed,2019-09-18T02:12:41Z,2019-09-19T06:03:12Z,the camera example on android browser is slow and laggy. any plan to make the model smaller and faster for mobile browsers?,"['Which model are you talking about?====='; ""> Which model are you talking about?I tested all three models in the 'webcam face tracking' example;they all appear slow on mobile browers.=====""; ""I get ~15 fps on my android phone using the TinyFaceDetector with mobile chrome. It's probably not possible to get much faster than this by creating an even smaller model.If you need even faster models for mobile devices; maybe you can manage it to train your own TFLite model; but you will have to embedd it into a native android / ios app instead of a web app then.I am not sure; how much room for optimization in the WebGL backend of tfjs there is remaining to make things run even faster; but I also noticed the tfjs team working on a webgpu backend. Maybe once they release that backend we will see even better performance in browsers.=====""; 'I see. Thanks for your explanation.=====']",Slow Execution,Poor Performance,WebGL Limits,WebGL,Backend,change platform,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",B.1.1,D.4
https://github.com/justadudewhohacks/face-api.js/issues/407,force usage of F16 textures,3,closed,2019-09-10T10:40:05Z,2019-10-12T11:15:11Z,"tfjs-core: Make it possible to force usage of F16 textures. #1902https://github.com/tensorflow/tfjs/pull/1902appears in tf-core 1.2.9 release (only f16 textures yet; not math!).1. can you (we :) add and test f16 textures?2. they added in ""To Do"": ""Add flag (defaults to false) that would turn on mediump precision."" Can we confirm; that mediump precision is useful in real aplications? (than they will add the flag more quickly :)E.g. image classification/detection + colab + NVIDIA GPU?","['So you are asking me to upgrade the tfjs-core version to latest?====='; ""not sure about detailes (I'm from webgl-dev-list https://www.ibiblio.org/e-notes/webgl/gpu/mul/sgemm.htm )1. if you just add  tf.webgl.forceHalfFloat()you will half GPU memory footprint; get ~10% acceleration and save battery.2. with mediump precision you can get x2 acceleration but for some reason e.g. SSD model works wrong with fp16 math (for TFjs team). Can we optimize any useful model for fp16 math (NVIDIA can :)? More detailes?=====""; ""These flags you are talking about can be enabled via tf. face-api.js doesn't deal with such kind of things; it just utilizes tfjs as a backend. So you can simply enable those flags via tf and the models of face-api.js will make use of them. > with mediump precision you can get x2 acceleration but for some reason e.g. SSD model works wrong with fp16 math (for TFjs team). Can we optimize any useful model for fp16 math (NVIDIA can :)? More detailes?I am not familar with such kind of optimizations. Again such optimizations are probably something that have to be done in the tfjs backend and not face-api.js; but correct me if I am wrong.=====""]",Slow Execution,Poor Performance,Improper Model Attribute,WebGL,Backend,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Optimization Issues""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.1.1,C.2
https://github.com/justadudewhohacks/face-api.js/issues/387,How can i fix this error,1,closed,2019-08-15T19:12:13Z,2019-09-13T08:53:39Z,face-api.min.js:1 Uncaught (in promise) Error: Failed to link vertex and fragment shaders.    at Wr (face-api.min.js:1)    at t.createProgram (face-api.min.js:1)    at face-api.min.js:1    at face-api.min.js:1    at t.getAndSaveBinary (face-api.min.js:1)    at t.compileAndRun (face-api.min.js:1)    at t.conv2dWithIm2Row (face-api.min.js:1)    at t.conv2d (face-api.min.js:1)    at Zt.engine.runKernel.x (face-api.min.js:1)    at face-api.min.js:1Wr @ face-api.min.js:1t.createProgram @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.getAndSaveBinary @ face-api.min.js:1t.compileAndRun @ face-api.min.js:1t.conv2dWithIm2Row @ face-api.min.js:1t.conv2d @ face-api.min.js:1Zt.engine.runKernel.x @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.runKernel @ face-api.min.js:1conv2d_ @ face-api.min.js:1conv2d @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.tidy @ face-api.min.js:1t.tidy @ face-api.min.js:1Kh @ face-api.min.js:1r.runMobilenet @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1t.scopedRun @ face-api.min.js:1t.tidy @ face-api.min.js:1t.tidy @ face-api.min.js:1r.forwardInput @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1n @ face-api.min.js:1Promise.then (async)o @ face-api.min.js:1(anonymous) @ face-api.min.js:1p @ face-api.min.js:1(anonymous) @ face-api.min.js:1e.runAndExtendWithFaceDetections @ face-api.min.js:1e.withFaceLandmarks @ face-api.min.js:1(anonymous) @ script.js:24face-api.min.js:1 C:\fakepath(179;2-41): warning X3550: array reference cannot be used as an l-value; not natively addressable; forcing loop to unrollC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\fakepath(179;2-41): warning X3550: array reference cannot be used as an l-value; not natively addressable; forcing loop to unrollC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D Shadershow can i fix this issue,['Duplicate of #327====='],Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Shader Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/327,Error: Failed to link vertex and fragment shaders.,6,closed,2019-06-18T15:03:51Z,2019-07-13T14:42:06Z,"I get this error:![‏‏errorImg](https://user-images.githubusercontent.com/18192032/59695587-2546fb00-91f3-11e9-8896-5373c99a0c19.PNG)package.json: > {  ""name"": ""eyes-blinking"";  ""version"": ""0.0.0"";  ""scripts"": {    ""ng"": ""ng"";    ""start"": ""ng serve"";    ""build"": ""ng build"";    ""test"": ""ng test"";    ""lint"": ""ng lint"";    ""e2e"": ""ng e2e""  };  ""private"": true;  ""dependencies"": {    ""@angular/animations"": ""~7.2.0"";    ""@angular/cdk"": ""~7.3.7"";    ""@angular/common"": ""~7.2.0"";    ""@angular/compiler"": ""~7.2.0"";    ""@angular/core"": ""~7.2.0"";    ""@angular/flex-layout"": ""^8.0.0-beta.25"";    ""@angular/forms"": ""~7.2.0"";    ""@angular/material"": ""^7.3.7"";    ""@angular/platform-browser"": ""~7.2.0"";    ""@angular/platform-browser-dynamic"": ""~7.2.0"";    ""@angular/router"": ""~7.2.0"";    ""core-js"": ""^2.5.4"";    ""face-api.js"": ""^0.20.0"";    ""hammerjs"": ""^2.0.8"";    ""ngx-webcam"": ""^0.2.2"";    ""rxjs"": ""~6.3.3"";    ""tracking"": ""^1.1.3"";    ""tslib"": ""^1.9.0"";    ""webpack"": ""^4.32.0"";    ""zone.js"": ""~0.8.26"";    ""@tensorflow/tfjs-core"": ""1.0.4"";    ""tfjs-image-recognition-base"": ""^0.5.1""  };  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.13.0"";    ""@angular/cli"": ""~7.3.6"";    ""@angular/compiler-cli"": ""~7.2.0"";    ""@angular/language-service"": ""~7.2.0"";    ""@types/jasmine"": ""~2.8.8"";    ""@types/jasminewd2"": ""~2.0.3"";    ""@types/node"": ""~8.9.4"";    ""@types/tracking"": ""^1.1.29"";    ""codelyzer"": ""~4.5.0"";    ""jasmine-core"": ""~2.99.1"";    ""jasmine-spec-reporter"": ""~4.2.1"";    ""karma"": ""~4.0.0"";    ""karma-chrome-launcher"": ""~2.2.0"";    ""karma-coverage-istanbul-reporter"": ""~2.0.1"";    ""karma-jasmine"": ""~1.1.2"";    ""karma-jasmine-html-reporter"": ""^0.2.2"";    ""protractor"": ""~5.4.0"";    ""ts-node"": ""~7.0.0"";    ""tslint"": ""~5.11.0"";    ""typescript"": ""~3.2.2"";    ""@tensorflow/tfjs-node"": ""^1.0.2""  }}","['Similar error; browser Chrome 75.0.3770.100 (Windows 10; 64 bit)face-api.js v0.20.0 and 0.19.0tf-core.esm.js:4786 C:\\fakepath(180;2-134): error X3500: array reference cannot be used as an l-value; not natively addressableC:\\fakepath(159;7-117): error X3511: forced to unroll loop; but unrolling failed.C:\\fakepath(157;7-117): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\\fakepath(180;2-134): error X3500: array reference cannot be used as an l-value; not natively addressableC:\\fakepath(159;7-117): error X3511: forced to unroll loop; but unrolling failed.C:\\fakepath(157;7-117): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\\fakepath(180;2-134): warning X3550: array reference cannot be used as an l-value; not natively addressable; forcing loop to unrollC:\\fakepath(180;2-134): error X3500: array reference cannot be used as an l-value; not natively addressableC:\\fakepath(159;7-117): error X3511: forced to unroll loop; but unrolling failed.C:\\fakepath(157;7-117): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D ShadersUncaught (in promise) Error: Failed to link vertex and fragment shaders.    at xr (tf-core.esm.js:4786)    at e.createProgram (tf-core.esm.js:5381)    at tf-core.esm.js:5578    at tf-core.esm.js:8507    at e.getAndSaveBinary (tf-core.esm.js:8524)    at e.compileAndRun (tf-core.esm.js:8506)    at e.conv2dWithIm2Row (tf-core.esm.js:8277)    at e.conv2d (tf-core.esm.js:8282)    at We.engine.runKernel.x (tf-core.esm.js:9797)    at tf-core.esm.js:1494====='; 'try this: right click on computer -> properties -> task manager -> display adapters -> right click on your graphic card (I have Intel (R) HD Graphics 3000) and update driver; disable device; enable device. It should do the job ====='; 'That didnt solved my problemAnd I have Intel (R) HD Graphics 3000 too====='; 'This error comes from the tfjs-core WebGL backend. Could be some device issue; for example some android devices too have the same issue: https://github.com/tensorflow/tfjs/issues/952; #151.Closing since this is not really something that face-api.js has control over; probably better to file an issue at tfjs. I would also recommend playing around with some tfjs examples first; to see whether this issue occurs and which ops might cause this issue.====='; ""@justadudewhohacks there's any workaround for this?In my case it's failing on all Samsung devices and Motorola.=====""; 'That didnt solved my problemAnd I have Intel (R) HD Graphics 4000 too; I5 and 6GB=====']",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/295,How to improve performance using node.js with openblas,7,closed,2019-05-14T11:51:04Z,2019-05-28T01:32:44Z,"I'm also experiencing slowness problem when using faceRecognition on api-faces. I use windows and my tests are taking around three seconds to identify the face.I already installed openblas on windows; but it looks like faces-api.js does not recognize it.Sorry; you are requested to use face-recognition.js and this repository is saying This package is pretty much obsolete. I recommend you to switch to face-api.js.Note: I'm already using dependencies:""@tensorflow/tfjs-node"": ""^1.0.2""Please help me; I'm loving this api.","['Does it print any warning when importing tfjs-node? If there is a version mismatch; it might not be able to speed up things via tfjs-node; but using ""@tensorflow/tfjs-node"": ""^1.0.2"" should actually work.Also keep in mind that face-api.js does not use openblas.====='; 'Thank you for the answer; please.See package.json:{\xa0\xa0""name"": ""ussrecognizeface"";\xa0\xa0""version"": ""1.0.0"";\xa0\xa0""description"": ""face identifier"";\xa0\xa0""main"": ""server.js"";\xa0\xa0""scripts"": {\xa0\xa0\xa0\xa0""test"": ""echo \\"" Error: no test specified \\ ""&& exit 1"";\xa0\xa0\xa0\xa0""start"": ""nodemon server.js --ignore * .marko.js""\xa0\xa0};\xa0\xa0""author"": ""Ulisses Silva de Souza"";\xa0\xa0""license"": ""ISC"";\xa0\xa0""dependencies"": {\xa0\xa0\xa0\xa0""@ tensorflow / tfjs-node"": ""1.0.2"";\xa0\xa0\xa0\xa0""canvas"": ""2.0.1"";\xa0\xa0\xa0\xa0""express"": ""^ 4.16.4"";\xa0\xa0\xa0\xa0""face-api.js"": ""^ 0.20.0"";\xa0\xa0\xa0\xa0""marko"": ""^ 4.16.13""\xa0\xa0};\xa0\xa0""devDependencies"": {\xa0\xa0\xa0\xa0""nodemon"": ""^ 1.19.0""\xa0\xa0}}See the log after npm install://////////////\xa0HOME LOG //////////////////////////////////> @ tensorflow / tfjs-node @ 1.0.2 install C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ @tensorflow \\ tfjs-node> node scripts / install.js* Downloading libtensorflow[=============================== 1150733 / bps 100% 0.0s* Building TensorFlow Node.js bindings> canvas@2.0.1 install C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download[canvas] Success: ""C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ canvas \\ build \\ Release \\ canvas-prebuilt.node"" is installed via remote> nodemon@1.19.0 postinstall C: \\ development \\ salesusWeb \\ DropboxSvnRepository \\ ussRecognizeFace \\ node_modules \\ nodemon> node bin / postinstall || exit 0Love nodemon? You can now support the project via the open collective:\xa0> https://opencollective.com/nodemon/donatenpm WARN ussrecognizeface@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules \\ fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {""os"": ""darwin""; ""arch"": ""any""} (current: {""os"": ""win32""; ""arch"": ""x64""})added 392 packages from 253 contributors and audited 2600 packages in 70.31sfound 0 vulnerabilities////////////// FIM LOG //////////////////////////////////See the log after faceRecognition.js:done; saved results to out/queryImage.jpg====='; 'Can the pc processor be the cause of being slow?My machine:Intel (R) Core (TM) i7-3537U CPU @ 2.00GHz 2.50 GHzMemory: 8GBsystem type: 64-bit====='; ""Hmm it could be; just curious; are you actually importing `import '@tensorflow/tfjs-node';` in your code?Also; do you mean the entire faceRecognition example is running for ~3s? Keep in mind that this example is processing 2 images; detecting all faces on them and for each detected face computing the landmarks and face descriptors; so 3s seems to be reasonable on the CPU.Usually in the browser execution is much faster since ops are run on the GPU via WebGL. If you have an nvidia gpu you could also use the gpu version of tfjs-node.You can also use the tiny face detector instead of the ssd mobilenet detector; which is faster but less accurate.=====""; '1 - Yes I am using require (""@ tensorflow / tfjs-node"");see your env.js:""use strict"";exports .__ esModule = true;// import nodejs bindings to native tensorflow;// not required; but will speed things up drastically (python required)require (""@ tensorflow / tfjs-node"");// implements nodejs wrappers for HTMLCanvasElement; HTMLImageElement; ImageDatavar canvas = require (\'canvas\');exports.canvas = canvas;var faceapi = require (""../../../ face-api.js"");// patch nodejs environment; we need to provide an implementation of// HTMLCanvasElement and HTMLImageElement; additionally an implementation// of ImageData is required; in case you want to use the MTCNNvar Canvas = canvas.Canvas; Image = canvas.Image; ImageData = canvas.ImageData;faceapi.env.monkeyPatch ({Canvas: Canvas; Image: Image; ImageData: ImageData});2 - Yes it is taking 3 seconds to compare images.3 - As I\'m developing an application that will compare a base image with several others in the repository; then 3 seconds is a lot.But thanks for the help.====='; 'Alright; yes 3s is a lot; thus it is better to get some GPU acceleration for example by using `@tensorflow/tfjs-node` in your nodejs app. But you will need an nvidia GPU.====='; 'I have NVIDIA GeForce GT 740m.However; I spent several hours trying to configure @ tensorflow / tfjs-node-gpu without success.Would you take a step by step?Using gpu I would increase the speed in how much?=====']",Slow Execution,Poor Performance,Incorrect Code Logic,TF(CPU),Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",B.1.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/262,[electron] produces different results compared to web,6,closed,2019-04-01T09:25:10Z,2019-04-10T05:58:43Z,,['???====='; '**Info:**Loading face-api.js in a renderer process of my electron app.face-api.js version 0.19.1Using face-api.js with electron![1](https://user-images.githubusercontent.com/13446367/55370581-7c5efd00-552d-11e9-938a-c2f56e7f4eb4.jpeg)![2](https://user-images.githubusercontent.com/13446367/55370617-98fb3500-552d-11e9-9464-54c5832808a1.jpeg)**electron app：**await faceapi.detectSingleFace(videoEl; options) === undefined**WEB**![3](https://user-images.githubusercontent.com/13446367/55370727-0d35d880-552e-11e9-80e8-975b8c32a34f.jpg)**electron app**![4](https://user-images.githubusercontent.com/13446367/55370776-440bee80-552e-11e9-8b99-3e65cff92ade.jpg)====='; '@justadudewhohacks ====='; '![1554278009373](https://user-images.githubusercontent.com/13446367/55462260-e6a89800-5628-11e9-90d8-bb6621a8bf89.jpg)====='; 'Which backends are you using on the web and electron example; webgl or cpu?====='; '@justadudewhohacks Thanks for you;i replaced electron with nw.js; nw.js is ok.====='],Incorrect Functionality,Incorrect Functionality,Cross-platform App Framework Incompatibility,Desktop,Platform,change cross-platform framework,change cross-platform framework,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.3
https://github.com/justadudewhohacks/face-api.js/issues/209,Slow at the first Check,3,closed,2019-02-01T17:41:54Z,2019-02-01T18:48:17Z,fullFaceDescriptions = await faceapi                                                    .detectSingleFace(videoEl; new faceapi.SsdMobilenetv1Options({ inputSize:256 }))                                                    .withFaceLandmarks()                                                    .withFaceDescriptor()desc=fullFaceDescriptions.descriptor1st time when the above code is executed; it takes around 3-4 seconds. Successive execution takes around half a second. Why is that so?,"['yeah because it loads the models first & then it works fine.====='; ""> yeah because it loads the models first & then it works fine.I have include these lines before that codeawait faceapi.loadSsdMobilenetv1Model('/src/store/models/')await faceapi.loadFaceLandmarkTinyModel('/src/store/models/')await faceapi.loadFaceLandmarkModel('/src/store/models/')await faceapi.loadFaceRecognitionModel('/src/store/models/')=====""; 'Basically answered this in #160:> The reason why the initial call takes longer; is due to the shaders being compiled; which is referred to as ""warmup time"".=====']",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,call tf.nextFrame,call tf.nextFrame,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.2,D.4
https://github.com/justadudewhohacks/face-api.js/issues/125,Use SSD_MobileNet_V1 model; cause box width and height must be positive numbers,2,closed,2018-11-02T09:43:14Z,2018-11-13T09:18:22Z,"Hello; I try to use face-api.js by follow this article[https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07](https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07)But when I run the code; the console output the error:## Error> face-api.js:1118 Uncaught (in promise) Error: Box.constructor - width (0.08514529466629028) and height (-0.15933257341384888) must be positive numbers>     at Function.Box.assertIsValidBox (face-api.js:1118)>     at Rect.Box (face-api.js:1103)>     at new Rect (face-api.js:1387)>     at face-api.js:4774>     at Array.map (<anonymous>)>     at SsdMobilenetv1.<anonymous> (face-api.js:4765)>     at step (face-api.js:317)>     at Object.next (face-api.js:298)>     at fulfilled (face-api.js:288)## EnvironmentChrome: 70.0.3538.77System: Windows10Node.js: v11.0.0express: v4.16.3## Code```var MODEL_URL = '/models'var canvas = $('#canvas')[0];var imgObj = new Image();imgObj.src = ""/images/face-sample.jpg"";imgObj.onload = function(){        var ctx = canvas.getContext('2d');        ctx.drawImage(this; 0; 0);    }async function start() {    var MODEL_URL = '/models'    await faceapi.loadSsdMobilenetv1Model(MODEL_URL)    await faceapi.loadFaceLandmarkModel(MODEL_URL)    await faceapi.loadFaceRecognitionModel(MODEL_URL)    console.log('Model loaded successed!')    var input = canvas    var fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()    var detectionsArray = fullFaceDescriptions.map(fd => fd.detection)    faceapi.drawDetection(canvas; detectionsArray; { withScore: true })    const landmarksArray = fullFaceDescriptions.map(fd => fd.landmarks)    faceapi.drawLandmarks(canvas; landmarksArray; { drawLines: true })    console.log(detectionsArray)}start()```## Sample Image![face-sample](https://user-images.githubusercontent.com/11308780/47908134-95547b00-dec7-11e8-9067-c4f46b22386e.jpg)## MessageWhen this code run in my friends' computer; it works. I'm confused about it.And When I try to play demo in this url [https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection](https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection); and I try to use SSD_MobileNet_V1 model; I got the same problem.","[""Ahh yeah; that might be because you run the thing on an Intel GPU probably (please correct me if I am wrong with that assumption)? Intel GPUs seems to not be supported yet (https://github.com/tensorflow/tfjs/issues/510); thus the predicted bounding boxes are totally wrong resulting in this error.Nevertheless; it might be better to handle bounding boxes; that are out of image borders; although it doesn't help in your case; since the prediction is simply wrong.You could use the TinyFaceDetector; which should work with an Intel GPU.=====""; ""> Ahh yeah; that might be because you run the thing on an Intel GPU probably (please correct me if I am wrong with that assumption)? Intel GPUs seems to not be supported yet ([tensorflow/tfjs#510](https://github.com/tensorflow/tfjs/issues/510)); thus the predicted bounding boxes are totally wrong resulting in this error.> > Nevertheless; it might be better to handle bounding boxes; that are out of image borders; although it doesn't help in your case; since the prediction is simply wrong.> > You could use the TinyFaceDetector; which should work with an Intel GPU.Okay; I found that TinyFaceDetector is work for me; thank you.(But it is not as accurate as SSD_MobileNet model XD)=====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change model,Changing model,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/81,Same code; different results (0.11.0/0.12.0),3,closed,2018-09-02T14:31:37Z,2019-01-17T09:31:32Z,Hi;I was locating faces with SSD Mobile Net (faceapi.locateFaces) because with other nets (MTCNN; Tiny Yolo v2) the result was unstable. Now I updated the library from 0.11.0 to 0.12.0 version and same code produces different results. At 0.11.0 version faceapi.locateFaces was stable but with 0.12.0 version is unstable (like MTCNN; Tiny Yolo v2 at 0.11.0).Greetings and thanks for the repository.,"[""Hmm; I couldn't notice any difference regarding ssd mobilenet . Can you upload a picture to show me what kind of differences you are seeing?The only real difference between 0.11.0 and 0.12.0 is; that I upgraded tfjs-core from 0.11.9 to 0.12.14.=====""; 'yep; the problems come with tfs-core >= 0.12.0. With unstable I want to say multiple detection in same region and continuous interruptions while detecting faces.====='; 'If this is still an issue with the recent version feel free to reopen; giving atleast one example showing the issue.=====']",Unstable,Poor Performance,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Inconsistent Results""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",B.3.2,E
https://github.com/infinitered/nsfwjs/issues/521,NSFWjs not working in lambda,5,open,2021-06-21T04:23:05Z,2021-07-16T12:09:58Z,"I tried using nsfwjs to detect images upload to s3 as safe or not but it is not working in lambda; I tried increasing memory to 10GB. But it still timed out :( Log: ```START RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851e Version: $LATEST2021-06-21T03:34:53.247Z    18f3c1c7-3341-4973-b35c-b638cadd851e    WARN============================Hi there :wave:. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================END RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851eREPORT RequestId: 18f3c1c7-3341-4973-b35c-b638cadd851e    Duration: 30033.57 ms    Billed Duration: 30000 ms    Memory Size: 10240 MB    Max Memory Used: 121 MB    Init Duration: 606.54 ms2021-06-21T03:35:23.277Z 18f3c1c7-3341-4973-b35c-b638cadd851e Task timed out after 30.03 seconds```Code (for local testing):```//  const aws = require('aws-sdk');const tf = require('@tensorflow/tfjs');const sizeOf = require('image-size');const jpeg = require('jpeg-js');const nsfwjs = require('nsfwjs')const axios = require('axios')//const s3 = new aws.S3({ apiVersion: '2006-03-01' });//exports.handler = async (event; context) => {async function abc(){  const model = await nsfwjs.load();  const readImage = buf => {    var imgprodata = sizeOf(buf)    if(imgprodata.type != ""JPEG"" && imgprodata.type != ""JPG"" && imgprodata.type != ""jpeg""&& imgprodata.type != ""jpg""){      let opt = {        height: imgprodata.height;        width: imgprodata.width;        data: buf;      }      const pixels = jpeg.encode(opt;100)      return pixels    } else {    const pixels = jpeg.decode(buf; true)    return pixels    }  }    const imageByteArray = (image; numChannels) => {    const pixels = image.data    const numPixels = image.width * image.height;    const values = new Int32Array(numPixels * numChannels);      for (let i = 0; i < numPixels; i++) {      for (let channel = 0; channel < numChannels; ++channel) {        values[i * numChannels + channel] = pixels[i * 4 + channel];      }    }      return values  }    const imageToInput = (image; numChannels) => {    const values = imageByteArray(image; numChannels)    const outShape = [image.height; image.width; numChannels];    const input = tf.tensor3d(values; outShape; 'int32');      return input;  }    // const key = event.Records[0].s3.object.key;  const key = '7c1e158e-26a8-44a1-b219-bf85741e4754'  var pic = await axios.get(`https://public-usercontent.dogegram.xyz/storage/${key}`; {    responseType: 'arraybuffer'  });  var pix = await readImage(pic.data);  var img = await imageToInput(pix; 3)    const predictions = await model.classify(img)  console.log(predictions)  return 'done'  }//}abc()```For testing:```npm init -ynpm i @tensorflow/tfjs image-size jpeg-js nsfwjs axios```","[""While the jpeg-js should work; there's a better way to read images in node now.I haven't updated the docs here on NSFWJS but you could use things like https://js.tensorflow.org/api_node/3.7.0/#node.decodeJpegI cover this in my book.   Please give tfjs-node a go; and let me know if that solves your issues.=====""; ""Tfjs-node is too large to fit in lambda's size limit :(=====""; 'Tagging the issue here:  https://github.com/tensorflow/tfjs/issues/2817====='; '@hrichiksite - have you seen this?  https://stackoverflow.com/questions/59899650/running-tensorflow-js-tfjs-node-on-aws-lambda-node-js====='; ""> @hrichiksite - have you seen this? https://stackoverflow.com/questions/59899650/running-tensorflow-js-tfjs-node-on-aws-lambda-node-jsHi; I have seen this but I didn't get how to load NPM modules from a different DIR; I had tried to find but no success :(Actually; The Issue here is not Size anymore but the fact that it is timing out even 60 secs. =====""]",Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.4,D.1
https://github.com/infinitered/nsfwjs/issues/31,Blocked ui,6,closed,2019-02-28T14:31:42Z,2019-04-30T09:35:59Z,I found ui been blocked for few seconds when execute ”nsfwjs.load('/path/to/model/directory/') “. How to optimize it?,"[""Good call; I noticed this; too.  This block seems to be built into Tensorflow; and is very likely the fetch call.  I really don't know what's causing it; but this would be a fantastic PR if someone could find/fix this issue.=====""; 'I opened an issue at tensorflowjs repo.[tfjs#1373](https://github.com/tensorflow/tfjs/issues/1373)====='; ""PR #78 is ready; so when that's merged; we can test this a bit more.=====""; 'From my testing; it looks like this is fixed in TFJS 1.x====='; 'Closing this; I feel 1.x was the fix.====='; ""I'm on tf 1.1.0 and it still blocks the UI. Does someone knows what i can do?=====""]",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",B.1.2,D.4
https://github.com/justadudewhohacks/face-api.js/issues/732,detectAllFaces() Leaks Memory in WebWorker,3,open,2020-11-25T18:18:05Z,2021-04-30T16:26:54Z,I can confirm that `detectAllFaces` is leaking memory. I pass frames to a WebWorker with postMessage and transfer lists; then create an ImageData in the WebWorker as shown below.If I run the code as shown; the Chrome process memory balloons and grows DURING the frame processing. (E.g. while the webworker is inside `detectAllFaces`. Within ~3 frames; Chrome is using 25% of the RAM and growing. After ~30 seconds; Chrome has consumed nearly all of my 16 GB of RAM.As soon as I comment out the line that has `detectAllFaces` - memory usage flatlines. Does not grow or change. And that is the ONLY code change I make - with detectAllFaces; memory leaks. Without that call; no memory leak. Therefore; something is going wrong with detectAllFaces - because there are no other references to the given imgData other than detectAllFaces.Any ideas on how to diagnose further or; hopefully; fix?```// Earlier in the code ...faceapi.nets.ssdMobilenetv1.loadFromUri('/models')this.faceDetectorOptions = new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 });// For every message from the parent thread; we do...const imgData = new ImageData(	new Uint8ClampedArray(message.data);	message.width;	message.height);const img = faceapi.createCanvasFromMedia(imgData);const results = await faceapi.detectAllFaces(img; this.faceDetectorOptions)```_Originally posted by @josiahbryan in https://github.com/justadudewhohacks/face-api.js/issues/345#issuecomment-733872505_,"[""I think this might be specific to using the `cpu` tensorflow backend...why?I'm running my app over VNC on a laptop in latest chrome - in the devtools of that chrome; I see warnings about `webgl` not supported. Tried using `wasm` - that failed completely. So logic is that it falls back to the `cpu` backend.Running the same code - EXACT same code - in a browser locally on my Mac (same Chrome version; just local - not over VNC) - and no `webgl` warnings; and I can even use `wasm` - and no memory leaks. (Using Chrome's Task Manager; the RAM usage for that tab + worker stays flat)So; something about the CPU backend...? What's up with that...?=====""; 'Can confirm the memory leak. @josiahbryan  could you solve that in the meanwhile?====='; 'Never tried to solve CPU memory leak - my use case did not depend on CPU backend because I could reply on using webgl in production.=====']",Memory Leak,Poor Performance,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.2.1,D.1
https://github.com/justadudewhohacks/face-api.js/issues/604,Node js real time,5,open,2020-04-24T20:04:04Z,2020-07-01T05:39:50Z,"Hello;thank you very much for this library!I'm trying to run a small face detection program in node js. It takes as input a frame read from the webcam and output the detection object.I see that the browser version does realtime detection while with the node version it takes around 350ms per frame which is very low compared.I took some measures of just the prediction step```console.time(""detect"")const detections = await faceapi.detectAllFaces(frame)console.timeEnd(""detect"")```detect: 442.732msdetect: 363.256msdetect: 365.847msdetect: 338.513msdetect: 334.374msdetect: 340.412msdetect: 324.549msdetect: 333.849msdetect: 327.226msdetect: 325.315msdetect: 322.517msdetect: 322.398msI am already importing the tfjs-node library and I am running the program on a 2;6 GHz Intel Core i7 quad-core 16gb ram MacBook.I just want to know if that frame rate is common or I am facing slow performances.Thank you","['@tonxxd - I\'m at pretty much the same point as you and coming up against this issue running tfjs in Node on a Rasp Pi 4. The best performance I can get at the moment is around 550-600ms....Detecting Faces: 595.007msDetecting Faces: 588.997msDetecting Faces: 549.516msDetecting Faces: 616.711msDetecting Faces: 580.152msDetecting Faces: 562.429msDetecting Faces: 540.243msDetecting Faces: 551.98msDetecting Faces: 532.101msDetecting Faces: 578.383msFrom reading a number of other issues; here are some things to check. * Do you have the same version of `@tensorflow/tfjs-node` as is in the current version of face-api.js\'s `package.json`? Currently this is... ```    ""@tensorflow/tfjs-core"": ""1.7.0"";    ""@tensorflow/tfjs-node"": ""1.7.0"";    ""@tensorflow/tfjs-node-gpu"": ""1.7.0"";    ""face-api.js"": ""0.22.2"";```* Have you tried using `tfjs-node-gpu` over `tfjs-node`; this made little difference for me on Raspberry Pi but may help on a Macbook. * What nets are you using; apparently some have different performance impacts?====='; ""Hi @ChrisDalley thanks for the quick response!I tried the tiny face detector and it gives much higher performances (~ 60ms per frame). Changing to node-gpu didn't increase performances (strange(?)).Anyway; what I don't understand is why the node version is slower than the browser one when tensorflow has access to the Native C modules in the node env. I have the exact same app in python (with tensorflow for python and opencv-python) and I get real-time detections without issues. =====""; ""I'm honestly not too sure on the difference between Python and the node implementation; it's my first time touching this library today. I'm going to do some more digging over the next few days and see what I can figure out - even getting down to 60ms for me would be better than where it's at right now! Maybe @justadudewhohacks knows more about the difference between Python + Node / where we are going wrong performance wise?=====""; '@ChrisDalley yes but that was on my MacBook; since I plan to run the app on fewer resources I am looking for a better strategy as you ====='; '@ChrisDalley could you able to get any improvement? I am also facing same issue in my MacBook. It is taking ~7.5 sec for each image which is quite a lot in these days. I used GPU as well; but not much difference.Please help if you got any breakthrough.Thanks in advance.=====']",Slow Execution,Poor Performance,Incorrect Code Logic,TF(CPU),Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",B.1.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/585,out of memory issue with face reacognition.js,1,open,2020-03-23T08:16:51Z,2020-11-25T18:21:37Z,Hii;The error is: out of memory issue. Is there any memory dispose method in face-api.js and i also tried with tensorflow memory leak issue still am getting the same error. what best could be done to resolve this??2020-03-23 13:41:31.839784: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocating tensor with shape[1;256;256;64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpuException code=0xc0000005 flags=0x0 at 0x00007FF95CFFA277. Access violation - attempting to read data at address 0x0000000000000010,"[""Did you ever find a solution to this? I'm running into leaks in WebWorkers myself and can't find a way to get memory be freed. (Noted in another issue: https://github.com/justadudewhohacks/face-api.js/issues/732)=====""]",Out of Mermory,Poor Performance,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.2] Out of Memory""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B,D.1
https://github.com/justadudewhohacks/face-api.js/issues/417,Poor performance in browser,8,open,2019-09-18T07:31:23Z,2019-10-14T02:13:47Z,`fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()` takes randomly 5-15 seconds. Is it expected when doing face detection in browser?,"['No; depending on the device you are running this on; this should only take a few miliseconds.`faceapi.detectAllFaces(input)` by default also by default uses the SSDMobilenetV1 model; which can potentially be slower on some devices. You can try out the tiny face detector instead (see README).Also make sure the WebGL backend is utilized: `console.log(tf.getBackend())`.====='; 'what is `tf` and how to import it?====='; '```jsimport * as faceapi from ""face-api.js"";faceapi.tf.getBackend()```@avalanche1 ====='; '@justadudewhohacks By saying ""a few miliseconds""; you mean ""a few hundreds miliseconds""? :)I\'m using face-api.js in a non-critical page and I got some stats.conf:* tiny face detector* face-api.js@0.20.1 It\'s roughly like these:Total users: about 20k.75% took below 500ms.10% took (500; 1500)ms.5% took 1000ms+.The extreme case did take more than 10 seconds.I\'ve tried face-api.js@0.20.1 on some computer; performance seems go down a little bit.What\'s your suggestions to improve performance? or gain some other stats?BTW; here are some cases I don\'t understand.```// took 1400ms to finish a single detectionwebgl: true; tf_backend: webgl; os: win7; cpu: Intel(R) Core(TM) i5-4200U CPU @ 1.60GHz// took 2400ms to finish a single detectionwebgl: null; tf_backend: cpu; os: win10; cpu: Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz```Thanks====='; '@zjlovezj; the tiny face detector model on my desktop machine with webgl backend and an input size of 160 runs at about 60 fps; so the time for inference is about 20ms or less. On my android it runs at 15 fps. The inference time obviously depends on the device.The question is; do your metrics include the shader compilation times? You should exclude the very first forward pass from your measurements.====='; 'Yes. I did exclude the first detection. And the numbers are average of 20 times of detection.====='; ""@zjlovezj I Think a repro would help; I'll try to hack up smth as well=====""; ""@avalanche1 I don't think a repo would help much on this performance issue. The results are all different on different machine with different configuration.=====""]",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,D.1
https://github.com/justadudewhohacks/face-api.js/issues/294,Browser example not working on chrome version 74.0.3729.131,4,open,2019-05-13T23:12:02Z,2021-02-25T11:36:39Z,Hi There;The browser example doesn't seem to work on the Chrome browser version 74.0.3729.131.Below is the error which I see in the console:C:\fakepath(179;2-41): warning X3550: array reference cannot be used as an l-value; not natively addressable; forcing loop to unrollC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with default flags. (ps_3_0) Retrying with avoid flow controlC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with avoid flow control flags. (ps_3_0) Retrying with prefer flow controlC:\fakepath(179;2-41): warning X3550: array reference cannot be used as an l-value; not natively addressable; forcing loop to unrollC:\fakepath(179;2-41): error X3500: array reference cannot be used as an l-value; not natively addressableC:\fakepath(158;7-60): error X3511: forced to unroll loop; but unrolling failed.C:\fakepath(156;7-60): error X3511: forced to unroll loop; but unrolling failed.Warning: D3D shader compilation failed with prefer flow control flags. (ps_3_0)Failed to create D3D ShadersUncaught (in promise) Error: Failed to link vertex and fragment shaders.    at linkProgram (tf-core.esm.js:17)    at e.createProgram (tf-core.esm.js:17)    at compileProgram (tf-core.esm.js:17)    at tf-core.esm.js:17    at e.getAndSaveBinary (tf-core.esm.js:17)    at e.compileAndRun (tf-core.esm.js:17)    at e.conv2dWithIm2Row (tf-core.esm.js:17)    at e.conv2d (tf-core.esm.js:17)    at ENV.engine.runKernel.x (tf-core.esm.js:17)    at tf-core.esm.js:17,['Hi;I am with the same error I verified that this only happens and machines with little memory and old processors I carry out the tests with machines with 8GB and Processors I5 worked perfectly; however the machines 4GB and Processes Phenom II X4 or similar gave error====='; 'I have intel I5 with 6GB and got same error. ====='; 'getting same error; on 6GB; i3. however its working with 16gb i7. ====='; '> Hi;> > I am with the same error I verified that this only happens and machines with little memory and old processors I carry out the tests with machines with 8GB and Processors I5 worked perfectly; however the machines 4GB and Processes Phenom II X4 or similar gave errorHi;I have 8gb and i5 and have the same problem on the Chrome; at the same time on the browser Safari works fine.====='],Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.4,D.1
https://github.com/justadudewhohacks/face-api.js/issues/742,Why is my app so fast with iPhone and slow with MacOS?,1,open,2020-12-30T15:40:04Z,2021-01-05T22:06:32Z,Hias many other users; I see that the very first recognition is successfully done after around ten seconds from the models loading. I have the most common scenario: a canvas where images are saved from the webcam stream. This actually happens with Chrome 87 on a MacBook Air 2019 with Big Sur installed.If I try the same webpage with my iPhone SE 2020 the first recognition happens in less than one second probably; with Safari.I would have expected the opposite behavior. Is there any reason why iPhone is so faster than Mac? I don't think it is a GPU nor a WebGL stuff... Thanks!,"[""you haven't said which backend is used in both cases (webgl; wasm; cpu)? (and if `wasm`; if `simd` is enabled or not)  but in general; MacBook Air 2019 does not have a decidated GPU - it has integrated Intel UHD which is pretty bad.   on the other hand; iPhone SE 2020 has an actual decent GPU. so no surprise your phone is faster than notebook - if you're using `webgl` backend.also note that fast vs slow is different than initialization time.   e.g; `webgl` shader initialization and upload can take quite a lot of time; but then run smooth after that - that is quite common for `webgl` backend with more complex models.  on the other hand; `wasm` backend has almost instant initialization but doesn't run anywhere as fast when compared to `webgl` on a good GPU.=====""]",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,D.1
https://github.com/justadudewhohacks/face-api.js/issues/677,I´m getting the following error; I´ll leave it in the comment,4,open,2020-08-07T01:25:10Z,2021-04-12T08:49:51Z,"613face-api.min.js:1 Uncaught (in promise) Error: Box.constructor - expected box to be IBoundingBox | IRect; instead have {""left"":0.15384615384615385;""top"":null;""right"":0.15384615384615385;""bottom"":null}    at Vp.Wp (face-api.min.js:1)    at new Vp (face-api.min.js:1)    at Gg.<anonymous> (face-api.min.js:1)    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)","[""Perhaps it's because your gpu is not compatible with the current tensorflow version. I've got the same problem and solved it changing the backend to cpu.=====""; ""> is not compatible with the current tensorflow version. I've got the same problem and solved it changing the backend to cpu.how did you change it?=====""; 'I was having this error when running the html served in a node environment. Running it from traditional apache works fine.====='; 'Got similar error on Ionic platform using Angular 10 and Capacitor.js```Line 314410 - Msg: ERROR Error: Uncaught (in promise): Error: Box.constructor - expected box to be IBoundingBox | IRect; instead have {""left"":null;""top"":null;""right"":null;""bottom"":null}    Error: Box.constructor - expected box to be IBoundingBox | IRect; instead have {""left"":null;""top"":null;""right"":null;""bottom"":null}```It is normal when serving in Angular. When serving in a smartphone using Capacitor; it happens. Any clue? or is it just capacitor webview\'s fault?Thank you.=====']",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/justadudewhohacks/face-api.js/issues/736,Bad performance for browsers,12,open,2020-12-07T18:07:42Z,,I have found that performance is terribly slow for browsers. But that's the case at least if you use windows.Tested right on the face-api site example : https://justadudewhohacks.github.io/face-api.js/face_recognition . I tested it on several different windows machines. It takes roughly 25 seconds to recognise faces.https://user-images.githubusercontent.com/30739218/101359816-b224bf00-3894-11eb-801a-31e1c3bbfe4a.gifOn linux it works quite alright; taking 3 seconds with the example above. However there is one caveat. It works alright on chrome at this moment. For firefox there is a bug:https://bugzilla.mozilla.org/show_bug.cgi?id=1678652https://bugzilla.mozilla.org/show_bug.cgi?id=1679671In short; for firefox v83; there was a fix applied to how `failIfMajorPerformanceCaveat` property works. Now if a library uses `failIfMajorPerformanceCaveat: true` then webgl context creation fails and the library falls back to CPU; which is rather slow.However; that's true for linux only for now. On windows it still uses webgl; and still rather slow (20-30 seconds) for any browser.I am not sure if that's the issue of face-api.js or tensorflow itself,"[""I had a similar issue (on a mac)are you using tfjs2.0? that helps. I also am using a [webworker](https://github.com/justadudewhohacks/face-api.js/issues/47)have a look here also-  it's pretty fast[webcam example w react](https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking/)=====""; 'I am using face-api.js of 0.22.2 which in turn uses tfjs-core"": ""1.7.0"".  I guess I can fork and upgrade tfjs to see if the issue is fixed there====='; '@cindyloo  @JeviScript  upgrade is not that trivial as there are breaking changes between tfjs 1.x and 2.x; including ops used inside models.  take a look at <https://github.com/vladmandic/face-api>====='; '@vladmandic I did read through your code but I was able to remove /nodemodules/faceapi tensorflow 1.7 so it would default to tfjs2.0 and it worked fine...====='; '@cindyloo I\'ve heard that being done before; but not sure what is it supposed to accomplish since TFJS 1.7 is bundled inside face-`api.js` during build process. And if you try to rebuild `face-api.js` using TFJS 2.x; it fails.  (`face-api.js` incorrectly lists `@tensorflow/tfjs-core"": ""1.7.0` as dependency when in reality its a devDepencency)====='; ""hmm - if I look inside the plainscript face-api.js; I don't see any specific bundling. Wouldn't that mean it would look to the package settings and node_modules to determine the library?=====""; ""face-api.js includes bundled tfjs-core.If you don't even look at the code; look at sizes - face-api on its own is ~200kb; remaining ~600kb is tfjs-core.=====""; ""thanks for the clarification @vladmandic. I can see it invoking the tflib nowso; having revisited the codebase; I am getting reasonably good performance. The fix for me was the throttling of the face detection and using an async call for the analysis'analyzeVideo' is essentially the onPlay method in the exampleeg:```const pullAndPostFrameTimer = (videoEl; mode) => {  const faceDetectorOptions = getFaceDetectorOptions();  if (!videoEl || videoEl.ended || !faceDetectorOptions || faceDetectorOptions == undefined) {    setTimeout(function() {      pullAndPostFrameTimer(videoEl; mode);    }; THROTTLETIMER);    return;  }  let canvasVideoBuff = document.querySelector(    `#videoBuff${mode}`  ) as HTMLCanvasElement;  analyzeVideo(videoEl; mode; faceDetectorOptions);   setTimeout(function() {    pullAndPostFrameTimer(videoEl; mode);  }; THROTTLETIMER);};```=====""; ""i'm not sure i understand as this example is not complete.is `analyzeVideo` async function? if yes; then you have race scenario that is reduced a bit by having a delay between frames.why not put a promise in `analyzeVideo` and then do something like:```jsanalyzeVideo(videoEl; mode; faceDetectorOptions).then(() => requestAnimationFrame(() => pullAndPostFrameTimer(videoEl; mode));```or easier to read with async/await```jsawait analyzeVideo(videoEl; mode; faceDetectorOptions);requestAnimationFrame(() => pullAndPostFrameTimer(videoEl; mode));```that way loop will run as fast as possible and there is no race.=====""; ""@vladmandic so appreciate the note to race conditions. I had to review those within javascript ([this is a good review](https://medium.com/@slavik57/async-race-conditions-in-javascript-526f6ed80665)) as well as  `requestAnimationFrame`so here's the thing- performance seems to suffer a little with using `requestAnimationFrame` as there is no delay imposed and it attempts w every result to detect a face...I guess I could do a sleep or something?=====""; ""When it comes to video; I don't like adding sleep to solve problems.  Ideally; you could make it a two separate loops; both using requestAnimationFrame:  - One loop that handles rendering and uses last known results  - Second loop that handles processing    Ideally get ImageData on main thread and pass it as a reference to web worker thread     to handle detection and send results back to main thread.  That way main UI thread gets minimal blocking and first loop will run at full refresh rate of the screen.    (that is one benefit of requestAnimationFrame - it will run up to refresh rate and not faster if not needed)=====""; ""I took @vladmandic's advice to separate the rendering logic from the facial analysis (see the analyzeVideo vs. drawFaces routines). I alos tried to make my code work with `requestAnimationFrame` as well as with `requestVideoFrameCallback` but it ended up being really laggy (not sure what I was doing incorreclty.  The best solution I have found is to use the setTimeout/setInterval pattern eg:```const pullAndPostFrameTimer = (mode; anonymousParticipants) => {  const faceDetectorOptions = getFaceDetectorOptions();  // for each element tag passed in; add a timer participants.map((p; index) => {    const videoEl = whichVideo(`${mode}Video${p}`) as HTMLVideoElement;    if (      !videoEl ||      videoEl.ended ||      videoEl.readyState < 3 ||      !faceDetectorOptions ||      faceDetectorOptions == undefined    ) {      return false;    }    analyzeVideo(videoEl; faceDetectorOptions); // async    drawFaces(mode; p);  //async    return true;  });  setTimeout(function () {    pullAndPostFrameTimer(mode; participants);  }; THROTTLETIMER); // 8000 while waiting for faces; then speed it up to 2000ms (my best reaction time is between 1000-2000ms)};const FaceVisualiser = (params) => {  const { mode; participants } = params;  useEffect(() => {    if (participants) {      if (pullAndPostFrameTimer(mode; participants)) {        THROTTLETIMER = FACE_DETECTING; // 2000 ms        return;      }    }    THROTTLETIMER = NO_FACE_YET;    return function cleanup() {      THROTTLETIMER = NO_FACE_YET;    };  });    [participants];  return <div />;};```=====""]",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",B.1.1,D.1
https://github.com/mwdchang/tfjs-gan/issues/2,Update to TFJS version 1.1.2,4,closed,2019-05-05T14:58:06Z,2019-07-15T03:58:23Z,Great example. I really appreciate the live githubPages pure javascript format.Getting zeroslike errors when updating to TFJS v1.1.2 any suggestions? Works fine on Version 0.15.3; but breaks at Version 1.0.0,"['https://github.com/tensorflow/tfjs/issues/1398#issuecomment-490806579====='; ""Thanks for the report/information! Will play around with upgrading options when there's some free time on the weekends.=====""; 'https://github.com/tensorflow/tfjs/issues/1211#issuecomment-491829787====='; 'Bumped to 1.2.2=====']",Reference Error,Crash,Incorrect Code Logic,TF(CPU),Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/296,MobilenetV2 to tensorlfowjs webmodel,2,closed,2019-05-15T07:16:12Z,2019-05-23T05:57:50Z,Hey!Can you share me your scripts on how you converted the weights provided in https://github.com/yeephycho/tensorflow-face-detection to tfjs. I tried this but hit unsupported ops issue.tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='embeddings' saved_model/ output/ValueError: Unsupported Ops in the model before optimizationQueueDequeueUpToV2; FIFOQueueV2I saw https://github.com/justadudewhohacks/face-api.js/issues/50; but don't konw how to transform it,"['Hey;Sorry but there is no script. I did it mostly manually. If the tensorflowjs_converter says; that there are unsupported ops; then these ops remain to be implemented by the tfjs team. I am not familar with what these ops are doing ""QueueDequeueUpToV2; FIFOQueueV2"" but in some cases it is possible to skip postprocessing operations at the end of the graph and implement them manually.Maybe you want to try to visualize the graph using tensorboard first; to figure out; where these ops occur in the graph.====='; 'Thank for your answer; I tried restore a new graph and drop these unsupported ops node;It worked=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,modify model,modify model,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unsupported Operator""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/justadudewhohacks/face-api.js/issues/50,SSD to tensorflowjs convertor,4,closed,2018-07-14T11:50:40Z,2019-05-15T07:10:21Z,Hey Vincent; Can you enlighten me on how you converted the weights provided in https://github.com/yeephycho/tensorflow-face-detection to tfjs. I tried this but hit unsupported ops issue.                            tensorflowjs_converter --input_format=tf_frozen_model  --output_node_names='concat;concat_1' frozen_inference_graph_face.pb outputUnsupported Ops in the modelWhere; TopKV2; Assert; NonMaxSuppression,"[""Hi;I didn't use the tensorflow.js converter; because at that time way more ops were not supported than the ones you mentioned. I extracted them with some own scripts.=====""; ""You are a rock star. Being able to get SSD to work when there are certain ops that aren't supported. It would be great if you can share them with the bigger community.  I am not sure how comfortable you would be to share the scripts. Can totally understand if you couldn't=====""; 'Thanks; but it might actually be easier than you think. You could simply load your graphdef with tf + python and then save the weight values to binary files with numpy and get their names and shape from the graph; to construct the manifest.json file.I already commented on the [tfjs ssd_mobilenet issue](https://github.com/tensorflow/tfjs/issues/188); with how you could approach converting an ssd model to tfjs.I will close this issue here; as it is probably more appropriate to discuss this @tfjs.====='; 'Thank you for your response=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/justadudewhohacks/face-api.js/issues/803,FaceExpressionNet tflite,1,open,2021-06-22T06:11:16Z,2021-06-30T05:40:54Z,"HiI want to convert an emotion recognition model to tflite. But after conversion; the model works much worse. She almost always predicts ""neutral""; ""angry"" or ""happy"". The model from face api demo works much better. I created a model architecture using tensorflow.keras (tf==2.3.1). Then I loaded the weights using [decode_weights](https://github.com/tensorflow/tfjs/blob/77c4ef10fd3d8458f6958a52bac92fdaae09b885/tfjs-converter/python/tensorflowjs/read_weights.py#L126). Then saved as tf SavedModel. Then I converted to tfliteCode for creating architecture ```from tensorflow.keras import layers; Modeldef get_face_exp_net():    inputs = layers.Input((112;112;3))    out1 = layers.ReLU()(layers.Conv2D(32; 3; strides=[2;2]; padding='same')(inputs))    out2 = layers.SeparableConv2D(32; 3; padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1; out2]))    out3 = layers.SeparableConv2D(32; 3; padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1; out2; out3]))    out4 = layers.SeparableConv2D(32; 3; padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1; out2; out3; out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(64; 3; strides=[2;2]; padding='same')(end))    out2 = layers.SeparableConv2D(64; 3; padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1; out2]))    out3 = layers.SeparableConv2D(64; 3; padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1; out2; out3]))    out4 = layers.SeparableConv2D(64; 3; padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1; out2; out3; out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(128; 3; strides=[2;2]; padding='same')(end))    out2 = layers.SeparableConv2D(128; 3; padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1; out2]))    out3 = layers.SeparableConv2D(128; 3; padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1; out2; out3]))    out4 = layers.SeparableConv2D(128; 3; padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1; out2; out3; out4]))    out1 = layers.ReLU()(layers.SeparableConv2D(256; 3; strides=[2;2]; padding='same')(end))    out2 = layers.SeparableConv2D(256; 3; padding='same')(out1)    in3 = layers.ReLU()(layers.Add()([out1; out2]))    out3 = layers.SeparableConv2D(256; 3; padding='same')(in3)    in4 = layers.ReLU()(layers.Add()([out1; out2; out3]))    out4 = layers.SeparableConv2D(256; 3; padding='same')(in4)    end = layers.ReLU()(layers.Add()([out1; out2; out3; out4]))    end = layers.AvgPool2D((7;7); strides=(2;2); padding='valid')(end)    end = layers.Flatten()(end)    # end = layers.Dropout(0.5)(end)    end = layers.Dense(7)(end)    end = layers.Softmax()(end)    return Model(inputs=inputs; outputs=end)```Code for loading of weights and converting to tflite```    model_config = {'func': get_face_exp_net();                     'name': 'face expression net';                     'manifest': 'models/tfjs_models/face_expression_model-weights_manifest.json';                     'weights': 'models/tfjs_models/face_expression_model-shard1'                     }    print('Converting of '; model_config['name'])    with open(model_config['manifest']) as json_file:        manifest = json.load(json_file)    weights_bytes = read_weights(model_config['weights'])    # [{'name':'layer_name1'; 'data':np.array with weights};     # {'name':'layer_name2'; 'data':np.array with weights};    # ...]    weights = tfjs.read_weights.decode_weights([manifest]; [weights_bytes])[0]        model = model_config['func']    target_layers_ids = []    for i; l in enumerate(model.layers):        if 'conv' in l.name or 'dense' in l.name:            # print(i; l.name; [w.shape for w in l.get_weights()])            target_layers_ids.append(i)    # set weights to model    w_counter = 0    for layer_id in target_layers_ids:        num_w = len(model.layers[layer_id].get_weights())        w_list = [w['data'] for w in weights[w_counter:w_counter+num_w]]        model.layers[layer_id].set_weights(w_list)        w_counter+=num_w    model_name = ""_"".join(model_config['name'].split())    tf_model_path = 'models/tf_models/{}'.format(model_name)    model.save(tf_model_path)    print('TF model is saved to '; tf_model_path)    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)    tflite_model = converter.convert()    tflite_model_path = 'models/tflite_models/{}.tflite'.format(model_name)    open(tflite_model_path; ""wb"").write(tflite_model)    print('TFLite model is saved to '; tflite_model_path)```My code for inference```class FaceExpressionModel:    def __init__(self; tflite_path=os.getcwd()+'/tflite_face_api/models/tflite_models/face_expression_net.tflite';                ):        self.model = tf.lite.Interpreter(model_path=tflite_path)        self.model.allocate_tensors()        self.input_details = self.model.get_input_details()        self.output_details = self.model.get_output_details()        self.image_size = self.input_details[0]['shape'][1:3] # [112;112]    def normalize(self; img):        img[...; 0] -= 122.782        img[...; 1] -= 117.001        img[...; 2] -= 104.298        return img / 255.0    def pad_to_square(self; img):        h; w; ch = img.shape        if h==w:            return img        max_dim = max(h; w)        dim_diff = int(np.abs(h - w) * 0.5)        pad_array = np.zeros((max_dim; max_dim; ch))        if h > w:            pad_array[:; dim_diff:dim_diff+w; :] = img        else:            pad_array[dim_diff:dim_diff+h; :; :] = img        return pad_array    def preprocessing(self; img):        img = cv2.resize(img; (self.image_size)).astype(np.float32)        img = self.pad_to_square(img)        img = self.normalize(img)        return np.expand_dims(img; axis = 0)    def predict(self; img):        """"""        Does inference; preprocessing and returns probabilities for emotions        Arguments:        ----------        img : 3d np.ndarray            Single rgb image        Return:        ---------        1d np.ndarray            array of class probabilites; values from 0 to 1        """"""        img = self.preprocessing(img)        self.model.set_tensor(self.input_details[0]['index']; img)          self.model.invoke()        preds = self.model.get_tensor(self.output_details[0]['index'])[0]        return preds```",['assuming everything is correct; likely issue is loss of resolution or clipping of values - `tflite` conversion quantizes models to `uint8` which may not be enough for all interim processing results.====='],Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,Model API,API,re-converter model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/204,Safari iOS Exceeded WebGL Texture Size,3,closed,2019-01-26T03:56:59Z,2020-01-31T10:35:27Z,Thanks for open-sourcing this library. It works perfectly for me on desktop Safari and Chrome. But it doesn't work for me on either browsers for iPhone. On Safari; I get the following error when I try loading the face detector and landmark detectors:`Unhandled Promise Rejection: Error: Requested texture size [6321x16] greater than WebGL maximum on this browser / GPU [4096x4096] in 161.50cc6bac.chunk.js:13429`Would you know how to handle this issue? Thanks.,"[""Unfortunately I don't have access to mobile safari. Which example are you referring to and which function call is throwing this error; can you post a stacktrace?=====""; ""Actually; I can run the demo api on Ipad Safari; but it's very sluggish. I am wondering why is that so. I am wondering if a surface pro could do better since I want to use it on a tablet. =====""; 'We were facing this problem also in our project. We were able to resolve this by manually loading the face-api.js rather than loading from npm.=====']",Data & Model Error,Crash,Device Incompatibility,Device,Device,manually loading framework,Changing version,framework,Model loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Limits""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.2,D.1
https://github.com/justadudewhohacks/face-api.js/issues/627,tsc can't compile examples,1,open,2020-05-31T04:40:22Z,2021-03-04T22:43:46Z,When running the node examples with ts-node; all is good; but if I want to compile with tsc I'm getting the following error:tsc faceDetection.ts```../../node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 556 more ...; readonly WAIT_FAILED: number; }'; but here has type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 557 more ...; readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number; }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../../../usr/local/lib/node_modules/typescript/lib/lib.dom.d.ts:16316:13    16316 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```So WebGL2RenderingContext is being declared in two separate parts...Thanks,['for those who come here in the future:https://github.com/tensorflow/tfjs/issues/2007====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGL,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Declaration Conflict""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",C,B.2
https://github.com/Volcomix/virtual-background/issues/3,Build failure,1,closed,2021-04-21T23:23:09Z,2021-04-22T07:41:04Z,"Hi;When trying to build the wasm binaries I am seeing the following errors and the build fails when I run `yarn build:tflite:all````INFO: Build option --crosstool_top has changed; discarding analysis cache.Analyzing: target //:tflite (1 packages loaded; 0 targets configured)Analyzing: target //:tflite (22 packages loaded; 120 targets configured)Analyzing: target //:tflite (60 packages loaded; 1195 targets configured)Analyzing: target //:tflite (72 packages loaded; 2053 targets configured)ERROR: Traceback (most recent call last):	File ""/root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/cpuinfo/BUILD.bazel""; line 131; column 27; in <toplevel>		"":emscripten_wasm"": COMMON_SRCS + EMSCRIPTEN_SRCS;Error: Duplicated key "":emscripten_wasm"" when creating dictionaryERROR: /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/org_tensorflow/tensorflow/lite/kernels/BUILD:335:11: no such target '@cpuinfo//:cpuinfo_with_unstripped_include_path': target 'cpuinfo_with_unstripped_include_path' not declared in package '' defined by /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/cpuinfo/BUILD.bazel and referenced by '@org_tensorflow//tensorflow/lite/kernels:cpu_backend_context'INFO: Repository emscripten_bin_linux instantiated at:  /tflite_src/WORKSPACE:56:22: in <toplevel>  /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/emsdk/emscripten_deps.bzl:28:21: in emscripten_depsRepository rule http_archive defined at:  /root/.cache/bazel/_bazel_root/7c8212d09d20008d9ab2f5e31ef362d8/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>ERROR: Analysis of target '//:tflite' failed; build aborted: Analysis failedINFO: Elapsed time: 5.551sINFO: 0 processes.FAILED: Build did NOT complete successfully (77 packages loaded; 2200 targets configured)FAILED: Build did NOT complete successfully (77 packages loaded; 2200 targets configured)```Can you please help?Thanks;Dipankar.","[""Hi @dipankadas; have you managed to fix the issue?I can't reproduce it locally but this error seems to be related to https://github.com/tensorflow/tensorflow/commit/d993cf4d99ce150b14916fdcc1f182c25f856ec7; which makes me remove a sed from the Dockerfile (https://github.com/Volcomix/virtual-background/commit/7b4c16eb89a768bf734bbf8d20b3180e5cd31a41#diff-377eaeb5506b9fb7e3100b820802c5a2b6f144b284f07825335761520f673ac0).Recreating the image and the container or just reverting the `/tensorflow_src/third_party/cpuinfo/BUILD.bazel` file in the container should fix the issue (sorry for the Tensorflow update process which is not automated very well yet).=====""]",Build & Install Failure,Build & Initialization Failure,Untimely Update,TF(CPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Build Failed""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.3
https://github.com/Volcomix/virtual-background/issues/1,TFLite fails to build in the Docker container,5,closed,2021-03-11T21:25:43Z,2021-03-12T21:00:31Z,Hi; I am trying to run your demo on my Mac and when I run `yarn build:tflite:all` and bazel tries to build tflite; it fails with this error :```$ docker exec -w /tflite_src tflite bazel build --config=wasm -c opt :tfliteExtracting Bazel installation...Starting local Bazel server and connecting to it...Loading:Loading: 0 packages loadedERROR: error loading package '': cannot load '@org_tensorflow//tensorflow:workspace.bzl': no such fileINFO: Elapsed time: 4.869sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)FAILED: Build did NOT complete successfully (0 packages loaded)error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```I would guess I have to change something in the `/tflite_src/WORKSPACE` file but I don't know how bazel works yet. I will try to fix the issue and will comment again if I find how to get rid of the error.,"[""Hi @RaphaelRoyerRivard; thanks for reporting that. I juste took a look at tensorflow repo and it looks like `tensorflow/workspace.bzl` is gone from the master branch.I'll investigate as soon as possible (most probably this weekend) to figure out how it has been replaced.As a workaround; maybe could you try checking out the tag v2.4.1 of tensorflow within the container; manually apply the seds from the dockerfile and try again.I don't have a computer so I can't be more specific right now but I will come back to you as soon as possible.=====""; 'Thanks a lot for your answer; I will try that.====='; 'I deleted the container and replaced the following line in Dockerfile```RUN git -C /tensorflow_src pull```with```RUN git -C /tensorflow_src fetchRUN git -C /tensorflow_src checkout 85c8b2a817f95a3e979ecd1ed95bff1dc1335cff```which is the 2.4.1 tag of Tensorflow and it built successfully :)====='; 'Nice thank you for the feedback!====='; '3d1593d should fix the issue.Let me know if you still have troubles to build TFLite (if you decide to switch back to tensorflow master :wink:).=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,TF(CPU),Backend,change framework version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Docker Build Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.1
https://github.com/justadudewhohacks/face-api.js/issues/828,Use with CPU without AVX support,3,open,2021-11-10T12:26:34Z,2021-11-11T12:20:29Z,Hello guys!How to use face-api.js with ubuntu 20.04 and CPU without AVX support for node.js? I found * .whl packages; but how to use them with face-api.js?,['you found `whl` of what?  and this is not question for `face-api.js`; its for `tfjs`  if you get `@tensorflow/tfjs-node` working; then `face-api.js` will *just work*  however; there is no recent prepackaged version `@tensorflow/tfjs-node` available anymore that doesnt require AVX  you can try installing some ancient version (dont remember what is the latest version without AVX) or build your own (painful)====='; '@tensorflow/tfjs-node lib compiled for CPU with support AVX instructions. I have server with CPU without AVX. recommend to update tensor compiled without avx instructions. there are ready-made whl packages on the web with the assembled tensor; but for python for example; https://github.com/furas/tensorflow-no-avx etc====='; 'now that you have `tensorflow` without AVX; you need to build `@tensorflow/tfjs-node` package (it links to `tensorflow`).but that is anything but a clean process. search on tfjs git issues.====='],Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] General Usage Issues""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.1
https://github.com/infinitered/nsfwjs/issues/332,Error: Cannot find module '@tensorflow/tfjs' and other error,2,closed,2020-05-05T14:00:58Z,2020-05-08T20:49:50Z,```Node: v13.12.0@tensorflow/tfjs-node: v1.7.4@tensorflow/tfjs: v1.7.4nsfwjs: 2.2.0```I tried to run NSFWJS on NodeJS;I copied the test code;```jsconst axios = require('axios') //you can use any http clientconst tf = require('@tensorflow/tfjs-node')const nsfw = require('nsfwjs')async function fn() {  const pic = await axios.get(`https://www.mankeynews.com/wp-content/uploads/2019/11/Anime.jpg`; {    responseType: 'arraybuffer';  })  const model = await nsfw.load() // To load a local model; nsfw.load('file://./path/to/model/')  // Image must be in tf.tensor3d format  // you can convert image to tf.tensor3d with tf.node.decodeImage(Uint8Array;channels)  const image = await tf.node.decodeImage(pic.data;3)  const predictions = await model.classify(image)  image.dispose() // Tensor memory must be managed explicitly (it is not sufficient to let a tf.Tensor go out of scope for its memory to be released).  console.log(predictions)}fn()```I installed the required dependencies (nsfwjs; axios and @tensorflow/tfjs-node)I ran the code; but it throws the following error```Cannot find module '@tensorflow/tfjs'Require stack:- %directory%\node_modules\@tensorflow\tfjs-node\dist\index.js- %directory%\nsfw.js```I installed `@tensorflow/tfjs`; but it returned another error```Unable to find the specified module.\\?\%path%\node_modules\@tensorflow\tfjs-node\lib\napi-v5\tfjs_binding.node```,"['Take a look at this ticket:  https://github.com/tensorflow/tfjs/issues/2046====='; ""Ok; I'll take a look at it=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Module Not Found""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/178,Registration of backend webgl failed,4,closed,2019-01-02T08:46:17Z,2020-09-24T13:58:06Z,"Nice to meet you Author;I use it in my html;it`s my code ：`<html><head><title>我的第一个 HTML 页面</title></head><body><img id=""myImg"" src=""~/Content/scripts/jquery/445e2c1c0e833b46ff9e12e89e9ddebb.jpg"" /><script src=""~/Content/scripts/jquery/face-api.js""></script><script>   async function init () {        const detections1 = await faceapi.detectFaceLandmarksTiny(faceImage);        console.log(detections);    })init()</body></html>`Nothing can be change ; in my Google Chrome `s f12 show me ![1546418554 1](https://user-images.githubusercontent.com/26179617/50584953-85e4b480-0ead-11e9-98aa-525caab7dfa4.png)i didn`t know what`s happen.but i use npm i;npm start ;it`s great.please help me;thank you very much.                                                                                                                                    Yours              zt",['Hmm; did you check; whether your device is able to run the WebGL backend? The error message comes from tfjs when initializing your environment once you first load the script.====='; 'I have this issue on a old device with intel graphics (Haswell) but which still can run other webgl examples (three.js).====='; 'I am also having this issue while using it on Moto G3 mobile====='; 'i have this on linux with nvidia 1070 maxq (mobile) gpu (currently used for webgl)error:* firefox 80 (linux)* brave Version 1.14.81 Chromium: 85.0.4183.102 (Official Build) (64-bit)working correctly:* chromium Version 85.0.4183.121 (Official Build) snap (64-bit)====='],Initialization Faliure,Build & Initialization Failure,Device Incompatibility,Device,Device,change backend,changing backend,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",C,D.1
https://github.com/justadudewhohacks/face-api.js/issues/286,React native support,1,closed,2019-05-07T09:23:58Z,2019-05-08T08:14:44Z,Please let me know if there is support for react native and if not what changes can be done to support react native,"[""According to @nkreeger in https://github.com/tensorflow/tfjs/issues/1346:> We don't fully support React Native - it is something we have on our roadmap but we haven't had a chance to figure out how to do this correctly.face-api.js is built on top of tfjs-core; so the environment this library can be used in is mainly determined by where tfjs-core runs as well.=====""]",Initialization Faliure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change platform,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Documentation Inaccuracy"",
    ""specific_type"": ""[E.1.1] Missing Support Information""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.3] Cross-platform App Framework Incompatibility""
  }
}
```",C,D.3
https://github.com/BeTomorrow/ReImproveJS/issues/6,Unknown Loss in example Code,1,closed,2018-07-19T20:56:29Z,2018-08-23T08:19:48Z,The Loss function in your example code does not exist anymore:You should change this line:```// Finally compile the model; we also exactly use tfjs's optimizers and loss functions// (So feel free to choose one among tfjs's)model.compile({loss: 'crossEntropy'; optimizer: 'sgd'})```to something like`model.compile({loss: 'meanSquaredError'; optimizer: 'sgd'})`,['Indeed; done and thank you !====='],Reference Error,Crash,Unimplemented Operator,Operator,API,parameter modifier,Modify API Parameter usage,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.1
https://github.com/infinitered/nsfwjs/issues/558,Cannot run in browser extensions,3,open,2021-11-18T04:06:23Z,2021-11-22T16:51:11Z,Manifest V3 for chromium based browsers require extensions have no remote-executed code (unsafe-eval must be disabled); nsfwjs uses new Function() 9 separate times; making the library unusable; since loading a function from a String is considered unsafe-eval.Is there a workaround; if anyone around her has used it with chromium-based apps? Would it be worth it for me to just fork & edit it myself?Isaac![156c8b4394cbfbf13802182d0c9a84a9](https://user-images.githubusercontent.com/65869106/142350142-cf5f4206-c818-4640-9514-f9d33b46eada.png),"[""For an extension have you tried this fix?  https://stackoverflow.com/questions/26242682/unsafe-eval-on-chrome-extensionFor the changes you're proposing; what do those look like?=====""; ""Those changes don't work. Unfortunately google is deprecating manifest v3; so unsafe-eval will not be a possible choice for chrome webstore developers come ~January 2022. After doing a bit of looking around; I can't find the offending code in the source on this repo; which leads me to believe it's being created by the minifier?https://unpkg.com/nsfwjs@2.3.0/dist/nsfwjs.min.jsNot sure what changes I'm proposing; but these are the recurring elements that make it unable to run in browser:```eval()Function()     // typically new Function()setTimeout()   // with non-callable argumentsetInterval()  // with non-callable argumentsetImmediate()execScript()```I looked around and there are a plethora of Chrome apps using your library (It's a sick library; by the way); and they also will not be able to create new ones using it after Jan '22![image](https://user-images.githubusercontent.com/65869106/142751336-7920966d-aafe-4929-9dcc-c7d2bc9ef1ab.png)=====""; ""Thanks so much!!!  I appreciate the kudos.   @isaackogan - I could use your help right quick!My guess is the issue is because my TypeScript is set to be supported as far back as ES5.https://github.com/infinitered/nsfwjs/blob/master/tsconfig.json#L10Would you mind pulling the code down; and trying different targets (https://www.typescriptlang.org/tsconfig#target) to see if one of these will remove the offending code?IF - that's the issue; I can release a second version on NPM that isn't as backwards compatible; but would work well on browser extensions.---Side note:  How do you find a list of chrome apps that are using NSFWJS?   I'd love to see that list!=====""]",Initialization Faliure,Build & Initialization Failure,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Browser Extension Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",C,D.2
https://github.com/justadudewhohacks/face-api.js/issues/311,Why not compatible with IE browsers? Is there a solution?,5,open,2019-06-04T05:44:17Z,2020-03-25T20:19:49Z,Why not compatible with IE browsers? Is there a solution?,"['I have same problem.====='; 'I would guess that support for IE is lacking because the last version of IE (11) is due for end-of-life in just a few months in January 2020.====='; ""It's around 10% and falling last I checked. Given the niche nature of projects developed with face recognition; you should prompt users to upgrade to a newer browser. There are options. Browsers older than IE 11 are basically nonexistent today thanks to the Windows 10 free edition baselining systems to at least IE 11 and/or Edge. =====""; 'Because IE does not support WebGL 2.0. Same with Edge.====='; ""To make it work with Edge you need to set this config option:faceapi.tf.ENV.set('WEBGL_PACK'; false);Works great for me on Edge now!If you only want to set it for IE/Edge you can do this:/** * detect IE * returns version of IE or false; if browser is not Internet Explorer */function detectIE() {  var ua = window.navigator.userAgent;  // Test values; Uncomment to check result …  // IE 10  // ua = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)';  // IE 11  // ua = 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko';  // Edge 12 (Spartan)  // ua = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0';  // Edge 13  // ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586';  var msie = ua.indexOf('MSIE ');  if (msie > 0) {    // IE 10 or older => return version number    return parseInt(ua.substring(msie + 5; ua.indexOf('.'; msie)); 10);  }  var trident = ua.indexOf('Trident/');  if (trident > 0) {    // IE 11 => return version number    var rv = ua.indexOf('rv:');    return parseInt(ua.substring(rv + 3; ua.indexOf('.'; rv)); 10);  }  var edge = ua.indexOf('Edge/');  if (edge > 0) {    // Edge (IE 12+) => return version number    return parseInt(ua.substring(edge + 5; ua.indexOf('.'; edge)); 10);  }  // other browser  return false;}=====""]",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Compatibility Issue with IE Browsers""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.2
https://github.com/justadudewhohacks/face-api.js/issues/328,Running FaceAPI.js within Web Worker (slow),1,open,2019-06-18T21:51:05Z,2019-06-21T15:26:35Z,We found that the video is choppy when running the faceAPI on certain devices.  Specifically; on IOS devices.  Normally; this can be fixed by running the code in a Web Worker.  The problem is that in order to run within a Web Worker we need to pass in ImageData / not DOM related objects like Canvas/Video/Image.  It also appears to run much; much slower when we run in a Web Worker.  Is there a way we can make this work?  What do you suggest?  What type of object can we create on the Web Worker side from an ImageData object that will run at around the same speed?,"[""That's because if you run this on a web worker; you're splitting the CPU to another thread but doing nothing to split loads for the GPU. Transfer of assets to a web worker are not free/fast until browsers support a zero copy strategy like offscreen canvas. Chrome does. Firefox does behind a flag. Safari does not. Zero copy is akin to shifting which CPU thread owns the pointer to that memory object (hence no copying). So let's say you do use Chrome and you do use offscreen canvas to transfer; you'd still need to trick the underlying TensorFlow library doing the ML and the faceapi.js library (both) into believing they are in a normal web browser environment (not node.js; and not webworker)  so that they will process the data using Canvas. At that point; you'll have CPU work running off the main ui thread; and you'll have the GPU processing the ML quickly; but again the limitation there is that is only possible with Chrome and Firefox; and I don't know if mobile iOS versions have those same APIs supported. I filed tickets with Tensorflow and worked on faceapi.js to do exactly this; but the use case I had was a kiosk app on the desktop. Mobile is a long shot.=====""]",Slow Execution,Poor Performance,Browser Incompatibility,Browser,Platform,change data copy stragy,change data copy stragy,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,D.2
https://github.com/justadudewhohacks/face-api.js/issues/342,Catch internal error in promise.,2,open,2019-07-04T09:41:41Z,2019-07-08T06:28:13Z,Hi; I get this error in when calling the promise to detect face + landmarks. Is there any possibility to catch it? Im trying to get the error through then/catch but not reaching this code.`Fragment shader compilation failed.`and `Uncaught (in promise) Error: Failed to compile fragment shader.`,['I think being able to catch this error would require some solution similar to what is described here: https://github.com/tensorflow/tfjs/issues/756. Did you try to wrap the failing code into a simple try/catch? Does it not work?====='; 'Yes; I tried; but there is not error Throwed just printed and continue; so it keeps giving errors... :( ====='],Browser & Device Error,Crash,Improper Exception Handling,WebGL,Backend,add exception handling,add exception handling,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",A.4,A.7
https://github.com/justadudewhohacks/face-api.js/issues/441,Examples work fine on chrome not on IE,3,open,2019-10-14T17:32:42Z,2020-03-25T20:19:24Z,Hi I am learning to use this very nice tool.It is possible to run the examples and my own local integrations on chrome.All other browsers dont work : Edge; IE11; firefox.1;As chrome is not my preferred browser i would like to get it running in something elseWhat am i missing or doing wrong. Is there a special need outside chrome.I do not get any error; it just does not work at all on IE11The examples keep loading forever without any progres. (green running line)2. it would be even great if it is possible to run just in node.js without browser; are there any examples of this ? (running in background with source an ip cam for face recognition)many thanks for every clue you can give on both :),"[""face-api.js definitely works in chrome and firefox; so use one of these. I haven't opened internet explorer in a long time to be honest; so there might be issues. If your preferred browser is IE you should probably overthink your choice in my opinion.Edit: I will not worry about IE support until edge will run on the chrome engine.=====""; ""Thank you !Firefox works fine; i made a script error.I agree IE is not the way to go; javascript support is bad in IE.Are there any examples for server side face recognition ?I tried Nodejs examples; but these seem also to run in browser ( seen the GPU usage on client side; it looks like these run client side.)Maybe i am doing again something wrong.The goal is to incorporate face api.js in a home domotica scenario using an IP camera as presence detection.The clientside webpage is just a webinterface to the backend (tablet)Many thanks for this great interface ! I love it so far….MikeVan: Vincent Mühler [mailto:notifications@github.com]Verzonden: zaterdag 26 oktober 2019 09:44Aan: justadudewhohacks/face-api.js <face-api.js@noreply.github.com>CC: sledgemhammer <m_sledge_m@hotmail.com>; Author <author@noreply.github.com>Onderwerp: Re: [justadudewhohacks/face-api.js] Examples work fine on chrome not on IE (#441)face-api.js definitely works in chrome and firefox; so use one of these.I haven't opened internet explorer in a long time to be honest; so there might be issues. If your preferred browser is IE you should probably overthink your choice in my opinion.—You are receiving this because you authored the thread.Reply to this email directly; view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/441?email_source=notifications&email_token=AIVOHL2ZIYAW2O7OLBYPJLLQQPYN3A5CNFSM4JASAS62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECKCJLA#issuecomment-546579628>; or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIVOHL22UBXLKVQMZWX3RE3QQPYN3ANCNFSM4JASAS6Q>.=====""; ""To make it work with Edge you need to set this config option:faceapi.tf.ENV.set('WEBGL_PACK'; false);Works great for me on Edge now!If you only want to set it on IE/Edge environments you can do something like this:/** * detect IE * returns version of IE or false; if browser is not Internet Explorer */function detectIE() {  var ua = window.navigator.userAgent;  // Test values; Uncomment to check result …  // IE 10  // ua = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)';  // IE 11  // ua = 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko';  // Edge 12 (Spartan)  // ua = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0';  // Edge 13  // ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586';  var msie = ua.indexOf('MSIE ');  if (msie > 0) {    // IE 10 or older => return version number    return parseInt(ua.substring(msie + 5; ua.indexOf('.'; msie)); 10);  }  var trident = ua.indexOf('Trident/');  if (trident > 0) {    // IE 11 => return version number    var rv = ua.indexOf('rv:');    return parseInt(ua.substring(rv + 3; ua.indexOf('.'; rv)); 10);  }  var edge = ua.indexOf('Edge/');  if (edge > 0) {    // Edge (IE 12+) => return version number    return parseInt(ua.substring(edge + 5; ua.indexOf('.'; edge)); 10);  }  // other browser  return false;}=====""]",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change env flag,Modifying the value of environment variable,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Browser Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.2
https://github.com/justadudewhohacks/face-api.js/issues/482,CPU vs GPU vs hardware side effects,4,open,2019-11-22T09:49:21Z,2019-12-08T02:05:37Z,Hello;I'm running FaceAPI on a raw webcam connected to a ChromeBit (so Chrome on ChromeOS). On the other hand I also capture raw audio using Script Processor to perform Speech Recognition (server side)It works very well to capture face; landmarks; etc ... so I let it run 1FPS to detect if there is someone in front of the camera.Each second the FaceAPI run the detection against the current frame. That create a CPU peak with side effects on the audio recording (look like some tremble / echo)My understanding is that even if JS is single thread; there is some side effect working on audio and video that could mess things up.Do you have some knowledge about tensorflow JS doing that kind of side effect ?I tried to move part of the audio processing in a Web Worker but there is still some issues,"[""Your question is less about TensorFlow and more about how javascript and browsers execute work. The browser will render choppy to the screen if the UI thread is blocked. This is the default thread. It is used for anything where the DOM interface is used on the object referenced. In this case; because you're processing a video stream; let's assume the processing of that video and handing off of that video to face-api is all being processed on the main UI thread. To optimize this; you could try only having the UI thread capture the video and convert it into data face-api could process. You could then run both on a worker thread which would not block the CPU thread. You end up possibly hitting a second issue though; which is that CPU bound processing is not the only issue. On machines with weak; old or integrated graphics cards; you can also become GPU bound. The UI thread also requires access to the GPU; and TensorFlow is not very polite in the way it splits work out to the GPU. Both cases result in choppy rendering which has nothing to do with the audio of a system. If your background process is dominating your CPU threads; that can also starve your browser of resources it needs to run the UI thread quickly. What you can do which will work; however; is to simplify the face detection model and framework used; or to offload this processing to the server where the language used to process it is a faster more low level threaded language. In the former camp; you lose features like landmarks and facial deltas and descriptors. In the latter; you can only scale by how much of your own processing power you want to pay for; and the bandwidth is the same as streaming video costs. Neither are ideal cases; but both work pretty well. https://github.com/auduno/headtrackr <-- this is a very very old project which used bleeding edge apis at the time and works fast; but has a limited feature set. It's incredible how fast you can get things to run when you lose the heft of tensorflow and big ML models.=====""; ""Thanks for the explanation and the repo; very interresting I'll dig into it.I found some basic bug/optimisation on my code to narrow the CPU usage. I'll clean it to better use Worker.I found something weird; in Face-API.js the samples get a frame from the camera then setTimeout(). So I did the same thing with a threashold for the timeout (I saw other library doing also that)BUT; when I started to play with start/stop vs clearTimeout it seems this was scramble https://stackoverflow.com/questions/54753149/frames-captured-from-webcam-processed-and-drawn-on-canvas-appear-out-of-orderMay be it's my code; can we assume setTimeout / clearTimeout on webcam frame is sync (because JS engine is single threaded ?) I'll try to simplified my code to check that=====""; 'Using setTimeout to defer to the next tick/event loop should really be considered a bad practice by now. Promises are way faster and nicer on the system than timeouts. They don’t run the same way; but produce the same result of yielding so other work can be done. It allows the javascript engine to decide how it wants to schedule the work and there isnt that nasty minumum 10ms limit between cycles like on setTimeout so all your broken apart work will actually finish faster. It really shows up when you’re running large batches of work  ====='; 'Btw; if you do look at that headtrackr repo; fair warning that it requires you to do minor tweaks to the getUserMedia calls to make it work with the current standard. Remember; it was using draft specs and bleeding edge apis back when it came out and it was an abandoned project so it hasnt been fixed or advanced. Just an example of how fast and light things can be when optimized fully for constrained hardware. I wanted to get into easing and predictive tracking and pupils for glancing. I really dont care about facial features (ears; jaw; mouth; eyebrows; emotions) as much as i just want z depth; facial fingerprint; and view angle. If i know where your eyes are and how far away they are and where they’re looking and can animate field of view and depth of field and stuff in 3d; the head coupled perspective thing would get super cool for building some simple 3d games like world runners. =====']",side effect,Crash,WebGL Limits,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A,D.4
https://github.com/justadudewhohacks/face-api.js/issues/516,How to improve speed of detectSingleFace(video).withFaceLandmarks().withFaceDescriptor() ?,5,closed,2019-12-30T05:01:02Z,2020-04-30T15:52:40Z,When loading in browser it takes about 12s to start recognising my face. With some console logging I found the issue to be faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor(); which takes up most of the time. How can I reduce it to about 3s instead?```video.addEventListener('play';()=>{    const faceMatcher = new faceapi.FaceMatcher(vectors;0.5)    var idVar = setInterval(async () => {        const detections2 = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor()        const results = faceMatcher.findBestMatch(detections2.descriptor) // for detectSingleFace        if (labels.includes(results['label'])) {            document.getElementById('msg').innerHTML=(`Hello; ${results['label']}!`)            clearInterval(idVar)            // setTimeout(window.close;5000)    }    };500)})```,"[""Purchasing good GPU may help (especially NVIDIA good gpu; like GTX 1060). If you using CPU backend; then i don't know; for me 10 fps more than enough on i9-9900K =====""; '@adumbz  can i get full your code====='; 'I am no longer working on this project so I will close this issue; appreciate your replies @dalisoft @fuadkhalis ====='; 'Hi; I am having the same time issue. Did you manage to make the process faster? Thank you. ====='; ""Hi @idkidk-idk; sorry for the late reply. I did not manage to do it; suspected it was due to GPU as I was mostly working on my laptop with integrated graphics. I did not notice this issued being raised up much; I would think that this program was built more for a single setup with continuous use; instead of short separate executions. I eventually switched to a paid API with FaceX for conversion and comparison using vectors returned from their API. It was a lot more lightweight as the heavy work was done on their side; and wasn't too expensive for short term projects (7usd per 5000 calls per month). Could get results in about 1-3s depending on your implementation. Hope this helps!=====""]",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,D.1
https://github.com/justadudewhohacks/face-api.js/issues/682,Not working in Firefox,1,open,2020-08-16T11:28:03Z,2020-08-20T13:35:49Z,Hello; unfortunally this library doesn't work in Firefox.Neither the demos nor my own implementation work. The console always outputs a warning and doesn't detect anything.``WebGL warning: readPixels: Buffer for `target` is null``In Chrome it works fine.,"[""This is confirmed. It's also happening in Chrome/Edge with a slightly different error in some cases (we haven't been able to determine which cases yet; but are testing further). I'm suspecting an update with browsers or maybe Windows is causing the problem; as it's happening in multiple browsers; but not every user is experiencing it.@justadudewhohacks are you still maintaining this library?=====""]",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Compatibility Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",D,D.2
https://github.com/justadudewhohacks/face-api.js/issues/798,Version of NodeJS for face-api?,1,closed,2021-05-28T14:09:20Z,2021-05-29T19:07:26Z,Hi I have installed Node version 16 it throws some method deprecated issue of tensorflow but It works well in Node 8.12.0; Is someone know what is the max version of Node I can use for this..?,['this original version of `face-api` has not been updated in over a year; so no wonder there are issues with latest node.try newer port of `face-api` which works with both new versions of tfjs; typescript and nodejs (yes; nodejs 16 is tested):<https://github.com/vladmandic/face-api>====='],Reference Error,Crash,Dependency Error,TF(CPU),Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",A.1,B.2
https://github.com/justadudewhohacks/face-api.js/issues/826,Hangs forever and unresponsive after sometime in desktop browser,3,open,2021-10-11T06:07:25Z,2021-10-21T17:10:59Z,In laptop browser the screen hangs and doesn't go forward with face detection. I identified its getting hanged in the below line. const detections = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceExpressions();Note: Works fine in mobile browsers,"['In mobile browsers too; when upgrade it. I think something changed in browsers in the last month.I try to find out what caused this; and found an error message in the console: ""Pass at least one tensor to tf.stack""With more debugging I found the first occurrence of this error at the ""FaceLandmark68NetBase.prototype.postProcess"" close to the line ""var landmarkTensors = output"" but I haven\'t been able to figure out how can be fix it yet.====='; 'well; this original version of `face-api.js` hasnt been updated since 04/2020 and uses quite old version of tfjs 1.7.6 internally which is based on es5 standard - chances something will break in latest browsers are not small.you can always try a newer port of `face-api`.====='; 'Thanks @vladmandic for the suggestion and awesome project.=====']",Browser Hangs,Poor Performance,Browser Incompatibility,Browser,Platform,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.2] Browser Hangs""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",B.1.2,D.2
https://github.com/Volcomix/virtual-background/issues/22,TFLite WASM build failing after XNNPACK update,2,closed,2021-07-14T10:24:10Z,2021-08-14T11:34:40Z,"[XNNPACK has recently been updated](https://github.com/google/XNNPACK/commit/ee029b28ad3cf2908f869d789de396f6a83a68d9) with the latest versions of `wasm_simd128.h` intrinsics.As those intrinsics are not yet included in [the version of LLVM used by emsdk](https://github.com/llvm/llvm-project/blob/llvmorg-12.0.0/clang/lib/Headers/wasm_simd128.h); TFLite WASM build is failing.tfjs doesn't have this issue yet when building the wasm backend because it relies on an [old version of XNNPACK](https://github.com/tensorflow/tfjs/blob/9f0d88a4b7db807a9074720d865ef8d95efc5a2f/WORKSPACE#L96) but I guess it should face the same issue as soon as XNNPACK version will be updated.Here are some details about [the error we have](https://github.com/Volcomix/virtual-background/runs/3063327177):```[177 / 1;157] Compiling XNNPACK/src/f32-avgpool/9p8x-minmax-wasm-c1.c; 2s processwrapper-sandbox ... (2 actions running)ERROR: /github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/external/XNNPACK/BUILD.bazel:4672:19: C++ compilation of rule '@XNNPACK//:wasm_ukernels' failed (Exit 1): emcc.sh failed: error executing command external/emsdk/emscripten_toolchain/emcc.sh '--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot' -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG ... (remaining 66 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandbox emcc.sh failed: error executing command external/emsdk/emscripten_toolchain/emcc.sh '--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot' -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG ... (remaining 66 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandboxexternal/XNNPACK/src/x32-pad/wasmsimd.c:32:24: error: implicit declaration of function 'wasm_v128_load32_splat' is invalid in C99 [-Werror;-Wimplicit-function-declaration]  const v128_t vfill = wasm_v128_load32_splat(fill_value);                       ^external/XNNPACK/src/x32-pad/wasmsimd.c:32:24: note: did you mean 'wasm_v16x8_load_splat'?external/emscripten_bin_linux/lib/clang/13.0.0/include/wasm_simd128.h:70:1: note: 'wasm_v16x8_load_splat' declared herewasm_v16x8_load_splat(const void *__mem) {^external/XNNPACK/src/x32-pad/wasmsimd.c:32:16: error: initializing 'const v128_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'  const v128_t vfill = wasm_v128_load32_splat(fill_value);               ^       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~2 errors generated.emcc: error: '/github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/sandbox/processwrapper-sandbox/1173/execroot/__main__/external/emscripten_bin_linux/bin/clang -DEMSCRIPTEN -fignore-exceptions -mllvm -combiner-global-alias-analysis=false -mllvm -enable-emscripten-sjlj -mllvm -disable-lsr -Xclang -iwithsysroot/include/SDL -target wasm32-unknown-emscripten -D__EMSCRIPTEN_major__=2 -D__EMSCRIPTEN_minor__=0 -D__EMSCRIPTEN_tiny__=14 -D_LIBCPP_ABI_VERSION=2 -Dunix -D__unix -D__unix__ -flegacy-pass-manager -Werror=implicit-function-declaration --sysroot=/github/home/.cache/bazel/_bazel_root/0a7049196223eb41bb90aa2e8797b78e/external/emscripten_bin_linux/emscripten/cache/sysroot -Xclang -iwithsysroot/include/compat --sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot -fdiagnostics-color -fno-strict-aliasing -funsigned-char -no-canonical-prefixes -DNDEBUG -fomit-frame-pointer -O3 -Wall -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/wasm-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/wasm-opt/bin/external/FP16 -iquote external/FXdiv -iquote bazel-out/wasm-opt/bin/external/FXdiv -iquote external/pthreadpool -iquote bazel-out/wasm-opt/bin/external/pthreadpool -Ibazel-out/wasm-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/wasm-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/wasm-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -isystem external/XNNPACK/include -isystem bazel-out/wasm-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/wasm-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/wasm-opt/bin/external/FP16/include -isystem external/FXdiv/include -isystem bazel-out/wasm-opt/bin/external/FXdiv/include -isystem external/pthreadpool/include -isystem bazel-out/wasm-opt/bin/external/pthreadpool/include -Wno-error=unused-function -msimd128 -Iinclude -Isrc -std=c99 -O2 -iwithsysroot/include/c++/v1 -iwithsysroot/include/compat -iwithsysroot/include -isystem external/emscripten_bin_linux/lib/clang/13.0.0/include -MD -MF bazel-out/wasm-opt/bin/external/XNNPACK/_objs/wasm_ukernels/1/wasmsimd.d -c -Wno-builtin-macro-redefined -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" -Werror external/XNNPACK/src/x32-pad/wasmsimd.c -o bazel-out/wasm-opt/bin/external/XNNPACK/_objs/wasm_ukernels/1/wasmsimd.o' failed (1)Target //:tflite-simd failed to buildUse --verbose_failures to see the command lines of failed build steps.INFO: Elapsed time: 59.927s; Critical Path: 8.59sINFO: 52 processes: 6 internal; 46 processwrapper-sandbox.FAILED: Build did NOT complete successfullyFAILED: Build did NOT complete successfully```Issue created in XNNPACK repository: https://github.com/google/XNNPACK/issues/1630",['It should definitely build under emsdk 2.0.25 as 2.0.25 comes with llvm 13 which has the necessary intrinsics.I used CMake instead of bazel and provided emscripten through other means because that works better in our case; and it built just fine.The fact that you linked llvm 12 above means that a pretty old emsdk is used for some reason. Emscripten follows tip of the tree llvm.====='; 'Indeed I followed the instructions without changing anything instead of specifying a more recent archive of emsdk. The bazel scripts only load the latest SDK prior to the specified archive.Fixed in this commit: https://github.com/Volcomix/virtual-background/commit/6bffb3e53d0614d6e9afc4e1430767654dc689f0Thank you for your help.====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,Wasm,Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TFLite WASM build failing""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",C,B.2
https://github.com/Volcomix/virtual-background/issues/23,Working on Safari Browser,5,closed,2021-07-15T12:42:06Z,2021-11-03T17:45:47Z,"@Volcomix Thank you for nice work . I have a question ""Whether this web app is supported on the Safari Browser?"" . I have tested on the safari browser but it is not working . Thank you ","['Hey @NaeemKhan333; thanks for asking 😄 This app relies on `webgl2` which needs to be enabled in Safari ""Experimental Features"" developer menu starting from v10.1 (https://caniuse.com/webgl2). SIMD is not supported though.====='; '@Volcomix Thank you for reply ; if I change webgl2 to webgl will it support the safari browser or not.====='; ""The GLSL shaders are not compatible with webgl so it won't work. Maybe could you just fallback to the canvas implementation on safari rather than loading the webgl2 pipeline by default: https://github.com/Volcomix/virtual-background/blob/8530c56ce419618a3679ca379e76bfd1518a35f5/src/App.tsx#L35We could detect this case the same way we detect unsupported SIMD; by trying to create a webgl2 context and by disabling the webl2 options if it is not available (plus fallback to canvas).=====""; 'Thank you ====='; ""Did anyone succeed in running the playground (https://volcomix.github.io/virtual-background) or the package in general on iPhone iOS 15.1 / 14.8 / 14.7/ etc?What is the recommended implementation for cases where WEBGL2 isn't available?> The GLSL shaders are not compatible with webgl so it won't work. Maybe could you just fallback to the canvas implementation on safari rather than loading the webgl2 pipeline by default:> > https://github.com/Volcomix/virtual-background/blob/8530c56ce419618a3679ca379e76bfd1518a35f5/src/App.tsx#L35> > We could detect this case the same way we detect unsupported SIMD; by trying to create a webgl2 context and by disabling the webl2 options if it is not available (plus fallback to canvas).=====""]",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change browser version,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.4,D.2
https://github.com/Volcomix/virtual-background/issues/26,Demo not running on Safari 14.1.12,2,closed,2021-10-07T14:41:19Z,2021-10-12T08:54:02Z,Page starts to load UI then blanks.Error in the console is: TypeError: null is not an object (evaluating 'm.VERTEX_SHADER'),"['Hi @slpn1;Indeed this demo relies on WebGL 2.0 which is supported on Safari 14.1 only after enabling it in the ""Experimental Features"" developer menu.It is supported by default starting from Safari 15 (https://caniuse.com/webgl2).====='; 'Thank you; and amazing work on this project!=====']",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change browser version,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.2
https://github.com/Volcomix/virtual-background/issues/30,Does not work on ipad,2,open,2021-10-27T12:42:20Z,2021-11-04T16:35:59Z,The applied on my website. However; it does not work on ipad. I have check your demo; it does not work on ipad too. Could you please help me to figure out the problem,"[""It would help if you manage to get any error from the JavaScript console; but I suspect that the issue is due to a version of Safari which doesn't support webgl2.You can find some discussions about this here: https://github.com/Volcomix/virtual-background/issues/23#issuecomment-881311562.You could handle this case by catching the error and fallback to the canvas pipeline.=====""; ""Referencing a new comment I've into issue #23 (which is closed):https://github.com/Volcomix/virtual-background/issues/23#issuecomment-959753509=====""]",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change browser version,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Not Supported""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.2
https://github.com/justadudewhohacks/face-api.js/issues/559,Not able to load models properly in ionic,4,open,2020-02-23T15:49:06Z,2021-11-30T13:00:30Z,"hi; I am not able to load the models successfully in ionic. However; there was no problem running it in broswer and serving in localhost. The errror is:""Based on the provided shape; [3;3;32;1]; the tensor should have 288 values but has 177""","['Inspect the network tab and make sure; that the shards are all loaded correctly and that they are not corrupted; e.g. the size of the fetched shards is unchanged.====='; 'Thanks man. Solved it by file accessing process recommended in ionic; as it can not access local file. Thanks again.====='; ""> accessingHi @juny58 ;I'm also facing a similar problem in ionic. Can you just brief me on how do you solve this?=====""; ""> Thanks man. Solved it by file accessing process recommended in ionic; as it can not access local file. Thanks again.How did you fix this issue on ionic? I'm facing same problem. Thanks!=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,Mobile,Platform,change file permissions,change file permissions,framework,Model Loading,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/358,how to load the models in ionic 3.,10,open,2019-07-22T11:47:08Z,2020-01-20T07:05:21Z,"Fetch API cannot load file:///android_asset/models/face_recognition_model-weights_manifest.json. URL scheme ""file"" is not supported.","[""Basically answered this here #73:> Hmm; that's odd. Usually browsers don't allow you to access files from your filesystem for security reasons; that's why the fetch request in model.load is failing in chrome (not sure why this works for you in firefox).> > If you look at my examples in this repo; you can see that I state; which folders can be publicly accessed (in server.js). This way you can load your model from an url e.g. net.load('models/my-model'). You can do the same for your model.> > With filepicker I mean; you can use an input element of type 'file'; which is another way to browse and load files from your browser. There should be plenty of tutorials out there; explaining how to implement a file upload.Another way is to monkey patch `readFile` as described here: #153 and then use net.loadFromDisk. I am not familar with Ionic so I don't know what kind of filesystem methods you have available to implement the patch.=====""; ""I have the same problems importing the modules; I am using Ionic 3 and I am trying to use monkeyPatch.faceapi.env.monkeyPatch({    readFile: filePath =>      Promise.all([        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');        await faceapi.nets.faceExpressionNet.loadFromUri('./models')      ]).then()});=====""; '@LeonardoDB not sure why you are using loadFromUri inside your patch. You are supposed to monkey patch readFile using ionics specific file system API.====='; 'Thanks @justadudewhohacks; your help was helpful in troubleshooting====='; '@rajkishoreandia @justadudewhohacks I am integrating face-api.js with an ionic3 project. With [this](https://stackoverflow.com/questions/56544635/using-face-api-js-in-cordova-with-android/); I was able to load the models for recognition. 1. Install [cordova-plugin-file](https://ionicframework.com/docs/v3/native/file/)2. Move all models to /src/assets/models3. In your page.ts; declare:`let filePathRoot = this.file.applicationDirectory + \'www/assets/models/\';`4. Create monkeyPatch```faceapi.env.monkeyPatch({       readFile: filePath =>         new Promise(resolve => {           this.file.resolveLocalFilesystemUrl(filePath).then(res => {             console.log(filePath);             if (res.isFile){               let fileExtension = filePath.split(""?"")[0].split(""."").pop();               let fileName = filePath.split(""?"")[0].split(""/"").pop();                 if (fileExtension === ""json"") {                   this.file.readAsText(filePathRoot; fileName).then((text) => {                     resolve(text);                   });                 } else {                   this.file.readAsArrayBuffer(filePathRoot; fileName).then((arrayBuffer) => {                     resolve(new Uint8Array(arrayBuffer));                   });                 }             }         });       });      Canvas: HTMLCanvasElement;      Image: HTMLImageElement;      ImageData: ImageData;      Video: HTMLVideoElement;      createCanvasElement: () => document.createElement(""canvas"");      createImageElement: () => document.createElement(""img"")     });```5. Load models from Disk```await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot);await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot);```6. Enjoy!```let resultsRef = await faceapi.detectAllFaces(image; new faceapi.TinyFaceDetectorOptions());alert(JSON.stringify(resultsRef));```Hope this helps. Any adjustments or improvements; please pass on to the community!====='; ""**@yagancadorin** Thank you for sharing this; i have a query in this that do you have an example reference to this code ? If not please explain this if you can 'Move all models to /src/assets/models'. Thank you.=====""; 'Please can anyone explain how to integrate FaceApi.js library in an ionic 3 projecct and how to use it?Sharing some code would definitely help====='; '> **@yagancadorin** Thank you for sharing this; i have a query in this that do you have an example reference to this code ? If not please explain this if you can \'Move all models to /src/assets/models\'.> > Thank you.I will try to make it easier. After installing the plugin; follow the steps:1. Download [JS file](https://github.com/justadudewhohacks/face-api.js/tree/master/dist); move to /src/assets/js/ and include into your /src/index.html```[...]<script src=""cordova.js""></script><script src=""assets/js/face-api.min.js""></script>[...]```2. Download [models](https://github.com/justadudewhohacks/face-api.js/tree/master/weights) and move all files to your /src/assets/models/3. Declare faceapi in your /src/pages/home/home.ts ```[...]declare var faceapi : any;@Component({  selector: \'page-home\';  templateUrl: \'home.html\'})export class HomePage { }[...]```4. Create and call async loadmodel function ```[...]  async loadModels(){    // set path to load models    let filePathRoot = this.file.applicationDirectory + \'www/assets/models/\';    // faceapi settings    faceapi.env.monkeyPatch({      readFile: filePath =>        new Promise(resolve => {          let fileExtension = filePath.split(""?"")[0].split(""."").pop();          let fileName = filePath.split(""?"")[0].split(""/"").pop();          this.file.resolveLocalFilesystemUrl(filePathRoot + fileName).then(res => {            if (res.isFile){                if (fileExtension === ""json"") {                  this.file.readAsText(filePathRoot; fileName).then((text) => {                    resolve(text);                  });                } else {                  this.file.readAsArrayBuffer(filePathRoot; fileName).then((arrayBuffer) => {                    resolve(new Uint8Array(arrayBuffer));                  });                }            }        });      });     Canvas: HTMLCanvasElement;     Image: HTMLImageElement;     ImageData: ImageData;     Video: HTMLVideoElement;     createCanvasElement: () => document.createElement(""canvas"");     createImageElement: () => document.createElement(""img"")    });    // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot);    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot);    await faceapi.nets.faceLandmark68Net.loadFromDisk(filePathRoot);    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot);  }[..]```5. Use your global faceapi variable normaly in code to create faceMatcher and detect/recognize faces.```[...]let detections = await faceapi.detectAllFaces(shotCam; new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors().withFaceExpressions();[...]```====='; '@justadudewhohacks  @yagancadorin thanks for the response.I am now implementing the same in ionic 3 .but my models are not getting loaded from file path.i am getting following error.ERROR Error: Uncaught (in promise): [object Object].FileError--->code: 1message: ""NOT_FOUND_ERR""__proto__: ObjectThanks in advance!====='; ""Face detection is not happening @justadudewhohacks  @yagancadorin After loading the models if i pass the image into face Api     // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot);    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot);    await faceapi.nets.faceLandmark68Net.loadFromDisk(filePathRoot);    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot);    await faceapi.nets.ssdMobilenetv1.loadFromDisk(filePathRoot)    console.log('after loading detection models')    // displaying the fetched image content    const myImg = document.getElementById('myImg')    const detections = await    faceapi.detectAllFaces(myImg).withFaceLandmarks().withFaceDescriptors()    console.log('detections';detections)prints nothing in console=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,Mobile,Platform,patch environment,Fix environment adaptability,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Fetch API Request Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/212,Loading the Models - Error,12,open,2019-02-08T19:14:30Z,2019-07-19T05:21:21Z,"I have been using the cordova face-api project together; when I'm loading the MODELS; I come across the following errors:CODE - 1:net.load(await faceapi.fetchNetWeights('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json'))ERROR - 1Fetch API cannot load file:///android_asset/www/js/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme ""file"" is not supported.i tryCODE - 2:await faceapi.nets.ssdMobilenetv1.loadFromDisk('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json');ERROR - 2;Error: readFile - filesystem not available for browser environmenti tryCODE - 3:const res = await axios.get('file:///android_asset/www/js/modelos/ssd_mobilenetv1_model-weights_manifest.json'; { responseType: 'arraybuffer' });		const weights = new Float32Array(res.data);net.load(weights);ERROR - 3;Error: Based on the provided shape; [1;1;64;128]; and dtype float32; the tensor should have 8192 values but has 2381    at assert (face-api.js:23)","['Hey not really familar with cordova; but I am assuming there is no difference in the webview and a browser in the following I am going to state:> ERROR - 1Fetch API cannot load file:///android_asset/www/js/models/ssd_mobilenetv1_model-weights_manifest.json.URL scheme ""file"" is not supported.A client application in a browser can not access your file system; thus using the fetch API with local files doesn\'t work. The same goes for XHR/axios (CODE 3).> ERROR - 2;Error: readFile - filesystem not available for browser environmentUsing loadFromDisk only works in a nodejs environment and not in the browser.To conclude:If you want to load the model from disk; checkout how to read files from the webview with cordova and monkeyPatch readFile as I pointed out [here](https://github.com/justadudewhohacks/face-api.js/issues/153#issuecomment-443508739).====='; 'Good afternoon;I\'m trying to use the ""faceapi.env.monkeyPatch"" but gave me the error below could tell me what I\'m doing wrong. so I understand you\'re not loading the local filescodefaceapi.env.monkeyPatch({\t\t\tcreateCanvasElement: () => document.getElementById(\'myCanvas\');\t\t\tcreateImageElement: () => document.getElementById(\'imageBase2\');\t\t\treadFile: () => fs.readFile(\'./home/guilherme/Documentos/modelos/\')\t\t})const input = document.getElementById(\'imageBase2\');\t\t\t\t\t\tlet fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors();Error:Error: SsdMobilenetv1 - load model before inference at SsdMobilenetv1.forwardInput (face-api.js:4308)====='; 'You probably forgot to load the model `await faceapi.nets.ssdMobilenetv1.loadFromDisk(filePath)`. Also you probably want to monkey patch it like this readFile: (filePath) => fs.readFile(filePath)====='; '@guilhermefurtado23 did you solve it? If so; can you share the solution?====='; 'Anyone achieved to get it working on cordova or ionic?I have tried httpd plugin on ionic to start a server inside the device an serve files but getting also getting ""not allowed by Access-Control-Allow-Origin"".This api is great and It will be cool to load the models from local file system.====='; '> Anyone achieved to get it working on cordova or ionic?> I have tried httpd plugin on ionic to start a server inside the device an serve files but getting also getting ""not allowed by Access-Control-Allow-Origin"".> This api is great and It will be cool to load the models from local file system.I DID THESAME AND I AM GETTING THIS ERROR ====='; ""IN MY OWN CASE I AM USING THE BROWSER AND AFTER THE INITIAL ERROR I HAD TO MOVE THE MODELS TO A SERVER SO I COULD LOAD THEM FROM THERE ONLY TO GET ANOTHER ERRORAccess to fetch at 'http:BTLINK/models/tiny_face_detector_model-weights_manifest.json' from origin 'null' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs; set the request's mode to 'no-cors' to fetch the resource with CORS disabled.=====""; 'Hi paaber;if you can use an externar server you could try to create a .htacces file with this content:Header set Access-Control-Allow-Origin ""*""Header set Access-Control-Allow-Methods ""GET;PUT;POST;DELETE""Header set Access-Control-Allow-Headers ""Content-Type; Authorization""Next; upload the file to the same folder you are trying to download the models .I made it with the more basic ionos.com hosting and it works.I hope this helps you.====='; 'thank you for the reply; i just did as you\'ve instructed but i still getthe same error message i will keep on trying hopefully i get it working soonOn Tue; 11 Jun 2019 at 11:47; dymdev <notifications@github.com> wrote:> Hi paaber;> if you can use an externar server you could try to create a .htacces file> with this content:>> Header set Access-Control-Allow-Origin ""*""> Header set Access-Control-Allow-Methods ""GET;PUT;POST;DELETE""> Header set Access-Control-Allow-Headers ""Content-Type; Authorization"">> Next; upload the file to the same folder you are trying to download the> models .> I made it with the more basic ionos.com hosting and it works.>> I hope this helps you.>> —> You are receiving this because you commented.> Reply to this email directly; view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/212?email_source=notifications&email_token=AMEGI5VFQK53PUVPVM673L3PZ57DFA5CNFSM4GWEVDCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXMW3UI#issuecomment-500788689>;> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AMEGI5VVDJ42GPMHWUDPMF3PZ57DFANCNFSM4GWEVDCA>> .>====='; 'Hi @dymdev; @guilhermefurtado23; @justadudewhohacks After many hours; get to run in Cordova with Android; it is worth remembering that I have no problems in the iOS; Electron and Browser.I posted the solution in https://stackoverflow.com/questions/56544635/using-face-api-js-in-cordova-with-android/56544636#56544636====='; 'hi dymdev so I finally got it to work after following your fix all I had to do was to change the http in the URL to https and my models are loading thanks ====='; ""  async detectFace(){       console.log('in 1');      const imageUpload = document.getElementById('imageUpload') as HTMLImageElement;      faceapi.env.monkeyPatch({        readFile: () => fs.readFile(MODEL_URL)        })      Promise.all([        await  faceapi.loadFaceRecognitionModel(MODEL_URL);        await faceapi.loadFaceLandmarkModel(MODEL_URL);        await faceapi.loadSsdMobilenetv1Model(MODEL_URL)      ]).then(start)      async function start() {}can somebody help me how to write this.i am using it ionic 3.@justadudewhohacks =====""]",Fetch Failure,Crash,Data/Model Inaccessibility,Mobile,Platform,patch environment,Fix environment adaptability,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/404,Ionic Capacitor - loadFromUri,5,closed,2019-09-06T13:28:37Z,2020-11-16T20:37:20Z,After i changed from a ionic cordova to capacitor build i experience issues loading the models.On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message```Error: Uncaught (in promise): Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607    Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607        at d (http://localhost/vendor.js:100321:4453)        at dn (http://localhost/vendor.js:100321:71403)        at fn (http://localhost/vendor.js:100321:70965)        at o (http://localhost/vendor.js:100321:444184)        at _c (http://localhost/vendor.js:100321:444219)        at http://localhost/vendor.js:100321:463380        at Array.forEach (<anonymous>)        at http://localhost/vendor.js:100321:463354        at Array.forEach (<anonymous>)        at http://localhost/vendor.js:100321:463142```In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.I load ssd with`await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models');`,"[""> After i changed from a ionic cordova to capacitor build i experience issues loading the models.> On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > ```> Error: Uncaught (in promise): Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607>     Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607>         at d (http://localhost/vendor.js:100321:4453)>         at dn (http://localhost/vendor.js:100321:71403)>         at fn (http://localhost/vendor.js:100321:70965)>         at o (http://localhost/vendor.js:100321:444184)>         at _c (http://localhost/vendor.js:100321:444219)>         at http://localhost/vendor.js:100321:463380>         at Array.forEach (<anonymous>)>         at http://localhost/vendor.js:100321:463354>         at Array.forEach (<anonymous>)>         at http://localhost/vendor.js:100321:463142> ```> > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > I load ssd with> `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models');`I use ionic with capacitor and load the models in this way:`await faceapi.loadSsdMobilenetv1Model('/assets/models');``await faceapi.loadFaceLandmarkModel('/assets/models');`The extension of the models changed them to bin![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)Similarly in the .json files![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)Running on android and ios  🎉🎉=====""; ""> > > > After i changed from a ionic cordova to capacitor build i experience issues loading the models.> > On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > ```> > Error: Uncaught (in promise): Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607> >     Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607> >         at d (http://localhost/vendor.js:100321:4453)> >         at dn (http://localhost/vendor.js:100321:71403)> >         at fn (http://localhost/vendor.js:100321:70965)> >         at o (http://localhost/vendor.js:100321:444184)> >         at _c (http://localhost/vendor.js:100321:444219)> >         at http://localhost/vendor.js:100321:463380> >         at Array.forEach (<anonymous>)> >         at http://localhost/vendor.js:100321:463354> >         at Array.forEach (<anonymous>)> >         at http://localhost/vendor.js:100321:463142> > ```> > > > > > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > I load ssd with> > `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models');`> > I use ionic with capacitor and load the models in this way:> > `await faceapi.loadSsdMobilenetv1Model('/assets/models');`> `await faceapi.loadFaceLandmarkModel('/assets/models');`> > The extension of the models changed them to bin> > ![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)> > Similarly in the .json files> > ![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)> > Running on android and ios 🎉🎉Thy this solved my issue :)=====""; ""> > > After i changed from a ionic cordova to capacitor build i experience issues loading the models.> > > On my PWA everything is working and the models are placed in the www/assests/models folder. But when i try to run the native android build i get this error message> > > ```> > > Error: Uncaught (in promise): Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607> > >     Error: Based on the provided shape; [1;1;512;512]; the tensor should have 262144 values but has 216607> > >         at d (http://localhost/vendor.js:100321:4453)> > >         at dn (http://localhost/vendor.js:100321:71403)> > >         at fn (http://localhost/vendor.js:100321:70965)> > >         at o (http://localhost/vendor.js:100321:444184)> > >         at _c (http://localhost/vendor.js:100321:444219)> > >         at http://localhost/vendor.js:100321:463380> > >         at Array.forEach (<anonymous>)> > >         at http://localhost/vendor.js:100321:463354> > >         at Array.forEach (<anonymous>)> > >         at http://localhost/vendor.js:100321:463142> > > ```> > > > > > > > > In my previous build for the native android app with cordova the models where located at assets/www/assets/models now with capacitor the folder structure changed to assets/public/assets/models.> > > I load ssd with> > > `await faceapi.nets.ssdMobilenetv1.loadFromUri('assets/models');`> > > > > > I use ionic with capacitor and load the models in this way:> > `await faceapi.loadSsdMobilenetv1Model('/assets/models');`> > `await faceapi.loadFaceLandmarkModel('/assets/models');`> > The extension of the models changed them to bin> > ![Captura de pantalla 2019-09-07 a las 1 13 13](https://user-images.githubusercontent.com/9834236/64465539-7937f080-d10d-11e9-9f80-0505827923e7.png)> > Similarly in the .json files> > ![Captura de pantalla 2019-09-07 a las 1 15 36](https://user-images.githubusercontent.com/9834236/64465574-b56b5100-d10d-11e9-92d3-4ca960730b81.png)> > Running on android and ios 🎉🎉> > Thy this solved my issue :)Thank you. Solved my issue as well=====""; 'Hello @MMadume; @esmircm and @SimonScholl I am also working in Ionic capacitor and facing some issues with loading a model. Can you please help me with that? Can you share how did you load your model ? Thank you====='; 'Hi I rename models to .bin and change this in json but say error in android2020-11-16 15:35:27.015 24121-24121/systemsweb.net E/Capacitor/Console: File: http://localhost/vendor-es2015.js - Line 41309 - Msg: ERROR Error: Uncaught (in promise): Error: SsdMobilenetv1 - load model before inference    Error: SsdMobilenetv1 - load model before inference        at SsdMobilenetv1.push../node_modules/face-api.js/build/es6/ssdMobilenetv1/SsdMobilenetv1.js.SsdMobilenetv1.forwardInput (http://localhost/tab3-tab3-module-es2015.js:13925:19)        at SsdMobilenetv1.<anonymous> (http://localhost/tab3-tab3-module-es2015.js:13959:35)        at step (http://localhost/vendor-es2015.js:123912:23)        at Object.next (http://localhost/vendor-es2015.js:123893:53)        at fulfilled (http://localhost/vendor-es2015.js:123883:58)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3470:30)        at Object.onInvoke (http://localhost/vendor-es2015.js:73011:33)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3469:36)        at Zone.run (http://localhost/polyfills-es2015.js:3229:47)        at http://localhost/polyfills-es2015.js:3963:40=====']",Fetch Failure,Crash,Data/Model Inaccessibility,Mobile,Platform,change model file extension,Modifying model file path/extension,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/113,Face not being detected when image contains face.,8,closed,2018-10-26T12:50:12Z,2020-09-07T06:09:57Z,"I'm trying to use your module with Electron in a renderer process.I'm trying to retrieve the face descriptor of this image:![woman](https://user-images.githubusercontent.com/21309909/47566105-7e1df800-d8df-11e8-8ae5-3614ee37e350.jpg)Which should work but doesn't. When I run the following code snippet; I get `undefined`:```javascriptvar input = document.getElementById(""myImg""); // Image of womanconsole.log(faceapi); // Just to check if I have the right module.(async () => {    const MODELS = ""./models""; // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    const fullFaceDescriptions = await faceapi // I tried the ""detectAllFaces(input)"" method too; with no success    .detectSingleFace(input)    .withFaceLandmarks()    .withFaceDescriptor()    console.log(fullFaceDescriptions); // Returns ""undefined""})();```When I try to run the following code snippet; I get an empty array:```javascriptvar input = document.getElementById(""myImg""); // Image of womanconsole.log(faceapi); // Just to check if I have the right module.(async () => {    const MODELS = ""./models""; // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    const fullFaceDescriptions = await faceapi // I tried the ""detectAllFaces(input)"" method too; with no success    .detectAllFaces(input)    .withFaceLandmarks()    .withFaceDescriptors()    if (!fullFaceDescriptors.length) { // This condition is satisfied.        console.log(fullFaceDescriptors); // Returns an empty array.        return;    }    console.log(fullFaceDescriptors); // Code doesn't even reach this point.})();```My HTML file:```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta name=""viewport"" content=""width=device-width; initial-scale=1.0"">    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">    <title>FAJS</title>    <style>    </style></head><body>    <img id=""myImg"" src=""woman.jpg"" alt="""">    <script src=""./node_modules/face-api.js/dist/face-api.min.js""></script>    <script src=""index.js""></script></body></html>```Please help as I really need this to work for a project of mine. ThanksP.S: Thanks for making this module. It's great!EDIT: I've been looking for 2 days now and haven't been able to find a solution due to which I had to open this issue. My apologies if this has a very easy fix and wasted your time.","['![foo](https://user-images.githubusercontent.com/31125521/47568097-7f2b4580-d930-11e8-8f68-ffeb6c7a5040.png)If you have an Intel GPU; use the new TinyFaceDetector; which should work on an Intel GPU instead of SSDMobilenetv1.Intel Support is still an open topic for tfjs: https://github.com/tensorflow/tfjs/issues/510====='; ""I don't even have a GPU and my computer is very slow...Does that mean that I won't be able to use this module?=====""; ""Oh well; then what kind of cpu? If you don't have a GPU then you can run everything on the CPU.Are there any errors thrown by tfjs; when trying to register a backend?=====""; ""Oh boy; this is embarrassing.I have an Intel Core i3 with 4 GB of RAM and no; I don't see any errors by tfjs anywhere.=====""; '@Frixoe did you manage to figure out the issue? Would be interested; what the issue was.====='; ""I am having a similar problem on a Samsung slate 7 with a i5 Processor 2467M;  Intel® HD Graphics 3000. I'm getting undefined when trying to compute a descriptor ; when clearly the image contains a face. How do I debug this ? I've tried TinyFaceDetector and SSD ; both return undefined.=====""; '@justadudewhohacks I guess the issue is; electron is integrating node env with the application. Hence face-api.js is considering it as node environment and running node specific code====='; ""@justadudewhohacks Sorry for the super late reply but I don't exactly remember what was causing the issue. I ended up fixing it somehow anyways.=====""]",Incorrect Functionality,Incorrect Functionality,Cross-platform App Framework Incompatibility,Desktop,Platform,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,D.3
https://github.com/Volcomix/virtual-background/issues/13,Better performance in bigger video Width,3,open,2021-05-27T15:30:54Z,2021-11-30T12:42:36Z,Very useful integration. Congrats on your implementation. My question is: Is there a way to increase the performance in bigger video size. I saw that right now it uses 640px video size. When you resize the video to 1980px the performance drops drastically. So is there a way to bypass this issue . Thanks in advance.,['Yeah; even the 1280×720 video of this demo get a performance drop when resizing through WebGL. I tried to mitigate this by downloading the pixels asynchronously after resizing on GPU but uploading video textures to GPU with WebGL is also pretty slow. Maybe there is a way to improve it by mixing 2D Canvas for resizing and WebGL for post-processing only.====='; '[Commercial sdk](https://medium.com/vectorly/building-a-more-efficient-background-segmentation-model-than-google-74ecd17392d5)  that improves the performance. Not sure how hard to implement.====='; 'Can this improve performance for large videos with the WebGL2 pipeline?1. Add a new sourceCanvas with the size of the mask.2. Resize by copying from sourcePlayback.html to sourceCanvas with a 2d context.3. Use sourceCanvas instead of sourcePlayback.html when calling gl.texImage2D here:4. https://github.com/Volcomix/virtual-background/blob/main/src/pipelines/webgl2/webgl2Pipeline.ts#L170====='],Slow Execution,Poor Performance,WebGL Limits,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.1,D.4
https://github.com/justadudewhohacks/face-api.js/issues/831,Matching faces between iOS (swift) and javascript,1,open,2021-12-04T23:43:22Z,2021-12-05T22:13:26Z,Hi; great work and thanks; this library has been an awesome easy inclusion to our solution.however; now we need to implement the same tensor models to find faces and the 68 landmarks on iOS devices using swift. To do so; we would prefer to convert the appropriate models to coreml using apples python converter.I’ve been trying to use the true converter to convert the js models back to the original models to load into python; but having a few issues. Could you provide any pointers? Do you have the original tf saved models or know where they are so we aren’t converting backwards?thanks so much again and keep up the great work!,['To be clear; we are intending to try and match people identified in a photo on one platform against people in different photos on the other platform; so we need to use the same models in both environments.Thanks again!====='],Reference Error,Crash,Unimplemented Operator,WebGL,Backend,add function,add function,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.1,A.1
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/34,Tensorflow and Tensorflowjs version,4,open,2020-09-11T15:05:46Z,2020-09-21T10:57:45Z,Hello @reiinakano;Thanks for your great repository. Can you tell me which version of Tensorflow and Tensorflowjs have you used?I want to convert Tensorflowjs model to TFLite model but I have some issue.Look forward to hearing from you soon;Thanks,"['@KienPM as I\'ve been working on a fork of this repo; I can answer that:This uses Tensorflowjs 1.0 (as you can notice in `package.json` as well `""@tensorflow/tfjs"": ""~1.0.0""`).What\'s the issue you\'re encountering with TFLite?====='; '@geni94 Thanks for your answer.I\'ve tried to convert TensorflowJS model to TFlite model according to [this answer](https://stackoverflow.com/questions/62544836/how-to-convert-from-tensorflow-js-json-model-into-tensorflow-savedmodel-or).When I run `tensorflowjs_converter --input_format""=tfjs_layers_model --output_format=keras tfjs_model.json hdf5_keras_model.hdf5` to convert from TensorflowJS model to Keras model with tensorflow 2.3.0; tensorflowjs 2.3.0; I got ""ValueError: Improper config format""When I run with tensorflow 1.15.0; tensorflowjs 1.7.4; I got ""KeyError: \'class_name\'""====='; 'I literally told you in the first comment that this library uses tensorflow 1.0; and yet you go ahead and try with 1.15; 1.7.4; 2.3.0; etc... The `package.json` specifies the version: `""@tensorflow/tfjs"": ""~1.0.0""`. I\'m not surprised you can\'t transform this model with TFLite; seeing that the versions do not match.====='; 'thank you!=====']",Data & Model Error,Crash,Inconsistent Modules,Operator,API,change framework version,Changing version,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Version Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.2,A.2
https://github.com/justadudewhohacks/face-api.js/issues/144,Face detection don't work on AWS Lambda due to node-canvas not working on node 8.10,24,closed,2018-11-21T00:38:45Z,2021-09-09T15:44:50Z,Hi there!I am trying to get this up and running on AWS Lambda but are having trouble as it seem the 'canvas' package is broken on node 8.10. I am trying to get some face detection going.https://github.com/Automattic/node-canvas/issues/1252#issuecomment-437598572It looks like its kind of a pain to try and run any other version of node on AWS Lambda; so I started trying to use it without the canvas package. As you mention in the README and I also noticed that the faceapi.locateFaces() function also takes a Tensor4D class as input. I have never used Tensorflow and I am a little confused as how to turn a ArrayBuffer from axios into a correctly shaped Tensor4D object.I am fetching a jpeg image using axios.I found the tf.tensor4d function but not sure what shape and dtype it should be.Do you have any idea?My code so far:```jsconst { data: imageBuffer } = await axios.get(url; {	responseType: 'arraybuffer'})const imageTensor = tf.tensor4d(imageBuffer; [?; ?; ? ;?])const faces = await detector.locateFaces(imageTensor)```Error messages look similar to this one:`Error: Based on the provided shape; [1;2;3;4]; and dtype float32; the tensor should have 24 values but has 68695`Any help is greatly appreciated!,"[""Hey man; hows it going?Ohh that's a pity; node-canvas is obviously the easiest way to use face-api.js with node.Well you can either pass in tf.Tensor3Ds or tf.Tensor4Ds with a batch size of 1:`const imageTensor = tf.tensor3d(imageBuffer; [height; width; channels])`; where channels should be 3 and in RGB order.However; the imageBuffer you receive from axios is most likely jpeg or png encoded right? You have to create the image tensor from the raw image data; so you will have to decode the data first. There are probably npm packages; which can do that for you. For example I have seen people using ffmpeg for this.=====""; ""Thanks!Tried the get-pixels npm package and it works!get-pixels return a 4 channel array all the time; even for jpegs 🤷 ; so had to remove the alpha channel.```typescriptconst pixels = await new Promise<Pixels>(resolve => {\tgetPixels(url; (err; pixels: Pixels) => {\t\tif (err) {\t\t\tthrow err\t\t}\t\tconsole.log('pixels:'; pixels)\t\tresolve(pixels)\t})})// remove alpha channelconst RGBValues = []pixels.data.forEach((px; i) => {\tif ((i + 1) % 4 != 0) {\t\tRGBValues.push(px)\t}})const imageTensor = tf.tensor3d(RGBValues; [pixels.shape[1]; pixels.shape[0]; 3]) as anyconst faces = await detector.locateFaces(imageTensor)```Now to the task of getting it up on Lambda without breaking their code size limit; wish me luck!=====""; ""> Thanks!> > Tried the get-pixels npm package and it works!> > get-pixels return a 4 channel array all the time; even for jpegs 🤷 ; so had to remove the alpha channel.> > ```ts> const pixels = await new Promise<Pixels>(resolve => {> \tgetPixels(url; (err; pixels: Pixels) => {> \t\tif (err) {> \t\t\tthrow err> \t\t}> \t\tconsole.log('pixels:'; pixels)> \t\tresolve(pixels)> \t})> })> > // remove alpha channel> const RGBValues = []> pixels.data.forEach((px; i) => {> \tif ((i + 1) % 4 != 0) {> \t\tRGBValues.push(px)> \t}> })> const imageTensor = tf.tensor3d(RGBValues; [pixels.shape[1]; pixels.shape[0]; 3]) as any> const faces = await detector.locateFaces(imageTensor)> ```> > Now to the task of getting it up on Lambda without breaking their code size limit; wish me luck!Thanks; that helped me. But it's quite slowly (1000ms) compared to canvas ( < 10ms ).=====""; 'tensorflow team is working on a solution: https://github.com/tensorflow/tfjs/issues/298#issuecomment-442263569====='; 'Nice!====='; 'It wasn\'t easy; but I finally got it running in a Lambda on AWS. The tfjs-node package includes a native library and has to be built using the correct environment. I achieved this using Docker and the `lambci/lambda:nodejs8.10` image from the https://github.com/lambci/docker-lambda project. This image seem to be one the best images to simulate the Lambda environment.The built tfjs-node module includes a 110mb binary file. `@tensorflow/tfjs-node/deps/lib/libtensorflow.so`The Lambda size limit is 250mb (uncompressed).Well; with 140mb left for other modules it should be fine right? I tried deploying the package using `serverless deploy` resulting in this cryptic error message.`ENOENT: no such file or directory; open \'/Users/bobmoff/Projects/picular/serverless-face-api/node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so\'`The file exists alright; but its a symlink that points to `libtensorflow.so -> /var/task/node_modules/@tensorflow/tfjs-node/deps/lib/libtensorflow.so`Ahh; the symlink was created inside the docker container and it is a not a relative link. Ok; so i delete the symlink and created a new one that is relative.`libtensorflow.so -> ../../deps/lib/libtensorflow.so`But zipping up the package; using `serverless deploy` results in a zip that when uncompressed is 343mb ?? Using Disk Inventory X on the uncompressed folder I saw this:<img width=""965"" alt=""two-binaries"" src=""https://user-images.githubusercontent.com/1033236/50040999-e40c3d80-004d-11e9-9b6c-a6ba5e412ae5.png"">Why are there 2 large binaries? Shouldn\'t one of them just be a symlink ? The symlink is gone; and replaced with a copy of the binary. Hmm. After a bit of research I learned that the default behaviour when zipping symlinks is that they get ""resolved"" (link is followed and original content is copied). Ok. But there is a --symlinks flag for the zip program that can modify this behaviour to keep symlinks as they are; instead of resolving them. This solved the size issue; nut now I realised that I depend on a local project that is linked that actually need to be resolved when packaged. I couldn\'t find any way to specify what folder/file should be resolved or not when packaging through serverless. There is a way to exclude files/folders from package though. I excluded the symlink.```yamlpackage:    exclude:        - node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so```So using the following commands I first package the project (excluding the symlink) and then manually include the symlink again into the zip archive. (`-y` is the shorthand for `--symlinks`)```sls package --package my-artifactszip -y my-artifacts/serverless-face-api.zip node_modules/@tensorflow/tfjs-node/build/Release/libtensorflow.so```Now I have a package that is 233mb! Yeah; 17mb to spare! Hehe. Ofc there are more size reduction that can be made be excluding more stuff; like excluding unused weights/models from face-api etc.. But I was just happy to get below the limit.Happy as a unicorn on christmas I deployed my neat ""little"" package to AWS.`sls deploy --package my-artifacts`.. and lived happily ever after. ====='; ""`libtensorflow.so` appears to be 183mb now! don't suppose you could share your `package.json` for version numbers?=====""; 'Sure; here are my deps; haven upgraded in a while :)```javascript{    ""@tensorflow/tfjs-node"": ""^0.2.1"";    ""axios"": ""^0.18.0"";    ""face-api.js"": ""^0.17.1"";    ""get-pixels"": ""^3.3.2"";    ""lodash"": ""^4.17.11"";    ""module-alias"": ""^2.1.0"";    ""moment"": ""^2.23.0"";    ""monk"": ""^6.0.6"";    ""url-join"": ""^4.0.0""}```====='; 'The tensorflow package (version 1.2.3) is now 294MB!  `libtensorflow.so.1.14.0` is now 216MB; and `libtensorflow_framework.so.1.14.0` is another 35MB. Pretty sure there is no way to get this package into Lambda. @MatthewAlner did you manage to get it working with a newer version? Or do I need to roll all the way back to the versions used by @bobmoff====='; 'Perhaps you could try to deploy using https://www.openfaas.com/ ====='; ""I didn't in the end 😞=====""; 'Thanks for all the helpful info in this issue; and of course thanks again to @justadudewhohacks . I did eventually manage to get it working on Lambda; but it wasn\'t easy. I ran into all the same problems that @bobmoff did; and if I\'d read his post more carefully at the start I would have saved myself a lot of time (in particular the `libtensorflow.so` linking issue). I\'m using the Node 10.x runtime.The newer versions of `tfjs-node` just get bigger and bigger; especially their bundled `.so` files. There is no way I could find to make the newer versions fit within Lambda\'s 250MB limit; so I had to roll way back to `v0.2.1`. You also need to roll `face-api.js` back to a similarly old version; as it seems `face-api.js` is coupled to a specific version of `tfjs-node`. Below is my deps:```  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^0.2.1"";    ""face-api.js"": ""^0.17.1"";    ""get-pixels"": ""^3.3.2""  }```If you really wanted to use the latest version of Tensorflow and Face-api; I suspect you could use a technique like this https://github.com/lucleray/tensorflow-lambda; which basically uploads a zipped copy of the dependencies; and then unzips them into `/tmp` before running them. There is obviously overhead for unzipping the files; but if your lambda instances are often reused then that initial overhead fades away.====='; ""Hey @bobmoff; just wondering if you had any issues with memory leaks in your setup? My face detection lambdas get reused most of the time because they get called so often; and I've noticed the memory used slowly grows with each invocation; until it finally breaks the Lambda memory limit and starts again. No idea if it's `get-pixels`; `tfjs-node` or `face-api.js` that is the issue.=====""; 'Howdy @henrydawson. No; sorry. I havent experienced that; but that is probably due to the fact that I havent had the pressure like you seem to have; so my containers/lambdas have probably been recycled before that happenes.Sorry to not be of any more help :/====='; 'Hi @henrydawson and all;just some feedback on canvas running on lambda and also a question. I\'ve managed to make canvas run on lambda by using https://github.com/lambci/docker-lambda to make some tests. Thanks to the ""build"" image I had to run a bash shell on the docker build image to:1/ run npm install 2/ copy 3 libraries from the docker build image /lib64 to the root of my project: libblkid.so.1; libmount.so.1; libuuid.so.1I\'m now trying to use https://github.com/lucleray/tensorflow-lambda for tensorflow but with no luck. My little project is based on one of the examples from the repository. It uses the common directory as in the examples directory and I\'ve tryed to subsitute the import \'@tensorflow/tfjs-node\'; from the env.ts file with import ""tensorflow-lambda"" but it does not work. It looks like tfjs-node have a mechanism to supersede the tfjs-core functions but this is not happening with tensorflow-lambda. Any hint on how I could make this happen ? Thanks in advance !  ====='; ""@bobmoff i am new to this; and wanted to ask if you've got an example code to get started with face-api.js and AWS lambda? =====""; ""> @bobmoff i am new to this; and wanted to ask if you've got an example code to get started with face-api.js and AWS lambda?Sorry. Not using this any more. But Lambda have upped the limit to 10GB for each function so size should not be a problem any longer.=====""; '@bobmoff I really wanna move this facial recognition to lambda. Any help would be highly appreciated. I tried with this version also```""dependencies"": {    ""@tensorflow/tfjs-node"": ""^0.2.1"";    ""face-api.js"": ""^0.17.1"";    ""get-pixels"": ""^3.3.2""  }```but still limit exceeding. ====='; 'Are you reaching the max limit of 10gb ?The issue I had was getting below 250mb.====='; '> Are you reaching the max limit of 10gb ?> > The issue I had was getting below 250mb.Going above deployment package unzipped limit size (250mb)====='; 'If you switch using containers you can use 10 gb.https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/====='; ""Hi; I did get this working on Lambda a long time ago; but that was with older versions; and I ran into a lot of issues (especially the hard to resolve memory issues mentioned above). Eventually; I gave up on Lambda and just run it on EC2 using spot instances; which was cheaper and better for my workload. It was also simpler as I didn't have mess around with the file size issues.That said; I recently used EFS with Lambda for some image processing that required extra space; and it was really easy to use. I imagine you could use EFS to get around all the size limits.https://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/=====""; '@bobmoff @henrydawson thank you both of you for pointing me in the direction. I will check both `aws lambda container` and `EFS`.====='; ""@bobmoff I have successfully deployed in Lambda using containers. But I'm facing one issue. I'm just doing `faceapi.detectSingleFace().withFaceLandmarks().withFaceDescriptor()` in lambda to detect single face and returning that output as a response via API Gateway. I can't able to return without doing `JSON.stringify(result)` and If I do stringify the result; that result not working in final comparison. I dont know what I'm doing wrong.update: I managed to fix it. And its working fine now.=====""]",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change API,Replace API with another effective one,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.1.1,D.1
https://github.com/justadudewhohacks/face-api.js/issues/340,Not working in Microsoft Edge - lack of TextEncoder support (since 0.20.1),1,open,2019-07-03T18:11:16Z,2020-08-31T13:52:21Z,"After bumping to v0.20.1; using Edge 44.18362.1.0; the console emits the following error:**'TextEncoder' is not defined**Microsoft's delightful Edge browser doesn't currently support TextEncoder or TextDecoder:  https://caniuse.com/#feat=textencoderThis fixes it:```                TextEncoder = function TextEncoder() {		}		TextEncoder.prototype.encode = function(s) {			const e = new Uint8Array(s.length);			for (let i = 0; i < s.length; i += 1) {				e[i] = s.charCodeAt(i);			}			return e;		}		TextDecoder = function TextDecoder() {					}		TextDecoder.prototype.decode = function(arr) {			let d = """";			for (let i = 0; i < arr.length; i += 1) {				d += String.fromCharCode(arr[i]);			}			return d;		} ","[""I'm working with Next.JS and there are packages in my project that have text encoder. How can I use this in my project?=====""]",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,patch environment,Fix environment adaptability,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.4,D.2
https://github.com/justadudewhohacks/face-api.js/issues/829,[ Electron ] Cannot import faceApi   Uncaught TypeError: this.util.TextEncoder is not a constructor,3,closed,2021-11-12T12:34:22Z,2021-11-16T19:33:45Z,I am trying to integrate it with electron+angluar application; it is working as expected in browser ; but not working in electron ;I'm getting this error ```tf-core.esm.js:17 Uncaught TypeError: this.util.TextEncoder is not a constructor    at new t (tf-core.esm.js:17)    at Module.6636 (tf-core.esm.js:17)    at __webpack_require__ (bootstrap:19)    at Module.4687 (main.js:153594)    at __webpack_require__ (bootstrap:19)    at Module.1858 (detail.module.ts:13)    at __webpack_require__ (bootstrap:19)    at Module.7882 (shared.module.ts:15)    at __webpack_require__ (bootstrap:19)    at Module.158 (main.js:174358)````I know it is something related to tenserflow but I dont know solve it.,['@DOCSPLOIT  I am facing the same issue; did you find a solution ?====='; 'Yes I found it; you can either use node version and use ipc events from electron to angular or by puting the faceapi source as asset and include in `angular.json` and use it directly in angular; best of luck :crossed_fingers: :====='; '> > > Yes I found it; you can either use node version and use ipc events from electron to angular or by puting the faceapi source as asset and include in `angular.json` and use it directly in angular; best of luck 🤞 :Thanks for the feedbacks; appreciate it.====='],Initialization Faliure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Desktop,Platform,change framework position,change framework position,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",C,D.3
https://github.com/bensonruan/Face-Mask/issues/1,TypeError: t.toFloat is not a function,2,closed,2021-04-24T16:39:42Z,2021-05-08T16:02:07Z,I was trying to understand how Tensorflow wroks on web. As your instruction I cloned your project and opened it on my browser. I faced the issue.`Uncaught (in promise) TypeError: t.toFloat is not a function    c https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    tidy https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    scopedRun https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    tidy https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    kn https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core:17    estimateFaces https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    a https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    a https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    r https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    r https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    estimateFaces https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh:17    detectFaces https://localhost/face/js/face-mask.js:109    <anonymous> https://localhost/face/js/face-mask.js:77    promise callback* https://localhost/face/js/face-mask.js:75`,"['This issue can be solved by downgrading tensorflow version to 2.6.0Replace below CDN in Index.html```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@2.6.0""></script>```that should work====='; '> > > This issue can be solved by downgrading tensorflow version to 2.6.0> > Replace below CDN in Index.html> > ```> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js""></script>> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0""></script>> <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@2.6.0""></script>> ```> > that should workThank you so much!=====']",Reference Error,Crash,Inconsistent Modules,Operator,API,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5906,The output of tf.tensor() is different when dtype is int32 and float32,2,closed,2021-11-30T16:43:05Z,2021-11-30T17:40:56Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):  script link- TensorFlow.js version (use command below): 3.11.0- Browser version:  chrome  95.0.4638.69 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**For uninitialized input array; `tf.tensor(input;""int32"")` returns 0; but `tf.tensor (input; ""float32"")` returns NaN.```    var data=new Array(5);    var i32 = tf.tensor(data; [5]; 'int32');    var f32 = tf.tensor(data; [5]; 'float32');    console.log(i32.arraySync())    console.log(f32.arraySync())```for example; the output of above code is:![image](https://user-images.githubusercontent.com/68681463/144089486-1ee7a962-20ae-42b6-8f24-10a927e60e0a.png)**Describe the expected behavior**For any dtype; the returned result should be the same.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['@liliquan0118 Thank you for reporting this; this seems to be caused by javascript difference between Int32Array and Float32Array:```var data=new Array(5);console.log(new Int32Array(data))VM301:1 Int32Array(5)\xa0[0; 0; 0; 0; 0; buffer: ArrayBuffer(20); byteLength: 20; byteOffset: 0; length: 5]undefinedconsole.log(new Float32Array(data))VM360:1 Float32Array(5)\xa0[NaN; NaN; NaN; NaN; NaN; buffer: ArrayBuffer(20); byteLength: 20; byteOffset: 0; length: 5]```I would recommend not to use uninitialized array; since the behavior of the system is undefined.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5906"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5906"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Operator,API,,,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,D.2
https://github.com/tensorflow/tfjs/issues/5955,Error: Random seed is not implemented for Orthogonal Initializer yet.,1,open,2021-12-20T07:51:07Z,2021-12-20T19:43:02Z,You probably forgot this.code:`let i = tf.initializers.orthogonal({gain:1; seed:12345});`3.12.0,['This will be future request ; could you please elaborate your use case.====='],Reference Error,Crash,Unimplemented Operator,Layer API,API,add support for operator,Add unsupported operator,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5950,Universal Sentence Encoder support for wasm backend,2,closed,2021-12-16T00:30:10Z,2021-12-16T18:38:33Z,<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 3.11.0- Are you willing to contribute it (Yes/No): yes; but I don't want to commit to doing it bc I don't yet understand how much work it requires.**Describe the feature and the current behavior/state.**I want to use the Universal Sentence Encoder model with a Wasm backend; because I don't have access to webgl and cpu is really slow. According to the [list of supported wasm models](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md); USE is not yet supported.**Will this change the current api? How?**I assume not but can't say for sure.**Who will benefit with this feature?**Anyone looking to use the USE model with a wasm backend.**Any Other info.**,['@pyu10055 @jasonmayes ====='; '@marcgreen USE is supported by wasm; we will update the readme to reflect that. thanks!====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5948,fromPixels for HTMLVideoElement is slow in Firefox and Safari with WebGL backend,1,open,2021-12-15T18:59:38Z,2021-12-15T19:08:19Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Mac OS Big Sur; running on Intel i7 Mac Mini (2018); Intel UHD Graphics 630- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.12.0- Browser version: Firefox 95; Safari 14.1.1- Tensorflow.js Converter Version: N/A**Describe the current behavior**Currently when using `tf.browser.fromPixels` on a `HTMLVideoElement` with an mp4 source; this method takes a long time on the WebGL backend (around 7 ms on my machine). According to profiling information from Firefox; most of the time is spent on copying the video data to a temporary canvas. When I disabled this temporary canvas in a custom fork tfjs and sent `HTMLVideoElement` directly to `backend.gpgpu.uploadPixelDataToTexture`; `fromPixels` does not even register in the profiler anymore and overall FPS goes up from 22 to 40 for my use case. I observe the same behavior in Safari; Chrome does not suffer from this issue.**Describe the expected behavior**fromPixels does not take 7ms to complete.**Standalone code to reproduce the issue**Our team is hitting this problem with mocap4face (https://github.com/facemoji/mocap4face/tree/main/js-example); you can also check a running demo at https://facemoji.co/mocap4face/ and run the Firefox profiling there to see the issue (make sure to check Show Gecko Platform Data in the Profiler settings to better see the canvas copy).**Other info / logs**The issue seems to be gone when I comment out [lines 58 to 69 in tfjs-backend-webgl/src/kernels/FromPixels.ts](https://github.com/tensorflow/tfjs/blob/20baee0ac6a39da914c659526bdc67709d700f9d/tfjs-backend-webgl/src/kernels/FromPixels.ts#L58-L69) (the `if (isImage || isVideo) { ... }` ).Note that the code seems to work fine without this extra condition in all the browsers I tested (even Windows and Mobile ones).,['related issue https://github.com/tensorflow/tfjs/issues/5947 ====='],Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,remove unnecessary code,remove unnecessary code,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5946,qna model fails with very short passages,4,closed,2021-12-15T12:38:18Z,2021-12-16T15:44:09Z,"### recreate/demonstration of the bug```jsconst tfjs = require('@tensorflow/tfjs-node');const qna = require('@tensorflow-models/qna');const question = ""Who did John see?"";const passage = ""John saw Mary"";qna.load()    .then((model) => {        return model.findAnswers(question; passage);    })    .then((answers) => {        console.log(answers);    });```This fails with an uncaught exception:```(node:2393) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'index' of undefined    at QuestionAndAnswerImpl.convertBack (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:323:34)    at QuestionAndAnswerImpl.getBestAnswers (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:282:27)    at QuestionAndAnswerImpl.<anonymous> (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:227:47)    at step (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:33:23)    at Object.next (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:14:53)    at fulfilled (./node_modules/@tensorflow-models/qna/dist/question_and_answer.js:5:58)    at processTicksAndRejections (internal/process/task_queues.js:93:5)```The above recreate is for Node.js because that was simpler to demonstrate; but I have seen the same errors happen when running in browsers as well. For example; if you paste the same question (""Who did John see?"") and passage (""John saw Mary"") into https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html you'll get the same uncaught exception in your browser console.```[Error] Unhandled Promise Rejection: TypeError: undefined is not an object (evaluating 't[u].index')	(anonymous function) (demo.20d541c6.js:1658:774)	asyncFunctionResume	(anonymous function)	promiseReactionJobWithoutPromise	promiseReactionJob```### Expected behaviorThis should return an answer (or at least successfully return an empty array).### CauseI think the problem is in a check (to see if the indexes are in the known tokens map) failing to take the output offset into account.https://github.com/tensorflow/tfjs-models/blob/ccfcb2ab9b479b7e053f8eafaf68bd75a71c6551/qna/src/question_and_answer.ts#L277```js        if (tokenToOrigMap[start] && tokenToOrigMap[end] && end >= start) {```This means the later indexes into the tokens map are not guaranteed safe; and can return undefined. https://github.com/tensorflow/tfjs-models/blob/ccfcb2ab9b479b7e053f8eafaf68bd75a71c6551/qna/src/question_and_answer.ts#L336-L339```js    const shiftedStart = start + OUTPUT_OFFSET;    const shiftedEnd = end + OUTPUT_OFFSET;    const startIndex = tokenToOrigMap[shiftedStart];    const endIndex = tokenToOrigMap[shiftedEnd];```### Suggested fixI think updating the check to include the output offset should correct the problem. This protects entry into the `convertBack` function so that it is only called with values that will definitely be in `tokensToOrigMap`.","[""I've put my suggested fix into a pull request in case that helps explain it better!https://github.com/tensorflow/tfjs-models/pull/888 =====""; 'Thank you @dalelane ; I have assigned the reviewer for PR. ====='; 'Related PR has been merged ; closing this issue. Please @mention to reopen if issue still persists.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5946"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5946"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Model API,API,condition replacer,condition replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5942,Npm installation fails.,19,open,2021-12-14T15:26:25Z,2021-12-21T18:40:08Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10 - Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: - - TensorFlow.js installed from (npm or script link): npm install @tensorflow/tfjs-node-gpu- TensorFlow.js version: 3.12.0- CUDA/cuDNN version: CUDA@11.5; cuDNN@8.3.1#define CUDNN_MAJOR 8#define CUDNN_MINOR 3#define CUDNN_PATCHLEVEL 1**Describe the problem**I opened up a default react ts project in vs 2022 stable version. View -> terminal. In terminal; npm install @tensorflow/tfjs or tfjs-node or tfjs-node-gpu. None of them works.I also tried npm install in the terminal directly opened from Windows 10 and failed.I don't know if this issue is caused by the internet since I'm in China. The familiar 404; you know.**Provide the exact sequence of commands / steps that you executed before running into the problem**As above.**Any other info / logs** npm install @tensorflow/tfjs-node-gpu> @tensorflow/tfjs-node-gpu@3.12.0 install E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu> node scripts/install.js gpu downloadGPU-windows-3.12.0.zip* Downloading libtensorflowhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.7.0.zip[==============================] 11266495/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/GPU-windows-3.12.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node-gpu@3.12.0 and node@14.17.3 (node-v83 ABI; unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/GPU-windows-3.12.0.zipgyp ERR! find VSgyp ERR! find VS msvs_version not set from command line or npm configgyp ERR! find VS VCINSTALLDIR not set; not running in VS Command Promptgyp ERR! find VS unknown version ""undefined"" found at ""C:\Program Files\Microsoft Visual Studio\2022\Community""gyp ERR! find VS checking VS2019 (16.11.31911.196) found at:gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community""gyp ERR! find VS - found ""Visual Studio C++ core features""gyp ERR! find VS - found VC++ toolset: v142gyp ERR! find VS - missing any Windows SDKgyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usegyp ERR! find VS looking for Visual Studio 2015gyp ERR! find VS - not foundgyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8gyp ERR! find VSgyp ERR! find VS **************************************************************gyp ERR! find VS You need to install the latest version of Visual Studiogyp ERR! find VS including the ""Desktop development with C++"" workload.gyp ERR! find VS For more information consult the documentation at:gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsgyp ERR! find VS **************************************************************gyp ERR! find VSgyp ERR! configure errorgyp ERR! stack Error: Could not find any Visual Studio installation to usegyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16gyp ERR! stack     at ChildProcess.exithandler (child_process.js:326:5)gyp ERR! stack     at ChildProcess.emit (events.js:375:28)gyp ERR! stack     at maybeClose (internal/child_process.js:1055:16)gyp ERR! System Windows_NT 10.0.19043gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""gyp ERR! cwd E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpugyp ERR! node -v v14.17.3gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:375:28)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1055:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:288:5)node-pre-gyp ERR! System Windows_NT 10.0.19043node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""E:\\ts practice\\tfjs with ts test\\tfjsWithTsTest\\tfjsWithTsTest\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd E:\ts practice\tfjs with ts test\tfjsWithTsTest\tfjsWithTsTest\node_modules\@tensorflow\tfjs-node-gpunode-pre-gyp ERR! node -v v14.17.3node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not oknpm WARN @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@7.16.0 requires a peer of @babel/core@^7.13.0 but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""win32"";""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules\webpack-dev-server\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""win32"";""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules\watchpack-chokidar2\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""win32"";""arch"":""x64""})npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node-gpu@3.12.0 install: `node scripts/install.js gpu download`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node-gpu@3.12.0 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:One last line is note copied. It's only a path.Tomorrow I'll try installing the python version with pip to check if both ways fail. Btw; would you please add some guide for typescript with tf. The type system is too helpful. Thanks.","['Yes this is due to links not accessible from China ; @pyu10055 @jasonmayes is there any way we can install tfjs from China ?====='; 'If you could try the python way to see if that fails too so we can narrow down if this is specific to TFJS or NPM that would be useful.====='; '> Yes this is due to links not accessible from China ; @pyu10055 @jasonmayes is there any way we can install tfjs from China ?> If you could try the python way to see if that fails too so we can narrow down if this is specific to TFJS or NPM that would be useful.Hi. I just started the installing of python version. The downloading speed is 7 to 40 kB/s; and it probably takes 4 to 8 hours according to the terminal. Offline installer is way better in such a case. Offline installer or any zip is definitely way better for js version since even the link is eventually accessible in China; the downloading speed would probably be very slow.If any issue occurs half way; all the efforts before the issue are wasted. Which could lengthen the installation dramatically.When I downloaded CUDA and cuDNN; the speed is 11 MB/s ish. I downloaded them via firefox. Even for those I have to download with the proxy; the speed is at least 400kB and in avg of 1MB/s.So; please consider offline installer. Thanks.====='; ""py 3.9.6pip 21.3.1Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.9 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.8 six-1.16.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.0 termcolor-1.1.0 typing-extensions-4.0.1 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.13.3 zipp-3.6.0WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)and then; import tensorflow as tf>>> print(tf.__version__)2.7.0I believe it works.Now I plan to update the npm and try again. But since vs2022 also failed; I don't expected the newest npm would work.=====""; ""Update:I reinstall node.js. Updated the npm (npm: '8.3.0';). Then it's very easy to install tensorflow with npm. Sorry for disturbing all of you. By the way; would you remind people to make sure they installed the newest npm. Because as you see; the log I posted yesterday didn't mention the version. Npm works for all the other cases expect for tensorflow in my computer. So I didn't doubt the npm to be out dated at all.Anyway; I don't have to learn python. :)=====""; 'Thank you @YagaoDirac. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5942"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5942"">No</a>====='; ""> Update: I reinstall node.js. Updated the npm (npm: '8.3.0';). Then it's very easy to install tensorflow with npm. Sorry for disturbing all of you. By the way; would you remind people to make sure they installed the newest npm. Because as you see; the log I posted yesterday didn't mention the version. Npm works for all the other cases expect for tensorflow in my computer. So I didn't doubt the npm to be out dated at all. Anyway; I don't have to learn python. :)I've upgraded to npm 8.3.0 and I'm on node 16.13.1.  I'm still getting the same error you were getting before. Are there any additional steps you did besides upgrading? =====""; '@yagaodirac Very happy to see you managed to resolve by using a higher version of NPM. I shall relay this message to the team to update this. Can you also let me know which tutorials you were following so I can check the documentation for those too.@deadly Are you also based in China? ====='; 'No; I am in America.====='; '@deadly Can you confirm more details about your system. OS / Node + NPM version etc ====='; '@jasonmayes My OS is Windows 10 (19043.1348). Running the node -v command; my node version is 16.3.1. Running the npm -v command; my npm version is 8.3.0.Here is the full error I get when trying to install:```npm ERR! path C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! command failednpm ERR! command C:\\WINDOWS\\system32\\cmd.exe /d /s /c node scripts/install.jsnpm ERR! CPU-windows-3.12.0.zipnpm ERR! https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.7.0.zipnpm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnpm ERR! node-pre-gyp info it worked if it ends with oknpm ERR! node-pre-gyp info using node-pre-gyp@1.0.4npm ERR! node-pre-gyp info using node@16.13.1 | win32 | x64npm ERR! node-pre-gyp info check checked for ""C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" (not found)npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! node-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! node-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.12.0 and node@16.13.1 (node-v93 ABI; unknown) (falling back to source compile with node-gyp)npm ERR! node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.12.0/CPU-windows-3.12.0.zipnpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@8.3.0npm ERR! gyp info using node@16.13.1 | win32 | x64npm ERR! gyp info oknpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@8.3.0npm ERR! gyp info using node@16.13.1 | win32 | x64npm ERR! gyp info find Python using Python version 3.10.1 found at ""C:\\Python310\\python.exe""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS msvs_version was set from command line or npm confignpm ERR! gyp ERR! find VS - looking for Visual Studio version 2015npm ERR! gyp ERR! find VS VCINSTALLDIR not set; not running in VS Command Promptnpm ERR! gyp ERR! find VS checking VS2019 (16.4.29806.167) found at:npm ERR! gyp ERR! find VS ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community""npm ERR! gyp ERR! find VS - found ""Visual Studio C++ core features""npm ERR! gyp ERR! find VS - found VC++ toolset: v142npm ERR! gyp ERR! find VS - found Windows SDK: 10.0.18362.0npm ERR! gyp ERR! find VS - msvs_version does not match this versionnpm ERR! gyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usenpm ERR! gyp ERR! find VS looking for Visual Studio 2015npm ERR! gyp ERR! find VS - not foundnpm ERR! gyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS valid versions for msvs_version:npm ERR! gyp ERR! find VS - ""2019""npm ERR! gyp ERR! find VS - ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! configure errornpm ERR! gyp ERR! stack Error: Could not find any Visual Studio installation to usenpm ERR! gyp ERR! stack     at VisualStudioFinder.fail (C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:121:47)npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:74:16npm ERR! gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:351:14)npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:70:14npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\find-visualstudio.js:372:16npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\util.js:54:7npm ERR! gyp ERR! stack     at C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\lib\\util.js:33:16npm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:404:5)npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)npm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)npm ERR! gyp ERR! System Windows_NT 10.0.19043npm ERR! gyp ERR! command ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\\\Users\\\\redacted\\\\node_modules\\\\@tensorflow\\\\tfjs-node\\\\lib\\\\napi-v8\\\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=C:\\\\Users\\\\redacted\\\\node_modules\\\\@tensorflow\\\\tfjs-node\\\\lib\\\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8"" ""--python=C:\\\\Users\\\\redacted\\\\.windows-build-tools\\\\python27\\\\python.exe"" ""--msvs_version=2015""npm ERR! gyp ERR! cwd C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! gyp ERR! node -v v16.13.1npm ERR! gyp ERR! node-gyp -v v8.3.0npm ERR! gyp ERR! not oknpm ERR! node-pre-gyp ERR! build errornpm ERR! node-pre-gyp ERR! stack Error: Failed to execute \'C:\\Program Files\\nodejs\\node.exe C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js configure --fallback-to-build --module=C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node --module_name=tfjs_binding --module_path=C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8 --python=C:\\Users\\redacted\\.windows-build-tools\\python27\\python.exe --msvs_version=2015\' (1)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (C:\\Users\\redacted\\node_modules\\@mapbox\\node-pre-gyp\\lib\\util\\compile.js:89:23)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.emit (node:events:390:28)npm ERR! node-pre-gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)npm ERR! node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)npm ERR! node-pre-gyp ERR! System Windows_NT 10.0.19043npm ERR! node-pre-gyp ERR! command ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Users\\\\redacted\\\\node_modules\\\\@mapbox\\\\node-pre-gyp\\\\bin\\\\node-pre-gyp"" ""install"" ""--fallback-to-build""npm ERR! node-pre-gyp ERR! cwd C:\\Users\\redacted\\node_modules\\@tensorflow\\tfjs-nodenpm ERR! node-pre-gyp ERR! node -v v16.13.1npm ERR! node-pre-gyp ERR! node-pre-gyp -v v1.0.4npm ERR! node-pre-gyp ERR! not oknpm ERR! * Downloading libtensorflownpm ERR!npm ERR! * Building TensorFlow Node.js bindingsnpm ERR! A complete log of this run can be found in:npm ERR!     C:\\Users\\redacted\\AppData\\Local\\npm-cache\\_logs\\2021-12-15T02_26_21_080Z-debug.log```The Visual Studio C++ workload error confused me; so I tried to reinstall the correct workload for Visual Studio; but this did not fix the issue.====='; 'Can you try using the windows sub system for linux. You should be able to install popular distros via the windows store for example like Ubuntu. If you install TensorFlow on those it should work just fine.====='; 'This tutorial may help if you have not done that before: https://ubuntu.com/tutorials/ubuntu-on-windows====='; 'same issue here cannot solve====='; '@jasonmayes installing the library from the ubuntu subsystem worked (node version 17.3.0 and npm 8.3.0). I upgraded to node 17.3.0 on my main windows system and still continued to get the same error.====='; 'It seems there may be an issue with Windows that needs further investigation however the recommended way to use for Windows 10 users for now would be to use the Ubuntu subsystem which seems to work for all.====='; 'Running into the same error on Node v14; Windows 11.====='; 'Re-opening for team to investigate once folk are back in the office after holidays as it seems to be effecting a lot of folk.=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(GPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5992,Unable to build @tensorflow/tfjs-node, ,open,2021-12-14T15:26:25Z,,,,Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Compile Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5940,A dynamic link library (DLL) initialization routine failed,7,open,2021-12-14T11:16:32Z,2021-12-22T05:26:53Z,**System information**- OS Windows 10 x64- TensorFlow.js installed from npm i @tensorflow/tfjs-node- TensorFlow.js version: 3.12.0- CUDA/cuDNN version: none**Describe the problem**after installation when i run this:const tf = require('@tensorflow/tfjs-node');a get error:node:internal/modules/cjs/loader:1183  return process.dlopen(module; path.toNamespacedPath(filename));                 ^Error: A dynamic link library (DLL) initialization routine failed.\\?\D:\NodeJS projects\tensorFlow\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node    at Object.Module._extensions..node (node:internal/modules/cjs/loader:1183:18)    at Module.load (node:internal/modules/cjs/loader:981:32)    at Function.Module._load (node:internal/modules/cjs/loader:822:12)    at Module.require (node:internal/modules/cjs/loader:1005:19)    at require (node:internal/modules/cjs/helpers:102:18)    at Object.<anonymous> (D:\NodeJS projects\tensorFlow\node_modules\@tensorflow\tfjs-node\dist\index.js:60:16)    at Module._compile (node:internal/modules/cjs/loader:1101:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)    at Module.load (node:internal/modules/cjs/loader:981:32)    at Function.Module._load (node:internal/modules/cjs/loader:822:12) {  code: 'ERR_DLOPEN_FAILED'},"['@Raptor5 please check a related issue here https://github.com/tensorflow/tfjs/issues/5354 ; you need to manually copy dlls ====='; '> @Raptor5 please check a related issue here #5354 ; you need to manually copy dllsI already did a rebuild and copied the dll and node file but still get this error. ====='; 'can you please share your package.json file ?====='; '> can you please share your package.json file ?Here it is{  ""name"": ""tftest"";  ""version"": ""1.0.0"";  ""description"": """";  ""main"": ""index.js"";  ""type"": ""module"";  ""scripts"": {    ""test"": ""echo \\""Error: no test specified\\"" && exit 1""  };  ""author"": """";  ""license"": ""ISC"";  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^3.12.0""  }}====='; 'what version of node version you are using ?====='; '> what version of node version you are using ?16.13.1 LTS. Also tried 15th and 14th versions====='; ""Using dependency walker figured out that tensorflow.dll uses other DLLs starting with api-ms-win-core...Those files are absent and were removed after Windows update. How come? Digging further found this:> Those DLLs are Windows's implementation detail and are subject to change at anytime. Basically Microsoft started moving around APIs starting from Windows 8. For example; APIs in Windows 8's api-ms-win-core-file-l1-2-0.dll got moved to api-ms-win-core-file-l1-2-1.dll in Windows 8.1. Old software still work because the dlls developer linked to are now just placeholders redirecting calls to the actual implementation. But nobody should link to the implementations directly; as that would be defeating the purpose of having such a redirection. Any software that report those dlls are missing are failing to accommodate the redirection (e.g. reporting a delay-loaded dependency as hard one). > Don't try to obtain those dlls. The only supported way to get those DLLs is to do a major Windows upgrade (e.g. from Windows 7 to Windows 10). Distributing those files is against Windows end user agreement; and those files you get from a higher version of Windows won't work if your Windows version is too low.=====""]",Initialization Faliure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] DLL Initialization Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/5937,[tjfs-node] Unsupported system: cpu-linux-arm64 in raspberry pi 4,7,open,2021-12-13T11:06:42Z,2021-12-17T07:42:49Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster) (raspberry pi 4B)- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.12.0 Installing tfjs-node; raspberry pi gave `Unsupported system: cpu-linux-arm64`.I used node.js v17.2.0I ran `npm install @tensorflow/tfjs-node` and this gave the error.This is the full error: ```npm ERR! code 1npm ERR! path /home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-nodenpm ERR! command failednpm ERR! command sh -c node scripts/install.jsnpm ERR! CPU-linux-3.12.0.tar.gznpm ERR! * Downloading libtensorflownpm ERR! /home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:106npm ERR!     throw new Error(`Unsupported system: ${libType}-${platform}-${os.arch()}`);npm ERR!           ^npm ERR! npm ERR! Error: Unsupported system: cpu-linux-arm64npm ERR!     at getPlatformLibtensorflowUri (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:106:11)npm ERR!     at downloadLibtensorflow (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:139:15)npm ERR!     at async run (/home/pi/Desktop/coronapredict/node_modules/@tensorflow/tfjs-node/scripts/install.js:208:5)npm ERR! npm ERR! Node.js v17.2.0```,"['Please check this issue #5173 ; this has been resolved with latest versions.====='; 'It was the latest version of tensorflow.====='; ""I tried with node.js v16 and v17; and they didn't work.=====""; '@Ddongddi can you please share your package.json ?====='; '```{  ""name"": ""coronapredict"";  ""version"": ""1.0.0"";  ""description"": """";  ""main"": ""index.js"";  ""scripts"": {    ""test"": ""echo \\""Error: no test specified\\"" && exit 1""  };  ""author"": """";  ""license"": ""ISC""}```My package.json file====='; '@Ddongddi we have support for arm v7 32 bit linux; is your RPI running on arm 64 linux?====='; '> @Ddongddi we have support for arm v7 32 bit linux; is your RPI running on arm 64 linux?Yes. I am running arm 64 debian=====']",Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unsupported System Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/5935,"tf-backend-wasm.node.js  occurred  ""memory out of bounds""",1,open,2021-12-10T17:50:41Z,2021-12-10T22:38:23Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  Linux Ubuntu 20.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): tfjs-backend-wasm  3.12.0- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**When running under the wasm backend many times; ""memory out of bounds"" will always appear.  At the beginning; the program can run normally; and after a short time; ""memory out of bounds"" appears all the time.  The strange thing is that when the shape of the tensor does not change; there is no such issue. **Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.```var tf= require(""@tensorflow/tfjs"");const wasm= require(""@tensorflow/tfjs-backend-wasm"");wasm.setWasmPaths('./node_modules/@tensorflow/tfjs-backend-wasm/dist/');async function fuzz() {        await tf.setBackend('wasm');        await tf.ready();        var random1=Math.floor(Math.random()*5+1);        var random2=Math.floor(Math.random()*35+1);        var random3=Math.floor(Math.random()*35+1);        var random4=Math.floor(Math.random()*35+1);        var input_array=new Array(random1*random2*random3*random4);        for (var i=0;i<input_array.length;i++){            input_array[i]=3;        }        try {            var input_tensor=await tf.tensor(input_array;[random1;random2;random3;random4];""float32"");            layer =  await tf.layers[""conv2d""]({filters:3;kernelSize:3;strides:1});            var prediction = await layer.apply(input_tensor);            tf.dispose(input_tensor);        }catch (e){           console.log(e);        }    }async function f(){    while (1){        await fuzz();    }}f();```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.",['my best-guess is that conv layer gets created with a kernel func for a specific shape and if shape is the same; it gets reused. but when shape changes; it creates new kernel func while previous one never got disposed.====='],Data & Model Error,Crash,Incorrect Code Logic,Wasm,Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.2] Out of Memory""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5932,tfjs-v3.12.0 and Wasm 3.12.0 for tfjs-models/pose-detection/ MoveNet multipose wasm,1,open,2021-12-09T17:03:33Z,2021-12-13T23:18:10Z,When I try to run the demo of the model : tfjs-models/pose-detection/ : https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenetWith tfjs-v3.12.0 and Wasm 3.12.0 and configuration model MoveNet type Multipose and backend Wasm i got the following error :Error: Kernel 'Reciprocal' not registered for backend 'wasm'![145433791-63df6bdb-87a1-4e08-a7cb-b72e41773c18](https://user-images.githubusercontent.com/32233417/145442149-e4f4c8e7-6c06-417b-92a3-8b591a4b64b1.png)The demo Woks well for the singlepose type (lightning).Regards,"[""Hi Na ; the demo needs to be updated to latest version https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet and v3.12.0 is complaining about missing op 'Reciprocal'=====""]",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5931,Bad advice in README for the TFJS version of the Universal Sentence Encoder,2,closed,2021-12-09T09:24:41Z,2021-12-11T02:34:00Z,Reading https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder the sample code needs to compute the dot product of two tensors and it uses arraySync on the tensors; nested for loops; and JavaScript defined vector operations. Wouldn’t it be better practice to use tf.layers.dot? The code would be much more concise and presumably much faster. (If it is the case the sample code predated tf.layers.dot then shouldn’t it be updated?),"['@pyu10055 Would you be able to confirm why it was done this way? I believe you were involved in this model?====='; ""@ToonTalk Thanks for reporting this; this should be computed on the gpu for efficiency.I used tf.matMul instead:```  const scores = tf.matMul(embeddings['queryEmbedding'];      embeddings['responseEmbedding']; false; true).dataSync();```=====""]",Regression,Poor Performance,Confused Document,Model API,model API,change API,Replace API with another effective one,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",B.3.1,B.4
https://github.com/tensorflow/tfjs/issues/5912,tfjs-react-native expo ERESOLVE unable to resolve dependency tree,2,closed,2021-12-02T02:23:23Z,2021-12-03T00:29:47Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):Only installing right now and following this guide [Here](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native) - OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS Big Sur- TensorFlow.js installed from (npm or script link): none yet- TensorFlow.js version (use command below): none yet- Browser version: chrome- Tensorflow.js Converter Version: none yetI have another issue [here](https://github.com/tensorflow/tfjs/issues/5897) and noticed it is probably my dependencies being the issue for the first one listed so I decided to start fresh. I reset everything and started installing all of the packages in order [here](https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native). When I get to `Install and configure react-native-fs` I get the following errors:![Screen Shot 2021-12-01 at 6 20 49 PM](https://user-images.githubusercontent.com/47552416/144345644-8c862ad3-2b56-4f19-84d6-e7c06cf98954.png)package.json:![Screen Shot 2021-12-01 at 6 22 09 PM](https://user-images.githubusercontent.com/47552416/144345798-d1f61d9e-2c1e-4017-8327-5aa12a79528a.png),"['Downgrading to npm 6 worked====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5912"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5912"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Mobile,Platform,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Dependency Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5900,tf.min on bool type tensor under wasm backend return wrong result.,1,closed,2021-11-26T06:58:41Z,2021-12-07T17:41:28Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): custom code- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOS 11.4- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ""@tensorflow/tfjs"": ""^3.11.0"";  ""@tensorflow/tfjs-backend-wasm"": ""^3.11.0"";- Browser version:""electron"": ""^13.0.0"";**Describe the current behavior**With wasm backend; the tf.min function on bool type tensor return wrong result.**Describe the expected behavior**The tf.min on bool type tensor  return false or 0.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.```jsimport * as tf from '@tensorflow/tfjs-core';import {setWasmPaths} from '@tensorflow/tfjs-backend-wasm';import wasmSimdPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-simd.wasm';import wasmSimdThreadedPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-threaded-simd.wasm';import wasmPath from '@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm.wasm';setWasmPaths({    'tfjs-backend-wasm.wasm': wasmPath;    'tfjs-backend-wasm-simd.wasm': wasmSimdPath;    'tfjs-backend-wasm-threaded-simd.wasm': wasmSimdThreadedPath });tf.setBackend('wasm').then(()=>console.log(tf.min(tf.tensor([true; false]; [2]; 'bool')).arraySync()))```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.The above code will print a non-zero number in console with a warn:![截屏2021-11-26 下午2 53 49](https://user-images.githubusercontent.com/30199027/143539324-71295895-4129-46d2-8823-10ba0d16bdeb.png)In the CPU backend; `console.log(tf.min(tf.tensor([true; false]; [2]; 'bool')).arraySync())` will print 0.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5900"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5900"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,add support for datatype,Add unsupported operator,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5869,tf.gather is needlessly slow; CPU-intensive,2,open,2021-11-18T23:24:23Z,2021-11-19T16:16:07Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 11 Home- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): [npm](https://unpkg.com/@tensorflow/tfjs@3.11.0/dist/tf.js)- TensorFlow.js version (use command below): 3.11.0- Browser version: Google Chrome 96.0.4664.45- Tensorflow.js Converter Version: ???**Describe the current behavior**`tf.gather` seems disproportionately slow; taking  33x the wall time of `tf.cumsum` with comparably sized input and output. When viewing performance data; runtime seems dominated by uploading and downloading textures; with much of the CPU time spent looping over indices to merely check that they are within range.It seems also that `tf.gatherND` does not suffer similar performance problems. So replacing ` tf.gather(x;ix)` with `tf.gatherND(x;ix.expandDims(1))` seems an appropriate workaround when gathering on the first axis.**Describe the expected behavior**I expect `tf.gather` to perform gathering on the GPU without synchronous operations. Any index checks (if necessary) should be done on GPU.**Standalone code to reproduce the issue**```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">    <meta name=""viewport"" content=""width=device-width; initial-scale=1.0"">    <title>Document</title></head><body>    <script crossorigin=""anonymous"" src=https://unpkg.com/@tensorflow/tfjs@3.11.0/dist/tf.js></script>    <script>        window.addEventListener(""load"";()=>{            const x = tf.range(0; 10)            const ix = tf.randomUniform([50000];0;10).cast('int32')            for (let i=0; i<1000; ++i){                tf.tidy(()=>{ tf.gather(x;ix);                tf.cumsum(ix); })            }        })    </script></body></html>```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.See screenshot and attached Chrome devtools profile.![image](https://user-images.githubusercontent.com/119948/142503841-13f871df-ed34-46eb-9d8a-b293589b896a.png)[Profile-20211118T172039.zip](https://github.com/tensorflow/tfjs/files/7566680/Profile-20211118T172039.zip)",['The slowness is caused by reading data back from gpu to cpu for out of bounds checking. We ever talked about it to skip this checking in production mode or do clamping in shader. @pyu10055====='; '@qjia7 thanks. that makes sense. I don’t understand your last sentence though.It would be nice to bring performance in line with `gatherND` by either eliminating bounds checking or doing it in the GPU kernel.====='],Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,remove unnecessary code,remove unnecessary code,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5866,tf.ready() throwing JSON Parse error with react-native 0.66.3,6,open,2021-11-18T03:35:56Z,2021-12-11T02:48:02Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.6- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iOS Simulator- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.11.0- CUDA/cuDNN version:**Describe the problem**After spinning up a fresh react-native app and installing tfjs [per the instructions](https://www.npmjs.com/package/@tensorflow/tfjs-react-native) I'm getting the following error:<img width=""300"" alt=""CleanShot 2021-11-17 at 20 24 13@2x"" src=""https://user-images.githubusercontent.com/53795920/142347037-29340711-81c6-4afd-b182-e2d2fdb2f893.png"">**Provide the exact sequence of commands / steps that you executed before running into the problem**1. Spin up a brand new react native app2.  Follow tfjs react native instructions**Any other info / logs**Minimal reproduction: https://github.com/kateinkim/tfjs-rn-example","['Sometimes we experience the same issue when just calling tf.ready() after starting react native app (iOS 15 device).@tensorflow/tfjs: 3.11.0react-native: 0.65.1====='; 'same for me - both android and iOS (real devices):```    ""@react-native-async-storage/async-storage"": ""^1.15.14"";    ""@tensorflow/tfjs"": ""^3.11.0"";    ""@tensorflow/tfjs-react-native"": ""^0.8.0"";    ""expo"": ""^43.0.3"";    ""expo-camera"": ""~12.0.3"";    ""expo-gl"": ""~11.0.3"";    ""expo-gl-cpp"": ""~11.0.1"";    ""react"": ""17.0.2"";    ""react-native"": ""0.66.3"";    ""react-native-fs"": ""^2.18.0"";```**solution**:downgrade react-native version to the [latest officially supported by expo](https://docs.expo.dev/versions/latest/#each-expo-sdk-version-depends-on-a) ( **0.64.3** for expo 43.0.3 )**extra steps due to react-native downgrade:****_iOS_**: Podfile -> comment this line if exists:``` # __apply_Xcode_12_5_M1_post_install_workaround(installer) ```**_Android_**: android/build.gradle``` allprojects {    repositories {        ...        // add next line. WARNING! jcenter will shutdown on February 2022        jcenter();    }}```[Example](https://github.com/raienko/holmes/pull/1/files)====='; '@tafsiri  or @lina128   any insight here?  This seems to be a blocker for our dev to make progress on a React Native component library.====='; 'Sorry for the delay.. I plan to take a look at this issue today or early next week. Thanks for the patience!====='; ""Hi folks;I took a closer look and it looks like the error happens right after the tfjs-react-native backend creates the GLView context ([here](https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/src/platform_react_native.ts#L171)). We actually found this problem before; and that's why we mentioned the working version combination [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native). I will move this paragraph to the main tfjs repo so people can easily see it. I also found a [similar issue](https://github.com/expo/expo/issues/15390) reported in the expo repo.  As @raienko mentioned; the solution is to downgrade. Sorry I don't have any better solutions for now. I will keep an eye on the expo repo and test the newer versions again when the problems on their side are fixed.@kateinkim just FYI that due to the usage of GLView (expo-gl); tfjs-react-native can only be tested on real devices; not simulators. Thanks!=====""; 'Thanks @jinjingforever! I tested it on a real device before but didn’t work so downgrade it is!=====']",Initialization Faliure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change react-native version,change react-native version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Tensor/Model Inaccessibility during Initialization""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.3
https://github.com/tensorflow/tfjs/issues/5863,[build] production code has a dependency on jasmine,1,closed,2021-11-17T15:20:54Z,2021-12-20T22:23:53Z,"`tfjs-core/src/base.ts` imports `test-util.ts` which is fine  but single line in `test-util.ts` causes entire `jasmine` to be a dependency of a production build!```export function expectArrayBuffersEqual(actual: ArrayBuffer; expected: ArrayBuffer) {  expect(new Float32Array(actual)).toEqual(new Float32Array(expected));}````except` is jasmine method; otherwise its undefined and results in> src/tfjs-core/src/test_util.ts:154:3 - error TS2552: Cannot find name 'expect'.if this line is removed; entire build works fine  use case: building updated typedefs for tfjs using> tsc --project ./typedefs.json```json{  ""compilerOptions"": {    ""target"": ""esnext"";    ""declaration"": true;    ""emitDeclarationOnly"": true;    ""outDir"": ""./types"";    ""lib"": [""esnext""; ""dom""];    ""strict"": false  };  ""files"": [""./src/tfjs-core/src/index.ts""]}```environment: tfjs 3.11.0","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5863"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5863"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,delete dependency,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Dependency Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5861,[webgl] shape mismatch between texture creation and data upload,1,closed,2021-11-17T04:48:31Z,2021-11-19T19:42:48Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Macosx 15- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): 3.11- TensorFlow.js version (use command below): 3.11- Browser version: latest- Tensorflow.js Converter Version: 3.11**Describe the current behavior**In WebGL uploadToGPU logic; the data is first uploaded to a texture in a dense format.But the texture is created using `createPackedMatrixTexture` method; which is half the size of the request width and height. This results a mismatch of the texture and upload size in the `uploadDenseMatrixToTexture`.This is somehow covered up by the fact that both texture creations are using `texImage2D` method; which creates a mutable texture that its shape/format can be changed later during the data upload.Given texImage2D manipulate mutable textures; it is not as efficient as texStorage2D + texSubImage2D; which will be able to catch the shape mismatch between creation and data upload.**Describe the expected behavior**match up the shape/format between texture creation and data upload.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5861"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5861"">No</a>=====']",Data & Model Error,Crash,Incorrect Code Logic,WebGL,Backend,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5860,Building tfjs-converter in a Google Colab environment,2,open,2021-11-16T21:39:33Z,2021-11-18T17:07:49Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Google Colab- TensorFlow.js installed from (npm or script link): pip- TensorFlow.js version: 3.11.0**Describe the problem**Having had my tfjs-converter issue very promptly solved (thank you!); I've tried to build tfjs-converter following [these instructions](https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/DEVELOPMENT.md). Here's the Colab I'm using:https://colab.research.google.com/drive/1bmMVsBE8pMwEspEeQCoCHg7gxELV7dHMAs you can see (the error logs are statically saved in that doc - no need to run it); it results in an error like this:```.........created virtual environment CPython2.7.17.final.0-64 in 1007ms  creator CPython2Posix(dest=/tmp/tmp.6SvzefkP81; clear=False; no_vcs_ignore=False; global=False)  seeder FromAppData(download=False; pip=bundle; setuptools=bundle; wheel=bundle; via=copy; app_data_dir=/root/.local/share/virtualenv)    added seed packages: pip==20.3.4; setuptools==44.1.1; wheel==0.37.0  activators BashActivator;CShellActivator;FishActivator;NushellActivator;PowerShellActivator;PythonActivatorLooking for wheel for python2: Python 2.7.17 ...The wheel should be build with 'bazel build python2_wheel python3_wheel' commandls: cannot access '../../dist/bin/tfjs-converter/python/*py2*.whl': No such file or directory```From that error message it seems like it has to do with Python versions; but Colab has binaries for python2 and python3 (in `/usr/bin`); so I don't see why that should be a problem.**Provide the exact sequence of commands / steps that you executed before running into the problem**See Colab link above.**Any other info / logs**N/A,"[""The `build_pip_package.sh` script is actually a misnomer; and should probably be renamed to `copy_pip_package.sh`. The pip package is actually first built with Bazel; with the command `bazel build python2_wheel python3_wheel`; but we exclude that command from `build_pip_package.sh` because in CI; we have already build the wheel with Bazel. @pyu10055 knows more about this build process.I gave this a try in the Colab; but I ran into a PyInquirer install error:```Collecting PyInquirer==1.0.3  Using cached PyInquirer-1.0.3.tar.gz (27 kB) (WARNING: Discarding https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz#sha256=c9a92d68d7727fbd886a7908c08fd9e9773e5dc211bf5cbf836ba90d366dee51 (from https://pypi.org/simple/pyinquirer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.ERROR: Could not find a version that satisfies the requirement PyInquirer==1.0.3ERROR: No matching distribution found for PyInquirer==1.0.3```Edit: [Here's a link to the colab](https://colab.research.google.com/drive/1QcpIC3EBAK68Ci1yGEEsdfT2pHMKc_1W#scrollTo=4dc0mnNuqSRs)=====""; ""@pyu10055 I added `pip install tensorflowjs` before the `bazel` command in mattsoulanille's notebook and got the same `PyInquirer` error. I may have misunderstood your instruction? The series of commands in the notebook are:```!wget https://github.com/bazelbuild/bazelisk/releases/download/v1.10.1/bazelisk-linux-amd64!chmod +x bazelisk-linux-amd64!mv bazelisk-linux-amd64 /usr/local/bin/bazel!git clone https://github.com/tensorflow/tfjs%cd tfjs/tfjs-converter/python!pip install tensorflowjs!bazel build python2_wheel python3_wheel!./build-pip-package.sh /content/my_tensorflowjs_pip```and it fails at the `bazel` command.=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,Layer API,API,change dependency,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Build Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5854,Facemesh/Face-Landmarks-Detection memory comparison,1,open,2021-11-16T02:28:35Z,2021-11-21T20:17:24Z,"I’ve been digging into finding a non-leaky way to use Face Landmarks Detections / facemesh.I’ve tested several different approaches all with demo code (no modifications) on chrome on osx using the webgl backend. Here is all the info about my testing incase it helps someone else. In each case; I ran the programs for 10 minutes and polled the JS heap size and GPU Process size (chrome task manager) every minute. ### Face Landmarks Detections [Demo](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection/demo) including the library via script tags/ unpkgVersions: ```tfjs-core@3.1.0tfjs-converter@3.1.0tfjs-backend-webgl@3.1.0tfjs-backend-cpu@3.1.0face-landmarks-detection@0.0.3```Result: ![Screen Shot 2021-11-15 at 7 11 14 PM](https://user-images.githubusercontent.com/3160465/141878431-88fd2fa8-a0e8-4f14-bc1d-5632917d24b0.png)Heap sizes`20.1;27.8;35.3;45.1;53.5;59.4;96.5;73.7;82.0;88.9`Delta- 68.8 mbGPU Process sizes `345;442;551;673;814;943;1000;1200;1300;1400`Delta- 1055 mbConclusion - significant memory problem. Has crashed chrome and caused the system to run out of memory frequently. ### Face Landmarks Detections [Demo](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection/demo) using NPM/Yarn/build tools in repoVersions: ```    face-landmarks-detection"": ""link:../dist"";    ""@tensorflow/tfjs-backend-wasm"": ""^3.1.0"";    ""@tensorflow/tfjs-backend-webgl"": ""^3.1.0"";    ""@tensorflow/tfjs-backend-cpu"": ""^3.1.0"";    ""@tensorflow/tfjs-converter"": ""^3.1.0"";    ""@tensorflow/tfjs-core"": ""^3.1.0"";```Result: ![Screen Shot 2021-11-15 at 6 54 16 PM](https://user-images.githubusercontent.com/3160465/141879236-f0d2bfac-0122-44c3-af0d-d87f436f1560.png)Heap sizes`17.3;18.9;18.7; 19.4;19.6;19.2;19.7;19.8;19.9`Delta- 2.6 mbGPU Process sizes `314;317;318;318;318;319;320;320;320`Delta- 6mbConclusion - still increases; but best option. Is it simply including via NPM or is it also about all the dev tools (babel; rollup; rollup; esbuild; yarn; etc ) that are being used?### Face Mesh [demo](https://codepen.io/mediapipe/pen/KKgVaPJ) run on a local server with linked / non-npm dependenciesVersions```camera_utils@0.3control_utils@0.6drawing_utils@0.3face_mesh@0.4```Result:![Screen Shot 2021-11-15 at 4 15 50 PM](https://user-images.githubusercontent.com/3160465/141879662-9ce94fa5-63f7-46f0-a1c6-0c6e70255286.png)Heap size`31.4;50.3;76.1;88.7;104;121;143;168;168;199`Delta - 167.6 mbGPU Process sizes `240 - 286 `Delta - 46 mb[More info on this issue](https://github.com/google/mediapipe/issues/1937#issuecomment-969348179)- also likely relevant thread about the details of webgl and memory. | demo+links heap delta | demo+links GPU delta | demo+npm heap delta | demo+npm GPU delta | facemesh heap delta | facemesh GPU delta ||-----------------------|----------------------|---------------------|--------------------|---------------------|--------------------|| 68.8 mb               | 1055 mb              | 2.6 mb              | 6 mb               | 167.6 mb            | 46   mb            |To my non expert eyes - using npm + the dev dependencies seems to be the secret sauce. Which of these dev dependencies are making the difference if any? Why is that approach so much better? ",['Hi there; Checking in about this. @mattsoulanille Any thoughts on why the memory management is so different with and without the dev dependencies? ====='],Memory Leak,Poor Performance,Dependency Error,WebGL,Backend,change dependency,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,B.2
https://github.com/tensorflow/tfjs/issues/5851,[wasm] Node tests fail with out of memory error,3,closed,2021-11-15T18:10:06Z,2021-11-16T19:01:13Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Debian 5.10.46- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): main branch (e34d7c3b748173f685c9ec3cf549709534a03a07)- TensorFlow.js version (use command below):- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**Running `yarn test-node` in tfjs-backend-wasm throws `RangeError: WebAssembly.instantiate(): Out of memory: wasm memory`.**Describe the expected behavior**Tests pass with no out-of-memory error.**Standalone code to reproduce the issue**In tfjs-backend-wasm; run `yarn && yarn build-deps && yarn build-npm && yarn test-node`. I've also observed that this bug only appears on some machines.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.This memory leak is likely in the setup / teardown of the backend that happens every time we use `describeWithFlags`. I created a fake test that just runs `describeWithFlags` 1000 times; and it consistently fails after running tests from 101 `describeWithFlags` calls.,"['This appears to be caused by the wasm-generated code subscribing to the `uncaughtException` event; which prevents GC across backend re-instantiations.====='; 'https://github.com/emscripten-core/emscripten/issues/12740====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5851"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5851"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,Wasm,Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/5847,Unsupported dtype in weight 'unknown_147': float16,1,closed,2021-11-15T00:00:56Z,2021-11-16T20:40:03Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu 20.04- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.js- TensorFlow.js version (use command below): 3.11.0- Browser version: Chrome 95- Tensorflow.js Converter Version: tensorflowjs 3.11.0; Dependency versions: keras 2.7.0;  tensorflow 2.7.0**Describe the current behavior**I'm trying to port OpenAI's CLIP model to tfjs; and I've used the following conversion command:```!tensorflowjs_converter --input_format tf_saved_model ./clip-text-vit-32-tf ./clip-text-vit-32-tfjs```[Here's the TF Saved Model](https://drive.google.com/drive/folders/1-7VMCP6OSpCLqBRu0qL7DW66Lk95yX8I?usp=sharing) that I'm trying to convert. When running the model with tfjs I get this error:![image](https://user-images.githubusercontent.com/1167575/141703862-615bb3dd-8266-4a9a-9281-60adc849b038.png)**Describe the expected behavior**If it's due to float16 not being supported; then I'd have expected `tensorflowjs_converter` to either automatically convert the float16 values to float32 like it seems to do with other unsupported datatypes; or throw an error telling me that this model cannot be converted into a tfjs model.**Standalone code to reproduce the issue**1. Download this HTML file: https://github.com/josephrocca/openai-clip-js/blob/main/tfjs-text-demo.html2. Download the tfjs model files from [this Google Drive link](https://drive.google.com/drive/folders/1-GI6-OTDiJcjYKTavoobbubc9BYjQDzW?usp=sharing) (or generate them using the above-linked Saved Model + the `tensorflowjs_converter` command). Ensure the folder is named `clip-text-vit-32-tfjs`.3. Put the tfjs model folder in the same directory as the HTML file and run a static file server (e.g. `deno run --allow-net --allow-read=. https://raw.githubusercontent.com/josephrocca/denoSimpleStatic/master/main.ts --port=3001`),"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5847"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5847"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,add type conversion,Replace data Shape/type,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5839,Cannot get WebGL context in Pop!_OS firefox,9,closed,2021-11-12T22:31:38Z,2021-11-15T20:08:34Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes; but not relating to this bug- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Pop!_OS 21.04- TensorFlow.js installed from (npm or script link): HTML Tag- TensorFlow.js version (use command below): tfjs 3.1.0- Browser version: Firefox 94.0**Describe the current behavior**I am getting the below error: Error when getting WebGL context:  Error: Cannot create a canvas in this context**Describe the expected behavior**tfjs should be able to get to get WebGL context. I am able to run use my website on chrome on the same machine with no issues.**Standalone code to reproduce the issue**Error arises during initialization; so just import tfjs via html tag.,"[""Unable to reproduce on PopOS 20.04 with Firefox v94:![image](https://user-images.githubusercontent.com/1167575/141646188-28c938bc-dd35-4e97-9bce-80d747d815f6.png)https://jsbin.com/wimogumana/edit?html;outputI'm not able to test PopOS 21.04 right now. Have you tried restarting your computer to see if the error persists? I sometimes get problems like this in Chrome (WebGL context lost); but restarting fixes it. I'm not sure whether it's a problem with tfjs or browser WebGL implementation; or nvidia linux drivers.Are you using a nvidia card? If so; and if the problem persists after restarting; try switching to integrated graphics if available (as a test to see if the bug is nvidia-driver-related).=====""; ""Modern Firefox disables WebGL by itself if it doesnt detect that GPU is adequate - so make sure you're running good display drivers as majority of Linux default ones are anything but ok  And do a quick test by navigating to <https://get.webgl.org/webgl2/> and <https://webglreport.com/?v=2>=====""; 'Thanks all; the tests that you have given are working for me; which indicates that webgl is actually working with firefox and that the issue is elsewhere.I\'m actually having the error when importing tfjs in a webworker. `importScripts(\'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js\');`It is the line above; in a webworker that I have having the issue. I assume when it says ""Error when getting WebGL context:  Error: Cannot create a canvas in this context""; it is referring to the webworker context? I am unsure. ====='; 'nothing to do with with **PopOs** or **TFJS**.by definition; **DOM** is not available in web workers (and **Canvas** is a DOM object)  alternative is `OffscreenCanvas` that TFJS is happy to use; but Firefox keeps that disabled  (I have no idea why would they still keep it disabled since it was added years ago)  navigate to `about:config` and enable `gfx.offscreencanvas.enabled`====='; ""I've enabled `gfx.offscreencanvas.enabled` but now firefox is crashing whenever I open the page.=====""; ""That's up to Firefox. Use some other browser if you want web workers to work. =====""; 'Alright then; thanks for your help. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5839"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5839"">No</a>====='; 'Alright then; thanks for your help. =====']",Initialization Faliure,Build & Initialization Failure,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Browser Context Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",C,D.2
https://github.com/tensorflow/tfjs/issues/5828,Ability to import tfjs-backend-wasm without bundler (like when importing `tf`),1,open,2021-11-09T23:41:00Z,2021-11-10T14:25:45Z,"**System information**- TensorFlow.js version (you are using): 3.11.0- Are you willing to contribute it (Yes/No): Depends on how hard this is to implement - I don't know my way around the tfjs codebase.**Describe the feature and the current behavior/state.**Currently we can load the wasm backend with script tags like this:```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.js""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.11.0/dist/tf-backend-wasm.js""></script>```I've tried doing something similar with the new JS module import syntax like this:```jsimport * as tf from ""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.fesm.js"";import ""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.11.0/dist/tf-backend-wasm.fesm.js"";```But this gives the following error related to `tf-backend-wasm.fesm.js`: ```Uncaught TypeError: Failed to resolve module specifier ""@tensorflow/tfjs-core"". Relative references must start with either ""/""; ""./""; or ""../"".```So importing `tf` works fine; but the wasm backend doesn't import properly. I think this is because it's expecting that I'd use a bundler? It would be nice if this worked without bundling. The script tag approach is nice and simple; and it would be nice if the wasm backend worked in a similarly simple way with the new module import syntax.","[""It's something jsDelivr will soon be able to support automatically and on the fly via our ESM functionality https://www.jsdelivr.com/esm=====""]",Initialization Faliure,Build & Initialization Failure,Untimely Update,Wasm,Backend,syntax modifier,syntax modifier,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Module Importing Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.3
https://github.com/tensorflow/tfjs/issues/5821,Unhandled Rejection (Error): Based on the provided shape; [4;4;64;128]; the tensor should have 131072 values but has 14636,1,closed,2021-11-07T06:13:00Z,2021-11-07T06:20:50Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS Big Sur- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.11.0- Browser version: Chrome 94.0.4606.81- Tensorflow.js Converter Version: **Describe the current behavior**```def Generator():    inputs = tf.keras.layers.Input(shape=[256;256;3])    down_stack = [        downsample(64; 4; apply_batchnorm=False); # (bs; 128; 128; 64)        downsample(128; 4); # (bs; 64; 64; 128)        downsample(256; 4); # (bs; 32; 32; 256)        downsample(512; 4); # (bs; 16; 16; 512)        downsample(512; 4); # (bs; 8; 8; 512)        downsample(512; 4); # (bs; 4; 4; 512)        downsample(512; 4); # (bs; 2; 2; 512)        downsample(512; 4); # (bs; 1; 1; 512)    ]    up_stack = [        upsample(512; 4; apply_dropout=True); # (bs; 2; 2; 1024)        upsample(512; 4; apply_dropout=True); # (bs; 4; 4; 1024)        upsample(512; 4; apply_dropout=True); # (bs; 8; 8; 1024)        upsample(512; 4); # (bs; 16; 16; 1024)        upsample(256; 4); # (bs; 32; 32; 512)        upsample(128; 4); # (bs; 64; 64; 256)        upsample(64; 4); # (bs; 128; 128; 128)    ]#    initializer = tf.random_normal_initializer(0.; 0.02)    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS; 4;                                         strides=2;                                         padding='same';                                         activation='tanh') # (bs; 256; 256; 3)    x = inputs    # Downsampling through the model    skips = []    for down in down_stack:        x = down(x)        skips.append(x)    skips = reversed(skips[:-1])    # Upsampling and establishing the skip connections    for up; skip in zip(up_stack; skips):        x = up(x)        x = tf.keras.layers.Concatenate()([x; skip])    x = last(x)    return tf.keras.Model(inputs=inputs; outputs=x)``````tfjs.converters.save_keras_model(generator; tfjs_target_dir)```exported the model in python; trying to load it in tensorflow js; but got an error: Unhandled Rejection (Error): Based on the provided shape; [4;4;64;128]; the tensor should have 131072 values but has 14636**Describe the expected behavior**Should be able to load the model.**Standalone code to reproduce the issue**https://colab.research.google.com/drive/1ivkK-P9VJyRmK_X4UBEAnKc6iRbYjnRR?usp=sharingin the javascript I'm simply loading the model like this:```import * as tf from ""@tensorflow/tfjs"";const MODEL_URL = ""/colorization-model.json"";const loadModel = async () => {  const model = await tf.loadLayersModel(MODEL_URL);};loadModel();``` **Other info / logs** Include any logs or source code that would be helpful toNo more info","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5821"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5821"">No</a>=====']",Data & Model Error,Crash,Unknown,Layer API,API,,,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",A.2,E
https://github.com/tensorflow/tfjs/issues/5808,tfjs_converter output format incorrect.,3,open,2021-11-03T16:59:47Z,2021-11-04T02:27:47Z,Hello i'm working with `tfjs_converter` on a saved model via the following command`tensorflowjs_converter --input_format=tf_saved_model --signature_name=serving_default --saved_model_tags=serve ./saved_model ./tfjs_model`This seems to convert my model fine however there is some oddity in the outputs when looking at the output tensors from the saved model.json.If I run the following command I get the following output tensors from the original saved model:```saved_model_cli show --dir ./saved_model --all``````MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:signature_def['__saved_model_init_op']:  The given SavedModel SignatureDef contains the following input(s):  The given SavedModel SignatureDef contains the following output(s):    outputs['__saved_model_init_op'] tensor_info:        dtype: DT_INVALID        shape: unknown_rank        name: NoOp  Method name is:signature_def['serving_default']:  The given SavedModel SignatureDef contains the following input(s):    inputs['input_tensor'] tensor_info:        dtype: DT_UINT8        shape: (1; -1; -1; 3)        name: serving_default_input_tensor:0  The given SavedModel SignatureDef contains the following output(s):    outputs['detection_anchor_indices'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:0    outputs['detection_boxes'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100; 4)        name: StatefulPartitionedCall:1    outputs['detection_classes'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:2    outputs['detection_multiclass_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100; 3)        name: StatefulPartitionedCall:3    outputs['detection_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:4    outputs['num_detections'] tensor_info:        dtype: DT_FLOAT        shape: (1)        name: StatefulPartitionedCall:5    outputs['raw_detection_boxes'] tensor_info:        dtype: DT_FLOAT        shape: (1; 51150; 4)        name: StatefulPartitionedCall:6    outputs['raw_detection_scores'] tensor_info:        dtype: DT_FLOAT        shape: (1; 51150; 3)        name: StatefulPartitionedCall:7  Method name is: tensorflow/serving/predict```Now using the converted tfjs model in node; I run the following command to get the outputNodes with the following output.```let model = await tf.loadGraphModel(handler);console.log(model.outputNodes)``````[  'detection_multiclass_scores';  'raw_detection_boxes';  'Identity:0';  'Identity_2:0';  'detection_boxes';  'raw_detection_scores';  'num_detections';  'Identity_4:0']```You can see that some of the output tensors didn't carry the names from the save model; they are also not ordered as the original saved model. Why do the output tensors: `'Identity:0'; 'Identity_2:0'; 'Identity_4:0'` not have the same names as  `detection_anchor_indices; detection_scores; detection_classes`. It's difficult to determine the output. This seems like a bug in the converter. Any insight on this issue is appreciated.,"['mapping output node names is a best effort and only added in recent versions; previously it was all just `identity_n`  but for mapping to work; tensors must have unique shape so they can be determined without errors. if two tensors have the same shape; there is no chance to know which one is which auto-magically  you can see that `detection_anchor_indices` and `detection_classes` have the same shape `[1; 100]`its just how converter works; not a bug as such  and order of tensors is not something to ever rely on. that is not even a limitation.====='; '@vladmandic thanks for that explanation that helps. Is there any way to prune or remove output tensors that may not be needed during the conversion using `tensorflowjs_converter` or a way to name or rename the outputs tensors or manually map them during the conversion?The problem is that when using these outputs in `tfjs` knowing which output to use is important; if we have some arbitrary output names it makes interpreting the output difficult; unless I can rely on `Identity_2:0` always mapping to `detection_anchor_indices`.====='; ""A) yes; you can rely on it since it's now defined in model.jsonB) you can run model.json through a prettyfier and then edit signature part to write down anything you want.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Operator,API,change model,Changing model,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5800,When in webgl backend;  tf.isNaN() produces incorrect results,8,open,2021-11-01T08:23:59Z,2021-11-29T19:25:45Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 3.11.0- Browser version:   Google Chrome  93.0.4577.63- Tensorflow.js Converter Version:- **Describe the current behavior**tf.isNaN() has different results when running on different backends. For example; for the following code:```     const y = tf.tensor1d([Infinity; .5; 4; -38.8; NaN]);     y.isNaN().print();```In **webgl backend**; the output is ```    Tensor    [false; false; false; false; false]```In **cpu backend**; the output is ```    Tensor    [false; false; false; false; true]```In **wasm backend**; get the error is ```Uncaught (in promise) Error: Kernel 'IsNan' not registered for backend 'wasm'```**These results indicate that tf.isNaN()cannot correctly recognize NaN in webgl backend.****Describe the expected behavior**According to the official documentation;tf.isNaN(x) RReturns which elements of x are NaN.Therefore; the correct result should be```    Tensor    [false; false; false; false; true]```**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.the code to reproduce the problem is```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>Title</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""> </script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl""></script>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js""></script></head><body><script> async function nan_test() {     const y = tf.tensor1d([Infinity; .5; 4; -38.8; NaN]);       tf.setBackend(""webgl"");     await tf.ready();     console.log(""webgl backend result:"");     y.isNaN().print();     tf.setBackend(""cpu"");     await tf.ready();     console.log(""cpu backend result:"");     y.isNaN().print();     tf.setBackend(""wasm"");     await tf.ready();     console.log(""wasm backend result:"");     y.isNaN().print();    }    nan_test();</script></body></html>```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attachedthe code to reproduce the problem[nan_test.zip](https://github.com/tensorflow/tfjs/files/7451685/nan_test.zip).","['I tried in lates chrome browser(MAC OS) and was not able to see above behavior ![image](https://user-images.githubusercontent.com/43972606/139707905-c6f8ae3c-60c1-4bfe-b35a-073a58afa264.png)====='; ""@rthadur Thank you for your prompt reply.  I did the test again; and still got incorrect results on **Ubuntu** (ThinkPad and Dell).  I also tested it on Mac OS and Windows; and I really can't see this behavior. This is strange; what is the reason?The output of chrome on Ubuntu is as follows:![image](https://user-images.githubusercontent.com/68681463/139823893-6d9d3d10-15a5-419c-8ca6-a836f29003fd.png)The same error appears on Tensorflow.js API web page (https://js.tensorflow.org/api/latest/?hl=zh-cn#isNaN)![image](https://user-images.githubusercontent.com/68681463/139839444-55aca7ac-ea51-420d-a05e-bd121499e1fb.png)=====""; 'Can you try run this https://js.tensorflow.org/debug/ and provide screenshot ?====='; 'This is screenshot:![image](https://user-images.githubusercontent.com/68681463/139906826-4084707e-32d6-4ecf-b0ae-3c50b8c852bc.png)====='; 'can you please upgrade to Chrome latest and try ?====='; 'I have updated Chrome to version 95.0.4638.69 and the error still exists. In addition; there is the same error on firefox. result on chrome:![image](https://user-images.githubusercontent.com/68681463/139910674-87055484-340c-4ee3-a5d4-6d693b4debd0.png)result on Firefox:![image](https://user-images.githubusercontent.com/68681463/139910363-8ab8ebc6-b30e-4957-96cb-48250ce313b7.png)The debug information is as follows![Selection_139](https://user-images.githubusercontent.com/68681463/139910003-bfa858c6-1076-44db-b814-7848697c9016.png)====='; '![image](https://user-images.githubusercontent.com/68681463/143088806-027cedbc-eea4-4c5d-b58a-86d1bb1eab8c.png)Thank you for fixing this issue; but I have some doubts. First ; I feel that the current changes cannot explain the incorrect results under Linux. In your fix;  delete ` return (val > 0.0 || val < 0.0) ? false : val != 0.0;` and add `return val != val;` . However; in Linux; when ` val=NaN`; the result of `(val > 0.0 || val < 0.0) ? false : val != 0.0` a is TRUE; as expected . So is this issue caused by the insufficient webgl support under Linux? as stated by ahmedsabie “Modify the implementation to also support UHD Graphics on Linux” .  Second; I would like to ask if this fix will be updated to the future version？ Thanks again.====='; ""The issue is caused by WebGL being optimized and supported slightly differently across different GPUs so behavior isn't always consistent. The issue is more of an Intel UHD Graphics issue rather than a Linux one. Yes the fix will be added to the next release once merged!  =====""]",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,behavior logic,behavior logic,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,D.1
https://github.com/tensorflow/tfjs/issues/5797,@tensorflow/tfjs-node installation failed in windows system,4,open,2021-10-30T13:21:35Z,2021-12-23T18:22:26Z,"I am using node version v10.24.1; Win 10 OS; VSCode IDE 1.61.2.  I have installed C/C++ extension to VSCode but still when I try to install @tensorflow/tfjs-node npm package it throws me error.node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.10.0/CPU-windows-3.10.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.10.0 and node@10.24.1 (node-v64 ABI; unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.10.0/CPU-windows-3.10.0.zipgyp ERR! find VSgyp ERR! find VS msvs_version not set from command line or npm configgyp ERR! find VS VCINSTALLDIR not set; not running in VS Command Promptgyp ERR! find VS checking VS2019 (16.11.31729.503) found at:gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""gyp ERR! find VS - ""Visual Studio C++ core features"" missinggyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usegyp ERR! find VS looking for Visual Studio 2015gyp ERR! find VS - not foundgyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8gyp ERR! find VSgyp ERR! find VS **************************************************************gyp ERR! find VS You need to install the latest version of Visual Studiogyp ERR! find VS including the ""Desktop development with C++"" workload.gyp ERR! find VS For more information consult the documentation at:gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsgyp ERR! find VS **************************************************************gyp ERR! find VSgyp ERR! configure errorgyp ERR! stack Error: Could not find any Visual Studio installation to usegyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)gyp ERR! stack     at findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16)gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)gyp ERR! stack     at findVisualStudio2015 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14)gyp ERR! stack     at regSearchKeys (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16)gyp ERR! stack     at regGetValue (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7)gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16gyp ERR! stack     at ChildProcess.exithandler (child_process.js:301:5)gyp ERR! stack     at ChildProcess.emit (events.js:198:13)gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)gyp ERR! System Windows_NT 10.0.10240gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=E:\\learning\\ReactJS\\nextapp\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v7\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=E:\\learning\\ReactJS\\nextapp\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v7"" ""--napi_version=7"" ""--node_abi_napi=napi"" ""--napi_build_version=7"" ""--node_napi_label=napi-v7""gyp ERR! cwd E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v10.24.1gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node --module_name=tfjs_binding --module_path=E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-node\lib\napi-v7 --napi_version=7 --node_abi_napi=napi --napi_build_version=7 --node_napi_label=napi-v7' (1)node-pre-gyp ERR! stack     at ChildProcess.cmd.on (E:\learning\ReactJS\nextapp\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:198:13)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:982:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:259:5)node-pre-gyp ERR! System Windows_NT 10.0.10240node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""E:\\learning\\ReactJS\\nextapp\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd E:\learning\ReactJS\nextapp\node_modules\@tensorflow\tfjs-nodenode-pre-gyp ERR! node -v v10.24.1node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not oknpm WARN @babel/plugin-syntax-jsx@7.14.5 requires a peer of @babel/core@^7.0.0-0 but none is installed. You must install peer dependencies yourself.npm WARN @typescript-eslint/parser@4.33.0 requires a peer of eslint@^5.0.0 || ^6.0.0 || ^7.0.0 but none is installed. You must install peer dependencies yourself.npm WARN eslint-config-next@11.1.2 requires a peer of eslint@^7.23.0 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-jsx-a11y@6.4.1 requires a peer of eslint@^3 || ^4 || ^5 || ^6 || ^7 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-react@7.26.1 requires a peer of eslint@^3 || ^4 || ^5 || ^6 || ^7 but none is installed. You must install peer dependencies yourself.npm WARN eslint-plugin-react-hooks@4.2.0 requires a peer of eslint@^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 but none is installed. You must install peer dependencies yourself.npm WARN tsutils@3.21.0 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-darwin-x64@11.1.2 (node_modules\@next\swc-darwin-x64):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-darwin-x64@11.1.2: wanted {""os"":""darwin"";""arch"":""x64""} (current: {""os"":""win32"";""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-darwin-arm64@11.1.2 (node_modules\@next\swc-darwin-arm64):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-darwin-arm64@11.1.2: wanted {""os"":""darwin"";""arch"":""arm64""} (current: {""os"":""win32"";""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: @next/swc-linux-x64-gnu@11.1.2 (node_modules\@next\swc-linux-x64-gnu):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for @next/swc-linux-x64-gnu@11.1.2: wanted {""os"":""linux"";""arch"":""x64""} (current: {""os"":""win32"";""arch"":""x64""})npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""win32"";""arch"":""x64""})npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! @tensorflow/tfjs-node@3.10.0 install: `node scripts/install.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @tensorflow/tfjs-node@3.10.0 install script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.","['Can you please try below commands : ```sudo apt-get updatesudo apt-get install build-essentialnpm i node-pre-gyp -g(if your using tfjs-node-gpu)npm rebuild @tensorflow/tfjs-node-gpu --build-from-source(or if your using tfjs-node instead)npm rebuild @tensorflow/tfjs-node --build-from-source```====='; 'Thanks for reply. sudo commands not working; I think it is linux based command I am windows user. and second thing I am not able to run ""npm install @tensorflow/tfjs-node"" command I throws error above. I am windows user (windows 10)====='; ""I've had a similar issue so hopefully this will work for you.First check these guidelines on [how to set up](https://github.com/Microsoft/nodejs-guidelines/blob/master/windows-environment.md#environment-setup-and-configuration) your computer for node;js  You need Visual Studio (msvs) 2017 or 2019 since I've personally ran into some issues with the 2022 and 2015 versions.Option 1 seems to not be working for a lot of people; so just download Visual Studio Community 2017 or 2019 from option 2 and make sure you have python 2.7 installed on your computer.Then you either configure your npm to run said version of msvs`npm config set msvs_version 2019`Or you just force the node installation with it`npm install @tensorflow/tfjs-node --msvs_version=2019`=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,,,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Pre-built binaries not installable""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5777,Bazel build for type declaration problem,3,closed,2021-10-27T14:48:48Z,2021-11-08T02:29:50Z,"When I make WebGPU bazel build according to https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md; I met a `error TS2688: Cannot find type definition file for '@webgpu/types'`. To reproduce this error simply; I made a sample code on tfjs-core; add 3 lines in 3 files as below:```diff --git a/package.json b/package.jsonindex cb3368b05..0fbaf1605 100644--- a/package.json+++ b/package.json@@ -22;6 +22;7 @@     ""@types/shelljs"": ""^0.8.7"";     ""@types/webgl-ext"": ""0.0.30"";     ""@types/webgl2"": ""0.0.6"";+    ""@webgpu/types"": ""0.1.6"";     ""ajv"": ""~6.3.0"";     ""argparse"": ""^1.0.10"";     ""chalk"": ""~2.4.2"";diff --git a/tfjs-core/src/BUILD.bazel b/tfjs-core/src/BUILD.bazelindex 71a4a9fdb..2f12c0169 100644--- a/tfjs-core/src/BUILD.bazel+++ b/tfjs-core/src/BUILD.bazel@@ -64;6 +64;7 @@ ts_library(     deps = [         ""@npm//@types"";         ""@npm//seedrandom"";+       ""@npm//@webgpu/types"";     ]; ) diff --git a/tfjs-core/src/base.ts b/tfjs-core/src/base.tsindex a36679efc..6122f34df 100644--- a/tfjs-core/src/base.ts+++ b/tfjs-core/src/base.ts@@ -29;6 +29;7 @@  */  // Serialization.+/// <reference types=""@webgpu/types"" /> import * as io from './io/io'; import * as math from './math'; import * as browser from './ops/browser';```Then `yarn build` command will report an error: ```yarn run v1.22.5$ bazel build :tfjs-core_pkgINFO: Analyzed target //tfjs-core:tfjs-core_pkg (1 packages loaded; 43 targets configured).INFO: Found 1 target...ERROR: /home/yunfei/00_yunfei/workspace/github/tfjs/tfjs-core/src/BUILD.bazel:57:11: Compiling TypeScript (prodmode) //tfjs-core/src:tfjs-core_src_lib failed: (Exit 1): tsc_wrapped.sh failed: error executing command bazel-out/host/bin/external/npm/@bazel/typescript/bin/tsc_wrapped.sh @@bazel-out/k8-fastbuild/bin/tfjs-core/src/tfjs-core_src_lib_tsconfig.jsontfjs-core/src/base.ts:32:23 - error TS2688: Cannot find type definition file for '@webgpu/types'.```I also tried use `import ""@webgpu/types""` instead of `/// <reference types=""@webgpu/types"" />`; but that will cause `yarn test-dev` failure.","[""Hi @haoyunfeix. `@webgpu/types` declares additional types for an already existing object (`navigator.gpu` etc). For that reason; I think we should include them the same way we include types from `@types`. That means adding them to the [`typeRoots` tsconfig option](https://www.typescriptlang.org/tsconfig#typeRoots).I've tried this out on your PR (#5362); and it works for me on Windows. [Here's my branch](https://github.com/mattsoulanille/tfjs/tree/bazel-webgpu) (I had to rebase off of master first to get it working on Windows). Do these changes also work for you?=====""; 'Yes; that fixes my problem. Thank you; @mattsoulanille ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5777"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5777"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGPU,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Type Definition Not Found""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5775,tsjs-node: sh: line 1: 18590 Illegal instruction: 4,7,closed,2021-10-27T14:14:49Z,2021-10-28T22:35:47Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOS 11.6 (Big Sur)- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.10.0- CUDA/cuDNN version: N/A**Describe the problem**Getting error `sh: line 1: 18590 Illegal instruction: 4` when trying to use tfjs-node. Works with tfjs (non-node version).**Provide the exact sequence of commands / steps that you executed before running into the problem**Just initializing instance using `const tf = require('@tensorflow/tfjs-node')`.**Any other info / logs**N/A.,"['Please provide codepen example or code snippet for us to reproduce. ====='; ""All I did was import tfjs-node `const tf = require('@tensorflow/tfjs-node')`=====""; ""my guess this is on Apple's M1 hardware? if so; `tensorflow` shared library does not support it (and thus neither can `tfjs-node`). search in older posts for alternatives - none great; but there are some ways to make it work.=====""; 'Yes; this is M1; when will this be supported? Mind looking to one of the older posts?====='; 'Thats a question for underlying `tensorflow` shared library; not `tfjs`  See <https://github.com/tensorflow/tensorflow/issues?q=M1>  ====='; 'Thank you @vladmandic ; we will track this request at one place here https://github.com/tensorflow/tfjs/issues/4514#issuecomment-953889743 ; will close this issue. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5775"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5775"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Device Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/5766,Tensorflow.JS version 3.10.0 is exceeding 100MB breaking src loads.,8,closed,2021-10-24T23:59:16Z,2021-10-25T21:14:55Z,When using:https://cdn.jsdelivr.net/npm/@tensorflow/tfjsBrowser shows: Package size exceeded the configured limit of 100 MB.Workaround:Using version 3.9.0 as link below.https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.jsHope it should be good break the build if the size exceeds 100Mbytes as it seems to be the default for chrome browsers.,"['Version 3.9.0 seems to have been only about 1.2MB. Has the library really become 80x larger?====='; '> Version 3.9.0 seems to have been only about 1.2MB. Has the library really become 80x larger?Odd! Checked also and it was only 1.2MB.  ====='; ""Hi; Martin from jsDelivr here.This is a per-package limit and it went from just below 100 MB to 135 MB in the latest release. We can remove the limit for this package to get it working again; just note that it's highly recommended to stay within 200 MB for best performance.=====""; 'Oh; is _the thing that must be at most 100MB_ not the same thing as _the thing that actually gets served up when you request `/npm/@tensorflow/tfjs`_? The latter is definitely only about 1.2MB for 3.9.0; not just below 100MB; so I guess they must be different things. In which case; my apologies for asking an ill-informed question.====='; 'We removed the package limit so the link works now.====='; 'Thanks a lot!!! Working good now!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5766"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5766"">No</a>====='; ""Thank you @MartinKolarik for increasing the package size limit.It looks like most of the size increase is in the `.map` files. I'm not completely sure what has caused this; but I'd guess it's related to the Bazel refactor. In the Bazel refactor; sourcemaps are inlined in the output `.js` files of `tsc` instead of stored in separate files. These `.js` files are then passed to Rollup for bundling; and Rollup probably includes the inlined sourcemaps in `sourcesContent`. This doesn't affect the bundle size; but it does increase the size of the sourcemaps Rollup generates.## @tensorflow/tfjs@3.10.0```130M\t@tensorflow/tfjs/dist130M\t@tensorflow/tfjs16M\t@tensorflow/tfjs/dist/tf.js.map16M\t@tensorflow/tfjs/dist/tf.fesm.js.map16M\t@tensorflow/tfjs/dist/tf.es2017.js.map16M\t@tensorflow/tfjs/dist/miniprogram15M\t@tensorflow/tfjs/dist/tf.min.js.map15M\t@tensorflow/tfjs/dist/miniprogram/index.js.map14M\t@tensorflow/tfjs/dist/tf.fesm.min.js.map14M\t@tensorflow/tfjs/dist/tf.es2017.min.js.map4.4M\t@tensorflow/tfjs/dist/tsconfig.tsbuildinfo4.3M\t@tensorflow/tfjs/dist/tf.node.js.map4.2M\t@tensorflow/tfjs/dist/tf.js3.9M\t@tensorflow/tfjs/dist/tf.es2017.js3.6M\t@tensorflow/tfjs/dist/tf.fesm.js1.2M\t@tensorflow/tfjs/dist/tf.min.js1.2M\t@tensorflow/tfjs/dist/miniprogram/index.js996K\t@tensorflow/tfjs/dist/tf.fesm.min.js988K\t@tensorflow/tfjs/dist/tf.node.js988K\t@tensorflow/tfjs/dist/tf.es2017.min.js700K\t@tensorflow/tfjs/dist/tf.min.js.html452K\t@tensorflow/tfjs/dist/tf.fesm.min.js.html452K\t@tensorflow/tfjs/dist/tf.es2017.min.js.html92K\t@tensorflow/tfjs/dist/tools88K\t@tensorflow/tfjs/dist/tools/custom_module80K\t@tensorflow/tfjs/tools76K\t@tensorflow/tfjs/tools/custom_module16K\t@tensorflow/tfjs/src12K\t@tensorflow/tfjs/tools/custom_module/model_parser_test.ts12K\t@tensorflow/tfjs/tools/custom_module/custom_module_test.ts```## @tensorflow/tfjs@3.9.0```97M\t@tensorflow/tfjs/dist97M\t@tensorflow/tfjs11M\t@tensorflow/tfjs/dist/tf.js.map11M\t@tensorflow/tfjs/dist/tf.fesm.js.map11M\t@tensorflow/tfjs/dist/tf.es2017.js.map11M\t@tensorflow/tfjs/dist/miniprogram9.7M\t@tensorflow/tfjs/dist/tf.min.js.map9.7M\t@tensorflow/tfjs/dist/miniprogram/index.js.map9.2M\t@tensorflow/tfjs/dist/tf.fesm.min.js.map9.2M\t@tensorflow/tfjs/dist/tf.es2017.min.js.map4.4M\t@tensorflow/tfjs/dist/tsconfig.tsbuildinfo4.3M\t@tensorflow/tfjs/dist/tf.node.js.map4.2M\t@tensorflow/tfjs/dist/tf.js3.9M\t@tensorflow/tfjs/dist/tf.es2017.js3.5M\t@tensorflow/tfjs/dist/tf.fesm.js1.2M\t@tensorflow/tfjs/dist/tf.min.js1.2M\t@tensorflow/tfjs/dist/miniprogram/index.js992K\t@tensorflow/tfjs/dist/tf.fesm.min.js988K\t@tensorflow/tfjs/dist/tf.node.js988K\t@tensorflow/tfjs/dist/tf.es2017.min.js700K\t@tensorflow/tfjs/dist/tf.min.js.html452K\t@tensorflow/tfjs/dist/tf.fesm.min.js.html452K\t@tensorflow/tfjs/dist/tf.es2017.min.js.html92K\t@tensorflow/tfjs/dist/tools88K\t@tensorflow/tfjs/dist/tools/custom_module80K\t@tensorflow/tfjs/tools76K\t@tensorflow/tfjs/tools/custom_module16K\t@tensorflow/tfjs/src12K\t@tensorflow/tfjs/tools/custom_module/model_parser_test.ts12K\t@tensorflow/tfjs/tools/custom_module/custom_module_test.ts```=====""]",Initialization Faliure,Build & Initialization Failure,Dependency Error,WebGL,Backend,change framework version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5765,Can a graph model loaded with tensorflow.js use data on GPU without transferring it to the CPU first?,9,open,2021-10-23T02:00:45Z,2021-12-20T22:28:07Z,Hi; I am currently using tfjs 3.8 to load a segmentation model (loaded as a tf.GraphModel) on the client side. To create the input Tensor; I call browser.fromPixels(imageData); which creates the Tensor on CPU from the ImageData object that is also on CPU. Since I'm using tfjs' webgl backend; the data is sent to the GPU when calling the model.predict(tensor) function. All of this works well; excepted that my ImageData object is created from an image on a canvas with a WebGLRenderingContext; meaning it comes from the GPU. This GPU->CPU->GPU data transfer is slowing down my process; which I am trying to optimize.I briefly searched tfjs and could not find a way to create a Tensor on GPU to prevent the GPU->CPU data transfer. Is there a way I could keep my data on the GPU?,"['if you call `browser.fromPixels(canvas)` and it can get `gl` context on it; it will use small GSLS program inside `fromPixels` implementation for `webgl`; so its GPU -> GPU. any other variation; it has to be prepared on CPU first.====='; 'Thanks a lot! I will try that :)====='; ""Very cool. Is there any TFJS documentation or examples for the fast GPU <--> GPU transfer paths?Is there a path that let's me access a texture ID from a Tensor. I.e. to take the result from; say predict() and continue other GPU processing without transferring to the CPU?=====""; ""@danwexler `fromPixels()` is an exception to otherwise rule that tensors start in heap space and then get created whatever the *backend* is in use - as only with `canvas` with `gl` context its actually possible to read data from canvas using *gsls*   but once tensor is in gpu; for majority of operations it stays there without transfers back and forth unless:- operation you're attempting doesn't have a native `gsls` implementation and instead uses cpu fallback    (afaik; there are no docs which ops use fallback implementation and there is no way to tell without looking at sources)    (afaik again; none of the core ops allow for fallback - you'd get op not implemented error; its primarily some auxiliary ops like `nonMaxSuppression`)- operation is deemed tiny and its faster to perform it on cpu than use shaders  (controllable via tf env variable; default threshold is 128 elements)  one sure way to break this pipeline it is to download tensor (`.data()` or `.array()` methods); do something with that data and then create new tensor from it. thats why i try to get as much done via tf ops until very last moment (even if its sometimes tedious) to download data.=====""; ""@vladmandic  Understood. In addition; I want to get the GPU texture ID for the final tensor so that I can; say; pass it to another library that performs image processing using WebGL shaders. FWIW; I'm also investigating[ implementing traditional image processing as TF models](https://discuss.tensorflow.org/t/tf-image-processing/5166); but if I can just get the texture ID and I implicitly know the format; it would make integration with many other GPU processing tools much easier.=====""; ""@danwexler only starting point i can think of is `tensor.id`; so where does it lead?  took a quick look; it seems that **tfjs** uses `weakMap` to map `tensor.id` to instance of  `DataStorage: {texture; dtype; texShape; usage; isPacked; slice}`  where `texture` is an actual instance of a `WebGLTexture`  and instance of `DataStorage` is private to each backend instance as `this.texData`  so you can actually read `tf.engine().backendInstance.texData.data` and remap it with `tensor.id`  and you have your `WebGLTexture`  btw; why a `weakMap` and not have those properties directly under `tensor`?  **tfjs** maintains its own ref counters for memory management and caches textures even when tensor is released  alternatively; leave texture management to **tfjs** and implement your `glsl` code as a custom tf operation using `tf.registerKernel({ kernelName; backendName; kernelFunc });`i'm curious what do you do with it - keep me posted?  =====""; '@vladmandic  Great info. Will keep you posted! ====='; ""Hi @vladmandic; I did try passing a `canvas` with a `webgl` context to the `browser.fromPixels` function and it looks like it effectively uses only the GPU to create the tensor. So thanks a lot for that!Now I'm trying to keep the model output on the GPU. I saw that `browser.toPixels` had a `canvas` parameter but it looks like it's used only for drawing on the `2d` context of the canvas with [the data copied on CPU](https://github.com/tensorflow/tfjs-core/blob/master/tfjs-core/src/ops/browser.ts#L97).I see the info you gave about the `texture` and `texData`; I will try using it but I fear this is a little much for someone without lots of experience with webgl like me.=====""; 'crosslinking feature request #5918 =====']",Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,change API,Replace API with another effective one,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5757,WebGPU implementation of tf.image.fromPixels is inverted on y-axis,5,closed,2021-10-21T14:20:54Z,2021-10-25T01:16:38Z,environment:- tfjs 3.9.0 using tfjs-backend-webgpu 0.0.1-alpha8- tfjs and tfjs-backend-webgpu fresh build from main branch as of todaybasically; if i use `tf.image.fromPixels`; resulting tensor is inverted on y-axis - thus fully corrupted  if i construct tensor manually; it works fine  (i havent noticed before since i'm mostly using tfjs in a web worker so not relying on tf.image.fromPixels),"[""@shaoboyan Please take a look if it's related with your change in chromium.=====""; '@vladmandic  May I know the input of you `tf.image.fromPixels`? Is the input WebGL Canvas? Do you use chrome canary? If you could provide the program that would be appreciatate; I could have a try locally. Because the flip issue may related to recently chromium changes for copyExternalImageToTexture which tfjs.fromPixels relies on..The reason for that change is WebGL uses a coordinate which origin is bottom-left; but WebGPU uses coordinate which origin is top-left. In previous CopyExternalImageToTexture impl; we copy WebGL Canvas from bottom-left to align with WebGL coordinate.But in practical; WebGL developers render things with flipY correction to ensure the canvas generate correct result ; so the previous behaviour makes the final copy visual result weired. After discussion; community decides to make copyExternalImageToTexture copy pixels from top-left origin always.But it also be possible that you hit the issue due to the canary is not update to (>= 97.0.4675.0). So if you could provide the sample; I could have a double check.@qjia7  Pls assign this issue to me. Also cc @kainino0x for FYI.====='; '> May I know the input of you tf.image.fromPixels? > Is the input WebGL Canvas? yes; input is `OffscreenCanvas` with `webgl` context  (im using a simple GLSL programs on it to do some basic corrections like brightness/contrast/etc. before passing it to TF)> Do you use chrome canary?  ah; in **chrome 96.0.1053.0** image is inverted  but in **chrome 97.0.4678.0** its all good  i was aware of coordinate system difference; but wasnt aware of the recent change in chrome  thanks for the hint; issue is not really an issue - feel free to close it====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5757"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5757"">No</a>====='; '@vladmandic  Thanks for your check and happy to know the fix in chromium helps!=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,change browser version,Changing device/browser,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.2
https://github.com/tensorflow/tfjs/issues/5742,backend tfjs-backend-webgpu correctness is false on e2e|posenet|1024|ResNet50|image,1,open,2021-10-19T09:13:38Z,2021-11-18T20:06:33Z,Steps:Run Test correctness on e2e posenet model with 1024|ResNet50|image configuration.Device:Tiger-Lake.Result:Prediction matches GPU is false. actual[score] = 0.615322583793279; expected[score] = 0.6023533163482652,['@axinging As we discussed; you had already set up to the framework to debug the similar issue; please see it; thank you.====='],Incorrect Functionality,Incorrect Functionality,Unknown,WebGPU,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/5740,[tensorflow/tfjs][tfjs-node-gpu] cuda_malloc_async fails with CUDA device attribute error,9,open,2021-10-18T23:44:13Z,2021-10-22T16:15:03Z,Using **tfjs-node-gpu** on a GKE cluster running on an n1-higmem-8 with an NVIDIA P4 or V100 GPU fails when the cuda_malloc_async allocater is set using TF_GPU_ALLOCATOR.**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): YES- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 18.04 64bit- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: none- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0- Browser version: none; only tested in node- Node version: 14.15.3- Tensorflow.js Converter Version: none**Describe the current behavior**The app is a video filter that loads applies a super-resolution layer model to each frame in a video file; batching N frames together into a Tensor4D  to scale up the resolution by 4x. I run `tf.memory()` after each frame to ensure that I am not leaking any tensors. After processing slightly more than 100x 1280x720 frames correctly; TF bails out and dumps the memory allocations; as well as displaying the message:> If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.However; when I do set `TF_GPU_ALLOCATOR=cuda_malloc_async`; my normally correct startup process fails with:> tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:72] Failed to get device attribute: CUDA error: invalid argument (CUDA_ERROR_INVALID_VALUE)**Describe the expected behavior**My primary issue is being able to use `model.predict()` on several hundred video frames; grouped together into batches; without running out of memory. I have eliminated any tensor leaks according to `tf.memory()`; so I'm not sure what to try next? I have seen discussions mentioning `tf.engine.startScope/endScope`; and I can also try `dispose()`ing my model every N frames and re-loading it; or even `tf.engine.reset()` every N frames; but these seem like band-aids for internal TFJS issues.I do not explicitly allocate any TF variables within my code; so I do not expect `tf.disposeVariables()` to help. Is it possible that the model allocates variables internally that would benefit from running `tf.disposeVariables()` every frame?I repeat the same allocation pattern for each video frame batch; but I cannot find any way of re-using the existing Tensors to avoid fragmentation.**Standalone code to reproduce the issue**Producing repro code is possible; but a significant effort. If there are no simple answers to this issue; then I will take the necessary time to mock up a repro.Basically; I start by decoding frames into separate files using ffmpeg. Then; the processing loop will pre-fetch the next batch of N frames (N typically is 1-10) into a T4D by loading the individual frames:```const stack = []for (i=0; i < N; ++i) stack.push(tf.node.decodeImage(fs.readFileSync(filename); 3))const t4d = tf.stack(stack)```Once pre-fetched; processing is just: `superresModel.predict(t4d)`The output batch is finished; I then extract the individual frames and save them back to new output files using:```const saveTensor3DAsImageFile = async (tensor; frameIdx; dstExpr) => {  const executedImage = await tf.node.encodePng(tensor)  tensor.dispose()  const filename = sprintf(dstExpr; frameIdx) // image output path  fs.writeFileSync(filename; executedImage)}```After all batches are finished; I just call `ffmpeg` again to re-encode the output frame files.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.[err.log](https://github.com/tensorflow/tfjs/files/7369103/err.log)[nvidia_smi.log](https://github.com/tensorflow/tfjs/files/7369153/nvidia_smi.log),"['@danwexler just want to make sure there are no memory leaks; in your preprocessing code:```for (i=0; i < N; ++i) stack.push(tf.node.decodeImage(fs.readFileSync(filename); 3))```I assume you have dispose the tensors within stack array? can you show the tf.memory output before and after the inference?thanks====='; ""Yes; apologies; I was just mocking the real function in the bug report. As I said; I print `tf.memory()` after each frame to ensure there are no new additional tensors or memory allocated. Here's the full code for my pre-fetch function:```const stackTensors = (imagesExp: string; batchStartFrame: number; batchSize: number) => {  const tensors: Array<tf.Tensor3D> = []  for (let j = 0; j < batchSize; ++j) {    const idx = batchStartFrame + j + 1    const frame = sprintf(imagesExp; idx)    const tensor: tf.Tensor3D = loadImageAsTensor3D(frame)    if (tensor) tensors.push(tensor)  }  const batch: tf.Tensor4D = <tf.Tensor4D> tf.stack(tensors)  tensors.forEach(tensor => tensor.dispose())  return batch}```=====""; 'Here\'s a typical output from `tf.memory()`:```2021-10-15T01:05:21.796333333Z Task starting:2021-10-15T01:05:21.796398331Z {2021-10-15T01:05:21.796486309Z   ""TensorFlowMemory"": {2021-10-15T01:05:21.796489621Z     ""unreliable"": true;2021-10-15T01:05:21.796492909Z     ""numTensors"": 308;2021-10-15T01:05:21.796496160Z     ""numDataBuffers"": 308;2021-10-15T01:05:21.796499492Z     ""numBytes"": 167044002021-10-15T01:05:21.796502823Z   }2021-10-15T01:05:21.796506027Z }2021-10-15T01:05:25.044486894Z Task completed:2021-10-15T01:05:25.044580754Z {2021-10-15T01:05:25.044670002Z   ""TensorFlowMemory"": {2021-10-15T01:05:25.044672942Z     ""unreliable"": true;2021-10-15T01:05:25.044675892Z     ""numTensors"": 308;2021-10-15T01:05:25.044678802Z     ""numDataBuffers"": 308;2021-10-15T01:05:25.044681744Z     ""numBytes"": 167044002021-10-15T01:05:25.044684701Z   }2021-10-15T01:05:25.044687538Z }```The allocated memory is the core upscale layer model; after warmup/predict.====='; '@danwexler the other thing I want to confirm is that are you using tfjs model or tf saved model for inference?====='; ""I'm using a pretrained super-resolution model loaded from a cached version of the [Idealo ESRGAN](https://github.com/idealo/image-super-resolution). The model is currently loaded from Unpkg at this location: `https://unpkg.com/@upscalerjs/models@0.8.27/idealo/gans` using `tf.loadLayersModel()`. That version is provided by the author of the [`npm upscaler` package](https://www.npmjs.com/package/upscaler).IOW; this is not a TFJS-provided model from TFHub; and I do believe it is a TF saved model. Please correct me if I'm wrong as I did not do the original training. I feel very much like I need to understand more about the internals of how models work in order to understand this issue.I believe these are the model files: [gans.zip](https://github.com/tensorflow/tfjs/files/7383212/gans.zip)Looking at this model file; it seems to be a Keras 2.4.0 model converted using the TFJS Converter v2.0.1.post1 =====""; 'FYI; this is all part of an unannounced product that is in development which allows you to run TFJS models both locally in the browser and at scale on a dedicated cluster of cloud VMs. So I do run this code both in `tfjs-node-gpu` and in the browser with `tfjs`; however the browser is typically used to adjust settings on a single frame rather than rendering the entire video. You *can* run the entire video processing locally too; it just runs much faster when split up across multiple VMs and on bigger GPUs.====='; '@danwexler are you using cuda 11.2? I believe TF 2.5.0+ would require 11.2 at least. seems this problem is fixed in the upcoming TF 2.7.0https://github.com/tensorflow/tensorflow/issues/48545====='; ""Understood. Good info. Unfortunately; 11.2 is not available using the default Google Kubernetes Engine (GKE) nvidia-driver-installer Daemon Set.I've upgraded to using the `tensorflow/tensorflow:nightly-gpu` Docker base file; and upgraded my GKE backplane to the Rapid channel since the backplane version changes the base NVIDIA driver and CUDA version. Unfortunately; it looks like that [still installs only CUDA v11.0.](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#cuda).I believe there is a way to install a different driver than is installed on GKE based on the backplane version. Do you know of any documentation or instructions on how to upgrade the CUDA version on a GKE VM via the standard nvidia-driver-installer Daemon Set?This is not a blocking issue for me during development. I'll be testing workarounds while I wait for the TF 2.7.0 release.However; it would be great if there was a way to reuse existing allocations rather than re-allocating the same large tensors for data pre-fetch and `model.predict()`. That would definitely avoid fragmentation with user control at the API level. Otherwise; it seems to me that the current allocator is just not optimized to detect existing free blocks to re-use; at least for larger blocks? Hopefully the cuda_malloc_async allocator is an improvement in this regard. Alternatively; I plan to look at `tf.engine.reset()` to clear out the entire allocator and re-load my model from scratch every N frames. Any other workarounds I should explore?=====""; '@danwexler engine reset could de-allocate all your weight tensors for the model; you would need to recreate and upload them to gpu again; and I am not sure it will improve GPU memory fragmentation.=====']",Memory Leak,Poor Performance,Untimely Update,TF(GPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.2.1,B.3
https://github.com/tensorflow/tfjs/issues/5734,Support for BERT's hub.KerasLayer preprocessors and encoders,3,open,2021-10-17T19:35:03Z,2021-10-28T18:04:10Z,"**System information**- TensorFlow.js version: 3.9.0- tfjs-react-native version: 0.7.0**Current behavior/state.**I'm trying to make a pilot project to see how we can use BERT in React Native with TensorFlow.js.The code is public: <https://github.com/thibaut-d/Bert-in-React-Native-with-TensorFlow.js>I train the following model:```python# Preprocessingbert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')# Bert encoderbert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2')# Modeldef build_model():  # Input  text_input = tf.keras.layers.Input(shape=(); dtype=tf.string; name='text')  # Preprocessing   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess; name='preprocessing')  # Encoder  encoder_inputs = preprocessing_layer(text_input)  encoder = hub.KerasLayer(tfhub_handle_encoder; trainable=True; name='BERT_encoder')  # Encoder's output  outputs = encoder(encoder_inputs)  net = outputs['pooled_output']  net = tf.keras.layers.Dropout(0.1)(net)  # Classifier  regression = tf.keras.layers.Dense(1; name='regression'; activation=None)(net)  classifier = tf.keras.layers.Dense(1; name='classifier'; activation='sigmoid')(net)  # Final output  outputs = {'regression': regression; 'classifier': classifier}  # Return the model  return tf.keras.Model(text_input; outputs)```I do not include the Python code for training and testing; as it is quite classic.I save it to different formats:```python# Save to Tensorflow SavedModelmodel.save(""./formats/tf_savedmodel"";save_format='tf')```*WARNING:absl:Found untraced functions such as restored_function_body; restored_function_body; restored_function_body; restored_function_body; restored_function_body while saving (showing 5 of 165). These functions will not be directly callable after loading.*```python# Save to HDF5model.save('./formats/tf_hdf5/model.h5';save_format='h5')```*No warning*Then I try different exports:```python# Keras HDF5 --> tfjs_layers_model!tensorflowjs_converter --input_format keras --output_format tfjs_layers_model ./formats/tf_hdf5/model.h5 ./formats/tfjs_layers_model_from_keras_hdf5```*No warning*```python# Keras HDF5 --> tfjs_graph_model!tensorflowjs_converter --input_format keras --output_format tfjs_graph_model ./formats/tf_hdf5/model.h5 ./formats/tfjs_graph_model_from_keras_hdf5```I get the error message:```Traceback (most recent call last):  File ""C:\Users\thiba\anaconda3\envs\bert\lib\runpy.py""; line 197; in _run_module_as_main    return _run_code(code; main_globals; None;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\runpy.py""; line 87; in _run_code    exec(code; run_globals)  File ""C:\Users\thiba\anaconda3\envs\bert\Scripts\tensorflowjs_converter.exe\__main__.py""; line 7; in <module>  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 813; in pip_main    main([' '.join(sys.argv[1:])])  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 817; in main    convert(argv[0].split(' '))  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 803; in convert    _dispatch_converter(input_format; output_format; args; quantization_dtype_map;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 504; in _dispatch_converter    dispatch_keras_h5_to_tfjs_graph_model_conversion(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 140; in dispatch_keras_h5_to_tfjs_graph_model_conversion    model = tf.keras.models.load_model(h5_path; compile=False)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\saving\save.py""; line 200; in load_model    return hdf5_format.load_model_from_hdf5(filepath; custom_objects;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\saving\hdf5_format.py""; line 180; in load_model_from_hdf5    model = model_config_lib.model_from_config(model_config;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\saving\model_config.py""; line 52; in model_from_config    return deserialize(config; custom_objects=custom_objects)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\layers\serialization.py""; line 208; in deserialize    return generic_utils.deserialize_keras_object(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\utils\generic_utils.py""; line 674; in deserialize_keras_object    deserialized_obj = cls.from_config(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\engine\functional.py""; line 662; in from_config    input_tensors; output_tensors; created_layers = reconstruct_from_config(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\engine\functional.py""; line 1273; in reconstruct_from_config    process_layer(layer_data)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\engine\functional.py""; line 1255; in process_layer    layer = deserialize_layer(layer_data; custom_objects=custom_objects)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\layers\serialization.py""; line 208; in deserialize    return generic_utils.deserialize_keras_object(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\utils\generic_utils.py""; line 659; in deserialize_keras_object    (cls; cls_config) = class_and_config_for_serialized_keras_object(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\keras\utils\generic_utils.py""; line 556; in class_and_config_for_serialized_keras_object    raise ValueError(ValueError: Unknown layer: KerasLayer. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.```I also tried to export form tf savedmodel; despite the warning:```python# tf_saved_model --> tfjs_graph_model!tensorflowjs_converter --input_format tf_saved_model --output_format=tfjs_graph_model ./formats/tf_savedmodel ./formats/tfjs_graph_model_from_tf_saved_model``````2021-10-17 21:13:22.786947: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2To enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.2021-10-17 21:13:23.483860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1789 MB memory:  -> device: 0; name: NVIDIA GeForce GTX 1050; pci bus id: 0000:01:00.0; compute capability: 6.1Traceback (most recent call last):  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\framework\ops.py""; line 3962; in _get_op_def    return self._op_def_cache[type]KeyError: 'CaseFoldUTF8'During handling of the above exception; another exception occurred:Traceback (most recent call last):  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\saved_model\load.py""; line 902; in load_internal    loader = loader_cls(object_graph_proto; saved_model_proto; export_dir;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\saved_model\load.py""; line 137; in __init__    function_deserialization.load_function_def_library(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\saved_model\function_deserialization.py""; line 388; in load_function_def_library    func_graph = function_def_lib.function_def_to_graph(copy)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\framework\function_def_to_graph.py""; line 63; in function_def_to_graph    graph_def; nested_to_flat_tensor_name = function_def_to_graph_def(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\framework\function_def_to_graph.py""; line 228; in function_def_to_graph_def    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\framework\ops.py""; line 3966; in _get_op_def    pywrap_tf_session.TF_GraphGetOpDef(self._c_graph; compat.as_bytes(type);tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on IDEAPAD. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib; accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph; as contrib ops are lazily registered when the module is first accessed.During handling of the above exception; another exception occurred:Traceback (most recent call last):  File ""C:\Users\thiba\anaconda3\envs\bert\lib\runpy.py""; line 197; in _run_module_as_main    return _run_code(code; main_globals; None;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\runpy.py""; line 87; in _run_code    exec(code; run_globals)  File ""C:\Users\thiba\anaconda3\envs\bert\Scripts\tensorflowjs_converter.exe\__main__.py""; line 7; in <module>  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 813; in pip_main    main([' '.join(sys.argv[1:])])  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 817; in main    convert(argv[0].split(' '))  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 803; in convert    _dispatch_converter(input_format; output_format; args; quantization_dtype_map;  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\converter.py""; line 523; in _dispatch_converter    tf_saved_model_conversion_v2.convert_tf_saved_model(  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py""; line 599; in convert_tf_saved_model    model = _load_model(saved_model_dir; saved_model_tags)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflowjs\converters\tf_saved_model_conversion_v2.py""; line 536; in _load_model    model = load(saved_model_dir; saved_model_tags)  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\saved_model\load.py""; line 864; in load    result = load_internal(export_dir; tags; options)[""root""]  File ""C:\Users\thiba\anaconda3\envs\bert\lib\site-packages\tensorflow\python\saved_model\load.py""; line 905; in load_internal    raise FileNotFoundError(FileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on IDEAPAD. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib; accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph; as contrib ops are lazily registered when the module is first accessed. If trying to load on a different device from the computational device; consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.```Since exporting to keras HDF5 and converting to tfjs_layers did not raise warning or errors; I tried to run it in React Native. TensforFlow.js successfully loads; with backend: rn-webgl.But when I load the model with:```const modelJson = require('./model/model.json');const m1 = require('./model/group1-shard1of5.bin');const m2 = require('./model/group1-shard2of5.bin');const m3 = require('./model/group1-shard3of5.bin');const m4 = require('./model/group1-shard4of5.bin');const m5 = require('./model/group1-shard5of5.bin'); const model = await tf.loadLayersModel(bundleResourceIO(modelJson; [m1;m2;m3;m4;m5]))```I get the following message:```[Error: Unknown layer: KerasLayer. This may be due to one of the following reasons:1. The layer is defined in Python; in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript; but is not registered properly with tf.serialization.registerClass().]```**So ?**My bet is that all these errors are linked to the fact that one or both of the hub.KerasLayer are not supported.```python# Preprocessingbert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')# Bert encoderbert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2')```Is there something I can do to load these layers?Does another similar model support it?The current option I see is to rely on HuggingFace transformers library on the Python side and to implement from scratch the preprocessing in JavaScript. Is it the only option?","['Did you get chance to check out this demo https://alexfi.dev/blog/tensorflowjs-bert-train ?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '> Did you get chance to check out this demo https://alexfi.dev/blog/tensorflowjs-bert-train ?Yes; this is what I mean by : ""The current option I see is to rely on HuggingFace transformers library on the Python side and to implement from scratch the preprocessing in JavaScript"".=====']",Reference Error,Crash,Unimplemented Operator,Layer API,API,,,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5724,Problems when running tfjs-tflite demo,2,closed,2021-10-13T21:49:04Z,2021-10-18T16:49:44Z,I run the [demo in tflite](https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite); the chrome console has such a problem![image_2021-10-14_00-41-29](https://user-images.githubusercontent.com/6616117/137217114-f6209ac5-b649-44cc-82d6-198ec3a08de0.png)：But `yarn build-deps` & `yarn` & `yarn watch` produce no errors.How could i avoid this errors?,"[""Thanks for the bug report; @alexionby. I'm able to reproduce the error. @jinjingforever; I think Parcel may be incorrectly bundling the downloaded wasm files. The error is still present when I switch to using dependencies from npm.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5724"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5724"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Layer API,API,build/install configuration,Modifying dependency configuration,framework,Model Loading,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL not supported""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5716,Error when running yarn test in tfjs-layers,4,closed,2021-10-12T13:08:14Z,2021-10-13T21:24:41Z,- TensorFlow.js version: 3.9.0Whenever I try to run the command ``` yarn test```inside the tfjs-layers folder I get this error:```yarn run v1.22.15$ yarn test-dev$ ibazel run :tfjs-layers_test --test-output=streamediBazel [1:02PM]: Querying for files to watch...Loading: 0 packages loadedERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelLoading: 0 packages loadedLoading: 0 packages loadediBazel [1:02PM]: Bazel query failed: exit status 7Loading: 0 packages loadedERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelLoading: 0 packages loadedLoading: 0 packages loadediBazel [1:02PM]: Bazel query failed: exit status 7iBazel [1:02PM]: Running :tfjs-layers_testLoading: Loading: 0 packages loadedERROR: Skipping ':tfjs-layers_test': no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelWARNING: Target pattern parsing failed.ERROR: no such target '//tfjs-layers:tfjs-layers_test': target 'tfjs-layers_test' not declared in package 'tfjs-layers' (did you mean 'tfjs-layers_pkg'?) defined by /workspace/tfjs/tfjs-layers/BUILD.bazelINFO: Elapsed time: 0.074sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)FAILED: Build did NOT complete successfully (0 packages loaded)iBazel [1:02PM]: Error running Bazel exit status 1error Command failed with exit code 4.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 4.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```I tried with a freshly cloned tfjs and the same error happened; what is the missing step,"[""Thanks for the bug report! This is a bug in the `package.json` test script. `tfjs-layers_test` should be `tfjs-layers_webgl2_test`. It was not caught by CI since CI does not run tests through `yarn test` and uses a different method instead. I'll submit a fix.=====""; ""Thank you @mattsoulanille. After changing my package.json locally and running 'yarn test' i got another error:```'external' is not recognized as an internal or external command; operable program or batch file. ```My os is Windows 10=====""; ""This looks like an issue with Bazel Karma tests on Windows. We're working on fixing this in #5660; and you can see any updates we post in that issue. At the moment; there's not a good workaround that we know of; but fixing this is high priority since it blocks almost all Windows development.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5716"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5716"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Layer API,API,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Missing Target Declaration""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5709,tensor cannot be created from clamped array,4,closed,2021-10-10T16:08:00Z,2021-10-12T15:33:11Z,tensor cannot be created from clamped arrayfor example; `(imageData: ImageData).data` is of type `Uint8ClampedArray`.```jsconst tensor = tf.tensor3d(imageData.data; [height; width; 4]; 'float32');```fails with error:```textUncaught (in promise) Error: tensor3d() requires values to be number[][][] or flat/TypedArray```but doing this *(which is completely pointless)* works just fine:```js// copy Uint8ClampedArray into Uint8Array: costs CPU and memoryconst tempArray = Uint8Array.from(imageData.data);const tensor = tf.tensor3d(tempArray; [height; width; 4]; 'float32');```of course; `tf.browser.fromPixels(ImageData)` works either way; but- its browser-only; not available in nodejs- passing just the data array instead of entire imageData object is sometimes preffered- and in most cases; it's actually slowerfiling as a bug since `tfjs` supports **TypedArrays** and `Uint8ClampedArray` is a **TypedArray**environment: tfjs 3.9.0,"['Submitted trivial PR #5710====='; '@vladmandic as the PR is merged ; can we close this issue ?====='; 'Closing - thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5709"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5709"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,add support for datatype,Add unsupported operator,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5705,Landmarks not placing correctly on back camera,4,closed,2021-10-09T00:21:58Z,2021-12-13T17:39:54Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  Copied from [here](https://github.com/jinjingforever/tfjs-react-native-pose-detection/blob/main/App.tsx)- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  macOS Big Sur 11.2.1- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone SE- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0- Browser version:Chrome 94- Tensorflow.js Converter Version:3.9.0**Describe the current behavior**When using back camera; landmarks are not placing correctly. They are placing off to the side. When camera is not pointed towards a person; landmarks are being placed randomly. Using the front camera works without any issue.![IMG_8438](https://user-images.githubusercontent.com/47552416/136633167-31ace200-4404-45bb-bc60-d643f3fa857d.PNG)![IMG_8455](https://user-images.githubusercontent.com/47552416/136636950-972865e4-e1cb-4d3d-b344-ba60299b5f56.jpg)**Describe the expected behavior**Landmarks expected to be placed in correct position and only when facing a person**Code**```import '@mediapipe/pose'import ""@tensorflow/tfjs-backend-webgl"";import React; { useEffect; useState; useRef } from 'react';import { StyleSheet; Text; View; Dimensions; Platform } from 'react-native';import { Camera } from 'expo-camera';import * as tf from '@tensorflow/tfjs';import * as posedetection from '@tensorflow-models/pose-detection';import { cameraWithTensors } from '@tensorflow/tfjs-react-native';import Svg; { Circle } from 'react-native-svg';import { ExpoWebGLRenderingContext } from 'expo-gl';const TensorCamera = cameraWithTensors(Camera);const WIN_WIDTH = Dimensions.get('window').width;const CAM_TEXTURE_WIDTH =  Platform.OS === 'ios'    ? Platform.isPad && WIN_WIDTH === 768      ? 720      : 1080    : 1600;const CAM_TEXTURE_HEIGHT =  Platform.OS === 'ios'    ? Platform.isPad && WIN_WIDTH === 768      ? 1280      : 1920    : 1200;const USE_PRESET = false;const CAM_TEXTURE_INITIAL_SIZE = 2000;const CAM_PREVIEW_WIDTH = Dimensions.get('window').width;const CAM_PREVIEW_HEIGHT =  CAM_PREVIEW_WIDTH / (Platform.OS === 'ios' ? 9 / 16 : 3 / 4);// The score threshold for pose detection results.const MIN_KEYPOINT_SCORE = 0.3;// The size of the resized output from TensorCamera.//// For movenet; the size here doesn't matter too much because the model will// preprocess the input (crop; resize; etc).const OUTPUT_TENSOR_WIDTH = 240;const OUTPUT_TENSOR_HEIGHT = 320;// Whether to auto-render TensorCamera preview.const AUTO_RENDER = false;export default function ScanScreen() {  const cameraRef = useRef(null);  const [camTextureSizeAvailable; setCamTextureSizeAvailable] =    useState(USE_PRESET);  const [camTextureWidth; setCamTextureWidth] = useState(    USE_PRESET ? CAM_TEXTURE_WIDTH : CAM_TEXTURE_INITIAL_SIZE  );  const [camTextureHeight; setCamTextureHeight] = useState(    USE_PRESET ? CAM_TEXTURE_HEIGHT : CAM_TEXTURE_INITIAL_SIZE  );  const [outputTensorWidth; setOutputTensorWidth] = useState(    USE_PRESET ? OUTPUT_TENSOR_WIDTH : CAM_TEXTURE_INITIAL_SIZE  );  const [outputTensorHeight; setOutputTensorHeight] = useState(    USE_PRESET ? OUTPUT_TENSOR_HEIGHT : CAM_TEXTURE_INITIAL_SIZE  );  const [tfReady; setTfReady] = useState(false);  const [model; setModel] = useState<posedetection.PoseDetector>();  const [poses; setPoses] = useState<posedetection.Pose[]>();  const [fps; setFps] = useState(0);  useEffect(() => {    async function prepare() {      // Camera permission.      await Camera.requestPermissionsAsync();      // Setup tfjs.      await tf.setBackend('rn-webgl');      await tf.ready();      // Load movenet model.      const model = await posedetection.createDetector(        posedetection.SupportedModels.MoveNet;        {          modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING;          enableSmoothing: true;          // minPoseScore: 1;        }      );      setModel(model);      // Ready!      setTfReady(true);    }    prepare();  }; []);  const calculateCameraTextureSize = async (    images: IterableIterator<tf.Tensor3D>;    updatePreview: () => void;    gl: ExpoWebGLRenderingContext  ) => {    console.log('Start calculating camera texture size');    const loop = async () => {      // Wait and throttle.      await new Promise<void>((resolve) => {        setTimeout(() => {          resolve();        }; 200);      });      // Get the tensor.      const imageTensor = images.next().value as tf.Tensor3D;      // Get the tensor values.      const imageTensorVals = imageTensor.arraySync();      tf.dispose([imageTensor]);      // A quick check to make sure the camera feed is ready (not black).      //      // Only check 50 pixels in the first row.      let ready = false;      for (let i = 0; i < 50; i++) {        if (!checkPixelsMatch(imageTensorVals[0][i]; [0; 0; 0])) {          ready = true;          break;        }      }      if (ready) {        // Check 10 rows and columns for edges; and store the possible        // edges in the arrays below. Eventually we will use the max values        // as the final edges.        const horizontalEdges = [];        const verticalEdges = [];        const sampleCount = 10;        for (let sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {          // Focus on the first 500 pixels to get sample rows/columns..          const rcIndex = Math.floor((500 / sampleCount) * sampleIndex);          let curHorizontalEdge = -1;          let curVerticalEdge = -1;          // In the current sample row/column; check 50 consecutive pixels          // to see if they have the same color or not.          const pixelsToCheck = 50;          for (            let anchorPixelIndex = 200;            anchorPixelIndex < CAM_TEXTURE_INITIAL_SIZE - pixelsToCheck - 1;            anchorPixelIndex++          ) {            const curRowAnchorPixel =              imageTensorVals[rcIndex][anchorPixelIndex];            const curColAnchorPixel =              imageTensorVals[anchorPixelIndex][rcIndex];            let foundHorizontalEdge = true;            let foundVerticalEdge = true;            for (let i = 1; i <= pixelsToCheck; i++) {              const rowPixelToCheck =                imageTensorVals[rcIndex][anchorPixelIndex + i];              const colPixelToCheck =                imageTensorVals[anchorPixelIndex + i][rcIndex];              if (!checkPixelsMatch(rowPixelToCheck; curRowAnchorPixel)) {                foundHorizontalEdge = false;              }              if (!checkPixelsMatch(colPixelToCheck; curColAnchorPixel)) {                foundVerticalEdge = false;              }              if (!foundHorizontalEdge && !foundVerticalEdge) {                break;              }            }            if (foundHorizontalEdge && curHorizontalEdge < 0) {              curHorizontalEdge = anchorPixelIndex;            }            if (foundVerticalEdge && curVerticalEdge < 0) {              curVerticalEdge = anchorPixelIndex;            }            if (curHorizontalEdge > 0 && curVerticalEdge > 0) {              break;            }          }          horizontalEdges.push(curHorizontalEdge);          verticalEdges.push(curVerticalEdge);        }        // Find the edges.        const horizontalEdge = Math.max(...horizontalEdges) + 1;        const verticalEdge = Math.max(...verticalEdges) + 1;        console.log(`Camera texture size: ${horizontalEdge}x${verticalEdge}`);        setCamTextureWidth(horizontalEdge);        setCamTextureHeight(verticalEdge);        setOutputTensorWidth(OUTPUT_TENSOR_WIDTH);        setOutputTensorHeight(OUTPUT_TENSOR_HEIGHT);        setCamTextureSizeAvailable(true);      } else {        requestAnimationFrame(loop);      }    };    loop();  };  const handleCameraStream = async (    images: IterableIterator<tf.Tensor3D>;    updatePreview: () => void;    gl: ExpoWebGLRenderingContext  ) => {    const loop = async () => {      // Get the tensor and run pose detection.      const imageTensor = images.next().value as tf.Tensor3D;      const startTs = Date.now();      const poses = await model!.estimatePoses(        imageTensor;        undefined;        Date.now()      );      const latency = Date.now() - startTs;      setFps(Math.floor(1000 / latency));      setPoses(poses);      tf.dispose([imageTensor]);      // Render camera preview manually when autorender=false.      if (!AUTO_RENDER) {        updatePreview();        gl.endFrameEXP();      }      requestAnimationFrame(loop);    };    loop();  };  const renderPose = () => {    if (poses != null && poses.length > 0) {      const keypoints = poses[0].keypoints        .filter((k) => (k.score ?? 0) > MIN_KEYPOINT_SCORE)        .map((k) => {          // Flip horizontally on android.          const x = Platform.OS === 'android' ? outputTensorWidth - k.x : k.x;          const y = k.y;          return (            <Circle              key={`skeletonkp_${k.name}`}              cx={(x / outputTensorWidth) * CAM_PREVIEW_WIDTH}              cy={(y / outputTensorHeight) * CAM_PREVIEW_HEIGHT}              r='4'              strokeWidth='2'              fill='#4f4bcb'              stroke='white'            />          );        });      return <Svg style={styles.svg}>{keypoints}</Svg>;    } else {      return <View></View>;    }  };  const checkPixelsMatch = (p1: number[]; p2: number[]) => {    return p1[0] === p2[0] && p1[1] === p2[1] && p1[2] === p2[2];  };  return (    <View style={styles.container}>      <View        style={{          ...styles.camContainer;          opacity: tfReady && camTextureSizeAvailable ? 1 : 0;        }}      >        {/* The hidden camera for figuring out the correct texture size */}        {!camTextureSizeAvailable && (          <TensorCamera            ref={cameraRef}            style={styles.camera}            autorender={true}            type={Camera.Constants.Type.back}            // tensor related props            cameraTextureWidth={camTextureWidth}            cameraTextureHeight={camTextureHeight}            resizeWidth={outputTensorWidth}            resizeHeight={outputTensorHeight}            resizeDepth={3}            onReady={calculateCameraTextureSize}             flashMode='auto'            useCustomShadersToResize={false}          />        )}        {/* The main camera */}        {camTextureSizeAvailable && tfReady && (          <TensorCamera            ref={cameraRef}            style={styles.camera}            autorender={AUTO_RENDER}            type={Camera.Constants.Type.back}            // tensor related props            cameraTextureWidth={camTextureWidth}            cameraTextureHeight={camTextureHeight}            resizeWidth={outputTensorWidth}            resizeHeight={outputTensorHeight}            resizeDepth={3}            onReady={handleCameraStream}             flashMode='auto'            useCustomShadersToResize={false}          />        )}        {renderPose()}        {/* {renderFps()} */}      </View>      {(!tfReady || !camTextureSizeAvailable) && (        <View style={styles.loadingMsg}>          <Text>Loading...</Text>        </View>      )}    </View>  );}```","['Hi @eledahl; thank you for the report. tfjs-react-native 0.7.0 should fix this problem. You no longer need to specify camera texture size. Please see an updated example [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection). Thank you!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5705"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5705"">No</a>====='; 'Hi @jinjingforever ; thank you for the answer and methodology. Do you know how one can implement the skeleton or is there any thought of implementing that. Perhaps the performance goes down ?====='; ""Hi @pmahan00 we have a posenet/movenet demo app and you can see how it draws the skeleton [here](https://github.com/tensorflow/tfjs-models/blob/master/posenet/demo/demo_util.js#L100). The app is not in react native but you can use the similar techniques. It shouldn't affect the performance too much. Hope it helps! =====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Mobile,Platform,change tfjs-react-native version(framework),Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5704,GPU utilization is very poor on multi-process or concurrently trained models,4,open,2021-10-09T00:16:32Z,2021-10-21T23:57:11Z,**System information**- TensorFlow.js v3.9.0 (Windows 10); `tfjs-node-gpu`- Are you willing to contribute it (Yes/No): Possibly if not overly complex changes**Describe the feature and the current behavior/state.**On average most model training greatly underutilizes the GPU; due to the nature of how GPU's scale. We all know that large models (tens or hundreds of millions of params) perform far more efficient (ops/watt) than smaller models (< 10M params); that is simply the nature of accelerators. The issue with tfjs is that there is no way for these smaller models to be trained in parallel (across multiple worker processes) OR concurrently with any significant benefits (more on this later).Let's take a look at the state of parallel vs concurrent training in tensorflow:**Multi-process**Multi-process scales linearly in the case of multiple GPU's; but this story is about the utilization of each of those individual GPU's. Due to tfjs lack of memory allocation limits; multi-process training on a GPU is not possible today. The simple solution would be to add a [similar interface to manage memory in Tensorflow 2 core](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)**Concurrency**Unlike multi-process training on a single GPU; concurrent training of models within a single process is actually possible today. In fact I've trained dozens and even hundreds of (<10M param) models; concurrently. While I can't speak to exactly why the performance and GPU utilization is so poor; I've run dozens of simple tests where-in at best I'll see 50-70% **increase** in GPU utilization (not to be confused with total utilization). While these benefits sound nice; it's quite devastating when we're talking sub 10% GPU utilization. It's not clear to me if this is a bug (or design flaw) in tfjs or tfcore; but I see no reason why we should not be able to fully utilize a GPU if we have enough models to saturate the GPU.Consider this contrived example (based on actual testing):* I can train a single ~130M param model to exceed 85% of GPU utilization (rtx 3080 ti for reference)* But training two ~65M param models concurrently will utilize less than 55% of the GPU* Adding additional ~65M param models may see slightly higher GPU utilization; but never more than 60%* For even smaller models; any amount of concurrency (even in the hundreds) will never even see 25% utilizationThe expectation is that so long as we've not run out of GPU memory; or have some sort of model or CPU inefficiency that prevents full utilization; we should be able to take full advantage of the GPU by running 2 or more models; concurrently.**Will this change the current api? How?**1. Enable multi-process GPU training by adding a [similar interface to manage memory in Tensorflow 2 core](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth).2. Resolve concurrency performance -- unclear if this would require an interface change or bug fix**Who will benefit with this feature?**This could benefit a lot of engineers; especially those that need to improve GPU utilization with smaller models.**Any Other info.**I've put hundreds of hours into tfjs and **love** it! Would love to make tfjs a more serious real-world training solution and a true first-class alternative to Python-based training. Thank you!,"['@asilvas Thank for providing the detail performance analysis of concurrent training with TFJS. One thing I want to confirm first; are you running training within browser or using our tfjs-node backend on nodejs?This would give us better idea where to investigate. thanks====='; 'Hi @pyu10055 -- This is `tfjs-node-gpu` with CUDA. ====='; '@asilvas Can you share your setup for training multiple models with TFJS? It will help us to identify the bottleneck quicker. Thanks!====='; ""Sure thing. Here's a contrived test: https://gist.github.com/asilvas/29c566709d0565a9638b7605ecf8e283Produced results:```node gpu-concurrency-test.js 1Training 1 x 24M param models took 25194ms; or 25194ms per modelnode gpu-concurrency-test.js 2Training 2 x 24M param models took 45338ms; or 22669ms per modelnode gpu-concurrency-test.js 5Training 5 x 24M param models took 102100ms; or 20420ms per modelnode gpu-concurrency-test.js 10Training 10 x 24M param models took 191775ms; or 19178ms per model```Despite using less than 5% of the GPU with a single model; training two concurrently netted only ~11% gains (despite a near idle GPU). Increasing concurrency slightly increases overall throughput; but by very insignificant gains.I didn't do much testing as I wasn't worried about the usefulness of these test models; but the results were consistent with real-world tests I've done.=====""]",Abnormal GPU Memory/Utilization,Poor Performance,Incorrect Code Logic,TF(GPU),Backend,memory management,Add API usage for memory management,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.3] Abnormal GPU Memory/Utilization""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",B.2.3,A.4
https://github.com/tensorflow/tfjs/issues/5702,Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT,4,closed,2021-10-08T15:28:31Z,2021-10-08T18:59:50Z,"I'm having this same issue since v3.8.0; running node v15.7.0; tfjs-node v3.8.0 or v3.9.0 on Mac Big Sur.But no issue running on Raspberry Pi 3 & 4.  I'm attaching the model I'm using.  const image = fs.readFileSync(imagePath);  const decodedImage = tfnode.node.decodeImage(new Uint8Array(image); 3);  const model = await tfnode.node.loadSavedModel(modelPath);  const predictions = model.predict(decodedImage);2021-05-31 18:58:58.942711: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: saved_model2021-05-31 18:58:59.291324: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }2021-05-31 18:58:59.291368: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: saved_model2021-05-31 18:59:00.692648: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.2021-05-31 18:59:03.160455: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: saved_model2021-05-31 18:59:04.360189: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 5417477 microseconds.2021-05-31 18:59:12.511652: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?2021-05-31 18:59:12.511706: W tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at constant_op.cc:79 : Invalid argument: Cannot parse tensor from tensor_proto.2021-05-31 18:59:12.546008: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?2021-05-31 18:59:12.546526: W tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at constant_op.cc:79 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANTtensor_shape {}variant_val {  type_name: ""tensorflow::TensorList""  metadata: ""\001\000\001\377\377\377\377\377\377\377\377\377\001\030\001""}/test-saved-model/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:449        var outputMetadata = this.binding.runSavedModel(id; this.getMappedInputTensorIds(inputs; inputTensorInfos); inputTensorInfos.map(function (info) { return info.name; }).join(';'); outputOpNames.join(';'));                                          ^Error: Session fail to run with error: Cannot parse tensor from proto: dtype: DT_VARIANTtensor_shape {}variant_val {  type_name: ""tensorflow::TensorList""  metadata: ""\001\000\001\377\377\377\377\377\377\377\377\377\001\030\001""}[model.zip](https://github.com/tensorflow/tfjs/files/7312434/model.zip)","['@playground This bug is introduced in TF 2.x and has been fixed in TF 2.6.0https://github.com/tensorflow/tensorflow/issues/44428I have verified your model works with tfjs-node using TF 2.6.0 binary on mac; and I will upgrade the tfjs-node to use TF 2.6.0 binary.====='; ""Hi @pyu10055  that's great; thanks for the update.  Will this be in the next release?=====""; 'Yes it will be part of next release. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5702"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5702"">No</a>=====']",Data & Model Error,Crash,Untimely Update,TF(CPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.2,B.3
https://github.com/tensorflow/tfjs/issues/5701,TFJS-TFLITE error when model outputs quantized values when calling predict(...),1,closed,2021-10-08T04:51:50Z,2021-10-08T04:58:40Z,"**System information**Linux Ubuntu 18.04.03; Chrome 93TensorFlow.js installed from (npm or script link):Latest from NPM via script tag:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.js""></script>```**Tensorflow.js Converter Version:**Model created via TFLITE converter via python file (quantization aware training)**Describe the current behavior:**When running a model that outputs int8 values via TFJS-TFLITE; such as an image generation model; the stack crashes with the error:`Error: values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings; or a TypedArray`This occurs at: `TFLiteModel.predict (tflite_model.ts:134)`**Possible source of issue:**It appears the model is outputting an Int8Array that is passed to tensorflow's `tensor(...)` method; however that method appears to only accept `Float32Array; Int32Array; or Uint8Array` as per https://github.com/tensorflow/tfjs/issues/4009#issue-713346237**Describe the expected behavior:**TFJS-TFLITE should ensure that quantized image model's output are casted to UInt8Array; instead of Int8Array... or should expand support in tensorflowJS to accept Int8Array **Standalone code to reproduce the issue**```// Produce quantized 8 bit model that outputs imageslet model = await tflite.loadTFLiteModel('URL TO 8 bit quantized image generation model');let cameraImage = new tf.tensor4d(      new Int32Array(        ctx.getImageData(0; 0; 256; 256).data);      [1; 256; 256; 4])cameraImage = tf.slice(cameraImage; [0; 0; 0; 0]; [1; 256; 256; 3]) // a faster way to slice off the alpha channel instead of loop+iflet result = model.predict(cameraImage) // crash```**Other info / logs**```VM9506:1 Uncaught TypeError: Cannot read properties of undefined (reading 'data')    at eval (eval at TFLiteModel.predict (tflite_model.ts:133); <anonymous>:1:13)    at TFLiteModel.predict (tflite_model.ts:133)```",['duplicate of https://github.com/tensorflow/tfjs/issues/5700 ; closing this issue.====='],Reference Error,Crash,Inconsistent Modules,Operator,API,add type conversion,Replace data Shape/type,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5700,TFJS-TFLITE error when model outputs quantized values when calling predict(...),1,closed,2021-10-08T04:51:46Z,2021-10-20T17:48:16Z,"**System information**Linux Ubuntu 18.04.03; Chrome 93TensorFlow.js installed from (npm or script link):Latest from NPM via script tag:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>  <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.js""></script>```**Tensorflow.js Converter Version:**Model created via TFLITE converter via python file (quantization aware training)**Describe the current behavior:**When running a model that outputs int8 values via TFJS-TFLITE; such as an image generation model; the stack crashes with the error:`Error: values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings; or a TypedArray`This occurs at: `TFLiteModel.predict (tflite_model.ts:134)`**Possible source of issue:**It appears the model is outputting an Int8Array that is passed to tensorflow's `tensor(...)` method; however that method appears to only accept `Float32Array; Int32Array; or Uint8Array` as per https://github.com/tensorflow/tfjs/issues/4009#issue-713346237**Describe the expected behavior:**TFJS-TFLITE should ensure that quantized image model's output are casted to UInt8Array; instead of Int8Array... or should expand support in tensorflowJS to accept Int8Array **Standalone code to reproduce the issue**```// Produce quantized 8 bit model that outputs imageslet model = await tflite.loadTFLiteModel('URL TO 8 bit quantized image generation model');let cameraImage = new tf.tensor4d(      new Int32Array(        ctx.getImageData(0; 0; 256; 256).data);      [1; 256; 256; 4])cameraImage = tf.slice(cameraImage; [0; 0; 0; 0]; [1; 256; 256; 3]) // a faster way to slice off the alpha channel instead of loop+iflet result = model.predict(cameraImage) // crash```**Other info / logs**```VM9506:1 Uncaught TypeError: Cannot read properties of undefined (reading 'data')    at eval (eval at TFLiteModel.predict (tflite_model.ts:133); <anonymous>:1:13)    at TFLiteModel.predict (tflite_model.ts:133)```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5700"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5700"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Operator,API,add type conversion,Replace data Shape/type,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5698,Compatibility with newer version of react native and expo,7,open,2021-10-07T14:34:53Z,2021-11-03T16:52:57Z,"While installing tfjs-react-native on my expo project; i've got this error```➜  smartstudio-app git:(feat/A410) ✗ npm install @tensorflow/tfjs-react-nativenpm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! While resolving: undefined@undefinednpm ERR! Found: expo-asset@8.3.3npm ERR! node_modules/expo-assetnpm ERR!   expo-asset@""~8.3.3"" from the root projectnpm ERR! npm ERR! Could not resolve dependency:npm ERR! peer expo-asset@""^7.0.0"" from @tensorflow/tfjs-react-native@0.7.0npm ERR! node_modules/@tensorflow/tfjs-react-nativenpm ERR!   @tensorflow/tfjs-react-native@""*"" from the root project```my config```   ""expo"": ""^42.0.0"";    ""expo-asset"": ""~8.3.3"";    ""expo-auth-session"": ""~3.3.1"";    ""expo-av"": ""~9.2.3"";    ""expo-camera"": ""~11.2.2"";    ""expo-constants"": ""~11.0.1"";```i also believe this library not to be comptabile with expo-camera over version 7 while i have version 11.forcing peer deps doesn't look like a good idea; but maybe we can write dependencies of tfjs-react-native like    ""expo-camera"": "">=7.0.0"";if expo does not break api between major versionThanks a lot for the good work; team !","['expo-camera latest version is supported ; please check out this repo https://github.com/jinjingforever/tfjs-react-native-pose-detection which was recently tested.  Thank you ====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; ""it works only with yarn. installing with npm; even with --force; gives plenty of errors at runtime and actually don't work=====""; 'Can you try with this latest example https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification; right now it only works with yarn. It might take some time to support npm as this is throwing some errors.====='; '@BigZ this looks like the infamous peer dependency issue; npm changed the behavior of peer dependencies:- `<=6.x.x` - peer dependencies are optional; when missing or ""incompatible"" it gives you a warning.- `>=7.x.x` - peer dependencies are enforced during install; when missing or ""incompatible"" it throws exactly this error.You should be able to install it using [`npm install --legacy-peer-deps`](https://docs.npmjs.com/cli/v7/using-npm/config#legacy-peer-deps). Fixing this issue takes a long time; all peer dependencies from all libraries used in the Expo or React Native community now have to _exactly match_ the right versions. It\'s [something more people aren\'t happy about](https://twitter.com/markdalgleish/status/1325755229883621376); but on the other hand; [they had some reasons to do it anyways](https://twitter.com/notbrent/status/1357481746841804800).====='; '@byCedric yes; yarn or legacy peer deps work; but signify there should be a change in package.jsonDo you think i should do a PR for the change i suggested in my original message ? (ie peerdeps using >= instead of ^)====='; '@BigZ that sounds good to me. Thank you!=====']",Build & Install Failure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Peer Dependency Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,D.3
https://github.com/tensorflow/tfjs/issues/5692,Feature: Support vscode-languagedetection on the wasm backend,8,open,2021-10-04T16:25:52Z,2021-10-11T18:02:29Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS Big Sur- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0 - Browser version: node 14.17.5- Tensorflow.js Converter Version: 3.9.0**Describe the current behavior**```Error: Kernel 'StringSplit' not registered for backend 'wasm'      at Engine.runKernel (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4466:19)      at stringSplit_ (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:24073:25)      at Object.stringSplit__op [as stringSplit] (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:5406:29)      at executeOp$h (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6506:35)      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6642:56      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4394:22      at Engine.scopedRun (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4404:23)      at Engine.tidy (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4393:21)      at Object.tidy (node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:10072:19)      at /Users/tyleonha/Code/Microsoft/vscode-languagedetection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6642:30      at executeOp$j (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:6660:7)      at _loop_1 (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7418:31)      at GraphExecutor.processStack (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7444:13)      at GraphExecutor.<anonymous> (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7370:41)      at step (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:81:23)      at Object.next (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:62:53)      at fulfilled (node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:52:58)```**Describe the expected behavior**Not throw error.**Standalone code to reproduce the issue**So I tried switching my library to using the wasm backend by changing this line:https://github.com/microsoft/vscode-languagedetection/blob/main/lib/index.ts#L163(and importing the right package of course)but it doesn't seem like my model is playing nice with the wasm backend.Steps are pretty simple:* clone vscode-languagedetection* `yarn add -D @tensorflow/tfjs-backend-wasm`* change the line above and also the import to use `wasm` instead of `cpu`* run `npm test` in the repoProvide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"[""Hi @TylerLeonhardt. You're seeing this error because the `StringSplit` kernel is not implemented in the wasm backend yet. You can see a list of what ops each backend supports in the [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0) or each backend's `register_all_kernels.ts` file ([here's the one for wasm](https://github.com/mattsoulanille/tfjs/blob/fix_ios_32_bit/tfjs-backend-wasm/src/register_all_kernels.ts#L107)).I'll change this issue to a feature request. It's possible there are other ops in your model that are also not yet supported by wasm. If you add them here; we can better track and implement them.=====""; ""@mattsoulanille unfortunately I don't own the model; it's @yoeo over in https://github.com/yoeo/guesslang/tree/master/guesslang/data/modeland I convert it into a tfjs model.maybe @yoeo could comment on that.=====""; 'I took a look at the [converted model.json](https://github.com/microsoft/vscode-languagedetection/blob/main/model/model.json); and it looks like the following ops are used. Ones with a checkmark are supported by the wasm backend; and the ones with a question mark did not appear in the ops matrix (I think they might correspond to another op) :* ? AddV2 (Add and AddN are supported)* ? BiasAdd* [x] Cast* ? ConcatV2 (Concat is supported)* [x] ExpandDims* [x] GatherNd* [x] GatherV2* [x] GreaterEqual* [x] NotEqual* [x] Pack* [x] Prod* [x] Reshape* [x] Select* ? Shape* [x] Slice* [x] Softmax* [ ] SparseFillEmptyRows* [ ] SparseReshape* [ ] SparseSegmentMean* [ ] SparseSegmentSum* ? StatelessWhile* [x] StridedSlice* [ ] StringToHashBucketFast* ? TensorListFromTensor* ? TensorListReserve* ? TensorListStack* [x] Tile* ? Where* [x] ZerosLike* [x] _FusedMatMulIt looks like sparse ops will also need to be implemented; along with StringToHashBucketFast.====='; 'THank you Matt for generating this list; all ops should be supported.ConcatV2 is same as Concat; AddV2; BiasAdd are same as Add.We might need to register Sparse and string ops; which are CPU ops; they can be supported in wasm by forwarding to CPU impl or implement these ops in c++ @ahmedsabie .The rest or control flow ops are supported since they exist in the model executors.====='; ""Just FYI; I started doing model validation for all models I'm loading  it compares ops used by model to ops registered by currently selected backend: <https://github.com/vladmandic/human/blob/980285d36367d3ae903455a4884ff5d8ff9d0ea1/src/models.ts#L90>=====""; ""@vladmandic; that validation script looks very useful. @pyu10055; it sounds like we could be doing this sort of validation in tfjs to give better error messages about missing ops (i.e. a complete list of missing ops instead of just the first one encountered). To avoid introducing a breaking change; we could run the validation the first time the model is run on a new backend instead of when it's loaded. This avoids the case where a user loads a model that doesn't work on the current backend but then switches to a backend that supports the model. Alternatively; or in addition; we could make it a warning instead of an error just in case the model is somehow able to run anyway ~(although this should never be the case)~.Edit: Thanks @vladmandic for pointing out cases where you may have ops in the model that are missing from tfjs but never run  during execution.=====""; ""> Alternatively; or in addition; we could make it a warning instead of an error just in case the model is somehow able to run anyway (although this should never be the case).@mattsoulanille actually; this does happen sometimes - model may have an execution path that simply does not get triggered.for example; one model I'm using frequently has a missing op `floormod`; but never had an execution error.=====""; '@mattsoulanille @vladmandic backend compatibility validation would be great feature; but it might need to be a runtime validation; as some of the control flow functions or others nodes might not be executed at runtime.  I think it might be good feature for our in-development debugger app @jinjingforever =====']",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5689,WebGPU Performance Issues,14,closed,2021-10-03T12:03:12Z,2021-12-13T11:52:19Z,i just tried new `tfjs-backend-webgpu` *0.0.1-alpha.8* on `tfjs` *3.9.0*   *environment: chrome 96 canary on windows 11*first; great job on adding tons of new ops - from perspective of supported kernel ops; `webgpu` is becoming usable!however; switch to WGSL is anything but useful so far - it comes as a **major performance degredation**  overall; `webgpu` has  gotten slower than `webgl`  (and `webgl` itself has become significantly slower since `tfjs` 3.4.0 - this is discussed separately in several open issues)  not to mention that new work that has gone into `webgl` to make it manageable (enable uniforms) has no effect on `webgpu`comparing warmup times  (fyi; my app by default uses 8 simple models running in parallel - total models size is actually tiny; below 30mb):- `webgl` *(default settings)*  > **14 sec** (double the value with uniforms enabled)- `webgl` with `WEBGL_PACK_DEPTHWISECONV=false` and `WEBGL_USE_SHAPES_UNIFORMS=true`  > **7 sec** (pretty good)- `webgpu` *(default settings)*  > **25 sec** (this is incredibily slow)- `webgpu` with `WEBGPU_USE_GLSL=true`  > **15 sec** (already slower than webgl)- `wasm` (no real warmup; included for refrerence only)  > **2 sec***imo; when developing new backend; goal should be that its better than the previous one - not just that it passes unit testsif `webgpu` is not significantly improved; it will be a d.o.a. once released*cc @qjia7 and @xhcao due to work on webgpu  cc @pyu10055 as assignee on webgl performance degradation issue,"[""@vladmandic Thanks for the good comments and data; as always! Chrome 94 was released on Sep 21; with WebGPU Origin Trial support. This means in addition to Chrome Canary; we may use Chrome Stable (still need option --enable-unsafe-webgpu) for WebGPU experiment now. Unfortunately;  Chrome decided not to support GLSL anymore for WebGPU  (changes happened in master so all the release channels would be impacted; including Canary and Stable); so WGSL is the only one that can be consumed now. We always align well with WebGPU development (My team also heavily contributes to WebGPU spec; CTS and Chrome impl) and started the TFJS GLSL to WGSL transition in June. After fixing many critical perf issues in Chrome (e.g.; workgroup memory init perf regression) together with Google and working around perf issues in TFJS (e.g.; hardware limits); we finished the transition after 3+ months of work.Internally we have daily track of performance against almost all the workloads defined in TFJS e2e benchmarks. Before switching to WGSL; we double-checked there was no performance regression regarding to warmup time and run time. For sure; due to resources; we could only cover very limited platforms (Actually only Intel Coffee Lake and Tiger Lake are under daily test); and very limited workloads. We'd like to hear more details from your side (e.g.; hardware configuration) to understand the regression. We'll investigate right after our holidays (We are off from Oct 1 to 7 for National Day Holidays). BTW;1. The uniform idea was already implemented in WebGPU backend. Google thought it great; so we're bringing it to WebGL backend.2. Comparing with WebGL; compiled shaders couldn't be cached in Chrome. We already raised this implementation issue to Chrome and it's going to take a while for its implementation (not easy). Thanks again for your valuable feedback; hopes to hear more details from your side about warmup regression (e.g.; hardware configuration); and looks forward to more collaborations in the future!=====""; ""Thank you for the notes; here are full details  I've created an automated test so its easy to check all scenarios...  # Performance TestingEnvironment: `tfsj` **3.9.0** and `tfjs-backend-webgpu` **0.0.1-alpha.8**Hardware: Notebook with Intel Coffee Lake i7-8750 and nVidia GTX 1050Ti## Notes- `WebGPU` `GSLS` code has been recently removed and cannot be compared with new `WGSL`- `WebGL` warmup has massive benefit of ~80% of browser shader caching- `WebGPU` warmup has little benefit of ~12% of browser shader caching- `WebGPU` is much faster on inference compared to `WebGL`- `WebGPU` is faster to warmup than `WebGL` in most cases     Except when `WebGL` shaders are cached in browser cross-session and uniforms are enabled   `WebGL` is 2x faster than `WebGPU` in that scenario showing necessity of caching support- `WebGL` performance benefits of uniforms is massive at 2x and I dont see any side-effects     Will this be enabled by default in the future?- `WebGL` packing caused massive performance regression in TFJS in 3.4.0 (3.3.0 is last unaffected version)    There are several open issues; but no progress?- Using `tf.zeros` as input is convinient; but does not produce realistic results    Test using real input image to excercise real-world model execution path## Test Results```js{ message: 'initial'; warmup: 3134; inference: 2638; tfjs: '3.9.0'; backend: 'wasm'; tensors: 304; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 3119; inference: 2618; tfjs: '3.9.0'; backend: 'wasm'; tensors: 304; agent: 'Chrome/94'; env: [] }{ message: 'initial'; warmup: 11836; inference: 61; tfjs: '3.9.0'; backend: 'webgl'; tensors: 304; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 2665; inference: 60; tfjs: '3.9.0'; backend: 'webgl'; tensors: 304; agent: 'Chrome/94'; env: [] }{ message: 'initial'; warmup: 6128; inference: 54; tfjs: '3.9.0'; backend: 'webgl'; tensors: 304; agent: 'Chrome/94'; env: [ { WEBGL_PACK_DEPTHWISECONV: false }; { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'cached'; warmup: 1202; inference: 67; tfjs: '3.9.0'; backend: 'webgl'; tensors: 304; agent: 'Chrome/94'; env: [ { WEBGL_PACK_DEPTHWISECONV: false }; { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'initial'; warmup: 5018; inference: 23; tfjs: '3.9.0'; backend: 'webgpu'; tensors: 304; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 4454; inference: 22; tfjs: '3.9.0'; backend: 'webgpu'; tensors: 304; agent: 'Chrome/94'; env: [] }```## IssuesUsing `WebGPU` backend is causing a lot of warnings although execution seems to work:```text> warning Binding size bigger than maximum uniform buffer binding size: binding 0 given 146313216 bytes; maximum is 16384 bytes    at ValidateBufferBinding (../../third_party/dawn/src/dawn_native/BindGroup.cpp:114)    at ValidateBindGroupDescriptor (../../third_party/dawn/src/dawn_native/BindGroup.cpp:290)    at CreateBindGroup (../../third_party/dawn/src/dawn_native/Device.cpp:1043)```## ReproductionFully automated test in `NodeJS` using `puppeteer` and reproducible anytime  Code available at <https://gist.github.com/vladmandic/fbdcaf7fe2e2add5c33b98936d4d5740>=====""; ""Above post is using single model (can be re-tested using any model; I've used **Inception v4** trained on **ImageNet 1k**)However; when I try `WebGPU` backend on my demo app; it runs at **~3 FPS** average while `WebGL` runs at **~9 FPS**  **that is 300% negative difference** in inference performance!  My best guess is that some ops get executed on CPU thus causing major slowdown  You can try using following URLs:- <https://vladmandic.github.io/human/demo/index.html?backend=webgl>- <https://vladmandic.github.io/human/demo/index.html?backend=webgpu>=====""; '@vladmandic Can you put the Inception v4 model somewhere that I can access? It seems that `http://wyse:10010/models/imagenet/inception-v4/model.json` is in your local server. The webgpu warning seems like a bug in our implementation.And for your demo app; I can reproduce the bad performance for webgpu. Thanks for the reporting. I will take a look. ====='; ""> Can you put the Inception v4 model somewhere that I can access? To keep it reproducible with a readily available public model; you can use any mid-complexity model;  here's an example with **EfficientNet-B5** from **TFhub**: <https://tfhub.dev/google/efficientnet/b5/classification/1>   (just convert from *TFSavedModel* to *TFJSGraphModel* using `tensorflowjs_converter`)  ```js{ message: 'initial'; warmup: 2645; inference: 1908; tfjs: '3.9.0'; backend: 'wasm'; tensors: 394; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 2330; inference: 1808; tfjs: '3.9.0'; backend: 'wasm'; tensors: 394; agent: 'Chrome/94'; env: [] }{ message: 'initial'; warmup: 20148; inference: 107; tfjs: '3.9.0'; backend: 'webgl'; tensors: 394; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 5374; inference: 105; tfjs: '3.9.0'; backend: 'webgl'; tensors: 394; agent: 'Chrome/94'; env: [] }{ message: 'initial'; warmup: 7428; inference: 119; tfjs: '3.9.0'; backend: 'webgl'; tensors: 394; agent: 'Chrome/94'; env: [ { WEBGL_PACK_DEPTHWISECONV: false }; { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'cached'; warmup: 2053; inference: 103; tfjs: '3.9.0'; backend: 'webgl'; tensors: 394; agent: 'Chrome/94'; env: [ { WEBGL_PACK_DEPTHWISECONV: false }; { WEBGL_USE_SHAPES_UNIFORMS: true } ] }{ message: 'initial'; warmup: 5087; inference: 64; tfjs: '3.9.0'; backend: 'webgpu'; tensors: 394; agent: 'Chrome/94'; env: [] }{ message: 'cached'; warmup: 4427; inference: 70; tfjs: '3.9.0'; backend: 'webgpu'; tensors: 394; agent: 'Chrome/94'; env: [] }```As you can see; data is pretty much the same as with **Inception v4** model  (even bigger impact of WEBGL packing and uniforms; but numbers tell the same story)  > And for your demo app; I can reproduce the bad performance for webgpuI've traced it down - there are couple of places where WebGPU is a touch slower than WebGL;  but by far the biggest issue is `tf.image.nonMaxSuppressionAsync`  `WebGL` runs **NMS** in **~25 ms** and `WebGPU` runs **NMS** in **~135 ms** (over **5x** slower)  FYI NMS function params are:```jsboxes.shape = [896; 4]scores.shape = [896]maxOutputSize = 1iouThreshold = 0.1scoreThreshold = 0.2```Also; it seems like `WebGPU` has some additional execution latency?  In more complex models; that is not visible since overall execution time is faster than `WebGL`  But with very simple models that execute in near-real-time `WebGPU` is slower than `WebGL`For example; running a `requestAnimationFrame` loop on [**BlazeFace**](https://tfhub.dev/tensorflow/tfjs-model/blazeface/1/default/1) `model.execute()`:- `WebGL`: 12 ms / frame- `WebGPU`: 20 ms / frame=====""; ""@vladmandic Thanks for the detailed information. I can run your benchmarks using `EfficientNet` now. Some comments below:1. I didn't meet the warning `warning Binding size bigger than maximum uniform buffer binding size: binding 0 given 146313216 bytes; maximum is 16384 bytes` using `EfficientNet`. Maybe it's `Inception v4` specific. If you can further narrow down which op with what kind of input shapes introduce this warning; it will be helpful for us.2. The `cached` is total unimplemented in chrome browser. See [gpuweb:2111](https://github.com/gpuweb/gpuweb/issues/2111); [dawn:549](https://bugs.chromium.org/p/dawn/issues/detail?id=549). So we have to wait the browser to support it. In TFJS level; we will see if we can further reduce the shader variance to reduce the warmup time.3. For `tf.image.nonMaxSuppressionAsync`; it may be not the culprit. Current; `nonMaxSuppressionAsync` only runs on cpu. There is no gpu kernel for it. So I guess the slowness is caused by the previous ops before `nonMaxSuppressionAsync`. `nonMaxSuppressionAsync` triggers all ops in gpu must finish execution. What kind model are you executing before `nonMaxSuppressionAsync`?4. For small size model; webgpu does have performance issue. We notice that the current conv2d/matmul is not efficient for irregular inputs; like M;N is small; K is very large. Or inputs height/width is smaller than filter height/width. We are working on this kind of shapes optimization. Will update here once we have progress. Thanks. =====""; ""> The cached is total unimplemented in chrome browser. See gpuweb:2111; dawn:549. So we have to wait the browser to support itThanks  I took a look and current approach by Chrome team doesnt seem very encouraging and the thread on the spec itself is idle for 8 months :(> In TFJS level; we will see if we can further reduce the shader variance to reduce the warmup timeMuch appreciated!> For small size model; webgpu does have performance issue. We notice that the current conv2d/matmul is not efficient for irregular inputs; like M;N is small; K is very large. Or inputs height/width is smaller than filter height/width. We are working on this kind of shapes optimization. Will update here once we have progress. Thanks.Thanks for confirming  > For tf.image.nonMaxSuppressionAsync; it may be not the culprit. Current; nonMaxSuppressionAsync only runs on cpu. There is no gpu kernel for it. So I guess the slowness is caused by the previous ops before nonMaxSuppressionAsync. nonMaxSuppressionAsync triggers all ops in gpu must finish execution. What kind model are you executing before nonMaxSuppressionAsync?You're right; perf problem is basically **ANY** first TF opereration executed in JS code (outside of the model) - there is a massive latency penalty  In my previous post; `nonMaxSuppressionAsync` just happened to be the one  Simple reproduction:```js  const numIterations = 50;  const arr = new Uint8Array(imageData?.data.buffer); // input data in my case is 4k imageData; but can be any dataset  const t0 = performance.now();  for (let i = 0; i < numIterations; i++) {    const rgba = tf.tensor(arr; [imageData.width; imageData.height; 4]; 'int32'); // create rgba tensor    const rgb = tf.slice3d(rgba; [0; 0; 0]; [-1; -1; 3]); // strip alpha channel    const tensor = tf.expandDims(rgb; 0); // create standard image tensor [1; height; width; 3]    // const data = await tensor.array(); // download data from gpu    tf.dispose([rgba; rgb; tensor]); // just dispose everything  }  const t1 = performance.now();  const avgTime = Math.round((t1 - t0) / numIterations);  console.log({ backend: tf.getBackend(); average: avgTime });```This loop in `WebGPU` is about **3x** slower than in `WebGL`  Setting `tf.ENV.set('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE'; 0)` reduces latency by 50%; but its **still slower** than `WebGL`Note that when disabled line that downloads data back from GPU is enabled; both `WebGL` and `WebGPU` slow down a lot since downloading data is slow (expected)  BUT - overal execution becomes faster for `WebGPU` than `WebGL` - WebGPU is fast; its just initial latency thats a killer  So running models in WebGPU is faster than WebGL; but preparing inputs and processing outputs adds huge penalty at the moment> I didn't meet the warning ...> Maybe it's Inception v4 specific.> If you can further narrow down which op with what kind of input shapes introduce this warning; it will be helpful for us.I'm getting warning even running this simple code from above; no model execution at all  Message is slightly different in Chrome 97 vs 94 and maximum binding size is much bigger; but error is pretty much the same:```logwarning Binding size (146313216) is larger than the maximum binding size (134217728). - While validating entries[0] as a Buffer - While validating [BindGroupDescriptor] against [BindGroupLayout] - While calling CreateBindGroup([BindGroupDescriptor]).DATA:  warning [BindGroup] is an error.    at ValidateObject (../../third_party/dawn/src/dawn_native/Device.cpp:473)    at ValidateSetBindGroup (../../third_party/dawn/src/dawn_native/ProgrammablePassEncoder.cpp:116)    at operator() (../../third_party/dawn/src/dawn_native/ComputePassEncoder.cpp:184)    at FinishInternal (../../third_party/dawn/src/dawn_native/CommandEncoder.cpp:1035)```There are no errors logged in `about://gpu` in Chrome=====""; ""@qjia7 did you have a chance to look at the `webgpu` latency issues i've mentioned? tests above is from 22 days ago.=====""; ""@vladmandic Sorry; I can't reproduce the latency issue you mentioned. From the code snippet you paste; it basically does nothing and is executed instantaneously in my side.```  const numIterations = 50;  const arr = new Uint8Array(imageData?.data.buffer); // input data in my case is 4k imageData; but can be any dataset  const t0 = performance.now();  for (let i = 0; i < numIterations; i++) {    const rgba = tf.tensor(arr; [imageData.width; imageData.height; 4]; 'int32'); // create rgba tensor    const rgb = tf.slice3d(rgba; [0; 0; 0]; [-1; -1; 3]); // strip alpha channel    const tensor = tf.expandDims(rgb; 0); // create standard image tensor [1; height; width; 3]    // const data = await tensor.array(); // download data from gpu    tf.dispose([rgba; rgb; tensor]); // just dispose everything  }  const t1 = performance.now();  const avgTime = Math.round((t1 - t0) / numIterations);  console.log({ backend: tf.getBackend(); average: avgTime });```Some update for the warmup time:- In tfjs level; we are reducing some shader variants firstly from binary ops #5791. Will start others when we get each operator's exact shader compilation time. Some challenges for us are that 1) it's hard to measure each operator's shader compilation time. Details can be found [here](https://bugs.chromium.org/p/chromium/issues/detail?id=1265616); [here](https://github.com/gpuweb/gpuweb/issues/2236). 2) browser devtools also [lack this support](https://bugs.chromium.org/p/dawn/issues/detail?id=1167). Fortunately; chrome just supported it in chrome://tracing. We can continue this work once it's ready in canary.- In webgpu level; google developer has started the work to [cache pipeline in memory](https://bugs.chromium.org/p/dawn/issues/detail?id=549). =====""; '@qjia7 thanks for the update!  and the description of the chromium queue handling is sounds like it could be the same root cause for what i\'m seeing as extreme latency issues  for reproduction; im guessing your test failed since `imageData` was empty; but you can even use `tf.zeros` to reproduce  no model needed; nothing - just a trivial loop  here\'s a live link: <https://vladmandic.github.io/tfjs-utils/src/latency-issue.html>  and output on my notebook:```textuser agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/97.0.4689.0 Safari/537.36tf version: 3.11.0-20211102backend: webgl | total time: 4556 ms | average time: 91 msbackend: webgpu | total time: 13058 ms | average time: 261 ms```basically; for any ""real"" work; `webgpu` is really fast - but for simple stuff that is done in js code outside of the model; latency is a killer - its 300% slower than `webgl`  unfortunately; in real-world; any model inference is followed by some post-processing in js and that is where this impact becomes as showstopper  setting `tf.ENV.set(\'WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE\'; 0)` reduces the latency by 50%;  but its still nowhere as fast as `webgl`====='; '@vladmandic Thanks for your live case. I can reproduce it now. After debugging; I find the time mainly costs on `queue.writeBuffer`. It seems that this API needs to be optimized in browser for big data uploading. I reported a bug to chromium https://bugs.chromium.org/p/chromium/issues/detail?id=1266727.====='; ""@vladmandic Jiawei in our team has fixed the `queue.writeBuffer` issue in chromium. You can retest your example https://vladmandic.github.io/tfjs-utils/src/latency-issue.html with latest chrome canary (--enable-unsafe-webgpu). The webgpu backend becomes much faster than before. In my machine; the latency is not that obvious now. We are also trying to use `mapAsync` instead of `writeBuffer`; which shows better perf than webgl with your example. But it brings another [issue](https://github.com/tensorflow/tfjs/pull/5928#discussion_r765587353). It's still in discussion; but hope we can provide the most performant solution soon.And for the long warmup time; we drafted the prototype for the parallel compilation; showing almost 4x speedup for the warmup time. Currently; we are discussing how to expose this capability uniformly between webgl and webgpu.  Will keep you updated.=====""; ""Thanks @qjia7!I've tested with Chrome 99 and latency is gone - **WebGPU** now performs on-par with **WebGL**I'm looking forward to other proposed changes (once issue is resolved) for `mapAsync` in #5928And I'm guessing you're still discussing how to align changes from #5826 with changes from #5815 ?Anyhow; I'm closing this issue as resolved...=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5689"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5689"">No</a>=====']",Slow Execution,Poor Performance,Incorrect Code Logic,WebGPU,Backend,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5661,"Export function ""nameScope"" in ""tfjs-layers"" module",2,open,2021-09-24T09:42:31Z,2021-09-25T06:20:29Z,"**System information**- TensorFlow.js version: 3.9.0- Are you willing to contribute it: YesFeature: Expose the function ""nameScope"" from ""tfjs-layers/src/common.ts"" to the tfjs-layers module. This enables the serialization/deserialization of custom layers that contain other layers with weights.It will add the function ""nameScope"" to the publicly visible API of tfjs-layers.This will benefit anybody who want the serialize/deserialize more complex custom layers.",['@JanKaul thank you ; do you wish to contribute for this ?====='; 'Yes; I will try to prepare a PR.====='],Reference Error,Crash,Unimplemented Operator,Layer API,API,add function,add function,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5660,yarn test fails on tfjs-backend-webgl on windows,12,open,2021-09-24T07:30:25Z,2021-11-12T07:17:21Z,"Steps to reproduce:1. Get latest tfjs resource code.2. cd tfjs-backend-webgl3.  yarn && yarn testBelow error will be reported```iBazel [3:14PM]: Running :tfjs-backend-webgl2_testLoading:Loading: 0 packages loadedAnalyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (0 packages loaded; 0 targets configured)Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)INFO: Repository python3_interpreter instantiated at:  D:/workspace/jiajia/tfjs/tfjs/WORKSPACE:231:13: in <toplevel>Repository rule http_archive defined at:  C:/users/jqin7/_bazel_jqin7/6lxz3o43/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>Analyzing: target //tfjs-backend-webgl:tfjs-backend-webgl2_test (1 packages loaded; 35 targets configured)INFO: Repository 'python3_interpreter' used the following cache hits instead of downloading the corresponding file. * Hash 'fb1a1114ebfe9e97199603c6083e20b236a0e007a2c51f29283ffb50c1420fb2' for https://www.python.org/ftp/python/3.8.11/Python-3.8.11.tar.xzIf the definition of 'python3_interpreter' was updated; verify that the hashes were also updated.ERROR: An error occurred during the fetch of repository 'python3_interpreter':   Traceback (most recent call last):        File ""C:/users/jqin7/_bazel_jqin7/6lxz3o43/external/bazel_tools/tools/build_defs/repo/http.bzl""; line 121; column 10; in _http_archive_impl                patch(ctx)        File ""C:/users/jqin7/_bazel_jqin7/6lxz3o43/external/bazel_tools/tools/build_defs/repo/utils.bzl""; line 161; column 21; in patch                fail(""Error applying patch command %s:\n%s%s"" %Error in fail: Error applying patch commandif [[ ""$OSTYPE"" == ""darwin""* ]]; then    ./configure --prefix=$(pwd)/bazel_install_py3 --with-openssl=$(brew --prefix openssl)else    ./configure --prefix=$(pwd)/bazel_install_py3fi:checking build system type... x86_64-pc-mingw64checking host system type... x86_64-pc-mingw64checking for python3.8... nochecking for python3... nochecking for python... pythonchecking for --enable-universalsdk... nochecking for --with-universal-archs... nochecking MACHDEP... ""mingw64_nt-10.0-177633""checking for gcc... nochecking for cc... nochecking for cl.exe... noconfigure: error: in `/c/users/jqin7/_bazel_jqin7/6lxz3o43/external/python3_interpreter':configure: error: no acceptable C compiler found in $PATHSee `config.log' for more detailsERROR: Error fetching repository: Traceback (most recent call last):        File ""C:/users/jqin7/_bazel_jqin7/6lxz3o43/external/bazel_tools/tools/build_defs/repo/http.bzl""; line 121; column 10; in _http_archive_impl                patch(ctx)        File ""C:/users/jqin7/_bazel_jqin7/6lxz3o43/external/bazel_tools/tools/build_defs/repo/utils.bzl""; line 161; column 21; in patch                fail(""Error applying patch command %s:\n%s%s"" %Error in fail: Error applying patch commandif [[ ""$OSTYPE"" == ""darwin""* ]]; then    ./configure --prefix=$(pwd)/bazel_install_py3 --with-openssl=$(brew --prefix openssl)else    ./configure --prefix=$(pwd)/bazel_install_py3fi:checking build system type... x86_64-pc-mingw64checking host system type... x86_64-pc-mingw64checking for python3.8... nochecking for python3... nochecking for python... pythonchecking for --enable-universalsdk... nochecking for --with-universal-archs... nochecking MACHDEP... ""mingw64_nt-10.0-177633""checking for gcc... nochecking for cc... nochecking for cl.exe... noconfigure: error: in `/c/users/jqin7/_bazel_jqin7/6lxz3o43/external/python3_interpreter':configure: error: no acceptable C compiler found in $PATHSee `config.log' for more detailsERROR: D:/workspace/jiajia/tfjs/tfjs/tfjs-converter/python/BUILD:6:11: //tfjs-converter/python:python3_runtime depends on @python3_interpreter//:files in repository @python3_interpreter which failed to fetch. no such package '@python3_interpreter//': Error applying patch commandif [[ ""$OSTYPE"" == ""darwin""* ]]; then    ./configure --prefix=$(pwd)/bazel_install_py3 --with-openssl=$(brew --prefix openssl)else    ./configure --prefix=$(pwd)/bazel_install_py3fi:checking build system type... x86_64-pc-mingw64checking host system type... x86_64-pc-mingw64checking for python3.8... nochecking for python3... nochecking for python... pythonchecking for --enable-universalsdk... nochecking for --with-universal-archs... nochecking MACHDEP... ""mingw64_nt-10.0-177633""checking for gcc... nochecking for cc... nochecking for cl.exe... noconfigure: error: in `/c/users/jqin7/_bazel_jqin7/6lxz3o43/external/python3_interpreter':configure: error: no acceptable C compiler found in $PATHSee `config.log' for more detailsERROR: D:/workspace/jiajia/tfjs/tfjs/tfjs-converter/python/BUILD:6:11: //tfjs-converter/python:python3_runtime depends on @python3_interpreter//:python3_bin in repository @python3_interpreter which failed to fetch. no such package '@python3_interpreter//': Error applying patch commandif [[ ""$OSTYPE"" == ""darwin""* ]]; then    ./configure --prefix=$(pwd)/bazel_install_py3 --with-openssl=$(brew --prefix openssl)else    ./configure --prefix=$(pwd)/bazel_install_py3fi:checking build system type... x86_64-pc-mingw64checking host system type... x86_64-pc-mingw64checking for python3.8... nochecking for python3... nochecking for python... pythonchecking for --enable-universalsdk... nochecking for --with-universal-archs... nochecking MACHDEP... ""mingw64_nt-10.0-177633""checking for gcc... nochecking for cc... nochecking for cl.exe... noconfigure: error: in `/c/users/jqin7/_bazel_jqin7/6lxz3o43/external/python3_interpreter':configure: error: no acceptable C compiler found in $PATHSee `config.log' for more detailsERROR: Analysis of target '//tfjs-backend-webgl:tfjs-backend-webgl2_test' failed; build aborted: Analysis failedINFO: Elapsed time: 40.135sINFO: 0 processes.FAILED: Build did NOT complete successfully (1 packages loaded; 35 targets configured)FAILED: Build did NOT complete successfully (1 packages loaded; 35 targets configured)iBazel [3:15PM]: Error running Bazel exit status 1error Command failed with exit code 4.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.","['cc @mattsoulanille ====='; ""Thanks for reporting this @qjia7. This looks like it's caused by the recent change that builds tfjs-converter with Bazel. That change builds specific versions of python 2 and 3 from source for consistency across machines. We were hoping this would make setting up converter more convenient; but it seems like in this case; it's getting in the way of testing other packages. I'll bring this up with the team.I expect that if you install the required libraries for compiling python2 and 3 from source;  the webgl tests will run. We've [documented this on the `tfjs-converter` readme](https://github.com/tensorflow/tfjs/blob/master/WORKSPACE#L287-L297); but perhaps it should be moved up to the main readme since it affects other packages as well. [Here's a  link to Python's docs on how to build Python from source on Windows](https://devguide.python.org/setup/#windows).=====""; 'Instead of building python from source; can we simply provide related binaries for consistency? In Chromium development; depot_tools (https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/depot_tools_tutorial.html) is used for this sake. To maintain the build on various platforms are much harder than maintaining the binaries. For this case; if I run ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat"" to set the compiler path; I may advance a bit; then I will encounter the below error (related to pthread support):checking for pthread_create in -lcma... noconfigure: error: could not find pthreads on your systemERROR: Analysis of target \'//tfjs-backend-webgl:tfjs-backend-webgl2_test\' failed; build aborted: Analysis failed.Windows is a very important platform for us to develop and test. Will you consider adding a Windows bot to catch such kind of regression?====='; '@gyagp Using a precompiled binary instead of building from source is a much better approach. Thanks for the recommendation! This has been implemented in #5699.We do not currently have plans to add Windows or MacOS to CI testing; but I will bring this up with the team since catching these kinds of regressions is important. Our CI is currently based on GCP Cloud Build; which (I believe) does not support Windows or MacOS; so this may require adopting a different CI system. At the moment; a significant amount of our CI logic is tied to `cloudbuild.yml` files; so if we go route of using a different CI system; it will likely be after we have converted all packages to build and test with Bazel and can run the tests with a single Bazel command.====='; '@mattsoulanille Thanks for fixing the python issue! After the fix; I encountered another error when running ""yarn test"" as below：INFO: Build completed successfully; 1 total actionINFO: Build completed successfully; 1 total actioniBazel [6:20PM]: Starting...################################################################################# Did you know iBazel can invoke programs like Gazelle; buildozer; and         ## other BUILD file generators for you automatically based on bazel output?     ## Documentation at: https://github.com/bazelbuild/bazel-watcher#output-runner  #################################################################################\'external\' is not recognized as an internal or external command;operable program or batch file.Do you use Compute Engine in GCP for CI? I think it supports Windows: https://cloud.google.com/migrate/compute-engine/docs/4.8/reference/supported-os-versions. ====='; ""I'm able to reproduce this error; and I'm working on fixing it. Thanks for reporting it; @gyagp!For CI; we don't actually use Compute Engine. Instead; we use [Cloud Build](https://cloud.google.com/build); which; as far as I can tell; only supports Linux containers (I'll try Windows anyway; but I don't expect it to work). There are other containers for building Windows-specific things on GCP; like the [dotnet builder](https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/dotnet); but they are based on Linux. We might be able to use something like [remote-builder](https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/remote-builder) to launch a Windows VM from a Cloud Build docker container; but I'll have to investigate more.=====""; 'I have a workaround that I\'m still evaluating as a potential solution to this problem. It has a few limitations. First; builds must run in Windows Subsystem for Linux. Second; headless Karma tests are unsupported; so tests will launch in visible Chrome windows. Here\'s how to use it:1. Install Debian in WSL2 (I have not tested WSL1; but it may work). [It\'s available from the Microsoft store](https://www.microsoft.com/en-us/p/debian/9msvkqc78pk6?activetab=pivot:overviewtab). You may need to install WSL2 first by following [these instructions](https://docs.microsoft.com/en-us/windows/wsl/install).2. Once you have a working Debian install in WSL; install node and yarn with `sudo apt update && sudo apt install nodejs && npm i -g yarn`. If you need to reset the root debian password; you can get a root shell from command prompt with `wsl -u root`.3. Make sure Chrome is installed on Windows. Then; find the path to `chrome.exe`. It\'s probably `C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`.4. Set the bash variable `CHROME_BIN` to the unix-style path to `chrome.exe`. For the above path; this command does it: `export CHROME_BIN=/mnt/c/Program\\ Files/Google/Chrome/Application/chrome.exe`. I recommend adding this to `~/.bashrc` with `echo ""export CHROME_BIN=/mnt/c/Program\\ Files/Google/Chrome/Application/chrome.exe"" > ~/.bashrc`3. Add the line `browser = ""Chrome;""` to [`tools/tfjs_web_test.bzl`](https://github.com/tensorflow/tfjs/blob/master/tools/tfjs_web_test.bzl#L77-L80) on line 79. This prevents Karma from defaulting to ""ChromeHeadless"".```    _make_karma_config(        name = config_file;        args = args;+       browser = ""Chrome"";    )```4. Run tests with `bazel run` or `ibazel run`. Many of the test scripts in `package.json` files use `bazel test`; which does not appear to work in this configuration; so these tests can not be run with `yarn test`. Instead; substitute `run` for `test` in them. For example; in [`tfjs-core`](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/package.json#L69); `yarn test-browser` runs `bazel test --flaky_test_attempts=3 :tfjs-core_test --test_output=all`Instead; you should run`yarn bazel run --flaky_test_attempts=3 :tfjs-core_test --test_output=all`directly from the command line.**Note that when using `bazel run` on karma tests; the tests will only start after you click `DEBUG` on the browser page that opens.**I have tested the above command for running core tests; and I\'ve tested running webgl tests. The webgl tests actually use `ibazel run` instead of `ibazel test`; so they can be launched with `yarn test`.The downside of this workaround is that `bazel test` does not work; so this isn\'t a good solution to the problem. I\'ll see if there\'s a way to get `bazel test` working or if there\'s a way to get karma_web_test working from Windows without using WSL.@gyagp @qjia7 ====='; ""@mattsoulanille For error `'external' is not recognized as an internal or external command;`; I have some findings; maybe it caused by Bazel generated middleware file which used default ‘/’ not ‘\\’ for Windows path separators.For example; on my machine the middleware file locates in C:\\Users\\haoyunfe\\AppData\\Local\\Temp>bazel_script_path372957383.bat ```@echo offcd C:/users/haoyunfe/_bazel_haoyunfe/p6pet2hx/execroot/tfjs  SET BUILD_WORKING_DIRECTORY=C:/workplace/github/tfjs/tfjs-backend-webgl    …    …    SET XML_OUTPUT_FILE=bazel-out/x64_windows-fastbuild/testlogs/tfjs-backend-webgl/tfjs-backend-webgl2_test/test.xml  external/bazel_tools/tools/test/tw.exe tfjs-backend-webgl/tfjs-backend-webgl2_test %*```When execute this batch file; in the last line; folder “external” was recognized as a command due to a “/” behind the name.So error message returned “'external' is not recognized as an internal or external command;”.I have no confidence to say that is a Bazel related bug; but we may try to change code in Bazel to rebuild a bazel.exe?Like using back slashes in the cmd path for Windows?https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/runtime/commands/RunCommand.java#L662~L677=====""; ""@haoyunfeix; Thanks for investigating this. I think the error you're seeing; (`'external' is not recognized as an internal or external command;`) only happens when running `ibazel`. I've run tests with `bazel`; and I get different errors related to the rules that are used in the BUILD files.I looked into the rule we're using to run karma; [karma_web_test](https://bazelbuild.github.io/rules_nodejs/Karma.html#karma_web_test); and [I don't think it supports Windows](https://github.com/bazelbuild/rules_nodejs/issues/1429).I tried using [`karma_web_test_suite`](https://bazelbuild.github.io/rules_nodejs/Karma.html#karma_web_test_suite) instead; which does support Windows; and [I actually had success](https://github.com/mattsoulanille/tfjs/tree/karma_windows). I was able to run the tfjs-core tests on chromium. However; there's an issue with using `karma_web_test_suite`. When running browser tests; it downloads the browser instead of using the locally-installed browser. This is great for test reproducibility; but the only [supported browsers](https://github.com/bazelbuild/rules_webtesting/tree/master/browsers) are Chromium and Firefox (chrome-external is supported by rules_webtesting but not by karma_web_test_suite). Several of our tests fail in the Chromium browser that `karma_web_test_suite` downloads; but they pass in Chrome (e.g. `fromPixels for HTMLVideoElement`).In addition to these Chromium test failures; several other test targets fail. For example; tests using `nodejs_test` do not find any specs to run.For now; I think the most stable development environment for Windows is WSL / WSL2. #5745 fixes the webtesting issues in WSL and makes the setup easier. You can try it out now (check DEVELOPMENT.md for instructions) or once it's merged (I'll ping this issue); and please let me know if it fixes this issue for you. Thanks!=====""; ""#5745; which fixes browser tests in WSL; has been merged. @qjia7 @haoyunfeix @gyagp please give it a try and let me know whether it works for you and whether it's a good enough solution to close this issue. Thanks!=====""; 'Verified and it also works for WebGPU backend testing using Bazel tool.Another question: Will it support `--grep` in command line like the describe in https://github.com/tensorflow/tfjs/blob/master/DEVELOPMENT.md#testing ?====='; ""@mattsoulanille I tried the new way to build on wsl. It works for me. Thanks for your great efforts on this. I have the save question as Yunfei. How should we use `--grep`? It seems that `yarn test --grep='packed'` not work for me.And another issue I met is it always reports that `Google Chrome can't read and write to its data directory /tmp/karma-16220143`. Failed to create data directory. Currently; I just click ok and ignore it.=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGL,Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5655,Unhandled promise rejection: Error: tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported,1,closed,2021-09-23T14:09:17Z,2021-09-23T16:44:53Z,"I have a created react native expo managed app. And tried to use mobile net. ```import * as mobilenet from '@tensorflow-models/mobilenet';import * as tf from '@tensorflow/tfjs';import { decodeJpeg } from '@tensorflow/tfjs-react-native';import * as FileSystem from 'expo-file-system';import * as ImagePicker from 'expo-image-picker';import React; { useEffect; useState } from 'react';import { Button; Image; StyleSheet; View } from 'react-native';const BACKEND_TO_USE = 'rn-webgl';export default function App() {  const [prediction; setPrediction] = useState<[]>();  const [image; setImage] = useState<string>('');  async function init() {    console.log('Loading mobilenet...');    const model = await mobilenet.load();    const fileUri = image;    const imgB64 = await FileSystem.readAsStringAsync(fileUri; {      encoding: FileSystem.EncodingType.Base64;    });    const imgBuffer = tf.util.encodeString(imgB64; 'base64').buffer;    const newData = new Uint8Array(imgBuffer);    const imageTensor = decodeJpeg(newData); // transforms byte array into 3d tensor    const prediction = await model.classify(imageTensor);    console.log(prediction);    // Use prediction in app.    setPrediction(prediction);  }  const pickImage = async () => {    let result = await ImagePicker.launchImageLibraryAsync({      mediaTypes: ImagePicker.MediaTypeOptions.All;      quality: 1;    });    console.log(result);    if (!result.cancelled) {      setImage(result.uri);    }  };  useEffect(() => {    async function init() {      await tf.ready();    }    init();  }; []);  return (    <View style={styles.container}>      <Button title=""Pick image"" onPress={pickImage} />      <Button title=""RUn"" onPress={init} />    </View>  );}const styles = StyleSheet.create({  container: {    flex: 1;    backgroundColor: '#fff';    alignItems: 'center';    justifyContent: 'center';  };});```**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No - OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: android emulator- TensorFlow.js installed from (npm or script link): yarn- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**`mobilenet.load()` is giving error [Unhandled promise rejection: Error: tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported.]at node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:5834:34 in __generator$argument_1at node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:84:17 in stepat node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:58:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow\tfjs-backend-webgl\dist\tf-backend-webgl.node.js:54:11 in __awaiterat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3700:31 in __generator$argument_1at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:129:21 in stepat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:71:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:57:11 in __awaiterat node_modules\@tensorflow-models\mobilenet\dist\index.js:177:45 in __generator$argument_1at node_modules\@tensorflow-models\mobilenet\dist\index.js:48:17 in stepat node_modules\@tensorflow-models\mobilenet\dist\index.js:20:47 in fulfilledat node_modules\react-native\node_modules\promise\setimmediate\core.js:37:13 in tryCallOneat node_modules\react-native\node_modules\promise\setimmediate\core.js:123:24 in setImmediate$argument_0    at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:130:14 in _callTimerat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:181:14 in _callImmediatesPassat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:441:30 in callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:387:6 in __callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:135:6 in __guard$argument_0at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:364:10 in __guardat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:134:4 in flushedQueue**Describe the expected behavior**mobile net should load **Standalone code to reproduce the issue**My dependency list ```""@react-native-async-storage/async-storage"": ""~1.15.0"";    ""@tensorflow-models/mobilenet"": ""^2.1.0"";    ""@tensorflow/tfjs"": ""^3.9.0"";    ""@tensorflow/tfjs-react-native"": ""^0.7.0"";    ""expo"": ""~42.0.1"";    ""expo-camera"": ""~11.2.2"";    ""expo-file-system"": ""~11.1.3"";    ""expo-gl"": ""~10.4.2"";    ""expo-gl-cpp"": ""~10.4.1"";    ""expo-image-picker"": ""~10.2.2"";    ""expo-status-bar"": ""~1.0.4"";    ""jpeg-js"": ""^0.4.3"";    ""react"": ""16.13.1"";    ""react-dom"": ""16.13.1"";    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-42.0.0.tar.gz"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-web"": ""~0.13.12""```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5655"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5655"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,WebGL,Backend,change backend,changing backend,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5652,[wasm] OOM for some conv2d input on build bots,2,closed,2021-09-23T02:24:44Z,2021-11-19T21:31:27Z,Test case: https://github.com/tensorflow/tfjs/pull/5637/commits/976623953e724be45730f8259a0d0944243d4afcBuild log: https://console.cloud.google.com/cloud-build/builds/46b2a97c-0c12-494a-9e7a-51e1627fe072;step=14?project=learnjs-174218```$ ./scripts/test-ci.sh$ tsc && ts-node --transpile-only --skip-ignore -P tsconfig.test.json src/test_node.tsPlatform node has already been set. Overwriting the platform with [object Object].Started(node:48) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 uncaughtException listeners added. Use emitter.setMaxListeners() to increase limit(node:48) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 unhandledRejection listeners added. Use emitter.setMaxListeners() to increase limitXNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_average_pooling2d_nhwc_f32 is not successful. XNN status for xnn_create_max_pooling2d_nhwc_f32 is not successful. XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.XNN status for xnn_create_max_pooling2d_nhwc_f32 is not successful. XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.XNN status for xnn_setup_max_pooling2d_nhwc_f32 is not successful. Got status 2. Use -c dbg to see XNN logs.failed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:54                throw ex;                ^RuntimeError: abort(RangeError: WebAssembly Instantiation: Out of memory: wasm memory). Build with -s ASSERTIONS=1 for more info.    at abort (/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:9596)    at /workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:11594error Command failed with exit code 7....................................**..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 7.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```,"['Fixed in #5852.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5652"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5652"">No</a>=====']",Out of Mermory,Poor Performance,Incorrect Code Logic,Wasm,Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.2] JS Variable Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",B,A.4
https://github.com/tensorflow/tfjs/issues/5641,wasm backend produces incorrect results for int32 tensors,3,closed,2021-09-19T17:14:31Z,2021-10-17T11:51:46Z,i was wondering why my app produces results with lower precision when using `wasm` backend and finally narrowed it down to `wasm` handling of **int32** tensors*environment: tfjs 3.9.0 on ubuntu 21.04**note: ive tried wasm with and without simd support*below is a very simple reproduction  (and i suggest to add something like this to automated tests in the future)  note that `tf.sum()` is just easiest way to reproduce; this issue is accross the board in tfjs```jsconst data = Array.from(imageData.data); // in my case data is imageData array so each value is just 0..255; but can be any datasetdata.length = 1024 * 1024 * 4; // lets crop array to specific size; just for testlet sumJS = 0;for (let i = 0; i < data.length; i++) sumJS += data[i];const tI32 = tf.tensor(data; [data.length]; 'int32');const tF32 = tf.tensor(data; [data.length]; 'float32');console.log({ backend: tf.getBackend(); arrayLength: data.length; jsSum: sumJS; tfSumI32: tf.sum(tI32).dataSync()[0]; tfSumF32: tf.sum(tF32).dataSync()[0] });```this is ok output when using `tfjs-node` as both js and tf produce same sum:```js{  backend: 'tensorflow';  arrayLength: 4194304;  jsSum: 1000114465;  tfSumI32: 1000114465;  tfSumF32: 1000114432 // sum of float is slightly off; but close}```but when using `wasm` backend; its completely **broken**:```js{  backend: 'wasm';  arrayLength: 4194304;  jsSum: 1000114465;  tfSumI32: 66326844; // completely broken  tfSumF32: 1023692544 // slightly off as well; but close}```and this is not a case of `int32` overflow as its reproducible with much lower values as well:(it seems that error occur with tensors with more than 64k values)```js {  backend: 'wasm';  arrayLength: 65536;  jsSum: 16042946;  tfSumI32: 16042946; // this is correct  tfSumF32: 16042946}``````js{  backend: 'wasm';  arrayLength: 131072;  jsSum: 31840717;  tfSumI32: 24302074; // this is incorrect!!!  tfSumF32: 31826932}```and those numbers are faaaar off any `int32` limitsand yes; i've checked that tensors get created correctly in both cases by downloading all values from it using `dataSync()` and comparing to original array,"[""one more trivial reproduction that shows exactly when errors starts to occure:```jsconst fs = require('fs');const tf = require('@tensorflow/tfjs');const wasm = require('@tensorflow/tfjs-backend-wasm');async function main() {  wasm.setWasmPaths('node_modules/@tensorflow/tfjs-backend-wasm/dist/');  await tf.setBackend('wasm');  await tf.ready();  console.log('tfjs:'; { version: tf.version_core; backend: tf.getBackend() });  const t = {};  const data = fs.readFileSync('dist/tfjs.esm.js.map');  for (let i = 0; i <= 22; i++) {    const arr = Array.from(data);    const size = 2 ** i;    arr.length = size;    t.i32 = tf.tensor(arr; [size]; 'int32');    t.f32 = tf.tensor(arr; [size]; 'float32');    t.sumI = tf.sum(t.i32);    t.sumF = tf.sum(t.f32);    const JS = arr.reduce((prev; curr) => prev += curr; 0);    const I32 = t.sumI.dataSync()[0];    const F32 = t.sumF.dataSync()[0];    console.log({ size; JS; I32; F32; ok: JS === I32 });    Object.keys(t).forEach((tensor) => tf.dispose(t[tensor]));  }}main();```output:```jstfjs: { version: '3.9.0'; backend: 'wasm' }{ size: 1; JS: 123; I32: 123; F32: 123; ok: true }{ size: 2; JS: 133; I32: 133; F32: 133; ok: true }{ size: 4; JS: 197; I32: 197; F32: 197; ok: true }{ size: 8; JS: 564; I32: 564; F32: 564; ok: true }{ size: 16; JS: 1180; I32: 1180; F32: 1180; ok: true }{ size: 32; JS: 2319; I32: 2319; F32: 2319; ok: true }{ size: 64; JS: 5041; I32: 5041; F32: 5041; ok: true }{ size: 128; JS: 10828; I32: 10828; F32: 10828; ok: true }{ size: 256; JS: 22156; I32: 22156; F32: 22156; ok: true }{ size: 512; JS: 45456; I32: 45456; F32: 45456; ok: true }{ size: 1024; JS: 91536; I32: 91536; F32: 91536; ok: true }{ size: 2048; JS: 184851; I32: 184851; F32: 184851; ok: true }{ size: 4096; JS: 371489; I32: 371489; F32: 371489; ok: true }{ size: 8192; JS: 742567; I32: 742567; F32: 742567; ok: true }{ size: 16384; JS: 1486349; I32: 1486349; F32: 1486349; ok: true }{ size: 32768; JS: 2999662; I32: 2999662; F32: 2999662; ok: true }{ size: 65536; JS: 6039441; I32: 6039441; F32: 6039441; ok: true }{ size: 131072; JS: 12112329; I32: 12112329; F32: 12112329; ok: true }{ size: 262144; JS: 23178955; I32: 19976980; F32: 23176744; ok: false }{ size: 524288; JS: 46226017; I32: 28329405; F32: 46208756; ok: false }{ size: 1048576; JS: 91071178; I32: 36552479; F32: 91093240; ok: false }{ size: 2097152; JS: 180992048; I32: 44872176; F32: 181083904; ok: false }{ size: 4194304; JS: 358417967; I32: 53091244; F32: 356742528; ok: false }```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5641"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5641"">No</a>====='; ""@jinjingforever there's a leftover line in the fix:```jsconsole.log('reduceshape'; reduceShape);````=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,add support for datatype,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5632,"[webgl] ""Switching WebGL + CPU backends"" Tests do not Run",2,open,2021-09-16T17:06:03Z,2021-09-20T17:52:22Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Debian 5.10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): Local build- TensorFlow.js version (use command below): Tried on [fe0dcb738daec71f950d93fd466825e21a915e25](https://github.com/tensorflow/tfjs/commit/fe0dcb738daec71f950d93fd466825e21a915e25) and [19ca872919c196787c2fbf9a8aa57000cd12dff0](https://github.com/tensorflow/tfjs/commit/19ca872919c196787c2fbf9a8aa57000cd12dff0).- Browser version: Chrome 93- Tensorflow.js Converter Version: n/a**Describe the current behavior**[Switching WebGL + CPU backends](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine_test.ts#L587-L601) tests do not run when running `karma start` (or `yarn test-dev`) in `tfjs-backend-webgl`. They seem to not run because `_engine.ENGINE.backendNames()` does not contain `webgl` even though the webgl tests are being run.**Describe the expected behavior**The above tests run.**Standalone code to reproduce the issue**In `tfjs-backend-webgl`; ```yarn build-depsyarn karma start```Then; in the Chrome debugger; open `engine_test.js` and set a breakpoint in the `predicate` function of the `Switching WebGL + CPU backends` tests. Check `_engine.ENGINE.backendNames()` and you'll see that `webgl` is missing.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.![webgl-no-test](https://user-images.githubusercontent.com/1474501/133653885-d68894d7-b523-4344-84f3-723895a63ec4.png),"['@mattsoulanille backend webgl register itself in the tfjs-backend-webgl/src/base.ts;is that file included in the test suite?====='; ""@pyu10055 backend webgl does seem to register itself in `base.ts`; but this happens after the engine test runs; which seems to be the wrong order. I'll see if I can get it to run before.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] WebGL backend not initialized""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5624,multiple and various tfjs build issues,1,open,2021-09-14T15:14:00Z,2021-10-17T12:47:04Z,"can someone take a look at the **tfjs build issues**? none are fatal;  but sheer number and variety is *not a good indicator*using `tfjs` from main branch...- obsolete packages:```logwarning @babel/polyfill@7.12.1: 🚨 This package has been deprecated in favor of separate inclusion of a polyfill and regenerator-runtime (when needed). See the @babel/polyfill docs (https://babeljs.io/docs/en/babel-polyfill) for more information.warning @babel/polyfill > core-js@2.6.12: core-js@<3.3 is no longer maintained and not recommended for usage due to the number of issues. Because of the V8 engine whims; feature detection in old core-js versions could cause a slowdown up to 100x even if nothing is polyfilled. Please; upgrade your dependencies to the actual version of core-js.warning rollup > fsevents@2.1.3: ""Please update to latest v2.3 or v2.2""warning rollup-plugin-babel@4.4.0: This package has been deprecated and is no longer maintained. Please use @rollup/plugin-babel.```- incorrectly marked external packages:    (this one is related to #4745 which did not resolve it fully)```logsrc/index.ts → dist/tf-backend-wasm.node.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencycreated dist/tf-backend-wasm.node.js in 9.9s```- obsolete api:```logwarning karma-typescript > url > querystring@0.2.0: The querystring API is considered Legacy. new code should use the URLSearchParams API instead.(node:3683) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.```- incorrect dependencies:```logwarning "" > @rollup/plugin-typescript@3.1.1"" has incorrect peer dependency ""rollup@^1.20.0"".```- missing dependencies:```logwarning "" > @rollup/plugin-typescript@3.1.1"" has unmet peer dependency ""tslib@*"".```- incorrect exports:```logWARNING:  ../link-package-core/node_modules/@tensorflow/tfjs-core/dist/hash_util.js (23:12) 'default' is not exported by '../link-package-core/node_modules/long/src/long.js'```- incorrect imports:```logWARNING:  'setBackend' and 'pow' are imported from external module '@tensorflow/tfjs-core' but never used```- bad code:```log > tfjs-layers/src/layers/normalization.ts:586:22: warning: Comparison using the ""!=="" operator here is always true    586 │             this.axis !== [nDims - 1]) {```- incorrect tree shaking:this was mentioned numerous times and never properly resolved. latest (still open) is #5182.and as a final note; build process works only with `yarn` - but then scripts sometimes call `npm instead`!?","[""FYI; I've created a custom TFJS build process <https://github.com/vladmandic/tfjs>:- runs on unmodified `tfjs` sources;- builds 10x faster- produces bundle which is 5x smaller: The main difference is that it always resolves to actual `.ts` sources using custom resolver in the build process- it doesnt allow any imports from `/dist`- it doesnt allow any imports from symlinked or virtual packagesFor example; original `tf.es2017.js` is **3.9MB** without `wasm` and `webgpu` backends which are another **2MB** on top  This new `tfjs.esm.js` is `2.5MB` *(1.3MB minified)* with `wasm` and `webgpu` included  =====""]",Build & Install Failure,Build & Initialization Failure,Unknown,Wasm,Backend,,,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Multiple Build Issues""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,E
https://github.com/tensorflow/tfjs/issues/5623,build issues for tfjs-backend-wasm,2,closed,2021-09-14T12:55:10Z,2021-09-14T15:12:08Z,i've previously reporteed #4745 which resolved it partially; but building `tfjs-backend-wasm` still throws a lot of unecessary warnings for unresolved dependencies which should be marked as external  tfjs version: git main branch dated 9/14/2021```logsrc/index.ts → dist/tf-backend-wasm.node.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencycreated dist/tf-backend-wasm.node.js in 9.9ssrc/index.ts → dist/tf-backend-wasm.min.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencyWARNING:  No name was provided for external module 'os' in output.globals – guessing 'os'WARNING:  Creating a browser bundle that depends on Node.js built-in modules ('path' and 'os'). You might need to include https://www.npmjs.com/package/rollup-plugin-node-builtinscreated dist/tf-backend-wasm.min.js in 9.4ssrc/index.ts → dist/tf-backend-wasm.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencyWARNING:  No name was provided for external module 'os' in output.globals – guessing 'os'WARNING:  Creating a browser bundle that depends on Node.js built-in modules ('path' and 'os'). You might need to include https://www.npmjs.com/package/rollup-plugin-node-builtinscreated dist/tf-backend-wasm.js in 5.5ssrc/index.ts → dist/tf-backend-wasm.es2017.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencyWARNING:  No name was provided for external module 'os' in output.globals – guessing 'os'WARNING:  Creating a browser bundle that depends on Node.js built-in modules ('path' and 'os'). You might need to include https://www.npmjs.com/package/rollup-plugin-node-builtinscreated dist/tf-backend-wasm.es2017.js in 5.6ssrc/index.ts → dist/tf-backend-wasm.es2017.min.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencyWARNING:  No name was provided for external module 'os' in output.globals – guessing 'os'WARNING:  Creating a browser bundle that depends on Node.js built-in modules ('path' and 'os'). You might need to include https://www.npmjs.com/package/rollup-plugin-node-builtinscreated dist/tf-backend-wasm.es2017.min.js in 7.5ssrc/index.ts → dist/tf-backend-wasm.fesm.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencycreated dist/tf-backend-wasm.fesm.js in 4.8ssrc/index.ts → dist/tf-backend-wasm.fesm.min.js...WARNING:  'os' is imported by wasm-out/tfjs-backend-wasm-threaded-simd.js; but could not be resolved – treating it as an external dependencyWARNING:  'os' is imported by os?commonjs-external; but could not be resolved – treating it as an external dependencycreated dist/tf-backend-wasm.fesm.min.js in 6.4s```,"['opening different issue for build problems====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5623"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5623"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unresolved Dependency""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5613,tensor dimension need to be 2d,1,closed,2021-09-13T16:50:04Z,2021-09-13T22:53:27Z,https://github.com/tensorflow/tfjs/blob/4dfcd782deda0a9fbe9f3cc5c60bbf4293e14577/tfjs-core/src/ops/diag.ts#L41-L43Code snippet error on official docs of tensorflowjs [here](https://js.tensorflow.org/api/latest/#diag)**It should be**` const x = tf.tensor2d([1; 2; 3; 4; 5; 6; 6; 8]; [4; 2]) tf.diag(x).print()`,['Thanks for reporting ; submitted a fix ====='],Document Error,Document Error,Confused Document,Operator,API,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5612,Installing qna model using npm doesn't NOT install the last version of the sources but older version with outdated version of tensorflow,5,open,2021-09-13T13:05:24Z,2021-09-13T21:14:11Z,<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): tfjs-node- TensorFlow.js version: 3.9.0- CUDA/cuDNN version: NAWhen installing qna model using npm install @tensorflow-models/qna; the package installed is referencing tensor v1.3.3 and NOT last major release v3.0.0 as indicated inside the last version of the source package.json. I assume NPM registry is NOT referencing the last commit of your sources.Thanks,"[""Thanks for the issue report; @fbx31. Can you please check that the version of `@tensorflow-models/qna` you've installed is `1.0.0`? `1.0.0` [lists `@tensorflow/tfjs-core` and `@tensorflow/tfjs-converter` as peer dependencies](https://github.com/tensorflow/tfjs-models/blob/master/qna/package.json#L15-L18); so you should be able to (and will need to) install a version of your choice of those packages to use automl.It's also possible you're using another package that lists core and cpu as dependencies; and that may be why you're seeing them installed at an earlier version than expected.=====""; 'Hello;Starting from fresh install; installing @tensorflow/tfjs-node (v3.9.0 OK); then installing qna model doing npm i @tensorflow-models/qna.I have the version 1.0.0 of the package installed with following peer dependencies:     ""peerDependencies"": {    ""@tensorflow/tfjs-core"": ""^1.5.2"";    ""@tensorflow/tfjs-converter"": ""^1.5.2""  };BUT in github; when i look the package.json in sources of qna; i have the following dependencies:  ""peerDependencies"": {    ""@tensorflow/tfjs-converter"": ""^3.0.0"";    ""@tensorflow/tfjs-core"": ""^3.0.0""  }That\'s why I said that npm package downloaded of qna seems to not be the last commit in github.Thanks====='; '@fbx31 thats correct ; we need a fresh build to npm ; @mattsoulanille can you please help push a fresh build to npm ?====='; 'My bad! I should have checked the NPM package; not GitHub. We can publish an updated build to NPM.====='; 'Thanks a lot; I;ll wait for the update instead of trying to rebuild it by myself :-)Have a nice day.=====']",Build & Install Failure,Build & Initialization Failure,Untimely Update,TF(CPU),Backend,publish new package,publish new package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Outdated Version Installation""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.3
https://github.com/tensorflow/tfjs/issues/5611,Same image after first prediction returns different [x;y;z] values for face-landmarks-detection model,3,open,2021-09-13T10:53:42Z,2021-09-21T13:59:24Z,"**System information**- Custom code- OS Platform and Distribution (MacOs BigSur):- TensorFlow.js installed from npm (latest)- Browser version: Chrome 93 StableFrom my package.json:``` ""dependencies"": {    ""@material-ui/core"": ""^4.12.3"";    ""@material-ui/icons"": ""^4.11.2"";    ""@tensorflow-models/face-landmarks-detection"": ""0.0.1"";    ""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"";    ""@tensorflow/tfjs-backend-wasm"": ""^3.8.0"";    ""@tensorflow/tfjs-backend-webgl"": ""^2.4.0"";    ""@tensorflow/tfjs-converter"": ""^2.4.0"";    ""@tensorflow/tfjs-core"": ""^2.4.0"";    ""axios"": ""^0.21.1"";    ""react"": ""^17.0.2"";    ""react-dom"": ""^17.0.2"";    ""react-redux"": ""^7.2.4"";    ""react-router-dom"": ""^5.2.0"";    ""react-scripts"": ""^4.0.3""  };```I have an image uploader using reactJs. All I'm doing is selecting an image file; creating an image element from with the img.src as the base64 encoded data of the image and passing it through the model to detect the landmark coordinates for that image.```async function predictImage(file) {      return new Promise((rs; rj) => {        try {          const imageElement = document.createElement(""img"");          imageElement.src = file;          imageElement.onload = async () => {           const preds = await model.estimateFaces({             input: imageElement;           });          return rs();        };        } catch (err) {          return rj(err);        }      });    }```I then store a selected set from the scaled mesh obj in a database for later comparison.Given image A and image B;Image A: ![jackie1](https://user-images.githubusercontent.com/5845311/133072304-a54731e8-5463-4cf6-a2c8-d5ef5dd1774a.jpeg)Image B: ![salma1](https://user-images.githubusercontent.com/5845311/133072332-22fead35-b7e5-4d14-b845-b59c79947d0a.jpg)If I run image A for the first time after loading the model I get a set of values; say from key point 6 from the [face mesh](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/mesh_map.jpg) provided; `[674.8491821289062;452.3935241699219;-8.618978500366211]`. I then upload image B and run it through the model without restarting or refreshing the page. If I then upload image A again; the values for the same key point 6 are now different from the first prediction of image A; `[336.6038818359375;387.3382263183594;-14.38083553314209]`.More extensive list of the key point values from initial upload and secondary upload:![Screenshot 2021-09-13 at 12 03 11](https://user-images.githubusercontent.com/5845311/133072756-7c681f50-dfd1-4828-b9e8-8e37cd05e66a.png)If I then refresh my page and upload image A; the values will match the first upload from the previous attempt.I have checked my upload logic to make sure I am not overlapping my images in any way and that I am passing a clean image to the model.Is this the expected behaviour? What would explain this behaviour? Has anyone else encountered this issue?","[""Thanks for the question; @hyprstack. face-landmarks-detection is actually stateful; i.e. past inputs to the model can affect future predictions. This is because the model pipeline [caches regions of interest](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/pipeline.ts#L430-L460) and then [runs landmark detection on just those regions](https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/pipeline.ts#L300-L304). I think this is done for performance reasons when running on a video stream; and it allows the pipeline to avoid running face detection if faces have not moved; although I'm not 100% sure (@lina128 would know more). @lina128 do you know of a way to avoid this caching in case a user wants to run the model on several separate static images?=====""; ""We'll add a staticImage option to the API soon.=====""; 'Thanks @mattsoulanille & @lina128 . Makes sense to optimise it like that. Will wait for the update for now :)=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Model API,API,add support for option,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Varying output across identical inputs""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5608,Type Error: forwardFunc_1 is not a function,3,closed,2021-09-11T20:18:41Z,2021-09-25T12:42:00Z,Type Error: forwardFunc_1 is not a functionwhen im using toxicity model,['i managed to fix this====='; 'ummm... may i know what you did?====='; '> ummm... may i know what you did?i reverted to a different version of npm and somehow this fixed the issue====='],Reference Error,Crash,Dependency Error,WebGL,Backend,change npm/node version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,B.2
https://github.com/tensorflow/tfjs/issues/5584,Breaking Change v3.8.0 => v3.9.0,10,closed,2021-09-05T05:19:28Z,2021-12-21T08:00:31Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.9.0**Describe the current behavior**Upgraded from 3.8.0 to 3.9.0; and typescript throws an error:> Error: node_modules/@tensorflow/tfjs-core/dist/hash_util.d.ts:2:49 - error TS2304: Cannot find name 'Long'.![image](https://user-images.githubusercontent.com/5757359/132116104-1a06c0df-e90b-48d9-9cc2-db62244a6b10.png)Using `typescript` 4.3.5.Perhaps missing `import Long from 'long';` in ![image](https://user-images.githubusercontent.com/5757359/132116245-cf1c1688-fbf8-48ab-91d8-84c0be4702c6.png)**Describe the expected behavior**No breaking change,"['Same issue====='; 'Looks like `hash_util.d.ts` somehow lost its reference types triple slash. Here it is at 3.8.0:```/// <reference types=""long"" />export declare function hexToLong(hex: string): Long;export declare function fingerPrint64(s: Uint8Array; len?: number): Long;```It [should have been automatically added by tsc](https://www.typescriptlang.org/docs/handbook/triple-slash-directives.html#-reference-types-); and I\'m not yet sure why `tsc` isn\'t including it in our Bazel compile.====='; ""I've tried to reproduce this locally by compiling the following:```import {hexToLong} from '@tensorflow/tfjs-core/dist/hash_util';console.log(hexToLong('0xABCDEF'));```tsc successfully compiled the above. Could you please send me a reproduction of this issue where tsc fails? Thanks!=====""; ""I am going to remove the release blocker tag from this issue since I have been unable to reproduce it. [Here's my attempt](https://github.com/mattsoulanille/tfjs-issue-5584-test).=====""; 'I was not able to reproduce the same error; will be closing this request ; please @mention to reopen. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5584"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5584"">No</a>====='; 'I am also having the same issue on a fresh Angular 12 install...Version is 3.11.0When I manually add `/// <reference types=""long"" />`to  the `hash_util.d.ts` it works as expected.Before```/// <amd-module name=""@tensorflow/tfjs-core/dist/hash_util"" />export declare function hexToLong(hex: string): Long;export declare function fingerPrint64(s: Uint8Array; len?: number): Long;```After```/// <amd-module name=""@tensorflow/tfjs-core/dist/hash_util"" />/// <reference types=""long"" />export declare function hexToLong(hex: string): Long;export declare function fingerPrint64(s: Uint8Array; len?: number): Long;```====='; 'This is still an issue for me.I have tried:1. `npm I -D @types/long`2. add `""types"": [""long""]` to `compilerOptions`But that did not help.====='; 'v3.12.0Added `@types/long` to my `compilerOptions.types` worked.====='; 'Step 1. npm I -D @types/longStep 2.  go to tsconfig.app.jsonStep 3.  add   ""types"": [""@types/long""]----->tsconfig.app.json this file will looks like{  ""extends"": ""./tsconfig.json"";  ""compilerOptions"": {    ""outDir"": ""./out-tsc/app"";   ###  _**""types"": [""@types/long""]**_  };  ""files"": [    ""src/main.ts"";    ""src/polyfills.ts""  ];  ""include"": [    ""src/**/*.d.ts""  ]}-------> it worked for me; hope will work for you as well. =====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,add import,Fix import confusion in program,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Type Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5577,ValueError: Unsupported Ops in the model before optimization StringSplitV2; StaticRegexReplace; StringLower; RaggedTensorToTensor,4,open,2021-09-02T18:24:17Z,2021-09-14T05:39:46Z,"**My Environment**Tensorflow.js version: 3.9.0Environment: Google Colab GPU InstanceImported libraries during conversion: tensorflow; tensorflowjs; tensorflow_io; tensorflow_text---**Model Structure**```model = tf.keras.Sequential([    encoder;    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary());output_dim=64;mask_zero=True);    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64));    tf.keras.layers.Dense(64; activation='relu');    tf.keras.layers.Dense(7)])```---**The Problem**I tried to convert a tensorflow saved model to a tensorflowjs model with```!tensorflowjs_converter \    --input_format=tf_saved_model \    --output_format=tfjs_graph_model \    --signature_name=serving_default \    --saved_model_tags=serve \    /content/drive/MyDrive/textClassificationModel4/ \    /content/drive/MyDrive/JSModels/textClassificationJS```It failed due to unsupported ops```Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter""; line 8; in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/converter.py""; line 813; in pip_main    main([' '.join(sys.argv[1:])])  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/converter.py""; line 817; in main    convert(argv[0].split(' '))  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/converter.py""; line 804; in convert    weight_shard_size_bytes; metadata_map)  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/converter.py""; line 533; in _dispatch_converter    metadata=metadata_map)  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 691; in convert_tf_saved_model    metadata=metadata)  File ""/usr/local/lib/python3.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 154; in optimize_graph    '; '.join(unsupported))ValueError: Unsupported Ops in the model before optimizationStringSplitV2; RaggedTensorToTensor; StringLower; StaticRegexReplace```---Can you please help me with this?",['just my $0.02 - like it says; `tensorflowjs_converter` does not support kernel ops used by your model:  `StringSplitV2`; `RaggedTensorToTensor`; `StringLower`; `StaticRegexReplace`in reality; those methods are part of `tf.strings` namespace and intended mostly as string handling helper methods  and not meant to be used inside compiled modelsexcept for `RaggedTensorToTensor` which is part of brand new tensor type - [Ragged Tensor](https://www.tensorflow.org/guide/ragged_tensor) which would be nice to have in TFJS but likely a major effort(best to open separate enhancement request for that)====='; 'So what should I do now if I want to convert the model to TFJS? ====='; '@Electric-Dragon You are welcome to contribute these ops to TFJS; similar to these PRs:https://github.com/tensorflow/tfjs/pull/5089https://github.com/tensorflow/tfjs/pull/5052====='; '@pyu10055 Any tips on how to contribute and build kernels for certain operations?====='],Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5575,Codelab on Tensorflow js,1,closed,2021-09-02T14:56:39Z,2021-09-02T18:46:56Z,https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#2Has references to link to tfjs-node is deprecated - this codelab needs to be updated_Install Node.js and npm. For supported platforms and dependencies; please see the tfjs-node installation guide._,['Submitted the change internally ; will be updated once the change is merged. Thank you so much====='],Document Error,Document Error,Confused Document,TF(CPU),Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Invalid Links"",
    ""specific_type"": ""[E.1.1] Deprecated References""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.4] Confused Document""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5573,Propagates NaNs failure on WebGL1,2,open,2021-09-02T01:12:27Z,2021-09-14T18:30:55Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: 95.0.4629.2 (Official Build) canary (64-bit) (cohort: Clang-64)- Tensorflow.js Converter Version:**Describe the current behavior**Unit test has 14 failures on WebGL1 backend; most of them related to NaN propagation **Describe the expected behavior**All passed**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.`cd tfjs-backend-webgl && yarn && yarn karma start --testEnv webgl1`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```Chrome 95.0.4629.2 (Windows 10) step kernel webgl1 {""WEBGL_VERSION"":1;""WEBGL_CPU_FORWARD"":false;""WEBGL_SIZE_UPLOAD_UNIFORM"":0} propagates NaNs FAILED        Error: Arrays differ: actual[4] = 0; expected[4] = NaN.            at <Jasmine>            at expectArraysPredicate (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:77834:23)            at expectArraysClose (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:77788:16)            at C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65502:28            at step (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65425:23)            at Object.next (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65406:53)            at fulfilled (C:/Users/haoyunfe/AppData/Local/Temp/karma-typescript-bundle--14664-0rZftuXqFvZf-.js:65397:58)......```",['In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks!====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,condition replacer,condition replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3"",
    ""specific_type"": ""D.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5572,loadLayersModel crashes in React Native without an error,1,open,2021-09-02T00:32:09Z,2021-09-07T19:34:27Z,"I am trying to load a tensorflow model in React Native. If I use tfjs in normal React; everything works fine; but in React Native the app crashes (strangely without an error).**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Android emulator as well as physical device- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ""@tensorflow/tfjs"": ""^3.9.0""; ""@tensorflow/tfjs-react-native"": ""^0.6.0""- React Native 'Flavour': React Native CLI**Describe the current behavior**When the code (example below) is executed; the app crashes; but no error is provided; leaving me wondering where to search for help.**Describe the expected behavior**Model is loaded.**Standalone code to reproduce the issue**```import * as tf from '@tensorflow/tfjs'import '@tensorflow/tfjs-react-native'import { bundleResourceIO } from '@tensorflow/tfjs-react-native'const modelJson = require('./model.json');const modelWeights = require('./group1-shard1of1.bin');console.log(modelJson)console.log(modelWeights)model = await tf.loadLayersModel(bundleResourceIO(modelJson; modelWeights));```Here are my model.json and group1-shard1of1.bin:[model_files.zip](https://github.com/tensorflow/tfjs/files/7095160/model_files.zip)Thank you for your help.","[""I spend some time on debugging this. I created two completetly fresh projects:1. one project with **npx react-native init**2. one project with **npx create-react-native-app**The project using create-react-native-app works; while the project with react-native init does not. For both projects I exactly followed the installation instructions. The only difference was; that create-react-native-app takes care of the installation of 'react-native-unimodules'. For the react-native init project I followed the instructions from the expo docs on how to install the [react-native-unimodules](https://docs.expo.dev/bare/installing-unimodules/). But still the result was: create-react-native-app works; while with react-native init the app crashes once tf code is executed at runtime.Maybe this has something to do with the recent migration of the react-native-unimodules package indicated [here](https://github.com/unimodules/react-native-unimodules).Note: As stated above this does not only affect loadLayersModel but seems to be a more general issue. The tests above were conducted using just the folowing code:```import React from 'react'import { View; Text } from 'react-native'import * as tf from '@tensorflow/tfjs';import '@tensorflow/tfjs-react-native';export default class App extends React.Component {  constructor(props) {    super(props);    this.state = {      isTfReady: false;    };  }  async componentDidMount() {    // Wait for tf to be ready.    await tf.ready();    // Signal to the app that tensorflow.js can now be used.    this.setState({      isTfReady: true;    });  }  render() {    return <View>      <Text>Test</Text>    </View>  }}```Though I tried to follow every step exactly like described in the docs; it is of course possible that I missed something.=====""]",app close,Crash,Cross-platform App Framework Incompatibility,Mobile,Platform,install dependency,Modifying dependency configuration,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.0] Other Device Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.3] Cross-platform App Framework Incompatibility""
  }
}
```",A,D.3
https://github.com/tensorflow/tfjs/issues/5551,In compiling `tfjs-backend-wasm`; emscripten will always use a default Python 3.5; which will cause a syntax error in `emcc.py`,1,closed,2021-08-29T11:57:56Z,2021-09-24T02:42:06Z,<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 16.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): cloned from github repository- TensorFlow.js version: v3.8.0- CUDA/cuDNN version: N/A**Describe the problem**Building `tfjs-backend-wasm` package requires emscripten to compile XNNPack into wasm. Emscripten will use Python to execute its `emcc.py` file.The default Python on my machine is a Python 3.5 at `/usr/bin/python3`. When executing `emcc.py`; it will raise a syntax error on the line 1625:> `exit_with_error(f'{setting} must be a multiple of WebAssembly page size (64KiB); was {settings[setting]}')`I find out that the 'f-string' is added to Python after Python 3.6; so I should use a newer version of Python 3 in compiling.There is a Python 3.7 installed on my machine (located at /usr/local/python3.7/bin/python3). I added the Python 3.7 to my `$PATH` variable and `$PYTHON` variable; and I have checked that now the commands `python`; `python3`; `/usr/bin/env python3`; `$PYTHON` all refer to the Python 3.7 executable. However; emscripten still uses the default Python 3.5 to run the `emcc.py` file; ignoring these variables and producing the same error. I even try to modify the first line in `emcc.py` from `#!/usr/bin/env python3` into `#!/usr/local/python3.7/bin/python3`; but it does not work at all.I do not have the root access on the machine; so I cannot remove the `/usr/bin/python3` executable; nor can I update or replace it. How could I let emscripten to use a Python 3.7 installed in a custom location?**Provide the exact sequence of commands / steps that you executed before running into the problem**```cd tfjs-backend-wasmyarn test # just for building the package```**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5551"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5551"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Wasm,Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Python Version Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5550,Test cases missing for error detection in tfjs-layers (In many functions).,3,open,2021-08-29T05:20:00Z,2021-09-13T18:03:30Z,There are a lot of test cases missing for error detection in tfjs-layers. Some are already added as an issue but others are not yet reported. Are you willing to work on this: YesThese are my initials findings for missing error detection test-cases:- Check Input Data function- Model.Summary- Model.Compile- standardizeDataIteratorOutput,"['I have made tests for the following functions. If anyone can confirm that these are missing. I will make a PR for these. And If anyone wants to add more missing test cases.====='; ""I don't see much movement on this issue. I think this issue might not be correctly portrayed by me. So; If there is no movement on it till tomorrow; I will close it.=====""; 'Hi @mattsoulanille can you please help with this request ?=====']",Document Error,Document Error,Improper Exception Handling,Layer API,API,add exception handling,add exception handling,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,A.7
https://github.com/tensorflow/tfjs/issues/5539,Support L2-Pooling.,2,closed,2021-08-27T08:05:40Z,2021-09-22T00:55:59Z,Hi;Is there any plan to support L2-Pooling?  Thanks.Link https://github.com/webmachinelearning/webnn-polyfill/pull/112.,['There are suggestion that you can decompose L2 pooling with following:```tf.sqrt(tf.avgPool(tf.square(h)) ```====='; 'Thanks much @pyu10055. Close it.  ====='],Reference Error,Crash,Unimplemented Operator,Layer API,API,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.0""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5537,Inconsistent Inference Performance,2,closed,2021-08-26T11:58:22Z,2021-08-29T15:49:52Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): **Yes**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): **Windows**- TensorFlow.js installed from (npm or script link): **npm**- TensorFlow.js version (use command below): **3.8.0**- Browser version:  **""5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/92.0.4515.159 Safari/537.36""****Describe the current behavior**I'm running a pix2pix generator model in the browser (in comes canvas; out comes image pixels).It runs in a while loop; until all the frames are generated.https://github.com/sign-language-processing/sign-translate/blob/master/src/app/pages/translate/pose-viewers/human-pose-viewer/human-pose-viewer.component.ts#L54-L60If I profile specifically the model inference - ```tsconst perf = performance.now();await this.pix2pix.translate(poseCanvas; canvas);console.log(performance.now() - perf);```I see the following in the console: Each frame takes a different amount of time to be calculated; some taking ~40ms; some taking 1000ms+.https://user-images.githubusercontent.com/5757359/130957721-eedda429-b8ea-4bc5-b0b7-befb04cfb627.mp4(Debug mode does not give much more information; as it is slow; and consistently takes ~1500ms).**Describe the expected behavior**- I expect the model inference to take roughly the same amount of time; each time it is being performed; as there is nothing running in the background.- I also expect this code to saturate the CPU/GPU; and to not have any idle time. Instead; in the `performance` tab I see more than half the time is idle.(My CPU is intel i9-9900K and my GPU is RTX 3070Ti; I would think this model would run like butter)**Other sources**- The code is available on https://github.com/sign-language-processing/sign-translate/- You can try it yourself at [sign.mt](https://sign.mt)","['This is due to it being performed on the main thread.Moved to a WebWorker; now exhibiting a very high data transfer time but at least it is consistent.(It takes 110ms round trip. 35ms to perform `fromPixels`; 11ms to perform model inference.)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5537"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5537"">No</a>=====']",Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5536,webgpu: fromPixels for HTMLVideoElement related cases fail to execute 'importExternalTexture',1,open,2021-08-26T08:17:18Z,2021-08-26T08:28:21Z,"Run `yarn test` under tfjs-backend-webgpu.Expect all cases pass. However; below cases fail:```FAILED fromPixels test-webgpu {""WEBGPU_CPU_FORWARD"":false} fromPixels for HTMLVideoElementdebug.js:21 Error: Failed to execute 'importExternalTexture' on 'GPUDevice': Failed to import texture from videoerror properties: Object({ code: 0; INDEX_SIZE_ERR: 1; DOMSTRING_SIZE_ERR: 2; HIERARCHY_REQUEST_ERR: 3; WRONG_DOCUMENT_ERR: 4; INVALID_CHARACTER_ERR: 5; NO_DATA_ALLOWED_ERR: 6; NO_MODIFICATION_ALLOWED_ERR: 7; NOT_FOUND_ERR: 8; NOT_SUPPORTED_ERR: 9; INUSE_ATTRIBUTE_ERR: 10; INVALID_STATE_ERR: 11; SYNTAX_ERR: 12; INVALID_MODIFICATION_ERR: 13; NAMESPACE_ERR: 14; INVALID_ACCESS_ERR: 15; VALIDATION_ERR: 16; TYPE_MISMATCH_ERR: 17; SECURITY_ERR: 18; NETWORK_ERR: 19; ABORT_ERR: 20; URL_MISMATCH_ERR: 21; QUOTA_EXCEEDED_ERR: 22; TIMEOUT_ERR: 23; INVALID_NODE_TYPE_ERR: 24; DATA_CLONE_ERR: 25 })    at Object.fromPixelsExternalImage (FromPixelsExternalImage.ts:84)    at Object.fromPixels [as kernelFunc] (FromPixels.ts:65)```However; if I change flag `WEBGPU_USE_IMPORT` to false; above case can pass. It seems that the import path has   problem.cc @shaoboyan ","[""@qjia7  I'll take this=====""]",Data & Model Error,Crash,Incorrect Code Logic,WebGPU,Backend,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5532,Unable to use the @tensorflow/tfjs-tflite package outside of a web browser,3,open,2021-08-25T18:37:52Z,2021-08-26T16:46:31Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Arch Linux- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): `yarn add @tensorflow/tfjs-tflite@0.0.1-alpha.4`- TensorFlow.js version: N/A- CUDA/cuDNN version: N/A**Describe the problem**The `@tensorflow/tfjs-tflite` package can't be used outside of a web browser due to its use of browser-specific APIs.**Provide the exact sequence of commands / steps that you executed before running into the problem**Create an empty NPM package and populate it with the following:```shell$ mkdir tensorflow-lite-repro && cd tensorflow-lite-repro$ yarn init $ cat package.json{  ""name"": ""tensorflow-lite-repro"";  ""version"": ""1.0.0"";  ""main"": ""index.js"";  ""license"": ""MIT"";  ""type"": ""module"";  ""dependencies"": {    ""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"";    ""@tensorflow/tfjs-core"": ""^3.8.0"";    ""@tensorflow/tfjs-tflite"": ""^0.0.1-alpha.4""  }}$ cat index.js// Adds the CPU backend.import '@tensorflow/tfjs-backend-cpu';// Import @tensorflow/tfjs-coreimport * as tf from '@tensorflow/tfjs-core';// Import @tensorflow/tfjs-tflite.import {loadTFLiteModel; TFLiteModel} from '@tensorflow/tfjs-tflite';```Next try to execute `index.js` using Node.```$ node --versionv16.7.0$ yarn --version1.22.5$ yarn install(snip)$ node index.js/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1261module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.getInstance=function(a;b;c){var d=a+b;c=URL.createObjectURL(new Blob([c];{type:""application/javascript""}));module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.instances.has(d)||(a=new module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader(a;                                                                                                                                                                                                   ^ReferenceError: Blob is not defined    at Function.module$exports$google3$third_party$tensorflow_lite_support$web$task$codegen$common$emscripten_module_loader.EmscriptenModuleLoader.getInstance (/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1261:196)    at Object.<anonymous> (/tmp/tensorflow-lite-repro/node_modules/@tensorflow/tfjs-tflite/dist/tf-tflite.node.js:1331:447)    at Module._compile (node:internal/modules/cjs/loader:1101:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)    at Module.load (node:internal/modules/cjs/loader:981:32)    at Function.Module._load (node:internal/modules/cjs/loader:822:12)    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:196:29)    at ModuleJob.run (node:internal/modules/esm/module_job:183:25)    at async Loader.import (node:internal/modules/esm/loader:178:24)    at async Object.loadESM (node:internal/process/esm_loader:68:5)```You can monkey-patch `Blob` (e.g. by adding `window.Blob = require(""blob-polyfill"")` before the `@tensorflow/tfjs-tflite` import); but it then fails because [`Window.self`](https://developer.mozilla.org/en-US/docs/Web/API/Window/self) doesn't exist.","['There is an older version on npm which is 3 months old ; we have a bunch of similar issues where users are using older version via npm ; requesting @jinjingforever to push newer version to npm. Thank you ====='; 'In the meantime; @rthadur do you know if it is possible to add the `@tensorflow/tfjs-tflite` package directly as a git dependency?====='; ""Thank you @Michael-F-Bryan for the report. Your are right. We actually haven't worked on nodejs support for tfjs-tflite yet (hence the alpha version). I will take a look at this soon (already in my todo list). Thanks! =====""]",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,Node.js,Platform,change platform,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.4
https://github.com/tensorflow/tfjs/issues/5531,[tfjs-tflite] ERROR: failed to delegate CONV_2D node #2,6,closed,2021-08-25T11:15:44Z,2021-09-04T02:51:35Z,"**System information**- OS Platform and Distribution: Ubuntu 20.04- Browser version: Chrome 92The following code fails with a bunch of errors:```html<img id=""imgInputEl"" src=""data:image/jpeg;base64;/9j/4AAQSkZJRgABAQEBOgE6AAD/4gIoSUNDX1BST0ZJTEUAAQEAAAIYAAAAAAIQAABtbnRyUkdCIFhZWiAAAAAAAAAAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAAHRyWFlaAAABZAAAABRnWFlaAAABeAAAABRiWFlaAAABjAAAABRyVFJDAAABoAAAAChnVFJDAAABoAAAAChiVFJDAAABoAAAACh3dHB0AAAByAAAABRjcHJ0AAAB3AAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAFgAAAAcAHMAUgBHAEIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFhZWiAAAAAAAABvogAAOPUAAAOQWFlaIAAAAAAAAGKZAAC3hQAAGNpYWVogAAAAAAAAJKAAAA+EAAC2z3BhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABYWVogAAAAAAAA9tYAAQAAAADTLW1sdWMAAAAAAAAAAQAAAAxlblVTAAAAIAAAABwARwBvAG8AZwBsAGUAIABJAG4AYwAuACAAMgAwADEANv/bAEMACgcHCAcGCggICAsKCgsOGBAODQ0OHRUWERgjHyUkIh8iISYrNy8mKTQpISIwQTE0OTs+Pj4lLkRJQzxINz0+O//bAEMBCgsLDg0OHBAQHDsoIig7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7O//AABEIADIAMgMBIgACEQEDEQH/xAAbAAACAwEBAQAAAAAAAAAAAAAABQIDBgQHAf/EACwQAAEDAwIEBAcBAAAAAAAAAAEAAgMEBREhMQYSQVEVMmFxFBYzUoGRwdH/xAAaAQACAwEBAAAAAAAAAAAAAAACBAABBQMG/8QAIhEAAgEEAgIDAQAAAAAAAAAAAAECAwQREhMhBTEUIiNB/9oADAMBAAIRAxEAPwD0LCg9zY28zzgKzC566MmE9Mjfslry6+PDKWWwqVPd9kYaymqZHRxSAvbqW9VN72McGueATsCd1jbnNLSVbJ6dxZLEc8w6rR2yqN1t0dZI0AvadB6EhILyj4dtfsNStMPOei/xCkM4gMwa86AHTKvIWR4hp+WNxHuEx4TvDrjRPp53F09Ppk7ub0P8TFjfO46muwK9vpHaPod8qFPCFqCh0t3GVyXGXEZwuzGiV3IvMTmgarB8tssP+DdukzK3BwlkcAn9gcGcP0zR0Ys/VERnkJAc84z2T+hqKYwNp6c4EbAMd/VZEqc+FzXpGlOSwonDesPgdnfKS8HvdDxIYx5ZInNP41/icXd2GEeqS2BwhvxlzoyMn96f6mPGNqogay/FnoOUJX4kEL1W6MbRj8KmrpI6mIsdlpI8zdwrQdVI7IKkIzWJLKCjJrtGa+UKY1BlkqJ5SfvcNPZdkPD1NC4OjLg4bHKbFfRsg4YJYx0Hyyb9mbulkrZQfh3ROHZ+Qk9v4Ur4J5JZZGBzzqGnOB2C3TlURqudG0pUnmKCnXnJatiIWWXH1ChPEJrSJw2ZeFNyEKMpFZ3Uh5UIUYSIuVbkIVopkEIQrBP/2Q==""><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.8.0/dist/tf.js""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.4/dist/tf-tflite.js""></script><script>  (async function() {    const esrgan = await tflite.loadTFLiteModel('https://tfhub.dev/captain-pool/lite-model/esrgan-tf2/1');    console.log(""esrgan:""; esrgan);    const outputTensor = tf.tidy(() => {      let img = tf.browser.fromPixels(imgInputEl);      let input = tf.sub(tf.div(tf.expandDims(img); 127.5); 1); // normalise [-1;1]      console.log(""input:""; input);            let outputTensor = esrgan.predict(input);      return tf.mul(tf.add(outputTensor; 1); 127.5) // de-normalize    });    console.log(outputTensor; outputTensor.shape);  })();</script>```![image](https://user-images.githubusercontent.com/1167575/130780608-8868d2ac-e772-4f4d-b53d-407778e575fc.png)...![image](https://user-images.githubusercontent.com/1167575/130780933-6b35b6ba-0ff4-4dc5-ab7a-a3cd0b982de5.png)","['just a thought; `tfjs-tflite@0.0.1-alpha.4` npm package is 3 months old which `tfjs` has went through quite a lot of changes since then - can you try with `tflite` built from main?i got a lot of errors with both `tfjs-tflite` and `tfjs-backend-webgpu` using old alpha builds; quite a few resolved after a fresh build.====='; 'I agree ; there is a older version on npm; requesting @jinjingforever for pushing newer version to npm.Thank you====='; 'Thank you guys for investigation. Actually tfjs-tflite@0.0.1-alpha.4 is indeed the latest version. The errors shown here are from the tflite c++ runtime. It is likely that the model is not working well with XNNPACK that tflite is using. I will do more digging with the tflite team and report back if I have anything. Thank you!====='; ""Hi; I just published a new version of the `tfjs-tflite` package (0.0.1-alpha.6) which should fix this problem. See this [demo](https://codepen.io/jinjingforever/pen/XWgbzQR). The model is a little bit slow but it works:) It might get a bit faster if you set up your server for [cross-origin isolation](https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite#performance) (codepen doesn't support it unfortunately).Thanks! =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5531"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5531"">No</a>====='; 'Thank you!=====']",abort,Crash,Dependency Error,Wasm,Backend,change tfjs-tflite version(framework),Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A,B.2
https://github.com/tensorflow/tfjs/issues/5529,[tfjs-backend-webgpu] Fail to build webgpu backend after upgrading @webgpu/types to version 0.1.6,3,closed,2021-08-25T08:59:49Z,2021-10-01T00:33:44Z,After upgrading `@webgpu/types to 0.1.6` in `package.json`; I got build error messages below:```src/index.ts → dist/tf-backend-webgpu.node.js...[!] (plugin typescript) Error: @rollup/plugin-typescript TS2322: Type 'CanvasRenderingContext2D | WebGLRenderingContext' is not assignable to type 'GPUCanvasContext'.  Type 'CanvasRenderingContext2D' is missing the following properties from type 'GPUCanvasContext': __brand; configure; unconfigure; getPreferredFormat; getCurrentTexturesrc\backend_webgpu.ts (135:7) 'GPUCanvasContext': __brand; configure; unconfigure; getPreferredFormat; getCurrentTexture135       this.dummyContext = this.dummyCanvas.getContext('gpupresent');          ~~~~~~~~~~~~~~~~~Error: @rollup/plugin-typescript TS2322: Type 'CanvasRenderingContext2D | WebGLRenderingContext' is not assignable to type 'GPUCanvasContext'.  Type 'CanvasRenderingContext2D' is missing the following properties from type 'GPUCanvasContext': __brand; configure; unconfigure; getPreferredFormat; getCurrentTexture    at error (C:\code\tfjs\tfjs-backend-webgpu\node_modules\rollup\dist\shared\rollup.js:10149:30)```Notice that build works fine if we use `@webpu/types` version 0.1.4.The error is report from `@rollup/plugin-typescript` and a simple fix by upgrading `@rollup/plugin-typescript` to latest version still cannot fix it.@kainino0x Do you have any suggestions?,"[""I think it may be because `@webgpu/types@0.1.6` is not for the version of `webgpu` that `tfjs-backend-webgpu` uses. I'm not sure though.=====""; 'I am pretty sure this was resolved as I remember discussing the workaround.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5529"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5529"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGPU,Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Type Assignment Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5520,[tfjs-react-native] Image classification of Livestream in real-time,4,open,2021-08-23T09:41:42Z,2021-08-26T19:58:18Z,**System information**- TensorFlow.js version: 3.8.0- tfjs-react-native version: 0.6.0- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**I wonder if there is a way to analyze the Livestream in real-time. For example; I've got a task to classify a mini object; such that the mobile's camera is not capable of doing that. In this case; I have an IP camera; which could return a Livestream url for the app. However; currently; there is only the `decodeJpeg` method; which runs statically; not dynamically. Actually; I'm trying to snapshot on the Livestream frame and recursively calling `decodeJpeg` method; however; this is weird and I think the performance interacting with react-native is not good. **Will this change the current API? How?**Yes; and I think it needs two steps:1. Allowing dynamic video streams to be the input (e.g. a Livestream url) of a model (which possibly will be a new API).2. Allowing a real-time callback function for result display.**Who will benefit from this feature?**For users that need to use other cameras instead of mobile native cameras; because not all types of objects or images taken by phones are suitable for particular cases; thanks a lot!,"['You can use tf.data.webcam https://js.tensorflow.org/api/latest/#data.webcam to live stream ; please check ====='; ""> You can use tf.data.webcam https://js.tensorflow.org/api/latest/#data.webcam to live stream ; please checkThanks a lot! This is cool; however; I notice that it is only working for webcams (usually link to PC by USB); instead of ip cameras (directly access live streams from a URL under the same network). I'm not quite sure if I could just input the live stream URL (RTSP/HTTP protocol) to `tf.data.webcam`; it seems that the method only accepts input of a DOM element such as `document.createElement('video')`.=====""; 'I think this is really close to the use of `cameraWithTensors`:https://js.tensorflow.org/api_react_native/0.6.0/#cameraWithTensorsIf the `onReady` function could be used for a Livestream URL; everything will be perfect. 🤔====='; ""Under the hood; `tf.data.webcam` still [takes individual frames from the camera stream](https://github.com/tensorflow/tfjs/blob/master/tfjs-data/src/iterators/webcam_iterator.ts#L154) and transforms them into tensors. cameraWithTensors [does the same thing](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L278-L283).I'm not sure how you're displaying the livestream on the app; but if you're rendering it in a WebGL2 context and know which texture corresponds to the video; you might be able to use [fromTexture](https://js.tensorflow.org/api_react_native/0.6.0/#fromTexture) to get a tensor representing a single frame of the video. Then; you can run that through your model. The way cameraWithTensors works is it [creates and expo-gl GLView](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L218) and [renders each frame to it](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L314). When the user requests the current frame as a tensor (or if it's set to autoRender); it converts the [WebGL texture into a tensor](https://github.com/tensorflow/tfjs/blob/tfjs-v3.8.0/tfjs-react-native/src/camera/camera_stream.tsx#L278-L284). You might be able to do something similar with your video feed. Ideally; we would have a method similar to [tf.browser.fromPixels](https://js.tensorflow.org/api/3.8.0/#browser.fromPixels) that works on React Native components (probably implemented as a higher-order component like [cameraWithTensors](https://js.tensorflow.org/api_react_native/0.6.0/#cameraWithTensors)); but implementing that in React Native is likely a lot more complex than in the browser.=====""]",Slow Execution,Poor Performance,Cross-platform App Framework Incompatibility,Mobile,Platform,change API,Replace API with another effective one,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.1.1,D.3
https://github.com/tensorflow/tfjs/issues/5517,[tensorflowjs_converter] InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0;0] (0 elements),4,closed,2021-08-20T14:30:54Z,2021-08-24T18:54:50Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  Ubuntu 20.04- TensorFlow.js installed from (npm or script link): `pip install tensorflowjs`- Tensorflow.js Converter Version:```sh$ tensorflowjs_converter  --versiontensorflowjs 3.8.0Dependency versions:  keras 2.6.0  tensorflow 2.6.0```**Describe the current behavior**```sh$ tensorflowjs_converter \>     --input_format=tf_frozen_model \>     --output_node_names final_result \>     ./tensorflowjs_model.pb \>     ./webTraceback (most recent call last):  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py""; line 496; in _import_graph_def_internal    results = c_api.TF_GraphImportGraphDefWithResults(tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0;0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT; Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze; MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1;1001]; [2] and with input tensors computed as partial shapes: input[1] = [0;0].```**Describe the expected behavior**Convert the frozen model to web format**Standalone code to reproduce the issue**```sh$ git clone git@github.com:google/emoji-scavenger-hunt.git$ cd emoji-scavenger-hunt/dist/model$ ls -ltotal 3568-rw-rw-r-- 1 huan huan 3580716 Aug 18 00:37 group1-shard1of1-rw-rw-r-- 1 huan huan   55746 Aug 18 00:37 tensorflowjs_model.pb-rw-rw-r-- 1 huan huan    9489 Aug 18 00:37 weights_manifest.json$ tensorflowjs_converter \>     --input_format=tf_frozen_model \>     --output_node_names final_result \>     ./tensorflowjs_model.pb \>     ./web```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```sh$ tensorflowjs_converter     --input_format=tf_frozen_model     --output_node_names final_result     ./tensorflowjs_model.pb     ./web2021-08-20 22:28:16.527933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-08-20 22:28:16.527994: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Traceback (most recent call last):  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py""; line 496; in _import_graph_def_internal    results = c_api.TF_GraphImportGraphDefWithResults(tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot reshape a tensor with 1001 elements to shape [0;0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT; Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze; MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1;1001]; [2] and with input tensors computed as partial shapes: input[1] = [0;0].During handling of the above exception; another exception occurred:Traceback (most recent call last):  File ""/home/huan/.local/bin/tensorflowjs_converter""; line 8; in <module>    sys.exit(pip_main())  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py""; line 813; in pip_main    main([' '.join(sys.argv[1:])])  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py""; line 817; in main    convert(argv[0].split(' '))  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py""; line 803; in convert    _dispatch_converter(input_format; output_format; args; quantization_dtype_map;  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/converter.py""; line 574; in _dispatch_converter    tf_saved_model_conversion_v2.convert_tf_frozen_model(  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 518; in convert_tf_frozen_model    graph = load_graph(frozen_model_path)  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 68; in load_graph    tf.import_graph_def(graph_def; name='')  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py""; line 549; in new_func    return func(*args; **kwargs)  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py""; line 400; in import_graph_def    return _import_graph_def_internal(  File ""/home/huan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py""; line 501; in _import_graph_def_internal    raise ValueError(str(e))ValueError: Cannot reshape a tensor with 1001 elements to shape [0;0] (0 elements) for '{{node MobilenetV1/Predictions/Reshape}} = Reshape[T=DT_FLOAT; Tshape=DT_INT32](MobilenetV1/Logits/SpatialSqueeze; MobilenetV1/Predictions/Reshape/shape)' with input shapes: [1;1001]; [2] and with input tensors computed as partial shapes: input[1] = [0;0].```Link to https://github.com/huan/emoji-net/issues/3","['Frozen models are not supported in latest versions. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5517"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5517"">No</a>====='; 'Hi @rthadur ;Thanks for the reply.I think the `frozen models` are still supported according to the help message of tensorflowjs_converter 3.8:```sh$ tensorflowjs_converter --help  --input_format {tf_hub;tf_saved_model;tfjs_layers_model;keras;tf_frozen_model;keras_saved_model}$ tensorflowjs_converter  --vesiontensorflowjs 3.8.0Dependency versions:  keras 2.6.0  tensorflow 2.6.0```We can see **tf_frozen_model** is listed in the `input_format`.And also; I can reproduce this problem in the older version of tensorflowjs_converter with version 0.8.5; and the following is the reproduce steps that have the exact same error message outputted as the latest version.## Reproduce this error with tensorflowjs_converter v0.8.5### 1. Get a Python 2.7 environment```sh$ docker run --rm -ti -v $(pwd):/model ubuntu:18.04 bash$ apt update && apt install -y python python-pip```### 2. Install```sh$ pip install tensorflowjs==0.8.5$ tensorflowjs_converter --versionUsing TensorFlow backend.tensorflowjs 0.8.5Dependency versions:  keras 2.2.2  tensorflow 1.13.1```### 3. Convert```sh# tensorflowjs_converter \\>     --input_format=tf_frozen_model \\>     --output_node_names final_result \\>     ./tensorflowjs_model.pb \\>     ./webUsing TensorFlow backend.Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter""; line 11; in <module>    sys.exit(main())  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/converter.py""; line 352; in main    strip_debug_ops=FLAGS.strip_debug_ops)  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py""; line 329; in convert_tf_frozen_model    graph = load_graph(frozen_model_path; output_node_names)  File ""/usr/local/lib/python2.7/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py""; line 67; in load_graph    tf.import_graph_def(graph_def; name=\'\')  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py""; line 507; in new_func    return func(*args; **kwargs)  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py""; line 430; in import_graph_def    raise ValueError(str(e))ValueError: Cannot reshape a tensor with 1001 elements to shape [0;0] (0 elements) for \'MobilenetV1/Predictions/Reshape\' (op: \'Reshape\') with input shapes: [1;1001]; [2] and with input tensors computed as partial shapes: input[1] = [0;0].```====='; ""As mentioned [here](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter#conversion-flags) it is supported for backward compatibility and we don't think we will be supporting older tfjs(0.86) and tenforflow(1.13) anymore. cc @pyu10055 @lina128 =====""]",Reference Error,Crash,Incorrect Code Logic,Operator,API,change model,Changing model,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5516,movenet-multipose model fails on inference using tfjs-backend-wasm,6,closed,2021-08-20T12:32:10Z,2021-11-23T01:58:24Z,[MoveNet MultiPose model](https://storage.googleapis.com/movenet/MoveNet.MultiPose%20Model%20Card.pdf) was released and published on [TFHub](https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1)but fails on inference using `tfjs-backend-wasm`:```human.js:56658 Uncaught (in promise) Error: Broadcasting along outer dims is not yet supported for float32 Less.    at Object.kernelFunc3 [as kernelFunc] (human.js:56658)    at kernelFunc3 (human.js:8316)    at human.js:8357    at Engine.scopedRun (human.js:8238)    at Engine.runKernelFunc (human.js:8355)    at Engine.runKernel (human.js:8275)    at less_ (human.js:12427)    at less__op (human.js:8849)    at executeOp11 (human.js:34000)```no issues running model using `tfjs-backend-webgl`  i'm reporting this as an issue instead of a feature as the model is officially published on `tfhub` thus expectation is that it works  environment: tfjs 3.8.0 on ubuntu 21.10,"['I just tested with tfjs-backend-wasm@3.11.0; it works. Can you confirm? @vladmandic ====='; '@lina128 ive retried with **tfjs 3.11.0** and tfjs fresh build from main (including building wasm binaries) and issue is the same:> Uncaught (in promise) Error: Broadcasting along outer dims is not yet supported for float32 Less.But to reproduce; input `[1; 256; 256; 3]` must be an actual image that contains body;  otherwise model short-circuits and it never reaches affected op(thats the general problem testing with synthetic inputs; model ops dont actually get excercised)====='; 'Can you try the demo here? https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenetChange the backend-runtime to tfjs-wasm. Does it work on your machine? Or can you share a code snippet that can reproduce the error?====='; 'Sure - the demo results in exactly the same error; screenshot below  (note: need to select `multipose` variation for reproduction)![image](https://user-images.githubusercontent.com/57876960/141872355-516e95bc-8340-49c3-aec3-a7f0ff767e65.png)====='; ""Ah; you said multipose; sorry I was testing singlepose. Ok; I'm able to reproduce the error now. Will get back to you soon.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5516"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5516"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5508,`RangeError: Maximum call stack size exceeded` executing specific model with a large input,1,closed,2021-08-19T17:58:01Z,2021-08-21T00:38:45Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS Catalina - Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): - Browser version: 3.8.0- Tensorflow.js Converter Version: 3.8.0**Describe the current behavior**In VS Code; it seems like we hit a `Maximum call stack size exceeded` error when the input into [the model we are using](https://github.com/microsoft/vscode-languagedetection/tree/07d8fc8b7a8e29a4a259664e1d58330fc45f034a/model) is too large. Here's the full stack trace:```ERR Maximum call stack size exceeded: RangeError: Maximum call stack size exceeded    at Xe (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:22892)    at Object.kernelFunc (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:105789)    at o (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:242459)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:243406    at y.scopedRun (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240964)    at y.runKernelFunc (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:243158)    at y.runKernel (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:241470)    at stringSplit_ (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:437551)    at Object.stringSplit__op [as stringSplit] (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:392714)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:212953    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:213212    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240843    at y.scopedRun (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240964)    at y.tidy (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:240777)    at Module.f (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:256518)    at /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:212600    at ar (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:215253)    at hr.processStack (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:224679)    at hr.executeWithControlFlow (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:223909)    at async hr._executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:222881)    at async Nt (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:187687)    at async Promise.all (index 0)    at async hr.executeWithControlFlow (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:223941)    at async hr._executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:222881)    at async fr.executeAsync (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/node_modules.asar/@vscode/vscode-languagedetection/dist/lib/index.js:2:230456)```from https://github.com/microsoft/vscode/issues/129597I ""fixed"" this by sending only 100000 characters to the model. That seems to be a good enough cut off... but I felt that I should report this anyway.Additionally something interesting is [our integration tests that run on [an input that is way smaller than 100000](https://github.com/microsoft/vscode/blob/main/extensions/vscode-api-tests/src/singlefolder-tests/untitled.languagedetection.test.ts#L27-L55) hit this error every so often and it's not clear why. Here's the stack trace from @bpasero:```Maximum call stack size exceeded: RangeError: Maximum call stack size exceeded    at Xe (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:22892)    at Object.kernelFunc (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:105789)    at o (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:242459)    at eval (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:243406)    at y.scopedRun (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:240964)    at y.runKernelFunc (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:243158)    at y.runKernel (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:241470)    at stringSplit_ (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:437551)    at Object.stringSplit__op [as stringSplit] (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:392714)    at eval (vscode-file://vscode-app/Users/bpasero/Development/Microsoft/monaco/out/../node_modules/@vscode/vscode-languagedetection/dist/lib/index.js:3:212953)```Note that we bundle tfjs into vscode-languagedetection now which is why the stack trace is different.**Describe the expected behavior**Not crash**Standalone code to reproduce the issue*** clone https://github.com/microsoft/vscode-languagedetection* edit [this line](https://github.com/microsoft/vscode-languagedetection/blob/main/test/index.test.ts#L65) to take in `{ maxContentSize: 1000000 }`* run the tests with `npm test`This should cause a test to fail and show the stack trace... let me know if it doesn't.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5508"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5508"">No</a>=====']",Data & Model Error,Crash,Incorrect Code Logic,CPU,Backend,algorithm logic,Modify API Parameter usage,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5501,[Unhandled promise rejection: TypeError: undefined is not a constructor (evaluating 'new pose.Pose')],9,closed,2021-08-17T03:06:28Z,2021-11-16T18:48:17Z,"I am implementing Pose solution on React Native following the installation and usage from [this doc](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection/src/blazepose_mediapipe). When I specified `{ runtime: 'tfjs' }` in `detectorConfig` variable; everything works fine. But I want to change runtime to 'mediapipe' since the performance table in the doc shows that the mediapipe runtime gives higher FPS; and when I change it this error raises **`[Unhandled promise rejection: TypeError: undefined is not a constructor (evaluating 'new pose.Pose')]`****System information**:- Windows 10- Testing on Android devices with expo- @mediapipe/pose version 0.4.1624666670- Programming Language: JavaScript**This is my code App.js:**```import * as tf from ""@tensorflow/tfjs-core"";import ""@tensorflow/tfjs-react-native"";import * as React from ""react"";import { View; Text; Image; StyleSheet; Platform } from ""react-native"";import { Camera } from ""expo-camera"";import { cameraWithTensors } from ""@tensorflow/tfjs-react-native"";import * as poseDetection from ""@tensorflow-models/pose-detection"";import '@mediapipe/pose'import ""@tensorflow/tfjs-backend-webgl"";const TensorCamera = cameraWithTensors(Camera);const estimationConfig = { flipHorizontal: true };var rafID;export default class App extends React.Component {  constructor(props) {    super(props);    this.state = {      loading: true;      isTfReady: false;      permission: false;      detector: null;    };  }  async loadBlazePoseModel() {    const model = poseDetection.SupportedModels.BlazePose;    const detectorConfig = {      runtime: 'mediapipe';      modelType: 'lite';      solutionPath: './node_modules/@mediapipe/pose'    };    const blazeposeDetector = await poseDetection.createDetector(      model;      detectorConfig    );    return blazeposeDetector;  }  handleCameraStream(images; updatePreview; gl) {    const {detector} = this.state;    const loop = async () => {      const nextImageTensor = images.next().value;            const poses = await detector.estimatePoses(        nextImageTensor;        estimationConfig;      );      console.log('Poses results:'; poses)      tf.dispose(nextImageTensor)      rafID = requestAnimationFrame(loop);    };    loop();  }  async componentDidMount() {    // Wait for tf to be ready.    await tf.ready();    // Signal to the app that tensorflow.js can now be used.    this.setState({      isTfReady: true;    });    // ask camera permission    const { status } = await Camera.requestPermissionsAsync();    this.setState({      permission: status === ""granted"";    });    const loadedDetector = await this.loadBlazePoseModel();    this.setState({      detector: loadedDetector;      loading: false    })  }  componentWillUnmount() {    if (rafID) {      cancelAnimationFrame(rafID);    }  }  render() {    const { loading; permission } = this.state;    if (permission === null) {      return <View />;    }    if (permission === false) {      return <Text>No access to camera</Text>;    }    let textureDims;    if (Platform.OS === ""ios"") {      textureDims = {        height: 1920;        width: 1080;      };    } else {      textureDims = {        height: 1200;        width: 1600;      };    }    return (      <View>        {loading ? (          <View>            <Text              style={{                fontSize: 60;                alignItems: ""center"";                flex: 1;                justifyContent: ""center"";              }}            >              Loading            </Text>          </View>        ) : (          <View>            <Text>Pose Estimation</Text>            <TensorCamera              // Standard Camera props              style={styles.camera}              type={Camera.Constants.Type.front}              // Tensor related props              cameraTextureHeight={textureDims.height}              cameraTextureWidth={textureDims.width}              resizeHeight={200}              resizeWidth={152}              resizeDepth={3}              onReady={this.handleCameraStream.bind(this)}              autorender={true}            />          </View>        )}      </View>    );  }}const styles = StyleSheet.create({  camera: {    position: ""absolute"";    left: 50;    top: 100;    width: 600 / 2;    height: 800 / 2;    zIndex: 1;    borderWidth: 1;    borderColor: ""black"";    borderRadius: 0;  };});```**package.json**:```{  ""main"": ""node_modules/expo/AppEntry.js"";  ""scripts"": {    ""start"": ""expo start"";    ""android"": ""expo start --android"";    ""ios"": ""expo start --ios"";    ""web"": ""expo start --web"";    ""eject"": ""expo eject""  };  ""dependencies"": {    ""@mediapipe/pose"": ""^0.4.1624666670"";    ""@react-native-async-storage/async-storage"": ""~1.15.0"";    ""@tensorflow-models/pose-detection"": ""^0.0.3"";    ""@tensorflow/tfjs"": ""^3.8.0"";    ""@tensorflow/tfjs-backend-wasm"": ""^3.8.0"";    ""@tensorflow/tfjs-backend-webgl"": ""^3.8.0"";    ""@tensorflow/tfjs-converter"": ""^3.8.0"";    ""@tensorflow/tfjs-react-native"": ""^0.6.0"";    ""expo"": ""~42.0.1"";    ""expo-camera"": ""~11.2.1"";    ""expo-gl"": ""~10.4.1"";    ""expo-gl-cpp"": ""~10.4.0"";    ""expo-status-bar"": ""~1.0.4"";    ""react"": ""16.13.1"";    ""react-dom"": ""16.13.1"";    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-42.0.0.tar.gz"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-unimodules"": ""~0.14.5"";    ""react-native-web"": ""~0.13.12""  };  ""devDependencies"": {    ""@babel/core"": ""^7.9.0""  };  ""private"": true}```**Complete Error Logs :**```[Unhandled promise rejection: TypeError: undefined is not a constructor (evaluating 'new pose.Pose')]at node_modules\@tensorflow-models\pose-detection\dist\blazepose_mediapipe\detector.js:69:28 in BlazePoseMediaPipeDetectorat node_modules\@tensorflow-models\pose-detection\dist\blazepose_mediapipe\detector.js:188:29 in __generator$argument_1at node_modules\@tensorflow-models\pose-detection\dist\blazepose_mediapipe\detector.js:32:17 in stepat node_modules\@tensorflow-models\pose-detection\dist\blazepose_mediapipe\detector.js:7:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow-models\pose-detection\dist\blazepose_mediapipe\detector.js:3:11 in <anonymous>at node_modules\@tensorflow-models\pose-detection\dist\create_detector.js:79:50 in __generator$argument_1at node_modules\@tensorflow-models\pose-detection\dist\create_detector.js:48:17 in stepat node_modules\@tensorflow-models\pose-detection\dist\create_detector.js:23:13 in <anonymous>at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\@tensorflow-models\pose-detection\dist\create_detector.js:19:11 in <anonymous>at App.js:36:36 in loadBlazePoseModelat node_modules\regenerator-runtime\runtime.js:63:36 in tryCatchat node_modules\regenerator-runtime\runtime.js:293:29 in invokeat node_modules\regenerator-runtime\runtime.js:63:36 in tryCatchat node_modules\regenerator-runtime\runtime.js:154:27 in invokeat node_modules\regenerator-runtime\runtime.js:189:16 in PromiseImpl$argument_0at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwoat node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolveat node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promiseat node_modules\regenerator-runtime\runtime.js:188:15 in callInvokeWithMethodAndArgat node_modules\regenerator-runtime\runtime.js:211:38 in enqueueat node_modules\regenerator-runtime\runtime.js:238:8 in exports.asyncat App.js:27:2 in loadBlazePoseModelat App.js:98:33 in componentDidMountat node_modules\regenerator-runtime\runtime.js:63:36 in tryCatchat node_modules\regenerator-runtime\runtime.js:293:29 in invokeat node_modules\regenerator-runtime\runtime.js:63:36 in tryCatchat node_modules\regenerator-runtime\runtime.js:154:27 in invokeat node_modules\regenerator-runtime\runtime.js:164:18 in PromiseImpl.resolve.then$argument_0at node_modules\react-native\node_modules\promise\setimmediate\core.js:37:13 in tryCallOneat node_modules\react-native\node_modules\promise\setimmediate\core.js:123:24 in setImmediate$argument_0at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:130:14 in _callTimerat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:181:14 in _callImmediatesPassat node_modules\react-native\Libraries\Core\Timers\JSTimers.js:441:30 in callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:387:6 in __callImmediatesat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:135:6 in __guard$argument_0at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:364:10 in __guardat node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:134:4 in flushedQueueat [native code]:null in flushedQueueat [native code]:null in invokeCallbackAndReturnFlushedQueue```","['It looks like you might be importing the type definition without also including the script.  Check your network tab of your browser and make sure you get a 200 response for pose.js (or if you are using a packager; for a script that includes pose.js in its entirety).  Before pose.js is loaded and executed; you will get exactly this error.Let me know how it goes!====='; ""I have check @mediapipe/pose package in node_modules and all files are there.![Screenshot 2021-08-21 140823](https://user-images.githubusercontent.com/73450708/130313861-abeb74ec-55d7-4315-b616-f4333b7c97a8.jpg)I also try changing `solutionPath` value in `detectorConfig` variable to ` const detectorConfig = {        runtime: 'mediapipe';         modelType: 'lite';         solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/pose@0.4.1624666670/'  };`And it's still not working. Of course; I can open 'https://cdn.jsdelivr.net/npm/@mediapipe/pose@0.4.1624666670/' in my browser and I have access to all files there.  Moreover; I have tried uninstalling and reinstalling the different versions of the package; and still not working.=====""; 'I have the same issue. The Problem seems to be [here in Line 17 and 42](https://github.com/tensorflow/tfjs-models/blob/master/pose-detection/src/blazepose_mediapipe/detector.ts). When i try to import mediapipe; and print the imported object; i get this:```javascriptimport * as pose from \'@mediapipe/pose\'// or import * as pose from \'@mediapipe/pose/pose.js\' doesn\'t matterconsole.log(pose);// Results in:Object {  ""default"": Object {};}```So i guess the `@tensorflow-models/pose-detection` library has the same issue in importing `@mediapipe/pose`.I get some Intellisense in VSCode though; so i guess @mhays-google you are right; however this seems to be a problem regarding the `@mediapipe/pose` package somehow; right?====='; ""That's correct.  I think that some import @mediapipe/poseimpl would help with this; but I haven't put a lot of thought into it yet.  It will be a lot easier if we can just open source the typescript; but we aren't there yet.In the interim; an additional script tag (or equivalent) will be required.=====""; 'Hy Guys; Anyone found solution to this problem ? ====='; 'Hi; we built a working example here; please reference: https://github.com/tensorflow/tfjs-examples/tree/master/react-native====='; 'Example isnt with mediapipe. I am having the same problem as he is. ====='; ""Hi;Unfortunately tfjs-react-native doesn't support loading WASM binaries which mediapipe runtime uses. I will update the README to make this clear.Thanks!=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5501"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5501"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Mobile,Platform,change model,Changing model,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5497,Kernel 'Unique' not registered for backend 'tensorflow',2,closed,2021-08-15T19:57:19Z,2021-08-16T20:25:47Z,"**System information**- OS Platform: windows 10- TensorFlow.js version: tfjs-node"": ""^3.8.0""when i try to apply tf.unique method on any tensor in tfjs-nodejs; per example:```const a = tf.tensor2d([[1; 0; 0]; [1; 0; 0]; [2; 0; 0]]);const {values; indices} = tf.unique(a; 1)```I face this error line:```(node:13336) UnhandledPromiseRejectionWarning: Error: Kernel 'Unique' not registered for backend 'tensorflow'    at Engine.runKernel (C:\Users\bibs2091\Desktop\PFE\node_modules\@tensorflow\tfjs-core\src\engine.ts:548:13)    at unique_ (C:\Users\bibs2091\Desktop\PFE\node_modules\@tensorflow\tfjs-core\src\ops\unique.ts:85:9)    at Object.unique__op [as unique] (C:\Users\bibs2091\Desktop\PFE\node_modules\@tensorflow\tfjs-core\src\ops\operation.ts:51:24)    at DataBlock.fromTensor (C:\Users\bibs2091\Desktop\PFE\lib\DataBlock.ts:59:40)    at C:\Users\bibs2091\Desktop\PFE\examples\basic.ts:10:10    at step (C:\Users\bibs2091\Desktop\PFE\examples\basic.ts:33:23)    at Object.next (C:\Users\bibs2091\Desktop\PFE\examples\basic.ts:14:53)    at C:\Users\bibs2091\Desktop\PFE\examples\basic.ts:8:71    at new Promise (<anonymous>)    at __awaiter (C:\Users\bibs2091\Desktop\PFE\examples\basic.ts:4:12)```Can you fix this please. ","['tf.unique is not implemented in tfjs-node; right now it is implemented for `cpu` and `webgl` only ; here is a feature request which already exist here https://github.com/tensorflow/tfjs/issues/4595 ; please comment on #4595 so that we can prioritize accordingly. Thank you.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5497"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5497"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,TF(CPU),Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5496,backend tfjs-backend-webgpu is missing several common kernel ops,21,closed,2021-08-14T21:57:06Z,2021-10-22T06:37:33Z,after converting my existing app to run with `tfjs-backend-webgpu` built from main branch  (latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old);  i've run into issues running some of the models (all of them work perfectly using `webgl` backend):> Kernel 'TopK' not registered for backend 'webgpu'> Kernel 'SplitV' not registered for backend 'webgpu'> Kernel 'FlipLeftRight' not registered for backend 'webgpu'> Kernel 'RotateWithOffset' not registered for backend 'webgpu'note: with remaining running models; with `webgpu` inference is 300-400ms while with `webgl` it's 400-550ms  which makes `webgpu` ~30% faster than `webgl`!btw; i'll try using webgpu in web workers next as single thread does not come even close of saturating gpu  (average utilization is ~25%)environment: `chrome/94 canary` with `tfjs` 3.8.0 and `tjfs-backend-webgpu` built from main,"['@xhcao Please take a look; thanks. ====='; 'I can work on this if you say...====='; '@carrycooldude; any contribution to WebGPU backend is highly appreciated! Once you have something ready for review; you may @qjia7 and we can invite other related reviewers. Thanks!====='; 'Hi; @carrycooldude Are you implementing these kernel ops? You could submit several patches; and each patch only implements one kernel op.I know It is difficult to implement these ops; if you have some problems; we could discuss the solutions. ====='; 'Sure @xhcao I started working on it; I will let you know if I having any issue and Also If possible can you make a project tab for me for the missing kernels====='; 'Hi; @carrycooldude; How many operators had been implemented? Do you need help from me to implement them? I think @vladmandic is waiting us to implement these operator; and verify it work on his refactoring. https://github.com/tensorflow/tfjs/issues/5468You could refer the implementations from webgl backend to implement the first version and optimize them in feature.====='; 'Can you give me some PR for reference====='; '@carrycooldude Below links implement these kernels on webgl backend; you could refer them.https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/TopK.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/SplitV.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/FlipLeftRight.tshttps://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/RotateWithOffset.ts====='; 'Thanks I will start working on this====='; 'Hi; @carrycooldude; how about the process of these kernels? Currently; I have spare time; and could help you to implement the kernels; which have been not started to implement.====='; 'Yes ; Actually Want to understand the webgl code implementation with webgpu in order to get which Files I have to import to implement those kernels====='; ""Could I take over these two kernels ('FlipLeftRight' and  'RotateWithOffset')? We could implement them synchronously.=====""; 'Sure ; Also if possible can you guide me through your implementation it will be great too 😇====='; '@carrycooldude You can refer to this one #5585; which implements a new kernel `DepthToSpace` for webgpu.====='; 'Thanks @qjia7 ====='; ""@carrycooldude ; I had implemented 'FlipLeftRight' and 'RotateWithOffset' kernels; https://github.com/tensorflow/tfjs/pull/5649 ;you could refer to them.=====""; 'Thanks yunfei for implementing the `split` kernel. Now; only `topK` kernel is unimplemented; but it is the most difficult one; the algorithm of webgl backend is based on the paper https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf; I will  investigate and implement this operator with @carrycooldude; and make some optimizations in future according to webgpu features.====='; 'Hi; @vladmandic ; all these four kernel ops had been implemented now; you could try your app on master branch. If there are some other issues; please tell us; thank you.====='; ""I just run first sanity test:- no missing kernel ops- quite a lot of refactoring on my side to avoid `dataSync`    (more than I wanted since async functions cannot be used inside `tf.tidy()`)- just submitted a trivial PR #5756i'll test performance and precision next...=====""; '**webgpu** implementation of `tf.image.fromPixels()` is inverted on y-axis; so that corrupts all model outputs  ive just created issue #5757 to track thatregarding performance; its still pretty bad compared to **webgl** - that is being tracked under #5689**webgl**- warmup initial **18sec**- warmup cached **5sec****webgpu**- warmup initial **21sec**- warmup cached **20sec**recent work on `WEBGL_PACK_DEPTHWISECONV` makes **webgl** warmup much faster than in 3.9.0 release  does that have any effect on **webgpu** or should it be implemented there separately as well?btw; im using `WEBGL_USE_SHAPES_UNIFORMS=true` since that has significant performance advantages for **webgl** warmup====='; '@vladmandic Thanks for the feedback. In fact; webgpu already uses the uniforms by default. The fix on webgl is not suitable for webgpu since the shader is very different from the problem one in webgl. But we are actively looking at the warmup issue. Hope we can give a solution soon. And for the browser cache issue; the webgpu developer in chromium replied us that this features is expected to be finished in this quarter.Close this one and use the separate bugs you reported to track the left issues. Thanks.=====']",Reference Error,Crash,Unimplemented Operator,WebGPU,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5495,tfjs-backend-webgpu bad handling of non-webgpu enabled browser,2,closed,2021-08-14T17:36:43Z,2021-08-14T21:53:36Z,"attempting to run `tfjs-backend-webgpu` on a non-webgpu enabled browser results  in a cryptic error instead of stating that `webgpu` is not available:environment: chrome/92 (which is not webgpu enabled); tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7```logbackend_webgpu.ts:79 Uncaught ReferenceError: GPUBufferUsage is not defined  at backend_webgpu.ts:79  at tf-backend-webgpu.es2017.js:20  at tf-backend-webgpu.es2017.js:21```same issue previously reported under #3003 which was closed without resolution due to ""lack of activity"" - what kind of resolution is that if there is nothing needed from submitter and it's up to devs to come up with a solution??cc @qjia7","['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5495"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5495"">No</a>=====']",Reference Error,Crash,Improper Exception Handling,WebGPU,Backend,publish new package,publish new package,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.1,A.7
https://github.com/tensorflow/tfjs/issues/5494,tfjs-backend-webgpu multiple issues,3,closed,2021-08-14T17:31:28Z,2021-08-16T07:11:31Z,i've tried using `tfjs-backend-webgpu` with 10 different models and got 4 different errors  success rate so far is zero as each model resulted in at least one errorenvironment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7## this error is most common```logTint Reflection failure:Workgroup X dimension exceeds maximum allowed:512 > 256    at ReflectShaderUsingTint (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:637)    at InitializeBase (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:1238)    at Create (../../third_party/dawn/src/dawn_native/d3d12/ShaderModuleD3D12.cpp:170)    at GetOrCreateShaderModule (../../third_party/dawn/src/dawn_native/Device.cpp:703)```note that depending on model; numbers in the error message may be differente.g. `1024 > 256` or `## followed by this error```logTint Reflection failure:Workgroup shared storage size for main exceeds the maximum allowed: 32768 > 16352    at ReflectShaderUsingTint (..<URL>)    at InitializeBase (..<URL>)    at Create (..<URL>)    at GetOrCreateShaderModule (..<URL>)```same as with first error; numbers in the error message may be different depending on the model  e.g. `16640 > 16352`## this happens only on one model; `mb3-centernet````logParse failedglslang.onefile.js:46 ERROR: 0:129: 'return' : type does not match; or is not convertible to; the function's return type glslang.onefile.js:9 Uncaught (in promise) Error: GLSL compilation failed    at Object.d.compileGLSLZeroCopy (glslang.onefile.js:9)    at compileProgram (webgpu_program.ts:75)    at backend_webgpu.ts:723    at WebGPUBackend.getAndSavePipeline (backend_webgpu.ts:444)    at WebGPUBackend.runWebGPUProgram (backend_webgpu.ts:722)```## and this is a common error in preprocessing(ok; this one is a missing feature; not a bug strictly speaking)```logengine.ts:548 Uncaught (in promise) Error: Kernel 'RotateWithOffset' not registered for backend 'webgpu'  at Engine.runKernel (engine.ts:548)  at rotateWithOffset_ (rotate_with_offset.ts:58)  at Object.rotateWithOffset__op [as rotateWithOffset] (operation.ts:51)```cc @qjia7,"['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">No</a>====='; ""@vladmandic Thanks for trying the webgpu backend. Sorry that the publish release one `0.0.1-alpha.7` hasn't been updated for a while. Recently; WebGPU changed a lot to reach the Chrome's origin trials (OT). So we did many changes to satisfy  them. So for now; we suggest build` tfjs-backend-webgpu` from main branch like you did. Currently; we still need 1~2 weeks to totally switch to use WGSL instead GLSL before releasing a new npm package.And it's really appreciated that you reported the issues in webgpu and provided your feedbacks. We will follow up them and make sure the issues can be resolved as soon as possible. Please let us know if you meet any issues. Thanks.=====""]",Data & Model Error,Crash,Untimely Update,WebGPU,Backend,publish new package,publish new package,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] GLSL compilation failed or unsupported kernel""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.2,B.3
https://github.com/tensorflow/tfjs/issues/5494,tfjs-backend-webgpu multiple issues,3,closed,2021-08-14T17:31:28Z,2021-08-16T07:11:31Z,i've tried using `tfjs-backend-webgpu` with 10 different models and got 4 different errors  success rate so far is zero as each model resulted in at least one errorenvironment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7## this error is most common```logTint Reflection failure:Workgroup X dimension exceeds maximum allowed:512 > 256    at ReflectShaderUsingTint (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:637)    at InitializeBase (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:1238)    at Create (../../third_party/dawn/src/dawn_native/d3d12/ShaderModuleD3D12.cpp:170)    at GetOrCreateShaderModule (../../third_party/dawn/src/dawn_native/Device.cpp:703)```note that depending on model; numbers in the error message may be differente.g. `1024 > 256` or `## followed by this error```logTint Reflection failure:Workgroup shared storage size for main exceeds the maximum allowed: 32768 > 16352    at ReflectShaderUsingTint (..<URL>)    at InitializeBase (..<URL>)    at Create (..<URL>)    at GetOrCreateShaderModule (..<URL>)```same as with first error; numbers in the error message may be different depending on the model  e.g. `16640 > 16352`## this happens only on one model; `mb3-centernet````logParse failedglslang.onefile.js:46 ERROR: 0:129: 'return' : type does not match; or is not convertible to; the function's return type glslang.onefile.js:9 Uncaught (in promise) Error: GLSL compilation failed    at Object.d.compileGLSLZeroCopy (glslang.onefile.js:9)    at compileProgram (webgpu_program.ts:75)    at backend_webgpu.ts:723    at WebGPUBackend.getAndSavePipeline (backend_webgpu.ts:444)    at WebGPUBackend.runWebGPUProgram (backend_webgpu.ts:722)```## and this is a common error in preprocessing(ok; this one is a missing feature; not a bug strictly speaking)```logengine.ts:548 Uncaught (in promise) Error: Kernel 'RotateWithOffset' not registered for backend 'webgpu'  at Engine.runKernel (engine.ts:548)  at rotateWithOffset_ (rotate_with_offset.ts:58)  at Object.rotateWithOffset__op [as rotateWithOffset] (operation.ts:51)```cc @qjia7,"['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">No</a>====='; ""@vladmandic Thanks for trying the webgpu backend. Sorry that the publish release one `0.0.1-alpha.7` hasn't been updated for a while. Recently; WebGPU changed a lot to reach the Chrome's origin trials (OT). So we did many changes to satisfy  them. So for now; we suggest build` tfjs-backend-webgpu` from main branch like you did. Currently; we still need 1~2 weeks to totally switch to use WGSL instead GLSL before releasing a new npm package.And it's really appreciated that you reported the issues in webgpu and provided your feedbacks. We will follow up them and make sure the issues can be resolved as soon as possible. Please let us know if you meet any issues. Thanks.=====""]",Data & Model Error,Crash,Untimely Update,WebGPU,Backend,publish new package,publish new package,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] GLSL compilation failed or unsupported kernel""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,B.3
https://github.com/tensorflow/tfjs/issues/5494,tfjs-backend-webgpu multiple issues,3,closed,2021-08-14T17:31:28Z,2021-08-16T07:11:31Z,i've tried using `tfjs-backend-webgpu` with 10 different models and got 4 different errors  success rate so far is zero as each model resulted in at least one errorenvironment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7## this error is most common```logTint Reflection failure:Workgroup X dimension exceeds maximum allowed:512 > 256    at ReflectShaderUsingTint (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:637)    at InitializeBase (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:1238)    at Create (../../third_party/dawn/src/dawn_native/d3d12/ShaderModuleD3D12.cpp:170)    at GetOrCreateShaderModule (../../third_party/dawn/src/dawn_native/Device.cpp:703)```note that depending on model; numbers in the error message may be differente.g. `1024 > 256` or `## followed by this error```logTint Reflection failure:Workgroup shared storage size for main exceeds the maximum allowed: 32768 > 16352    at ReflectShaderUsingTint (..<URL>)    at InitializeBase (..<URL>)    at Create (..<URL>)    at GetOrCreateShaderModule (..<URL>)```same as with first error; numbers in the error message may be different depending on the model  e.g. `16640 > 16352`## this happens only on one model; `mb3-centernet````logParse failedglslang.onefile.js:46 ERROR: 0:129: 'return' : type does not match; or is not convertible to; the function's return type glslang.onefile.js:9 Uncaught (in promise) Error: GLSL compilation failed    at Object.d.compileGLSLZeroCopy (glslang.onefile.js:9)    at compileProgram (webgpu_program.ts:75)    at backend_webgpu.ts:723    at WebGPUBackend.getAndSavePipeline (backend_webgpu.ts:444)    at WebGPUBackend.runWebGPUProgram (backend_webgpu.ts:722)```## and this is a common error in preprocessing(ok; this one is a missing feature; not a bug strictly speaking)```logengine.ts:548 Uncaught (in promise) Error: Kernel 'RotateWithOffset' not registered for backend 'webgpu'  at Engine.runKernel (engine.ts:548)  at rotateWithOffset_ (rotate_with_offset.ts:58)  at Object.rotateWithOffset__op [as rotateWithOffset] (operation.ts:51)```cc @qjia7,"['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">No</a>====='; ""@vladmandic Thanks for trying the webgpu backend. Sorry that the publish release one `0.0.1-alpha.7` hasn't been updated for a while. Recently; WebGPU changed a lot to reach the Chrome's origin trials (OT). So we did many changes to satisfy  them. So for now; we suggest build` tfjs-backend-webgpu` from main branch like you did. Currently; we still need 1~2 weeks to totally switch to use WGSL instead GLSL before releasing a new npm package.And it's really appreciated that you reported the issues in webgpu and provided your feedbacks. We will follow up them and make sure the issues can be resolved as soon as possible. Please let us know if you meet any issues. Thanks.=====""]",Reference Error,Crash,Untimely Update,WebGPU,Backend,publish new package,publish new package,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] GLSL compilation failed due to type mismatch""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.2,B.3
https://github.com/tensorflow/tfjs/issues/5494,tfjs-backend-webgpu multiple issues,3,closed,2021-08-14T17:31:28Z,2021-08-16T07:11:31Z,i've tried using `tfjs-backend-webgpu` with 10 different models and got 4 different errors  success rate so far is zero as each model resulted in at least one errorenvironment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7## this error is most common```logTint Reflection failure:Workgroup X dimension exceeds maximum allowed:512 > 256    at ReflectShaderUsingTint (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:637)    at InitializeBase (../../third_party/dawn/src/dawn_native/ShaderModule.cpp:1238)    at Create (../../third_party/dawn/src/dawn_native/d3d12/ShaderModuleD3D12.cpp:170)    at GetOrCreateShaderModule (../../third_party/dawn/src/dawn_native/Device.cpp:703)```note that depending on model; numbers in the error message may be differente.g. `1024 > 256` or `## followed by this error```logTint Reflection failure:Workgroup shared storage size for main exceeds the maximum allowed: 32768 > 16352    at ReflectShaderUsingTint (..<URL>)    at InitializeBase (..<URL>)    at Create (..<URL>)    at GetOrCreateShaderModule (..<URL>)```same as with first error; numbers in the error message may be different depending on the model  e.g. `16640 > 16352`## this happens only on one model; `mb3-centernet````logParse failedglslang.onefile.js:46 ERROR: 0:129: 'return' : type does not match; or is not convertible to; the function's return type glslang.onefile.js:9 Uncaught (in promise) Error: GLSL compilation failed    at Object.d.compileGLSLZeroCopy (glslang.onefile.js:9)    at compileProgram (webgpu_program.ts:75)    at backend_webgpu.ts:723    at WebGPUBackend.getAndSavePipeline (backend_webgpu.ts:444)    at WebGPUBackend.runWebGPUProgram (backend_webgpu.ts:722)```## and this is a common error in preprocessing(ok; this one is a missing feature; not a bug strictly speaking)```logengine.ts:548 Uncaught (in promise) Error: Kernel 'RotateWithOffset' not registered for backend 'webgpu'  at Engine.runKernel (engine.ts:548)  at rotateWithOffset_ (rotate_with_offset.ts:58)  at Object.rotateWithOffset__op [as rotateWithOffset] (operation.ts:51)```cc @qjia7,"['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5494"">No</a>====='; ""@vladmandic Thanks for trying the webgpu backend. Sorry that the publish release one `0.0.1-alpha.7` hasn't been updated for a while. Recently; WebGPU changed a lot to reach the Chrome's origin trials (OT). So we did many changes to satisfy  them. So for now; we suggest build` tfjs-backend-webgpu` from main branch like you did. Currently; we still need 1~2 weeks to totally switch to use WGSL instead GLSL before releasing a new npm package.And it's really appreciated that you reported the issues in webgpu and provided your feedbacks. We will follow up them and make sure the issues can be resolved as soon as possible. Please let us know if you meet any issues. Thanks.=====""]",Reference Error,Crash,Untimely Update,WebGPU,Backend,publish new package,publish new package,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] GLSL compilation failed due to type mismatch""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.1,B.3
https://github.com/tensorflow/tfjs/issues/5493,tfjs-backend-webgpu incorrect tree shaking when bundling,2,closed,2021-08-14T14:58:29Z,2021-08-14T21:53:48Z,`tfjs-backend-webgpu` does not define `sideEffects` in it's `package.json` causing incorrect tree shaking to occur  result is that entire `flags_webgpu.ts` gets dropped so none of the global variables get registeredat minimum; it should follow similar `sideEffects` settings as defined in `tfjs-backend-webgl` package  but also take a look at open issue #5182environment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7cc @qjia7,"['closing as resolved when building `tfjs-backend-webgpu` from main branch;  seems that latest released version `tfjs-backend-webgpu 0.0.1-alpha.7` is just too old====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5493"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5493"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Untimely Update,WebGPU,Backend,publish new package,publish new package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Incorrect Tree Shaking""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.3
https://github.com/tensorflow/tfjs/issues/5492,posibility of a 3.8.1 release with updated adm-zip,3,closed,2021-08-14T14:09:19Z,2021-08-17T16:58:51Z,Hi; @lina128 @pyu10055; there is a high severity vulnerability introduced by package **adm-zip**:### Issue Description   I noticed that a vulnerability is introduced in **_@tensorflow/tfjs-node@3.8.0_**:     Vulnerability **SNYK-JS-ADMZIP-1065796** (high severity) affects package **_adm-zip_** (versions:<0.5.2):  [https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796)  The above vulnerable package is referenced by **_@tensorflow/tfjs-node@3.8.0_** via:`@tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`Since **_@tensorflow/tfjs-node@3.8.0_** ([2;176 downloads per week](https://www.npmjs.com/package/@tensorflow/tfjs-node/v/3.8.0?activeTab=versions)) is referenced by **18** downstream projects (e.g.; [stills 21.3.6](https://www.npmjs.com/package/stills) (latest version); [node-red-contrib-facial-recognition 0.30.105](https://www.npmjs.com/package/node-red-contrib-facial-recognition) (latest version); [@nlpjs/open-question 4.22.0](https://www.npmjs.com/package/@nlpjs/open-question) (latest version); [twitterfollowerexplorer 3.1.11](https://www.npmjs.com/package/twitterfollowerexplorer) (latest version); [roboflow-node 0.2.16](https://www.npmjs.com/package/roboflow-node) (latest version)); the high severity vulnerability [**SNYK-JS-ADMZIP-1065796**](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796) can be propagated into these downstream projects and expose security threats to them via the following package dependency paths: (1)`stills@21.3.6 ➔ @tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`(2)`twitterfollowerexplorer@3.1.11 ➔ @tensorflow/tfjs-node@3.8.0 ➔ adm-zip@0.4.16`  **......**If **_@tensorflow/tfjs-node@3.8.\*_** removes the vulnerable package from the above version; then its fixed version can help downstream users decrease their pain.**Given the large number of downstream users; could you help update your package to remove the vulnerability from @tensorflow/tfjs-node@3.8.0 ?**### Fixing suggestionsIn **_@tensorflow/tfjs-node@3.8.1_**; maybe you can kindly try to perform the following upgrade(**not crossing major version**) :`adm-zip ^0.4.11 ➔ ^0.5.2`; **Note:**_**adm-zip@0.5.2**(>=0.5.2) has fixed the vulnerability [**SNYK-JS-ADMZIP-1065796**](https://snyk.io/vuln/SNYK-JS-ADMZIP-1065796)._Thank you for your attention to this issue and welcome to share other ways to resolve the issue.Best regards;^_^,"[""Thank you @evansrobert ; submitted a fix ; let's wait for PR to get reviewed.=====""; '@rthadur Thanks for your feedback.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5492"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5492"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Dependency Error,TF(CPU),Backend,change dependency version,Modifying dependency configuration,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Dependency Error""
  }
}
```",D,B.2
https://github.com/tensorflow/tfjs/issues/5491,ssd mobilenet v3 10+ times slower than native app,12,open,2021-08-14T13:20:37Z,2021-12-14T17:44:11Z,Hello;I trained ssd mobilenet v3 (large minimalistic); non-quantized standard model (tflite).The execution time of this model in a native android application (google pixel 4a 5g) is 15 ms.Using tensorflow.js version 3.3.0 (on later versions the model doesn't work) the time is:cpu backend - 2600 mswebgl backend - 220 mswasm backend - 180 msI also tried the alpha version of tf-js tflite and it showed the best results - around 115-130 ms in any backend. But this is also very slow.1. Is it possible to somehow speed up these models?2. If not; what ssd models should be trained to have high performance on mobile phones?Thanks.,"['Have you tried using tflite for mobile version https://www.tensorflow.org/lite/examples/object_detection/overview ?====='; 'rthadur;I tried right now this model (COCO SSD MobileNet v1):webgl backend - 1200 mswasm backend - 1100 mstf-js tflite - 570 msIn accordance with the Performance Benchmarks on Pixel 4 (https://www.tensorflow.org/lite/examples/object_detection/overview#performance_benchmarks); it should work 20 ms in a native application; and works on the web almost 30 times slower than the best option - tf-js tflite====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; ""Sorry; I haven't looked into this.. Will keep this open for now and take a look when I have more bandwidth.=====""; 'Hi @Jove125; I had a brief chat with our TL. We do see some performance issues with self-trained ssd mobilenet models; and we probably need to invest more time on optimization. For now; you can take a look at [here](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd#technical-details-for-advanced-users) for some optimization ideas (for converted TFJS model). Thank you!====='; 'Closing as stale. Please @mention us if this needs more attention.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5491"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5491"">No</a>====='; ""> Hi @Jove125; I had a brief chat with our TL. We do see some performance issues with self-trained ssd mobilenet models; and we probably need to invest more time on optimization. For now; you can take a look at [here](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd#technical-details-for-advanced-users) for some optimization ideas (for converted TFJS model). Thank you!jinjingforever; thanks; I've already seen these optimization ideas.Are there any optimization ideas for the tflite model (for web)?And are there any planned timelines to optimize the performance of this solution?Thank you!=====""; ""Hi @Jove125; sorry we don't have a timeline for this work now. I will bring this up in our team meeting and hopefully it can be prioritized in the future.=====""; 'Will keep this open for now.====='; ""> Hi @Jove125; sorry we don't have a timeline for this work now. I will bring this up in our team meeting and hopefully it can be prioritized in the future.Hi @jinjingforever;Are there any updates on this issue?=====""; ""Sorry @Jove125 not yet... Some of my teammates are exploring some ideas of optimizing the webgl backend in general. Hope that will help with this issue as well. But unfortunately I don't have a concrete timeline. =====""]",Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,optimizing model,optimizing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5490,Blazeface result is not correct on WASM backend,6,open,2021-08-14T12:05:53Z,2021-10-05T23:03:34Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  Yes; I am using the estimateFace function of blazeface to detect the actual face in the image uploaded. Here is my code for reference:```  const detectFace = async () => {    const p = document.getElementById('result');    p.innerHTML = ""Detecting ..."";    console.log('Loading model...')    const model = await blazeface.load({scoreThreshold: 0.9});    console.log('Loaded model...')    var predictions;    var t0; t1;    for (j = 0; j < 5; j++) {        var t0 = performance.now()        // Pass in a video stream (or an image; canvas; or 3D tensor) to obtain an        // array of detected faces from the MediaPipe graph. If passing in a video        // stream; a single prediction per frame will be returned.        predictions = await model.estimateFaces(canvas; false);        console.log(predictions);        var t1 = performance.now()    }    if (predictions.length > 0) {        var prediction = predictions[0]        var text = ""The face detection completed in "" + (t1 - t0) + "" ms.<br>The face matched with predictions of "" + predictions[0].probability[0];        text += ""<br>""        text += `<strong>Right Eye:</strong> <br> x = ${prediction.landmarks[0][0]}; y = ${prediction.landmarks[0][1]}<br>`        text += `<strong>Left Eye:</strong> <br> x = ${prediction.landmarks[1][0]}; y = ${prediction.landmarks[1][1]}<br>`        text += `<strong>Nose:</strong> <br> x = ${prediction.landmarks[2][0]}; y = ${prediction.landmarks[2][1]}<br>`        text += `<strong>Mouth:</strong> <br> x = ${prediction.landmarks[3][0]}; y = ${prediction.landmarks[3][1]}<br>`        text += `<strong>Right Ear:</strong> <br> x = ${prediction.landmarks[4][0]}; y = ${prediction.landmarks[4][1]}<br>`        text += `<strong>Left Ear:</strong> <br> x = ${prediction.landmarks[5][0]}; y = ${prediction.landmarks[5][1]}<br>`        renderPrediction(predictions);    } else {        var text = ""No face found in the image."";    }    p.innerHTML = text;}```- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  This behaviour is reproducible across multiple OS of both desktop & mobile devices.- TensorFlow.js installed from (npm or script link):  https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0- TensorFlow.js version (use command below):  3.1.0  - Browser version:  this behavior is reproducible across multiple Browser versions including latest Chrome 92**Describe the current behavior**I am using the estimateFace function of blazeface to detect the actual face in the image uploaded. It is giving `0.9804430603981018` probability when I placed the mobile device image to detect. If the face is present in the frame; then it detects properly the face and results are as expected. But if we don't have any face in the frame; in that case the blazeface detection results shows that face exists while it is Mobile device with `0.9804430603981018` probability with 0.9 scoreThreshold. (Attaching the image for reference)**Describe the expected behavior**It should show result of face detected only if any face is present in frame. If there is no face in frame; then there is no point to show face detected with such high probability i.e. of 98% for mobile device on frame.**Standalone code to reproduce the issue**You can use the following codepen link to reproduce: https://codepen.io/deepanshusharma012/pen/MWmRaLWAttaching the mobile device pic on which the issue is reproducible; Also the blazeface models that I'm using for face detection.`Issue Reproducible Snapshot:` ![Screenshot from 2021-08-14 17-17-12](https://user-images.githubusercontent.com/41855523/129445669-3d1142af-9f78-49ad-b085-b82153f84f39.png)`Image to reproduce the issue`![Proctoring Snapshot - 2021-08-12T11_03_56 388Z](https://user-images.githubusercontent.com/41855523/129445676-30295d7a-04b3-4b12-9da5-3056877a7174.jpeg)`Blazeface models used in face detection:`[blazeface models.zip](https://github.com/tensorflow/tfjs/files/6986431/blazeface.models.zip)","[""@lina128 @rthadur Hope you are doing great!Is there any update on this issue or any ETA to resolve it? Actually this issue is a blocker for our application and detecting the false events.It would be great if you respond on it and try to roll it's fix asap.=====""; ""Hi @deepanshusharma012 ; thank you for reporting this. I tested it with our own demo; same problem. Actually we're working on a new BlazeFace model; which is going to be more accurate. It will be released this quarter.=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; ""The work is still going on this issue. I hope we'll be getting activity on this issue; so no need to mark it stalled.=====""; 'Hi @lina128; as you said the new BlazeFace model (more accurate one) will be released by Q3 quarter. So; are we still working on it; or it has released? or any ETA for the same.====='; 'Sorry; I actually meant Q4. I replied in September; and we are already thinking for Q4 :) Sorry for the confusion.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Model API,API,change model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5488,Replace deprecated `setWasmPath` with `setWasmPaths`,3,closed,2021-08-13T14:03:16Z,2021-08-13T15:10:18Z,Bad way (only uses `tfjs-backend-wasm.wasm`): https://github.com/tensorflow/tfjs-models/blob/8faa92d8933d8036deae63b034db399c09173a2e/facemesh/demo/index.js#L27Good way (uses `tfjs-backend-wasm-simd.wasm` or `tfjs-backend-wasm-threaded-simd.wasm` if possible):https://github.com/tensorflow/tfjs-models/blob/8faa92d8933d8036deae63b034db399c09173a2e/face-landmarks-detection/demo/index.js#L28,"['Hi @kungfooman `facemesh` is no longer supported ; please refer to `face-landmarks-detection `. Thank you ![image](https://user-images.githubusercontent.com/43972606/129371594-f0bd395e-b5e7-45d6-ab4b-4d8279ed8536.png)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5488"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5488"">No</a>====='; 'Well; too bad then; `facemesh` detects my face better than `face-landmarks-detection`.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Model API,API,change model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Incorrect Instructions""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.4] Confused Document""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5486,Blazeface result is not correct on WASM backend,3,closed,2021-08-13T05:37:07Z,2021-08-13T19:57:27Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):  This is happening even in demo page for blazeface model (i.e. https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html).- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:  Haven't checked yet on mobile; but probably it will exist in all the devices- TensorFlow.js installed from (npm or script link):  https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0- TensorFlow.js version (use command below):  3.1.0- Browser version: Chrome 91**Describe the current behavior**  Open the blazeface model demo page on `https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html`.   It is detecting the face on hands kept in front of webcam. If the face is present in the frame; then it detects properly the face and results are as expected. But if we don't have any face in the frame; in that case the blazeface detection results shows that face exists while it is Hand/mobile; etc.**Describe the expected behavior**  It should show result of face detected only if any face is present in frame. If there is no face in frame; then there is no point to show face detected.**Other info / logs** ![Screenshot from 2021-08-13 11-03-57](https://user-images.githubusercontent.com/41855523/129309933-86df836d-f3b2-4e0b-8a15-8fab539dc4cf.png)![Screenshot from 2021-08-13 11-04-43](https://user-images.githubusercontent.com/41855523/129309935-04ec7f13-ff90-4f0d-8b3a-586488615063.png),"['It tries to detect all the object but you need to check the probability ; which will vary for actual face and some random objects. You can try to get the code locally and check for percentage accuracy.This question is better asked on [Discourse-TF](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5486"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5486"">No</a>====='; ""@rthadur Can you please tell me whats the probability of actual face should be then?As; we are calculating the probablity of face using the face landmarks. But with those; it's giving 98% probability of actual face on mobile device.Attaching the image for same.![Screenshot from 2021-08-14 01-16-28](https://user-images.githubusercontent.com/41855523/129411377-b094200b-25f3-4a54-8545-2c809b22a9dc.png)=====""]",Incorrect Functionality,Incorrect Functionality,Confused Document,Model API,API,add data postprocess,Add data processing,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,B.4
https://github.com/tensorflow/tfjs/issues/5482,fromPixels results are inconsistent for image input,5,closed,2021-08-13T03:01:37Z,2021-11-22T20:25:25Z,"Reproduce steps:  1. Build and run e2e(yarn build-deps; npx-http-server); then load bodypix or posenet; select input type image.   2. Click “Test correctness”; when the test is done;  click it again.   3. Check the results from console; ypu will see the results mismatch.Problem investigation:  This problem is caused by chromium's canvas implementation. It can be workaround with recreate canvas context every time; or turn on chrome://flags/#new-canvas-2d-api. I have filed a bug for chromium here: https://bugs.chromium.org/p/chromium/issues/detail?id=1239467; and WIP fix here: https://chromium-review.googlesource.com/c/chromium/src/+/3092523.Test case:     Besides above reproduce steps; you can also use below single page case to reproduce this issue:```<meta charset=""utf-8""><meta name=""viewport"" content=""width=device-width;initial-scale=1;maximum-scale=1.0; user-scalable=no""><body>    <div id=""info"" style='display:none'></div>    <div id=""predictions""></div>    <video id=""video"" playsinline style=""    -webkit-transform: scaleX(-1);    transform: scaleX(-1);    display: none;    width: auto;    height: auto;    "">    </video>    <canvas id=""output"" style=""""></canvas>    <canvas id=""hand_cut"" style=""""></canvas>    <div id=""status""></div>    <script src=""https://unpkg.com/@tensorflow/tfjs-core@latest/dist/tf-core.js"" crossorigin></script>    <script>        function getURLState(url) {            let params = new URLSearchParams(url);            const keys = [...params.keys()];            if (keys.length === 0) return 0;            if (params.has('recreate')) {                return true;            }            return false;        }        async function loadImage(imagePath) {            const imageBucket =                'https://storage.googleapis.com/tfjs-models/assets/posenet/';            const image = new Image();            const promise = new Promise((resolve; reject) => {                image.crossOrigin = '';                image.onload = () => {                    resolve(image);                };            });            image.src = `${imageBucket}${imagePath}`;            return promise;        }        function consoleLogArray(array; printLen = 3; comment = '') {            if (printLen > array.length) {                throw new Error(""Print len is bigger than array.length"");            }            console.log(comment + array.slice(0; printLen).toString());        }        async function fromPixelWrap(recreateContext = false) {            const image = await loadImage('tennis_standing.jpg');            const fpImage = tf.browser.fromPixels(image);;            return fpImage;        }        async function testFromPixel() {            const recreateContext = getURLState(window.location.search);            console.log(recreateContext);            const fpImage1 = await fromPixelWrap(recreateContext);            const fpImage2 = await fromPixelWrap(recreateContext);            consoleLogArray(await fpImage1.data());            consoleLogArray(await fpImage2.data());            tf.test_util.expectArraysClose(await fpImage1.data(); await fpImage2.data());        }        const bindPage = async () => {            await testFromPixel();        }        bindPage();    </script></body>```<html><body><!--StartFragment-->Google Chrome | 94.0.4605.0 (Official Build) canary (64-bit) (cohort: Clang-64)-- | --Revision | a03befa72fc2a3d296e323116474694029ca21aa-refs/branch-heads/4605@{#1}OS | Windows 10 OS Version 2004 (Build 19041.1110)JavaScript | V8 9.4.146<!--EndFragment--></body></html>","['Hi @axinging; will clearRect() before drawing another image help?====='; '@lina128 ; indeed I have tried the clearRect; not work. Currently the workarounds are : recreate canvas context every time; or turn on chrome://flags/#new-canvas-2d-api.====='; '@axinging Will adopt the recreate canvas context every time for now and mitigate to new API when it is released based on https://bugs.chromium.org/p/chromium/issues/detail?id=1239467#c5====='; ""Hi; we tested this behavior on three browsers. Safari and Firefox don't have this issue; the first draw are both on GPU. Only Chrome has this issue. For Chrome; we explored three methods; all of them have some caveats. (See this PR for detail: https://github.com/tensorflow/tfjs/pull/5864) Method 1 is the most promising; but the problem is that it is still not very reliable. In some environments; the first few drawImage calls happen in CPU (e.g. if they are in the same scope in JS); so drawing one more time still doesn't work in these cases. Also this approach will incur duplicate fromPixel call to draw; download and upload data. We think what you observed most likely only happens in test; and in reality it's unlikely someone wants to call fromPixels on the same image twice. But it's a nice finding; we can add a warning in this method's doccomment; so that people are aware of this; and if they really want the two calls to yield exactly same result; they can call fromPixels three times and only use the last two results for comparison. The same applies to your test. We can change the benchmark test logic so that it creates tensor from a data array (basically ImageData) that represents the image. After all; we don't want to compare different rendering engine; we want to compare;whether there's difference between backends.Given that we have a workaround; we prefer not to modify the original behavior; as stated above the fix would add unnecessary computation and is not reliable.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5482"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5482"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Browser Incompatibility,Browser,Platform,behavior logic,behavior logic,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,D.2
https://github.com/tensorflow/tfjs/issues/5479,WEBGL_USE_SHAPES_UNIFORMS causes test failures on MacOS,3,closed,2021-08-12T23:06:30Z,2021-08-23T16:14:08Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOS 10.13.6- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: Chrome 92.0.4515.107- Tensorflow.js Converter Version:**Describe the current behavior**WebGL tests using `WEBGL_USE_SHAPES_UNIFORMS` fail on MacOS Chrome 92.**Describe the expected behavior**WebGL tests using `WEBGL_USE_SHAPES_UNIFORMS` pass on MacOS Chrome 92.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.`yarn run-browserstack --browsers=bs_chrome_mac --testEnv webgl2 --flags '{""WEBGL_USE_SHAPES_UNIFORMS"": true}'`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```Chrome 92.0.4515.107 (Mac OS 10.13.6) where webgl2 {""WEBGL_USE_SHAPES_UNIFORMS"":true} Tensor3D FAILED	Error: Arrays differ: actual[2] = 3; expected[2] = 5.```[More logs in this nightly run](https://console.cloud.google.com/cloud-build/builds/b634d7e6-4b93-46b4-b484-1a5c5f99d578?project=834911136599)","['I will take a look why there are so many failures with WEBGL_USE_SHAPES_UNIFORMS = true.====='; ""Fixed by #5502  (not sure why the automation didn't close it).=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5479"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5479"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,variable replacer,variable replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5475,[tfjs-react-native] tf.loadGraphModel with bundled files slow,6,open,2021-08-12T14:25:49Z,2021-11-07T23:46:19Z,"**Packages installed**- ""react-native"": ""0.63.4"";- ""@tensorflow/tfjs"": ""^3.8.0"";- ""@tensorflow/tfjs-automl"": ""^1.2.0"";- ""@tensorflow/tfjs-react-native"": ""^0.6.0"";- ""expo-gl"": ""^10.4.2"";- ""expo-gl-cpp"": ""^10.4.1"";- Tensorflow.js Converter Version: v3.6.0 (as reported by the model file)**Describe the current behavior**Loading a graph model (13MB) bundled in a react-native application (bare) takes an excessive amount of time on Android devices; taking upwards of 60s.**Model**We have trained an image classification model using Google Vision and exported it as a Tensorflow.js package using their dashboard. After downloading it from the Google Vision dashboard we haven't modified the model in any way.The model.json file weighs 167KB. The weights are sharded into 4 files (each named group1-shardxof4.bin); three of which weigh 4.2MB and the last 400KB; totalling to 13MB.The three top lines of the model.json file read:```""format"": ""graph-model"";""generatedBy"": ""2.7.0"";""convertedBy"": ""TensorFlow.js Converter v3.6.0"";```**Init code**As for the model initialization in the application; we make sure that tensorflow is ready by running and awaiting tf.ready() early on in the application's lifecycle and making sure it resolves successfully before loading the model.In the file where classification happens; we then import the necessary libraries; require the bundled model files and loadGraphModel; such as:```jsimport * as tf from '@tensorflow/tfjs';import { bundleResourceIO } from '@tensorflow/tfjs-react-native';const modelJson = require('../../../assets/model/model.json');const modelWeights1 = require('../../../assets/model/group1-shard1of4.bin');const modelWeights2 = require('../../../assets/model/group1-shard2of4.bin');const modelWeights3 = require('../../../assets/model/group1-shard3of4.bin');const modelWeights4 = require('../../../assets/model/group1-shard4of4.bin');// Basic model setup; code wrapped in a functional component which I'm not including hereconst setupModel = async () => {  try {    // Create GraphModel    const ioHandler = bundleResourceIO(modelJson; [ modelWeights1; modelWeights2; modelWeights3; modelWeights4 ]);    const graphModel = await tf.loadGraphModel(ioHandler);    // Save model for further use    // ...  }  catch (modelSetupError) {    // ...  }};useEffect(() => {  setupModel();}; []);```graphModel is later used to create and store an automl.ImageClassificationModel; which isn't relevant here.**The issue**The issue is the **tf.loadGraphModel** method; which takes **upwards of 60s** to resolve on slightly older Android devices - such as Nexus 5x - while making the app interface completely unresponsive in the meantime.Running the application as a built release apk resulted in:Xiaomi Redmi 7: taking ~24s to load the model;Nexus 5x: taking **~60s to load the model**;Samsung A10: taking ~78s to load the model.For comparison:when ran locally on an iPhone 7 or 8 plus: ~3s to load the model;when archived; downloaded from Testflight and ran on iPhone7 or 8 plus: ~9s;when serving the model files from a locally ran node server and loading the model through http using automl.loadImageClassification(modelUrl): **17s to load the model on Nexus 5x**.**Describe the expected behavior**I would expect the loadGraphModel method to resolve faster than the reported times. Given that it's pretty simple; with only ~3k images used to train; but it's hard for me to judge whether the observed load times are reasonable and would love for someone to let me know what kind of load performance could be expected from a 13MB model. Also; if anyone could point me to how the model load time could be optimized in react-native; any steps that could've been missed and would affect the load time or a better approach to using the Google Vision-trained model (automl) in react-native; I'd greatly appreciate that 🙂 Additionally; is there any proven way of loading the model without completely blocking the js thread while it happens?Let me know if any additional information would be helpful to resolving the issue 🙂 ","['Any chance of a follow-up?====='; '@Caundy thank you for the detail report ; is the slowness happening for first load or every time ; if it happens for first time it is expected and you may need to write a warm up code to optimize it before loading ?====='; ""@rthadur Thanks for responding :)To answer your question; I built the apk; ran it on the Nexus and ran a couple of consecutive model loads using the tf.loadGraphModel method to test the times. The resulting load times are inconsistent; with the first load **not** taking significantly longer than the later ones. While most of the time it takes ~61 seconds to load the model - regardless of whether it's the first time or the fifth - I've also seen it being loaded in 25s; only to be reloaded in ~61s right after. The loads also only appear to either take ~61s or ~25s; with no in-between.In general I would be aiming to load the model just once per application session and store it in some service property to be used when needed without the necessity of reloading it for each use. That - along with the fact that loading the model blocks users from interacting with the app - is why the initial load time is important to me :) I am aware that the first **inference** might be much slower than the consecutive ones due to caching that happens on that first run; but does that also apply to **loading the model itself**? How would I go about writing the code to optimize it before loading it?=====""; '@Caundy did you get chance to look at this older issue https://github.com/tensorflow/tfjs/issues/2177 which was dealing with similar bundle issue ?====='; ""@rthadur I believe the similarities end with both issues using the tfjs-react-native package and bundled model files. I'm not experiencing any issues using the bundleResourceIO method; as that handler resolves successfully and quickly. I am experiencing extremely long model load times using the loadGraphModel method; which are either 61s or 25s to load the model; seemingly without any rule to it.=====""; 'very same issue (the difference is that the graph model NEVER loads); on ios in release mode. in debug mode everything works like a charm. i am using expo. =====']",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,D.1
https://github.com/tensorflow/tfjs/issues/5468,webgpu backend does not support sync calls,7,closed,2021-08-11T16:44:22Z,2021-10-13T12:20:10Z,since this is documented in code as by-design; i'm flagging it as a feature request instead of a bug  but in reality its a blocker for future `webgpu` adoption  ```jsconst data = tensor.dataSync();```results in error:```logindex.js:883 Error: WebGPU readSync is only available for CPU-resident tensors.    at WebGPUBackend.readSync (backend_webgpu.ts:338)    at Engine.readSync (engine.ts:1206)    at Tensor.dataSync (tensor.ts:364)```i know same functionality can be achieved by using```jsconst data = await tensor.data();```but that requires quite a lot of changes to existing apps - so unless even when `webgpu` is widely available; it will not be able to use it as a plug-and-play instead of existing `webgl` backend  even worse; `tf.tidy()` does not support async calls. which means that memory management became soo much more complicated.and there are other JS functions that don't support async/await calls; for example `array.map()`; so any iterator would have to be reworked to use for loops.also note that usage of `dataSync()` is not accidental to start with since its faster for very small tensors and where blocking main thread is irrelevantall-in-all; support for sync calls is a must or webgpu is a non-starter.environment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7,"[""@vladmandic It is a known issue for us that webgpu does not support sync read. Another similar issue is #5092. Currently WebGPU spec only provides the async one `mapAsync`. There are some discussions whether to support a `readBuffer` API in WebGPU https://github.com/gpuweb/gpuweb/issues/1972. But it properly continues after the WebGPU MVP. We will follow up this issue and provide some inputs from TFJS side that we need the sync read API badly. For now; I'd suggest you try to use `data()` instead `dataSync` in your application. It's glad to know that you are trying the webgpu backend. Do you see any other problems that webgpu may be missing?=====""; ""thanks for the reference linki'll play with `webgpu` on a side; but i can't test it is any of my existing code due to extensive use of `tf.tidy` which doesn't support async calls.and rewritting code to use `await tensor.data()` would not be trivial as main reason for `tf.tidy` is so i can use chained calls.```jsconst scores = tf.tidy(() => predictions.slice([0; 0]; [-1; 1]).sigmoid().squeeze().dataSync());```or```jsconst data = tf.tidy(() => input.cast('float32').div(255).sub(0.5).expandDims(0).arraySync());```without `tf.tidy()` this would be 4 temp variables and later 4 calls to `tf.dispose()` - going from one-liner to 9 lines plus refactoring the caller functions to deal with async results  without `webgpu` supporting sync reads; only option i see is enhancing `tf.tidy` so it works with async calls - not sure how feasible is that?  and given the reluctance of `webgpu` team to implement sync reads; that may be what's needed...`array.map` not supporting async calls (ok; it can be wrapped into `await promise.all`; but that is really not efficient) is a different story and not `tfjs` specific; so lets stick to `tfjs` specific issues.=====""; ""@vladmandic We have some discussion with `webgpu` team to implement sync reads. Our feedback is `sync reads` is not a good solution; which will block the main thread and hurt the performance. You can find the detailed explanation here https://github.com/gpuweb/gpuweb/issues/1972#issuecomment-900117505. Even they provide one finally; it will be available only in workers https://github.com/tensorflow/tfjs/pull/5505#issuecomment-904917818; which require putting the whole webgpu backend to worker. It will produce new issues; for example; how to support Canvas/HTMLVideoElement in workers.As for the `tf.tidy`; I think you can still put the sync part in the tidy and move the async API out of tidy(). ```const scores = tf.tidy(() => predictions.slice([0; 0]; [-1; 1]).sigmoid().squeeze().dataSync());```=>```const result = tf.tidy(() => predictions.slice([0; 0]; [-1; 1]).sigmoid().squeeze());const scores = await result.data();result.dispose()``` I ever asked @lina128 about the tf.tidy() issue. She said `tidy doesn't support async calls; because it is not designed for async methods; and async memory management is pretty manageable in our experience.`. @lina128 @pyu10055 Can we provide a `tf.tidyAsync()` API for developers since we are exposing the `tf.data()`?=====""; '@qjia7 I know how to make it work (and my example was a trivial one) - the problem is when a method wrapped in `tf.tidy` is a long one and uses interim results inside the method (e.g.;  checking scores before proceeding to process strides). It can all be made to work; question is of effort  Moving forward; I will not rely on sync calls or tf.tidy at all for new code; but the problem is existing code base  And just to illustrate; just take a look at your own official pre-built TFJS modules at <https://github.com/tensorflow/tfjs-models> - From the first 10; **NONE** would work with `tfjs-backend-webgpu` without major refactoring  ====='; "">And just to illustrate; just take a look at your own official pre-built TFJS modules at https://github.com/tensorflow/tfjs-models - From the first 10; NONE would work with tfjs-backend-webgpu without major refactoringI haven't looked at the latest status of all of those models. But we have some local test for the posenet/pose-detection demos with webgpu backend. The changes are very easy; just like webgl backend. For example; https://github.com/tensorflow/tfjs-models/pull/824. Apply that PR; and use a local build webgpu npm package to replace the release one (since it doesn't work now.). Then you can test the webgpu backend for that demo. For some demos; if you meet `WebGPU readSync is only available for CPU-resident tensors.` error by some ops implementation which use `dataSync` rather than the user directly calling `tensor.dataSync()`; it should be a bug in webgpu that we need to fix.Can you point me the code snippets that you have seen that need major refactoring in https://github.com/tensorflow/tfjs-models ? I may have misunderstandings.=====""; ""> The changes are very easy; just like webgl backendi've finished refactoring my code and it works  (except for missing kernel ops required for some models as i've documented in #5496)  i have no problem with `data` vs `dataSync`; my comments are more general how it's a breaking change that will cause some solution incompatibilities with the other backends  off-topic; i think moving forward i'll just avoid `tf.tidy` completly and use silly anti-pattern like this to make sure all tensors are deallocated at the end of the method: `for (const tensor of Object.keys(t)) tf.dispose(t[tensor])` where `t` is an object containing all tensor variables instead of creating each as a separate variable=====""; 'closing this issue as its per-design  i hope this is well documented since a lot of ppl are going to run into this once webgpu is released  =====']",Reference Error,Crash,Unimplemented Operator,WebGPU,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Feature Gap in WebGPU Support""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5467,webgpu backend incompatible with web workers,6,closed,2021-08-11T16:36:45Z,2021-08-16T20:18:03Z,trying out `tfjs-backend-webgpu`; there are quite a few instances in the code where it attempts to access **DOM** elements which are by definition *not available* in web workers:```jsif (!(pixels instanceof HTMLVideoElement) ...``````logUncaught (in promise) ReferenceError: HTMLVideoElement is not defined```environment: chrome/94 canary; tfjs 3.8.0; tfjs-backend-webgpu 0.0.1-alpha.7,"['Ah; yea. Thanks for catching this.====='; ""@vladmandic  I've made a fix for this issue here https://github.com/tensorflow/tfjs/pull/5472 but I've got some problems to add a worker test to cover this in tfjs for now. I've checked the type checker part in some local framework in worker and it won't report the error.And It is appreciate that if you could help to integrate this fix and build the latest tfjs webgpu backend to see whether there is other issues remain to run this backend in worker. cc @qjia7 too.=====""; ""@shaoboyan had some issues building new `tfjs-backend-webgpu` as i didn't realize that your patch is on top of your tree; not main. anyhow; all sorted and tested - it works!  =====""; '@vladmandic Big thanks!====='; 'Thank you ; closing this issue as the PR has been merged and latest changes will be available in next release.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5467"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5467"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,WebGPU,Backend,type/env checker,Fix environment adaptability,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5466,tensorflowjs_converter does not work for Tensorflow Object Detection API faster-rcnn model,9,closed,2021-08-11T12:30:39Z,2021-08-26T15:59:08Z,I trained the **faster_rcnn_resnet50_v1_640x640_coco17_tpu-8 model** using custom dataset using my laptop and exported the model into .pb format as shown below.![image](https://user-images.githubusercontent.com/28196102/129028169-1e563618-abe0-43cb-8651-5517ddfa9fd9.png)Then I tried using the 'tensorflowjs_converter' to convert it into .json and bin file. This method works for SSD MobileNet and EfficientDet model but an error pops up whenever I try to convert the faster rcnn .pb model.This is the **error**:![image](https://user-images.githubusercontent.com/28196102/129028666-558462f7-ca05-4240-95ee-4c8f68b2851d.png)Please let me know how can I solve this problem?Thank you in advance.,"['As the error message suggests op `broadcast` is not implemented in webgl and wasm ; this would be a feature request. Did you try loading in cpu ?====='; 'related feature request in wasm https://github.com/tensorflow/tfjs/issues/3580====='; 'No; I haven’t try loading it in CPU; may I know what is the issue with loading it in GPU? How should I proceed? Also; I am not sure how to load it in CPU …====='; 'We have a PR that is adding the missing BroadcastArgs op.ref https://github.com/tensorflow/tfjs/pull/4636====='; 'Thank you for replying; may I know if the BroadcastArgs op has been added already? If it has already been added; does that mean I need to uninstall tensorflowjs and reinstall it again?====='; 'Not yet; likely will be in the next release. thanks====='; 'Alrighty; thanks again. Looking forward to the upgraded version.====='; 'Closing this issue as the related PR has been merged ; as mentioned above changes will be available in next release. Thank you.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5466"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5466"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5460,Collect kernels that are heavily used in models and call for uniforms,1,open,2021-08-11T03:00:35Z,2021-08-11T03:12:10Z,To resolve #5205; we are using uniforms instead of constants to reduce shader variants. There are two kinds of uniforms. One is shapes uniforms; which is behind a flag `WEBGL_USE_SHAPES_UNIFORMS` and is for shaders common parts used in `shader_compiler`. The other is the custom uniforms `customUniforms`; which is used for each kernel's `userCode` part. So to make a kernel whole uniform; we need to 1) enable shapes uniform 2) use custom uniforms. Currently; below kernels have supported whole uniform.1. unary ops2. binary ops3. im2col program4. encode/decode [packed] matrix program5. depthwiseconv2d6. reshapeSince webgl supports lots of kernels; we'd like to support the most frequently used kernels list instead of all of them. Once we confirm that uniforms do bring big warmup time advantages and small impact for the predict time; we will finish all of them and totally switch to uniforms by default. Here I'd like to collect the ops that are heavily used; but haven't been supported with uniforms.matmul is the ono we are working on; but we met some [perf issue](https://github.com/tensorflow/tfjs/pull/5297/files#diff-ae73f9d8e507323398a73f338b6cba08e197afddf6595e7d9ab20161dcd526a1R88),['@pyu10055 @lina128 @jinjingforever Please provide the ops that are heavily used in models and call for uniforms.====='],Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,webgl shader type replacer,Replace data Shape/type,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5454,TFJS WebGL on WebWorker still blocks GUI,13,open,2021-08-10T20:44:16Z,2021-10-26T01:13:22Z,"After investigating a [report by a user](https://discuss.tensorflow.org/t/how-to-get-the-duration-of-predict/3311/18) who was attempting to use web workers + tfjs to reduce lagging of GUI when model performs inference I have noticed that when using backend ""webgl"" web worker makes no effect on executing code in a separate context as this does not apply to GPU.I have confirmed that by setting backend to ""cpu"" there is no performance issue; as the TFJS execution correctly is executed on a new thread; such that the browser DOM updates are not interfered with. I have also confirmed that browser relies on GPU to update DOM for anything visual - not CPU.Thus the request here is how to limit ""webgl"" execution to leave enough processing for DOM updates for other user tasks to prevent this ""jankiness"" from occuring.Demo of issue:https://codepen.io/jasonmayes/project/editor/DBYaRjSimply change first line of tfWorker.js to import one of:`importScripts('dist/cpu.js');`or for WebGL backend change to:`importScripts('dist/main.js'); `Confirmed this issue across devices including Windows 10; Desktop; Chrome and also Xiaomi 8 (Android phone with Snapdragon 845 processor)Example output from Chrome Dev tools shows GPU block when using WebGL backend and also that all execution comes from WebWorker.js.![unnamed](https://user-images.githubusercontent.com/4972997/128932303-e422ed1d-ef87-46dc-bc36-ee2aac0fe260.png)","['Will the next version solve this problem???I also encountered this problem====='; 'Team is currently discussing issue to see what may be best path of action for this as WebGL seems to be shared resource unlike like CPU on WebWorker which separates execution so that 2 processes can not eat the resources of the other. Please check back on this bug for updates. In the meantime if you are able to execute on ""WASM"" for your TFJS model or even ""CPU"" I have confirmed these work as CPU based and when in web worker execute on a different thread as intended which does not block the GUI. It is only WebGL backend that is effected by this for now which is often the default form of execution for models if no backend is specified.====='; 'I don\'t understand. Which part of WebGL is not threaded? Can you provide alink to a description of the underlying WebGL issue?You can certainly render to offscreen canvases using a second GL contextwithout blocking the main thread. Sure the GPU is a shared resource; but isit really true that the browser doesn\'t use a separate GL context andoff-screen rendering in the web worker threads? Or is it something aboutTFJS that is improperly using WebGL?On Wed; Aug 11; 2021; 11:59 AM Jason Mayes ***@***.***> wrote:> Team is currently discussing issue to see what may be best path of action> for this as WebGL is not ""threaded"" like you can do with CPU on WebWorker.> Please check back on this bug for updates. In the meantime if you are able> to execute on ""WASM"" for your TFJS model or even ""CPU"" I have confirmed> these work as CPU based and when in web worker execute on a different> thread as intended which does not block the GUI. It is only WebGL backend> that is effected by this for now which is often the default form of> execution for models if no backend is specified.>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/5454#issuecomment-897073644>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAEQE2KWVKC5KWWWYYCBHA3T4LCCZANCNFSM5B43IDPA>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>> .>====='; 'I made a slightly simplified version of the code to the demo the original reporter created here: https://codepen.io/jasonmayes/project/editor/DBYaRj Executing this - click the big button at the bottom - and you will notice the webcam feed freezes as do the red numbers rendered to the left of the button.If I change the TFJS backend to CPU it does not have this ""lag"".I am unsure of the exact reason that is causing this; if you have suggestions of what it could be please do feel free to suggest; but the only difference here is the backend being used indicating something is not sharing the GPU as intended when using WebGL backend.I will follow up with the team as to how WebGL execution works in TFJS when called from a WebWorker context in case something needs to be changed there. ====='; 'Seems like there\'s at least an equal chance the bugs are in TFJS as inWebGL. I\'d also suggest testing in other browsers; e.g. Firefox. Some linksdiscussing non-blocking WebGL rendering in web workers:Chrome: https://developers.google.com/web/updates/2018/08/offscreen-canvasFirefox (see WebGL Worker <https://github.com/kripken/webgl-worker> repo):https://research.mozilla.org/2014/07/22/webgl-in-web-workers-today-and-faster-than-expected/TFJS: ***@***.***/webworker-in-tensorflowjs-49a306ed60aaMany more on D3 and other tools with simple web searches.This is an issue that would definitely benefit from some investigation bythe TFJS dev team as performance regressions in WebGL are easy to miss.Updates can break the fast paths; and careless data copies and transferscan easily overwhelm performance gains.On Thu; Aug 12; 2021; 9:53 AM Jason Mayes ***@***.***> wrote:> I made a slightly simplified version of the code to the demo the original> reporter created here: https://codepen.io/jasonmayes/project/editor/DBYaRj>> Executing this - click the big button at the bottom - and you will notice> the webcam feed freezes as do the red numbers rendered to the left of the> button.>> If I change the TFJS backend to CPU it does not have this ""lag"".>> I am unsure of the exact reason that is causing this; if you have> suggestions of what it could be please do feel free to suggest; but the> only difference here is the backend being used indicating something is not> sharing the GPU as intended when using WebGL backend.>> —> You are receiving this because you commented.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/5454#issuecomment-897802279>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAEQE2IILXYAP4OHDPMVBN3T4P4A3ANCNFSM5B43IDPA>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>> .>====='; ""In my first glance at your example; you are likely to be bound by data transfer rather than compute. You are copying image buffers from the video to the webworker using `postMessage({img: bufferT; taskType: 'tf_model'})`; which copies the bufferT. Instead you should be using the second argument for postMessage to send the buffer as a Transferable. You should also test it out with much larger images; to ensure you are compute bound. I'm not exactly sure what your model is doing; but also make sure that you do enough computation entirely down on the GPU to minimize the texture download costs.I'm not totally sure; but you are also not using any of the resulting computed results; which is a bit artificial. However; that may be fine for this testing; if you want to ignore transfer costs from the GPU back to the DOM.=====""; 'The original user who reported the issue can not share their model / all code so instead for now has shared the demo code you see above and put some placeholder code to execute as detailed here in place of the real model that replicates the issue they had: https://discuss.tensorflow.org/t/how-to-get-the-duration-of-predict/3311/18 which is what is being executed when `tfWrapper.processImg(inputFloat); `  is called.```  let testRand = tf.randomNormal([1024 * 1024 * 4]);  let {values; indices} = tf.topk(testRand; 1024 * 1024);  let valueArray = values.arraySync();  let indArray = indices.arraySync();```====='; 'I think the next steps are:1. Check the `tf.randomNormal` implementation. I\'d be surprised if that runs on the GPU at all (WebGL does not have random number support). Often this is simulated in a number of ways if you do want it to run within a shader; e.g. using a texture filled with `drand48()` values from the CPU. Regardless; it is likely doing some CPU work and we want to know why it isn\'t doing that work within the Worker thread.2. Check the `tf.topk` implementation to see if that uses a GPU-side reduce or; instead; if it reads back to the CPU and does the work on the CPU; and; again; why this might be blocking the main thread.3. Good to see that we\'re doing readback of the value and indices array; which is a bit better; but I\'d again be very concerned that this test is doing very little GPU work compared to a normal TF network. In order for the GPU to be faster; the amount of work done on the GPU must be much larger than the ""work"" required to transfer the data to the GPU and back again. It is *very* common for simple tests to fail at this and send people off to lala optimization land.One difficult situation will be dealing with the WebGL GPU drivers. If we run a software path for WebGL that runs in the driver; it will most likely run in the main thread.====='; 'As another point of reference: I write a Firefox addon that spawns a ""content page"" for performing model processing; then have the background page interact with it over messages. The content page is more-or-less a normal web page and it blocks for several seconds during the model load and first inference with the WebGL backend as the shaders compile. Specifically; it blocks the entire browser: other tabs cannot be switched to for part of the load time; etc. I always assumed this had to do with some type of resource contention around the main render thread; but do not know the root cause.Unfortunately there is not a Chrome port due to missing WebExtensions API\'s; but if you are curious to see code that exhibits the issue; the code and current model can be [found here](https://github.com/wingman-jr-addon/wingman_jr). I had even just created [a minimal reproduction](https://github.com/wingman-jr-addon/wingman_jr/tree/load-perf-regression-tfjs-3.4.0-minimal-repro-webpage) for the TF.js for a different issue that might be helpful for anyone looking too.====='; 'Any update on this issue? I use tf.signal.stft in a web worker with webgl backend and UI freezes until processing is finished.====='; 'So the team have been researching into how to reduce GPU load when loading models / inference to give time back to browser render to reduce freezing of GUI etc. It seems to be more about how the browser schedules tasks to GPU. From what I understand WebWorker is aimed at CPU multithreading; and does not account for GPU tasks that are spawned from it which are still shared with the main browser level GPU render thread it seems. However focusing on GPU sharing instead we have had some promising results but still working on refining publishable solution to prod. This is actively under investigation though. ====='; '@jasonmayes I had quick followup. Some background text:""https://www.tensorflow.org/js/guide/platform_environment""TensorFlow.js executes operations on the GPU by running WebGL shader programs. These shaders are assembled and compiled lazily when the user asks to execute an operation. The compilation of a shader happens on the CPU on the main thread and can be slow. TensorFlow.js will cache the compiled shaders automatically; making the second call to the same operation with input and output tensors of the same shape much faster. Typically; TensorFlow.js applications will use the same operations multiple times in the lifetime of the application; so the second pass through a machine learning model is much faster.TensorFlow.js also stores tf.Tensor data as WebGLTextures. When a tf.Tensor is created; we do not immediately upload data to the GPU; rather we keep the data on the CPU until the tf.Tensor is used in an operation. If the tf.Tensor is used a second time; the data is already on the GPU so there is no upload cost. In a typical machine learning model; this means weights are uploaded during the first prediction through the model and the second pass through the model will be much faster.""1. How do these parts relate to using webworkers for webgl and the above thread? What I noticed is the first time inference is usually five times slower so if these parts such as the compilation can be done without freezing the ui that would be amazing. 2.  I know offscreen canvas is is not available in browsers such as safari/firefox. I think a lot of production use cases will need to support these especially Safari? Can a solution be made with that in mind? IE maybe the compilation part which might not require the canvas reference be made using the webworker so all browsers can see that speed up?Thanks;Rohan====='; 'I shall let @lina128 reply as she is currently working on the final solution to this which looks very promising in terms of that initial load blocking issue you mentioned by allowing time back to the browser render thread to do what it needs to do for updating GUI; and maybe she has some thoughts on q2 too.  Essentially though:In the WebGL API; some methods are blocking; while others are not. These are different from the sync/async concept in JS. The blocking methods in WebGL mean that; certain WebGL entry points cause synchronous stalls on the calling thread (possibly the same calling thread Chrome uses for GUI rendering I believe as everything is rendered via GPU now on Chrome if I remember correctly). Even basic requests can take as long as 1ms; but they can take even longer if they need to wait for all graphics work to be completed.In the new version Na is working on; we first compile and link everything without waiting because they are async; and then check and wait everything at the end; instead of individually; which gives time back to the browser to do other things it needs to perform in that time frame too.=====']",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,algorithm logic,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.2,D.4
https://github.com/tensorflow/tfjs/issues/5453,[wasm] failed to asynchronously prepare wasm,5,closed,2021-08-10T18:08:32Z,2021-08-12T19:10:23Z,<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 20.04 WSL2- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: v3.8.0- CUDA/cuDNN version: N/A**Describe the problem**An error occurred in the build process in CI when sending a PR. The error appears at the wasm backend testing stage. Log says the error is `failed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory`. This problem was not encountered when testing on the local machine. I don't know what causes this. Could someone explain this?**Provide the exact sequence of commands / steps that you executed before running into the problem**Sumbit a PR.**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.Log:```shellfailed to asynchronously prepare wasm: RangeError: WebAssembly Instantiation: Out of memory: wasm memoryRangeError: WebAssembly Instantiation: Out of memory: wasm memory/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:54                throw ex;                ^RuntimeError: abort(RangeError: WebAssembly Instantiation: Out of memory: wasm memory). Build with -s ASSERTIONS=1 for more info.    at abort (/workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:9596)    at /workspace/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.js:9:11594error Command failed with exit code 7.```Code:[[wasm] Add bincount kernel #5448](https://github.com/tensorflow/tfjs/pull/5448),"[""@raffizulvian can you please check this related PR where '`sinh`' kernal was added https://github.com/tensorflow/tfjs/pull/4845  and https://github.com/tensorflow/tfjs/pull/4486. Thank you =====""; 'Also please check you chrome version ; I see there was an issue with chrome browser here earlier https://stackoverflow.com/questions/55039923/why-does-chrome-eventually-throw-out-of-memory-wasm-memory-after-repeatedly-r ====='; 'Hi @rthadur; the error did not appear in my chrome in my local machine (build; lint; and test passed). But it made my PR #5448 check fail as shown in [https://console.cloud.google.com/cloud-build/builds/7dd9824e-1f5a-48a6-849c-a84cf29fe5ad?project=learnjs-174218](https://console.cloud.google.com/cloud-build/builds/7dd9824e-1f5a-48a6-849c-a84cf29fe5ad?project=learnjs-174218).====='; 'I assigned the PR to @lina128 ; you can discuss at one place regarding the error . I will close this issue. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5453"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5453"">No</a>=====']",Out of Mermory,Poor Performance,Unknown,Wasm,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] WebAssembly Instantiation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B,E
https://github.com/tensorflow/tfjs/issues/5452,(ranged) Sigmoid layer in advanced activation function,7,open,2021-08-10T17:38:59Z,2021-08-29T04:10:53Z,- Are you willing to contribute it (Yes I am willing to):In tfjs; there is a lot of activation functions as a layer; but what is weirdly missing is layers.sigmoid.This day I was making a dot product matrix factorization recommender system and the output of the dot layer should be in between 0 and 1; for this layers.sigmoid is needed.FastAI also offer ranged sigmoid which is very interesting; it squeeze the value into a range ]a;b[ instead of ]1;0[; this can be also used in my case when i need to predict ratings in a certain range (let's say ratings from 1 star to 5 stars).I can contribute to add this feature.,"['cc @pyu10055 ====='; '@rthadur Can I work on this feature (If no one else is working on this currently) ??====='; '@sourabh112 please go ahead ; thank you ====='; '@rthadur Thanks for the opportunity and sorry for the late reply. I was making myself comfortable with the code of the advanced activation layer. In order to accommodate the sigmoid layer in advanced activation layers. I just need to make some changes to the` exports_layers.ts` and `advanced_activations.ts` files. I will make a PR hopefully by tomorrow. with these changes with a working example. After that; I will add test in `advanced_activations.ts` and make any changes if needed.====='; ""@rthadur As for the ranged sigmoid function suggested by @bibs2091 might be a difficult task because (As per my knowledge) there is no file related to the ranged sigmoid activation function and I have to implement that from scratch. In doing so; I would need some (Actually a lot in the tfjs-core part where actually these functions start) help. If you allow doing that; then I will do that in the same PR after implementing stuff included in the above comment. If you think I'm wrong somewhere please correct me.=====""; '@rthadur the implementation of SigmoidRange is in this pull request https://github.com/tensorflow/tfjs/pull/5548 ; kindly review it it and tell me what do you think.@sourabh112  I am sorry I did not notice that you have been assigned; I already started working on the issue days ago and I wrote in my issue that I am ready to contribute to the idea. ====='; '@bibs2091 No problem; I was just starting to work on it. And after going through your PR. I got to know that I was on quite a bit wrong path. And sorry for not reading the last line of the issue.=====']",Reference Error,Crash,Unimplemented Operator,Layer API,API,add support for layers,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Requirement for Sigmoid Layer""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5449, ReferenceError: FormData is not defined ' when attempting to call a POST endpoint to save a model (tfjs-node -> tfjs-core)),7,open,2021-08-10T10:50:09Z,2021-09-17T18:27:18Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 20.04.2- TensorFlow.js installed from (npm or script link): npm install @tensorflow/tfjs-node- TensorFlow.js version (use command below):  @tensorflow/tfjs-node@3.8.0- Browser version: No browser involved- Tensorflow.js Converter Version: @tensorflow/tfjs-converter@3.8.0**Describe the current behavior**When trying to call an http POST endpoint to save a model; a ReferenceError is thrown at @tensorflow/tfjs-core/dist/tf-core.node.js:8270:    ReferenceError: FormData is not defined (see log at end of post for full stacktrace) **Describe the expected behavior**The code should execute without throwing a ReferenceError**Standalone code to reproduce the issue**```/*Dependencies: npm install @tensorflow/tfjs-node*/tf = require(""@tensorflow/tfjs-node"") function create_and_compile_model(){    let res_model = tf.sequential({        layers: [            tf.layers.dense({inputShape: [2]; units: 1; activation: 'sigmoid'})        ]    }    )    res_model.compile({optimizer: tf.train.adam(learningRate=0.05); loss:tf.losses.sigmoidCrossEntropy}) // bce=Binary cross entropy    return res_model  }    async function main(){    model = create_and_compile_model()    await model.save(tf.io.http('http://localhost:3000/save_model'); {method: 'POST'})    } main()```Note: I did not use the standard saving code from the docs; which would be`await model.save('http://localhost:3000/save_model') ` because then I got errors concerning not being able to find save handlers for the URL; although the endpoints were working. I figured this was related to https://github.com/tensorflow/tfjs/issues/723; but could not get it to work with any combination of imports; so opted for this approach. If the explicit usage of tf.io.http makes no sense; feel free to let me know and I'll further debug my initial approach with the standard model.save(url) instead.Note: this is the maximally reduced version of the actual code. The actual code creates an express server with get_model and save_model endpoints. Obviously this minimal example is not working code as the endpoint does not exist; but it already generated the error. I can provide working stand-alone code (up to this bug) with the express server if relevant.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Full traceback:```/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8270                        init.body = new FormData();                                        ^ReferenceError: FormData is not defined    at HTTPRequest.<anonymous> (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8270:41)    at step (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:82:23)    at Object.next (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:63:53)    at /nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:56:71    at new Promise (<anonymous>)    at __awaiter (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:52:12)    at HTTPRequest.save (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:8260:16)    at Sequential.<anonymous> (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:19791:60)    at step (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:9723:23)    at Object.next (/nodeProjects/minitest/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:9704:53)```I made a first attempt at debugging by adding the FormData dependency to tf-core.node.js; but then another similar error occurred a few lines later. I think the code is not executing some require statements; but I don't know where it's going wrong.","['The above method should work ; I believe this is a bug ![image](https://user-images.githubusercontent.com/43972606/128973948-401fba79-72bd-47e5-a65c-6e518ee1bf4d.png)====='; 'I have the same issue with the model.save() method which is giving ReferenceError: FormData is not defined.  In the node modules (node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js) form-data is required.====='; ""If anyone has an intuition as to what the underlying problem could be and could point me in the right direction; I'd be more than happy to try my hand at fixing this=====""; 'https://www.npmjs.com/package/form-data says from version 3.x FormData has dropped support for node@4.x. Not sure; but this might be the reason for the reference error.====='; ""I did some more research; and I think I've got the problem resolved; albeit in an ad-hoc manner. If one of the devs confirms this makes sense; I'll be happy to make a pull request so you can check if it breaks anything else.The issue is that Node.js does not natively have FormData; while browserside Javascript does. The same applies for Blob. Two changes are required to fix the issue; both to tf-core.node.js:1. Add the following line at the start of the file:`const FormData = require('form-data');`(note: this might break browerside Javascript code; in case this code is supposed to run browserside as well)2. There is already some logic present for detecting when node is being used rather than browserside Javascript:```// Use Buffer on Node.js instead of Blob/atob/btoavar useNodeBuffer = typeof Buffer !== 'undefined' &&    (typeof Blob === 'undefined' || typeof atob === 'undefined' ||        typeof btoa === 'undefined');```After these lines I added the following; to explicitly replace all usage of Blob by Buffer:```if(useNodeBuffer){    Blob = Buffer }```Note: at first sight the variable useNodeBuffer was introduced to solve the issue of Blob not being present in Node. However; it seems to be implemented in a way which is prone to issues. I think the original idea was to include an if-statement whenever Blob or something similar was used; and to implement different logic for Node than for browserside Javascript. But obviously it's easy to forget this; which I think is what happened when implementing the logic for creating that http request. Sources:https://stackoverflow.com/questions/63576988/how-to-use-formdata-in-node-js-without-browserhttps://stackoverflow.com/questions/14653349/node-js-cant-create-blobs=====""; '@HDegroote thank you for details ; will you be interested in submitting a PR ?====='; '@rthadur I hadn\'t realised the original source was in typescript: my debugging was on the generated Javascript file; and my proposed solution for Blob doesn\'t seem to work with typescript due to different types.I\'ve never worked with typescript; and I\'m struggling because whatever I do I keep getting type errors. Could anyone who knows typescript have a quick look? I think it\'s a fairly easy fix to make (although probably indicative of a larger issue in the project; because this will happen wherever Blob or FormData is used on Node). The relevant file is ""tfjs-core/src/io/http.ts"". The changes to make are:1. add `const FormData = require(\'form-data\');` at the top (and ensure this doesn\'t break browser-based javascript)2. Either monkey-patch Blob or introduce an explicit ""if"" for the two usages around line 100; in function ` async save(modelArtifacts: ModelArtifacts): Promise<SaveResult>`=> For the monkeypatch solution see https://stackoverflow.com/questions/14653349/node-js-cant-create-blobs;  or simply replace by Buffer as I did in my previous comment=> For the explicit \'if\'; the code should be something like the following (with useNodeBuffer defined as in my previous comment):```    if(useNodeBuffer){      init.body.append(\'model.json\'; new Buffer.from([JSON.stringify(modelTopologyAndWeightManifest)]; { type: JSON_TYPE }); \'model.json\');    }    else{        init.body.append(\'model.json\'; new Blob([JSON.stringify(modelTopologyAndWeightManifest)]; { type: JSON_TYPE }); \'model.json\');    }```A cleaner solution would probably use a makeBlob function; and put the \'if\' there; to reduce code duplication.In case it\'s relevant; I\'m using the following code to create a server to handle the save_model requests: https://gist.github.com/HDegroote/44bf3bfe919b0a473f4a52ca4ea5b4d6 =====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,type/env checker,Fix environment adaptability,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5447,blazeface's result is not correct on benchmarks test on webgl backend,3,closed,2021-08-10T08:17:27Z,2021-08-23T00:51:54Z,"Steps to reproduce:1. git clone https://github.com/tensorflow/tfjs2. cd tfjs/tfjs-backend-webgl3. yarn && yarn build-npm4. Do below changes to use your own build npm package.```--- a/e2e/benchmarks/local-benchmark/index.html+++ b/e2e/benchmarks/local-benchmark/index.html@@ -88;9 +88;8 @@ limitations under the License.   </div>   <script src=""https://unpkg.com/@tensorflow/tfjs-core@latest/dist/tf-core.js"" crossorigin></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-backend-cpu@latest/dist/tf-backend-cpu.js"" crossorigin></script>-  <script src=""https://unpkg.com/@tensorflow/tfjs-backend-webgl@latest/dist/tf-backend-webgl.js"" crossorigin></script>+  <script src=""../../../bazel-out/x64_windows-fastbuild/bin/tfjs-backend-webgl/tf-backend-webgl.js"" crossorigin></script>```5. cd ..6. npx http-server7.  Open the browser and navigate to http://localhost:8080/e2e/benchmarks/local-benchmark/8. Choose `blazeface` and `webgl` backend; click `Test correctness`. Expect the result is `true`; but get `false`.It's a regression. The result is correct with the latest publish npm package.","[""Will take a look if it's caused by the uniform change. If not; will un-assign myself.=====""; ""It seems that it's caused by PR #5422. Will continue this tomorrow.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5447"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5447"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,webgl shader type replacer,Replace data Shape/type,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5446,Allow setting number of threads programmatically for WASM backend,2,closed,2021-08-09T12:45:12Z,2021-10-15T21:24:12Z,"Currently number of WASM threads for `tfjs-backend-wasm` is fixed in build process for WASM files:`tfjs-backend-wasm/src/cc/BUILD`:```shellcc_binary(    name = ""tfjs-backend-wasm-threaded-simd.js"";    srcs = [""backend.cc""] + KERNELS_WITH_KEEPALIVE;    linkopts = BASE_LINKOPTS + [        ""-s EXPORT_NAME=WasmBackendModuleThreadedSimd"";        ""-s MALLOC=emmalloc"";        ""-s USE_PTHREADS=1"";        ""-s PROXY_TO_PTHREAD=1"";        # Many x86-64 processors have 2 threads per core; so we divide by 2.        ""-s PTHREAD_POOL_SIZE="" +        ""'Math.min(4; Math.max(1; (navigator.hardwareConcurrency || 1) / 2))'"";)```On the other hand; it can be freely set programmatically for `tfjs-tflite` backend:`tfjs-tflite/src/tflite_model.ts`:```js  if (options && options.numThreads !== undefined) {    curOptions.numThreads = options.numThreads;  } else {    curOptions.numThreads = await getDefaultNumThreads();  }```Plus since it evaluates `navigator.hardwareConcurrency` during runtime; it auto-adjusts to users hardware much better;  (unlike `tfjs-backend-wasm` which just uses 4 threads regardless of available hardware)  Environment: TFJS 3.8.0CC @jinjingforever as author of code for TFLite",['cc @ahmedsabie @jinjingforever ====='; '@ahmedsabie @jinjingforever @mattsoulanille any chance of getting this? should be trivial since it already works in `tflite` backend.====='],Initialization Faliure,Build & Initialization Failure,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",C,A.1
https://github.com/tensorflow/tfjs/issues/5420,yarn build-deps under tfjs-backend-webgpu failed,2,closed,2021-08-03T01:59:04Z,2021-08-03T18:14:13Z,"With https://github.com/tensorflow/tfjs/pull/5416 applied. There are still two issues when run yarn build-deps under tfjs-backend-webgpu on Windows:1. The command line is too long```ERROR: /build/tfjs/tfjs-core/src/BUILD.bazel:44:16: Action tfjs-core/src/tests.ts failed: (Exit 1): enumerate_tests_bin.bat failed: error executing command bazel-out/host/bin/tools/enumerate_tests_bin.bat -r tfjs-core/src -o bazel-out/x64_windows-fastbuild/bin/tfjs-core/src/tests.ts tfjs-core/src/browser_util_test.ts tfjs-core/src/buffer_test.ts ... (remaining 244 argument(s) skipped)The command line is too long.Target //tfjs-core:tfjs-core_pkg failed to buildUse --verbose_failures to see the command lines of failed build steps.```2.  There is a typo here:```+++ b/link-package/package.json@@ -6;7 +6;7 @@   ""license"": ""Apache 2.0"";   ""private"": true;   ""scripts"": {-    ""build"": ""yarn build-backend-cpu && yarn build-backend-webgo && yarn build-core && yarn build-tflite && yarn reinstall"";+    ""build"": ""yarn build-backend-cpu && yarn build-backend-webgl && yarn build-core && yarn build-tflite && yarn reinstall"";```","['1. The ""command line is too long"" error is caused by the new [enumerate_tests rule](https://github.com/tensorflow/tfjs/pull/5361/files#diff-7ead6c2057626da9b8cec1097fc4325f10355b6dfcfca60bf643e94bb96bc701R12-R17). This rule passes the path of every test file as an argument. I\'ll work on a fix for this by [writing the test file paths to a file](https://docs.bazel.build/versions/main/skylark/lib/actions.html#write) instead of passing them in command line arguments.2. Typo is fixed by #5419 (thanks @qjia7)!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5420"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5420"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Operator,API,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Command Line Too Long""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5418,WebGL tests are running incorrectly,1,closed,2021-08-02T22:59:28Z,2021-08-12T22:50:01Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**WebGL tests are running core tests using the CPU backend; and the WebGL tests themselves are not actually being run.**Describe the expected behavior**WebGL runs core tests using the webgl backend and runs its own tests.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.1. In `tfjs-backend-webgl`; run `yarn bazel run :tfjs-backend-webgl_test`.2. Click `debug` on the Chrome window that opens.3. Open the chrome debug tools.4. Search for a webgl test name in the logs; e.g. `fromPixels + regular math opfromPixels + regular math op` from `webgl_ops_test.ts`. It does not appear in the logs; meaning that test was not run.5. Open a `tfjs-core` test; e.g. `mat_mul_test.ts`; and pause in a spec. Then; look at `_tfengine.backendName`. It is `cpu`.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5418"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5415,Windows Bazel tests fail in git bash,1,open,2021-08-02T16:56:32Z,2021-08-05T05:28:21Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**Bazel builds on Windows work in git bash; but tests fail.**Describe the expected behavior**Tests and builds both work in git bash on Windows.**Standalone code to reproduce the issue**Run `yarn test` in tfjs-core.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Will be posted when I get a chance to run them on a Windows machine.,"['After successfully executing `yarn build-deps` under `tfjs-backend-webgl`; when run `yarn test-dev`; below errors are shown:```\'external\' is not recognized as an internal or external command;operable program or batch file.```If I directly call `bazel test :tfjs-backend-webgl_test --test_output=streamed`; below errors are shown:```ERROR(tools/test/windows/tw.cc:1294) ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\\users\\jqin7\\_bazel_jqin7\\6lxz3o43\\execroot\\tfjs\\bazel-out\\x64_windows-fastbuild\\bin\\tfjs-backend-webgl\\tfjs-backend-webgl_test""): %1 is not a valid Win32 application. (error: 193)ERROR(tools/test/windows/tw.cc:1451) Failed to start test process (arg: C:\\users\\jqin7\\_bazel_jqin7\\6lxz3o43\\execroot\\tfjs\\bazel-out\\x64_windows-fastbuild\\bin\\tfjs-backend-webgl\\tfjs-backend-webgl_test)FAIL: //tfjs-backend-webgl:tfjs-backend-webgl_test (see C:/users/jqin7/_bazel_jqin7/6lxz3o43/execroot/tfjs/bazel-out/x64_windows-fastbuild/testlogs/tfjs-backend-webgl/tfjs-backend-webgl_test/test.log)Target //tfjs-backend-webgl:tfjs-backend-webgl_test up-to-date:  dist/bin/tfjs-backend-webgl/tfjs-backend-webgl_testINFO: Elapsed time: 102.233s; Critical Path: 45.67sINFO: 112 processes: 86 internal; 9 local; 17 worker.INFO: Build completed; 1 test FAILED; 112 total actions//tfjs-backend-webgl:tfjs-backend-webgl_test                             FAILED in 0.0s=====']",Build & Install Failure,Build & Initialization Failure,Unknown,Operator,API,,,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Bazel test failure on Windows""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,E
https://github.com/tensorflow/tfjs/issues/5400,BlazePose Mediapipe-gpu official demo no longer works,1,closed,2021-07-29T16:40:48Z,2021-07-29T21:31:46Z,When I go to this link provided by the official TFJS models repo for pose-detection: https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=blazepose it throws this error on the console instead:![image](https://user-images.githubusercontent.com/33485877/127531130-4dd91a2c-10a0-42a0-9387-15b756e1c237.png)And nothing loads. This only happens when using blazepose with mediapipe-gpu backend.It was working before a couple days ago then I cleared my cache and it no longer works. So if it's still working for you make sure you clear your cache or change your browser and then try that link.**System information**Tried on Chrome 91 on Ubuntu 21.04 and Microsoft Edge Latest on Windows 10 2004,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5400"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5400"">No</a>=====']",Reference Error,Crash,Dependency Error,Model API,API,change dependency version,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] API Request Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,B.2
https://github.com/tensorflow/tfjs/issues/5398,[webgl] Bazel build includes tfjs-core,2,closed,2021-07-29T03:44:18Z,2021-08-03T18:27:37Z,<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**bundle size of webgl backend increased 1MB due to including tfjs-core files; there maybe also has a risk that we use different version of tfjs-core in one APP.**Provide the exact sequence of commands / steps that you executed before running into the problem**`cd tfjs-backend-webgl && yarn && yarn build`**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.``` ls -lh ../dist/bin/tfjs-backend-webgl/dist/tf-backend-webgl.js-r--r--r-- 1 yf Domain Users 1.9M Jul 28 10:09 ../dist/bin/tfjs-backend-webgl/dist/tf-backend-webgl.js```,"[""We likely need to mark tfjs-core as external in the tfjs_web_bundle rule. I'm OOO today; but I can take a look on Monday. Thanks for catching this!=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5398"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5398"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,WebGL,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Bundle Size Increase""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5397,[e2e] Failed to catch failure caused by tint change,2,open,2021-07-29T03:27:21Z,2021-08-02T07:17:45Z,Recently due to tint changed the shared memory size policy; all webgpu conformance should fail. However; e2e failed to catch this error. There are two reasons for this:1. The predict results of most models are objects; instead of arrays. Current implementation assumes that the predict results are arrays; which results in some fake pass. This can be fixed by https://github.com/tensorflow/tfjs/pull/5384.2. Models with tensor input still got passed with https://github.com/tensorflow/tfjs/pull/5384 applied; though they are indeed failed due to tint change. This is because the input for tensor are some special values(such as zeros); and so does the predict results. We should change those tensor values to guarantee the test is valid.,['@axinging can we close this issue ?====='; '@rthadur; Below issue is still not resolved:2. Models with tensor input still got passed with [e2e] Fix object compare #5384 applied; though they are indeed failed due to tint change. This is because the input for tensor are some special values(such as zeros); and so does the predict results. We should change those tensor values to guarantee the test is valid.====='],Incorrect Functionality,Incorrect Functionality,Improper Exception Handling,WebGPU,Backend,add support for datatype,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.7
https://github.com/tensorflow/tfjs/issues/5394,Fatal Error of tf.softmax in webgl mode,3,open,2021-07-28T18:51:27Z,2021-07-31T13:09:27Z,FATAL ERROR; tf.softmax may compute incorrect output when webgl mode is set; such as tf.setBackend('webgl');Before softmax.Tensor```    [[[[-0.0293463; -1000000000; -1000000000; ...; -1000000000; -1000000000; -1000000000];       [-0.2220541; 0.231505   ; -1000000000; ...; -1000000000; -1000000000; -1000000000];       [0.0011374 ; -0.0965061 ; -0.0123992 ; ...; -1000000000; -1000000000; -1000000000];       ...;       [0.0958247 ; 0.0556024  ; 0.1016628  ; ...; -0.0275311 ; -1000000000; -1000000000];       [-0.0398366; 0.0641379  ; 0.1550138  ; ...; -0.1018429 ; -0.216876  ; -1000000000];       [0.259409  ; -0.5869865 ; 0.1881166  ; ...; 0.4390099  ; -0.0278636 ; 0.0095359  ]];```After softmax...print.js:34 Tensor```    [[[[3.4028234663852886e+38; 3.4028234663852886e+38; 3.4028234663852886e+38; ...; 3.4028234663852886e+38; 3.4028234663852886e+38; 3.4028234663852886e+38];       [0.3885149             ; 0.6114851             ; 0                     ; ...; 0                     ; 0                     ; 0                     ];       [0.345599              ; 0.3134487             ; 0.3409523             ; ...; 0                     ; 0                     ; 0                     ];       ...;       [0.0180443             ; 0.0173329             ; 0.01815               ; ...; 0.0159502             ; 0                     ; 0                     ];```,"['In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks!  ====='; 'Here is the code for debugging:```    tf.setBackend(\'webgl\');    // tf.setBackend(\'cpu\');    const dd = tf.randomNormal([32; 4; 64; 64]; 0; 1; ""float32"");    dd.print();    const soft_result = tf.softmax(dd);    soft_result.print();```Result:```Tensor    [[[[3.4028234663852886e+38; 3.4028234663852886e+38; 3.4028234663852886e+38; ...; 3.4028234663852886e+38; 3.4028234663852886e+38; 3.4028234663852886e+38];       [0.0013382             ; 0.0037457             ; 0.0191183             ; ...; 0.0049057             ; 0.0420697             ; 0.0039236             ];       [0.0101191             ; 0.0029704             ; 0.0039451             ; ...; 0.0120634             ; 0.0049344             ; 0.0137982             ];       ...;       [0.00935               ; 0.0045935             ; 0.01144               ; ...; 0.0026718             ; 0.0123825             ; 0.0601572 ```When cpu mode is set; Result:```Tensor    [[[[0.0249545; 0.0066435; 0.0079386; ...; 0.0166403; 0.0036677; 0.0053585];       [0.0052016; 0.0013217; 0.0245201; ...; 0.0107317; 0.027073 ; 0.0042508];       [0.0253805; 0.0059142; 0.0172799; ...; 0.0073204; 0.0035138; 0.0060198];       ...;       [0.0092601; 0.0108018; 0.0047493; ...; 0.0158178; 0.0266986; 0.0167777];       [0.0063152; 0.0478238; 0.0008511; ...; 0.0145955; 0.001258 ; 0.0340839];       [0.006514 ; 0.0144928; 0.0038216; ...; 0.012091 ; 0.0135662; 0.0175768]];```====='; 'Test on official website:INTEL GPU; Chrome browser![image](https://user-images.githubusercontent.com/23742600/127448423-650fce1f-ba34-4ea9-8862-1b5386a2d9b7.png)=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5390,Different output in wasm vs webgl backend,1,closed,2021-07-28T10:54:23Z,2021-08-25T17:58:56Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Also on iPhone X- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js- TensorFlow.js version (use command below): Latest (3.8.0)- Browser version: Chrome 92.0.4515.107 (latest)- Tensorflow.js Converter Version: Latest (3.8.0)**Describe the current behavior**On the wasm backend the model output varies radically compared to the webgl output (without no warnings or errors logged). The below screenshot shows output using the wasm backend; which comes from here: https://replit.com/@epi-morphism/tfjs-bug2#index.html![Screenshot (2849)](https://user-images.githubusercontent.com/74825640/127310476-e0f0096e-3b2b-449c-88d9-428f91b0d72b.png)**Describe the expected behavior**Using webgl or cpu as the backend produces the expected proper output; as shown in the screenshot below; which comes from here https://replit.com/@epi-morphism/tfjs-bug#index.html.![Screenshot (2850)](https://user-images.githubusercontent.com/74825640/127310703-0d1d5e4c-c922-4b8e-83ac-7a68864d1930.png)**Standalone code to reproduce the issue**Incorrect output (wasm backend): https://replit.com/@epi-morphism/tfjs-bug2#index.htmlCorrect ouput (webgl or cpu backend): https://replit.com/@epi-morphism/tfjs-bug#index.html,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5390"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5390"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,variable replacer,variable replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5387,Unsupported Ops in the model before optimization UnsortedSegmentSum; RightShift; LeftShift,1,open,2021-07-28T00:45:51Z,2021-07-30T18:08:06Z,<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): tensorflowjs==3.8.0- Are you willing to contribute it (Yes/No): No**Describe the feature and the current behavior/state.*** I tried to convert pretrained tensorflow model from [here](https://github.com/soCzech/TransNetV2/tree/master/inference) to tfjs; but I get following error;`ValueError: Unsupported Ops in the model before optimizationUnsortedSegmentSum; RightShift; LeftShift`* So I used `--skip_op_check` option and converted as tfjs successfully; but when I run it on browser ; I get following error.  * with `webgl` backend: `Error: spaceToBatchND for rank > 4 with a WebGL backend not implemented yet`  * with `cpu` backend: `TypeError: Unknown op 'LeftShift'`**Will this change the current api? How?*** The ops that stated above can be handled.**Who will benefit with this feature?*** Anyone who uses tfjs**Any Other info.**Thank you for this great work. 🙏 🙏,['@moono will you be interested in contributing ? if yes here are the guidelines https://github.com/tensorflow/tfjs/blob/master/CONTRIBUTING_MISSING_OP.md thank you ====='],Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5382,Webpack fails when using Bazel's `.mjs` output files,1,closed,2021-07-26T21:40:13Z,2021-08-11T17:35:27Z,"**Describe the current behavior**Webpack fails to bundle when run on Bazel-created `.mjs` outputs.**Describe the expected behavior**Webpack successfully bundles when given Bazel outputs.**Standalone code to reproduce the issue**In the [webpack_test](https://github.com/mattsoulanille/tfjs/tree/webpack_test/e2e/webpack_test) branch; I've created a simple webpack bundle that depends on the Bazel-compiled outputs of `tfjs-core` and `tfjs-backend-cpu`. To reproduce the bug; run the following in that directory:```shyarnyarn build-depsyarn build```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```$ yarn buildyarn run v1.22.10$ webpackassets by status 673 bytes [cached] 1 assetorphan modules 4.55 KiB [orphan] 2 modulesruntime modules 274 bytes 1 module./app.js + 2 modules 5.37 KiB [built] [code generated]WARNING in ./app.js 22:10-21export 'tensor1d' (imported as 'tf') was not found in '@tensorflow/tfjs-core' (module has no exports)ERROR in ../../link-package/node_modules/@tensorflow/tfjs-backend-cpu/dist/index.mjs 18:0-23Module not found: Error: Can't resolve './base' in '/home/msoulanille/tfjs/link-package/node_modules/@tensorflow/tfjs-backend-cpu/dist'Did you mean 'base.js'?BREAKING CHANGE: The request './base' failed to resolve only because it was resolved as fully specified(probably because the origin is a '*.mjs' file or a '*.js' file where the package.json contains '""type"": ""module""').The extension in the request is mandatory for it to be fully specified.Add the extension to the request. @ ./app.js 19:0-38...```We are likely misusing the `.mjs` extension by not including the file extension in relative imports.A possible workaround is to add the following to the webpack config when bundling tfjs; but we still need to fix our outputs to work without this: https://github.com/graphql/graphql-js/issues/2721#issuecomment-723008284```jsmodule: {  rules: [{    test: /\.m?js/;    resolve: {      fullySpecified: false    }  }];}```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5382"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5382"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Operator,API,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Webpack Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5378,[WebGL] incorrect computational result when use depthwise conv2d with specific options,1,closed,2021-07-26T00:26:12Z,2021-08-17T01:17:06Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): All platforms- TensorFlow.js installed from (npm or script link): tf.js 3.8.0 webgl backend.- TensorFlow.js version (use command below):- Browser version: Chrome 92.0.4515.107 **Describe the current behavior**This issue is captured when running DeepLab V3 MobileNet V2 model; test depthwise conv2d with following options/input/weights; test result from webgl backend is incorrect.```  const input =tf.randomNormal([1; 65; 65; 960]);  const weights = tf.randomNormal([3; 3; 960; 1]);  const conv = tf.depthwiseConv2d(input; weights; 1; 'same'; 'NHWC'; 4);```**Describe the expected behavior**The computational result is same with wasm and cpu backend.**Standalone code to reproduce the issue**Here's a simple test; I just compared the first value of the output buffer; which is obviously different from result from wasm and cpu backends.CodePen: https://codepen.io/honry/pen/wvdMmva**Other info / logs**This issue is derived from https://github.com/tensorflow/tfjs/issues/5293,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5378"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5378"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,condition replacer,condition replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5366,Error: The shape of dict[‘input_tensor’] provided in model.execute(dict) must be [1;-1;-1;3],11,open,2021-07-22T14:26:40Z,2021-08-25T15:55:10Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**MacOS Big SurNode v15.7.0NPM 7.20.0I'm running @tensorflow/tfjs-node v 3.7.0 I attempted to convert my saved model to friendly model json using tensorflowjs_converter```    tensorflowjs_converter \                                --input_format=tf_saved_model \       --output_format=tfjs_graph_model \        --saved_model_tags=serve \    --signature_name=serving_default \    /saved_model \   /json-model```When run model.predict(); it's throwing these errors```Error: The shape of dict['input_tensor'] provided in model.execute(dict) must be [1;-1;-1;3]; but was [1;600;800;4]    at Object.assert (/object-detection/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:337:15)    at /object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7478:28    at Array.forEach (<anonymous>)    at GraphExecutor.checkInputShapeAndType (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7470:29)    at GraphExecutor.<anonymous> (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7272:34)    at step (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:81:23)    at Object.next (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:62:53)    at /object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:55:71    at new Promise (<anonymous>)    at __awaiter (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:51:12) ```I have also tried model.executeAsync()  instead of model.predict(); executeAsync() NOTE:  When use with SavedModel(without the conversion) it works for both image typesfor jpg image```Image: 1440000 bytes with shape: Tensor {  kept: false;  isDisposedInternal: false;  shape: [ 1; 600; 800; 3 ];  dtype: 'int32';  size: 1440000;  strides: [ 1440000; 2400; 3 ];  dataId: {};  id: 918;  rankType: '4';  scopeId: 2}Error: Invalid TF_Status: 3Message: In[0] and In[1] has different ndims: [1;8;8;64;2] vs. [2;1]```for png image```Error: The shape of dict['input_tensor'] provided in model.execute(dict) must be [1;-1;-1;3]; but was [1;600;800;4]```If I add ----control_flow_v2=True to do the conversion; it will fail to loadGraphModel.```I'm running @tensorflow/tfjs-node v 3.7.0 and I'm still getting this error ""Cannot read property 'outputs' of undefined"" when tried to load model json that was converted from saved model using tensorflowjs_converter.When I changed to @tensorflow/tfjs-node@next; it would throw ""Cannot read property 'children' of undefined""model = await tf.loadGraphModel(modelPath);2021-07-10 13:26:00.618147: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMATo enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.TypeError: Cannot read property 'outputs' of undefinedat /object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:3851:31at Array.forEach ()at /object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:3848:29at Array.forEach ()at OperationMapper.mapFunction (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:3846:18)at /object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:3679:56at Array.reduce ()at OperationMapper.transformGraph (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:3678:48)at GraphModel.loadSync (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7763:68)at GraphModel. (/object-detection/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:7737:52)```","['Can you try to reshape like this `tensor = tensor.reshape([1; 224; 224; 3]); before you call predict ?`====='; ""Yes I did that too; that didn't help.=====""; 'Also with tfjs-node v3.8.0 seems to have broken the functionality for loadSavedModel() for MacOS.  Looks like a regression bug; I have posted that here https://github.com/tensorflow/tensorflow/issues/38260/#issuecomment-881928290====='; 'In order to expedite the trouble-shooting process; please provide a code pen example or sample code to reproduce the issue. Thanks! ====='; 'Basically this is what I didtfnode.loadGraphModel(modelUrl);const image = fs.readFileSync(imageFile);let decodedImage = tfnode.node.decodeImage(image);tried both predict and executeAsyncI can share the converted model.json if that will help.====='; 'sure; if possible a working codepen example or a git repo. Thank you ====='; 'Here is the converted model.json https://github.com/playground/tfjs-object-detection/tree/main/model-json-service/model====='; '@rthadur v3.8.0 seems to have broken for Mac for loadSavedModel; model.predict() is throwing this error```Error: Session fail to run with error: Cannot parse tensor from proto: dtype: DT_VARIANTtensor_shape {}variant_val {  type_name: ""tensorflow::TensorList""  metadata: ""\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\030\\001""}         [[{{node StatefulPartitionedCall/StatefulPartitionedCall/map/TensorArrayV2_1/_9__cf__9}}]]```====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Any update?====='; ""Any update?I meet the same issue; but the different thing is when I use `tf.browser.fromPixels(imgPath)`; I got the shape 3(it's a JPG file); not 4(as the doc says it should be 4). the img path is an image URL that stores in the cloud.The same thing is that when I use the `model.execute` or `model.predict`; I got the `Uncaught (in promise) Error: The shape of dict['input_tensor'] provided in model.execute(dict) must be [1;-1;-1;3]; but was [711;474;3]` error.=====""]",Data & Model Error,Crash,Untimely Update,TF(CPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,B.3
https://github.com/tensorflow/tfjs/issues/5357,tfjs build fail due to java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc,1,open,2021-07-20T14:36:46Z,2021-08-02T16:41:02Z,This can only be reproduced in Windows 10 cmd.  When try this in Git Bash; everything goes well.Test platform:```Windows 10 cmd```How to:```c:\tfjs\tfjs-backend-cpu>yarn build-npm```Logs:```$ bazel build :tfjs-backend-cpu_pkgWARNING: Ignoring JAVA_HOME; because it must point to a JDK; not a JRE.INFO: Analyzed target //tfjs-backend-cpu:tfjs-backend-cpu_pkg (1 packages loaded; 22 targets configured).INFO: Found 1 target...ERROR: C:/tfjs/tfjs-core/src/BUILD.bazel:33:11: Compiling TypeScript (prodmode) //tfjs-core/src:tfjs-core_src_lib failed: Worker process quit or closed its stdin stream when we tried to send a WorkRequest:---8<---8<--- Exception details ---8<---8<---java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(356): NativeProcess:WriteStdin(6472): The pipe is being closed.        at com.google.devtools.build.lib.windows.WindowsSubprocess.writeStream(WindowsSubprocess.java:270)        at com.google.devtools.build.lib.windows.WindowsSubprocess.access$000(WindowsSubprocess.java:32)        at com.google.devtools.build.lib.windows.WindowsSubprocess$ProcessOutputStream.write(WindowsSubprocess.java:56)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.doFlush(CodedOutputStream.java:3062)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.write(CodedOutputStream.java:2994)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeLazy(CodedOutputStream.java:3013)        at com.google.protobuf.ByteString$LiteralByteString.writeTo(ByteString.java:1392)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeBytesNoTag(CodedOutputStream.java:2801)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeBytes(CodedOutputStream.java:2775)        at com.google.devtools.build.lib.worker.WorkerProtocol$Input.writeTo(WorkerProtocol.java:238)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeMessageNoTag(CodedOutputStream.java:2855)        at com.google.protobuf.CodedOutputStream$OutputStreamEncoder.writeMessage(CodedOutputStream.java:2824)        at com.google.devtools.build.lib.worker.WorkerProtocol$WorkRequest.writeTo(WorkerProtocol.java:1091)        at com.google.protobuf.AbstractMessageLite.writeDelimitedTo(AbstractMessageLite.java:95)        at com.google.devtools.build.lib.worker.ProtoWorkerProtocol.putRequest(ProtoWorkerProtocol.java:38)        at com.google.devtools.build.lib.worker.SingleplexWorker.putRequest(SingleplexWorker.java:117)        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.execInWorker(WorkerSpawnRunner.java:453)        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.actuallyExec(WorkerSpawnRunner.java:227)        at com.google.devtools.build.lib.worker.WorkerSpawnRunner.exec(WorkerSpawnRunner.java:144)        at com.google.devtools.build.lib.exec.SpawnRunner.execAsync(SpawnRunner.java:240)        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:140)        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:102)        at com.google.devtools.build.lib.actions.SpawnStrategy.beginExecution(SpawnStrategy.java:47)        at com.google.devtools.build.lib.exec.SpawnStrategyResolver.beginExecution(SpawnStrategyResolver.java:65)        at com.google.devtools.build.lib.analysis.actions.SpawnAction.beginExecution(SpawnAction.java:331)        at com.google.devtools.build.lib.actions.Action.execute(Action.java:127)        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$5.execute(SkyframeActionExecutor.java:855)        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.continueAction(SkyframeActionExecutor.java:1016)        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.run(SkyframeActionExecutor.java:975)        at com.google.devtools.build.lib.skyframe.ActionExecutionState.runStateMachine(ActionExecutionState.java:129)        at com.google.devtools.build.lib.skyframe.ActionExecutionState.getResultOrDependOnFuture(ActionExecutionState.java:81)        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.executeAction(SkyframeActionExecutor.java:472)        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.checkCacheAndExecuteIfNeeded(ActionExecutionFunction.java:834)        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.compute(ActionExecutionFunction.java:307)        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:477)        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:398)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)        at java.base/java.lang.Thread.run(Unknown Source)---8<---8<--- End of exception details ---8<---8<---```,"[""I'm able to reproduce this on windows; and I think it might have something to do with how Bazel uses workers to speed up compilation. Passing `--spawn_strategy=local` to `yarn build` to disable them causes a different error about missing WSL. After installing WSL; it fails again; complaining about a missing `npm/@bazel/typescript/bin/tsc_wrapped.sh`. =====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,CPU,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Compilation Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5345,Broadcast over complex Values on CPU backend does not work well,7,open,2021-07-19T10:18:24Z,2021-09-27T22:38:29Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10 Professionnal (build 19041.1110)- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Not on Mobile device; but here are the laptop's specs :    - Intel Core i7-10875H CPU 2.30GHz    - 32Go RAM    - Operating System x64- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.7.0- Browser version: -- Tensorflow.js Converter Version: -**Describe the current behavior**While trying to use some ""simple"" operations on a complex tensor such as add; multiply or substract; only 1 over 2 term of the tensor is actually computed; the second is marked as NaN + NaNj.Example : const test = tf.complex([1; 1; 1; 2; 3; 1]; [1; 4; 5; 6; 8; 1]);test.mul(2).print() // [2 + 2j; NaN + NaNj; 2 + 10j; NaN + NaNj; 6 + 16j; NaN + NaNj]test.add(1).print() // [2 + 1j; NaN + NaNj; 2 + 5j; NaN + NaNj; 4 + 8j; NaN + NaNj]However if I use a Tensor of the same size from the complex one to execute this operation; the problem does not appear anymore.**Describe the expected behavior**When using a simple digit instead of a tensor of the same size; the operation should be computed properly.Example :const test = tf.complex([1; 1; 1; 2; 3; 1]; [1; 4; 5; 6; 8; 1]);test.mul(2).print() // [2 + 2j; 2 + 8j; 2 + 10j; 4+ 12j; 6 + 16j; 2+ 2]**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.in node console :const tf = require(""@tensorflow/tfjs"")const test = tf.complex([1; 1; 1; 2; 3; 1]; [1; 4; 5; 6; 8; 1]);test.mul(2).print()**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['I tried on tfjs [version](https://js.tensorflow.org/api/3.7.0/) @3.7.0 it is working as expected ; please see below ; ![image](https://user-images.githubusercontent.com/43972606/126213900-82eee957-371c-4811-a054-7baab19a2308.png)====='; 'So it must be a node console or NodeJS problem; because when I do it in the console; the problem is here.![image](https://user-images.githubusercontent.com/14946801/126972920-3dc0ac7a-fd46-42c0-9446-cb6603c598ab.png)====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'This is working as expected on console as well ; please check![image](https://user-images.githubusercontent.com/43972606/128402337-3bbcbb1f-face-4b19-9d6f-fad4e5cd15d2.png)====='; 'Closing as stale. Please @mention us if this needs more attention.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5345"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5345"">No</a>====='; ""Hello;I ran several tests to see where that would go wrong and obtained these results :- In the browser (backends webgl) with tensorflow editor or console in the navigator -> works fine- With modules @tensorflow/tfjs-node | @tensorflow/tfjs-node-gpu with node.js (script or console) -> works fine- In the browser with a **cpu** backend the problem occurs (@tensorflow/tfjs) (see image below)![image](https://user-images.githubusercontent.com/14946801/134887376-dd5aa703-10d5-47d4-8947-dec0d1c4489e.png)- In a **node console**; importing just **@tensorflow/tfjs** that registers a **cpu** backend the problem occurs (see image below)![image](https://user-images.githubusercontent.com/14946801/134887148-0e58b730-673c-4666-a23b-939a0d243fb1.png)As shown above; the broadcast operation (and probably other operations as well) on complex numbers don't work as expected when using the vanilla backend.Thank you for your understanding.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,CPU,Backend,change backend,changing backend,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5343,WEBGL_PACK_DEPTHWISECONV=true seems to cause significant first inference performance drop,17,closed,2021-07-19T05:30:37Z,2021-10-21T12:41:59Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes. Production code at [https://github.com/wingman-jr-addon/wingman_jr/pull/135](https://github.com/wingman-jr-addon/wingman_jr/pull/135); minimal reproduction at [https://github.com/wingman-jr-addon/wingman_jr/pull/136](https://github.com/wingman-jr-addon/wingman_jr/pull/136)- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Win 10 Home 21H1- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A; but laptop specs:```ideapad FLEX5-1570Processor	Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz   2.90 GHzInstalled RAM	16.0 GB (15.9 GB usable)System type	64-bit operating system; x64-based processorPen and touch	Pen and touch support with 10 touch points```- TensorFlow.js installed from (npm or script link):https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.4.0/dist/tf.min.jshttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tf-backend-wasm.jshttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm.wasmhttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm-simd.wasmhttps://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@3.4.0/dist/tfjs-backend-wasm-threaded-simd.wasm- TensorFlow.js version (use command below): 3.4.0- Browser version: Firefox 90.0 64 bit- Tensorflow.js Converter Version: Unknown; but probably 2.7.0Current behavior - Upgrading from 3.3.0 to 3.4.0 experienced major performance drop on load+first inference time. 3.3.0 sees times of about 8.8s; 3.4.0 sees times about 14.4s. It pains me to report a bug related to WEBGL_PACK as so much work has gone into this feature; but ... It appears that setting `WEBGL_PACK_DEPTHWISECONV=false` on 3.4.0 returns to performance found in 3.3.0. Regression with default flags has been found to exist in at least 3.6.0 and 3.8.0 as well. (This was found on a bisection to upgrade from 2.7.0 to 3.8.0 to get the new shader compilation performance improvements started in #5205 )Expected behavior - 3.4.0 with the flag default `WEBGL_PACK_DEPTHWISECONV=true` offers similar or better performance to 3.3.0.Minimal reproduction: [https://github.com/wingman-jr-addon/wingman_jr/pull/136](https://github.com/wingman-jr-addon/wingman_jr/pull/136)Note this is a Firefox plugin; but TF.js is loaded via a content tab rather in the background context so it should be acting quite similarly to a normal browsing context.Attached is output from Firefox's about:support; which includes more detailed graphics issues that may be relevant to the matter at hand.[FF90_about_support.txt](https://github.com/tensorflow/tfjs/files/6838167/FF90_about_support.txt),"['Hi Ahmed;I think you probably have more knowledge about the webgl backend so assigning this to you:) Please help take a look when you have a chance. Really appreciate it!====='; ""(Also; I see that I neglected to link off to the PR that led me to this issue - [https://github.com/tensorflow/tfjs/pull/4909](https://github.com/tensorflow/tfjs/pull/4909) - I don't know that the work done in the bulk of the PR is by any means the cause; but the change of the flag's default option is what caused the regression.)=====""; '@wingman-jr-addon If the initialization is larger than before; there are possibly two causes:1. There are configuration of the depthwise conv2d ops that require more packed shaders to be compiled comparing to unpacked depthwise kernel. But I do see any setup could cause that.2.  The packed shader takes much longer to compile compares to unpacked version.Item 2 might be browser specific; can you help to verify if this behavior occurs on firefox and chrome web page? Thanks====='; 'Thanks for idea to try @pyu10055 . I changed the plugin to run in a web page and then ran across Firefox 90 and Chrome 92.In general; Chrome had significantly better performance; but there was still a performance gap between the flag being on and off for reload times. I would run the test by opening the browser; loading the web page; recording the time; and then hitting refresh and making note of times after that. The first load times tended to be much higher than subsequent times in Chrome. Also; I was only running one browser at a time during testing in case of some sort of GPU contention.Here are the raw times in seconds:FF 90; defaults: 14.5; 13.5; 14.2FF 90; flag=false: 9.6; 8.9; 9.1; 8.5Chrome 92; defaults: 13.9; 7.0; 6.8; 6.9; 7.0Chrome 92; flag=false: 14.9; 5.2; 4.8I reloaded the last Chrome test and tried it several more times; seeing reload times of 4.5-5.5 seconds.So; I guess I\'m not sure what to make of the results for Chrome - I\'m not sure if I should trust the reload results or if I should run a bunch of ""first run"" tests. For Firefox; the results are slower across the board but consistent on reload.Let me know what you think. Thanks!====='; ""My guess is that Chrome have better caching on the shader across page reloads. One thing you could try; not necessarily relates to depthwise conv2d; use uniform variables for unary and binary ops.```tf.env().set('WEBGL_USE_SHAPES_UNIFORMS'; true);```We have observe significant reduction of initial loading time.=====""; ""Thanks for the tip; I am seeing about a 0.5-1.0s reduction in load time on FF 90!But getting back to the matter at hand - any clue why we might be seeing such a performance gap on `WEBGL_PACK_DEPTHWISECONV=true` though?It could be that it just depends on hardware and mine is not the primary target; I'd just like to make sure there isn't some other issue hanging out.=====""; ""@wingman-jr-addon @pyu10055  My guess `WEBGL_PACK_DEPTHWISECONV=true` brings more shaders to compile. You can compare the `binaryCaches`'s size in `backend_webgl.ts` between `WEBGL_PACK_DEPTHWISECONV=true` and `WEBGL_PACK_DEPTHWISECONV=false` to see how many shaders are added (they probably are encode/decode matrix shaders. If that's the case; I think this PR #5297 may resolve your issue by using uniforms to encode/decode matrix programs). And; in another side; we should check why so many encode/decode shaders are introduced. Can them be avoided/reduced? In my opinion; if we go to the packed path; it will be best if we only pack data at the beginning and unpack the data at the last and try to avoid encode/decode the data in the middle. Maybe we can check if there is any chance to optimize it.=====""; ""Cross-posting from #5205 I've tested this on my notebook with 3 different models of medium-high complexity  | Model | DataSet | WEBGL_PACK_DEPTHWISECONV | WEBGL_USE_SHAPES_UNIFORMS | Warmup | Execution | Note || --- | --- | --- | --- | --- | --- | --- || Inception-v4 | ImageNet | True | False | 11.2sec | 42ms | Default| Inception-v4 | ImageNet | False | False | 10.8sec | 45ms || Inception-v4 | ImageNet | False | True | 10.8sec | 45ms || Inception-v4 | ImageNet | True | True | 11.2sec | 42ms || SSD/MobileNet-v2 | OpenImages | True | False | 14.7 | 2.1sec | Default| SSD/MobileNet-v2 | OpenImages | False | False | 13.3sec | 2.2sec || SSD/MobileNet-v2 | OpenImages | False | True | 12.7sec | 2.1sec || SSD/MobileNet-v2 | OpenImages | True | True | 13.6sec | 2.1sec || EfficientDet-D4 | CoCo | True | False | 23.1sec | 12.9sec | Default| EfficientDet-D4 | CoCo | False | False | 16.1sec | 14.5sec || EfficientDet-D4 | CoCo | False | True | 15.9sec | 14.0sec || EfficientDet-D4 | CoCo | True | True | 21.1sec | 13.00sec |All-in-all:- WEBGL_USE_SHAPES_UNIFORMS helps to significantly reduce warmup with *NO* negative impact on subsequent inference  - WEBGL_PACK_DEPTHWISECONV increases warmup too much even if subsequent inference is slightly fasterAs it is; I'll be setting `WEBGL_USE_SHAPES_UNIFORMS=True` and `WEBGL_PACK_DEPTHWISECONV=False` on my projects as even with uniforms enabled (which does help); it's still too slow on warmup  *Note: Chrome does extensive shader caching between sessions; so simple page reload is not sufficient and full browser restart is needed between tests*=====""; '@rthadur Would you agree the awaiting response label can probably be removed from this issue now?====='; ""@ahmedsabie @qjia7 @rthadur @pyu10055 Any updates on this? As you can see; WEBGL_PACK_DEPTHWISECONV=True (which is default value) has a **massive** negative performance impact - and it's gotten far worse in newer versions of TFJS. This is a major regression and it has very little updates.And yes; using WEBGL_USE_SHAPES_UNIFORMS is much better; but - a) it's not a solution; it's an alternative; b) it's not widely implemented; c) almost nobody knows about it.=====""; 'see #5689 for fully reproducible code and additional performance notes.====='; '@vladmandic @wingman-jr-addon I think this could be caused by the packed depthwise conv2d shader could be much larger in size than unpacked depthwise conv2d version. This could be related to the filter size; since the packed version expand the loop of the filter width into code. Can you share what is the filter size for depthwise conv2d in your model? And the other question is; we have a way to make the initial warm non UI blocking; basically by yielding the JS thread and removing all GL block calls (parallel shader compilation). But the overall warm time might still be similar. Will this behavior be helpful for your use cases?====='; ""> Can you share what is the filter size for depthwise conv2d in your model?I have seen the same behavior in almost **every off-the-shelf model**  Just pick any from TFHub - I've provided performance data for `Inception-v4`; `EfficientDet-D4`; `EfficientNet-B5`; `MobileNet-v2`And only model that doesn't have the problem is ancient `MobileNet-v2`  (and since I have an automated test for this; I can reproduce using any given model)  What is the intended benefit of the packed conv2d shader? I don't see much benefit of it: - few percent faster inference (never more than 2-5%; hardly worth it)- few hundred percent slowdown in warmup (average 50-400%; massive)> And the other question is; we have a way to make the initial warm non UI blocking; basically by yielding the JS thread and removing all GL block calls (parallel shader compilation). But the overall warm time might still be similar. Will this behavior be helpful for your use cases?Yes and No :)- Yes for overall usage as **any** reduction of blocking calls is very welcome.- No as I already use web workers in most cases anyhow;    so my UI is not really blocked - but since app requires models to work; not much use of app until warmup has finished  =====""; 'We have seen significant performance gain on mobile device with the packed depthwise conv2d shader; especially for android devices.Thank you for the insights; we will see if we can avoid the slow start up time it introduces.As for UI blocking; even with web worker; the UI can be blocked; since chrome has a single GL command queue; any blocking GL call will prevent new GL commands to be flushed to GPU.  ====='; ""Thanks @pyu10055;  Perhaps as a start; you could do conditional `WEBGL_PACK_DEPTHWISECONV = isMobile()` ?  It doesn't solve a problem; just makes less people immediately affected  I just tried on Android. Yes; inference performance difference on Android is visible (unlike on desktop)  IMO - still; negative impact of slow warmup causing users to simply give up and close the app outweighs performance benefits of actual inference  Re: UI Blocking - true; if there is any GL usage elsewhere. Anyhow; if you can make it non-blocking; that is very much welcomeAnd when will WEBGL_USE_SHAPES_UNIFORMS become a default?=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5343"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5343"">No</a>====='; 'closing the loop after testing using todays code in main branch:warmup is now about 2x fasterno material difference regardless if `WEBGL_PACK_DEPTHWISECONV` is enabled or disabeld so that issue is resolved  do note that enabling `WEBGL_USE_SHAPES_UNIFORMS` performs much better *(2x faster warmup)* regardless of packing (actually packing improvements make it even faster)!**webgl** default - warmup initial 53sec in 3.9.0 -> 32sec in main branch- warmup cached 15sec -> 12sec**webgl** with uniforms enabled- warmup initial 23sec -> 18sec- warmup cached 14sec > 5sec@pyu10055 please consider enabling uniforms as default=====']",Regression,Poor Performance,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/5334,Opera web worker support. Fails with 'window is not defined',1,closed,2021-07-15T12:34:15Z,2021-08-12T23:32:30Z,**System information**- OS Platform and Distribution: macOS 11.4- TensorFlow.js installed from npm:- TensorFlow.js version: 3.5.0- Browser version: Opera 77**Describe the current behavior**I have an app based on the body-pix examplehttps://github.com/tensorflow/tfjs-models/tree/master/body-pixThe tensorflow and body-pix code run inside a web worker. The app works fine in Chrome and Edge.I did a test with Opera and it fails with 'window is not defined'```video-filter.worker.ts:74 ReferenceError: window is not defined    at Module.isMobile (device_util.js:32)    at Object../node_modules/@tensorflow/tfjs-backend-webgl/dist/flags_webgl.js [as evaluationFn] (flags_webgl.js:148)    at Environment.evaluateFlag (environment.js:101)    at Environment.get (environment.js:67)    at MathBackendWebGL.runWebGLProgram (backend_webgl.js:686)    at Object.fromPixels [as kernelFunc] (FromPixels.js:60)    at kernelFunc (engine.js:463)    at engine.js:524    at Engine.scopedRun (engine.js:337)    at Engine.runKernelFunc (engine.js:520)```**Describe the expected behavior**Opera to be supported.**Standalone code to reproduce the issue**The issue is caused by the function isMobile()https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/device_util.ts#L33In Opera web worker:navigator.userAgent is ''navigator.vendor is undefinedwindow is undefined.In Chrome web worker:navigator.userAgent is 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML; like Gecko) Chrome/91.0.4472.114 Safari/537.36'navigator.vendor is undefinedwindow is undefined.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5334"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5334"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Browser,Platform,type/env checker,Fix environment adaptability,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5313,Outdated Documentation,1,closed,2021-07-12T02:58:55Z,2021-07-14T17:56:19Z,"The documentation appears to be outdated.The main readme states:> ⚠️ We recently released TensorFlow.js 2.0. If you have been using TensorFlow.js via a script tag without specifying a version and see an error saying no backends are found; then you should read our release notes for instructions on how to upgrade.npmjs shows that v2.0.0 was one year ago. The current ""latest"" version is 3.7.0; which was published last month.- https://www.npmjs.com/package/@tensorflow/tfjs/v/3.7.0- https://www.npmjs.com/package/@tensorflow/tfjs/v/2.0.0The concern is that developers are likely using the README for v2 code while using v3.Additionally; [WINDOWS_TROUBLESHOOTING](https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md) is outdated.> Currently; node-gyp requires Python 2.x to work properly. If Python 3.x is installed; you will see Build & Install Failures. Also double check your python version python --version and update the Windows $PATH as needed.That statement is inaccurate. See https://github.com/nodejs/node-gyp/issues/1977#issuecomment-567131298> You do not need Python 2.7 in order to run node-gyp. **_Python 3 has been supported since version 5.0.5_**. If you have verified that your issue is python related; as in that node-gyp is trying to run a script which contains errors according to the python 3 interpreter; then try the following:...",['@djbreen7 thanks for checking this ; I have submitted 2 separate PRs to remove above lines ====='],Document Error,Document Error,Confused Document,TF(CPU),Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Configuration & Dependency Error""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5295,tfjs-node README out of date,5,closed,2021-07-06T21:40:18Z,2021-08-04T16:51:38Z,The tfjs-node documentation does not support the actual tensorflow version; 2.5.0 that is supported in the code. Please update.,"['Can you please more specific ; where would you like to update ? ====='; 'We should update the correct supported versions of cuda; tensorflow; cuDNN; etc. ====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '/reopen====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5295"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5295"">No</a>=====']",Document Error,Document Error,Confused Document,TF(CPU),Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5293,[WebGL] Failed to compile fragment shader,8,closed,2021-07-06T02:32:01Z,2021-07-26T00:26:55Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): All platforms- TensorFlow.js version (use command below):  tf.js 3.7.0 webgl backend.- Browser version: Chrome 93.0.4545.0**Describe the current behavior**WebGL backend throws `Failed to compile fragment shader` error while using `depthwiseConv2d` op with specific options. While WebGPU backend doesn't reproduce.**Describe the expected behavior**No error.**Standalone code to reproduce the issue**CodePen: https://codepen.io/honry/pen/wvdMmva```    const input = tf.fill([1; 65; 65; 960]; 0);    const weights = tf.fill([3; 3; 960; 1]; 1);    const result = tf.depthwiseConv2d(input; weights; 1; 'same'; 'NHWC'; 4);```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Please check error log from https://codepen.io/honry/pen/wvdMmva,"['Thanks @Honry ; how about the cpu backend and wasm backend?====='; '@huningxin; this issue only reproduce on webgl backend.====='; 'A similiar situation could be occurred by the following codes (i.e. depthwiseCon2d with ( strides == 2 ) ):```//await tf.setBackend( ""cpu"" ); // Error not happend in CPU.await tf.setBackend( ""webgl"" ); // Error happend in WebGL.let a = [  111; 112; 113; 114; 121; 122; 123; 124; 131; 132; 133; 134;  141; 142; 143; 144; 151; 152; 153; 154; 211; 212; 213; 214;  221; 222; 223; 224; 231; 232; 233; 234; 241; 242; 243; 244;  251; 252; 253; 254; 311; 312; 313; 314; 321; 322; 323; 324;  331; 332; 333; 334; 341; 342; 343; 344; 351; 352; 353; 354];let shape = [ 3; 5; 4 ];let x = tf.tensor( a; shape );let filters = tf.tensor( [ 11; 22; 33; 44 ]; [ 1; 1; 4; 1 ] );// When ( strides == 1 ); No problem.let c = tf.depthwiseConv2d( x; filters; 1; ""valid"" );c.print();let d = tf.depthwiseConv2d( x; filters; 1; ""same"" );d.print();// When ( strides == 2 ); error: ""Failed to compile fragment shader.""let e = tf.depthwiseConv2d( x; filters; 2; ""valid"" );e.print();// When ( strides == 2 ); error: ""Failed to compile fragment shader.""let f = tf.depthwiseConv2d( x; filters; 2; ""same"" );f.print();```====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5293"">No</a>====='; ""@pyu10055; many thanks for fixing this issue! The `compile error` had been fixed; but the computed result looks wrong.Here's a simple test; I just compared the first value of the output buffer; which is obviously different from result from `wasm` and `cpu` backends.https://codepen.io/honry/pen/wvdMmva=====""; '@pyu10055; friendly ping. :) Should I create a separate issue for incorrect computational result?====='; '@Honry please open a new issue. Thank you ====='; ""Thanks @rthadur; I've created a new issue at https://github.com/tensorflow/tfjs/issues/5378.=====""]",Browser & Device Error,Crash,Incorrect Code Logic,WebGL,Backend,variable replacer,variable replacer,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",A.4,A.4
https://github.com/tensorflow/tfjs/issues/5292,[BUG] With require.js inside electron app ; does not work,5,open,2021-07-05T07:21:41Z,2021-07-29T01:55:55Z,Here is my scenerio:My app is using require.js and is running inside electron.I kept getting error and then i fugred it out.```          ENV.registerFlag('IS_NODE'; function () {            // ignore NODE if AMD is found            if (typeof define === 'function' && define.amd) {              return false;            }            return typeof process !== 'undefined' && typeof process.versions !== 'undefined' && typeof process.versions.node !== 'undefined';          });```Please Update function ```ENV.registerFlag('IS_NODE'; function ```so that it ignores NODE if AMD is found,"['@gbaned can you provide a full running example to expedite the reproduction of the issue. thanks====='; '@mafar In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks!====='; '> @mafar In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks!example will require an electron mock up with require.js inside it. Too much for me :)But let me explain plzWhen require.js is running inside electorn as in my case; - ```tfjs``` is loaded via require.js (AMD); i dont load it via node.js (commonJs)- but when code reaches at point as shown above ; ```IS_NODE``` becomes true since we are inside electron - - but from here dependencies start to fail loading becuse initialy it was loaded via ```AMD``` and now it switched to ```CommonJS ```- - Simply put; if  ```tfjs``` is loaded via ```AMD``` ; it should not consider ```CommonJs``` at all; to avoid dependency resolution conflicts====='; ""We may need to add an `IS_COMMONJS` flag to decide if we can `require()` dependencies. There are some cases where knowing we're running in node is important apart from knowing whether we can `require()` dependencies (e.g. [WASM multithread support](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-backend-wasm/src/flags_wasm.ts#L39-L44)).The only place I see where we're checking `IS_NODE` and then calling `require()` is in [file_data_source.ts](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-data/src/sources/file_data_source.ts#L46-L50). Alternatively; we could just check for commonjs here and avoid adding the new flag.What do you think; @pyu10055?=====""; ""> We may need to add an `IS_COMMONJS` flag to decide if we can `require()` dependencies. There are some cases where knowing we're running in node is important apart from knowing whether we can `require()` dependencies (e.g. [WASM multithread support](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-backend-wasm/src/flags_wasm.ts#L39-L44)).> > The only place I see where we're checking `IS_NODE` and then calling `require()` is in [file_data_source.ts](https://github.com/tensorflow/tfjs/blob/b8024c0def121515917a03c66c9556373bb2bbf4/tfjs-data/src/sources/file_data_source.ts#L46-L50). Alternatively; we could just check for commonjs here and avoid adding the new flag.> > What do you think; @pyu10055?I agree. This wIll be a nice solution. If you provide me with a PR; I can test it using my project and report back quick if this is working=====""]",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,Desktop,Platform,type/env checker,Fix environment adaptability,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,A.4
https://github.com/tensorflow/tfjs/issues/5291,About the Bug of executeAsync Asynchronous Function,5,open,2021-07-05T03:08:47Z,2021-07-30T14:23:52Z,@rthadur @lina128I studied the object detection project of tfjs and found that the code of `tf.loadGraphModel(weights).executeAsync(input)` has some bugs when it is called.Bug description:When there is a detected object in the input picture; the program is normal; as shown in the figure.![ksnip_20210705-103113](https://user-images.githubusercontent.com/41098760/124410046-27e60000-dd7c-11eb-8934-4bc6145d336f.png)As can be seen from the above figure; the object has been detected normally; and there is no error message in the chrome console.Let's look at a bug; as shown in the figure below:![ksnip_20210705-103511](https://user-images.githubusercontent.com/41098760/124410277-b6f31800-dd7c-11eb-809d-822f94fc5d5a.png)When I upload a picture without an object to be detected by the detector; the `executeAsync` function will throw an error message and terminate the program at the same time.This happened when I used the camera to detect it. When there is no object to be detected in the first frame or other frames in the camera; an error will be reported and the program will be terminated.##### This is my complete project code:https://drive.google.com/file/d/11uuOqKprEc5Ot30WuWRwwxMbSaDb7Lk-/view?usp=sharing- There is my code and model in this project- Have my profile- Steps for usage:```shell# 1. Unzip# 2. Install dependencies with yarnyarn# 3. Runyarn start```I want the program to not display this kind of error message in pictures without detected objects; and the program can run normally.I tried many exception catching methods; such as try catch; ErrorBoundary tag; etc.; but nothing worked. I think it’s not Reactjs's problem; but tfjs's problem.So please experts help me; thank you.,"['see #4716 for details (still open)====='; '> see #4716 for details (still open)I have read your question before; but I still want to ask you; is this error because there is no object to be detected by the model in the picture you entered? In other words; is the problem you encountered because executeAsync function cannot handle the returned empty object? Thank you.====='; ""I'm guessing it's about empty return as TFJS is connecting layers; it doesn't necessarily has to be output layer. =====""; 'Thank you @vladmandic ; I will this close this issue and track the same at #4716 ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5291"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5291"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,WebGL,Backend,allow shape,allow shape,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5279,Build tfjs-data with Bazel,1,closed,2021-07-01T20:33:15Z,2021-10-22T01:58:27Z,This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package; take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275; #5278,['Closed by #5748 ====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,converting package,converting package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Bazel Build Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5278,Build tfjs-layers with Bazel,1,closed,2021-07-01T20:32:34Z,2021-10-22T01:57:55Z,This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package; take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275.Tests depend on #5277.,['Closed by #5672 ====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,Layer API,API,converting package,converting package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Bazel Build Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5277,Build tfjs-backend-webgl with Bazel,1,closed,2021-07-01T20:31:53Z,2021-09-29T16:57:26Z,This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package; take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275,['Closed by #5562 ====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGL,Backend,converting package,converting package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Bazel Build Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5276,Build tfjs-converter with Bazel,1,closed,2021-07-01T20:30:16Z,2021-09-08T20:42:33Z,This issue is part of the [Adopt Bazel](https://github.com/tensorflow/tfjs/projects/17) project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package; take a look at the [Bazel Migration](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md) doc.Depends on #5275,['converter has been migrated to Bazel.====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,converting package,converting package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5275,Build tfjs-core and tfjs-backend-cpu with ts_library,1,closed,2021-07-01T20:26:26Z,2021-07-19T20:31:31Z,"This issue is part of the ""Adopt Bazel"" project and tracks converting the above package(s) to build with Bazel. For details on how to convert a package; take a look at the [Bazel Migration doc](https://github.com/tensorflow/tfjs/blob/master/BAZEL_MIGRATION.md).",['Closed by #5133====='],Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,converting package,converting package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5273,Loading a LayersModel with the HTTP IOHandler does not produce the same result as loading it from file system,2,closed,2021-07-01T10:05:23Z,2021-07-08T20:49:44Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 20.04; Node 16.3.0- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.7.0- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**Calling `model.fit` on a model that was loaded with `tf.loadLayersModel` from the filesystem works as expected.However; doing the same thing on a model loaded from a HTTP URL fails with the message `The model needs to be compiled before being used`.**Describe the expected behavior**I would expect the above error not to happen. Ideally; loading a model should give the same result no matter where the model was loaded from.**Standalone code to reproduce the issue**First; unzip [my-model.zip](https://github.com/tensorflow/tfjs/files/6747283/my-model.zip); so that we have the following files:- `my-model/model.json`- `my-model/weights.bin`Then; start a local HTTP server to serve the `my-model` directory locally:```bashpython -m http.server --directory my-model```Finally; run the following script:```jsconst tf = require(""@tensorflow/tfjs-node"");const fetch = require(""node-fetch"");async function loadLocalModel() {  const model = await tf.loadLayersModel(    tf.io.fileSystem(""my-model/model.json"")  );  await model.fit(tf.ones([2; 28 * 28]); tf.zeros([2; 10]));}async function loadRemoteModel() {  const model = await tf.loadLayersModel(    tf.io.http(""http://localhost:8000/model.json""; {      fetchFunc: fetch;    })  );  await model.fit(tf.ones([2; 28 * 28]); tf.zeros([2; 10]));}async function run() {  console.log(""\n---- LOADING LOCAL MODEL ----\n"");  await loadLocalModel();  console.log(""\n---- LOADING REMOTE MODEL ----\n"");  await loadRemoteModel();}run();```**Other info / logs**<details><summary>Output of <code>python -m http.server</code></summary>```Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...127.0.0.1 - - [01/Jul/2021 11:52:35] ""GET /model.json HTTP/1.1"" 200 -127.0.0.1 - - [01/Jul/2021 11:52:35] ""GET /weights.bin HTTP/1.1"" 200 -```</details><details><summary>Output of <code>node bug.js</code></summary>```2021-07-01 11:52:35.227362: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.---- LOADING LOCAL MODEL ----Epoch 1 / 1eta=0.0 =================================================================> 100ms 49971us/step - acc=0.00 loss=0.00 ---- LOADING REMOTE MODEL ----/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16779        var _this = _super.call(this; message) || this;                           ^RuntimeError: The model needs to be compiled before being used.    at new RuntimeError (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16779:28)    at Sequential.<anonymous> (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:27575:27)    at step (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16710:23)    at Object.next (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16691:53)    at /home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16684:71    at new Promise (<anonymous>)    at __awaiter (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16680:12)    at Sequential.fit (/home/maxime/code/tf-bug/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:27572:16)    at loadRemoteModel (/home/maxime/code/tf-bug/bug.js:17:15)    at processTicksAndRejections (node:internal/process/task_queues:96:5)```</details>","[""I'm noticing that when using the HTTP IOHandler's `.load()` method; the `trainingConfig` field of the model JSON gets dropped. Seems like this could be a linked problem. I'll investigate a bit; and if the fix isn't too complicated; I'll submit a PR to solve this.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5273"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5273"">No</a>=====']",Data & Model Error,Crash,Incorrect Code Logic,Operator,API,algorithm logic(json parse),algorithm logic(json parse),framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5259,Error: Operands could not be broadcast together with shapes 1554 and 1554;2,2,open,2021-06-29T10:08:42Z,2021-07-07T14:32:22Z,"**System information**- OS Platform and Distribution - Windows 10- Browser: Chrome; Version 91.0.4472.106- Tensorflow.js Converter Version: 3.7.0My model was trained using the tensorflow/models/research/object_detection/model_main.py script in TF 1.15 using the following command:`python model_main.py --logtostderr --model_dir=training --pipeline_config_path=training/ssdlite_mobilenet_v3_small_320x320_coco.config`I then exported the inference graph:`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v3_large_320x320_coco.config --trained_checkpoint_prefix training/model.ckpt-400000 --output_directory inference_graph`Then converted it to tensorflowjs with tensorflowjs_converter version 3.7.0:`tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes;detection_classes;detection_features;detection_multiclass_scores;detection_scores;num_detections;raw_detection_boxes;raw_detection_scores' --saved_model_tags=serve --signature_name=serving_default --output_format=tfjs_graph_model ./inference_graph/saved_model ./inference_graph/web_model`Then when I try running it in the web:```async function init() {        console.log(""Start model load"");        model = await tf.loadGraphModel('./model.json');        console.log(""Finish model load"");}async function predict() {        console.log(""executing model"");        const img = document.getElementById('img_example');        tf_img = tf.browser.fromPixels(img);        tf_img = tf_img.expandDims(0);        console.log(tf_img.shape)  // Image dimension is	vidHeight = tf_img.shape[1];        vidWidth = tf_img.shape[2];        //Perform the detection with your layer model:        let predictions = await model.executeAsync(tf_img);        for (let i = 0; i < predictions.length; i++){            console.log(predictions[i].dataSync())        }        //Draw box around the detected object:        renderPredictionBoxes(predictions[4].dataSync(); predictions[1].dataSync(); predictions[2].dataSync());        //Dispose of the tensors (so it won't consume memory)        tf_img.dispose();    }```I get the broadcast shapes error:```Uncaught (in promise) Error: Operands could not be broadcast together with shapes 1554 and 1554;2.    at iT (broadcast_util.js:81)    at where_ (where.js:61)    at where__op (operation.js:51)    at logical_executor.js:78    at operation_executor.js:83    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at BI (globals.js:192)    at operation_executor.js:83```I assume that 2 is the number of classes; and 1554 is the derivative of the size of the picture.I tried another model with 3 classes and it was 3 at the end.[web_model.zip](https://github.com/tensorflow/tfjs/files/6732461/web_model.zip)","[""Am I the first one who tried to launch the object detection model (mobilenet v3; trained on tensorflow 1.15); or does everything work for everyone and does not work only for me?I've tried different versions of Tensorflow.js Converter; tried different models (moblinet v3 small; large; large minimalistic); tried convert from saved model and frozen model. But the same error always occurs.=====""; 'I found that the functionality works on version 3.3.0. Version 3.4.0 and higher gives an error.Please fix the bug that appeared since version 3.4.0.Issue can be closed after fixing the bug in the new version.Thank you.=====']",Data & Model Error,Crash,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.2,E
https://github.com/tensorflow/tfjs/issues/5259,Error: Operands could not be broadcast together with shapes 1554 and 1554;2,2,open,2021-06-29T10:08:42Z,2021-07-07T14:32:22Z,"**System information**- OS Platform and Distribution - Windows 10- Browser: Chrome; Version 91.0.4472.106- Tensorflow.js Converter Version: 3.7.0My model was trained using the tensorflow/models/research/object_detection/model_main.py script in TF 1.15 using the following command:`python model_main.py --logtostderr --model_dir=training --pipeline_config_path=training/ssdlite_mobilenet_v3_small_320x320_coco.config`I then exported the inference graph:`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v3_large_320x320_coco.config --trained_checkpoint_prefix training/model.ckpt-400000 --output_directory inference_graph`Then converted it to tensorflowjs with tensorflowjs_converter version 3.7.0:`tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes;detection_classes;detection_features;detection_multiclass_scores;detection_scores;num_detections;raw_detection_boxes;raw_detection_scores' --saved_model_tags=serve --signature_name=serving_default --output_format=tfjs_graph_model ./inference_graph/saved_model ./inference_graph/web_model`Then when I try running it in the web:```async function init() {        console.log(""Start model load"");        model = await tf.loadGraphModel('./model.json');        console.log(""Finish model load"");}async function predict() {        console.log(""executing model"");        const img = document.getElementById('img_example');        tf_img = tf.browser.fromPixels(img);        tf_img = tf_img.expandDims(0);        console.log(tf_img.shape)  // Image dimension is	vidHeight = tf_img.shape[1];        vidWidth = tf_img.shape[2];        //Perform the detection with your layer model:        let predictions = await model.executeAsync(tf_img);        for (let i = 0; i < predictions.length; i++){            console.log(predictions[i].dataSync())        }        //Draw box around the detected object:        renderPredictionBoxes(predictions[4].dataSync(); predictions[1].dataSync(); predictions[2].dataSync());        //Dispose of the tensors (so it won't consume memory)        tf_img.dispose();    }```I get the broadcast shapes error:```Uncaught (in promise) Error: Operands could not be broadcast together with shapes 1554 and 1554;2.    at iT (broadcast_util.js:81)    at where_ (where.js:61)    at where__op (operation.js:51)    at logical_executor.js:78    at operation_executor.js:83    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at BI (globals.js:192)    at operation_executor.js:83```I assume that 2 is the number of classes; and 1554 is the derivative of the size of the picture.I tried another model with 3 classes and it was 3 at the end.[web_model.zip](https://github.com/tensorflow/tfjs/files/6732461/web_model.zip)","[""Am I the first one who tried to launch the object detection model (mobilenet v3; trained on tensorflow 1.15); or does everything work for everyone and does not work only for me?I've tried different versions of Tensorflow.js Converter; tried different models (moblinet v3 small; large; large minimalistic); tried convert from saved model and frozen model. But the same error always occurs.=====""; 'I found that the functionality works on version 3.3.0. Version 3.4.0 and higher gives an error.Please fix the bug that appeared since version 3.4.0.Issue can be closed after fixing the bug in the new version.Thank you.=====']",Data & Model Error,Crash,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/5259,Error: Operands could not be broadcast together with shapes 1554 and 1554;2,2,open,2021-06-29T10:08:42Z,2021-07-07T14:32:22Z,"**System information**- OS Platform and Distribution - Windows 10- Browser: Chrome; Version 91.0.4472.106- Tensorflow.js Converter Version: 3.7.0My model was trained using the tensorflow/models/research/object_detection/model_main.py script in TF 1.15 using the following command:`python model_main.py --logtostderr --model_dir=training --pipeline_config_path=training/ssdlite_mobilenet_v3_small_320x320_coco.config`I then exported the inference graph:`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v3_large_320x320_coco.config --trained_checkpoint_prefix training/model.ckpt-400000 --output_directory inference_graph`Then converted it to tensorflowjs with tensorflowjs_converter version 3.7.0:`tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes;detection_classes;detection_features;detection_multiclass_scores;detection_scores;num_detections;raw_detection_boxes;raw_detection_scores' --saved_model_tags=serve --signature_name=serving_default --output_format=tfjs_graph_model ./inference_graph/saved_model ./inference_graph/web_model`Then when I try running it in the web:```async function init() {        console.log(""Start model load"");        model = await tf.loadGraphModel('./model.json');        console.log(""Finish model load"");}async function predict() {        console.log(""executing model"");        const img = document.getElementById('img_example');        tf_img = tf.browser.fromPixels(img);        tf_img = tf_img.expandDims(0);        console.log(tf_img.shape)  // Image dimension is	vidHeight = tf_img.shape[1];        vidWidth = tf_img.shape[2];        //Perform the detection with your layer model:        let predictions = await model.executeAsync(tf_img);        for (let i = 0; i < predictions.length; i++){            console.log(predictions[i].dataSync())        }        //Draw box around the detected object:        renderPredictionBoxes(predictions[4].dataSync(); predictions[1].dataSync(); predictions[2].dataSync());        //Dispose of the tensors (so it won't consume memory)        tf_img.dispose();    }```I get the broadcast shapes error:```Uncaught (in promise) Error: Operands could not be broadcast together with shapes 1554 and 1554;2.    at iT (broadcast_util.js:81)    at where_ (where.js:61)    at where__op (operation.js:51)    at logical_executor.js:78    at operation_executor.js:83    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at BI (globals.js:192)    at operation_executor.js:83```I assume that 2 is the number of classes; and 1554 is the derivative of the size of the picture.I tried another model with 3 classes and it was 3 at the end.[web_model.zip](https://github.com/tensorflow/tfjs/files/6732461/web_model.zip)","[""Am I the first one who tried to launch the object detection model (mobilenet v3; trained on tensorflow 1.15); or does everything work for everyone and does not work only for me?I've tried different versions of Tensorflow.js Converter; tried different models (moblinet v3 small; large; large minimalistic); tried convert from saved model and frozen model. But the same error always occurs.=====""; 'I found that the functionality works on version 3.3.0. Version 3.4.0 and higher gives an error.Please fix the bug that appeared since version 3.4.0.Issue can be closed after fixing the bug in the new version.Thank you.=====']",Regression,Poor Performance,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.2,E
https://github.com/tensorflow/tfjs/issues/5259,Error: Operands could not be broadcast together with shapes 1554 and 1554;2,2,open,2021-06-29T10:08:42Z,2021-07-07T14:32:22Z,"**System information**- OS Platform and Distribution - Windows 10- Browser: Chrome; Version 91.0.4472.106- Tensorflow.js Converter Version: 3.7.0My model was trained using the tensorflow/models/research/object_detection/model_main.py script in TF 1.15 using the following command:`python model_main.py --logtostderr --model_dir=training --pipeline_config_path=training/ssdlite_mobilenet_v3_small_320x320_coco.config`I then exported the inference graph:`python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v3_large_320x320_coco.config --trained_checkpoint_prefix training/model.ckpt-400000 --output_directory inference_graph`Then converted it to tensorflowjs with tensorflowjs_converter version 3.7.0:`tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes;detection_classes;detection_features;detection_multiclass_scores;detection_scores;num_detections;raw_detection_boxes;raw_detection_scores' --saved_model_tags=serve --signature_name=serving_default --output_format=tfjs_graph_model ./inference_graph/saved_model ./inference_graph/web_model`Then when I try running it in the web:```async function init() {        console.log(""Start model load"");        model = await tf.loadGraphModel('./model.json');        console.log(""Finish model load"");}async function predict() {        console.log(""executing model"");        const img = document.getElementById('img_example');        tf_img = tf.browser.fromPixels(img);        tf_img = tf_img.expandDims(0);        console.log(tf_img.shape)  // Image dimension is	vidHeight = tf_img.shape[1];        vidWidth = tf_img.shape[2];        //Perform the detection with your layer model:        let predictions = await model.executeAsync(tf_img);        for (let i = 0; i < predictions.length; i++){            console.log(predictions[i].dataSync())        }        //Draw box around the detected object:        renderPredictionBoxes(predictions[4].dataSync(); predictions[1].dataSync(); predictions[2].dataSync());        //Dispose of the tensors (so it won't consume memory)        tf_img.dispose();    }```I get the broadcast shapes error:```Uncaught (in promise) Error: Operands could not be broadcast together with shapes 1554 and 1554;2.    at iT (broadcast_util.js:81)    at where_ (where.js:61)    at where__op (operation.js:51)    at logical_executor.js:78    at operation_executor.js:83    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at BI (globals.js:192)    at operation_executor.js:83```I assume that 2 is the number of classes; and 1554 is the derivative of the size of the picture.I tried another model with 3 classes and it was 3 at the end.[web_model.zip](https://github.com/tensorflow/tfjs/files/6732461/web_model.zip)","[""Am I the first one who tried to launch the object detection model (mobilenet v3; trained on tensorflow 1.15); or does everything work for everyone and does not work only for me?I've tried different versions of Tensorflow.js Converter; tried different models (moblinet v3 small; large; large minimalistic); tried convert from saved model and frozen model. But the same error always occurs.=====""; 'I found that the functionality works on version 3.3.0. Version 3.4.0 and higher gives an error.Please fix the bug that appeared since version 3.4.0.Issue can be closed after fixing the bug in the new version.Thank you.=====']",Regression,Poor Performance,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/5246,Upgrading from 3.5.0 to 3.6.0 introduces shader compile error 'xTexelC8Ready' : undeclared identifier,7,closed,2021-06-23T12:14:37Z,2021-07-26T14:26:00Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.4- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12; Pixel 2- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- Browser version: Chrome and Safari- Tensorflow.js Converter Version:**Describe the current behavior**After upgrading from @tensorflow/tfjs 3.5.0 to 3.6.0 fragment shader compilation fails. I believe the problem is the introduction of the int xTexelC${c * 2}Ready variables in tfjs-backend-webgl/conv_packed_gpu_depthwise.ts. As can be seen from the attached compilation error variables up to int xTexelC4Ready are created; but the code then goes in to try and use variable  xTexelC8Ready.[crash.txt](https://github.com/tensorflow/tfjs/files/6701660/crash.txt)**Describe the expected behavior**No fragment compilation failure**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Can you please try with latest 3.7.0 ; also please specify the steps you were following.Thank you====='; '@stevexbritton if you can share the model would help us identify what are the configurations for the failing kernels. thanks====='; 'Sorry for the delay in replying.Yes it does go wrong with 3.7.0.I have created a codepen example (https://codepen.io/stevexbritton/pen/wvJVExo?editors=1111) demonstrating the problem. I hope this helps;Steve====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Hi;Have you had a chance to look at this? Is the codepen example I supplied adequate?I""m only asking because I don\'t want this issue closed due to inactivity.Thanks;Steve====='; 'Seems to work with release 3.8.0.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">No</a>=====']",Browser & Device Error,Crash,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Shader Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.4,E
https://github.com/tensorflow/tfjs/issues/5246,Upgrading from 3.5.0 to 3.6.0 introduces shader compile error 'xTexelC8Ready' : undeclared identifier,7,closed,2021-06-23T12:14:37Z,2021-07-26T14:26:00Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.4- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12; Pixel 2- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- Browser version: Chrome and Safari- Tensorflow.js Converter Version:**Describe the current behavior**After upgrading from @tensorflow/tfjs 3.5.0 to 3.6.0 fragment shader compilation fails. I believe the problem is the introduction of the int xTexelC${c * 2}Ready variables in tfjs-backend-webgl/conv_packed_gpu_depthwise.ts. As can be seen from the attached compilation error variables up to int xTexelC4Ready are created; but the code then goes in to try and use variable  xTexelC8Ready.[crash.txt](https://github.com/tensorflow/tfjs/files/6701660/crash.txt)**Describe the expected behavior**No fragment compilation failure**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Can you please try with latest 3.7.0 ; also please specify the steps you were following.Thank you====='; '@stevexbritton if you can share the model would help us identify what are the configurations for the failing kernels. thanks====='; 'Sorry for the delay in replying.Yes it does go wrong with 3.7.0.I have created a codepen example (https://codepen.io/stevexbritton/pen/wvJVExo?editors=1111) demonstrating the problem. I hope this helps;Steve====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Hi;Have you had a chance to look at this? Is the codepen example I supplied adequate?I""m only asking because I don\'t want this issue closed due to inactivity.Thanks;Steve====='; 'Seems to work with release 3.8.0.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">No</a>=====']",Browser & Device Error,Crash,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Shader Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/5246,Upgrading from 3.5.0 to 3.6.0 introduces shader compile error 'xTexelC8Ready' : undeclared identifier,7,closed,2021-06-23T12:14:37Z,2021-07-26T14:26:00Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.4- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12; Pixel 2- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- Browser version: Chrome and Safari- Tensorflow.js Converter Version:**Describe the current behavior**After upgrading from @tensorflow/tfjs 3.5.0 to 3.6.0 fragment shader compilation fails. I believe the problem is the introduction of the int xTexelC${c * 2}Ready variables in tfjs-backend-webgl/conv_packed_gpu_depthwise.ts. As can be seen from the attached compilation error variables up to int xTexelC4Ready are created; but the code then goes in to try and use variable  xTexelC8Ready.[crash.txt](https://github.com/tensorflow/tfjs/files/6701660/crash.txt)**Describe the expected behavior**No fragment compilation failure**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Can you please try with latest 3.7.0 ; also please specify the steps you were following.Thank you====='; '@stevexbritton if you can share the model would help us identify what are the configurations for the failing kernels. thanks====='; 'Sorry for the delay in replying.Yes it does go wrong with 3.7.0.I have created a codepen example (https://codepen.io/stevexbritton/pen/wvJVExo?editors=1111) demonstrating the problem. I hope this helps;Steve====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Hi;Have you had a chance to look at this? Is the codepen example I supplied adequate?I""m only asking because I don\'t want this issue closed due to inactivity.Thanks;Steve====='; 'Seems to work with release 3.8.0.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">No</a>=====']",Regression,Poor Performance,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Shader Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.4,E
https://github.com/tensorflow/tfjs/issues/5246,Upgrading from 3.5.0 to 3.6.0 introduces shader compile error 'xTexelC8Ready' : undeclared identifier,7,closed,2021-06-23T12:14:37Z,2021-07-26T14:26:00Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.4- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12; Pixel 2- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- Browser version: Chrome and Safari- Tensorflow.js Converter Version:**Describe the current behavior**After upgrading from @tensorflow/tfjs 3.5.0 to 3.6.0 fragment shader compilation fails. I believe the problem is the introduction of the int xTexelC${c * 2}Ready variables in tfjs-backend-webgl/conv_packed_gpu_depthwise.ts. As can be seen from the attached compilation error variables up to int xTexelC4Ready are created; but the code then goes in to try and use variable  xTexelC8Ready.[crash.txt](https://github.com/tensorflow/tfjs/files/6701660/crash.txt)**Describe the expected behavior**No fragment compilation failure**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Can you please try with latest 3.7.0 ; also please specify the steps you were following.Thank you====='; '@stevexbritton if you can share the model would help us identify what are the configurations for the failing kernels. thanks====='; 'Sorry for the delay in replying.Yes it does go wrong with 3.7.0.I have created a codepen example (https://codepen.io/stevexbritton/pen/wvJVExo?editors=1111) demonstrating the problem. I hope this helps;Steve====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Hi;Have you had a chance to look at this? Is the codepen example I supplied adequate?I""m only asking because I don\'t want this issue closed due to inactivity.Thanks;Steve====='; 'Seems to work with release 3.8.0.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5246"">No</a>=====']",Regression,Poor Performance,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Shader Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/5245,Otsu Image Threshold Broken in tfjs-react-native,1,open,2021-06-22T20:48:49Z,2021-06-22T21:52:32Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): N/A- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Android 9.0 Samsung Galaxy S9 Plus- TensorFlow.js installed from (npm or script link): master branch- TensorFlow.js version (use command below): master branch- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**In the image threshold op; otsu's method returns a different (incorrect) result for webgl in tfjs-react-native than in the browser.**Describe the expected behavior**Otsu's method returns the correct result in tfjs-react-native.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.Run CI tests on tfjs-react-native. The bug may also appear when running the integration tests of tfjs-react-native on a local android phone.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.I visualized the test results and for some reason; the otsu threshold test output matches the expected output for binary. This is almost certainly a coincidence.![](https://user-images.githubusercontent.com/1474501/122586563-b1be6b00-d011-11eb-99b5-e2523060f2ae.png)Logs:https://console.cloud.google.com/cloud-build/builds/15aa4850-a102-45f4-9525-c06588d6f668;step=7?project=learnjs-174218More details on this PR: https://github.com/tensorflow/tfjs/pull/5217,['[Reference colab contributed to the TF repo](https://colab.sandbox.google.com/drive/1CdVfa2NlkQBga1E9dBwHved36Tk7Bg61)[TF Proposal](https://github.com/tensorflow/addons/issues/358)====='],Incorrect Functionality,Incorrect Functionality,Dependency Error,Mobile,Platform,change dependency version,Modifying dependency configuration,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3""
  }
}
```",D,B.2
https://github.com/tensorflow/tfjs/issues/5242,`tfjs-automl` image classification memory leak on Node.js,4,open,2021-06-22T16:36:16Z,2021-08-09T23:37:00Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Repo link below- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Darwin 20.5.0 x64 (macOS Big Sur 11.4)- TensorFlow.js installed from (npm or script link): npm- System information:```jsconst os = require('os');const { node; v8 } = process.versions;const plat = `OS: ${os.type()} ${os.release()} ${os.arch()}\nNode.js: ${node}\nV8: ${v8}`;const cpus = os.cpus().map(cpu => cpu.model).reduce((o; model) => {  o[model] = (o[model] || 0) + 1;  return o;}; {});const cpusInfo = Object.keys(cpus).map((key) => {  return `${key} x ${cpus[key]}`;}).join('\n');console.log(`${plat}\nCPU: ${cpusInfo}\nMemory: ${os.totalmem() / (1024 * 1024)} MiB\n`);// OS: Darwin 20.5.0 x64// Node.js: 14.17.0// V8: 8.4.371.23-node.63// CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz x 16// Memory: 32768 MiB```**Describe the current behavior**`Resident Set Size` keeps getting bigger.**Standalone code to reproduce the issue**https://github.com/SukkaW/tfjs-memory-leak-example**Other info / logs** Include any logs or source code that would be helpful to```shgit clone https://github.com/SukkaW/tfjs-memory-leak-example && cd tfjs-memory-leak-examplenpm inode index.js``````Platform info:OS: Darwin 20.5.0 x64Node.js: 14.17.0V8: 8.4.371.23-node.63CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz x 16Memory: 32768 MiBdaisy 146083840 2035808daisy 152354816 2035808daisy 153202688 2035808daisy 157728768 2035808daisy 163180544 2035808daisy 167120896 2035808daisy 169332736 2035808daisy 169385984 2035808daisy 170770432 2035808daisy 172281856 2035808daisy 174080000 2035808daisy 174120960 2035808daisy 176160768 2035808.....```,"['The above GitHub link does not work ; please check; meanwhile can you check this comment https://github.com/tensorflow/tfjs/issues/4015#issuecomment-704335135 if it helps ; thank you ====='; '> The above GitHub link does not work ; please check; meanwhile can you check this comment [#4015 (comment)](https://github.com/tensorflow/tfjs/issues/4015#issuecomment-704335135) if it helps ; thank you@rthadur The repo should be available now.I will look into the link you provided; thanks for your reply!====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; ""Why the issue is marked as stale? I don't see the issue is being solved; or if it is a dupe of #3061=====""]",Memory Leak,Poor Performance,Unknown,TF(CPU),Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,E
https://github.com/tensorflow/tfjs/issues/5237,"Kernel 'Linspace"" not registered for backend 'undefined'",3,closed,2021-06-20T21:08:42Z,2021-06-24T23:45:25Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): 3.7.0- TensorFlow.js version (use command below): 3.7.0- Browser version: 3.7.0**Describe the current behavior**```> tf.linspace(0; 9; 10)Uncaught Error: Kernel 'LinSpace' not registered for backend 'undefined'```In browser docs example or locally in nodeMight be connected to a recent commit but nothing obvious to me so haven't submitted PR yet: https://github.com/tensorflow/tfjs/commit/293d357e884cb903fc9c8a4ae9c48eb130f05e3b#diff-f085c223aa4f2e310b69b8f405f28107ae81921dbf1468392683f4e83fb9a0d7**Standalone code to reproduce the issue**```tf.linspace(0; 9; 10)```,"['@jinjingforever I was able to reproduce the same ; please check.====='; 'Thank you for the report @asilvas! The PR above should fix this issue (it will be in the next release). For now; a workaround is wrapping the code around `tf.ready`:```jstf.ready(()=>{  tf.linspace(0; 9; 10);  // ...});```====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5237"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5237"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,change code order,Adjust API invocation sequence,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5234,Fail to load converted model in node.js,2,closed,2021-06-18T20:11:08Z,2021-06-22T01:18:32Z,Hi guys! I'm new to TensoFlowjs and I'm struggling to find a solution to this error. I'm trying to train a Blazeface model in Python and convert it to use in node.js.**System information**- OS Platform and Distribution: Windows 10- TensorFlow.js installed from npm- TensorFlow.js version 3.6.0- Tensorflow.js Converter Version: tensorflowjs 3.7.0; - Dependency versions: keras 2.4.0; tensorflow 2.3.1- Node version: 15.14.0- NPM version: 7.7.6**Describe the problem**I trained a BlazeFace model from code provided by https://github.com/FurkanOM/tf-blazeface and adapted it to get 68 landmarks https://github.com/FurkanOM/tf-blazeface/issues/2.I saved the trained model with TF 2.3.1 `model.save('modelSaved.h5')`I converted `modelSaved.h5` with `tensorflowjs_converter --input_format keras --weight_shard_size_bytes 60000000 modelSaved.h5 converted_models/blazeface`.The first time I tried to load the model in Js; I got the error:```import * as tf from '@tensorflow/tfjs'await tf.setBackend('webgl')const modelJson = 'https://***/models/blazeface1/model.json';const modelLoaded = await tf.loadLayersModel(modelJson);```**Error: Unknown layer: HeadWrapper. This may be due to one of the following reasons:1. The layer is defined in Python; in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript; but is not registered properly with tf.serialization.registerClass().**After some research I build a HeadWrapper class in JavaScript. The original HeadWrapper is here: https://github.com/FurkanOM/tf-blazeface/blob/master/blazeface.py**custom_layer.js** ```import * as tf from '@tensorflow/tfjs'; class HeadWrapper extends tf.layers.Layer {   constructor() {     super({});     this.supportsMasking = true;   } computeOutputShape(inputShape) {     return [inputShape[0]; inputShape[1]; inputShape[2]; 2 * inputShape[3]]   }  call(inputs; kwargs) {     let input = inputs;     if (Array.isArray(input)) {       batch_size = input[0].shape[0];       input = input[0];     }     this.invokeCallHook(inputs; kwargs);     const origShape = input.shape;     // need something to do here     outputs= origShape;     return tf.concat(outputs; 1);   }``` Then loading again the model in js:```import * as tf from '@tensorflow/tfjs'import {headwrapper} from '../custom-layer/custom_layer';await tf.setBackend('webgl')const layer = headwrapper();const modelJson = 'https://***/models/blazeface1/model.json';const modelLoaded = await tf.loadLayersModel(modelJson);```I got the error:**TypeError: Cannot read property ‘length’ of undefined load**Then I got stucked. Can you help?h5 saved model: https://drive.google.com/file/d/1NXzZnBbPcvMBMCxStChhkfhC9TloR47E/view?usp=sharingConverted model and weights: https://drive.google.com/drive/folders/1OvQTG9wDLDLjZ17TMDNWY4K1RrDtTuSD?usp=sharingThanks in advance!,"['@leolellisr this might be related issue to the repo https://github.com/FurkanOM/tf-blazeface ; This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow.js) since it is not a bug or feature request. There is also a larger community that reads questions there.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5234"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5234"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Layer API,API,add support for layers,Add unsupported operator,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5216,Safari 15 / iOS 15 Vertex/Shader Errors,6,closed,2021-06-12T16:15:36Z,2021-07-14T18:11:11Z,DESCRIPTIONI'm getting a consistent vertex/fragment error across the official TFJS demos using Safari 15 on the new developer release of iOS 15.0. It would seem changes have been introduced into Webkit that are causing this; so hopefully shouldn't require any changes by the tensorflow team; but I did want to raise it here.ERROR LOG:Internal error compiling shader with Metal backend.Please submit this shader; or website as a bug to https://bugs.webkit.orgError: Failed to link vertex and fragment shaders.STEPS TO REPRODUCE:The error can be seen running these demos on Safari 15:[https://emojiscavengerhunt.withgoogle.com/](https://emojiscavengerhunt.withgoogle.com/)[https://magenta.tensorflow.org/demos/performance_rnn/index.html](https://magenta.tensorflow.org/demos/performance_rnn/index.html)[https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html](https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html)HARDWARE:I am using an iPhone XS running iOS 15. I tested these on a iPhone X running 14.6 and had no errors.REPORTING:I filed a bug report with WebKit too; but this may be something a team member may be better positioned to follow up on as it seems to be breaking pretty consistently across my own code and the official TFJS demos:[https://bugs.webkit.org/show_bug.cgi?id=226953](https://bugs.webkit.org/show_bug.cgi?id=226953),"['@heystoney can you paste the details of this failure? We usually show the shader source code and where the compilation fails.You might need to use safari remote debug to get the error message. thanks.====='; ""For sure. So using [this one](https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html) as example (but the error is the same in each) in Safari 15:Before erroring it will log:**Internal error compiling shader with Metal backend. (webgl_util.ts:154)Please submit this shader; or website as a bug to https://bugs.webkit.org**The console error that follows is:**Unhandled Promise Rejection: Error: Failed to link vertex and fragment shaders.**The error always occurs when tfjs is running its training (or segmentation for bodypix). In this case; the line that triggers the error is:`await demo.train(trainIterations; batchSize; numTestExamples);`That's all I get in my logs; so I'm not sure if there is an additional debug mode these could be run in with more information; but hopefully that helps. I had also attempted to run this page as well:https://js.tensorflow.org/debug/It didn't output any tf.ENV information or navigator.userAgent or run any tf.scalar calculations. No logs either.=====""; ""Just a small update -- I ran the tests again in the new iOS15 Beta 2 and this error still exists across the official TFJS examples.A [patch](https://bugs.webkit.org/show_bug.cgi?id=226953) had been created by the Webkit team; but I'm not sure if it was included in the new iOS release. This may need to be followed up on by someone closer to the Webkit team so TFJS can continue working when iOS15 is released.=====""; ""One more quick update -- heard back from a member of the Webkit team that their patch for this issue did not make it into iOS15 Beta 2; but they are hopeful it will be in the next one. I'll test when Beta 3 is released and update/close the ticket if it's fixed.=====""; 'This has been fixed in iOS 15 beta 3. Closing it out.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5216"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5216"">No</a>=====']",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,patch browser,Changing device/browser,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Limits""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,D.2
https://github.com/tensorflow/tfjs/issues/5205,[perf] improve shader compilation for WebGL with KHR_parallel_shader_compile extension,16,open,2021-06-09T17:55:40Z,2021-10-07T05:13:35Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below): 3.7.0- Browser version: NA- Tensorflow.js Converter Version: NA**Describe the current behavior**The initial inference on current TFJS webGL backend is much slower; which is caused by shader compilation and texture allocation.**Describe the expected behavior**With the latest extension KHR_parallel_shader_compile; there is a chance to speed up the shader compilation and reduce the initial inference time.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['cc @qjia7 ====='; '@pyu10055 Will be someone be assigned for this issue or do you need any help from us?====='; '@qjia7 If you have bandwidth; we would love to have you help with the initial investigation. As of today; our shader compilations are performed at per-op execution time.  It would be interesting to see how would the extension fit into this scenarios.====='; ""There are several things we can have a try:1. Simply apply `KHR_parallel_shader_compile` extension and don't let the shader compilation block the main process. So it can partially hide the data uploading time. Previous the process is `upload data to gpu` -> `compile shader` -> run. Now; the two processes  `upload data to gpu` and `compile shader` can be parallel.2. Use uniforms instead of constant value so that the shader generation doesn't depend on runtime shapes. We can get the static shader string of each op before it's executed. It's also the ideal scenario to use  `KHR_parallel_shader_compile` to let multiple shaders really parallel. However; in this scenario; we don't know how many shaders need to be pre-compiled. To compile all of them; it may doesn't make sense since the users may only use several of them. A method may be that we can pre-compile some widely/frequently used shaders; like conv2d; matmul; depthwiseConv2d; add; relu. And leave others to be compiled in execution.  The advantage of uniforms is not only for `KHR_parallel_shader_compile`. It can also greatly reduce the shader variants; which means the total shader numbers will also be great reduced.3. The issue in step 2 is that in backend; we don't know the model information. So we don't know the ops set to be executed. One idea is that in upper level; we can see the model and the whole graph. Maybe we can provide a path to backend and tell the backend 'Hi; these ops will be executed; can you precompile them?'. In this case; we can do more things in backend; not only for compilation; also execution optimization.We'd love to try 1 and 2. But 3 needs your help since it will change the upper framework. How do you think?=====""; ""just my $0.02...  first - i LOVE this proposal! This is probably the biggest issue with WebGL nowadays as slow app startup turns users away.(1) doesn't do much due to how tfjs shader compilation is structured  (3) is really interesting and should be doable without any changes to existing code: you could extend class `GraphModel` to add method `warmup` which can be executed optionally  (and to make it clean; such `warmup` method should call equivalent method in the backend (for any backend other than GL; it would simply return immediately))enumerating ops in `GraphModel` once it's loaded is easy and fast:```tsconst model: GraphModel = tf.loadGraphModel('test/model.json');const ops: Record<string; Array<string>> = {};for (const op of Object.values(model.executor.graph.nodes) as Array<{category: string; op: string}>) {  if (!ops[op.category]) ops[op.category] = [];  if (!ops[op.category].includes(op.op)) ops[op.category].push(op.op);}console.log('ops used by model:'; ops);```output:```jsops used by model: {  graph: [ 'Const'; 'Placeholder'; 'Identity' ];  convolution: [ '_FusedConv2D'; 'FusedDepthwiseConv2dNative'; 'DepthwiseConv2dNative'; 'Conv2D'; 'MaxPool' ];  arithmetic: [ 'Mul'; 'Add'; 'FloorDiv'; 'FloorMod'; 'Sub' ];  basic_math: [ 'Relu6'; 'Relu'; 'Sigmoid' ];  reduction: [ 'Mean' ];  image: [ 'ResizeBilinear' ];  slice_join: [ 'ConcatV2'; 'GatherV2'; 'StridedSlice' ];  transformation: [ 'Reshape'; 'Cast'; 'ExpandDims' ];  logical: [ 'Equal' ];  evaluation: [ 'TopKV2' ]}```=====""; '@qjia7 I agree with @vladmandic that option 2 and 3 looks like crucial to gain performance gain on the parallel compilation.Similar to the warm up run; the graph model can have a compilation step; and the engine should have a compile API in comparison to current execution API; to avoid any texture upload.====='; ""(Non-technical comment: I write a browser plugin that basically blocks browser functionality for 10 seconds during model loading; so I'm quite happy to hear about performance improvement ideas here and plan to watch the progress eagerly! Thanks!)=====""; 'Thanks for your inputs. I will take a look at the step 2 `Use uniforms instead of constant value` as a start.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5205"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5205"">No</a>====='; ""@qjia7 Thanks for your hard work! I was so excited to give this a try as I saw TF.js 3.8.0 was released! My plugin is still back on 2.7.0; so I did a quick upgrade.Unfortunately; somewhere between 2.7.0 and 3.8.0 the performance for model load plus first inference time became much worse.  I have linked off in the issue above but the overall time went from about 9 seconds to 13.5 seconds just due to TF.js version change; so it doesn't look like I'll be upgrading quite yet.What kind of performance numbers were others here seeing from this PR? (also @vladmandic  @pyu10055 )=====""; ""@wingman-jr-addon This issue has not been finished. It may be closed by accident. Currently; using shapes uniforms is disabled by default. You need to set `WEBGL_USE_SHAPES_UNIFORMS` to true to use it. And so far; only unary/binary applied it. For other ops; they are on the way. For example; conv2d; you can find here https://github.com/tensorflow/tfjs/pull/5297.  I don't expect it will bring big perf regression since it's disabled by default. Can you share me a reproducible example? I can double-check it whether it's related with the changes by bringing uniforms. =====""; ""Thank you for the detailed explanation @qjia7 - if it's hidden behind a flag; I'm guessing that this regression has nothing to do with your recent work. Based on that; let me do some bisecting on versions and see if I can narrow the cause down a bit further and then provide a minimal reproduction either here or in an appropriate issue.=====""; ""@qjia7 Through bisection I've narrowed it down to a change that occurred between 3.3.0 and 3.4.0. I'll do some more looking but that means it is definitely not related to this functionality.=====""; ""I've tested this on my notebook with 3 different models of medium-high complexity  | Model | DataSet | WEBGL_PACK_DEPTHWISECONV | WEBGL_USE_SHAPES_UNIFORMS | Warmup | Execution | Note || --- | --- | --- | --- | --- | --- | --- || Inception-v4 | ImageNet | True | False | 11.2sec | 42ms | Default| Inception-v4 | ImageNet | False | False | 10.8sec | 45ms || Inception-v4 | ImageNet | False | True | 10.8sec | 45ms || Inception-v4 | ImageNet | True | True | 11.2sec | 42ms || SSD/MobileNet-v2 | OpenImages | True | False | 14.7 | 2.1sec | Default| SSD/MobileNet-v2 | OpenImages | False | False | 13.3sec | 2.2sec || SSD/MobileNet-v2 | OpenImages | False | True | 12.7sec | 2.1sec || SSD/MobileNet-v2 | OpenImages | True | True | 13.6sec | 2.1sec || EfficientDet-D4 | CoCo | True | False | 23.1sec | 12.9sec | Default| EfficientDet-D4 | CoCo | False | False | 16.1sec | 14.5sec || EfficientDet-D4 | CoCo | False | True | 15.9sec | 14.0sec || EfficientDet-D4 | CoCo | True | True | 21.1sec | 13.0ms |All-in-all:- WEBGL_USE_SHAPES_UNIFORMS helps to significantly reduce warmup with *NO* negative impact on subsequent inference  - WEBGL_PACK_DEPTHWISECONV increases warmup too much even if subsequent inference is slightly fasterAs it is; I'll be setting `WEBGL_USE_SHAPES_UNIFORMS=True` and `WEBGL_PACK_DEPTHWISECONV=False` on my projects as even with uniforms enabled (which does help); it's still too slow on warmup  *Note: Chrome does extensive shader caching between sessions; so simple page reload is not sufficient and full browser restart is needed between tests*=====""; ""Thank you @vladmandic for your much more thorough analysis. I'm sure that took quite some time. I'll be watching over on the issue where you cross-posted as we look at this issue specifically.=====""; 'see #5689 for fully reproducible code and additional performance notes.=====']",Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,webgl shader type replacer,Replace data Shape/type,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/5203,tfjs-backend-wasm: Webpack can't resolve 'os' error,2,open,2021-06-09T15:07:05Z,2021-07-01T18:47:05Z,"**System information**- macOS Catalina 10.15.7- TensorFlow.js installed from NPM- TensorFlow.js version: 3.7.0- Node version: 14.17.0- NPM version: 7.16.0**Describe the problem**After installing package `npm i @tensorflow/tfjs-backend-wasm` and adding[ all required settings ](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/starter/webpack/README.md) for Webpack bundler it fails with an error:```ERROR in ./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js 9:26970-26988Module not found: Error: Can't resolve 'os' in '/Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out'BREAKING CHANGE: webpack < 5 used to include polyfills for node.js core modules by default.This is no longer the case. Verify if you need this module and configure a polyfill for it.If you want to include a polyfill; you need to:	- add a fallback 'resolve.fallback: { ""os"": require.resolve(""os-browserify/browser"") }'	- install 'os-browserify'If you don't want to include a polyfill; you can use an empty module like this:	resolve.fallback: { ""os"": false }resolve 'os' in '/Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out'  Parsed request is a module  using description file: /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/package.json (relative path: ./wasm-out)    resolve as module      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/tfjs-backend-wasm/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/@tensorflow/node_modules doesn't exist or is not a directory      /Users/Bob/Node/tensorflow/node_modules/node_modules doesn't exist or is not a directory      looking for modules in /Users/Bob/Node/tensorflow/node_modules        single file module          using description file: /Users/Bob/Node/tensorflow/package.json (relative path: ./node_modules/os)            no extension              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os doesn't exist            .js              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.js doesn't exist            .json              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.json doesn't exist            .wasm              Field 'browser' doesn't contain a valid alias configuration              /Users/Bob/Node/tensorflow/node_modules/os.wasm doesn't exist        /Users/Bob/Node/tensorflow/node_modules/os doesn't exist      /Users/Bob/Node/node_modules doesn't exist or is not a directory      /Users/Bob/node_modules doesn't exist or is not a directory      /Users/Bob/Documents/node_modules doesn't exist or is not a directory      /Users/Bob/node_modules doesn't exist or is not a directory      /Users/node_modules doesn't exist or is not a directory      /node_modules doesn't exist or is not a directory @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/backend_wasm.js 19:0-85 261:20-52 262:19-42 @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/base.js 19:0-51 20:0-72 20:0-72 20:0-72 20:0-72 24:27-31 25:15-26 @ ./node_modules/@tensorflow/tfjs-backend-wasm/dist/index.js 18:0-23 18:0-23 @ ./src/index.js 2:0-39```---**U.P.D.**And the deps of my `package.json` file:```{  ...  ""devDependencies"": {    ""html-webpack-plugin"": ""^5.3.1"";    ""webpack"": ""^5.38.1"";    ""webpack-cli"": ""^4.7.2"";    ""webpack-dev-server"": ""^3.11.2""  };  ""dependencies"": {    ""@tensorflow/tfjs"": ""^3.7.0"";    ""@tensorflow/tfjs-backend-wasm"": ""^3.7.0""  }}```My `webpack.config.js` file:```const HtmlWebpackPlugin = require('html-webpack-plugin');const path = require('path');module.exports = {   module: {      rules: [         {            test: /\.wasm$/i;            type: 'javascript/auto';            use: [               {                  loader: 'file-loader';               };            ];         };      ];   };   ...}```My `index.js` file:```import * as tf from '@tensorflow/tfjs';import '@tensorflow/tfjs-backend-wasm';import wasmSimdPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-simd.wasm';import wasmSimdThreadedPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm-threaded-simd.wasm';import wasmPath from './node_modules/@tensorflow/tfjs-backend-wasm/dist/tfjs-backend-wasm.wasm';setWasmPaths({  'tfjs-backend-wasm.wasm': wasmPath;  'tfjs-backend-wasm-simd.wasm': wasmSimdPath;  'tfjs-backend-wasm-threaded-simd.wasm': wasmSimdThreadedPath});...```---Please help.",['see <https://github.com/tensorflow/tfjs/issues/4745> for details.====='; 'Thank you @vladmandic can we close this to track the issue in #4745 ?====='],Build & Install Failure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Module Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5199,[Perf] The performance of conv2d is very very poor if the inChannel and outChannel are small and height and width are large in webgl.,1,open,2021-06-09T03:14:21Z,2021-11-04T17:35:10Z,Tested using https://honry.github.io/webnn-samples/style_transfer/?backend=webgl <!--StartFragment-->Type | Time(ms) | Inputs | Output-- | -- | -- | --Conv2D | 82.86 | input0: 4D[1;548;548;4]input1: 4D[9;9;4;3] | 1;540;540;3Conv2D | 63.09 | input0: 4D[1;548;548;3]input1: 4D[9;9;3;4] | 1;540;540;4<!--EndFragment-->,['cc @pyu10055 @lina128 @jinjingforever ====='],Slow Execution,Poor Performance,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",B.1.1,E
https://github.com/tensorflow/tfjs/issues/5197,[Perf] The time of Conv2DBackpropInput is very long in BlazePose/hand_detector models in WebGL,3,open,2021-06-09T02:39:37Z,2021-11-04T17:35:22Z,hand_detector<!--StartFragment-->Kernel | Time(ms) | Inputs | Output | GPUPrograms-- | -- | -- | -- | --Conv2DBackpropInput | 9.73 | input0: 4D[1;16;16;256]input1: 4D[2;2;128;256] | 1;32;32;128 | UnpackProgram: 0.026083; Conv2DDerInputProgram: 9.706294Conv2DBackpropInput | 4.93 | input0: 4D[1;8;8;256]input1: 4D[2;2;256;256] | 1;16;16;256 | UnpackProgram: 0.011333; Conv2DDerInputProgram: 4.91648<!--EndFragment-->BlazePose with inputSize 256; inputType Tensor<!--StartFragment-->Kernel | Time(ms) | Inputs | Output | GPUPrograms-- | -- | -- | -- | --Conv2DBackpropInput | 20.20 | input0: 4D[1;7;7;1152]input1: 4D[2;2;192;1152] | 1;14;14;192 | UnpackProgram: 0.042083; Conv2DDerInputProgram: 20.159669<!--EndFragment-->,['fyi @pyu10055 @lina128 @jinjingforever ====='; 'This is very informative; thank you @qjia7 ! In the GPUPrograms column; is Conv2DDerInputProgram the packed version and UnpackProgram the unpacked version; both from WebGL backend?====='; '@lina128 the UnpackProgram is the shader to unpack outputs from the previous kernel; the Conv2DDerInputProgram is the shader for Conv2DBackpropInput; which one supports unpacked texture.====='],Slow Execution,Poor Performance,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",B.1.1,E
https://github.com/tensorflow/tfjs/issues/5192,tfjs-layers bundles contain a copy of tfjs-core,1,open,2021-06-08T15:16:24Z,2021-06-08T19:56:25Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): Github- TensorFlow.js version (use command below): 11ea58c8fe2ba94d6cb27d9714a5941601ae62a7- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**tfjs-layers bundles contain a copy of tfjs-core.Looking at the node bundle; we see that it includes the full definition for the `Tensor` class (search for `class Tensor`); which is defined in tfjs-core. I'm pretty sure this should not be bundled in tfjs-layers. Strangely; the node bundle still `require`s tfjs-core in `var tfc = require('@tensorflow/tfjs-core');`. **Describe the expected behavior**tfjs-core should be external to the tfjs-layers bundle. None of the classes defined in tfjs-core should be duplicated in the tfjs-layers bundle.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.All commands run in `tfjs-layers`:To enable visualization for all tfjs-layers bundles; replace `visualize` with `true` in the rollup config:`sed -i 's/if (visualize)/if (true)/g' rollup.config.js`Then; run `yarn build-deps && yarn build-npm` to build the bundles.Then; open the generated visualization website at `dist/tf-layers.*.html`**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"[""This is caused by including gradients from tfjs-core's `dist` directory in the layers bundle. The gradients reference other tfjs-core files in `dist`; like `tensor.js`; and these are redundant with importing the `tfjs-core` bundle itself.To keep bundle size low; `register_all_gradients` is not actually included in the `tf-core*.js` bundles; so the import from `dist` is required. Should gradients be in their own bundle (which would depend on tfjs-core as an external dependency)? Is it possible to factor them out? If so; then tfjs-layers could include the gradients bundle instead of importing them from dist. The bundle would know that `tfjs-core` is external; and wouldn't include any extra copies of `Tensor` or other core classes / objects from `tfjs-core`.=====""]",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Layer API,API,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Incorrect Bundle Inclusion""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5182,Incorrect SideEffects settings in TFJS cause bad builds due to tree shaking,1,open,2021-06-06T16:20:20Z,2021-06-08T16:27:51Z,"TFJS changed behavior several times how to deal with tree-shaking and side-effects;  but it's still broken.For example; `tfjs-backend-webgl:package.json` has this:```json  ""sideEffects"": [    ""./dist/register_all_kernels.js"";    ""./dist/flags_webgl.js"";    ""./dist/base.js"";    ""./dist/index.js""  ]```What is the point of `sideEffects` pointing to JS files in `/dist/`? 1. If user is using prebuilt bundles of tfjs from `/dist`; it's irrelevant2. If user is trying to build `@tensorflow/tfjs-backend-webgl`; it will be **broken**3. If user is trying to build `@tensorflow/tfjs`; then this is relevant     since the meta-bundle imports from `@tensorflow/tfjs-backend-webgl/dist/index.js` which references `./dist`At minimum; sideEffects should point to TS files in `./src`; as well as JS `./dist`  Note that `tfjs-backend-webgl:package.json` is used only as an example; same bad behavior is present in all `tfjs` packages.  Proof that it's broken?  Building custom TFJS bundle and build works just fine; but then fails in runtime with.  ```logUncaught Error: Cannot evaluate flag 'CPU_HANDOFF_SIZE_THRESHOLD': no evaluation function found.    at Environment.evaluateFlag (environment.ts:138)```Why? Because entire import `./src/flags_webgl.ts` was dropped due to tree-shaking  and flags never got registered to start with.If I re-run build with bundler set to simply ignore any `sideEffects` fiels within TFJS;  it works just fine; but bundle is massive.On the other hand; even better solution is not to use `sideEffects` at all by moving flat code in those files into a method (e.g. `init()` and calling that method from single place in the parent code where needed. I don't see why should any code be immediately executed during import - that is just a bad practice.  Environment: TFJS 3.7.0 on Ubuntu 21.04",['cc @tafsiri due to previous work on sideEffects====='],Initialization Faliure,Build & Initialization Failure,Misconfiguration,WebGL,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Incorrect SideEffects settings""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5180,[webgpu]fall back to cpu or webgl,12,closed,2021-06-05T11:36:55Z,2021-06-15T05:55:10Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):yes; tfjs functional api model with custom layersself browserified tfjs-backend-webgpu github pull- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):chrome canary windows- TensorFlow.js installed from (npm or script link):link https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js at time of writing 3.7- TensorFlow.js version (use command below):3.7- Browser version:Version 93.0.4533.0 (Official Build) canary (64-Bit)**Describe the current behavior**model is running fine with webglif i change backend to webgpu on unsafe canary; following error shows up:Error: Kernel 'LogicalAnd' not registered for backend 'webgpu'**Describe the expected behavior**i know webgpu is at an alpha stage - but why doesn't it fall back to cpu or webgl for that part? am i missing something**Standalone code to reproduce the issue**i can provide the browserified version of the webgpu-backend; if needed.,"[""i also tried with tf.env().set('WEBGPU_CPU_FORWARD'; true); but no effect.=====""; ""@BenjaminWegener Thanks for trying the webgpu backend. Current; the supported kernels are limited in webgpu. You can find all supported kernels [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgpu/src/register_all_kernels.ts). `LogicalAnd` is not supported yet. But it's easier to be added. >i also tried with tf.env().set('WEBGPU_CPU_FORWARD'; true); but no effect.Setting `WEBGPU_CPU_FORWARD` true doesn't mean to fall back to cpu. It's just one of the conditions that it will run to cpu. Only if [all conditions](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgpu/src/backend_webgpu.ts#L837) are satisfied (it is based on performance consideration); it will go the cpu path. But all of these are based on that the corresponding kernel has been registered. > fall back to cpu or webgl.In our current design; we should register the kernel first. Then we can choose to fall back to cpu or use the backend implementation in the kernel config file. So the fix is to register the missed kernel for webgpu backend. =====""; 'Thank you for your answer. I need `tf.LogicalAnd` and `tf.Tile`. I will have to wait.====='; '@BenjaminWegener `tf.LogicalAnd` and `tf.Tile` are supported now in webgpu. Please let us know if you still meet any problems. Thanks.====='; 'Thank you very much; it really helps.If i run my model; the next Problem is`UnsortedSegmentSum`.====='; '...====='; ""@BenjaminWegener Do you mind to share more information about your model? Do you see any bad performance on webgl so that you want to try webgpu? Currently our main efforts are on performance . We want to make sure that webgpu really brings great performance when it's officially released.  But the supported ops may be not full. So I want to know the gap to enable your model. =====""; ""no problem. i am trying to implement a transformer like model in tfjs. just download or clone the repo an try it out. use chrome w/ webgpu .https://github.com/BenjaminWegener/transformer-tfjsthe model runs fine with ~6gig of GPU memory with webgl; i just wanted to try the speed of webgpu. so no; it isn't crucial to me; that it runs with webgpu (yet).thank you for your support-=====""; ""@BenjaminWegener Can you review your model and list all the needed ops so that we can know the overall gap in webgpu? It's not efficient for us to implement these ops one by one. If you can provide the overall status; we can evaluate our efforts and give you better support.=====""; 'I stripped everything unneccesary from the model. Looks like tf.layers.embedding uses `UnsortedSegmentSum`; which is the only OP my model is missing. But I cannot tell; if tf.layers.embedding uses another unsupported OP.====='; ""i replaced the embedding layer with a feedforward net. (dense-activation-dense). but if i use 'relu' activation like `tf.layers.dense({units: UNITS; activation: 'relu'}).apply(x)` i get the error that op `Step` isn't implemented. so i replace that with a custom gelu activation; which seems to be working. so for now i'm closing this. thank you for your help!=====""; ""@BenjaminWegener Glad to know that you work around this issue. We'd like to hear your experience on webgpu. Do you see any perf issue with your model on webgpu? Please feel free to file bugs if you see any problems :) =====""]",Reference Error,Crash,Unimplemented Operator,WebGPU,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5173,Issue Raspberry install tfjs-node 3.7.0 ; wrong ELF class: ELFCLASS64,18,closed,2021-06-04T13:46:57Z,2021-08-12T14:57:49Z,"When I install tfjs-node; and run it```bashWelcome to Node.js v14.16.0.Type "".help"" for more information.> const tf = require('@tensorflow/tfjs');undefined> require('@tensorflow/tfjs-node')node-pre-gyp info This Node instance does not support builds for Node-API version 8node-pre-gyp info This Node instance does not support builds for Node-API version 8Uncaught:Error: /home/pi/program/nanocptscenariorunner/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1122:18)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)>```When doing```bashnpm rebuild @tensorflow/tfjs-node ....gyp verb node dev dir /home/pi/.cache/node-gyp/14.16.0gyp verb `which` succeeded for `make` /usr/bin/makegyp info spawn makegyp info spawn args [ 'V=1'; 'BUILDTYPE=Release'; '-C'; 'build' ]../binding/tfjs_backend.cc: In function ‘TFE_TensorHandle* tfnodejs::CreateTFE_TensorHandleFromStringArray(napi_env; int64_t*; uint32_t; TF_DataType; napi_value)’:../binding/tfjs_backend.cc:196:64: error: ‘TF_TString’ was not declared in this scope                                          array_length * sizeof(TF_TString)));                                                                ^~~~~~~~~~../binding/tfjs_backend.cc:198:15: error: ‘t’ was not declared in this scope....```Nodejs: 14.13Raspbian lite strechRaspberry 3 armv7l ==> 32 bits","['@franckOL there is a mis-match between the binding (TF 2.4.1) and RPI TF binary (v1.4.0)We will try to build a new RPI TF binary (2.4.1). TF have dropped official support for RPI (no new releases); If you have manually built before; please share your steps. thanks====='; 'In fact I understand than `npm rebuild @tensorflow/tfjs-node` is the rebuilding process; but as you see I have another issue on that (`TF_TString` unknown symbol)====='; ""@pyu10055 @franckOL I'm running into the same issue in Raspberry p3I'm running tfjs-node v3.3.0 due to a regression bug with v3.7 as suggested by @pyu10055 https://github.com/tensorflow/tfjs/issues/5161#issuecomment-854972551;has this been resolved for v3.7?What to do to get it to run in RPi```> node index.jsnode:internal/modules/cjs/loader:1167  return process.dlopen(module; path.toNamespacedPath(filename));                 ^Error: /server/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (node:internal/modules/cjs/loader:1167:18)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at Module.require (node:internal/modules/cjs/loader:997:19)    at require (node:internal/modules/cjs/helpers:92:18)    at Object.<anonymous> (/server/node_modules/@tensorflow/tfjs-node/dist/index.js:60:16)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14) {  code: 'ERR_DLOPEN_FAILED'}```=====""; '@playground As you see a PR is done to fix that; already merged; just waiting for release 3.7.x====='; 'Thanks @franckOL; I was also checking with @pyu10055 if the other regression bug has been addressed so I can run v3.7.x.====='; 'Any updates on this?====='; 'cc @mattsoulanille ====='; 'Test today version 3.8.0 : always wrong ELF class: ELFCLASS64====='; 'V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.====='; '> V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.@playground dans rpi4 is 64 bits platform ?====='; '> > V3.8.0 works for me on rpi4 and Nvidia NX but broke for Mac.> > @playground dans rpi4 is 64 bits platform ?oh yes ==> Broadcom BCM2711; Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHzhttps://www.raspberrypi.org/products/raspberry-pi-4-model-b/specifications/but rpi3 64 bit tooQuad Core 1.2GHz Broadcom BCM2837 64bit CPUTry to clean everything and see.I work on rapsbian lite.====='; ""@franckOL rpi4 running ubuntu 32bit and  NX 64bit; I'm using SavedModel.=====""; 'Just try again; a clean env on raspberry 3 ; raspbian : ```Welcome to Node.js v14.16.0.Type "".help"" for more information.> const tf = require(\'@tensorflow/tfjs\')undefined> require(\'@tensorflow/tfjs-node\')node-pre-gyp info This Node instance does not support builds for Node-API version 8node-pre-gyp info This Node instance does not support builds for Node-API version 8Uncaught:Error: /tmp/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node: wrong ELF class: ELFCLASS64    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1122:18)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)```====='; '@franckOL I\'m running my app with  node v14.17.3 and in a docker container with node v16.3.0 and both loads up fine```platform  linuxStarted on 30002021-08-02 13:09:39.283490: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./model2021-08-02 13:09:40.358427: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }2021-08-02 13:09:40.358568: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./model2021-08-02 13:09:44.264451: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.2021-08-02 13:09:44.837456: W tensorflow/core/platform/profile_utils/cpu_utils.cc:118] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency2021-08-02 13:09:50.320029: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.321330: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.322138: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.330052: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.335342: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600 exceeds 10% of free system memory.2021-08-02 13:09:50.622648: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./model2021-08-02 13:09:53.412700: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 14129215 microseconds.loading time:  ./model; 25695.750468969345version:  { name: \'Demo model\'; version: \'1.0.0\' }./public/images/image.pngpredictions: 1 {  detectedBox: [ \'0.026\'; \'0.328\'; \'0.215\'; \'0.538\' ];  detectedClass: \'hard-hat\';  detectedScore: \'1.000\'}time took:  19462.35897707939``````FROM arm32v7/node:16.3.0RUN apt-get update -y && apt-get install -y apt-utils && apt-get install fswebcam -y && apt-get install mpg123 -y && apt-get install vim -yWORKDIR /serverCOPY . /serverRUN npm install -g npmRUN npm installVOLUME /demo_model_mms_helper_shared_volumeEXPOSE 3000CMD [ ""npm""; ""start"" ]```====='; '@franckOL could you please update if you are still facing same issue ?====='; '> @franckOL could you please update if you are still facing same issue ?I update my version nodejs form 14.17.1 to 14.17.3. And it is **OK**====='; 'Thank you ; closing this issue .====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5173"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5173"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,update tensorflow.so,Modifying dependency configuration,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Node-API Version Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/5172,Issue converting keras model to JavaScript and subsequent loading,2,open,2021-06-04T12:59:02Z,2021-08-31T04:54:57Z,"I am having difficulty loading a model that has been converted using the 'TensorFlow.js converter' . Any help would be appreciated!**System information**Google ColabP5jsTensorflow 2.5.0TensorFlow.js Converter v3.7.0**Describe the current behavior**- Unable to load a model that has been converted from 'keras_saved_model' into 'tfjs_layers_model' format using 'TensorFlow.js converter'. The following errors appear in console upon attempts to load via the P5.js editor & 'tf.loadLayersModel(MODEL_PATH)':initializers.js:131 The shape of the input tensor ([null;148;148;256]) does not match the expectation of layer conv2d_1: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;73;73;256]) does not match the expectation of layer conv2d_2: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;69;69;128]) does not match the expectation of layer conv2d_3: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;32;32;128]) does not match the expectation of layer conv2d_4: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;30;30;64]) does not match the expectation of layer conv2d_5: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;14;14;64]) does not match the expectation of layer conv2d_6: [null;150;150;3]initializers.js:131 The shape of the input tensor ([null;5;5;32]) does not match the expectation of layer conv2d_7: [null;150;150;3]util_base.js:154 Uncaught (in promise) Error: Based on the provided shape; [256]; the tensor should have 256 values but has 0    at Wv (util_base.js:154)    at Mw (engine.js:1086)    at Lw (tape.js:54)    at Ww (engine.js:1190)    at RM (container.js:211)    at container.js:196    at c (runtime.js:63)    at Generator._invoke (runtime.js:293)    at Generator.next (runtime.js:118)    at bv (runtime.js:747)- Saving the model as 'keras' format (model.h5) and converting into 'tfjs_layers_model' format does not resolve the issue- Manually editting the model.json file resolves the ""initializers.js:131..."" errors but not the ""util_base.js:154..."" error**Standalone code to reproduce the issue**https://colab.research.google.com/drive/1EhMHvteiQqPGvZKti6cvjGuDspDXzDXP?usp=sharing**Other info / logs**[model.zip](https://github.com/tensorflow/tfjs/files/6598599/model.zip)[modelConverted.zip](https://github.com/tensorflow/tfjs/files/6598601/modelConverted.zip)[modelDescription.zip](https://github.com/tensorflow/tfjs/files/6598625/modelDescription.zip)",['@josephzonghaozhu sorry for the delay ; were you able to run the model before converting ? ====='; 'Thanks for taking a look! I was able to run the model with no issues before converting; I added an example at the bottom of the colab ====='],Data & Model Error,Crash,Unknown,Operator,API,,,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,E
https://github.com/tensorflow/tfjs/issues/5166,typo in documentation,1,closed,2021-06-03T17:20:05Z,2021-06-03T18:21:55Z,"[https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#4](https://codelabs.developers.google.com/codelabs/tensorflowjs-nodejs-codelab/#4)I believe ""sparseCategoricalCrossentroy"" should be ""sparseCategoricalCrossentropy""This template is for miscellaneous issues not covered by the other issue categories.For questions on how to work with TensorFlow.js; or support for problems that are not verified bugs in TensorFlow.js; please go to [StackOverflow](https://stackoverflow.com/tags/tensorflow.js).",['This will be fixed and published accordingly. Thank you ====='],Document Error,Document Error,Confused Document,TF(CPU),Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5162,Tensorflow Js predict output zero for all classes,11,closed,2021-06-03T00:48:30Z,2021-06-28T19:26:15Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): built using expo SDK 39 on Ubuntu 20.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Any Android Phone- TensorFlow.js installed from (npm or script link): npm - TensorFlow.js version: @tensorflow/tfjs"": ""^2.6.0"";""@tensorflow/tfjs-react-native"": ""^0.4.0"";- CUDA/cuDNN version: Keras Model built on CUDA 11.2 in a GPU environment. The model was then converted to layers as well  graph models**Describe the problem**Tensorflow Predict out put always zero; irrespective whatever model we choose for any image**Provide the exact sequence of commands / steps that you executed before running into the problem**The following function was used to load the modeluseEffect(() => { async function loadModel(){  const tfReady = await tf.ready();  const modelJson = await require(""./assets/model/model.json"");  const modelWeight = await require(""./assets/model/group1-shard.bin"");   const InsectDetector = await       tf.loadLayersModel(bundleResourceIO(modelJson;modelWeight));    setInsectDetector(InsectDetector)  } } loadModel()}; []); The following code was used to load an image for classificaionconst imgBuffer = tf.util.encodeString(Imgb64; 'base64').buffer;  const raw = new Uint8Array(imgBuffer);  imageTensor = decodeJpeg(raw);  const imageTensoresized =  tf.image.resizeBilinear(imageTensor; [224; 224]);  const offset = tf.scalar(255.0);  const normalized = (imageTensoresized).div(offset);  let Result = InsectDetector.predict(normalized.reshape([1;224;224;3]));  console.log(""[+] Insect Class Prediction Result"";Result)  let PrData = Result.dataSync()  let InsectClass = Result.argMax().dataSync()[0]  console.log(""[+] Insect Class Prediction done Insect class "";PrData;InsectClass)**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.The out put of predict from console.log{ kept: false;      isDisposedInternal: false;      shape: [ 102 ];      dtype: 'float32';      size: 102;      strides: [];      dataId: {};      id: 1162;      rankType: '1';      scopeId: 848 };The ouput of .dataSync() fo 102 classes '0': 0;      '1': 0;      '2': 0;      '3': 0;      '4': 0;      '5': 0;      '6': 0;      '7': 0;      '8': 0;      '9': 0;      '10': 0;      '11': 0;      '12': 0;      '13': 0;      '14': 0;      '15': 0;      '16': 0;      '17': 0;      '18': 0;      '19': 0;      '20': 0;      '21': 0;      '22': 0;      '23': 0;      '24': 0;      '25': 0;      '26': 0;      '27': 0;      '28': 0;      '29': 0;      '30': 0;      '31': 0;      '32': 0;      '33': 0;      '34': 0;      '35': 0;      '36': 0;      '37': 0;      '38': 0;      '39': 0;      '40': 0;      '41': 0;      '42': 0;      '43': 0;      '44': 0;      '45': 0;      '46': 0;      '47': 0;      '48': 0;      '49': 0;      '50': 0;      '51': 0;      '52': 0;      '53': 0;      '54': 0;      '55': 0;      '56': 0;      '57': 0;      '58': 0;      '59': 0;      '60': 0;      '61': 0;      '62': 0;      '63': 0;      '64': 0;      '65': 0;      '66': 0;      '67': 0;      '68': 0;      '69': 0;      '70': 0;      '71': 0;      '72': 0;      '73': 0;      '74': 0;      '75': 0;      '76': 0;      '77': 0;      '78': 0;      '79': 0;      '80': 0;      '81': 0;      '82': 0;      '83': 0;      '84': 0;      '85': 0;      '86': 0;      '87': 0;      '88': 0;      '89': 0;      '90': 0;      '91': 0;      '92': 0;      '93': 0;      '94': 0;      '95': 0;      '96': 0;      '97': 0;      '98': 0;      '99': 0;      '100': 0;      '101': 0 };","['@z2043947 can you verify if what are the values for the input of the model looks fine:`normalized .print()`And ensure the model requires normalization result between [0.0; 1.0]====='; 'Thanks PingSo the value has to be between 0.0 and 1.0? Cannot be 0 or greater than 1?The attached is the converted tensor from the image.Tensor        [[[218; 214; 233];          [153; 149; 168];          [154; 153; 171];          ...;          [209; 214; 234];          [204; 207; 228];          [208; 211; 232]];             [[230; 226; 245];          [162; 158; 177];          [157; 156; 174];          ...;          [145; 150; 170];          [142; 145; 166];          [147; 150; 171]];             [[221; 216; 234];          [152; 147; 165];          [149; 145; 164];          ...;          [135; 140; 160];          [131; 134; 155];          [137; 140; 161]];             ...         [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]];             [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]];             [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]]]After dividing by scalar 255 it is all zeros. That does not look right?Tensor        [[[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];             [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];             [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];             ...         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];             [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];             [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]]]regards;SankaranSankaran Iyer+61 431 499 867Sent from Mail for Windows 10From: Ping YuSent: Friday; 4 June 2021 3:33 AMTo: tensorflow/tfjsCc: z2043947; MentionSubject: Re: [tensorflow/tfjs] Tensorflow Js predict output zero for allclasses (#5162)@z2043947 can you verify if what are the values for the input of the model looks fine:normalized .print()And ensure the model requires normalization result between [0.0; 1.0]—You are receiving this because you were mentioned.Reply to this email directly; view it on GitHub; or unsubscribe.====='; ""> After dividing by scalar 255 it is all zeros. That does not look right?```jsconst normalized = imageTensoresized.cast('float32').div(255);```=====""; ""> > After dividing by scalar 255 it is all zeros. That does not look right?> > ```js> const normalized = imageTensoresized.cast('float32').div(255);> ```It is not dividing by a scalar that results in zeros. It is the following statementconst imageTensoresized = tf.image.resizeBilinear(imageTensor; [224; 224]); Same problem with resizeNearestNeighbor as well.  I am now exploring alternate ways of resizing the image.=====""; ""> It is not dividing by a scalar that results in zeros. It is the following statement> `const imageTensoresized = tf.image.resizeBilinear(imageTensor; [224; 224]);`You're using `imageTensor = decodeJpeg(raw);` - what is `decodeJpeg`?  And more importantly; what is the shape of the resulting `imageTensor`? It should be `[1; height; width; 3]`=====""; '> > It is not dividing by a scalar that results in zeros. It is the following statement> > `const imageTensoresized = tf.image.resizeBilinear(imageTensor; [224; 224]);`> > You\'re using `imageTensor = decodeJpeg(raw);` - what is `decodeJpeg`?> And more importantly; what is the shape of the resulting `imageTensor`? It should be `[1; height; width; 3]`import {fetch; bundleResourceIO;decodeJpeg} from ""@tensorflow/tfjs-react-native"";decodeJpeg is provided by tensorflow/tfjs-react-native. It returns  tensor3d(buffer; [height; width; channels]) . The tensor seems to be OK. The following is imageTensor;print() added after decodeJpegTensor        [[[218; 214; 233];          [153; 149; 168];          [154; 153; 171];          ...;          [209; 214; 234];          [204; 207; 228];          [208; 211; 232]];         [[230; 226; 245];          [162; 158; 177];          [157; 156; 174];          ...;          [145; 150; 170];          [142; 145; 166];          [147; 150; 171]];         [[221; 216; 234];          [152; 147; 165];          [149; 145; 164];          ...;          [135; 140; 160];          [131; 134; 155];          [137; 140; 161]];         ...         [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]];         [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]];         [[243; 243; 243];          [243; 243; 243];          [243; 243; 243];          ...;          [255; 255; 255];          [255; 255; 255];          [255; 255; 255]]]Tensor becomes zero after executing this statement const imageTensoresized = tf.image.resizeBilinear(imageTensor; [224; 224]);imageTensoresized.print()  added after the statement results inTensor        [[[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];         ...         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]];         [[0; 0; 0];          [0; 0; 0];          [0; 0; 0];          ...;          [0; 0; 0];          [0; 0; 0];          [0; 0; 0]]]====='; 'The shape of imageTensoresized is [224;224;3] as required even though it is all zero ====='; 'I have now ensured that a feed an input tensor between 0 and 1 of shape 1;224;224;3 to the model without using resizeBilinear.  The tensor fed to the mode is as follows:Tensor    [[[[0.654902 ; 0.8039216; 0.5176471];       [0.6470588; 0.7960784; 0.509804 ];       [0.6431373; 0.7921569; 0.5058824];       ...;       [0.5568628; 0.7568628; 0.2588235];       [0.5568628; 0.7568628; 0.2588235];       [0.4901961; 0.6901961; 0.1921569]];      [[0.6588235; 0.8078431; 0.5215687];       [0.6470588; 0.7960784; 0.509804 ];       [0.6352941; 0.7843137; 0.4980392];       ...;       [0.5647059; 0.7647059; 0.2666667];       [0.5529412; 0.7529412; 0.254902 ];       [0.4823529; 0.682353 ; 0.1843137]];      [[0.6627451; 0.8117647; 0.5254902];       [0.6392157; 0.7882353; 0.5019608];       [0.6196079; 0.7686275; 0.4823529];       ...;       [0.5568628; 0.7647059; 0.254902 ];       [0.5333334; 0.7411765; 0.2313726];       [0.454902 ; 0.6627451; 0.1529412]];      ...      [[0.3764706; 0.572549 ; 0.145098 ];       [0.3882353; 0.5843138; 0.1568628];       [0.3921569; 0.5882353; 0.1529412];       ...;       [0.5450981; 0.7372549; 0.3333333];       [0.5450981; 0.7372549; 0.3333333];       [0.5450981; 0.7372549; 0.3333333]];      [[0.3764706; 0.572549 ; 0.145098 ];       [0.3882353; 0.5843138; 0.1568628];       [0.3921569; 0.5882353; 0.1529412];       ...;       [0.5450981; 0.7333333; 0.3411765];       [0.5450981; 0.7333333; 0.3411765];       [0.5450981; 0.7333333; 0.3411765]];      [[0.3803922; 0.5764706; 0.1490196];       [0.3882353; 0.5843138; 0.1568628];       [0.3921569; 0.5882353; 0.1529412];       ...;       [0.5450981; 0.7333333; 0.3411765];       [0.5450981; 0.7333333; 0.3411765];       [0.5450981; 0.7333333; 0.3411765]]]]But I still get predictions of zero.  I have converted the .h5 file and tried with both layer and graph models. I don know what the issue is now.Insect Class Prediction Result Tensor {  ""dataId"": Object {};  ""dtype"": ""float32"";  ""id"": 297;  ""isDisposedInternal"": false;  ""kept"": false;  ""rankType"": ""2"";  ""scopeId"": 1;  ""shape"": Array [    1;    102;  ];  ""size"": 102;  ""strides"": Array [    102;  ];} 0.78978 0.14285714285714285INFO13:30[+] Insect Class Prediction done Insect class  Float32Array [  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;  0;]====='; ""I now verified that I get correct predictions when I turn on remote debugging using the chrome debugger tools. The issue is only in the app's JS on the phone!=====""; 'Thank for you for confirming ; please mention@ to reopen if you still see issue ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5162"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5162"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,D.1
https://github.com/tensorflow/tfjs/issues/5158,Do not get model inferences; only thing that comes on the screen is laggy camera screen when using tensorcamera that also crushes after 1-2 minutes,6,closed,2021-06-02T15:00:51Z,2021-07-29T06:20:00Z,I do not get any warnings or errors - only that async storage has been renamed in new expo; so I have older one installed not to cause an issue (as new one crushed the code)all code I use is here https://stackoverflow.com/questions/67790431/tfjs-react-native-for-real-time-object-detection-expo-managed-workflow,"['can you please check this issue https://github.com/tensorflow/tfjs/issues/3808 if it is related ? Thank you ====='; '@rthadur thanks a lot for your answer! I tried that; but the same happens + after one minute expo go gets closed by itself. Now I tried this example code: https://github.com/tafsiri/tfjs-expo-managed-example and again the same happened; even after reinstalling Expo Go on my android device. Otherwise my expo works just fine. Is it possible; that all of this is because I try to run the code on my android 10 ?====='; 'tried on Android 11; same problem. only difference is that expo does not crush after 1-2 minutes====='; 'please check a related issue here https://github.com/tensorflow/tfjs/issues/4902====='; 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available; and we will reopen the issue. Thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5158"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5158"">No</a>=====']",Memory Leak,Poor Performance,Cross-platform App Framework Incompatibility,Mobile,Platform,change dependency version,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.2.1,D.3
https://github.com/tensorflow/tfjs/issues/5153,How to build tf.min.js from source code?,1,open,2021-06-01T01:16:30Z,2021-06-01T01:38:32Z,**System information**- OS Platform and Distribution: Linux Ubuntu 18.04 (x86_64) on dual Intel Xeon 5600; node 11.1.0; Linux 5.4 kernel; gcc 7.5.0- TensorFlow.js installed from (npm or script link): github source code- TensorFlow.js version: 3.6.1- CUDA/cuDNN version: Cuda 10.0; Nvidia Geforce GTX 1060**Describe the problem**Sorry if I'm missing something trivial. I need to build dist/tf.min.js from source code since my CPU does not support AVX instruction set. I don't know if should use bazel; or where are the detailed build instructions. When I runnpm run releasethe script gets blocked asking for a github user~~~ Creating new release branch tfjs_3.6.1 ~~~Username for 'https://github.com':**Provide the exact sequence of commands / steps that you executed before running into the problem**npm run releaseon options; Release unit 0; Phase 0; New version 3.6.1 and when trying to give a Username for github process is not responding anymore to keyboard input,"['I have the same issue as https://github.com/tensorflow/tfjs/issues/2631 - changing tfjs-node to ""@tensorflow/tfjs"": ""1.2.2"" on package.json makes simple examples to work; but it is possible to build newer versions with support for not AVX-enabled CPUs?=====']",Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Build Blocks""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/5151,TensorflowJS Issue : Looking for previous version of cusolver64_10,3,closed,2021-05-31T19:11:59Z,2021-07-30T14:38:34Z,System informationOS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10TensorFlowJS version: 3.6.1GCC/Compiler version (if compiling from source):CUDA/cuDNN version: 11.3GPU model and memory: RTX 2060 MAXQDescribe the problemGPU build is looking for cusolver64_10 instead of latest cusolver64_11 version,"['@fireholster I guess this needs to be fixed in TF Core firstly ; see here for similar issue here https://github.com/tensorflow/tensorflow/issues/44291 .cc @pyu10055 @lina128 ====='; 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available; and we will reopen the issue. Thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5151"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5151"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,WebGPU,Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Version Mismatch""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5136,Cannot read property 'children' of undefined with using Tensorflow Object Detection API model in tfjs,1,open,2021-05-28T01:00:33Z,2021-06-04T15:37:54Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm install @tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: Chrome 90 on Windows 10- Tensorflow.js Converter Version: 3.6.0Hyperlinks are capitalized to prevent confusion(because the color settings(and blue filter) on my monitor make it hard to see the difference between them)**Describe the current behavior**I used transfer learning using a pretrained model from the Tensorflow Object Detection API; which i converted to tensorflow-js using the tensorflow-converter API in python. View the ipynb notebook [HERE](https://colab.research.google.com/drive/13NLcyDDHeenFDymqfiGMGeF4o_p1l-Wb). I then followed @hugozanini's [REPO](https://github.com/hugozanini/TFJS-object-detection) which had a template for using your tensorflow object detection models in javascript;  I copied the index.json file and put it into a sandbox; replacing his model.json file with mine. The repo which contains it can be found [here](https://github.com/dewball345/ear-model-test)**Describe the expected behavior**I was expecting the model to work and the program to run normally; like it did in this [DEMO](https://glitch.com/edit/#!/real-time-object-detection). Instead; I got ```index.js:1437 TypeError: Cannot read property 'children' of undefined    at operation_mapper.js:409    at Array.forEach (<anonymous>)    at operation_mapper.js:403    at Array.forEach (<anonymous>)    at OperationMapper.mapFunction (operation_mapper.js:401)    at operation_mapper.js:163    at Array.reduce (<anonymous>)    at OperationMapper.transformGraph (operation_mapper.js:162)    at GraphModel.loadSync (graph_model.js:159)    at GraphModel._callee$ (graph_model.js:119)    at tryCatch (runtime.js:62)    at Generator.invoke [as _invoke] (runtime.js:288)    at Generator.prototype.<computed> [as next] (runtime.js:114)    at asyncGeneratorStep (asyncToGenerator.js:3)    at _next (asyncToGenerator.js:25)```**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.[HERE](https://colab.research.google.com/drive/13NLcyDDHeenFDymqfiGMGeF4o_p1l-Wb) is the colab notebook I used to train my model[HERE](https://codesandbox.io/s/purple-dawn-n2nbh?file=/src/index.js) is the sandbox where I tried to use the tensorflow.js model **Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Error Message:```index.js:1437 TypeError: Cannot read property 'children' of undefined    at operation_mapper.js:409    at Array.forEach (<anonymous>)    at operation_mapper.js:403    at Array.forEach (<anonymous>)    at OperationMapper.mapFunction (operation_mapper.js:401)    at operation_mapper.js:163    at Array.reduce (<anonymous>)    at OperationMapper.transformGraph (operation_mapper.js:162)    at GraphModel.loadSync (graph_model.js:159)    at GraphModel._callee$ (graph_model.js:119)    at tryCatch (runtime.js:62)    at Generator.invoke [as _invoke] (runtime.js:288)    at Generator.prototype.<computed> [as next] (runtime.js:114)    at asyncGeneratorStep (asyncToGenerator.js:3)    at _next (asyncToGenerator.js:25)```,['Oh; and btw this is using efficiendet.====='],Reference Error,Crash,Unknown,Model API,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,E
https://github.com/tensorflow/tfjs/issues/5123,[Codelab]: Making Predictions from 2D Data,1,closed,2021-05-25T09:40:22Z,2021-05-25T19:14:44Z,"The section [Define the model architecture](https://codelabs.developers.google.com/codelabs/tfjs-training-regression/index.html#3) is not able to complete due to code error.In the HTML code example `<script src=""script.js""></script>` appears before `<body>`. Since the JavaScript code is executed before the HTML body tag has loaded; document.body is null.A possible solution is to move the script at the end of the body tag; once it’s been loaded:```<body><script src=""script.js""></script></body>```",['This is fixed ; please verify.====='],Document Error,Document Error,Confused Document,WebGL,Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5113,Dead link in tutorial,1,closed,2021-05-24T20:31:06Z,2021-05-25T18:56:01Z,This page:https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0This link is dead:This codelab will not go over the theory behind audio recognition models. If you are curious about that; **check out this tutorial**.,['This is fixed now; please verify.====='],Document Error,Document Error,Confused Document,WebGL,Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5111,[Codelab]: TensorFlowJS Comment Spam Detection,2,closed,2021-05-23T05:58:32Z,2021-06-03T21:27:12Z,[Found a typo in Section 8 Serve the machine learning model](https://codelabs.developers.google.com/codelabs/tensorflowjs-comment-spam-detection?continue=https%3A%2F%2Fdevelopers.google.com%2Flearn%2Fpathways%2Fget-started-text-classification-web%3Fhl%3Den%23codelab-https%3A%2F%2Fcodelabs.developers.google.com%2Fcodelabs%2Ftensorflowjs-comment-spam-detection#7)The current sentence is like this. **First; if you have not done so already; unzip the files you downloaded for the model downloaded at the start of this codelab.** The word downloaded is repeating and it bit confusing. It will be better if we remove the second occurrence of word downloaded so the sentence will be more understandable like this.**First; if you have not done so already; unzip the files you downloaded for the model  at the start of this codelab**,['Thanks for the feedback on this one. I shall update that and push it out shortly. Apologies for the delay I was out of office.====='; 'Changes made they should be pushed live shortly (next day or two).====='],Document Error,Document Error,Confused Document,WebGL,Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5110,Backend WASM does not implement kernel op `mod`,3,open,2021-05-22T15:45:41Z,2021-11-05T17:26:30Z,As subject line says; trivial math operation `tf.mod` is missing from `tfjs-backend-wasm`...```logindex.js:646 Error: Kernel 'Mod' not registered for backend 'wasm'    at Engine.runKernel (engine.ts:540)    at mod_ (mod.ts:63)    at mod__op (operation.ts:51)    at executeOp (arithmetic_executor.ts:45)    at operation_executor.ts:61    at engine.ts:467    at Engine.scopedRun (engine.ts:478)    at Engine.tidy (engine.ts:465)    at tidy (globals.ts:192)    at operation_executor.ts:60```Issue found during porting and optimizing MobileNet-v3 with CenterNet object detection model: <https://github.com/vladmandic/mb3-centernet>Environment: TFJS 3.6.1 with WASM backend with Edge/Chromium 90 on Windows 10,"['I can work on this ====='; 'cc: @rthadur @pyu10055 ====='; ""workaround until #5317 is resolved:```js    const kernelMod = {      kernelName: 'Mod';      backendName: 'wasm';      kernelFunc: (op) => tf.tidy(() => tf.sub(op.inputs.a; tf.mul(tf.div(op.inputs.a; op.inputs.b); op.inputs.b)));    };    tf.registerKernel(kernelMod);```=====""]",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/5093,why is @tensorflow/tfjs-backend-wasm still in alpha？,1,closed,2021-05-19T07:18:06Z,2021-05-24T18:41:38Z,why is @tensorflow/tfjs-backend-wasm still in alpha？what problems is facing？Can @tensorflow/tfjs-backend-wasm be used in project online？Thank you,['Thank you for the report! Looks like the README file is outdated. It is no longer in alpha and it is definitely can be used in online projects. I will update it.The only thing to note is that not all our pre-trained models support the WASM backend. I will update the list in the README file too. Thanks!====='],Document Error,Document Error,Confused Document,Wasm,Backend,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/5092,moveData doesn't support read data asynchronously,1,open,2021-05-19T04:37:07Z,2021-05-20T05:26:08Z,When we switch backend; for some global tensors; it needs to call [moveData](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine.ts#L422) to move data from the original backend to the current backend by using [readSync](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/engine.ts#L425). However; for webgpu backend; it doesn't support read data back synchronously from GPU to CPU. So if the data is already in GPU; and we switch backend from webgpu to other backends. The readSync error will be thrown.We should provide a solution to support moving data asynchronously.,"[""@pyu10055 @lina128 @jinjingforever Let's use this issue to track the dataSync problem in moveData; which happens when we switch backend from webgpu to others.=====""]",Initialization Faliure,Build & Initialization Failure,Unimplemented Operator,WebGPU,Backend,add support for operator,Add unsupported operator,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",C,A.1
https://github.com/tensorflow/tfjs/issues/5084,M1 Chip,1,closed,2021-05-18T16:14:52Z,2021-06-28T17:45:00Z,<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using):- Are you willing to contribute it (Yes/No):**Describe the feature and the current behavior/state.****Will this change the current api? How?****Who will benefit with this feature?****Any Other info.**,['the library no support M1 Apple chip; but is there a plan to support it?====='],Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Feature Request for Support""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.4,D.1
https://github.com/tensorflow/tfjs/issues/5080,CI failure: Uncaught ReferenceError: require is not defined thrown,1,closed,2021-05-17T17:50:35Z,2021-05-18T22:29:59Z,CI failure at test: computation in worker cpu {} tensor in worker FAILEDThrow: Uncaught ReferenceError: require is not defined thrown,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5080"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5080"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,add import,Fix import confusion in program,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5071,Audio is glitching/distortion using playRawAudio,8,closed,2021-05-14T01:43:08Z,2021-06-11T22:01:25Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows10- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.6.0- speech-command version: 0.5.4- Browser version: Version 90.0.4430.212 (Official Build) (x86_64)**Describe the current behavior**I'm using basic code from [the example in README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) and the audio doesn't sound right. You can hear glitching/distortion in the audio**Describe the expected behavior**Expected to have audio without glitching/distortion**Standalone code to reproduce the issue**[Example in Codesandbox](https://codesandbox.io/s/tfjs-play-raw-audio-v0y12?file=/src/App.tsx:1100-1112),"[""@lina128 I tried [this solution](https://stackoverflow.com/a/38555027/9340882); but it didn't work..=====""; '<!--/* Font Definitions */@font-face\t{font-family:""Cambria Math"";\tpanose-1:2 4 5 3 5 4 6 3 2 4;}@font-face\t{font-family:Calibri;\tpanose-1:2 15 5 2 2 2 4 3 2 4;}/* Style Definitions */p.MsoNormal; li.MsoNormal; div.MsoNormal\t{margin:0in;\tfont-size:11.0pt;\tfont-family:""Calibri"";sans-serif;}a:link; span.MsoHyperlink\t{mso-style-priority:99;\tcolor:blue;\ttext-decoration:underline;}.MsoChpDefault\t{mso-style-type:export-only;}@page WordSection1\t{size:8.5in 11.0in;\tmargin:1.0in 1.0in 1.0in 1.0in;}div.WordSection1\t{page:WordSection1;}-->I would sat stop but its to late now\xa0Sent from Mail for Windows 10\xa0From: adotnusiyanSent: Thursday; May 20; 2021 11:54 PMTo: tensorflow/tfjsCc: SubscribedSubject: Re: [tensorflow/tfjs] Audio is glitching/distortion using playRawAudio ***@***.***I tried this solution; but it didn\'t work..—You are receiving this because you are subscribed to this thread.Reply to this email directly; view it on GitHub; or unsubscribe.\xa0====='; '@lina128 Is there any updates on this issue? Thanks ====='; 'Hi; I took a look at this; not sure why audio glitches. This is not a typical use of the API and our model seems run well. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5071"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5071"">No</a>====='; ""@lina128 What's the typical use?I did not do any custom code. It's what you suggested [here](https://github.com/tensorflow/tfjs/issues/4736#issuecomment-802300190). And just like what's written in [README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) =====""; ""I mean playing back the raw audio is not the main purpose of this API. As long as the model is working well; there's nothing to fix.=====""; '@lina128 If it\'s not the ""main"" purpose; it doesn\'t mean there\'s ""nothing"" to fix. Maybe it should have a low priority; but not ""closed""! =====']",side effect,Crash,Unknown,Model API,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A,E
https://github.com/tensorflow/tfjs/issues/5063,React Native with TypeScript tfjs-models/universal-sentence-encoder error on model load,1,open,2021-05-12T16:19:41Z,2021-05-14T14:03:23Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): iOS 14.5- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: n/a- Tensorflow.js Converter Version: @tensorflow/tfjs-react-native@0.5.0**Describe the current behavior**I'm trying to use tfjs-models/universal-sentence-encoder within a React Native app by following these instructions. However; I get the following error when I try to load the model:`TypeError: undefined is not an object (evaluating '_universalSentenceEncoder.default.load'`**Describe the expected behavior**It should load the use model; compute embeddings; and print them to the console.**Standalone code to reproduce the issue**```import React; { useEffect; useState } from 'react';require('@tensorflow/tfjs');const use = require('@tensorflow-models/universal-sentence-encoder');export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            'Hello.';            'How are you?'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(err);      }    };  }; []);````","['This was fixed via some code changes and downgrading @tensorflow/tfjs to 3.0.0. See [this post](https://stackoverflow.com/questions/67507110/react-native-with-typescript-tfjs-models-universal-sentence-encoder-error-on-mod/67514193#67514193) for details on change. Code changed to the following:```import React; { useEffect; useState } from \'react\';import ""@tensorflow/tfjs-react-native"";import * as tf from \'@tensorflow/tfjs\';import * as use from \'@tensorflow-models/universal-sentence-encoder\';export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            await tf.ready();            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            \'Hello.\';            \'How are you?\'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(`ERROR: ${err}`);      }    };  }; []);```I am; however; still only able to produce valid embeddings when running the code on an emulator. Running on physical iOS device produces an embeddings vector full of NaNs:`[javascript] Tensor    [[NaN; NaN; NaN; ...; NaN; NaN; NaN];     [NaN; NaN; NaN; ...; NaN; NaN; NaN]]`On an iOS emulator; the vector is full of valid floating point values.iOS Emulator: iPhone 11; iOS 14.5Physical iPhone: iPhone 11; iOS 14.5.1=====']",Reference Error,Crash,Inconsistent Modules,Mobile,Platform,change framework version,change framework version,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5063,React Native with TypeScript tfjs-models/universal-sentence-encoder error on model load,1,open,2021-05-12T16:19:41Z,2021-05-14T14:03:23Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): iOS 14.5- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: n/a- Tensorflow.js Converter Version: @tensorflow/tfjs-react-native@0.5.0**Describe the current behavior**I'm trying to use tfjs-models/universal-sentence-encoder within a React Native app by following these instructions. However; I get the following error when I try to load the model:`TypeError: undefined is not an object (evaluating '_universalSentenceEncoder.default.load'`**Describe the expected behavior**It should load the use model; compute embeddings; and print them to the console.**Standalone code to reproduce the issue**```import React; { useEffect; useState } from 'react';require('@tensorflow/tfjs');const use = require('@tensorflow-models/universal-sentence-encoder');export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            'Hello.';            'How are you?'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(err);      }    };  }; []);````","['This was fixed via some code changes and downgrading @tensorflow/tfjs to 3.0.0. See [this post](https://stackoverflow.com/questions/67507110/react-native-with-typescript-tfjs-models-universal-sentence-encoder-error-on-mod/67514193#67514193) for details on change. Code changed to the following:```import React; { useEffect; useState } from \'react\';import ""@tensorflow/tfjs-react-native"";import * as tf from \'@tensorflow/tfjs\';import * as use from \'@tensorflow-models/universal-sentence-encoder\';export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            await tf.ready();            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            \'Hello.\';            \'How are you?\'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(`ERROR: ${err}`);      }    };  }; []);```I am; however; still only able to produce valid embeddings when running the code on an emulator. Running on physical iOS device produces an embeddings vector full of NaNs:`[javascript] Tensor    [[NaN; NaN; NaN; ...; NaN; NaN; NaN];     [NaN; NaN; NaN; ...; NaN; NaN; NaN]]`On an iOS emulator; the vector is full of valid floating point values.iOS Emulator: iPhone 11; iOS 14.5Physical iPhone: iPhone 11; iOS 14.5.1=====']",Reference Error,Crash,Inconsistent Modules,Mobile,Platform,change framework version,change framework version,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.3.1,A.2
https://github.com/tensorflow/tfjs/issues/5063,React Native with TypeScript tfjs-models/universal-sentence-encoder error on model load,1,open,2021-05-12T16:19:41Z,2021-05-14T14:03:23Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): iOS 14.5- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: n/a- Tensorflow.js Converter Version: @tensorflow/tfjs-react-native@0.5.0**Describe the current behavior**I'm trying to use tfjs-models/universal-sentence-encoder within a React Native app by following these instructions. However; I get the following error when I try to load the model:`TypeError: undefined is not an object (evaluating '_universalSentenceEncoder.default.load'`**Describe the expected behavior**It should load the use model; compute embeddings; and print them to the console.**Standalone code to reproduce the issue**```import React; { useEffect; useState } from 'react';require('@tensorflow/tfjs');const use = require('@tensorflow-models/universal-sentence-encoder');export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            'Hello.';            'How are you?'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(err);      }    };  }; []);````","['This was fixed via some code changes and downgrading @tensorflow/tfjs to 3.0.0. See [this post](https://stackoverflow.com/questions/67507110/react-native-with-typescript-tfjs-models-universal-sentence-encoder-error-on-mod/67514193#67514193) for details on change. Code changed to the following:```import React; { useEffect; useState } from \'react\';import ""@tensorflow/tfjs-react-native"";import * as tf from \'@tensorflow/tfjs\';import * as use from \'@tensorflow-models/universal-sentence-encoder\';export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            await tf.ready();            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            \'Hello.\';            \'How are you?\'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(`ERROR: ${err}`);      }    };  }; []);```I am; however; still only able to produce valid embeddings when running the code on an emulator. Running on physical iOS device produces an embeddings vector full of NaNs:`[javascript] Tensor    [[NaN; NaN; NaN; ...; NaN; NaN; NaN];     [NaN; NaN; NaN; ...; NaN; NaN; NaN]]`On an iOS emulator; the vector is full of valid floating point values.iOS Emulator: iPhone 11; iOS 14.5Physical iPhone: iPhone 11; iOS 14.5.1=====']",Regression,Poor Performance,Inconsistent Modules,Mobile,Platform,change framework version,change framework version,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5063,React Native with TypeScript tfjs-models/universal-sentence-encoder error on model load,1,open,2021-05-12T16:19:41Z,2021-05-14T14:03:23Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): iOS 14.5- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 12- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: n/a- Tensorflow.js Converter Version: @tensorflow/tfjs-react-native@0.5.0**Describe the current behavior**I'm trying to use tfjs-models/universal-sentence-encoder within a React Native app by following these instructions. However; I get the following error when I try to load the model:`TypeError: undefined is not an object (evaluating '_universalSentenceEncoder.default.load'`**Describe the expected behavior**It should load the use model; compute embeddings; and print them to the console.**Standalone code to reproduce the issue**```import React; { useEffect; useState } from 'react';require('@tensorflow/tfjs');const use = require('@tensorflow-models/universal-sentence-encoder');export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            'Hello.';            'How are you?'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(err);      }    };  }; []);````","['This was fixed via some code changes and downgrading @tensorflow/tfjs to 3.0.0. See [this post](https://stackoverflow.com/questions/67507110/react-native-with-typescript-tfjs-models-universal-sentence-encoder-error-on-mod/67514193#67514193) for details on change. Code changed to the following:```import React; { useEffect; useState } from \'react\';import ""@tensorflow/tfjs-react-native"";import * as tf from \'@tensorflow/tfjs\';import * as use from \'@tensorflow-models/universal-sentence-encoder\';export default function App() {  useEffect(() => {    console.log(""App is starting..."")        const init = async () => {      // initialize state variables       // console.log(""App is initializing services..."")            await tf.ready();            // Load the model.      try {        use.load().then((model: any) => {          // Embed an array of sentences.          const sentences = [            \'Hello.\';            \'How are you?\'          ];          model.embed(sentences).then((embeddings: any) => {            // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.            // So in this example `embeddings` has the shape [2; 512].            embeddings.print(true /* verbose */);          });        });      }      catch (err) {        console.log(`ERROR: ${err}`);      }    };  }; []);```I am; however; still only able to produce valid embeddings when running the code on an emulator. Running on physical iOS device produces an embeddings vector full of NaNs:`[javascript] Tensor    [[NaN; NaN; NaN; ...; NaN; NaN; NaN];     [NaN; NaN; NaN; ...; NaN; NaN; NaN]]`On an iOS emulator; the vector is full of valid floating point values.iOS Emulator: iPhone 11; iOS 14.5Physical iPhone: iPhone 11; iOS 14.5.1=====']",Regression,Poor Performance,Inconsistent Modules,Mobile,Platform,change framework version,change framework version,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.3.1,A.2
https://github.com/tensorflow/tfjs/issues/5054,Too many duplicated code in all backends,2,open,2021-05-08T06:58:06Z,2021-05-12T15:28:54Z,There are too many duplicated codes in all backends. For example:1. Validation code. For the parameter validation; we can do it in the front end instead of doing it in all backends. 2. For some ops; they are using *ImplCPU for all backends. 3.  For some ops; they don't need a backend specific kernel. For example; `Identity`; `ExpandDims`.4. For some common optimizations; for example; in some situations; this operation is just return an Identity of itself or reshape. All of above situations; maybe we can put them in the front end (`fjs-core/src/ops/`) instead of making a copy in all backends.Take #5025 as an example;  it seems that `SparseSegmentMean` is using a cpu implementation. I suppose it will be same for all backends although only cpu/webgl was covered in that PR.,['@pyu10055 @lina128 Any thoughts on this?====='; '@qjia7 Thank you for raising this issue. The backend kernel is designed this way to allow tree shaking and custom bundle. We can discuss more in our weekly sync.====='],Build & Install Failure,Build & Initialization Failure,Inconsistent Modules,WebGL,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": """"
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",C,A.2
https://github.com/tensorflow/tfjs/issues/5050,Switch new Buffer to Buffer.from,1,closed,2021-05-07T20:56:30Z,2021-05-10T20:15:08Z,https://github.com/tensorflow/tfjs/blob/e18e0984ab494c18d83b08b7b8bff0fdac79af05/scripts/cloud_funcs/send_email/index.js#L32Refer:- https://nodesource.com/blog/understanding-the-buffer-deprecation-in-node-js-10/- https://docs.guardrails.io/docs/en/vulnerabilities/javascript/insecure_use_of_language_framework_api.html,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5050"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5050"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,change API,Replace API with another effective one,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Incorrect instructions"",
    ""specific_type"": ""[E.1.1] Deprecated API Usage""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5034,TensorFlow.LayersModel.dispose() leaves tensors behind,4,closed,2021-05-04T21:48:24Z,2021-05-19T23:31:56Z,Not quite sure this should be reported; since I found a workaround and I'm not sure it applies to **all** models; but here it goes:### System information- Have I written custom code: no- OS Platform and Distribution: Windows 10 Pro 20H2 (build 19042.928)- TensorFlow.js installed from: npm - TensorFlow.js version: ^3.3.0- Browser version: Microsoft Edge Version 90.0.818.51 (Official build) (64-bit)### Code```jsimport * as TensorFlowLayers from '@tensorflow/tfjs-layers';console.log('numTensors (before model): ' + TensorFlow.memory().numTensors);const modelUrl = '[link to a model JSON]'; // this is the only line you must changeconst model = await TensorFlowLayers.loadLayersModel(modelUrl);console.log('numTensors (before dispose): ' + TensorFlow.memory().numTensors);model.dispose();console.error('numTensors (after dispose): ' + TensorFlow.memory().numTensors);```### Describe the current behaviorNot all tensors are disposed. See output:>numTensors (before model): 0numTensors (before dispose): 264numTensors (after dispose): 260### Describe the expected behaviorI expected to be no tensors after the call to `.dispose()`.### AlternativeI thought about using `.tidy`; but `loadLayersModel` is async so I couldn't.I managed to free all tensors using the bellow code (although TypeScripts gives me an error; requiring me to cast `model` to `any`):```jsTensorFlow.dispose(model as any);```,"['@schdck import is not correct @tensorflow/tfjs-layers has been moved to mono repo tfjs ; please check the documentation here as how to use tfjs layer https://github.com/tensorflow/tfjs/tree/master/tfjs-layers#tensorflowjs-layers-high-level-machine-learning-model-api====='; ""Hey @rthadur! Issue remains with the following code:```jsimport * as TensorFlow from '@tensorflow/tfjs';console.log('numTensors (before model): ' + TensorFlow.memory().numTensors);const model = await TensorFlow.loadLayersModel(modelUrl);console.log('numTensors (before dispose): ' + TensorFlow.memory().numTensors);model.dispose();console.error('numTensors (after dispose): ' + TensorFlow.memory().numTensors);```Output:>numTensors (before model): 0numTensors (before dispose): 264numTensors (after dispose): 260=====""; 'please check this document as how the memory management happens in tfjs https://www.tensorflow.org/js/guide/tensors_operations#memory . cc @mattsoulanille @lina128 ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5034"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5034"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,Layer API,API,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.3] Inf/None/Null Results"",
    ""specific_type"": ""[D.3.1] Non-numerical outputs of tensor memory management""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/5033,npm WARN deprecated node-pre-gyp@0.14.0,1,closed,2021-05-04T21:09:05Z,2021-05-06T17:43:52Z,https://github.com/tensorflow/tfjs/blob/984b8b1fe031604a4d9e35b7b583c248077c5cb0/tfjs-node/package.json#L78https://github.com/tensorflow/tfjs/blob/984b8b1fe031604a4d9e35b7b583c248077c5cb0/tfjs-node-gpu/package.json#L81```consolenpm WARN deprecated node-pre-gyp@0.14.0: Please upgrade to @mapbox/node-pre-gyp: the non-scoped node-pre-gyp package is deprecated and only the @mapbox scoped package will recieve updates in the future```,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5033"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5033"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5023,Getting Error for export { ImageClassificationModel; loadImageClassification } from './img_classification when trying to do object detection with AutoML model in NodeJS,2,closed,2021-05-03T11:08:56Z,2021-05-04T20:49:07Z,"I am trying to make Object detection in Node (Not Html) with custom model that was trained. **System information**- OS Platform - Mac OS Catalina- TensorFlow.js installed using NPM- TensorFlow.js version          ""dependencies"": {            ""@tensorflow/tfjs-automl"": ""^1.1.0"";            ""@tensorflow/tfjs-node"": ""^3.6.1""          }**Describe the current behavior**Trying to make object detection predict working in Node js. Below is the code`const tf = require(""@tensorflow/tfjs-node"");const automl = require(""@tensorflow/tfjs-automl"");const fs = require(""fs"");const model_url = ""/model.json"";const image_path = process.argv.slice(2)[0];if (!image_path) {  throw new Error(""/MegaFortuneWheel.png"");}const image = fs.readFileSync(""/MegaFortuneWheel.png"");const decoded_image = tf.node.decodeJpeg(image);async function run() {  const model = await automl.loadObjectDetection(model_url);  const predictions = await model.detect(decoded_image);  console.log(predictions);}run().catch(console.error);`**Standalone code to reproduce the issue**Below is full code with the model fileshttps://github.com/ayandebbarman/TensorFlowObjectDetection**Other info / logs** Include any logs or source code that would be helpful to This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations; rebuild TensorFlow with the appropriate compiler flags./Users/ayan.barman/Documents/Node/TF_git/node_modules/@tensorflow/tfjs-automl/dist/index.js:18export { ImageClassificationModel; loadImageClassification } from './img_classification';^^^^^^SyntaxError: Unexpected token 'export'    at Object.compileFunction (node:vm:355:18)    at wrapSafe (node:internal/modules/cjs/loader:1038:15)    at Module._compile (node:internal/modules/cjs/loader:1072:27)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:988:32)    at Function.Module._load (node:internal/modules/cjs/loader:828:14)    at Module.require (node:internal/modules/cjs/loader:1012:19)    at require (node:internal/modules/cjs/helpers:93:18)    at Object.<anonymous> (/Users/ayan.barman/Documents/Node/TF_git/tf.js:2:16)    at Module._compile (node:internal/modules/cjs/loader:1108:14)","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5023"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5023"">No</a>====='; 'Duplicate of https://github.com/tensorflow/tfjs/issues/5017=====']",Initialization Faliure,Build & Initialization Failure,Misconfiguration,TF(CPU),Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5019,bodyPix drawBokehEffect does not accept OffscreenCanvas,5,closed,2021-05-02T20:43:00Z,2021-05-04T14:13:10Z,**System information**- Have I written custom code: Yes- OS Platform and Distribution: macOS 10.15.4- TensorFlow.js installed from: script link- TensorFlow.js version: 3.6.0- BodyPix version: 2.1.0- Browser version: Chrome 90.0.4430.93- Tensorflow.js Converter Version: ?**Describe the current behavior**When I call `bodyPix.drawBokehEffect` and provide an `HTMLCanvasElement` as the second argument; everything works fine. However; when I provide an `OffscreenCanvas` instance; I get an error.**Describe the expected behavior**[This PR](https://github.com/tensorflow/tfjs-models/pull/610) appears to fix the issue; but it's not available in the latest releases of tfjs (3.6.0) or bodypix (2.1.0).I'd like to request a release that includes this change. If this change is already published; please let me know which version contains it as I believe I'm using the latest version of both libraries.Thank you!,"['@jpodwys thank you soon new version will be published to npm ====='; ""That's great news; thank you!=====""; 'New Version is published in npm https://www.npmjs.com/package/@tensorflow-models/body-pix ; thank you @lina128 ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5019"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5019"">No</a>====='; ""Thank you; it's working great!=====""]",Data & Model Error,Crash,Incorrect Code Logic,Model API,API,add support for datatype,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/5017,node-pre-gyp info This Node instance does not support builds for N-API version 8,20,open,2021-04-30T13:26:52Z,2021-12-16T16:34:40Z,"I am trying to use TensorFlow with Firebase Cloud Functions. I'm on Node.js v14.16.0 which should be Node-API v7.And I get this when starting the server:```console$ firebase serve!  D:\folder_name\functions\node_modules\@tensorflow\tfjs-automl\dist\index.js:18export { ImageClassificationModel; loadImageClassification } from './img_classification';^^^^^^SyntaxError: Unexpected token 'export'    at wrapSafe (internal/modules/cjs/loader.js:979:16)    at Module._compile (internal/modules/cjs/loader.js:1027:27)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)    at Module.load (internal/modules/cjs/loader.js:928:32)    at Function.Module._load (internal/modules/cjs/loader.js:769:14)    at Module.require (internal/modules/cjs/loader.js:952:19)    at require (internal/modules/cjs/helpers.js:88:18)    at Object.<anonymous> (D:\folder_name\functions\index.js:14:11)    at Module._compile (internal/modules/cjs/loader.js:1063:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)!  We were unable to load your functions code. (see above)$ head -n 14 index.jsconst functions = require(""firebase-functions"");        express = require(""express"");        app = express();        admin = require(""firebase-admin"");        cookieParser = require(""cookie-parser"");        Busboy = require(""busboy"");        path = require(""path"");        os = require(""os"");        fs = require(""fs"");        vader = require(""vader-sentiment"");        morgan = require(""morgan"");        axios = require(""axios"");        tfnode = require(""@tensorflow/tfjs-node"");        automl = require(""@tensorflow/tfjs-automl"");$ nodeWelcome to Node.js v14.16.0.Type "".help"" for more information.> console.log(process.versions.napi)7undefined> .exit$ cat package.json""dependencies"": {                ""@tensorflow/tfjs-automl"": ""^1.1.0"";                ""@tensorflow/tfjs-converter"": ""^3.3.0"";                ""@tensorflow/tfjs-core"": ""^3.3.0"";                ""@tensorflow/tfjs-node"": ""^3.6.1"";        };```MCVE available at: https://github.com/aravindvnair99/TFJS-Issue-5017I've set up a GitHub Actions workflow to reproduce the issue on multiple OS and Node.js versions. Results are available here: https://github.com/aravindvnair99/TFJS-Issue-5017/actionsIt's failing in all cases. Workflow configuration: https://github.com/aravindvnair99/TFJS-Issue-5017/blob/main/.github/workflows/node.js.yml","['Latest version has support for napi version 8 https://github.com/tensorflow/tfjs/pull/4991/files ; please try latest version. Thank you ====='; ""> Latest version has support for napi version 8 https://github.com/tensorflow/tfjs/pull/4991/files ; please try latest version. Thank you@rthadur I'm sorry; I didn't understand. I'm on Node.js 14 which is 7 and not 8.=====""; '@aravindvnair99 Thank you for reporting; can you share the full error message? ====='; ""> @aravindvnair99 Thank you for reporting; can you share the full error message?@pyu10055 It's what I put in my first message itself. Everything else wasn't an error message. Just the usual Firebase loading.=====""; ""@pyu10055 @rthadur @gbaned  I've put together an MCVE with instructions to reproduce: https://github.com/aravindvnair99/TFJS-Issue-5017Update: I've added 4 different test cases.=====""; ""I've set up a GitHub Actions workflow to reproduce the issue on multiple OS and Node.js versions. Results are available here: https://github.com/aravindvnair99/TFJS-Issue-5017/actionsIt's failing in all cases. Workflow configuration: https://github.com/aravindvnair99/TFJS-Issue-5017/blob/main/.github/workflows/node.js.yml=====""; '@aravindvnair99  Looks like the failure is not on the tfjs-node library but the tfjs-autoML; you can use the latest version 1.2.0 of tfjs-autml which have solve the node.js compatibility issue.====='; '@pyu10055 Can confirm updating to @tensorflow/tfjs-automl v1.2.0 has resolved the issue.Fixed in https://github.com/tensorflow/tfjs/pull/5024====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5017"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5017"">No</a>====='; 'I am still getting `node-pre-gyp info This Node instance does not support builds for N-API version 8` and using node v14.16.0. Also only trying to use `""@tensorflow/tfjs-node"": ""3.6.1""`. No automl.Any ideas?====='; '> I am still getting `node-pre-gyp info This Node instance does not support builds for N-API version 8` and using node v14.16.0. Also only trying to use `""@tensorflow/tfjs-node"": ""3.6.1""`. No automl.> Any ideas?@Fabiansson I too still get it. But I had closed this issue as the other bug I\'ve mentioned had got fixed. That bug prevented me from using the module itself. I guess I\'ll re-open this issue.@pyu10055 Could you confirm which Node.js versions are supported? It would be good to have this mentioned in the documentation as well as I found certain files aren\'t available in the bucket from where it\'s fetching files to build.====='; ""To be clear I did not manage to install the newest tfjs-node on any Node Version. I tested on Node: 10x; 12x; 14x and even 16x. Would have to look up which version exactly. I tried on Windows; Ubuntu and WSL(Ubuntu). Some combinations will not install at all and also manual building does not work reliably. I'm genuinly surprised not more people are having this problem. I really don't know what else I could try. Maybe someone can specify a exact working combination of Node-Version; OS and tfjs-node version so I could try this.=====""; 'Make sure you have the necessary **build tools** installed in your system. ( For ubuntu or debian based use `sudo apt install build-essential` ; for windows `npm i windows-build-tools`).Now try these versions :Node 16.x@tensorflow/tfjs-node 3.7.0Tested today on Ubuntu 20.04.====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Not stale====='; '@pyu10055 @rthadur Could you confirm which Node.js versions are supported? It would be good to have this mentioned in the documentation as well as certain files are not available in the bucket from where it is fetching files to build.====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Not stale====='; ""@aravindvnair99 Not sure if you're still running into this; but I just solved a similar issue in one of my projects. In my case; I was using electron; which bundles its own version of Node (separate from the version I had installed). In my code I ran `console.log('Version: ' + process.version);` and it spat out v14.6.0; which was Napi 7. Updating electron (and therefore the version of node bundled with it) fixed the issue. Firebase might similarly bundle an old version of node with their CLI.=====""; '@aravindvnair99 did you get chance to check above solution ?=====']",Initialization Faliure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] SyntaxError: Unexpected token 'export'""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5007,Possible memory leak in tfjs-node,1,closed,2021-04-28T23:10:46Z,2021-04-29T15:37:15Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: NA- TensorFlow.js installed from (npm or script link): NA- TensorFlow.js version (use command below): 3.5- Browser version: NA- Tensorflow.js Converter Version: NA**Describe the current behavior**`process.memoryUsage().rss` in node.js shows a steady increase with time. All the tensors created are wrapped inside `tf.tidy()````jsconst result = tf.tidy(() => {    const x = {        'Age': tf.tensor([1]);        ...      }      // logger(`x = ${JSON.stringify(x)}`)    return predict(model; x)  });```**Describe the expected behavior**RSS memory should be freed up regularly by the node.js process to prevent it from crashing due OOM**Standalone code to reproduce the issue**Simply push the **Run** button in https://replit.com/@NitinPasumarthy/memory-leak-tfjs-node#index.js and observe the latest memory-usage-[[timestamp]].log CSV file**Other info / logs** Include any logs or source code that would be helpful tohttps://replit.com/@NitinPasumarthy/memory-leak-tfjs-node#memory-usage-1619649567559.logMany such logs are available with small variations of code,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5007"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5007"">No</a>=====']",RSS increase,Poor Performance,Incorrect Code Logic,TF(CPU),Backend,memory management,Add API usage for memory management,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B,A.4
https://github.com/tensorflow/tfjs/issues/5000,posenet\demo\coco.js: function __clone()... could not be cloned error when trying to start dev demo server,4,closed,2021-04-28T04:26:19Z,2021-05-06T22:46:13Z,"Hi; I cloned the tfjs-models repository and was about to try out posenet demo.I followed the instructions as per GitHub: https://github.com/tensorflow/tfjs-models/tree/master/posenet/demoeverything went well but when I did yarn watch; I got the following error:""PS C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo> yarn watchyarn run v1.22.10$ cross-env NODE_ENV=development parcel index.html --no-hmr --openServer running at http://localhost:1234 ×  C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\coco.js: function __clone() {    var node2 = new Node();    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    var node2 = new Node();    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    at Object.serialize (v8.js:256:7)    at _default (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\util\clone-deep.js:16:30)    at normalizeFile (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\normalize-file.js:52:36)    at normalizeFile.next (<anonymous>)    at run (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transformation\index.js:31:50)    at run.next (<anonymous>)    at Function.<anonymous> (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\@babel\core\lib\transform-ast.js:20:41)    at Generator.next (<anonymous>)    at evaluateSync (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\gensync\index.js:251:28)    at Function.sync (C:\Users\raywl\Documents\achiever\Android\AI services\tfjs-models\posenet\demo\node_modules\gensync\index.js:89:14)""a browser tab also loaded up with the same build error.I am running on Windows 10 and my editor is Visual Studio Code:Version: 1.55.2 (user setup)Commit: 3c4e3df9e89829dce27b7b5c24508306b151f30dDate: 2021-04-13T09:35:57.887ZElectron: 11.3.0Chrome: 87.0.4280.141Node.js: 12.18.3V8: 8.7.220.31-electron.0OS: Windows_NT x64 10.0.19041Please advise.","['@FlyWong thanks for reporting ; i could reproduce the same in MAC OS ; will dig deep more in to actual issue  and keep you posted.====='; 'Hi;is there any update on this issue?________________________________From: Rajeshwar Reddy T ***@***.***>Sent: Thursday; April 29; 2021 1:10 AMTo: tensorflow/tfjs ***@***.***>Cc: FlyWong ***@***.***>; Mention ***@***.***>Subject: Re: [tensorflow/tfjs] posenet\\demo\\coco.js: function __clone()... could not be cloned error when trying to start dev demo server (#5000)@FlyWong<https://github.com/FlyWong> thanks for reporting ; i could reproduce the same in MAC OS ; will dig deep more in to actual issue and keep you posted.—You are receiving this because you were mentioned.Reply to this email directly; view it on GitHub<https://github.com/tensorflow/tfjs/issues/5000#issuecomment-828626998>; or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACPS7UWSWOQPFTPJBQXSCWTTLA6QNANCNFSM43WIMJ3Q>.====='; 'this is a duplicate of https://github.com/tensorflow/tfjs/issues/4921 ; will track the same issue at one place. Closing this issue. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5000"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5000"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Model API,API,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Parcel Build Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4995,navigator.hardwareConcurrency and native pthread gets different thread pool size,1,closed,2021-04-27T05:19:32Z,2021-04-28T18:00:39Z,"(On Intel i7-9700)By navigator.hardwareConcurrency: https://github.com/tensorflow/tfjs/pull/4994; the WASM_THREAD_POOL_SIZE is 4. WASM_THREAD_POOL_SIZE is got by:```Math.min(4; Math.max(1; (navigator.hardwareConcurrency || 1) / 2));```By native pthread:  https://github.com/tensorflow/tfjs/pull/4942; the WASM_THREAD_POOL_SIZE is 1. WASM_THREAD_POOL_SIZE is got by:```// emscripten_num_logical_cores corresponds to navigator.hardwareConcurrency.// Many x86-64 processors have 2 threads per core; so we are dividing by 2.#ifdef __EMSCRIPTEN_PTHREADS__int num_cores = emscripten_num_logical_cores() / 2;#elseint num_cores = 1;#endifint min_num_threads = 1;int max_num_threads = 4;int thread_pool_size =  // thread_pool_size is WASM_THREAD_POOL_SIZE    std::min(std::max(num_cores; min_num_threads); max_num_threads);```Does this possible due to #ifdef __EMSCRIPTEN_PTHREADS__?BTW; code has rebased; and https://github.com/tensorflow/tfjs/pull/4957 is included.Reproduce steps:1.  Use local wasm:```+++ b/e2e/benchmarks/local-benchmark/index.html@@ -91;7 +91;7 @@ limitations under the License.   <script src=""https://unpkg.com/@tensorflow/tfjs-backend-webgl@latest/dist/tf-backend-webgl.js""></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-layers@latest/dist/tf-layers.js""></script>   <script src=""https://unpkg.com/@tensorflow/tfjs-converter@latest/dist/tf-converter.js""></script>-  <script src=""https://unpkg.com/@tensorflow/tfjs-backend-wasm@latest/dist/tf-backend-wasm.js""></script>+  <script src=""../../../tfjs-backend-wasm/dist/tf-backend-wasm.js""></script>```2.  Build:cd tfjs-backend-wasmtfjs-backend-wasm$ yarn & yarn build-npmcd ../e2ee2e$ yarn build-deps3. Serup servercd ../../npx http-server4.  Open http://127.0.0.1:8080/tfjs/e2e/benchmarks/local-benchmark/","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4995"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4995"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Inconsistent Thread Pool Size""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4988,Is real time object detection possible using TensorCamera and Cocossd ?,4,closed,2021-04-26T13:46:42Z,2021-07-14T17:19:20Z,"When I use Tensor Camera 1. It is very much laggy 2. I guess it keeps capturing each frame and thus we just get one item in prediction using the mobilenet model 3. In case we use cocossd for multiple image detection it throw error """"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead""""So How is it possible to make a real-time multiple image detection; I can see an example in flutter so is that the same possible in React Native anyway? Flutter example that I need in React Native : - https://heartbeat.fritz.ai/turning-the-mobile-camera-into-a-real-time-object-detector-with-flutter-and-tensorflow-lite-f412962b1805","['You can refer tensorflow js face landmark detection from the tensorflow website itself it might help you ;for capturing the images u can use openCV ====='; 'This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow.js) since it is not a bug or feature request. There is also a larger community that reads questions there.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4988"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4988"">No</a>====='; '> > > When I use Tensor Camera> >     1. It is very much laggy> >     2. I guess it keeps capturing each frame and thus we just get one item in prediction using the mobilenet model> >     3. In case we use cocossd for multiple image detection it throw error """"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead""""> > > So How is it possible to make a real-time multiple image detection; I can see an example in flutter so is that the same possible in React Native anyway?> > Flutter example that I need in React Native : - https://heartbeat.fritz.ai/turning-the-mobile-camera-into-a-real-time-object-detector-with-flutter-and-tensorflow-lite-f412962b1805Did you find a solution?=====']",Browser Hangs,Poor Performance,Unknown,WebGL,Backend,change API,change API,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",B.1.2,E
https://github.com/tensorflow/tfjs/issues/4978,Installation issue @tensorflow/tfjs-node-gpu with npm on Win 10,2,closed,2021-04-23T21:10:42Z,2021-04-26T21:17:22Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10 Pro- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.5.0- CUDA/cuDNN version: 11.2**Describe the problem**I can't install tfjs-node-gpu through npm.npm ERR! code 1npm ERR! path c:\IA\node_modules\@tensorflow\tfjs-node-gpunpm ERR! command failednpm ERR! command C:\Windows\system32\cmd.exe /d /s /c node scripts/install.js gpu downloadnpm ERR! GPU-windows-3.5.0.zipnpm ERR! * Downloading libtensorflownpm ERR!npm ERR! * Building TensorFlow Node.js bindingsnpm ERR! symlink ./lib/napi-v8 failed:  Error: Command failed: node scripts/deps-stage.js symlink ./lib/napi-v8npm ERR!   * Symlink of lib\napi-v8\tensorflow.dll failed; creating a copy on disk.npm ERR! node:internal/process/promises:245npm ERR!           triggerUncaughtException(err; true /* fromPromise */);npm ERR!           ^npm ERR!npm ERR! [Error: ENOENT: no such file or directory; copyfile 'c:\IA\node_modules\@tensorflow\tfjs-node-gpu\deps\lib\tensorflow.dll' -> 'c:\IA\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v8\tensorflow.dll'] {npm ERR!   errno: -4058;npm ERR!   code: 'ENOENT';npm ERR!   syscall: 'copyfile';npm ERR!   path: 'c:\\IA\\node_modules\\@tensorflow\\tfjs-node-gpu\\deps\\lib\\tensorflow.dll';npm ERR!   dest: 'c:\\IA\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v8\\tensorflow.dll'npm ERR! }npm ERR!npm ERR!     at ChildProcess.exithandler (node:child_process:326:12)npm ERR!     at ChildProcess.emit (node:events:369:20)npm ERR!     at maybeClose (node:internal/child_process:1067:16)npm ERR!     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5) {npm ERR!   killed: false;npm ERR!   code: 1;npm ERR!   signal: null;npm ERR!   cmd: 'node scripts/deps-stage.js symlink ./lib/napi-v8'npm ERR! }**Provide the exact sequence of commands / steps that you executed before running into the problem**`npm install @tensorflow/tfjs-node-gpu`I've got the same issue with ""npm install @tensorflow/tfjs-node""**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.[2021-04-23T21_07_53_494Z-debug.log](https://github.com/tensorflow/tfjs/files/6368115/2021-04-23T21_07_53_494Z-debug.log)","['I tested older tfjs-node-gpu versions and the installation is working until 3.1.0The issue is present since 3.2.0====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4978"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4978"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(GPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Missing Dependency Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4971,Chrome Deprecation warning: SharedArrayBuffer will require cross-origin isolation as of M91,10,closed,2021-04-22T23:43:00Z,2021-09-09T03:17:59Z,Running `TFJS` 3.5.0 with `WASM` backend and *SIMD* & *multithreading* enabled results in warnings in browser console:```log[Deprecation] SharedArrayBuffer will require cross-origin isolation as of M91; around May 2021.  See https://developer.chrome.com/blog/enabling-shared-array-buffer/ for more details.  (anonymous)	@	flags_wasm.ts:49  evaluateFlag	@	environment.ts:141  getAsync	@	environment.ts:88```Which happens in internal WASM test in `flags_wasm.ts`:```js// Test for transferability of SABs (needed for Firefox)// https://groups.google.com/forum/#!msg/mozilla.dev.platform/IHkBZlHETpA/dwsMNchWEQAJnew MessageChannel().port1.postMessage(new SharedArrayBuffer(1));// This typed array is a WebAssembly program containing threaded// instructions.return WebAssembly.validate(new Uint8Array([  0; 97; 115; 109; 1; 0;  0;  0; 1; 4; 1;  96; 0;   0;  3; 2; 1;  0; 5;  4; 1;  3;   1;   1; 10; 11; 1; 9; 0; 65; 0;  254; 16; 2; 0; 26; 11]));```Environment: TFJS 3.5.0 with Edge/Chromium 90 on Windows 10 with WASM backend (SIMD and MultiThreading enabled),"['Yes; this change will be out with Chrome 91 release. Please see the official link for how to setup coop and coep: https://web.dev/coop-coep/====='; ""Here's some updated progress on this change: https://developer.chrome.com/blog/enabling-shared-array-buffer/=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4971"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4971"">No</a>====='; '@lina128 What if it is impossible to implement cross-origin isolated pages? Will tensorflow.js wasm backend add an `ArrayBuffer` fallback for unavailable `SharedArrayBuffer`; to make it less efficient but still workable?====='; 'Hi @SukkaW; without cross-origin isolation; the wasm baskend will fallback to simd only (without multi-threading support). ====='; '> without cross-origin isolation; the wasm baskend will fallback to simd only (without multi-threading support).Thanks!====='; 'Thanks @jinjingforever . Another question is: could cross-origin isolation be enabled for [tfjs e2e benchmark](https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html) site? Otherwise; users can only test it without multi-threading support.====='; ""Good point @huningxin! I think the e2e benchmark is hosting on github page which doesn't support custom headers. I will discuss with the team and see where to move it. Thanks! =====""; '@huningxin We set up a new benchmark site that has the cross-origin isolation set up: https://tfjs-benchmarks.web.app/local-benchmark/. Thanks!====='; 'It works like a charm. Thanks @jinjingforever .=====']",Reference Error,Crash,Browser Incompatibility,Browser,Platform,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Deprecation Warning""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.1,D.2
https://github.com/tensorflow/tfjs/issues/4947,Delete called on a Tensor not referenced with Node.js worker_threads,2,closed,2021-04-16T18:46:49Z,2021-06-03T14:58:15Z,Using `tf.tidy` in a Node.js worker thread sometimes causes `Error: Delete called on a Tensor not referenced` if two instances of the thread are running at the same time.- OS: Windows 10 20H2- TensorFlow.js version: v3.3.0- Node version: v14.16.0```js// master.jsconst { Worker } = require('worker_threads');const w1 = new Worker('worker.js');const w2 = new Worker('worker.js');// worker.jsconst { tidy; scalar } = require('@tensorflow/tfjs-node');tidy(() => {    const s = scalar(3.14);});``````Error: Delete called on a Tensor not referenced (tensor_id: 2392)    at Object.<anonymous> (<anonymous>)    at NodeJSKernelBackend.disposeData (node_modules\@tensorflow\tfjs-node\dist\nodejs_kernel_backend.js:264:30)    at Engine.disposeTensor (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3381:26)    at Tensor.dispose (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:2361:21)    at Engine.scopedRun (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3017:13)    at Engine.tidy (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3001:21)    at Object.tidy (node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:8744:19)```,"['related PR has been merged ; @bmcdorman thanks for the contribution.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4947"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4947"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,TF(CPU),Backend,type replacer,Replace data Shape/type,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4939,model .infer() cannot specify endpoint output on script link but works on npm,3,closed,2021-04-15T12:33:37Z,2021-04-16T10:40:20Z,**System information**- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: just on desktop browser- TensorFlow.js installed from script link and nodejs : 3.3- MobileNet version: 2.1.0**Describe the current behavior**I would like specify one endpoint as an output of MobileNet.According to the example [here](https://github.com/tensorflow/tfjs-models/blob/master/knn-classifier/README.md) it should works by using:```javascriptmobilenetModule.infer(img1; 'conv_preds');mobilenetModule.infer(img1; 'conv_dw_12');```But when I made following tests; which strangely still works:![image](https://user-images.githubusercontent.com/33302970/114869312-ca53ae80-9e31-11eb-9ba1-b1a08d17007a.png)However; when I use nodejs + browserify to build a javascript file and run a web; the endpoint function seems to work normally because it raise error when I give random endpoint name:```javascriptconst infer_map4 = () => this.mobilenet.infer(image; 'conv_dw_12fsdafsdafsd');```![image](https://user-images.githubusercontent.com/33302970/114869493-fff89780-9e31-11eb-917c-c667a7f5b9a2.png)I feel like it is quite important to not only get a naive output from the network; but getting some internal values in the CNN.Is there any way that can help fix this?,"['Here I suppose nodejs source code and the code by HTML script input should be the same if the version is the same?I first check the nodejs-version code; especially the part that does ""infer"" in MobileNet:```javascriptMobileNet.prototype.infer = function (img; endpoint) {        var _this = this;        if (endpoint != null && this.endpoints.indexOf(endpoint) === -1) {            throw new Error(""Unknown endpoint "" + endpoint + "". Available endpoints: "" +                (this.endpoints + "".""));        }        return tf.tidy(function () {            if (!(img instanceof tf.Tensor)) {                img = tf.fromPixels(img);            }            var normalized = img.toFloat()                .sub(_this.normalizationOffset)                .div(_this.normalizationOffset);            var resized = normalized;            if (img.shape[0] !== IMAGE_SIZE || img.shape[1] !== IMAGE_SIZE) {                var alignCorners = true;                resized = tf.image.resizeBilinear(normalized; [IMAGE_SIZE; IMAGE_SIZE]; alignCorners);            }            var batched = resized.reshape([1; IMAGE_SIZE; IMAGE_SIZE; 3]);            var model;            if (endpoint == null) {                model = _this.model;            }            else {                if (_this.intermediateModels[endpoint] == null) {                    var layer = _this.model.layers.find(function (l) { return l.name === endpoint; });                    _this.intermediateModels[endpoint] =                        tf.model({ inputs: _this.model.inputs; outputs: layer.output });                }                model = _this.intermediateModels[endpoint];            }            return model.predict(batched);        });    };```It seems that it is finding an endpoint for output normally.However; the source code from HTML script link input seems a bit different:```javascript        e.prototype.infer = function(e; r) {            var o = this;            return void 0 === r && (r = !1);            a.tidy(function() {                e instanceof a.Tensor || (e = a.browser.fromPixels(e));                var i = e.toFloat().sub(o.normalizationOffset).div(o.normalizationOffset)                  ; n = i;                if (224 !== e.shape[0] || 224 !== e.shape[1]) {                    n = a.image.resizeBilinear(i; [224; 224]; !0)                }                var s; l = n.reshape([-1; 224; 224; 3]);                if (r) {                    var c = t[o.version];                    s = o.model.execute(l; c).squeeze([1; 2])                } else {                    s = o.model.predict(l).slice([0; 1]; [-1; 1e3])                }                return s            })        }```I guess ```e``` here is the image tensor and ```r``` is supposed to be the ""endpoint"".But the logic here seems to be: as long as there is a valid input of ```r```; it outputs one specific layer value.====='; 'Problem solved by using model.execute similar to [here](https://github.com/tensorflow/tfjs-models/blob/238dd2e909d7f371b787605cf503a2888c00bf09/mobilenet/src/index.ts#L227-L236)It would be great if model.infer() allows us to directly input the endpoint name.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4939"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4939"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Dependency Error,Model API,API,change API,Replace API with another effective one,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",D,B.2
https://github.com/tensorflow/tfjs/issues/4937,Env flag affects the peak memory in e2e benchmark.,1,open,2021-04-15T07:55:51Z,2021-04-16T00:59:36Z,Steps to reproduce:1. Access https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html2. Use the default backend `wasm` and click `Run benchmark`.3. See the left tablePeak memory | 19.63 MB-- | --4. Switch to `webgl` backend. Change any flags; for example `webgl pack`. And then switch back to `wasm`.5. Click `Run benchmark` again. The peak memory increases as below:Peak memory | 33.51 MB-- | --6. If we repeat 4 and 5; you will find the peak memory gets bigger and bigger. However; if you don't change the flag; the peak memory won't change. It happens for all backends that the flag will affect the peak memory. It seems like a bug.,"[""fyi @lina128. This is the issue that I mentioned in yesterday's meeting.=====""]",Memory Leak,Poor Performance,Incorrect Code Logic,Operator,API,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/4936,[wasm]  Error downloading wasm-binaries.tbz2 when yarn build or build-npm,3,closed,2021-04-15T07:12:38Z,2021-05-19T05:57:54Z,"In windows; under tfjs-backend-wasm folder;  yarn build(or build-npm) failed due to Error downloading.In linux; everything goes well.When I tried yarn build-npm; it complains:```Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp12762910739068097029/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not Found```It seems  file https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2 can not be downloaded by chrome. And https://storage.googleapis.com/webassembly doesn't contains windows build.Then I tried (a cmd used in yarn build):```yarn bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasm ```It also complains the same file 404:```C:\workspace\wasm\tfjs\tfjs-backend-wasm>yarn bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasmyarn run v1.22.5$ C:\workspace\wasm\tfjs\tfjs-backend-wasm\node_modules\.bin\bazel build -c opt //tfjs-backend-wasm/src/cc:tfjs-backend-wasmINFO: Repository emscripten_bin_win instantiated at:  C:/workspace/wasm/tfjs/WORKSPACE:27:22: in <toplevel>  C:/users/abc/_bazel_abc/js5lmfgj/external/emsdk/emscripten_deps.bzl:48:21: in emscripten_depsRepository rule http_archive defined at:  C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>WARNING: Download from https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2 failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not FoundERROR: An error occurred during the fetch of repository 'emscripten_bin_win':   Traceback (most recent call last):        File ""C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl""; line 111; column 45; in _http_archive_impl                download_info = ctx.download_and_extract(Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundERROR: Error fetching repository: Traceback (most recent call last):        File ""C:/users/abc/_bazel_abc/js5lmfgj/external/bazel_tools/tools/build_defs/repo/http.bzl""; line 111; column 45; in _http_archive_impl                download_info = ctx.download_and_extract(Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundINFO: Repository nodejs_windows_amd64 instantiated at:  C:/workspace/wasm/tfjs/WORKSPACE:9:13: in <toplevel>  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/index.bzl:77:23: in yarn_install  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:757:15: in node_repositories  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:777:18: in _maybeRepository rule node_repositories_rule defined at:  C:/users/abc/_bazel_abc/js5lmfgj/external/build_bazel_rules_nodejs/internal/node/node_repositories.bzl:696:41: in <toplevel>ERROR: C:/users/abc/_bazel_abc/js5lmfgj/external/emsdk/BUILD:27:6: @emsdk//:binaries depends on @emscripten_bin_win//:all in repository @emscripten_bin_win which failed to fetch. no such package '@emscripten_bin_win//': java.io.IOException: Error downloading [https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.tbz2] to C:/users/abc/_bazel_abc/js5lmfgj/external/emscripten_bin_win/temp14758550607092921566/wasm-binaries.tbz2.tar.bz2: GET returned 404 Not FoundERROR: Analysis of target '//tfjs-backend-wasm/src/cc:tfjs-backend-wasm' failed; build aborted: Analysis failedINFO: Elapsed time: 1.663sINFO: 0 processes.FAILED: Build did NOT complete successfully (1 packages loaded; 2 targets configured)    Fetching @emscripten_npm_win; Restarting.    Fetching @xnnpack; Cloning 3bfbdaf00211b313b143af39279bb6bf1f7effc0 of https://github.com/google/XNNPACK.giterror Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```**System information**- TensorFlow.js version: commit 081b28468bf18505326b543f1ec450d725e1dbb4 (HEAD -> master; origin/master; origin/HEAD)Date:   Wed Apr 14 20:04:45 2021 -0700**Describe the problem****Provide the exact sequence of commands / steps that you executed before running into the problem****Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.","[""Thanks for reporting this.I'm setting up a VM to reproduce it; and I've talked to the author of the Emscripten rules. We have found out that it's caused by an incorrect extension on the download. It should end in `.zip` instead of `.tbz2` for Windows. For example; https://storage.googleapis.com/webassembly/emscripten-releases-builds/win/89202930a98fe7f9ed59b574469a9471b0bda7dd/wasm-binaries.zip; should be downloadable. I'll send a patch to Emscripten once I get it working. However there may be some other Windows comparability issues other than having an incorrect URL; so I can't guarantee this will be a quick fix.As a workaround; you can use [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install-win10).=====""; 'Thanks @mattsoulanille! Then I will close this first.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4936"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4936"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Download Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.4] Confused Document""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4934,Benchmark tool freeze with tfjs@3.4.0,2,closed,2021-04-15T01:02:49Z,2021-04-22T19:40:05Z,The benchmark tool freeze (no response) with the latest tfjs@3.4.0 release. https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html,"['@lina128 this may related to https://github.com/tensorflow/tfjs/issues/4932====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4934"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4934"">No</a>=====']",Browser Hangs,Poor Performance,Misconfiguration,Wasm,Backend,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",B.1.2,B.1
https://github.com/tensorflow/tfjs/issues/4932,tfjs 3.4.0 regression: wasm causes browser hang,9,closed,2021-04-14T19:32:42Z,2021-04-22T20:43:37Z,tfjs 3.4.0 was just released and unfortunately wasm module is somewhat broken  to be precise; `tfjs-backend-wasm.wasm` and `tfjs-backend-wasm-simd.wasm` seem to work (although haven't tested in depth)  what's definitely broken is `tfjs-backend-wasm-threaded-simd.wasm`and symptoms are simple - complete browser hang;  up to a point where browser window is nonresponsive to close requests and requires a hard killto reproduce: - enable `WebAssembly SIMD support` in <chrome://flags>- enable `WebAssembly threads support` in <chrome://flags>- set backend to `wasm`- run any model inferencethis used to work in all recent versions of tfjsenvironment: tfjs 3.4.0 on windows 10 with chrome 89 or edge 89,"['https://github.com/tensorflow/tfjs/issues/4796 this might be related ; will try to test in my local as well and update ; thank you cc @mattsoulanille ====='; 'I test in my local; the problem is gone after turning off the SIMD flag in chrome. We are working on a fix. Thank you for reporting the bug!====='; '@mattsoulanille This maybe related to the recent changes to bazel wasm build; https://github.com/tensorflow/tfjs/pull/4769====='; ""any updates on this one? as much as i'd love to use tfjs 3.4.0 due to support of tf2; this is a blocker for me.=====""; ""No updates yet; but I've been able to reproduce it. I agree with Ping that this is likely related to changes made in #4769; and I'm taking a look.=====""; ""@mattsoulanille - thanks - *'taking a look'* is an update; just wanted to know what's going on as it's a blocker for me.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4932"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4932"">No</a>====='; ""@mattsoulanille thanks!given it's a regression that causes hangs; does this warrant spinning up a tfjs service build (e.g.; 3.4.1)?(cc @pyu10055)=====""; 'Hi @vladmandic ; we are releasing 3.5.0 with this fix today.=====']",Regression,Poor Performance,Misconfiguration,Wasm,Backend,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",B.3.1,B.1
https://github.com/tensorflow/tfjs/issues/4930,tfjs 3.4.0 has been released on cdnjs; but it's not on github releases page and there are no release notes,3,closed,2021-04-14T11:34:13Z,2021-04-14T18:43:47Z,`tfjs` **3.4.0** has been released on <https://www.npmjs.com/package/@tensorflow/tfjs>;  but it's not on <https://github.com/tensorflow/tfjs/releases> releases page and there are no release notes  environment: tfjs 3.4.0 on ubuntu 20.10,"['@vladmandic release notes has been updated and i see it here https://github.com/tensorflow/tfjs/releases; thanks for noticing this.cc @lina128 ====='; ""yup; it's online now; closing the issue.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4930"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4930"">No</a>=====']",Document Error,Document Error,Untimely Update,Operator,API,publish new package,publish new package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Missing Release Notes"",
    ""specific_type"": ""[E.1.1] Invalid Information about Releases""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.4] Confused Document""
  }
}
```",E,B.3
https://github.com/tensorflow/tfjs/issues/4927,Using WASM with NodeJS but LoadGraphModel does not accept local file as input,1,open,2021-04-14T01:01:24Z,2021-08-25T12:08:16Z,`WASM` backend is supported in `NodeJS` environments as per  <https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html>> In Node.js; the WASM backend is a great solution for devices that don’t support the TensorFlow binary or you don’t want to build it from source.And yes; loading `tfjs` and `tfjs-backend-wasm` and setting backend to `wasm` works fine - but then how to use it?```jstf.loadGraphModel('file://models/model.json');``````errorRuntimeError: abort(TypeError: Only HTTP(S) protocols are supported).```So I've started a dev web server just to load models using *http://...* - and it works - but that really goes against idea of running NodeJS with WASM.I understand the scope of this issue is more of an enhancement; but since `WASM` is supported in `NodeJS`; expectation is that it should work natively.Also; note that missing local file I/O handler is just for model loader as loading *.wasm files from local filesystem works just fine.Just as a test; I've tried loading `tfjs-node` with `tfjs-backend-wasm` as `tfjs-node` provides local file I/O handler; but that causes hard conflict.Environment: TFJS 3.3.0 on Ubuntu 20.10 with NodeJS 15.14.0,['Just ran into the same problem!====='],Fetch Failure,Crash,Incorrect Code Logic,Wasm,Backend,change model path,Modifying model file path/extension,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.3"",
    ""specific_type"": ""C.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.3,A.4
https://github.com/tensorflow/tfjs/issues/4921,[tfjs-models] PoseNet demo cannot build successfully,3,closed,2021-04-12T01:35:06Z,2021-05-14T00:01:25Z,<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): **Window 10; Ubuntu 20.04**- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: - TensorFlow.js installed from (npm or script link): **npm**- TensorFlow.js version: **3.3.0**- CUDA/cuDNN version:**Describe the problem**Cannot start the posenet demo by 'yarn watch'**Provide the exact sequence of commands / steps that you executed before running into the problem**1. `git clone https://github.com/tensorflow/tfjs-models && cd tfjs-models/posenet`2. `yarn && yarn build`3. `cd demo && yarn && yarn watch`**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.```yunfei@yunfei-bj:~/00_yunfei/workspace/github/tfjs-models/posenet/demo$ yarn watchyarn run v1.22.5$ cross-env NODE_ENV=development parcel index.html --no-hmr --openServer running at http://localhost:1234🚨  /home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/coco.js: function __clone() {    var node2 = new Node();    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    var node2 = new Node();    for (var key in this) {      // Do not clone comments tha...<omitted>... } could not be cloned.    at Object.serialize (v8.js:257:7)    at _default (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/@babel/core/lib/transformation/util/clone-deep.js:16:30)    at normalizeFile (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/@babel/core/lib/transformation/normalize-file.js:52:36)    at normalizeFile.next (<anonymous>)    at run (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/@babel/core/lib/transformation/index.js:31:50)    at run.next (<anonymous>)    at Function.<anonymous> (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/@babel/core/lib/transform-ast.js:20:41)    at Generator.next (<anonymous>)    at evaluateSync (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/gensync/index.js:251:28)    at Function.sync (/home/yunfei/00_yunfei/workspace/github/tfjs-models/posenet/demo/node_modules/gensync/index.js:89:14)```This regression issue happens after https://github.com/tensorflow/tfjs-models/commit/048e44d6a2bae18023e317cc7115f543de9b370c; I don't know the root cause but https://github.com/tensorflow/tfjs-models/pull/637 will fix this build error by revert @babel/core to 7.6.4,"['@pyu10055 @lina128 PTAL; Thanks====='; '@haoyunfeix thanks for reporting; I can confirm the same issue on Mac OS ; @pyu10055 @lina128 should we revert the babel/core version ? if yes I can submit a PR====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4921"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4921"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Model API,API,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4907,Difference between First Run on Benchmark Run and In Code,22,open,2021-04-07T03:53:28Z,2021-04-16T23:59:23Z,When I run a model for first time; it is slow. This is expected. However; the difference in times I get for the first time is way different in benchmarking setup(https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html) and my own custom setup https://github.com/rohanmuplara/tester/tree/test_tfjs/graph I see the time difference for warmup inference differ between 200ms and 2000ms. like 10 to 20x difference. I observe this for almost all my models.  I have tried to record a screencast of exactly steps that I followed https://share.descript.com/view/4UBmlV8jmqd. All these experiments were on chrome and on mac with newest tfjs build,"['I have simplified it even more. I am using tfhub mobilenet and basic predict https://github.com/rohanmuplara/tester/tree/test_tfjs2/graph and am noticing 10x difference.I was wondering any suggestions how to reduce this warmup time. Also; when does this warmup copying to the gpu terminate. From my observation; reloading the page causes it to be slow again. Let say I have an iframe same domain on many pages. Is there a way to take advantage of this prerunning one page when another users switches to another page but the same iframe will it work? Is there a way to cache stuff in gpu?Thanks;Rohan====='; 'Sometimes `async` is an interesting source of non-determinism. I also noticed that it appears you are getting the Tensor but never actually getting the data out of it - which is an important step that takes time. What happens if you run the following code for benchmark.js? For me times increased; times were already fairly smooth anyways. (FF87; Win 10)```function runModel(model; tensors; returnTensorReferences ) {    let num_outputs = model.outputs.length;    const predictionsTensor =  model.predict(tensors).dataSync();    return predictionsTensor;}async function benchmarkInput (model_path; tensors; num_runs) {    console.time(""model loading time"");  let model = await tf.loadGraphModel(model_path; { fromTFHub: true });  console.timeEnd(""model loading time"");  console.time(""first prediction"");  const predictions = runModel(model; tensors; false);  console.timeEnd(""first prediction"");  let subsequent_times =new Float32Array(num_runs - 1);  for (let i = 0; i < num_runs - 1 ; i++) {    let begin= window.performance.now();    const predictions = runModel(model; tensors; true);    let end= window.performance.now();    let time = (end-begin) ;    subsequent_times[i] = time;  }  console.log(""subsequent predictions are in ms""; subsequent_times);  console.log(""the average of the subequent predictions are""; average(subsequent_times));}function average(array) {    let average = array.reduce((a; b) => a + b) / array.length;    return average;}function benchmarkInputDefininedInCode() {    let tensor1 = tf.ones([224; 224;3]);    tensor1 = tensor1.expandDims(0);    benchmarkInput(""https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/feature_vector/2/default/1""; [tensor1]; 100);}benchmarkInputDefininedInCode();```====='; ""@ wingman-jr-addon. Totally get your point about datasync. If you look at first post above https://github.com/tensorflow/tfjs/issues/4907#issue-851978678; there is a more complicated setup where I do do datasync. I just didn't do datasync above to make it easier; ie is syncing time causing delays. I do agree subsequent iterations are really fast and I tried your setup above and doesn't really make a difference.I do agree that subsequent times are good and only really considering the first time.   The problem for me is the first iteration time.  It is (10 to 20 more times slower than subequent runs).  My two questions are a. https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html this is way slower(10-20x)  than I get in benchmarking tool for first run.  Subsequent runs in my setup are relative same time as benchmarking tool but the first one don't agree. and b.  I was wondering any suggestions how to reduce this warmup time. Also; when does this warmup copying to the gpu terminate. From my observation; reloading the page causes it to be slow again. Let say I have an iframe same domain on many pages. Is there a way to take advantage of this prerunning one page when another users switches to another page but the same iframe will it work? Is there a way to cache stuff in gpu?=====""; '@rohanmuplara  Ah; well if you\'re wondering about the specifically why the first inference is so slow; that was discussed over in #1715 - basically the shaders are compiling if you\'re using WebGL the first time through.  As a side note: try the WASM backend as a comparison once; but be aware performance differences on backends vary greatly from machine to machine. As an example; WASM is much slower on my machine the but first inference has very little penalty.Regarding caching on the GPU ... Well; maybe if you were able to do some type of communication between web pages and indicated that one was the ""server"" and the others decided to become ""clients"" when they opened and detected there was already a ""server"" present.====='; ""@wingman-jr-addon My question is twofold a. what is the difference between the benchmark tool and my setup The problem for me is the first iteration time. It is (10 to 20 more times slower than subequent runs). My two questions are a. https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html this is way slower(10-20x) than I get in benchmarking tool for first run. Subsequent runs in my setup are relative same time as benchmarking tool but the first one don't agree. on b.  I do agree first inference on wasm is quicker and than much slower for subsequent runs.  My question is what the default behavior of tfjs in terms of reloading the page. Additionally; in tfjs; is there a way to cache the shaders compilation or let the tfjs code know that the tfjs shaders are already present. =====""; '@rohanmuplara I see; sorry for being dense.Well; I may be able to solve part of your mystery but not all of it. If I\'m not mistaken; the benchmark may actually be doing a prediction inside the model load itself. From https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html :```    async function loadModelAndRecordTime() {      updateStateFromURLState();      const benchmark = benchmarks[state.benchmark];      state.modelType = benchmark[\'type\'];      state.isModelChanged = false; // used to clean the performance history      if (benchmark[\'load\'] == null) {        throw new Error(`Please provide a load method for \'${state.benchmark}\' model.`);      }      await showMsg(\'Loading the model\');      let start = performance.now();      const inputSize = parseFloat(state.inputSize);      model = await benchmark.load(inputSize; state.architecture; state.inputType);      state.inputs = [];      if (model.inputs) {        // construct the input state for the model        for (let modelInputIndex = 0; modelInputIndex < model.inputs.length; modelInputIndex++) {          let modelInput = model.inputs[modelInputIndex];          if (modelInput.shape == null) continue;          let shape = modelInput.shape.map(e => e == null ? -1 : e);          state.inputs.push({            name: modelInput.name;            shape: [...shape];            dtype: modelInput.dtype;            range: [0; 1000]          });        }      }      predict = benchmark.predictFunc(inputSize);      const elapsed = performance.now() - start;      await showMsg(null);      appendRow(timeTable; \'Model load\'; printTime(elapsed));    }```Now looking at your video I see that the model load time is still too small to account for this; and I see similar for my own models (<200ms). Notably; it also does not match my experience in using the model in my own context either: I expected load plus first inference times of >10 seconds.I\'m pretty sure this is an artifact of the benchmark tool somehow because if I load the standard `mobilenet_v2` benchmark the times are much longer than `custom` despite the model being ""smaller"" than my model. Do you experience an unexpected difference in timing between your custom benchmark and `mobilenet_v2`? I\'m not sure how big your model is supposed to be.One more thing I find that makes me suspicious of the benchmark. I watched your video closely and while the warmup time is ~150ms; it actually looks like it was working from about 1:49 to 1:53; which would be consistent with your ~3500ms in your own code. So... I think the benchmark may have a bug?====='; ""Hey man no need to apologize. I am super appreciative of all your help! Yes in the benchmark tool I get this discrepancy every time. I used a custom model (have tried at least 20.)  and have observed this error in all of them.  I think you are correct that line shouldn't be there or more documentation be around it. I think the problem is there is no await for that prediction. Please see this video https://share.descript.com/view/0Fqye7z8hGW where I try to explain the bug. I am very unsure to be clear.So I just want to be clear in my own javascript setup(non-benchmark tool) this is the expected behavior and this is how long it is supposed to take. Is there any good workaround for webgl. Is there a way to preserve the textures in gpu(caching whatever) after page reload or a new page with same iframe enabled on both tabs. I will be frank I don't much about this so any help here would be appreciated about how tfjs behaves and how to optimize this=====""; ""@rohanmuplaraNo idea why benchmarking tool would result such a huge difference on first inference other than minor difference that:a) benchmark tool uses `model.executeAsyc()` for custom graph-based models and `model.predict()` only for layers-based models     while it uses `model.predict()` for all predefined modelsb) benchmark tool uses `tf.randomNormal()` to setup input tensor (vs your use of `tf.ones()`)On the topic of GL shader caching; this gets tricky:- TFJS maintains `numBytesAllocated` and if this is higher than threshold;    it simply calls GL function `unbindColorTextureFromFramebuffer`    However:  - Default threshold is infinite by default      (see `WEBGL_FLUSH_THRESHOLD` and `WEBGL_DELETE_TEXTURE_THRESHOLD`)      So that's not an issue (unless you're low on GPU memory)    - TFJS cannot maintain allocation table between page refreshes or between different iframes      There is simply no way to do it in JS due to page isolation enforced by all modern browsers      Just check `tf.engine().state.numBytes`; `tf.engine().backendInstance.numBytesInGPU` and `tf.engine().backendInstance.textureManager.numBytesAllocated`      values on page refresh - it's all zeroes- After `unbind`; it's up to the browser to perform actual GPU memory garbage collection    I've looked around and there is no way to tune it    (and different browsers do behave differently; for example Firefox is much more aggressive than Chrome)Now; one way that comes to mind would be to try save entire state of the `tf.engine().backendInstance.textureManager`  (bit more complicated as most values are read-only and would need to access them on a lower level)  to somewhere like LocalStorage in browser and upon page load to restore it - but that would get very messy very fast  And there is still no guarantees when browser's garbage collection would kick in as there would always be a delay between page load and before state is restored; so should probably download all GL textures; save them as well and restore them on load.=====""; ""@vladmandic thanks for all your points again. not sure if you care about the details but I **think not sure ** issue is https://share.descript.com/view/0Fqye7z8hGW. the load model calls (predictFunction) with no await and in custom models these are async (so they return once executeSync is called not returned) so this time doesn't get accounted for. On the point of optimization; I get all your points about multiple tabs and not having reference.  I don't 1. On the textureManager suggestion writing to disk; is time consuming part creating the gl textures from the model or is it copying to the gpu(this impacts whether writing to local storage is helpful.) 2. Could we maybe precompute these ie maybe there is a few differences per browser; but precompute this beforehand offline and load this directly from cloud storage. 3. changes required to tfjs: In addition to making some of these writeonly; I guess some code would have to be changed in tfjs to not reallocate and compile everything to the gpu when calling predict for the first time and to use this texture manager. . =====""; '`predictFunction` has variable definition:- for predefined models it\'s simply `model.predict()` which is a sync call- for custom models; if model is of type layers; it\'s also just `model.predict()`- for custom models; if model is of type graph; it wraps it in a try/catch block trying `model.execute()` (which is also a sync function) first and `await model.executeAsync()` in catch block if model fails because it has any async ops  (which does mean that for models that have async ops; it will try twice and totally skew results.)Regarding ""optimizations""; yes; I think it would be much faster to store them to disk and reload them - but the scope is massive; it would pretty much become a new type of a precompiled model  Need to download all precompiled shaders and textures after the first run and then restore them as part of model loading. And not just GL save/restore; but also make sure that TFJS state (textureManager) is valid.Why do I think it would be much faster?a) initial compile of shaders is time consumingb) uploading unchanged parts of textures is very time consuming    (just try setting WEBGL_DELETE_TEXTURE_THRESHOLD=0 so textures are deallocated on each frame and see the difference)Also note that a shader code has a lot of conditional statements that test GPU GL capabilities and use appropriate functions accordingly; so such ""precompiled"" model would not be a generic WebGL model and would only work on newer GPUs (and on newer browsers and with modern drivers)I love the idea; but I think the scope is just massive...If I have to look forward to something; it would be the `WebGPU` backend - currently it\'s in early stages of development (and it only works on debug versions of browsers); but it could help significantly in the future.====='; ""Yes I agree with everything with you're saying.   As you mentioned; in custom graph models is as async functions because it is calling executeAsync.  Although there is an await within this function; there is no await outside it I think main culprit is this line   https://github.com/tensorflow/tfjs/blob/38f8462fe642011ff1b7bcbb52e018f3451be58b/e2e/benchmarks/local-benchmark/index.html#L469. So in the case of custom graph models because it is an async function(using await internally) and there is no await outside it behaves unexpectedly.Your point on the webgpu makes  sesnse=====""; '`predictFunc` would not return `timeInfo` without await - see :```jstimeInfo = await timeInference(() => predict(model); numRuns);```where `predict` is:```js    const start = performance.now();    const res = await predict();    const elapsedTime = performance.now() - start;    times.push(elapsedTime);```where this `predict` is a conditional function that uses `model.predict()` or `model.execute()` or `model.executeAsync()`also; initial inference time is just `timeInfo.times[0]`; there is no other handling for it.====='; ""Sure I think the point is that predict is called in the load function itself so the warmup time is actually the second time it is called which is a little unintuitive. In this call; there is no await used so for custom models this time of the first run isn't even counted in the load function. Ihttps://github.com/tensorflow/tfjs/blob/38f8462fe642011ff1b7bcbb52e018f3451be58b/e2e/benchmarks/local-benchmark/index.html#L469=====""; ""possibly - i never relied on that e2e benchmark other when tfjs staff asked me to run it - i prefer to run my own benchmarks  anyhow; the core here is slow initial inference when using `webgl` backend and we've covered that  =====""; '@rohanmuplara @vladmandic @wingman-jr-addon great discussion here; seems the inference time measurements are bit confusing here. I Agree we should not pack the first inference into the model loading and there might be a bug in measuring as well. We will address those.====='; '@vladmandic @pyu10055 @wingman-jr-addon thanks for all your help!====='; ""@vladmandic @pyu10055 @wingman-jr-addon https://share.descript.com/view/YT5OGQnajc3. To be clear; I only care about first time inference speed.  I noticed if I have two models with same architectures; the second one's (first time) inference is really fast. I was wondering why that happened and how to reason about it. I believe that they have some weights that are similar and some weights that are different. I have a pipeline of 5 models so was wondering stuff such as if they have same exact architectures/different weights does it help with first time inference speed.  https://github.com/rohanmuplara/tester/blob/tfjs_weird/graph/stitched.js=====""; ""I was pretty sure I'd read someplace that the shader compilation happens the first time through. If so; I would expect that the weights are only data rather than a difference in shader; so that would align with your experience. I'm not familiar with the guts though.As an experiment; I suspect that with the WASM backend you will likely not see as much of a difference between model runs.=====""; '@wingman-jr-addon I am not too interested in wasm backend as it is significantly slower. So my question on your response is a. I had two different model references so somehow either tfjs or gpu have to figure out that these are the same b. when does it do this; ie if models have on more layer that is different? like how exactly same do the architectures have to be; trying to get intuition on this.  Who would be best person to ping about this?====='; ""it's not per-model or per-layer; it's per kernel op - each kernel op is compiled into a shader.  so there is a lot that can be reused between models  and regarding weights; note that although that bin files may look very different; a lot of models start in the same place (same base weights which are only added on during training)  so if models are based on the same architecture (e.g. mobilenet v2 being extremely common starting point); they likely share >50% of weights as well  =====""; '@vladmandic quick follow up. a. who does this kernel op duplication check is it it tfjs code/ or gpu? b. totally agree that weights are very similar; ie the architecture I used had was 99% mobilenet which by default loads pretrained weights;  I was wondering if the weights were different; do you still get savings. If I have a pipeline of 5 models; they will all need to have different weights(they do different things) but if architectures are same do I get savings====='; ""it's not deduplication; it's simple compile of any used op on its first use - then it's registered in tfjs as existing so whoever calls it next; doesn't need to compile it again. that can be second usage of the same op from the same model or different model; doesn't matter. and there is always just one implementation of any given op - just try importing multiple instances of tfjs and you'll get tons of warnings in the console about 'op already registered'regarding weights; they are not a monolithic thing; they are extracted as needed from a weights file to be used by a specific ops.  and when an op is compiled to a gl shader and it it has weights as param; those weights are uploaded as a gl texture and to a gpu. and tfjs maintains a map of uploaded textures. so if a different model has 50% of different weights; but 50% of same weights as they are inherited from a common model; you do get significant saving.to summarize - you get savings on a) compiling ops as shaders (which is irrelevant of weights); b) uploading weights as textures (which do must match)*disclaimer: all this is unofficial and comes from my experience with tfjs*=====""]",Slow Execution,Poor Performance,Incorrect Code Logic,WebGL,Backend,change benchmark,change benchmark,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/4892,conv2dTranspose gives different resutls on WebGL and WASM backends,3,closed,2021-04-01T09:59:13Z,2021-04-24T15:19:05Z,"**System information**- TensorFlow.js version: 3.2- Browser version: Chrome 89 / pop! OS 20.04 LTSOutput of the [tf.layers.conv2dTranspose](https://js.tensorflow.org/api/latest/#layers.conv2dTranspose) on WebGL backend is different from WASM backend even for simple cases. Other layers like conv2d; upsample; etc work fine (with very small numerical differences).Simple reproduction code:```javascriptvar cfg = {    filters: 1;    kernelSize: 2;     strides: 2;     padding: ""same"";     batchInputShape: [1; 144 ; 256; 3];    useBias: false;    kernelInitializer: 'ones'}async function main() {    await tf.setBackend('webgl');    const data_from_webgl = await evaluateModel();    await tf.setBackend('wasm');    const data_from_wasm = await evaluateModel();    console.log(data_from_webgl);    console.log(data_from_wasm);}async function evaluateModel() {    const model = tf.sequential();    model.add(tf.layers.conv2dTranspose(cfg));    const prediction = tf.tidy(() => {        const imgTensor = tf.randomNormal([1; 144; 256; 3]; 0; 1; 'float32'; 11111);        const y = model.predict(imgTensor);        return y;    });    return prediction.data();}main();```","['@drnemor I am curious if you are using tf.randomNormal; the input would change on every execution; it will lead to different output.====='; ""@pyu10055 I used the same random seed; so randomNormal gives same tensor on each execution regardless of chosen backend. You can check that by replacing ```model.add(tf.layers.conv2dTranspose(cfg)) ``` with ```model.add(tf.layers.activation({batchInputShape: [1; 144 ; 256; 3]; activation: 'linear'}))```. This gives exactly the same tensors on both backends. Also; you can check other layers like ```tf.layers.conv2d```; they also produce the same results; but not ```conv2dTranspose```. =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4892"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4892"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,type replacer,Replace data Shape/type,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.0""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4883,Align NaN propagation behavior with TF for Max/Min ops,1,closed,2021-03-29T20:48:12Z,2021-05-04T03:31:35Z,"Since TensorFlow 2.4.0; the NaN will always propagated for Max/Min ops; we need to align TFJS with TF.```>>> tf.__version__'2.4.1'>>> x = tf.constant([2; float(""nan""); 2]; dtype=tf.float32)2021-03-29 10:24:35.228622: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices; tf_xla_enable_xla_devices not set2021-03-29 10:24:35.229125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.>>> a = tf.raw_ops.Max(input=x; axis=0)>>> a<tf.Tensor: shape=(); dtype=float32; numpy=nan>```","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4883"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4883"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Wasm,Backend,condition replacer,condition replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3"",
    ""specific_type"": ""D.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4852,Unknown op 'IsNan'. File an issue at https://github.com/tensorflow/tfjs/issues so we can add it; or register a custom execution with tf.registerOp(),2,closed,2021-03-22T12:09:27Z,2021-03-23T17:46:21Z,"tfjs.js:41127 Uncaught TypeError: Unknown op 'IsNan'. File an issue at https://github.com/tensorflow/tfjs/issues so we can add it; or register a custom execution with tf.registerOp()    at tfjs.js:41127    at eM (tfjs.js:41129)    at tfjs.js:41436    at tfjs.js:9567    at e.t.scopedRun (tfjs.js:9576)    at e.t.tidy (tfjs.js:9560)    at dx (tfjs.js:13205)    at e.t.execute (tfjs.js:41422)    at e.t.execute (tfjs.js:42063)    at e.t.predict (tfjs.js:42040)Model: centernet_mobilenetv2_fpn_kptstf version: 2.2.0tfjs version: 2.1.0my code:function abc(){		if(model !=''){			img = tf.browser.fromPixels(video);			const smalImg = tf.image.resizeNearestNeighbor(img; [512; 512]);			const resized = tf.cast(smalImg; 'float32');			t4d = tf.tensor4d(Array.from(resized.dataSync());[1;512;512;3])			var predictions=  model.predict(t4d).data();			console.log(""prediction:""; predictions)			let x = predictions[0]	        let y = predictions[1]	        let width = predictions[2]	        let height = predictions[3]	        drawRectangle(boxcanvas; x; y ;width; height);			}					requestAnimationFrame(abc);	}","['Related PR has been merged and will be available in next stable release; thank you.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4852"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4852"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4844,"When in ""webgl"" backend and image width is odd; the second run of tf.conv2d() may be wrong.",5,closed,2021-03-20T12:48:53Z,2021-07-20T00:09:53Z,"**System information**- OS Platform and Distribution: Chrome OS; Android- Mobile device (the issue happens on mobile device; too): ASUS ZenPhone Max Pro- TensorFlow.js installed from: script link- TensorFlow.js version: 3.2.0- Browser version:  89.0.4389.82 (Stable) (64 bits)**Describe the current behavior**- When tf.gtBackend() is ""webgl"".  - If the width of the input is even; the result of tf.conv2d() is correct.  - If the width of the input is odd; the result of first time tf.conv2d() is correct. But the result of  second times run may be wrong.- When tf.gtBackend() is ""wasm"" or ""cpu"".  - The above problem seems not existed.**Describe the expected behavior**- No matter which backend; what kinds image width (even or odd); how many times to run tf.conv2d(); the result should be same and corrrect.**Standalone code to reproduce the issue**- https://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02- (The code could be pasted into any example console box of Tensorflow.js API (https://js.tensorflow.org/api/3.2.0/) web page to run.)","['I was able to reproduce the same in MAC book pro.cc @lina128 ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4844"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4844"">No</a>====='; 'Thank you all efforts for this strange problem. However; re-try the test codes in the gist (https://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02) in 3.8.0 tensorflow.js (https://js.tensorflow.org/api/3.8.0/) example console box. It seems that the problem still exists.====='; 'Hi @ColorfulCakeChen; the fix has not been released yet. It will be released in 3.9.0.====='; ""> Hi @ColorfulCakeChen; the fix has not been released yet. It will be released in 3.9.0.Oh. Thanks. It's my fault too impatient. XD=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,WebGL,Backend,condition replacer,condition replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4843,tf.conv2d produces different results for browser and node,6,closed,2021-03-20T01:36:53Z,2021-04-27T15:38:01Z,i've been chasing down why same image model (object detection) has slightly different results in browser and node environments and it comes down to results of `tf.conv2d` being slightly different for the exactly same inputs.in browser; `cpu`; `webgl` and `wasm` backends produce identical results (and WEBGL_CONV_IM2COL has no effect).but `tfjs-node` using `tensorflow` backend produces different result.example code:```jsconsole.log('Input:'; x.shape; x.size; 'sum:'; x.reshape([786432]).sum().dataSync()[0]); // input does not change (checked values)console.log('Filter:'; params.filters.shape; params.filters.size; 'sum:'; params.filters.reshape([864]).sum().dataSync()[0]); // params do not change (checked values)console.log('Strides'; strides);let out = tf.conv2d(x; params.filters; strides; 'same');console.log('Conv2d 1st 5 values:'; out.shape; out.size; out.dataSync().slice(0; 5)); // output has different values!console.log('Conv2D sum of all values:'; tf.reshape(out; [2097152]).sum().dataSync()[0]); // silly sum just to see how much results diverged```browser output:```logInput: [ 1; 512; 512; 3 ] 786432 sum: -631754.625Filter: [ 3; 3; 3; 32 ] 864 sum: 0.07897007465362549Strides [ 2; 2 ]Conv2d 1st 5 values: [ 1; 256; 256; 32 ] 2097152 Float32Array(5) [ 0.02585916966199875; 0; 0; 0; 0 ]Conv2D sum of all values: -23342.779296875```node output:```logInput: [ 1; 512; 512; 3 ] 786432 sum: -631754.625Filter: [ 3; 3; 3; 32 ] 864 sum: 0.07897007465362549Strides [ 2; 2 ]Conv2d 1st 5 values: [ 1; 256; 256; 32 ] 2097152 Float32Array(5) [ 0.026323730126023293; 0; 0; 0; 0 ]Conv2D sum of all values: -24542.615234375```you can see that value of just first entry is already different and that a simple checksum is off by ~1%environment: tfjs 3.3.0 on chrome 89 and ubuntu 20.10,"['@vladmandic I think this might be related to the input data; can you verify the input are the same for node and browser? I suspect fromPixels and decodeJPeg might produce different pixel values.====='; ""@pyu10055 that's the first thing i've thought of as well :)and yes; `decodeJpeg` and `fromPixels` do produce different results - specifically; RGB values in `fromPixels` are offset by +1  i've also double-checked behavior of `alignCorners` and similar items when performing `resizeBilinear`  but i've handled that and that's why i'm printing the checksum of the input (after normalization) now - to confirm input is 100% identical  (if there were any differences; I'd have implemented something like `canvas.js` decoding which is uniform on both platform)=====""; '@vladmandic the WebGL has precision loss when stored on texture; it is usually rather small. The input sum is negative seems to be weird; is it overflowing already?====='; ""@pyu10055 > the WebGL has precision loss when stored on texture; it is usually rather smallThe thing is `WebGL` and `WASM` produce results identical up to 5th decimal point (after that it's up to WebGL precision loss)  But `tfjs-node` produces results which are ~2-5% different than either `WebGL` or `WASM` which is not small> The input sum is negative seems to be weird; is it overflowing already?Input here is just an image resized to 1x512x512x3 and normalized  Sum is just a (very) cheap way to do a hash to make sure inputs are same; but given the size of the array no wonder its overflowing.But...I've just tried with *TFJS 3.5.0* where `tfjs-node` ships with **TF2** and difference is almost gone  (sum of conv2d values now shows divergence of ~0.15% - that is at least 25x improvement)  (and more importantly; model predictions actually match)So I guess bug was in TF1 implementation of `conv2d` - and finally updating TFJS to use TF2 resolved this issue as wellFeel free to close the issue=====""; 'that is great to know; thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4843"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4843"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Untimely Update,TF(CPU),Backend,update tensorflow.so,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,B.3
https://github.com/tensorflow/tfjs/issues/4842,Missing kernel implementations after bundle optimization,4,closed,2021-03-19T19:14:37Z,2021-03-28T20:30:46Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Oly tested on Windows; Chrome 89.0.4389.90- TensorFlow.js installed from (npm or script link): from npm- TensorFlow.js version: 3.3.0I'm trying to optimize the bundle of the library that I'm creating which is using TensorFlow. I followed the instructions from https://www.tensorflow.org/js/tutorials/deployment/size_optimized_bundles.Here is the content of my custom_tfjs_config.json file:```{    ""kernels"": [        ""FromPixels"";        ""Reshape"";        ""ResizeBilinear"";        ""Cast"";        ""RealDiv"";        ""PadV2"";        ""FusedConv2D"";        ""fusedConv2d__op"";        ""FusedDepthwiseConv2D"";        ""fusedDepthwiseConv2d__op"";        ""Add"";        ""StridedSlice"";        ""Multiply"";        ""Pack"";        ""Conv2DBackpropInput"";        ""Relu"";        ""Concat"";        ""Max"";        ""Sub"";        ""Exp"";        ""Sum"";        ""Identity"";        ""Slice""    ];    ""backends"": [        ""cpu""    ];    ""models"": [""./custom_tfjs/model.json""];    ""outputPath"": ""./custom_tfjs"";    ""forwardModeOnly"": true}```tfjs-custom-module will then generate 2 files.custom_tfjs_core.js with the following content:```import {registerKernel} from '@tensorflow/tfjs-core/dist/base';import '@tensorflow/tfjs-core/dist/base_side_effects';export * from '@tensorflow/tfjs-core/dist/base';```and custom_tfjs.js with the following content:```import {registerKernel} from '@tensorflow/tfjs-core/dist/base';import '@tensorflow/tfjs-core/dist/base_side_effects';export * from '@tensorflow/tfjs-core/dist/base';export * from '@tensorflow/tfjs-converter';//backend = cpuexport * from '@tensorflow/tfjs-backend-cpu/dist/base';import {fromPixelsConfig as FromPixels_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/FromPixels';registerKernel(FromPixels_cpu);import {reshapeConfig as Reshape_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Reshape';registerKernel(Reshape_cpu);import {resizeBilinearConfig as ResizeBilinear_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/ResizeBilinear';registerKernel(ResizeBilinear_cpu);import {castConfig as Cast_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Cast';registerKernel(Cast_cpu);import {realDivConfig as RealDiv_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/RealDiv';registerKernel(RealDiv_cpu);import {padV2Config as PadV2_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/PadV2';registerKernel(PadV2_cpu);import {fusedConv2DConfig as FusedConv2D_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/FusedConv2D';registerKernel(FusedConv2D_cpu);import {fusedDepthwiseConv2DConfig as FusedDepthwiseConv2D_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/FusedDepthwiseConv2D';registerKernel(FusedDepthwiseConv2D_cpu);import {addConfig as Add_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Add';registerKernel(Add_cpu);import {stridedSliceConfig as StridedSlice_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/StridedSlice';registerKernel(StridedSlice_cpu);import {multiplyConfig as Multiply_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Multiply';registerKernel(Multiply_cpu);import {packConfig as Pack_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Pack';registerKernel(Pack_cpu);import {conv2DBackpropInputConfig as Conv2DBackpropInput_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Conv2DBackpropInput';registerKernel(Conv2DBackpropInput_cpu);import {reluConfig as Relu_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Relu';registerKernel(Relu_cpu);import {concatConfig as Concat_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Concat';registerKernel(Concat_cpu);import {maxConfig as Max_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Max';registerKernel(Max_cpu);import {subConfig as Sub_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Sub';registerKernel(Sub_cpu);import {expConfig as Exp_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Exp';registerKernel(Exp_cpu);import {sumConfig as Sum_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Sum';registerKernel(Sum_cpu);import {identityConfig as Identity_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Identity';registerKernel(Identity_cpu);import {sliceConfig as Slice_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/Slice';registerKernel(Slice_cpu);```When I try to compile using these custom files; I get an error because of this:```import {fromPixelsConfig as FromPixels_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/FromPixels';```This file doesn't exist.I also tried to use the webgl backend instead. This generates a similar content pointing to the webgl implementations of all those kernels. FromPixels is OK since the file exists.But then I get some other errors linked to FromPixels. At some point in my code; I do the following:```const input = browser.fromPixels(imageData);const resizedInput = image.resizeBilinear(input; [this._modelConfig.inHeight; this._modelConfig.inWidth]);const resizedInputFt = cast(resizedInput; 'float32');const batched = resizedInputFt.reshape([1; this._modelConfig.inHeight; this._modelConfig.inWidth; 3]);```With the optimized version using webgl backend; I get an error when reshape is being called because the function doesn't exist. When I look at the ""input"" object; I can see that the reshape functions is missing in __prototype__ which is not the case when I use the non optimized version of TensorFlow. Any idea why?Also; going back to the cpu version. If I remove those 2 lines:```import {fromPixelsConfig as FromPixels_cpu} from '@tensorflow/tfjs-backend-cpu/dist/kernels/FromPixels';registerKernel(FromPixels_cpu);```I can compile but I get the same error as with webgl.","['Thank you for the report CyrilGC! I will take a look at this issue this week.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4842"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4842"">No</a>====='; ""Hi CyrilGC;I pushed a fix that will skip kernels that don't exist in certain backend (like FromPixels in the cpu backend). For now; please just comment it out like you did to workaround this.The error about reshape: I suspect it is related to the chained ops. Could you please double check the chained ops section here: https://www.tensorflow.org/js/tutorials/upgrading_to_3_0#users_of_tensorflowtfjs-core. Thank you!=====""; ""Great; I'll keep that workaround until the fix is released. Thank you for the note about the chained op. It is working properly again and the output is now 900KB smaller.=====""]",Initialization Faliure,Build & Initialization Failure,Unimplemented Operator,CPU,Backend,add check,Fix environment adaptability,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",C,A.1
https://github.com/tensorflow/tfjs/issues/4838,Can not convert TensorFlow model to TensorFlowJS: Unsupported Ops in the model before optimization,29,closed,2021-03-19T15:14:11Z,2021-06-24T15:42:55Z,Hi; I work on VS Code and we are trying to use TensorFlow for automatic programming language classification based on file content.To be concise; we need this working in a browser; thus we would like to use TensorFlowJS; and we would like to reuse this TensorFlow model https://github.com/yoeo/guesslang/tree/master/guesslang/data/modelWe are trying to convert this Model to a TensorFlowJs model using your conversion scripts and we are hitting the following issue:```$ tensorflowjs_wizard2021-03-17 23:39:53.238316: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-03-17 23:39:53.238359: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Welcome to TensorFlow.js Converter.? Please provide the path of model file or the directory that contains model files.If you are converting TFHub module please provide the URL. /workspaces/tfjs-notebooks/master/guesslang-master/guesslang/data/model/saved_model.pb? What is your input model format? (auto-detected format is marked with *) Tensorflow Saved Model *? What is tags for the saved model? serve? What is signature name of the model? signature name: predict? Do you want to compress the model? (this will decrease the model precision.) No compression (Higher accuracy)? Please enter shard size (in bytes) of the weight files? 4194304? Do you want to skip op validation?This will allow conversion of unsupported ops;you can implement them as custom ops in tfjs-converter. No? Do you want to strip debug ops?This will improve model execution performance. Yes? Do you want to enable Control Flow V2 ops?This will improve branch and loop execution performance. Yes? Do you want to provide metadata?Provide your own metadata in the form:metadata_key:path/metadata.jsonSeparate multiple metadata by comma.? Which directory do you want to save the converted model in? /workspaces/tfjs-notebooks/src/new_model? The output already directory exists; do you want to overwrite it? Yesconverter command generated:tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=predict --strip_debug_ops=True --weight_shard_size_bytes=4194304 /workspaces/tfjs-notebooks/master/guesslang-master/guesslang/data/model /workspaces/tfjs-notebooks/src/new_model15:34but alas...ValueError: Unsupported Ops in the model before optimizationSparseFillEmptyRows; StringToHashBucketFast; SparseReshape; SparseSegmentSum; SparseSegmentMean```1. Is it even possible to convert this model to TensorFlowJS?2. Do we have something misconfiguredI am a TensorFlow beginner so I apologise for not the nicest issue. Any help is appreciated. Thank youfyi @tylerLeonhardt,"['Hi @isidorn; based on the log; this model uses SparseTensor; currently it is not supported by TensorFlow.js. We will investigate how much effort is needed to add that support; and get back to you. It would also be good to understand your timeline for this feature? Thanks====='; '@pyu10055 thanks for your reply! We do not have a strict timeline; we just think it would be a very cool feature to have. Especially for new users who do not know how to set a language mode; so automatic classification would help a lot.Our plan was to experiment with this in April; but we can push it out to May; June or whenever this is possible.In case you are more interested about this feature on the VS Code you can take a look at this issue https://github.com/microsoft/vscode/issues/118455 it has a proof of concept demo and some discussion and a linked PR====='; '@isidorn Nice; we are heavy users of VSCode; thank you for the great tool. We have done some research on the SparseTensor ops; I think it will be fairly straightforward for us to add those missing ops. Initial estimation would be around May timeframe.====='; '@pyu10055 fantastic news; thanks a lot 👏 I am really glad you like VS Code 😊 ====='; ""@isidorn My understanding is that VSCode is an electron app; we have node package tfjs-node that is built on top of tensorflow c++ runtime. And we have the [saved model direct execution API](https://blog.tensorflow.org/2020/01/run-tensorflow-savedmodel-in-nodejs-directly-without-conversion.html). With which you don't need to do the model conversion; and any saved model can be executed directly. I am wondering if that can get you started?=====""; 'VS Code is also used for GitHub Codespaces which is ""vscode in your browser"":![image](https://user-images.githubusercontent.com/2644648/112207981-0bf78d80-8bd5-11eb-853a-eaf44e63b018.png)So we can\'t assume we\'ll always have a node runtime.====='; ""@pyu10055 thanks and yes you are correct we are an Electron app; however as @TylerLeonhardt pointed out we also have to run in the Browser for various scenarios. Apart from that the new Electron that is coming out is changing it's architecture so that all renderer processes are node free; and only one shared process can have a node dependency. We would prefer to do the classification in the renderer where it will be used.So due to the above we would like to drop the node dependency.But you are also right; that was enough to get us started; so @TylerLeonhardt published a VS Code extension that uses the node dependency; so you can try it out in case you are interested https://marketplace.visualstudio.com/items?itemName=TylerLeonhardt.auto-languageHowever our final goal is to have this in VS Code core=====""; 'Just a heads up; my extension only works on linux as I never took the time to figure out how to properly distribute the native bits for each platform within the extension.====='; '@isidorn @TylerLeonhardt The extension looks very nice; and now we understood more about your use case; we will add those missing ops and keep you posted.====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Pls no bot 🙏====='; 'Just to check if there are any updates regarding this issue?Also adding @yoeo to the discussion; the original author of the Model we want to convert.Thanks!====='; '@isidorn All ops are supported; we are investigating some issues with the model execution; will try to update here as it progresses.====='; '@isidorn @TylerLeonhardt The execution issue has been fixed; and the model is compatible with TFJS now.you can try out the demo here https://storage.googleapis.com/tfjs-testing/guesslang-demo/index.htmlSince the model is fairly small and involves dynamic shaped string tensors; it is actually faster on CPU backend.Currently the demo is compiled with the latest code from the master branch of TFJS; we will release a new version next week; after that you should be able to use the npm package directly.====='; '@pyu10055 this is amazing; I just tried out the demo! We will look into this on the VS Code side next milestone; for more details on our side you can follow this issue https://github.com/microsoft/vscode/issues/118455Thanks a lot 👏 👏 ====='; '@pyu10055 do you think you can let us know when the npm package goes out?====='; '@TylerLeonhardt should be this week.====='; '@TylerLeonhardt @isidorn we have just released 3.7.0; please take a look at the updated [demo](https://storage.googleapis.com/tfjs-testing/guesslang-demo/index.html)====='; 'Super cool ✨ Next week I will try to look into integrating this into vscode; the week after I am on vacation; so I might look into it once I get back. Though maybe @TylerLeonhardt jumps on this in the meantimeShould we close this issue and continue the discussion in our repo? https://github.com/microsoft/vscode/issues/118455Thanks a lot 👏 ====='; 'Closing this issue since the model has been fully supported with 3.7.0; please keep us posted on your progresses. cheers; happy Friday.====='; ""@pyu10055 I'm curious; what did you use to convert the model into the tfjs model? tensorflow_wizard?=====""; '@TylerLeonhardt `tensorflow_wizard` should work; also there is `tensorflow_converter`; if you familiar with the params.====='; '@pyu10055 I\'m very curious what your `tensorflow_converter` command looked like while converting the model. I gave it a go just now and didn\'t have the same luck:```root@a4861c6ad41f:/guesslang/guesslang/data/model# tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=classification --strip_debug_ops=True --weight_shard_size_bytes=4194304 /guesslang/guesslang/data/model ./foo2021-06-11 04:33:01.965439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-06-11 04:33:01.965500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-06-11 04:33:03.004337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcuda.so.1\'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory2021-06-11 04:33:03.004400: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)2021-06-11 04:33:03.004426: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a4861c6ad41f): /proc/driver/nvidia/version does not exist2021-06-11 04:33:03.004720: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMATo enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.2021-06-11 04:33:03.202627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)2021-06-11 04:33:03.203916: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299965000 HzWARNING:tensorflow:Issue encountered when serializing global_step.Type is unsupported; or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing variables.Type is unsupported; or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.WARNING:tensorflow:Issue encountered when serializing trainable_variables.Type is unsupported; or the types of the items don\'t match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.2021-06-11 04:33:03.576848: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8; compute capability >= 0.0): 02021-06-11 04:33:03.577123: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session2021-06-11 04:33:03.584601: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn\'t exist in graphWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:394: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.Instructions for updating:This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:399: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py:857: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`Traceback (most recent call last):  File ""/usr/local/bin/tensorflowjs_converter""; line 8; in <module>    sys.exit(pip_main())  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 813; in pip_main    main([\' \'.join(sys.argv[1:])])  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 817; in main    convert(argv[0].split(\' \'))  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 804; in convert    weight_shard_size_bytes; metadata_map)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 533; in _dispatch_converter    metadata=metadata_map)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 691; in convert_tf_saved_model    metadata=metadata)  File ""/usr/local/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 154; in optimize_graph    \'; \'.join(unsupported))ValueError: Unsupported Ops in the model before optimizationReadVariableOp; OptionalFromValue; OptionalNone```I ran this in a fresh python:3.6-buster docker container.====='; 'I just published a new version of my extension:https://marketplace.visualstudio.com/items?itemName=TylerLeonhardt.auto-languageJust to ""get it working"" I pull in the model.json you have hosted:https://github.com/TylerLeonhardt/vscode-autountitledlanguage/blob/main/src/extension.ts#L97but interestingly; I did originally try downloading the `model.json` but it looks like `tf.loadGraphModel(` does not allow me to pass it a file path or a `file://` uri. It says the Uri is not absolute. Any ideas around this? Any other way to provide the model.json to tfjs?====='; '@tyler I just tried out your extension and this works great! Awesome job 👏 @pyu10055 thanks again for making this possible!I have commented with some ideas on what we should do in order to ship this to all our users ✨ https://github.com/microsoft/vscode/issues/118455====='; ""Also; @pyu10055 if you'd like me to open new issues on any of the above feel free to let me know. I'm happy to :)=====""; '@TylerLeonhardt you can use following command:```tensorflowjs_converter --input_format=tf_saved_model --skip_op_check model web_model```====='; '@TylerLeonhardt TFJS model loading API aims for browser usage; which uses fetch API under the hood. It does have a file loader; but it is only for node.js. For loading from local file system within vscode; you might need to implement the custom loader API.You can take a look at this doc https://www.tensorflow.org/js/guide/save_load====='; '@pyu10055 did you mean this section:![image](https://user-images.githubusercontent.com/2644648/123292423-e1fd8200-d4c7-11eb-93d2-0ac7c4d992e2.png)Looks like all of the links 404. Example: https://github.com/tensorflow/tfjs-core/blob/master/src/io/types.ts#L165~Have they moved somewhere else?~ahh... found ithttps://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/io/types.ts=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,change API,Replace API with another effective one,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unsupported Ops in the Model""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4824,Backend WASM does not implement kernel SparseToDense,1,open,2021-03-17T15:15:39Z,2021-06-03T16:15:38Z,attempting to run nanodet object detection model  converted from pytorch - conversion notes and example of model usage are at:  <https://gist.github.com/vladmandic/d0115e3899860391382d58550bd49b3f>```logengine.ts:540 Uncaught (in promise) Error: Kernel 'SparseToDense' not registered for backend 'wasm'    at Engine.runKernel (engine.ts:540)    at sparseToDense_ (sparse_to_dense.ts:88)    at sparseToDense__op (operation.ts:51)    at executeOp15 (slice_join_executor.ts:183)    at operation_executor.ts:92    at engine.ts:467    at Engine.scopedRun (engine.ts:478)    at Engine.tidy (engine.ts:465)    at tidy (globals.ts:192)    at operation_executor.ts:91```same graph model works without issues in nodejs with `tensorflow` backend and browser with `webgl` backendenvironment: tfjs 3.3.0 on ubuntu 20.10 with chrome 88,['cc @lina128 @jinjingforever ====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4811,TypeError: Cannot read property 'length' of null (dilations is null in depthwiseConv2dNativeBackpropInput_),8,closed,2021-03-14T21:54:51Z,2021-08-17T14:35:59Z,**System information**- From the [simple-object-detection](https://github.com/tensorflow/tfjs-examples/tree/master/simple-object-detection) example- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): tried both on Mac OS and Windows  and Windows WSL- TensorFlow.js installed from (npm or script link): npm library- TensorFlow.js version (use command below): tested on ^2.6.0 and ^3.3.0**Describe the current behavior**Playing with the [simple-object-detection](https://github.com/tensorflow/tfjs-examples/tree/master/simple-object-detection) example. Ran `npm run train` and the following happensStep 1: (Phase 1 of 2: initial transfer learning) - this runs without any problems```javascript  const tBegin = tf.util.now();  console.log(`Generating ${args.numExamples} training data...`);  const { images; targets } = await trainingData();  const { model; fineTuningLayers } = await buildObjectDetectionModel();  model.compile({ loss: customLossFunction; optimizer: tf.train.rmsprop(5e-3) });  model.summary();  // Initial phase of transfer learning.  // console.log('Images Shape:'; images.shape);  // console.log('Targets Shape'; targets.shape);  console.log('Phase 1 of 2: initial transfer learning');  await model.fit(images; targets; {    epochs: args.initialTransferEpochs;    batchSize: args.batchSize;    validationSplit: args.validationSplit;    callbacks: args.logDir == null ? null : tfn.node.tensorBoard(args.logDir; {      updateFreq: args.logUpdateFreq    })  });```Step 2: (Phase 2 of 2: fine-tuning phase) - this is where it breaks```javascript  // Fine-tuning phase of transfer learning.  // Unfreeze layers for fine-tuning.  for (const layer of fineTuningLayers) {    console.log(fineTuningLayers);    layer.trainable = true;  }  model.compile({ loss: customLossFunction; optimizer: tf.train.rmsprop(2e-3) });  model.summary();  // Do fine-tuning.  // The batch size is reduced to avoid CPU/GPU OOM. This has  // to do with the unfreezing of the fine-tuning layers above;  // which leads to higher memory consumption during backpropagation.  console.log('Phase 2 of 2: fine-tuning phase. Top Layer Name:'; topLayerName; 'Group Names:'; topLayerGroupNames);  await model.fit(images; targets; {    epochs: args.fineTuningEpochs;    batchSize: Math.round(args.batchSize / 2);    validationSplit: args.validationSplit;    callbacks: args.logDir == null ? null : tfn.node.tensorBoard(args.logDir; {      updateFreq: args.logUpdateFreq    })  });```I get this error after `model.fit(images; targets; {...` is executed:![image](https://user-images.githubusercontent.com/7416676/111085676-9bb28300-84d5-11eb-9cdf-af32be17e081.png)I debugged down a bit further - turns out that somewhere at the kernel input (I think??); it's giving a dilation value of `null` (should be `number` or `number[]`).![image](https://user-images.githubusercontent.com/7416676/111085448-ab7d9780-84d4-11eb-8226-fe246256050d.png)![image](https://user-images.githubusercontent.com/7416676/111085562-13cc7900-84d5-11eb-82f8-31eed68c301c.png)Please advise; I'm going off the tfjs-examples repo simple-object-dectectionhttps://github.com/tensorflow/tfjs-examples/blob/master/simple-object-detection/package.json...then just running `npm install` and `npm run train`,"[""Still broken; but found a workaround (though probably the model is not even transfer learning)1. train.js change the tensorflow import from `@tensorflow/tfjs` to `@tensorflow/tfjs-node`2. replaced `const topLayerGroupNames = ['conv_pw_9'; 'conv_pw_10'; 'conv_pw_11'];` to `const topLayerGroupNames = [ 'conv_pw_11'];`Both changes are required; one without the other leads to the same problem. I have no idea why this fixes it.=====""; '@opiepj #2 did it for me. You are my hero!====='; '@opiepj can we close this issue ? ====='; ""No it doesn't work; I had to keep layers 10-11 frozen... was only able to transfer learn on the 12th layer.Outputs are still wack =====""; '+1====='; 'After five month; anything new?====='; 'Related PR has been merged ; the change will be available in next release. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4811"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4811"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,variable replacer,variable replacer,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4806,[tfjs-react-native] Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'a.substr'),13,closed,2021-03-11T17:28:55Z,2021-04-03T09:40:59Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): `Ubuntu 20.04`- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: `Pixel_3a_API_30_x86.avd`- TensorFlow.js installed from (npm or script link):- TensorFlow.js version: `npm install @tensorflow/tfjs`; - CUDA/cuDNN version:**Describe the problem**I'm trying to run some sample function found in your documentation https://js.tensorflow.org/api/latest/#tf.LayersModel.fit in a react-native-app.if i run this example on the web executing `npm run web` and check the browser window it loads; it works fine.If however i run the same example by running `npm run web` then scan the QR code using my android phone; or if i hit `a` to run a android emulator; in both cases i get the following error. `[Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'a.substr')]`What is it about the function `do_training_example` that ive introduced that it does not like when running via the QR code on my phone or via the emulator. If i remove this function the error goes.```[Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'a.substr')]at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3754:8 in isMobileat node_modules/@tensorflow/tfjs-backend-webgl/dist/tf-backend-webgl.node.js:1043:17 in <global>at http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:161631:39 in getat http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:249141:41 in runWebGLProgramat node_modules/@tensorflow/tfjs-backend-webgl/dist/tf-backend-webgl.node.js:5217:12 in MathBackendWebGL.prototype.uploadToGPUat http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:249103:25 in <unknown>at [native code]:null in mapat node_modules/@tensorflow/tfjs-backend-webgl/dist/tf-backend-webgl.node.js:5016:8 in MathBackendWebGL.prototype.runWebGLProgramat node_modules/@tensorflow/tfjs-backend-webgl/dist/tf-backend-webgl.node.js:8060:37 in intat http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:163816:35 in kernelFuncat node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3199:17 in scopedRun$argument_2at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3011:12 in Engine.prototype.scopedRunat node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3195:12 in Engine.prototype.runKernelFuncat node_modules/@tensorflow/tfjs/dist/tf.node.js:3548:12 in f2at node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:24702:13 in tfc.tidy$argument_0at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3001:126 in scopedRun$argument_2at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3011:12 in Engine.prototype.scopedRunat [native code]:null in mapat node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3001:126 in scopedRun$argument_2at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3011:12 in Engine.prototype.scopedRunat node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:24838:87 in tfc.tidy$argument_0at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3001:126 in scopedRun$argument_2at node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3011:12 in Engine.prototype.scopedRunat http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:206783:36 in <unknown>at http://192.168.1.104:19001/index.bundle?platform=android&dev=true&hot=false&minify=false:199674:24 in stepat node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16694:22 in stepat node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16694:22 in stepat node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:16681:40 in fulfilledat node_modules/react-native/node_modules/promise/setimmediate/core.js:37:13 in tryCallOneat node_modules/react-native/node_modules/promise/setimmediate/core.js:123:24 in setImmediate$argument_0at [native code]:null in flushedQueueat [native code]:null in invokeCallbackAndReturnFlushedQueue```**Any other info / logs**You can see ive only added the function `do_training_example` and executing that within the `componentDidMount`App.tsc```import * as tf from '@tensorflow/tfjs';import '@tensorflow/tfjs-react-native';import React; { Component } from 'react';import { Text; View } from 'react-native';export default class App extends React.Component {    constructor(props: any) {        super(props);        this.state = {            isTfReady: false;        };    }    async componentDidMount() {        // Wait for tf to be ready.        await tf.ready();        // Signal to the app that tensorflow.js can now be used.        this.setState({            isTfReady: true;        });        await this.do_training_example();    }    do_training_example = async () => {        const model = tf.sequential({            layers: [tf.layers.dense({ units: 1; inputShape: [10] })]        });        model.compile({ optimizer: 'sgd'; loss: 'meanSquaredError' });        for (let i = 1; i < 5; ++i) {            const h = await model.fit(tf.ones([8; 10]); tf.ones([8; 1]); {                batchSize: 4;                epochs: 3            });            console.log(""Loss after Epoch "" + i + "" : "" + h.history.loss[0]);        }    }    render() {        return (          <View style={{              flex: 1;              justifyContent: ""center"";              alignItems: ""center""            }}>            <Text>Hello; world!</Text>          </View>        );    }}```","['I think so promise is here rejecting and you have to use catch for that may be that can be issue but further can you share the link so that I can see and I will give you update on that ====='; '@carrycooldude Ye the error message does say promise rejection; im wondering why. Sorry which link do you want me to share you with? To recreate it; you can just simply use a blank project with tf react native setup and then copy the code in App.js.Theres nothing else ive added; only the function `do_training_example` which is code i obtained from official docs https://js.tensorflow.org/api/latest/#tf.LayersModel.fit.Im assuming its something to do with `await model.fit`====='; '> @carrycooldude Ye the error message does say promise rejection; im wondering why. Sorry which link do you want me to share you with?> > To recreate it; you can just simply use a blank project with tf react native setup and then copy the code in App.js.> > Theres nothing else ive added; only the function `do_training_example` which is code i obtained from official docs https://js.tensorflow.org/api/latest/#tf.LayersModel.fit.Okay Let me Try====='; '@carrycooldude im still struggling with this; ive not found out why this is happening. Any ideas?====='; '@kaykhancheckpoint I was having the same issue with the last @tensorflow/tfjs version (3.2.0) . Downgrading to version 3.0.0 fixed the problem for me====='; '@dhiogocorrea Right; so this is a bug that needs to be fixed as oppose to something i am doing wrong in the setup/build process. ====='; 'Can confirm after downgrading @tensorflow/tfjs from 3.3.0 to 3.0.0 it works now.====='; ""Same here on `expo` while using `expo-gl`. It seems that `tensorflow > 3.0.0` have issues with environment detection.If you don't want to downgrade and you can wait until this is fixed; knowing your app is only going to run on mobile; you can place this code early in your program to avoid this error:```/** * Original functions throw an error in: * @tensorflow/tfjs-core/dist/tf-core.node.js:3749 * This may be related to TFJS as it for some reason tries to load node webgl * code instead of using expo-gl bridge. * Not sure how to figure it out so here's a workaround. */tf.device_util.isMobile = () => truetf.device_util.isBrowser = () => false```=====""; ""Yes true this is bug ; Version issueOn Sun; 14 Mar 2021; 20:38 Ralf Gutkowski; ***@***.***> wrote:> Same here on expo while using expo-gl. It seems that tensorflow > 3.0.0> have issues with environment detection.>> If you don't want to downgrade and you can wait until this is fixed;> knowing your app is only going to run on mobile; you can place this code> early in your program to avoid this error:>> /**>  * Original functions throw an error in:>  * @tensorflow/tfjs-core/dist/tf-core.node.js:3749>  * This may be related to TFJS as it for some reason tries to load node webgl>  * code instead of using expo-gl bridge.>  * Not sure how to figure it out so here's a workaround.>  */> tf.device_util.isMobile = () => true> tf.device_util.isBrowser = () => false>> —> You are receiving this because you were mentioned.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4806#issuecomment-798923670>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AJZ4ZSFVH74PFPSHKEGN3HTTDTGO3ANCNFSM4ZAWKYLA>> .>=====""; ""Thanks for the investigation; everyone! It looks like this is caused by the new `WEBGL_FLUSH_THRESHOLD` flag that increases Android webgl performance. [It makes a call to `device_util.isMobile()`](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/flags_webgl.ts#L203) to determine whether it should be enabled or not; and when that call is removed (and replaced with `return 1`); the react native backend works. I'm looking into why this call causes the error.=====""; ""for those wanting to use @tensorflow/tfjs-core 3.3.0 on react native here's a patch:```diff --git a/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js b/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.jsindex b058771..5d3b9ca 100644--- a/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js+++ b/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js@@ -3749;22 +3749;10 @@ function _isNavigatorDefined() {     return typeof navigator !== 'undefined' && navigator != null; } function isMobile() {-    if (_isNavigatorDefined()) {-        // tslint:disable-next-line:no-any-        var a = navigator.userAgent || navigator.vendor || window.opera;-        // tslint:disable-next-line:max-line-length-        return /(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i-            .test(a) ||-            // tslint:disable-next-line:max-line-length-            /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-/i-                .test(a.substr(0; 4));-    }-    return false;+    return true; } function isBrowser() {-    return (typeof window !== 'undefined' && window.document != null) ||-        //@ts-ignore-        (typeof WorkerGlobalScope !== 'undefined');+    return false; }  var device_util = {```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4806"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4806"">No</a>====='; ""Choosing a backend with`tf.setBackend('cpu')`solved the problem for me.=====""]",Reference Error,Crash,Incorrect Code Logic,Operator,API,type/env checker,Fix environment adaptability,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4803,[tfjs-react-native] Build and setup issue with camera,3,closed,2021-03-10T21:11:39Z,2021-03-11T17:40:18Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): `Ubuntu 20.04`- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: `Pixel_3a_API_30_x86.avd`- TensorFlow.js installed from (npm or script link):- TensorFlow.js version: `npm install @tensorflow/tfjs`; - CUDA/cuDNN version:**Describe the problem**I am following the steps to setup tfjs react native; ive gone from step 1 - step 4 however i am blocked with an error message after i run when i use either `npm run web` or `npm run android`. The problem both relate to the camera; i have installed the prequisite `expo install expo-camera`.**Provide the exact sequence of commands / steps that you executed before running into the problem**1. `npm run web````Failed to compile./home/kay/kay/powerlevelai/mobileapp/node_modules/@tensorflow/tfjs-react-native/dist/camera/camera_stream.js 281:12Module parse failed: Unexpected token (281:12)You may need an appropriate loader to handle this file type; currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders|             const cameraComp = (|             //@ts-ignore see https://github.com/microsoft/TypeScript/issues/30650>             <CameraComponent key='camera-with-tensor-camera-view' {...(cameraProps)} ref={(ref) => (this.camera = ref)}/>);|             // Create the glView if the camera has mounted.|             let glViewComponent = null;```alternatively i have setup an android emulator using AVD.1. `npm run android````Deprecated Gradle features were used in this build; making it incompatible with Gradle 7.0.Use '--warning-mode all' to show the individual deprecation warnings.See https://docs.gradle.org/6.3/userguide/command_line_interface.html#sec:command_line_warningsFAILURE: Build failed with an exception.* What went wrong:Could not determine the dependencies of task ':app:compileDebugJavaWithJavac'.> Could not resolve all task dependencies for configuration ':app:debugCompileClasspath'.   > Could not find com.google.android:cameraview:1.0.0.     Required by:         project :app > project :expo-camera```**Any other info / logs****package.json**```{  ""main"": ""index.js"";  ""scripts"": {    ""android"": ""react-native run-android"";    ""ios"": ""react-native run-ios"";    ""web"": ""expo start --web"";    ""start"": ""react-native start"";    ""test"": ""jest""  };  ""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.14.1"";    ""@react-native-community/async-storage"": ""^1.12.1"";    ""@tensorflow/tfjs"": ""^3.3.0"";    ""@tensorflow/tfjs-react-native"": ""^0.5.0"";    ""expo"": ""~40.0.0"";    ""expo-camera"": ""~9.1.0"";    ""expo-gl"": ""~9.2.0"";    ""expo-gl-cpp"": ""~9.2.0"";    ""expo-splash-screen"": ""~0.8.0"";    ""expo-updates"": ""~0.4.0"";    ""react"": ""16.13.1"";    ""react-dom"": ""16.13.1"";    ""react-native"": ""~0.63.4"";    ""react-native-fs"": ""^2.16.6"";    ""react-native-gesture-handler"": ""~1.8.0"";    ""react-native-reanimated"": ""~1.13.0"";    ""react-native-screens"": ""~2.15.0"";    ""react-native-unimodules"": ""~0.12.0"";    ""react-native-web"": ""~0.13.12""  };  ""devDependencies"": {    ""@babel/core"": ""~7.9.0"";    ""@types/react"": ""~16.9.35"";    ""@types/react-dom"": ""~16.9.8"";    ""@types/react-native"": ""~0.63.2"";    ""babel-preset-expo"": ""~8.3.0"";    ""jest-expo"": ""~40.0.0"";    ""typescript"": ""~4.0.0""  };  ""jest"": {    ""preset"": ""react-native""  };  ""private"": true}```**App.tsx**```import * as tf from '@tensorflow/tfjs';import '@tensorflow/tfjs-react-native';import React; { Component } from 'react';export default class App extends React.Component {  constructor(props: any) {    super(props);    this.state = {      isTfReady: false;    };  }  async componentDidMount() {    // Wait for tf to be ready.    await tf.ready();    // Signal to the app that tensorflow.js can now be used.    this.setState({      isTfReady: true;    });  }  render() {    return null;  }}```","['Hi Kay;I haven\'t set up Android emulator on my machine; but for the web; I actually encountered the same issue. It is fixed by the steps below:- Run: `expo customize:web`  - Use the space key to select the `webpack.config.js` entry; then press ""enter""  - This will create a bare-minimum `webpack.config.js` file.- Edit the `webpack.config.js` file to make webpack transpile all `@tensorflow` packages. Here is what my file looks like```const createExpoWebpackConfigAsync = require(\'@expo/webpack-config\');module.exports = async function(env; argv) {  const config = await createExpoWebpackConfigAsync(      {        ...env;        babel: {          dangerouslyAddModulePathsToTranspile: [            // Ensure that all packages starting with @tensorflow are            // transpiled.            \'@tensorflow\';          ];        };      };      argv);  return config;};``` - Now `npm run web` should workPlease check if this helps with the Android build as well. Thanks!====='; ""@jinjingforever Hey;I followed your steps and got `npm run web` working. Is this some extra setup step that needs to now be added to the readme?The `npm run android` still does not work for another reason which i'll make a new issue for when i revist it.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4803"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4803"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Module Parse Failed""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,D.3
https://github.com/tensorflow/tfjs/issues/4802,Crash installing tfjs-node-gpu: Undefined variable module_name in binding.gyp while trying to load binding.gyp,5,closed,2021-03-10T18:06:07Z,2021-03-22T17:35:57Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu 20.10; Ubuntu 18.04; CentOS 7.2.1511- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): `""@tensorflow/tfjs-node-gpu"": ""^3.0.0""` in package.json- TensorFlow.js version: `""@tensorflow/tfjs-node-gpu"": ""^3.0.0""` in package.json- CUDA/cuDNN version: CUDA 10.0; CuDNN 7**Describe the problem**Crash when attempting to install `@tensorflow/tfjs-node-gpu`.**Provide the exact sequence of commands / steps that you executed before running into the problem**1. Use `git clone` to clone down a pre-existing project that depends on `@tensorflow/tfjs-node-gpu`2. `cd` into the directory3. Run `npm install`4. See error**Any other info / logs**Installation logs: [tfjs-node-gpu-install-log.txt](https://github.com/tensorflow/tfjs/files/6117941/tfjs-node-gpu-install-log.txt)Note that I only have admin rights over the Ubuntu 20.10 machine.Note also that I reliably get the same issue inside the `nvcr.io/nvidia/tensorflow:19.02-py3` Docker container.","['@pyu10055 here is the related issue which was occurring on windows https://github.com/tensorflow/tfjs/issues/4401#issuecomment-795773373 ; I tried on Mac and o see no issues.====='; '@rthadur I am not using macOS or Windows. I do not have a Windows machine or a Mac computer to test on. I am using (mostly) Ubuntu Linux. I have tested on these 4 machines: - CentOS 7.2 (node v15.5.0; npm 7.3.0) - Ubuntu 18.04 (direct; node v15.5.0; npm 7.6.0)      - This machine has Docker installed - tried in the Docker container `nvcr.io/nvidia/tensorflow:19.02-py3` (node v15.5.0; npm 7.6.0); which has CUDA 10 installed inside it and I have successfully used this exact container for GPU acceleration previously. - Ubuntu 20.10 (node v15.10.0; npm 7.5.6) - Ubuntu 20.04 (node v15.10.0; npm 7.6.1)I only have `sudo` rights on the last 2 machines.**Edit:** I have also tried with Node.js 14; and I see the same problem.**Edit 2:** I do _not_ experience the issue if I use the `node:lts-buster` Docker container (node v14.16.0; npm 6.14.11; running on the Ubuntu 18.04 box above)====='; '@sbrl Sorry I was not able to reproduce this error; I tried the same docker file nvcr.io/nvidia/tensorflow:19.02-py3 and installed node both (v14.16.0; v15.11.0) and npm v7.6.0; tfjs-node-gpu v3.3.0 was install successfully. can you upgrade node to 15.11.0 and try again?====='; ""Huh; that's really odd. I've tried on all 3 of those machines again now and I can't reproduce it either - even with Node.js v15.5.0. Perhaps it was a transitory issue that was fixed by someone. Though why it would mysteriously affect all these different machines all at the same time I don't know.I'll close this for now; but if I experience the issue again I'll reopen.Thanks for the help; @pyu10055!=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4802"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4802"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Undefined Variable Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/4796,Threaded SIMD tests for the wasm backend crash the browser,3,closed,2021-03-09T19:02:07Z,2021-04-22T19:40:08Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): 3.2- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Debian rodete- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below):- Browser version: Chrome ~88- Tensorflow.js Converter Version:**Describe the current behavior**Threaded SIMD tests for the wasm backend create a new webworker for each test; crashing the page with hundreds of webworkers.**Describe the expected behavior**Each test does not have its own webworker; or webworkers are cleaned up after the test is finished.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Bundling all tests (with e.g. esbuild) does not fix this. While test files are no longer individually requested from Karma; each test still re-initializes the wasm backend (requesting the `.wasm` file each time) and creates a new webworker.====='; 'Hey @mattsoulanille; Actually for test and fixed-width SIMD maybe there is some issues with Web Assembly ; Can you help me out in this![image](https://user-images.githubusercontent.com/41143496/110775224-ade9b280-8284-11eb-8ec7-f0e95abb9f78.png)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4796"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4796"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,Wasm,Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/4793,Error: Cannot find name 'EmscriptenModule'. when using wasm as a backend in Angular,1,closed,2021-03-09T10:26:28Z,2021-03-09T18:19:54Z,### Error: node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm.d.ts:18:44 - error TS2304: Cannot find name 'EmscriptenModule'.I got this error while executing this line in Angular.`import '@tensorflow/tfjs-backend-wasm';`Also; I installed and activated Emscripten SDK (version 2.0.14); EMSDK path also declared.`./emsdk activate 2.0.14&& source ./emsdk_env.sh&& echo $EMSDK`>   '/home/emsdk',['@sivarajakani i see you already commented in a related issue https://github.com/tensorflow/tfjs/issues/3866#issuecomment-793419787 ; will wait for @jinjingforever response. Thank you for your patience.====='],Initialization Faliure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Missing Type Declaration""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4775,Tfjs error with macOS 11 on m1 chip,1,closed,2021-03-03T18:27:20Z,2021-03-03T19:19:54Z,Trying to run basic get started example with tfjs in max m1 chip and getting illegal argument : 4 error. Is there anything specific need to be done for Mac m1 chip .,['There is already issue opened here for M1 Chip ; currently tfjs does not work well with M1 ; please check back here https://github.com/tensorflow/tfjs/issues/4775====='],Browser & Device Error,Crash,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.5] Training Argument Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.4,D.1
https://github.com/tensorflow/tfjs/issues/4768,Inconsistent behaviour across macOS when using WebGL backend,1,closed,2021-03-02T09:28:29Z,2021-03-02T17:09:55Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Most were derived from the example.- OS Platform: macOS Catalina; Macbook Pro 2018- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):```    ""@tensorflow-models/posenet"": ""^2.2.1"";    ""@tensorflow/tfjs-backend-webgl"": ""^2.8.6"";    ""@tensorflow/tfjs-core"": ""^2.8.6"";```- Browser version: Chrome Version 88.0.4324.192 (Official Build) (x86_64)- Tensorflow.js Converter Version: ` ""@tensorflow/tfjs-converter"": ""^3.2.0"";`**Describe the current behavior**I'm running a simple posenet detection using WebWorker; it runs fine on my Macbook Pro 2017; but when I tried it on Macbook Pro 2018 it throws me the following error; which is extremely odd considering it worked on an older Macbook.**Describe the expected behavior**Posenet should be able to detect the user's pose.**Standalone code to reproduce the issue**https://labs.bebensiganteng.com/dubai-proj/**Other info / logs** Include any logs or source code that would be helpful toCode;```const tf = require('@tensorflow/tfjs-core');const posenet = require('@tensorflow-models/posenet');require('@tensorflow/tfjs-backend-webgl');const RES_WIDTH = 257;const RES_HEIGHT = 200;let model;const getPosenet = async () => {  await tf.setBackend(""webgl"");  model = await posenet.load({    architecture: 'ResNet50';    outputStride: 32;    inputResolution: { width: RES_WIDTH; height: RES_HEIGHT };    quantBytes: 2  });}getPosenet();export const calculatePose = async (props)=> {  if(model) {    const input = tf.browser.fromPixels(props.data);    const predictions = await model.estimateSinglePose(input; {      flipHorizontal: false    });    return predictions;  }  return null}```Error Logs (some of the lines were omitted);```canvas_util.ts:45 Could not get context for WebGL version 2webgl_util.ts:537 Error when getting WebGL context:  Error: Cannot create a canvas in this context    at canvas_util.ts:74    at canvas_util.ts:82    at j (canvas_util.ts:41)    at mt (webgl_util.ts:532)    at Object.evaluationFn (flags_webgl.ts:37)    at t.value (environment.ts:138)    at t.value (environment.ts:94)    at t.value (environment.ts:107)    at Object.evaluationFn (flags_webgl.ts:31)    at t.value (environment.ts:138)engine.ts:340 Error: WebGL is not supported on this device    at new n (backend_webgl.ts:142)    at Object.factory (base.ts:25)    at t.value (engine.ts:301)    at t.<anonymous> (engine.ts:254)    at l (runtime.js:63)    at Generator._invoke (runtime.js:293)    at Generator.next (runtime.js:118)    at r (asyncToGenerator.js:3)    at c (asyncToGenerator.js:25)    at asyncToGenerator.js:32engine.ts:393 Uncaught (in promise) Error: Could not initialize any backends; all backend initializations failed.    at t.value (engine.ts:393)    at t.get (engine.ts:192)    at t.value (engine.ts:764)    at i (tensor_ops_util.ts:75)    at o (tensor.ts:56)    at Module.p (io_utils.ts:225)    at t.value (graph_model.ts:160)    at t.<anonymous> (graph_model.ts:134)    at l (runtime.js:63)    at Generator._invoke (runtime.js:293)```",['@bebensiganteng thank you for reporting ; same issue has been tracked here ; please check https://github.com/tensorflow/tfjs/issues/4284 .====='],Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.4,D.2
https://github.com/tensorflow/tfjs/issues/4766,Backend WASM does not implement kernel ResizeNearestNeighbor,1,open,2021-02-28T15:05:06Z,2021-06-03T16:15:38Z,attempting to run nudenet model using `wasm` backend  converted from tf saved to tfjs graph using tensorflowjs_convert:```logengine.js:391 Uncaught (in promise) Error: Kernel 'ResizeNearestNeighbor' not registered for backend 'wasm'    at Engine2.runKernel (engine.js:391)    at resizeNearestNeighbor_ (resize_nearest_neighbor.js:60)    at Object.resizeNearestNeighbor__op [as resizeNearestNeighbor] (operation.js:44)    at executeOp$9 (image_executor.js:34)    at operation_executor.js:62    at engine.js:327    at Engine2.scopedRun (engine.js:337)    at Engine2.tidy (engine.js:326)    at tidy (globals.js:175)    at operation_executor.js:62```same model works fine using `weblgl` backend  as per #4386 it has been implemented; but it seems it was implemented for `cpu` backend only although title specifies `wasm`?environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10,['cc @lina128 @pyu10055 @mattsoulanille ====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4765,Backend WASM does not implement kernel LRN,1,open,2021-02-28T13:52:00Z,2021-06-03T16:15:38Z,attempting to run places365 model; googlenet variation  converted from caffe -> tf saved -> tfjs graph fails using `wasm` backend:```logUncaught (in promise) Error: Kernel 'LRN' not registered for backend 'wasm'    at Engine2.runKernel (engine.js:391)    at localResponseNormalization_ (local_response_normalization.js:53)```same graph model works fine in `nodejs` using `tensorflow` backend.environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10,['cc @pyu10055 @lina128 @mattsoulanille ====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4764,model converted with tensorflowjs_convert fails with webgl backend due to use of Infinity constant,3,closed,2021-02-28T13:49:59Z,2021-03-05T19:15:31Z,I've converted Places365 from Caffe model (GoogleNet variation) to TF saved model using MMDNN and then to TFJS graph model using tensorflowjs_convert  (if needed; I can post actual conversion code or the model itself)  Resulting graph model works fine with tfjs-node (fully tested) and `wasm`; but fails in browser using `webgl` backend:error occurs during call to `predict()`:```logwebgl_util.js:82 Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.js:82)    at GPGPUContext.createProgram (gpgpu_context.js:199)    at compileProgram (gpgpu_math.js:44)    at backend_webgl.js:694    at MathBackendWebGL.getAndSaveBinary (backend_webgl.js:731)    at MathBackendWebGL.runWebGLProgram (backend_webgl.js:693)    at Object.padV2$1 [as kernelFunc] (PadV2.js:27)    at kernelFunc (engine.js:463)    at engine.js:524    at Engine.scopedRun (engine.js:337)```with actual error being:```logERROR: 0:178: 'Infinity' : undeclared identifier```and generated GLSL code in question is:```glslvoid main() {  ivec4 outputLoc = getOutputCoords();  vec4 result = vec4(0.);  ivec4 rc = outputLoc;  if (any(lessThan(rc; start)) || any(greaterThanEqual(rc; end))) {    result[0] = float(-Infinity); // <-- ERROR 'Infinity' : undeclared identifier  } else {    ivec4 source = rc - start;    result[0] = getChannel(getX(source.x;source.y;source.z;source.w); vec2(source.z;source.w));  }  ...```Which comes from `tfjs-backend-webgl/mirror_pad_packed_gpu.ts` `class PadPackedProgram`:```glsl  result[${i}] = float(${constantValue});```Where `constantValue` is set in attributes of PadV2 in the model itself.Per OpenGL specs; there are no pre-defined constants; so this is bound to fail unless constant is either converted to value or set by TFJS itself.Or another way would be to address this in the converter itself so `Infinity` is translated to numeric value during conversion?Environment: tfjs 3.2.0 on chrome 88 on ubuntu 20.10```tensorflowjs_converter --versiontensorflowjs 3.2.0Dependency versions:  keras 2.4.0  tensorflow 2.4.1```,"[""Update:I've worked around the issue by modifying original Python model code from  ```pythonpool2_3x3_s2_pad = tf.pad(conv2_norm2; paddings = [[0; 0]; [0; 1]; [0; 1]; [0; 0]]; constant_values=float('-Inf'))```to:```pythonpool2_3x3_s2_pad = tf.pad(conv2_norm2; paddings = [[0; 0]; [0; 1]; [0; 1]; [0; 0]]; constant_values=float('-3.4028235e+38'))````where `-3.4028235e+38` is value of `-tf.float32.max`Now converted model works with with TFJS WebGL backendBut this should really be handled by TFJS itself as in many cases model code is not available=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4764"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4764"">No</a>====='; 'just saw the linked PR; thanks!=====']",Browser & Device Error,Crash,WebGL Limits,WebGL,Backend,webgl shader type replacer,webgl shader type replacer,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.4,D.4
https://github.com/tensorflow/tfjs/issues/4759,arraySync gets tripped up by complex numbers,5,closed,2021-02-27T09:51:20Z,2021-03-17T23:41:01Z,**System information**Using Tensorflow JS 3.2.0 in NodeJS v15.5.0; MacOS 10.15.7 (19H512)**Describe the current behavior**Running the below code produces the error:```Error: [32;1025] does not match the input size 65600.```**Describe the expected behavior**The code should return a 2-dimensional array of size 32800.**Standalone code to reproduce the issue**```javascriptlet tf = require('@tensorflow/tfjs')let samples = new Float32Array(18048);let res = tf.signal.stft(tf.tensor(samples); 2048; 512; 2048);console.log(res.arraySync());```**Other info / logs**Tensor info after running stft:```Tensor {  kept: false;  isDisposedInternal: false;  shape: [ 32; 1025 ];  dtype: 'complex64';  size: 32800;  strides: [ 1025 ];  dataId: { id: 3014054 };  id: 453;  rankType: '2';  scopeId: 0}```,"[""Ok I think I see what's going on; it's having trouble with the real and imaginary parts of the complex number right?=====""; 'Well; in my case I can solve this because I need to run `abs()` on my Tensor anyways which gets rid of the imaginary part.====='; ""Ok so; after reading a bit more I'll summarize:When complex64 was added (https://github.com/tensorflow/tfjs/issues/565); it was decided that dataSync should return both parts of the complex number. That's why the array returned by dataSync is twice as big. However; when arraySync was added this wasn't taken into account; which is why it prints an error. It's trying to fit the bigger array into the same shape.So dataSync is working correctly; but arraySync is not. A potential fix for arraySync would be to change it's shape to return an array where each element is a pair of real/imaginary parts of the complex number; to mirror the behavior of dataSync.I assume the above also applies to the non-sync'd version of arraySync.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4759"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4759"">No</a>====='; 'Thanks for the report Joonatan! =====']",Data & Model Error,Crash,Incorrect Code Logic,Operator,API,parameter modifier,Modify API Parameter usage,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] ArraySync Shape Mismatch with Complex Numbers""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/4746,tfjs-vis not compatible with tfjs 3.1.0 and higher,8,closed,2021-02-25T14:28:30Z,2021-04-23T01:02:50Z,"regression in tfjs 3.1.0 cannot create sequential modelwhen importing tfjs using```jsimport * as tf from '@tensorflow/tfjs';```and running through a bundler (`esbuild`)  it fails to add layer to sequential model:```jsconst model = tf.sequential({ name: 'testModel' });model.add(tf.layers.dense(layerDense);```error backtrace using tfjs 3.2.0 (error is same in tfjs 3.1.0 but line numbers may change):```logengine.js:699 Uncaught (in promise) TypeError: e.backend.decComplexRef is not a function    at b.disposeTensor (engine.js:699)    at x.dispose (tensor.js:283)    at Dense.addWeight (topology.ts:1298)    at Dense.build (core.ts:244)    at topology.ts:993    at nameScope (common.ts:48)    at Dense.apply (topology.ts:979)    at Sequential.add (models.ts:478)    at createRNN (model.js:47) <-- this is where model.add is called```environment: tfjs 3.1.0 or 3.2.0 on chrome 88reproduction code: <https://github.com/vladmandic/stocks>error is same regardless of backend used (cpu; webgl; wasm)now; i've tried tfjs 3.0.0 and it WORKS without issues  upgrading to tfjs 3.1.0 causes breaking issue which stays the same in tfjs 3.2.0  however; when using tfjs via script tag in html instead of import in js; everything works (even with tfjs 3.2.0):```html<script src=""../node_modules/@tensorflow/tfjs/dist/tf.es2017.js""></script>```all this likely indicates that some dropped import between tfjs 3.0.0 and tfjs 3.1.0 causes incorrect tree shaking.only hit is <https://github.com/tensorflow/tfjs/issues/4609>; but that was closed as stale and never resolved.","[""Hi @vladmandic ; decComplexRef is removed in 3.2.0. A quick search of the repo shows no occurence of it in our codebase. Can you rm -rf dist node_modules yarn.lock and yarn install everything again? It's most likely cached install.=====""; 'ah; found it: latest `tfjs-vis` is still built against older `tfjs`;  so if its loaded it causes tfjs version conflict and results in this error  changing title to `tfjs-vis not compatible with tfjs 3.1.0 and higher````logrm -rf node_modules/ package-lock.json ~/.npm/_cacache/npm installgrep @tensorflow package.json    ""@tensorflow/tfjs-vis"": ""^1.5.0"";    ""@tensorflow/tfjs"": ""=3.2.0"";    ""@tensorflow/tfjs-backend-wasm"": ""=3.2.0"";    ""@tensorflow/tfjs-backend-webgl"": ""=3.2.0"";grep -Rl decComplexRef node_modules/@tensorflow/  node_modules/@tensorflow/tfjs-vis/dist/tfjs-vis.umd.min.js  node_modules/@tensorflow/tfjs-vis/dist/tfjs-vis.umd.min.js.mapgrep @tensorflow node_modules/@tensorflow/tfjs-vis/package.json    ""@tensorflow/tfjs-backend-webgl"": ""3.0.0"";    ""@tensorflow/tfjs-core"": ""3.0.0"";    ""@tensorflow/tfjs-layers"": ""3.0.0"";```and `tfjs-vis` cannot be loaded as import; only as umd.i went ahead and did a custom build of tfjs-vis using tfjs 3.2.0 and targeting esm output and it works like a charm.(looks like tfjs-vis build procedure has not been modernized like the rest of tfjs was.)====='; '@rthadur @lina128 just to confirm; tfjs-vis 3.3.0 is still BROKEN as it\'s still built against tfjs 3.0.0 which doesn\'t work:this should be a trivial fix - it\'s just that your build platform does not update dependencies for tfjs-vis; why no progress?```log grep @tensorflow package.json    ""@tensorflow/tfjs-vis"": ""^1.5.0"";    ""@tensorflow/tfjs"": ""^3.3.0"";    ""@tensorflow/tfjs-core"": ""^3.3.0"";    ""@tensorflow/tfjs-backend-wasm"": ""^3.3.0"";    ""@tensorflow/tfjs-backend-webgl"": ""^3.3.0"";grep @tensorflow node_modules/@tensorflow/tfjs-vis/package.json  ""name"": ""@tensorflow/tfjs-vis"";    ""@tensorflow/tfjs-core"": ""3.0.0"";    ""@tensorflow/tfjs-layers"": ""3.0.0"";    ""@tensorflow/tfjs-backend-webgl"": ""3.0.0"";    ""@tensorflow/tfjs-core"": "">= 1.0.0""```====='; ""@rthadur @lina128 @pyu10055 I just checked and this is STILL broken in 3.4.0. And it's a regression since `tfjs` 3.1.0This is a completely TRIVIAL fix and it's still lingering - `tfjs-vis` simply has not updated it's dependencies since tfjs 3.0.0=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">No</a>====='; 'Will publish a new version of the tfjs-vis now====='; 'Published tfjs-vis 1.5.1 with tfjs 3.5.0 dependencies. Thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4746"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Operator,API,change framework version,Changing version,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/4745,tfjs-backend-wasm 3.2.0 additional undocumented dependencies,1,closed,2021-02-25T13:19:25Z,2021-07-08T21:54:12Z,"upgrading from tfjs 3.1.0 to tfjs 3.2.0  when including `@tensorflow/tfjs-backend-wasm` and running through bundler;  it used to have following external dependencies: `['fs'; 'buffer'; 'util']` which had to be accounted for (meaning marked as excluded or external) in bundler configuration for browser target  but now; it has extra dependency `'os'` that comes from `node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js` and is due to new `emscripten` as it comes from:`emscripten_num_logical_cores() { if(ENVIRONMENT_IS_NODE) return require(""os"").cpus().length }`now; i'm totally ok with that; but that's a breaking change that's not documented.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4745"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4745"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Configuration & Dependency Error""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4736,speech-command playRawAudio plays blank audio,8,closed,2021-02-24T17:56:13Z,2021-05-08T23:15:50Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution: Windows10- TensorFlow.js installed from: npm- TensorFlow.js version: 3.2.0- speech-command version: 0.5.2- Browser version: Version 88.0.4324.192 (Official Build) (x86_64)**Describe the current behavior**I'm trying to use [playRawAudio](https://github.com/tensorflow/tfjs-models/blob/59c6ecb1d6bddfe287d4a6559ae1a70f5b423a8d/speech-commands/src/index.ts#L84) method by providing `RawAudioData` object. `Float32Array` as [data](https://github.com/tensorflow/tfjs-models/blob/59c6ecb1d6bddfe287d4a6559ae1a70f5b423a8d/speech-commands/src/types.ts#L727) and `sampleRateHz:44100`. There's an audio playing with the same duration of `data`; but the audio is blank (no audio)I provided `Float32Array` array from `SpeechCommandRecognizerResult.spectrogram.data` in `listen` function; then concatenated the array using `concatenateFloat32Arrays` util function**Describe the expected behavior**Expected to play the correct audio; not a blank audio.**Standalone code to reproduce the issue**[Example in Codesandbox](https://codesandbox.io/s/tfjs-listen-recognize-scores-iyrjd),"['@adotnusiyan did you find a solution? i also need this and got same issue====='; 'Not yet.Hopefully waiting @lina128 ====='; ""Hi @adotnusiyan and @zipweight ; I'm investigating this; I'm seeing a sampleRate mismatch error in my browser; not sure if it is related; just fyi: https://github.com/tensorflow/tfjs-models/pull/612=====""; 'Hi @adotnusiyan; I looked at the code you provided; it seems you provide the spectrum data to the playRawAudio function. You need to provide raw audio sound; not the spectrum data.====='; '@lina128 How to play the spectrum data as audio?Or how can I get raw audio data from listen function?Thanks ====='; 'If you have a transferRecognizer with collectExample({includeRawAudio = true}). And then get the rawAudio from tranferRecognizer.getExamples()[0].example.rawAudio.If you just want a generic solution; you need to implement your own.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4736"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4736"">No</a>====='; ""@lina128 I tried your suggestion with basic code from [the example in README](https://github.com/tensorflow/tfjs-models/tree/b5d49c0f5ba2057cc29b40317126c5f182495f96/speech-commands#transfer-learning) and the audio doesn't sound right. You can hear some distortion/glitching in the audio[You can try it yourself in this example](https://codesandbox.io/s/tfjs-play-raw-audio-v0y12)=====""]",side effect,Crash,Unknown,Model API,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A,E
https://github.com/tensorflow/tfjs/issues/4732,Failed to execute clientWaitSync,7,open,2021-02-23T20:12:06Z,2021-08-26T16:44:25Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): **yes**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): **Ubuntu 20.04.1 64 bit (kernel 5.8.0-43); GeForce GTX 980 Ti**- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): 2.8.5- TensorFlow.js version (use command below):- Browser version: **88.0.4324.182 (Official Build) (64-bit)**- Tensorflow.js Converter Version: n/a**Describe the current behavior**Caught an error![image](https://user-images.githubusercontent.com/1530410/108898900-a364c300-75cc-11eb-9f02-b34ec399189f.png)Traced it to `node_modules/@tensorflow/tfjs-backend-webgl/src/gpgpu_context.ts`:```ts  private createFence(gl: WebGLRenderingContext): FenceContext {    let query: WebGLQuery|WebGLSync;    let isFencePassed: () => boolean;    if (env().getBool('WEBGL_FENCE_API_ENABLED')) {      const gl2 = gl as WebGL2RenderingContext;      const sync = gl2.fenceSync(gl2.SYNC_GPU_COMMANDS_COMPLETE; 0);      gl.flush();      isFencePassed = () => {        const status = gl2.clientWaitSync(sync; 0; 0); // <----- failed here        return status === gl2.ALREADY_SIGNALED ||            status === gl2.CONDITION_SATISFIED;      };```**Describe the expected behavior****Standalone code to reproduce the issue**Unfortunately it's almost impossible to create a good reproducible case for this kind of error.,"['@kirill-konshin it is very hard for us to help with out reproduction code. Are you trying to load in web worker ? ====='; '@rthadur yes; the code is in web worker.====='; '@kirill-konshin Here is an [simple example](https://codepen.io/pyu10055/pen/LYbQdMj) of running tfjs in webworker; it tries to download the tensor from webgl backend; it did not encounter the fencing error.Can you try to create a reproducible example based on that? thanks.====='; ""@pyu10055 I know it's hard to investigate w/o code; but I can't make the concise example because then I will have to drag the entire image processing system that we have... I've traced the error to TF code; it's something internal. The issue does not reproduce 100% of times; it's very sporadic.=====""; '@kirill-konshin TF code you traced to looks fine; we need to figure out under what condition the fenceSync function is not longer valid.    A good way to isolate the problem is to look at the output of your image processing part before it feed into TF code.   ====='; ""> @kirill-konshin TF code you traced to looks fine; we need to figure out under what condition the fenceSync function is not longer valid. A good way to isolate the problem is to look at the output of your image processing part before it feed into TF code.Nothing changes on our side; we're just passing more and more frames from camera to TF but at some point in time the `fenceSync`; which is not accessible by us; is breaking in a way described earlier...=====""; 'I ran into the same error; and since this is the only result and my setup seems similar (""more and more frames""); it could apply.setup:* Arch Linux 5.12.12-arch1-1* integrated Intel HD Graphics 5500* tfjs 3.8.0 via npm* **not** in webworker* model: mobilenet_v2_100_224* Chromium 91.0.4472.114 (Firefox 90.0 is OK!)The error happens [in gpugpu_context](https://github.com/tensorflow/tfjs/blob/04633433fdb1806fbac58b0db31d0f0313ff3a05/tfjs-backend-webgl/src/gpgpu_context.ts#L243) when ``` const sync = gl2.fenceSync(gl2.SYNC_GPU_COMMANDS_COMPLETE; 0);```fails; and `sync` is null.  I couldn\'t manage to `try catch` this error. I also tried calling `fenceSync` on the same canvas/context before calling TF; but there it succeeded.My app had problems before. I (naively) created lots of canvases and contexts and ran into context losses. I dealt with them by ""baking"" the renders into static 2d contexts. Still; I had a hard time disposing of the objects and GPU memory in general (I generate with THREE.js).The problem stopped occurring when I reduced the number of intermediate create-render-bake steps; so I assumed that *zombie contexts* might be lurking somewhere; or there might be timing problems/race conditions (I read somewhere that Chrom* and Firefox handle this differently).I tried several approaches to improve the cleanup of my canvases; butwhat fixed the problem (ultimately; I hope) was to throw in THREE.js\'s [forceContextLoss()](https://threejs.org/docs/index.html?q=renderer#api/en/renderers/WebGLRenderer.forceContextLoss).I\'d share some code; but my app is still in development and quite messy.  HTH anyway :)=====']",Data & Model Error,Crash,Browser Incompatibility,Browser,Platform,change browser,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.3""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",A.2,D.2
https://github.com/tensorflow/tfjs/issues/4727,Allow tf.node.encodeJpeg to accept other tensor 3D types (Int32),3,closed,2021-02-22T22:58:07Z,2021-08-26T16:58:12Z,"**System information**- TensorFlow.js version (you are using): 2.8.6- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**Currently the [tf.node.encodeJpeg()](https://js.tensorflow.org/api_node/3.1.0/#node.encodeJpeg) method only accepts Uint8 Tensor3D as input. This is incovenient as many of the tensors generated by images or the [ react-native HOC](https://js.tensorflow.org/api_react_native/0.5.0/#cameraWithTensors) are in dtype: Int32. This makes converting generated tensors back to jpg/png difficult as there isn't any methods for converting Int32 to uint8.**Will this change the current api? How?**This will allow additional input the the encodeJpeg method to allow a broader range of inputs. In addition; this will add more consistency as decodeJpeg() generates an Int32 tensor; but it can't be converted back to the original jpeg with encodeJpeg().**Who will benefit with this feature?**React native users and other users who deal with Tensor3D in Int32 format.**Any Other info.**Example: Tensor3D generated from the [react-native HOC](https://js.tensorflow.org/api_react_native/0.5.0/#cameraWithTensors) ; the dtype is Int32 and therefore the original image can't be accessed for OCR; image processing etc.<img width=""234"" alt=""Screen Shot 2021-02-22 at 5 57 38 PM"" src=""https://user-images.githubusercontent.com/44853346/108781153-74e8d880-7537-11eb-919d-dfc35e982be9.png"">","['cc @pyu10055 @lina128 @mattsoulanille ====='; ""@JJwilkin  In file nodejs_kernel_backend.ts where encodeJpeg and decodeJpeg is implemented; the function is using the enum value as this.binding.TF_UINT8 but is returning  'int32' as the dtype (Line number 110-113). So in a way this functionality of encodejpeg to accept int32 is already implemented.=====""; ""@charuu I agree ; thank you for pointing out. pasting the code sample for quick reference here . Closing this request. Please @mention to reopen if this still needs attention. Thank you ```case this.binding.TF_UINT8:        // TensorFlow uses UINT8 as dtype for image tensor. UINT8 is not        // supported in TFJS yet; cast it to int32.        dtype = 'int32';        break;```=====""]",Reference Error,Crash,Incorrect Code Logic,TF(CPU),Backend,add support for datatype,Add unsupported operator,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4725,tensorflowjs_converter: Add Support for Operations used by BigQuery ML models,4,closed,2021-02-22T17:51:25Z,2021-04-26T16:16:05Z,**System information**- TensorFlow.js version (you are using): 3.1.0 (version 2.4.1 of tensorflow)- Are you willing to contribute it (Yes/No): I'm not familiar enought with this or tensorflow's codebase so I don't think I could write the code to fix this issue. But I think I can contribute in anything else you need; like testing; reviewing the code or giving more context. **Describe the feature and the current behavior/state.**The company I work for is building a Deep Neural Network Regressor (DNN_REGRESSOR) using BigQuery ML and we plan on deploying in a Node.js environment. The initial tests worked fine but recently when we tried to convert the Saved Model to Tensorflowjs it failed with the following error:```shValueError: Unsupported Ops in the model before optimizationSparseSegmentMean; LookupTableSizeV2; StringToHashBucketFast; SparseReshape; SparseFillEmptyRows```I'm converting the model in this way: ```shtensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --signature_name=predict --weight_shard_size_bytes=25000000 --saved_model_tags=serve source target```As a result we can't convert model nor deploy it; rendering it unusable for us. I'm using tabular data with high cardinality categorical features and these ones seem to be at the root of the problem.BigQuery ML doesn't give any options in which operations to include or not; so we depend on you giving support for these operations or maybe offering a workaround that I don't know of.**Will this change the current api? How?**Definitely; because it will require adding those Operations as supported; but that shouldn't break any working code.**Who will benefit with this feature?**Every user that is producing *BigQuery ML* models and then using them with Tensorflowjs can benefit from this. We're using tabular data in a pretty straightforward way so I don't think this is an edge case that you won't encounter anymore and it would probably be a nice synergy to have ML models created in BigQuery that work smoothly in Tensorflowjs.,"['cc @pyu10055 ====='; ""@rthadur @pyu10055 I have been familiarizing myself with the library. I'll try adding at least LookupTableSizeV2 and StringToHashBucketFast myself. I'll start with LookupTableSizeV2. I think this only requires returning the size of the Hash Table and should be pretty similar to other operations like LookupTableFind. Do you think this approach is OK? Do you agree on adding those operations?I read the contributing guidelines but would appreciate any help or guidance you can offer. Thanks in advance!=====""; ""@rthadur @pyu10055 [This PR](https://github.com/tensorflow/tfjs/pull/4755) adds one of the operations I mentioned previously. I haven't signed the CLA on behalf of my employer yet but will do it soon. I'd appreciate your review. Thanks in advance!=====""; '@fneubaum-sulvo Thank you; will review the PR.=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4719,"speech-commands demo failing with large training sets - with ""TypeError: n.array is not a function""",5,closed,2021-02-19T19:55:56Z,2021-02-27T13:13:45Z,I'm loading speech-commands from npm; using version 0.5.2. If I revert to 0.4.2; I don't see the error shown below; so this looks like a recent regression. (Could it be a build problem; similar to the one fixed in https://github.com/tensorflow/tfjs/issues/4709 ?) ### When do I get the error?When I call `transferRecognizer.train` You can see an example of what I'm doing at https://github.com/tensorflow/tfjs-models/blob/62f4f9c20db0eef4cae1ee82d000b9cd69696a58/speech-commands/demo/index.js#L456-L466 Calling `train()` results in an uncaught exception### What error am I getting?```Uncaught (in promise) TypeError: n.array is not a function    at speech-commands.min.js:17    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at e.getData (speech-commands.min.js:17)    at r.collectTransferDataAsTfDataset (speech-commands.min.js:17)    at r.<anonymous> (speech-commands.min.js:17)    at speech-commands.min.js:17    at Object.next (speech-commands.min.js:17)```### How to recreate I _think_ this is only happening with training data that is large enough to trigger this:```Detected large dataset: total duration = 98900 ms > 60000 ms. Training transfer model using fitDataset() instead of fit()```When I use training data small enough to not see that message; I don't see the error.### More info about the exceptionI tried rebuilding the app with speech-commands.js (instead of the minified version) to get a more useful stack trace:```Uncaught (in promise) TypeError: tfd.array is not a function    at speech-commands.js:1067    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at Dataset.getData (speech-commands.js:966)    at TransferBrowserFftSpeechCommandRecognizer.collectTransferDataAsTfDataset (speech-commands.js:2536)    at TransferBrowserFftSpeechCommandRecognizer.<anonymous> (speech-commands.js:2616)    at step (speech-commands.js:95)    at Object.next (speech-commands.js:76)```Catching the error in a debugger; I can see it's caused by this line:![image](https://user-images.githubusercontent.com/1444788/108553596-d2d4b080-72ea-11eb-9380-967e4cc841c4.png)`tfd` isn't null/undefined; and looks like it's the right sort of shape; but sure enough; it doesn't have an `array` function.![image](https://user-images.githubusercontent.com/1444788/108553702-fbf54100-72ea-11eb-8b62-3a66a6ca28c8.png)And if I let it try to step past that line; I get the stack trace copied above![image](https://user-images.githubusercontent.com/1444788/108553762-0ca5b700-72eb-11eb-8173-e0cdfb7c9bbe.png),"['@dalelane Thank you for the detailed debugging session; this seems to be kinda weird; the tfd (tfjs-data) object seems to become tfjs-core. Is it possible for you to create a codepen example of this? thanks!====='; ""Not easily; sorry. But essentially I'm just doing this:```jsvar baseRecognizer = speechCommands.create('BROWSER_FFT');baseRecognizer.ensureModelLoaded()    .then(function () {         var transferRecognizer = baseRecognizer.createTransfer('uniqueid');                  // collect at least 60 seconds worth of examples         // e.g.  many times calling             //  transferRecognizer.collectExample('label');                               return transferRecognizer.train();    });```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">No</a>====='; '@dalelane Please verify with the new release 0.5.3; thanks.====='; '@pyu10055 That seems to have done the trick - thanks!=====']",Reference Error,Crash,Misconfiguration,Model API,API,configuration,Modifying dependency configuration,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,B.1
https://github.com/tensorflow/tfjs/issues/4719,"speech-commands demo failing with large training sets - with ""TypeError: n.array is not a function""",5,closed,2021-02-19T19:55:56Z,2021-02-27T13:13:45Z,I'm loading speech-commands from npm; using version 0.5.2. If I revert to 0.4.2; I don't see the error shown below; so this looks like a recent regression. (Could it be a build problem; similar to the one fixed in https://github.com/tensorflow/tfjs/issues/4709 ?) ### When do I get the error?When I call `transferRecognizer.train` You can see an example of what I'm doing at https://github.com/tensorflow/tfjs-models/blob/62f4f9c20db0eef4cae1ee82d000b9cd69696a58/speech-commands/demo/index.js#L456-L466 Calling `train()` results in an uncaught exception### What error am I getting?```Uncaught (in promise) TypeError: n.array is not a function    at speech-commands.min.js:17    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at e.getData (speech-commands.min.js:17)    at r.collectTransferDataAsTfDataset (speech-commands.min.js:17)    at r.<anonymous> (speech-commands.min.js:17)    at speech-commands.min.js:17    at Object.next (speech-commands.min.js:17)```### How to recreate I _think_ this is only happening with training data that is large enough to trigger this:```Detected large dataset: total duration = 98900 ms > 60000 ms. Training transfer model using fitDataset() instead of fit()```When I use training data small enough to not see that message; I don't see the error.### More info about the exceptionI tried rebuilding the app with speech-commands.js (instead of the minified version) to get a more useful stack trace:```Uncaught (in promise) TypeError: tfd.array is not a function    at speech-commands.js:1067    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at Dataset.getData (speech-commands.js:966)    at TransferBrowserFftSpeechCommandRecognizer.collectTransferDataAsTfDataset (speech-commands.js:2536)    at TransferBrowserFftSpeechCommandRecognizer.<anonymous> (speech-commands.js:2616)    at step (speech-commands.js:95)    at Object.next (speech-commands.js:76)```Catching the error in a debugger; I can see it's caused by this line:![image](https://user-images.githubusercontent.com/1444788/108553596-d2d4b080-72ea-11eb-9380-967e4cc841c4.png)`tfd` isn't null/undefined; and looks like it's the right sort of shape; but sure enough; it doesn't have an `array` function.![image](https://user-images.githubusercontent.com/1444788/108553702-fbf54100-72ea-11eb-8b62-3a66a6ca28c8.png)And if I let it try to step past that line; I get the stack trace copied above![image](https://user-images.githubusercontent.com/1444788/108553762-0ca5b700-72eb-11eb-8173-e0cdfb7c9bbe.png),"['@dalelane Thank you for the detailed debugging session; this seems to be kinda weird; the tfd (tfjs-data) object seems to become tfjs-core. Is it possible for you to create a codepen example of this? thanks!====='; ""Not easily; sorry. But essentially I'm just doing this:```jsvar baseRecognizer = speechCommands.create('BROWSER_FFT');baseRecognizer.ensureModelLoaded()    .then(function () {         var transferRecognizer = baseRecognizer.createTransfer('uniqueid');                  // collect at least 60 seconds worth of examples         // e.g.  many times calling             //  transferRecognizer.collectExample('label');                               return transferRecognizer.train();    });```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">No</a>====='; '@dalelane Please verify with the new release 0.5.3; thanks.====='; '@pyu10055 That seems to have done the trick - thanks!=====']",Reference Error,Crash,Misconfiguration,Model API,API,configuration,Modifying dependency configuration,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",B.3.1,B.1
https://github.com/tensorflow/tfjs/issues/4719,"speech-commands demo failing with large training sets - with ""TypeError: n.array is not a function""",5,closed,2021-02-19T19:55:56Z,2021-02-27T13:13:45Z,I'm loading speech-commands from npm; using version 0.5.2. If I revert to 0.4.2; I don't see the error shown below; so this looks like a recent regression. (Could it be a build problem; similar to the one fixed in https://github.com/tensorflow/tfjs/issues/4709 ?) ### When do I get the error?When I call `transferRecognizer.train` You can see an example of what I'm doing at https://github.com/tensorflow/tfjs-models/blob/62f4f9c20db0eef4cae1ee82d000b9cd69696a58/speech-commands/demo/index.js#L456-L466 Calling `train()` results in an uncaught exception### What error am I getting?```Uncaught (in promise) TypeError: n.array is not a function    at speech-commands.min.js:17    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at e.getData (speech-commands.min.js:17)    at r.collectTransferDataAsTfDataset (speech-commands.min.js:17)    at r.<anonymous> (speech-commands.min.js:17)    at speech-commands.min.js:17    at Object.next (speech-commands.min.js:17)```### How to recreate I _think_ this is only happening with training data that is large enough to trigger this:```Detected large dataset: total duration = 98900 ms > 60000 ms. Training transfer model using fitDataset() instead of fit()```When I use training data small enough to not see that message; I don't see the error.### More info about the exceptionI tried rebuilding the app with speech-commands.js (instead of the minified version) to get a more useful stack trace:```Uncaught (in promise) TypeError: tfd.array is not a function    at speech-commands.js:1067    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at Dataset.getData (speech-commands.js:966)    at TransferBrowserFftSpeechCommandRecognizer.collectTransferDataAsTfDataset (speech-commands.js:2536)    at TransferBrowserFftSpeechCommandRecognizer.<anonymous> (speech-commands.js:2616)    at step (speech-commands.js:95)    at Object.next (speech-commands.js:76)```Catching the error in a debugger; I can see it's caused by this line:![image](https://user-images.githubusercontent.com/1444788/108553596-d2d4b080-72ea-11eb-9380-967e4cc841c4.png)`tfd` isn't null/undefined; and looks like it's the right sort of shape; but sure enough; it doesn't have an `array` function.![image](https://user-images.githubusercontent.com/1444788/108553702-fbf54100-72ea-11eb-8b62-3a66a6ca28c8.png)And if I let it try to step past that line; I get the stack trace copied above![image](https://user-images.githubusercontent.com/1444788/108553762-0ca5b700-72eb-11eb-8173-e0cdfb7c9bbe.png),"['@dalelane Thank you for the detailed debugging session; this seems to be kinda weird; the tfd (tfjs-data) object seems to become tfjs-core. Is it possible for you to create a codepen example of this? thanks!====='; ""Not easily; sorry. But essentially I'm just doing this:```jsvar baseRecognizer = speechCommands.create('BROWSER_FFT');baseRecognizer.ensureModelLoaded()    .then(function () {         var transferRecognizer = baseRecognizer.createTransfer('uniqueid');                  // collect at least 60 seconds worth of examples         // e.g.  many times calling             //  transferRecognizer.collectExample('label');                               return transferRecognizer.train();    });```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">No</a>====='; '@dalelane Please verify with the new release 0.5.3; thanks.====='; '@pyu10055 That seems to have done the trick - thanks!=====']",Regression,Poor Performance,Misconfiguration,Model API,API,configuration,Modifying dependency configuration,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,B.1
https://github.com/tensorflow/tfjs/issues/4719,"speech-commands demo failing with large training sets - with ""TypeError: n.array is not a function""",5,closed,2021-02-19T19:55:56Z,2021-02-27T13:13:45Z,I'm loading speech-commands from npm; using version 0.5.2. If I revert to 0.4.2; I don't see the error shown below; so this looks like a recent regression. (Could it be a build problem; similar to the one fixed in https://github.com/tensorflow/tfjs/issues/4709 ?) ### When do I get the error?When I call `transferRecognizer.train` You can see an example of what I'm doing at https://github.com/tensorflow/tfjs-models/blob/62f4f9c20db0eef4cae1ee82d000b9cd69696a58/speech-commands/demo/index.js#L456-L466 Calling `train()` results in an uncaught exception### What error am I getting?```Uncaught (in promise) TypeError: n.array is not a function    at speech-commands.min.js:17    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at e.getData (speech-commands.min.js:17)    at r.collectTransferDataAsTfDataset (speech-commands.min.js:17)    at r.<anonymous> (speech-commands.min.js:17)    at speech-commands.min.js:17    at Object.next (speech-commands.min.js:17)```### How to recreate I _think_ this is only happening with training data that is large enough to trigger this:```Detected large dataset: total duration = 98900 ms > 60000 ms. Training transfer model using fitDataset() instead of fit()```When I use training data small enough to not see that message; I don't see the error.### More info about the exceptionI tried rebuilding the app with speech-commands.js (instead of the minified version) to get a more useful stack trace:```Uncaught (in promise) TypeError: tfd.array is not a function    at speech-commands.js:1067    at engine.js:467    at e.t.scopedRun (engine.js:478)    at e.t.tidy (engine.js:465)    at Object.vx [as tidy] (globals.js:192)    at Dataset.getData (speech-commands.js:966)    at TransferBrowserFftSpeechCommandRecognizer.collectTransferDataAsTfDataset (speech-commands.js:2536)    at TransferBrowserFftSpeechCommandRecognizer.<anonymous> (speech-commands.js:2616)    at step (speech-commands.js:95)    at Object.next (speech-commands.js:76)```Catching the error in a debugger; I can see it's caused by this line:![image](https://user-images.githubusercontent.com/1444788/108553596-d2d4b080-72ea-11eb-9380-967e4cc841c4.png)`tfd` isn't null/undefined; and looks like it's the right sort of shape; but sure enough; it doesn't have an `array` function.![image](https://user-images.githubusercontent.com/1444788/108553702-fbf54100-72ea-11eb-8b62-3a66a6ca28c8.png)And if I let it try to step past that line; I get the stack trace copied above![image](https://user-images.githubusercontent.com/1444788/108553762-0ca5b700-72eb-11eb-8173-e0cdfb7c9bbe.png),"['@dalelane Thank you for the detailed debugging session; this seems to be kinda weird; the tfd (tfjs-data) object seems to become tfjs-core. Is it possible for you to create a codepen example of this? thanks!====='; ""Not easily; sorry. But essentially I'm just doing this:```jsvar baseRecognizer = speechCommands.create('BROWSER_FFT');baseRecognizer.ensureModelLoaded()    .then(function () {         var transferRecognizer = baseRecognizer.createTransfer('uniqueid');                  // collect at least 60 seconds worth of examples         // e.g.  many times calling             //  transferRecognizer.collectExample('label');                               return transferRecognizer.train();    });```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4719"">No</a>====='; '@dalelane Please verify with the new release 0.5.3; thanks.====='; '@pyu10055 That seems to have done the trick - thanks!=====']",Regression,Poor Performance,Misconfiguration,Model API,API,configuration,Modifying dependency configuration,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",B.3.1,B.1
https://github.com/tensorflow/tfjs/issues/4716,"Error: ""Cannot read property '0' of undefined"" for texShape",9,closed,2021-02-18T20:32:40Z,2021-07-21T19:49:13Z,Somehow `inputInfo.shapeInfo.texShape` is undefined when running inference on `COCO SSD` model on a specific image (and works for 99% of other images)  Actual backtrace is:```log  TypeError: Cannot read property '0' of undefined    at LCe (shader_compiler.js:555)    at G4 (shader_compiler.js:91)    at dCe (shader_compiler.js:103)    at shader_compiler.js:36    at Array.map (<anonymous>)    at fCe (shader_compiler.js:36)    at KCe (gpgpu_math.js:43)    at backend_webgl.js:692    at $N.getAndSaveBinary (backend_webgl.js:720)    at $N.runWebGLProgram (backend_webgl.js:691)```Matching code is in `shader_compiler.js:getPackedSampler1D()`:```jsfunction getPackedSampler1D(inputInfo) {    const texName = inputInfo.name;    const funcName = 'get' + texName.charAt(0).toUpperCase() + texName.slice(1);    const texShape = inputInfo.shapeInfo.texShape;    const packedTexShape = [Math.ceil(texShape[0] / 2); Math.ceil(texShape[1] / 2)];    const glsl = getGlslDifferences();    return `    vec4 ${funcName}(int index) {      vec2 uv = packedUVfrom1D(        ${packedTexShape[0]}; ${packedTexShape[1]}; index);      return ${glsl.texture2D}(${texName}; uv);    }  `;}```Somehow `inputInfo.shapeInfo` is defined; but does not have a valid `texShape` property.  Unfortunately; entire code is too long to post for quick reproduction; so if additional info is needed please suggest diagnostic steps.  So far I've found several images (out of thousands) where this occurs 100%.Environment: `TFJS` 3.1.0 with `WebGL` backend and `Chrome` 88 browser,"[""update with backtrace using non-minified code:```logprocessImage.js:299 TypeError: Cannot read property '0' of undefined    at getPackedSampler1D2 (shader_compiler.js:555)    at getPackedSamplerFromInInfo2 (shader_compiler.js:91)    at getInputSamplingSnippet2 (shader_compiler.js:103)    at shader_compiler.js:36    at Array.map (<anonymous>)    at makeShader2 (shader_compiler.js:36)    at compileProgram2 (gpgpu_math.js:43)    at backend_webgl.js:692    at MathBackendWebGL2.getAndSaveBinary (backend_webgl.js:720)    at MathBackendWebGL2.runWebGLProgram (backend_webgl.js:691)```=====""; 'Hi @vladmandic Can you share the image with us? Can we reproduce it with tfjs coco ssd model?====='; ""@lina128 seems that root cause is another model (custom one) and the fact that coco ssd model runs concurrently is pretty much a timing thing. the custom model definitely experiences this problem; like i said about 1 image in every 500 so far.if you're willing to dig deeper; i can create a test project and upload that custom model with it (it's about 72mb quantized to f16).=====""; 'i\'ve created a self-contained minimal repository containing reproduction code; test images (one ok and one producing error) and model itself: <https://github.com/vladmandic/tfjs-error-test>when run with `npm start` it produces following output in the browser:```logInit TFJS: 3.2.0 Backend: webglLoading model...Model loaded: ./model/model.jsonEngine state: 146462740 bytesImage loaded: ""imgok"" 300 x 300OKResult[0] shape: 1;300;4Result[1] shape: 1;300Image loaded: ""imgerr"" 300 x 600ErrorTypeError: Cannot read property \'0\' of undefined    at getPackedSampler1D (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72172:51)    at getPackedSamplerFromInInfo (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71708:24)    at getInputSamplingSnippet (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71720:20)    at https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:23    at Array.map ()    at makeShader (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:14)    at compileProgram (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72888:24)    at https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74315:24    at MathBackendWebGL.getAndSaveBinary (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74352:41)    at MathBackendWebGL.runWebGLProgram (https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74314:33)```====='; ""@rthadur @lina128is there any chance of this being looked at?it's open since Feb with full reproduction available?=====""; 'Getting below error after passing one result;I guess this is related the to image `imagerr`````Init TFJS: 3.2.0 Backend: webglLoading model...Model loaded: ./model/model.jsonEngine state: 146462740 bytesImage loaded: ""imgok"" 300 x 300OKResult[0] shape: 1;300;4Result[1] shape: 1;300Image loaded: ""imgerr"" 300 x 600ErrorgetPackedSampler1D@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72172:33getPackedSamplerFromInInfo@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71708:24getInputSamplingSnippet@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71720:20makeShader/inputSamplingSnippet<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:46makeShader@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:71653:14compileProgram@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:72888:34runWebGLProgram/binary<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74315:24getAndSaveBinary@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74352:41runWebGLProgram@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:74314:33l$@https://localhost:8001/dist/index.js:3465:186kernelFunc@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2920:34runKernelFunc/<@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2981:31scopedRun@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2794:29runKernelFunc@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2977:18runKernel@https://localhost:8001/node_modules/@tensorflow/tfjs/dist/tf.es2017.js:2850:25Dk@https://localhost:8001/dist/index.js:24:7887r@https://localhost:8001/dist/index.js:12:26001u_@https://localhost:8001/dist/index.js:35:20238Aw/r````====='; '@rthadur exactly - some sample image work fine; some cause this error - and it\'s really up to images; i\'d say about ~5% of my test set causes this problem and i cannot figure out why. `imgok` is an example of ""good"" input image and `imgerr` is an example of input image that causes problems.====='; ""I'll look into this next week. Stay tuned. Thank you for reporting the bug and the reproducible code.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4716"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4716"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,WebGL,Backend,allow shape,allow shape,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4710,BlazeFace demo freeze with wasm simd option,1,closed,2021-02-17T23:05:47Z,2021-02-24T01:10:39Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOS- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): Tested two versions: 3.1.0 and 2.8.0- TensorFlow.js version (use command below):- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**The blazeface [demo](https://storage.googleapis.com/tfjs-models/demos/blazeface/index.html) freeze with wasm simd option. **Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4710"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4710"">No</a>=====']",Browser Hangs,Poor Performance,Dependency Error,Wasm,Backend,change dependency version,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.2] Browser Hangs""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",B.1.2,B.2
https://github.com/tensorflow/tfjs/issues/4709,Latest version of speech-commands library won't load in browser,3,closed,2021-02-17T22:32:58Z,2021-02-19T10:01:45Z,"For a simple recreate:```<html><head><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js""></script><script src=""https://unpkg.com/@tensorflow-models/speech-commands@0.5.1""></script></head><body></body></html>```The browser console shows the error:```speech-commands@0.5.1:17 Uncaught TypeError: Cannot read property 'Buffer' of undefined    at speech-commands@0.5.1:17    at Ms (speech-commands@0.5.1:17)    at speech-commands@0.5.1:17    at speech-commands@0.5.1:17    at speech-commands@0.5.1:17```This used to work; and if you pin the speech-commands version you can see earlier versions don't have this problem.For example; this works correctly```<html><head><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.1.0/dist/tf.min.js""></script><script src=""https://unpkg.com/@tensorflow-models/speech-commands@0.4.2""></script></head><body></body></html>```For real examples of this in the wild; see pages like these:- https://glitch.com/~abrasive-ixora (cf. https://yashints.dev/blog/2019/12/16/tfjs-transfer-learning )- https://livecodestream.dev/post/speech-recognition-with-tensorflowjs/ - https://bensonruan.com/speech-recognition-with-tensorflow-js/- https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab#2(which; at the time of writing; are currently broken because they're using https://unpkg.com/@tensorflow-models/speech-commands to just grab the latest released version)I haven't tried this on every browser; but it's broken on the current versions of Firefox and Chrome on MacOS.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4709"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4709"">No</a>====='; '@dalelane Thank you for reporting this bug; new build 0.5.2 has been release; I have checked some of the website; they seems to be working as expected. ====='; '@pyu10055 Fantastic - thanks for fixing it so quickly!=====']",Initialization Faliure,Build & Initialization Failure,Misconfiguration,Model API,API,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4695,Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node.,3,closed,2021-02-17T15:01:19Z,2021-02-18T02:08:20Z,"**System information**- OS Platform and Distribution : macOS 11.2.1 Big Sur 20D74- TensorFlow.js installed from: `yarn add @tensorflow/tfjs-node`- TensorFlow.js version: `3.1.0`**Describe the problem**1. create a new node project2. install tensorflow with `yarn add @tensorflow/tfjs-node`3. add import or require statement to index.js file4. receive the error**Any other info / logs**rebuilding from source did work but didn't solve the problem```bashnpm rebuild @tensorflow/tfjs-node --build-from-source``````jsimport tf from '@tensorflow/tfjs-node'const model = tf.sequential();``````js{  ""type"": ""module"";  ""version"": ""1.0.0"";  ""main"": ""index.js"";  ""scripts"": {    ""start"": ""node index.js""  };  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^3.1.0""  }}``````bash> sw_versProductName:    macOSProductVersion: 11.2.1BuildVersion:   20D74``````bash> sysctl -n machdep.cpu.brand_string                                                                                                                                                                                                                                                        Apple M1``````bashnode --version        v15.5.1``````bash> node index.js/node_modules/@tensorflow/tfjs-node/dist/index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +          ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node. Please run command 'npm rebuild @tensorflow/tfjs-node --build-addon-from-source' to rebuild the native addon module. If you have problem with building the addon module; please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (node_modules/@tensorflow/tfjs-node/dist/index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:199:29)    at ModuleJob.run (node:internal/modules/esm/module_job:152:23)    at async Loader.import (node:internal/modules/esm/loader:166:24)    at async Object.loadESM (node:internal/process/esm_loader:68:5)error Command failed with exit code 1.```","['i placed the project into my documents folder; where there is no space in the path. That is a workaround for now.the next issue is```bash> node index.js[1]    59351 illegal hardware instruction  node index.js```====='; 'This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4695"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4695"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Native Addon Module Not Found""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/4682,Double CSV Load,14,open,2021-02-15T06:46:31Z,2021-06-29T16:17:42Z,"The default CSV loader loads each CSV twice.Example:```ts     let data = [];      const csvDataset = tf.data.csv(""https://s3.amazonaws.com/ir_public/temp/chess_labels.csv"");      const column_names = await csvDataset.columnNames();      const sample = csvDataset.take(10);      await sample.forEachAsync((row) => data.push(Object.values(row)));      console.log(data);```![image](https://user-images.githubusercontent.com/997157/107914018-2af46700-6f27-11eb-8421-f4b880a26167.png)This is particularly problematic with large CSV files.","['Hi @GantMan can the second one be from the browser cache? Also; can you check whether both have response body?====='; '@lina128 - sorry for the delay.  Been busy.  I do not think it is cache; because the network is hit twice.  If I load a 700mb CSV I wait for the file twice.  Would you like a demo with a larger file?====='; ""Yeah; that'll be great. I tried above example in a codepen; and couldn't reproduced. Please share a larger file.=====""; ""@lina128 - I believe I have a line on the issue being caused by a secondary library.   I'll report back when I am 100% sure.=====""; ""Hey Lina!   OK; so the issue is definitely with TFJS.  And it's affecting Danfo.js## Here's a reproduction so you can see the one line of code that is causing the issue.**EXAMPLE 1:**  Danfo.js is double loading but TFJS is fineExample Codepen:  https://codepen.io/gantman/pen/abBRObOAs you can see here; TFJS loads the chess_labels dataset once; but Danfo.js is having an issue of loading the apple dataset twice.   At first glance it appears TFJS is fine; and Danfo must have the bug.   This is not correct; apparently.**EXAMPLE 2:**  Accessing the sample in TFJS (one line of code); causes a double load.Example Codepen:  https://codepen.io/gantman/pen/dyOgmGeBy adding `await sample.forEachAsync((row) => data.push(Object.values(row)));` the 'chess_labels.csv' is loaded twice.=====""; '@lina128 - were you able to reproduce with the above examples?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'replying to un-stale.   @lina128 was the demo good enough?====='; '@pyu10055 @lina128 Just checking if you can look into this soon.====='; ""Hi @GantMan ; sorry I haven't had a chance to look into it yet. @pyu10055 Do you have any idea what could cause this double loading?=====""; '@GantMan @lina128 I took a quick look at the implementation of [URLDataSource](https://github.com/tensorflow/tfjs/blob/master/tfjs-data/src/sources/url_data_source.ts#L43)It currently does not support data caching; which means every data iteration would cause a separate data download.The main concern is the memory usage if we retain the data for iterators. ====='; 'Thanks for catching this!  Yeah; with large CSV files the performance hit is very noticeable.====='; '@GantMan can we close this ?====='; ""I think this is a pretty significant bug.  Using TFJS to grab a CSV file means pulling the file TWICE across the network.    I don't know where the standard is but I assume it's less than O(2n).=====""]",data load error,Crash,Incorrect Code Logic,Operator,API,,,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Data Loaded Multiple Times""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A,A.4
https://github.com/tensorflow/tfjs/issues/4681,Unknown op 'SparseTensorDenseMatMul' and 'SparseMatMul',1,open,2021-02-15T05:03:41Z,2021-06-03T16:15:37Z,**System information**- TensorFlow.js version (you are using): 3.0.0- Are you willing to contribute it (Yes/No):**Describe the feature and the current behavior/state.**I am trying to convert a TF2 saved model using [sparse_dense_matmul](https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul) (or alternatively [matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) with `b_is_sparse=True`; but can't as tfjs does not support that yet.**Will this change the current api? How?** The only change will be a new transformation for people to use.**Who will benefit with this feature?**I am using this function in my TF model to multiply a tensor from my training batch against a transformation matrix (not dissimilar from a [DFT matrix](https://en.wikipedia.org/wiki/DFT_matrix)). My transformation matrix happens to be very sparse (full of 0s in my case) and is suitable to be converted to a sparse tensor. However; I would not get the speed boost (I would get the memory boost) that comes with a sparse tensor unless I use a function like sparse_dense_matmul or sparse matmul.Functionality like this is very useful in signal processing; particularly audio processing (my use case) where transformation matrices like this exist.**Any Other info.**,['cc @ping @mattsoulanille ====='],Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4680,tf.nonmaxsuppression() in webgl locks the ui thread. call tf.nonmaxsuppressionasync() instead,4,closed,2021-02-14T06:20:00Z,2021-07-14T17:19:49Z,Getting this warning when using Cocossd in tfjs/react-native- TensorCamera preview looks laggy for real-time object detection- Getting only one object in prediction array all timePlease give any solution for smooth camera view and fast object detection through Cocossd,"[""It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tfjs/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.=====""; '> Getting this warning when using Cocossd in tfjs/react-native> > * TensorCamera preview looks laggy for real-time object detection> * Getting only one object in prediction array all time> > Please give any solution for smooth camera view and fast object detection through CocossdDid you got the solution ??====='; 'Any update on this?====='; '> > > Getting this warning when using Cocossd in tfjs/react-native> >     * TensorCamera preview looks laggy for real-time object detection> >     * Getting only one object in prediction array all time> > > Please give any solution for smooth camera view and fast object detection through CocossdDid you find a solution?=====']",Browser Hangs,Poor Performance,WebGL Limits,WebGL,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",B.1.2,D.4
https://github.com/tensorflow/tfjs/issues/4679,`@tensorflow/tfjs` is missing dependency on `seedrandom`,5,closed,2021-02-14T02:19:03Z,2021-04-09T16:09:41Z,**System information**- TensorFlow.js installed from (npm or script link): `3.0.0`**Describe the current behavior**`@tensorflow/tfjs-data` has a peer dependency on `seedrandom`:https://github.com/tensorflow/tfjs/blob/1612c9a316dd4edeea9c9ef9cd6ab029f43741b5/tfjs-data/package.json#L71`@tensorflow/tfjs` depends on `@tensorflow/tfjs-data`; but does not provide the `seedrandom` peer dependency:https://github.com/tensorflow/tfjs/blob/1612c9a316dd4edeea9c9ef9cd6ab029f43741b5/tfjs/package.json#L92**Describe the expected behavior**`@tensorflow/tfjs` should provide the `seedrandom` peer dependency for `@tensorflow/tfjs-data`.,"['This is a duplicate ; here is the similar issue opened to track the same https://github.com/tensorflow/tfjs/issues/3875 ; Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4679"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4679"">No</a>====='; '@rthadur Are you sure this is the same issue? This is a dependency issue - not a bundling issue.====='; 'Any solution for this issue? Thx====='; '> Any solution for this issue? ThxWhat package manager are you using?=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,add dependency,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4659,[Feature] Auto check for version compatibility when individual package is loaded,4,open,2021-02-09T05:23:04Z,2021-06-29T01:35:37Z,"**System information**Browser: Chrome `88.0.4324.96`OS: macOS `10.15.7`Copying and pasting the [Via a script tag](https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md#via-a-script-tag) section of the WASM readme throws the following errors:```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.4.0/dist/tf.min.js"" type=""text/javascript""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js""></script><script>tf.setBackend('wasm').then(() => {});</script>``````kernel_registry.ts:111 Uncaught Error: The kernel 'undefined' for backend 'wasm' is already registered    at Object.c [as registerKernel] (kernel_registry.ts:111)    at register_all_kernels.ts:200    at tf-backend-wasm.js:20    at tf-backend-wasm.js:21``````engine.ts:235 Uncaught (in promise) Error: Backend name 'wasm' not found in registry    at t.<anonymous> (engine.ts:235)    at tf.min.js:2    at Object.next (tf.min.js:2)    at tf.min.js:2    at new Promise (<anonymous>)    at r (tf.min.js:2)    at t.setBackend (engine.ts:233)    at Object.t.setBackend (globals.ts:276)    at index.html:3```","[""you're force-loading `tfjs` 1.4.0 while `wasm` is loaded without version string; so defaults to latest 3.0.0 - no chance those two can work together.=====""; 'Ah; that makes sense; thanks! Is there a way to catch that incompatible versions of `tfjs*` are being used and surface an error that reflects that?====='; ""Each module has its own version string; so it should be easy to compare if major version matches or not.  But that's a feature request for devs.=====""; '@phoenix-meadowlark That is a great idea; we will look into that.=====']",Initialization Faliure,Build & Initialization Failure,Inconsistent Modules,Wasm,Backend,change framework version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.2
https://github.com/tensorflow/tfjs/issues/4656,Please publish face-landmarks-detection 0.0.3 to npm,8,closed,2021-02-08T22:29:08Z,2021-02-18T02:01:15Z,It looks like the face-landmarks-detection model was updated to work with TFJS 3.0 a few weeks agohttps://github.com/tensorflow/tfjs-models/blob/2af288d98609b32df2075cf7226c694989a5d476/face-landmarks-detection/package.json#L3But npm still only has the previous versionhttps://www.npmjs.com/package/@tensorflow-models/face-landmarks-detectionCan someone publish the latest TFJS 3.0-compatible version to npm; please?,"['plz ^^====='; ""Wait there was a 0.0.3??? I was getting a huge bug with the npm version that toFloat does not exist on type Tensor3d and it had to do with a line in the package. What's going on?=====""; 'Our team will publish the tfjs 3.x compatible version today. Stay tuned.====='; '🤩====='; '0.0.3 is published.====='; 'Thank you so much. You guys are literal superheroes. 🚀 Love the work you do and keep it up!====='; 'Thank you closing this as we have published to npm.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4656"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4656"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Untimely Update,Model API,API,publish new package,publish new package,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Untimely Update""
  }
}
```",C,B.3
https://github.com/tensorflow/tfjs/issues/4654,TF.js Models Handpose live example link is broken,1,closed,2021-02-08T16:28:36Z,2021-02-10T01:36:32Z,on the README for [TF.js-models](https://github.com/tensorflow/tfjs-models) HandPose live example link is broken,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4654"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4654"">No</a>=====']",Document Error,Document Error,Confused Document,Model API,API,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/4650,Kernel function Ceil is not implemented for WASM backend,1,closed,2021-02-06T20:05:53Z,2021-02-11T19:28:31Z,Attempting to run a model using `ceil` function and `WASM` backend` results in:```Uncaught (in promise) Error: Kernel 'Ceil' not registered for backend 'wasm'```Same model works ok with WebGL backend.Environment: TFJS 3.0.0OS: Ubuntu 20.10,['thanks!====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4641,Error: TensorList shape mismatch: Shapes -1 and 3 must match,11,closed,2021-02-05T10:59:20Z,2021-04-26T16:23:47Z,**System information**- I'm trying to convert a TF Object Detection model to SavedModel and then TF.js without any customization- I'm working on Colab to export the model and I load the converted model on Chrome.- Chrome version 88.0.4324.146 (Official Build) (64-bit)- TensorFlow.js installed from yarn- tfjs-core: ^3.0.0- tfjs-converter: ^3.0.0- TF 2.4.1- Python 3.6.9I'm currently trying to convert to TF.js one of the Object Detection models from the TF2 OD ZOO; in particular SSD MobileNet V2 FPNLite 320x320.When I convert the model pre-existing SavedModel to TF.js from the saved_model folder I'm able to import it in my browser and launch it through executeAsync(). If I keep the original pipeline.config and try to create another SavedModel from the provided checkpoint using this line```python exporter_main_v2.py --input_type image_tensor \    --pipeline_config_path ./pre-trained-models/ssd320/pipeline.config \    --trained_checkpoint_dir ./pre-trained-models/ssd320/checkpoint_0 \    --output_directory ./pre-trained-models/ssd320/exported_model```after I convert it to TF.js with the following line```tensorflowjs_converter \    --input_format=tf_saved_model \    --saved_model_tags=serve \    ./pre-trained-models/ssd320/path-to-savedmodel-folder \    ./pre-trained-models/tfjs_test```I encounter the following error when I try to launch the inference on my browser```util_base.js?a6b2:141 Uncaught (in promise) Error: TensorList shape mismatch:  Shapes -1 and 3 must match    at Module.assert (util_base.js?a6b2:141)    at assertShapesMatchAllowUndefinedSize (tensor_utils.js?74aa:24)    at TensorList.setItem (tensor_list.js?41f7:182)    at Module.executeOp (control_executor.js?de9e:188)    at eval (operation_executor.js?be85:52)    at executeOp (operation_executor.js?be85:94)    at GraphExecutor.processStack (graph_executor.js?33ef:390)    at GraphExecutor.executeWithControlFlow (graph_executor.js?33ef:350)    at async GraphExecutor._executeAsync (graph_executor.js?33ef:285)    at async GraphModel.executeAsync (graph_model.js?9724:316)```I tried with SSD MobileNet v2 320x320 (no FPN here) and the outcome is the same. I'm starting to think that it may be connected to the use of exporter_main_v2.py but I wouldn't know how to convert the model without it.Could you please help me figure out something more about the cause of this issue?,"['I am having the exact same problem. I am trying to use a model I trained using the TF2 Obj Detection API. I did transfer learning on top of the MobileNet V2 SSD model; exported to saved graph using exporter_main_v2.py. I used the tensorflowjs_converter.I get the same message from just trying to feed in a Zero Tensor; made with this code:````    const zeroTensor = tf.zeros([1; 300; 300; 3]; \'int32\');    // Warmup the model.    const result = await this.model.executeAsync(zeroTensor) as tf.Tensor[];````When I pause code execution in the browser and inspect; the loaded graph model has the following input signature. ````signature:  inputs:    input_tensor:0:    dtype: ""DT_UINT8""    name: ""input_tensor:0""    tensorShape:      dim: Array(4)      0: {size: ""1""}      1: {size: ""-1""}      2: {size: ""-1""}      3: {size: ""3""}````I also checked the TF Save Model and it had the following signature:````!saved_model_cli show --dir {model_export_dir}saved_model --allThe given SavedModel SignatureDef contains the following input(s):    inputs[\'input_tensor\'] tensor_info:        dtype: DT_UINT8        shape: (1; -1; -1; 3)        name: serving_default_input_tensor:0  The given SavedModel SignatureDef contains the following output(s):    outputs[\'detection_anchor_indices\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:0    outputs[\'detection_boxes\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100; 4)        name: StatefulPartitionedCall:1    outputs[\'detection_classes\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:2    outputs[\'detection_multiclass_scores\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100; 2)        name: StatefulPartitionedCall:3    outputs[\'detection_scores\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 100)        name: StatefulPartitionedCall:4    outputs[\'num_detections\'] tensor_info:        dtype: DT_FLOAT        shape: (1)        name: StatefulPartitionedCall:5    outputs[\'raw_detection_boxes\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 1917; 4)        name: StatefulPartitionedCall:6    outputs[\'raw_detection_scores\'] tensor_info:        dtype: DT_FLOAT        shape: (1; 1917; 2)        name: StatefulPartitionedCall:7  Method name is: tensorflow/serving/predict````I also checked and the Save Model from the TF2 Object Detection zoo has that same saved model:http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gzDoes the input signature for the model need to be explicitly set to 1;300;300;3 somewhere?Is there a better way to run a Custom TF2 Object Detection model in TFjs?====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4641"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4641"">No</a>====='; 'I tried too many times the problem persists. How am I going to do this? Can you help me? ====='; 'I have the same exact error when training the model using the object detection API. all works fine right to the point where I convert the model to TFJS.after loading an image I got the error:TensorList shape mismatch: Shapes -1 and 3 must match====='; ""I've been working for a week. This error appears to be from open source applications that were not in previous versions. I even tried from old models. They are working. But the one I just created doesn't work. This error continues. How do we fix this?```tfjs-core: ^3.0.0tfjs-converter: ^3.0.0TF 2.4.1Python 3.6.9```=====""; 'This issue is resolved in https://github.com/tensorflow/tfjs/pull/4657 ; which is released in 3.1.0; please try the latest version.====='; 'I checked my tensorflow version; still have this issue```yarn list v1.22.5├─ @tensorflow/tfjs-backend-cpu@3.1.0├─ @tensorflow/tfjs-backend-webgl@3.1.0├─ @tensorflow/tfjs-converter@3.1.0├─ @tensorflow/tfjs-core@3.1.0├─ @tensorflow/tfjs-data@3.1.0├─ @tensorflow/tfjs-layers@3.1.0├─ @tensorflow/tfjs@3.1.0Done in 2.02s.```Tried to remove node_modules and reinstall; same outcome.Also; I ran the default implementation of coco-ssd (TF1) and got this![image](https://user-images.githubusercontent.com/75074426/108539853-cf3c2c00-72e0-11eb-8b61-76542e0d420d.png)No signature; and negative input sizes.SOLVED: another dependency was requiring tfjs 3.0====='; '> This issue is resolved in #4657 ; which is released in 3.1.0; please try the latest version.I am still having the issue despite having the version 3.1.0. Please help.Thanks====='; '@nikhilparmar Can you share your code and model or steps to produce? Thank you.====='; 'I believe this issue has been fixed; please try out tfjs version > 3.5.0====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4641"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4641"">No</a>=====']",Data & Model Error,Crash,Incorrect Code Logic,Operator,API,allow shape,allow shape,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/4638,Unhandled Rejection (TypeError): Unknown op 'TensorListFromTensor',4,closed,2021-02-04T16:15:17Z,2021-08-24T16:17:47Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes; I referred the following GitHub repo: https://github.com/hugozanini/TFJS-object-detection/blob/master/src/index.js- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): !pip install tensorflowjs[wizard]- TensorFlow.js version (use command below): 3.0.0- Browser version: Chrome 88.0.4- Tensorflow.js Converter Version: 3.0.0**Describe the current behavior**The web app crashes when the webcam turns ON.**Describe the expected behavior**The browser should be able to detect cartoon shown from the web cam.**Standalone code to reproduce the issue**https://github.com/NSTiwari/TFJS-Custom-Object-Detection/blob/main/src/index.js![tfjs](https://user-images.githubusercontent.com/37960279/106920534-324e9000-6731-11eb-90f0-0d5c8325375f.png),"['In the package.json it still uses 1.74 version of tfjs ; can you try updating to latest version ? Thank you ====='; 'Also the above mentioned op is added in latest versions ; please refer the PR here https://github.com/tensorflow/tfjs/pull/3432 ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4638"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4638"">No</a>====='; 'Hello All. I have ensured I am using the latest version of the tfjs and tfsj-converter. I still get the same error.below are my dependencies added""dependencies"": {    ""@tensorflow/tfjs"": ""3.8.0"";    ""@tensorflow/tfjs-converter"": ""3.8.0"";    ""@tensorflow/tfjs-core"": ""3.8.0"";    ""@tensorflow/tfjs-node"": ""3.8.0"";    ""react"": ""16.5.2"";    ""react-dom"": ""16.5.2"";    ""react-magic-dropzone"": ""1.0.1"";    ""react-scripts"": ""2.0.3""  };Is there any way I can solve this ? I have followed the tutorial from tensorflow blog https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html and used the repository https://github.com/hugozanini/TFJS-object-detection/blob/master/src/index.js for my programAny help is appreciated as I dont know what to do next. Completely stuck@rthadur=====']",Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4637,Uncaught (in promise) TypeError: Cannot read property 'name' of undefined,16,closed,2021-02-04T13:15:40Z,2021-08-26T11:24:04Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10 Education 20H2 64-bit- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.0.0- CUDA/cuDNN version: Not using**Describe the problem**Previously I had a trained model from YOLOv5. Then I wanted to try running it in the browser; so I converted it to the tfjs model. However; I have a problem when inference. **Provide the exact sequence of commands/steps that you executed before running into the problem**I call the element of an image; then I change it to tensor4d then I predict; but this problem always arises:""Uncaught (in promise) TypeError: Cannot read property 'name' of undefined""**Any other info/logs**Uncaught (in promise) TypeError: Cannot read property 'name' of undefined    at graph_executor.js:307    at Array.reduce (<anonymous>)    at GraphExecutor.executeFunctionAsync (graph_executor.js:306)    at Module.executeOp (control_executor.js:31)    at async Promise.all (:3000/index 0)    at async GraphExecutor.executeWithControlFlow (graph_executor.js:351)    at async GraphExecutor._executeAsync (graph_executor.js:285)    at async GraphModel.executeAsync (graph_model.js:316)","['@pravastacaraka can you please share minimal code to reproduce the error ; you can create a repo or use codepen. Thank you====='; ""Sorry for the late response; I've fixed this problem=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4637"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4637"">No</a>====='; 'What did you do? i have the same problem. ====='; '@dewball345 Unfortunately there is no way to fix this yet. As we know on PR https://github.com/ultralytics/yolov5/pull/1127 using ops `tf.image.combined_non_max_suppression()` which apparently is not supported by TensorFlow.js. So we have to convert YOLOv5 model `*.pt` to TensorFlow.js model without using `--tf-nms` argument and apply NMS manually in our website program.====='; ""> Sorry for the late response; I've fixed this problemOkay; so why did you put this then? also; I converted from pytorch to onnx to tensorflow .pb then to tensorflowjs with tensorflow_converter. Was there anything else that you did?=====""; 'I will try later using this PR as well; though I think there may be something wrong with my input. Can i see your tensorflow.js inference code? thanks!====='; ""Okay; so i was able to conduct inference in tensorflow.js and am currently working on a non-max suppression algorithm. What i did previously is that I converted pytorch -> onnx -> tensorflow saved model -> tensorflow.js. In the last step; there may have been some kind of conversion error. I suggest you use @zldrobit's tensorflow branch of the ultralytics' yolov5 repo. You can find the link at https://github.com/zldrobit/yolov5/. =====""; '@dewball345 Have you finished the algorithm of non-max suppression?====='; ""No; not yet. You know; it's. Actually quite simple(just turn the non maxsuppression function in utils.general to js)On Fri; Jun 11; 2021; 5:48 AM Pravasta Caraka ***@***.***>wrote:> @dewball345 <https://github.com/dewball345> Have you finished the> algorithm of non-max suppression?>> —> You are receiving this because you were mentioned.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4637#issuecomment-859557452>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AHGJKVHNEHCPPGNS7WGSJB3TSIAZLANCNFSM4XCZ3W4A>> .>=====""; ""Okay; you can expect it soon; i'm probably going to make a repo with itOn Fri; Jun 11; 2021 at 4:20 PM Abhiraam Eranti ***@***.***>wrote:> No; not yet. You know; it's. Actually quite simple(just turn the non max> suppression function in utils.general to js)>> On Fri; Jun 11; 2021; 5:48 AM Pravasta Caraka ***@***.***>> wrote:>>> @dewball345 <https://github.com/dewball345> Have you finished the>> algorithm of non-max suppression?>>>> —>> You are receiving this because you were mentioned.>> Reply to this email directly; view it on GitHub>> <https://github.com/tensorflow/tfjs/issues/4637#issuecomment-859557452>;>> or unsubscribe>> <https://github.com/notifications/unsubscribe-auth/AHGJKVHNEHCPPGNS7WGSJB3TSIAZLANCNFSM4XCZ3W4A>>> .>>>=====""; '@dewball345 I really appreciate it. Awaited the good news!====='; '@pravastacaraka Okay; i implemented the nms as well as some pre/post processing; and you can view it [here](https://github.com/dewball345/ear.js)====='; 'hello @pravastacaraka : i am still getting this error on using the ssd_mobilenet v2 model.![22399021-BB51-4439-A318-AA0C18A7CE58](https://user-images.githubusercontent.com/67813597/130784452-eaff2d6d-ef7f-4aeb-bb82-4ac11019d443.jpeg)I used the repo and training from tensorflow blog below.https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.htmlIs there any help you all can offer. I am bit new to this object detection and I think I am stuck.====='; ""This shouldnt be an error with the model? Looks like you were using some template and forgot to change the number of classes to your amount... I'm guessing the index i was greater than the number of classes that you had in your model; but I'm not sure=====""; 'Hi @dewball345 : I will recheck this but I am quite sure I had changed the number of classes to 6 which is what my required number of classes are supposed to be. However; the prediction is returning a number like ""1190"" which is thus obviously outside the expected classes json defined earlier in the program. that is also why the model is not able to get the name for class 1190 which is not in my master definition. but the problem is; this same model is able to detect the objects correctly on server using python. the converted model to tensorflowjs is where the problem is. Would you have any idea on why this can be ?=====']",Reference Error,Crash,Inconsistent Modules,Operator,API,re-converter model,Changing model,framework,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/4624,Unknown op: BroadcastArgs,2,open,2021-02-01T02:26:25Z,2021-06-03T16:15:36Z,**System information**- TensorFlow.js version (you are using): 3.0.0- Are you willing to contribute it (Yes/No): No; but can if told what to change and where**Describe the feature and the current behavior/state.**BroadcastArgs are not supported. [tf.linspace](https://www.tensorflow.org/api_docs/python/tf/linspace) changed implementations between TF versions 2.2.0 and 2.3.0 to use BroadcatsArgs and now models using linspace cannot be converted to TFJS.An example model:```import tensorflow as tftf.keras.Sequential([tf.keras.Input(shape=(4;)); tf.keras.layers.Lambda(lambda x: tf.linspace(0; 1024; 1))])```If a model like this is run through the tfjs converter the following error is returned:```Unsupported Ops in the model before optimizationBroadcastArgs```**Will this change the current api? How?**No**Who will benefit with this feature?**People who use functions that use BroadcastArgs such as linspace**Any Other info.**,['Facing the same issue when converting polices trained by tf agents to tensorflowjs models.====='; 'cc @pyu10055 @mattsoulanille ====='],Reference Error,Crash,Unimplemented Operator,Operator,API,add support for operator,Add unsupported operator,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4623,Performance issue with tf.signal.stft,8,closed,2021-01-31T18:37:47Z,2021-06-29T01:52:16Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm i @tensorflow/tf-node- TensorFlow.js version (use command below): 2.8.5; but master branch is affected as well- Browser version: N/A- Tensorflow.js Converter Version: N/A**Describe the current behavior**The tf.signal.stft function takes a 1D tensor; and calculates the FFT over a sliding window. The expected result is a 2D Tensor where all the strides contain the FFT according to the input parameters. I believe there's a performance bottleneck in the stft.ts implementation; in the stft_ function: the input (and processed) tensor is sliced into 1D tensors; and rfft is called in a loop. The output is aggregated into an output buffer; which is concatenated and then returned. **Describe the expected behavior**The called tf.spectra.rfft function is batch-aware. There's no reason to slice the processed tensor in stft_; you can pass windowedSignal to the rfft function; still get the same result. The benefit is; that there's no extra memory and processing required to slice/concat the results. Also; in rfft (where fft is called; which is a kernel function) gets a boost (may the backend be in node-cpu / -gpu / webgl / etc) from the calls working on larger set of data. My measurements show at least 3-fold performance increase (see example) using tf-node-cpu.Please consider reviewing the sfft_ function.**Standalone code to reproduce the issue**https://gist.github.com/harangp/b4c21a6c1ce9e2c8b0c1da804dde784d**Other info / logs**Output on my machine (i5 7th gen running at 3Ghz):```Standard aproach ----------------2021-01-31T18:17:31.568Z 'ellapsed time:' 1102021-01-31T18:17:31.650Z 'ellapsed time:' 802021-01-31T18:17:31.790Z 'ellapsed time:' 1392021-01-31T18:17:31.847Z 'ellapsed time:' 562021-01-31T18:17:31.903Z 'ellapsed time:' 552021-01-31T18:17:31.961Z 'ellapsed time:' 572021-01-31T18:17:32.010Z 'ellapsed time:' 482021-01-31T18:17:32.059Z 'ellapsed time:' 482021-01-31T18:17:32.113Z 'ellapsed time:' 532021-01-31T18:17:32.161Z 'ellapsed time:' 472021-01-31T18:17:32.210Z 'ellapsed time:' 482021-01-31T18:17:32.262Z 'ellapsed time:' 512021-01-31T18:17:32.314Z 'ellapsed time:' 512021-01-31T18:17:32.362Z 'ellapsed time:' 472021-01-31T18:17:32.415Z 'ellapsed time:' 522021-01-31T18:17:32.465Z 'ellapsed time:' 492021-01-31T18:17:32.510Z 'ellapsed time:' 442021-01-31T18:17:32.552Z 'ellapsed time:' 422021-01-31T18:17:32.607Z 'ellapsed time:' 542021-01-31T18:17:32.655Z 'ellapsed time:' 48Sped up approach ----------------2021-01-31T18:17:32.667Z 'ellapsed time:' 112021-01-31T18:17:32.685Z 'ellapsed time:' 92021-01-31T18:17:32.696Z 'ellapsed time:' 102021-01-31T18:17:32.717Z 'ellapsed time:' 92021-01-31T18:17:32.728Z 'ellapsed time:' 102021-01-31T18:17:32.746Z 'ellapsed time:' 122021-01-31T18:17:32.768Z 'ellapsed time:' 92021-01-31T18:17:32.779Z 'ellapsed time:' 112021-01-31T18:17:32.799Z 'ellapsed time:' 112021-01-31T18:17:32.808Z 'ellapsed time:' 92021-01-31T18:17:32.828Z 'ellapsed time:' 112021-01-31T18:17:32.848Z 'ellapsed time:' 122021-01-31T18:17:32.871Z 'ellapsed time:' 182021-01-31T18:17:32.884Z 'ellapsed time:' 122021-01-31T18:17:32.903Z 'ellapsed time:' 92021-01-31T18:17:32.915Z 'ellapsed time:' 122021-01-31T18:17:32.938Z 'ellapsed time:' 112021-01-31T18:17:32.964Z 'ellapsed time:' 112021-01-31T18:17:32.984Z 'ellapsed time:' 122021-01-31T18:17:32.993Z 'ellapsed time:' 9Does it return the same results?Tensor    true```,"[""Hi; @harangp!I have taken the liberty of creating a test of this issue for web client. Also I've added 3 new test tensors (zeros; randomNormal and randomUniform) to verify that all is working at it should.In my case I've noticed a ~10x improvement on webgl backend. I was unable to test it with the wasm backend as it doesn't currently support complex tensors.My standalone code to reproduce the issue:https://gist.github.com/JPery/70df24354427475e72d2adcf33e94928Output log on my machine (i7 4th gen running at 2;3Ghz with Nvidia 750M) using Google Chrome (v88.0.4324.96):```index.html:108 Backend: webglindex.html:67 ****************  testTensor: ones ****************index.html:69 ---------------- Warming up ----------------DevTools failed to load SourceMap: Could not load content for https://cdn.jsdelivr.net/npm/@tensorflow/tf.min.js.map: HTTP error: status code 404; net::ERR_HTTP_RESPONSE_CODE_FAILUREindex.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 76index.html:53 Elapsed time: 66index.html:53 Elapsed time: 48index.html:53 Elapsed time: 42index.html:53 Elapsed time: 39index.html:53 Elapsed time: 42index.html:53 Elapsed time: 53index.html:53 Elapsed time: 52index.html:53 Elapsed time: 57index.html:53 Elapsed time: 55index.html:53 Elapsed time: 54index.html:53 Elapsed time: 46index.html:53 Elapsed time: 51index.html:53 Elapsed time: 47index.html:53 Elapsed time: 49index.html:53 Elapsed time: 64index.html:53 Elapsed time: 46index.html:53 Elapsed time: 44index.html:53 Elapsed time: 58index.html:53 Elapsed time: 51index.html:78 AVG tf.signal.stft 52index.html:79 STDev tf.signal.stft 8.79772697916911index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 22index.html:53 Elapsed time: 3index.html:53 Elapsed time: 2index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 13index.html:53 Elapsed time: 9index.html:53 Elapsed time: 62index.html:53 Elapsed time: 132index.html:53 Elapsed time: 7index.html:53 Elapsed time: 43index.html:53 Elapsed time: 2index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 2index.html:90 AVG custom_stft 5.2index.html:91 STDev custom_stft 3.8026306683663087index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: zeros ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 67index.html:53 Elapsed time: 68index.html:53 Elapsed time: 61index.html:53 Elapsed time: 58index.html:53 Elapsed time: 51index.html:53 Elapsed time: 64index.html:53 Elapsed time: 62index.html:53 Elapsed time: 49index.html:53 Elapsed time: 41index.html:53 Elapsed time: 68index.html:53 Elapsed time: 65index.html:53 Elapsed time: 67index.html:53 Elapsed time: 88index.html:53 Elapsed time: 78index.html:53 Elapsed time: 68index.html:53 Elapsed time: 65index.html:53 Elapsed time: 58index.html:53 Elapsed time: 78index.html:53 Elapsed time: 59index.html:53 Elapsed time: 68index.html:78 AVG tf.signal.stft 64.15index.html:79 STDev tf.signal.stft 10.209187039132939index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 19index.html:53 Elapsed time: 9index.html:53 Elapsed time: 5index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 9index.html:53 Elapsed time: 6index.html:53 Elapsed time: 10index.html:53 Elapsed time: 9index.html:53 Elapsed time: 5index.html:53 Elapsed time: 6index.html:53 Elapsed time: 3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 7index.html:53 Elapsed time: 53index.html:53 Elapsed time: 4index.html:90 AVG custom_stft 6.65index.html:91 STDev custom_stft 3.439113257803529index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: randomNormal ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 49index.html:53 Elapsed time: 57index.html:53 Elapsed time: 45index.html:53 Elapsed time: 55index.html:53 Elapsed time: 68index.html:53 Elapsed time: 45index.html:53 Elapsed time: 53index.html:53 Elapsed time: 39index.html:53 Elapsed time: 42index.html:53 Elapsed time: 43index.html:53 Elapsed time: 52index.html:53 Elapsed time: 38index.html:53 Elapsed time: 43index.html:53 Elapsed time: 44index.html:53 Elapsed time: 482index.html:53 Elapsed time: 44index.html:53 Elapsed time: 95index.html:53 Elapsed time: 51index.html:53 Elapsed time: 59index.html:78 AVG tf.signal.stft 50.7index.html:79 STDev tf.signal.stft 12.446284586172695index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------index.html:53 Elapsed time: 7index.html:53 Elapsed time: 5index.html:53 Elapsed time: 8index.html:53 Elapsed time: 4index.html:53 Elapsed time: 10index.html:53 Elapsed time: 14index.html:53 Elapsed time: 6index.html:53 Elapsed time: 8index.html:53 Elapsed time: 5index.html:53 Elapsed time: 9index.html:53 Elapsed time: 6index.html:53 Elapsed time: 7index.html:53 Elapsed time: 6index.html:53 Elapsed time: 42index.html:53 Elapsed time: 5index.html:53 Elapsed time: 6index.html:53 Elapsed time: 4index.html:53 Elapsed time: 5index.html:53 Elapsed time: 15index.html:90 AVG custom_stft 6.95index.html:91 STDev custom_stft 2.991237202229205index.html:93 Does it return the same results?index.html:102 trueindex.html:67 ****************  testTensor: randomUniform ****************index.html:69 ---------------- Warming up ----------------index.html:74 ---------------- Standard aproach ----------------index.html:53 Elapsed time: 34index.html:53 Elapsed time: 48index.html:53 Elapsed time: 49index.html:53 Elapsed time: 63index.html:53 Elapsed time: 91index.html:53 Elapsed time: 81index.html:53 Elapsed time: 70index.html:53 Elapsed time: 57index.html:53 Elapsed time: 75index.html:53 Elapsed time: 88index.html:53 Elapsed time: 106index.html:53 Elapsed time: 3702index.html:53 Elapsed time: 50index.html:53 Elapsed time: 59index.html:53 Elapsed time: 72index.html:53 Elapsed time: 86index.html:53 Elapsed time: 69index.html:53 Elapsed time: 56index.html:53 Elapsed time: 57index.html:78 AVG tf.signal.stft 81.55index.html:79 STDev tf.signal.stft 68.38309367087744index.html:81 ---------------- Warming up ----------------index.html:85 ---------------- Speed up approach ----------------3index.html:53 Elapsed time: 4index.html:53 Elapsed time: 8index.html:53 Elapsed time: 53index.html:53 Elapsed time: 4index.html:53 Elapsed time: 32index.html:53 Elapsed time: 6index.html:53 Elapsed time: 42index.html:53 Elapsed time: 3index.html:53 Elapsed time: 8index.html:53 Elapsed time: 11index.html:53 Elapsed time: 73index.html:53 Elapsed time: 5index.html:90 AVG custom_stft 5.15index.html:91 STDev custom_stft 1.9817921182606413index.html:93 Does it return the same results?index.html:102 true```Thanks so much for reporting this issue and I hope my work helps.Best regards!=====""; ""Hi @JPery ; thanks for taking the time and confirming my find. Also; I like the code you've brought to the web. I've run your code on my machine in a Chrome (v: 87.0.4280.141) as well for reference; here are the results (see below). The interesting thing is my GPU is an integrated Intel HD Graphics 620; but the results are roughly the same. So I would say WebGL definitely packs a punch here.```stftLayerExperiment_2.html:108 Backend: webglstftLayerExperiment_2.html:67 ****************  testTensor: ones ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------DevTools failed to load SourceMap: Could not load content for https://cdn.jsdelivr.net/npm/@tensorflow/tf.min.js.map: HTTP error: status code 404; net::ERR_HTTP_RESPONSE_CODE_FAILUREstftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 122stftLayerExperiment_2.html:53 Elapsed time: 101stftLayerExperiment_2.html:53 Elapsed time: 75stftLayerExperiment_2.html:53 Elapsed time: 91stftLayerExperiment_2.html:53 Elapsed time: 112stftLayerExperiment_2.html:53 Elapsed time: 83stftLayerExperiment_2.html:53 Elapsed time: 60stftLayerExperiment_2.html:53 Elapsed time: 59stftLayerExperiment_2.html:53 Elapsed time: 38stftLayerExperiment_2.html:53 Elapsed time: 45stftLayerExperiment_2.html:53 Elapsed time: 47stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 43stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 30stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:78 AVG tf.signal.stft 57.3stftLayerExperiment_2.html:79 STDev tf.signal.stft 28.81683535713108stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 11stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 73stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 52stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:53 Elapsed time: 6stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:90 AVG custom_stft 5.4stftLayerExperiment_2.html:91 STDev custom_stft 2.267156809750927stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: zeros ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 50stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 37stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 51stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:53 Elapsed time: 352stftLayerExperiment_2.html:53 Elapsed time: 31stftLayerExperiment_2.html:53 Elapsed time: 29stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 20stftLayerExperiment_2.html:53 Elapsed time: 34stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 95stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 24stftLayerExperiment_2.html:53 Elapsed time: 240stftLayerExperiment_2.html:78 AVG tf.signal.stft 46.8stftLayerExperiment_2.html:79 STDev tf.signal.stft 47.226687370595876stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 10stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 6stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 3.65stftLayerExperiment_2.html:91 STDev custom_stft 1.796524422322168stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: randomNormal ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 51stftLayerExperiment_2.html:53 Elapsed time: 442stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 282stftLayerExperiment_2.html:53 Elapsed time: 19stftLayerExperiment_2.html:53 Elapsed time: 16stftLayerExperiment_2.html:53 Elapsed time: 20stftLayerExperiment_2.html:53 Elapsed time: 25stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 26stftLayerExperiment_2.html:53 Elapsed time: 44stftLayerExperiment_2.html:53 Elapsed time: 206stftLayerExperiment_2.html:53 Elapsed time: 39stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:78 AVG tf.signal.stft 38.2stftLayerExperiment_2.html:79 STDev tf.signal.stft 39.60378769764327stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 55stftLayerExperiment_2.html:53 Elapsed time: 80stftLayerExperiment_2.html:53 Elapsed time: 15stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 9stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 7stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 13stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 10.35stftLayerExperiment_2.html:91 STDev custom_stft 19.71363741170056stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 truestftLayerExperiment_2.html:67 ****************  testTensor: randomUniform ****************stftLayerExperiment_2.html:69 ---------------- Warming up ----------------stftLayerExperiment_2.html:74 ---------------- Standard aproach ----------------stftLayerExperiment_2.html:53 Elapsed time: 27stftLayerExperiment_2.html:53 Elapsed time: 30stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 29stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 23stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 36stftLayerExperiment_2.html:53 Elapsed time: 22stftLayerExperiment_2.html:53 Elapsed time: 41stftLayerExperiment_2.html:53 Elapsed time: 42stftLayerExperiment_2.html:53 Elapsed time: 19stftLayerExperiment_2.html:53 Elapsed time: 21stftLayerExperiment_2.html:53 Elapsed time: 54stftLayerExperiment_2.html:53 Elapsed time: 82stftLayerExperiment_2.html:53 Elapsed time: 853stftLayerExperiment_2.html:53 Elapsed time: 40stftLayerExperiment_2.html:53 Elapsed time: 27stftLayerExperiment_2.html:53 Elapsed time: 28stftLayerExperiment_2.html:78 AVG tf.signal.stft 74.45stftLayerExperiment_2.html:79 STDev tf.signal.stft 179.1715588479377stftLayerExperiment_2.html:81 ---------------- Warming up ----------------stftLayerExperiment_2.html:85 ---------------- Speed up approach ----------------2stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 4stftLayerExperiment_2.html:53 Elapsed time: 3stftLayerExperiment_2.html:53 Elapsed time: 52stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 5stftLayerExperiment_2.html:53 Elapsed time: 8stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 32stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:53 Elapsed time: 35stftLayerExperiment_2.html:53 Elapsed time: 2stftLayerExperiment_2.html:90 AVG custom_stft 3.1stftLayerExperiment_2.html:91 STDev custom_stft 1.5459624833740309stftLayerExperiment_2.html:93 Does it return the same results?stftLayerExperiment_2.html:102 true```=====""; '@harangp @JPery Awesome work for great perf improvement; any of you want to contribute the change to the library? thanks!====='; '> @harangp @JPery Awesome work for great perf improvement; any of you want to contribute the change to the library? thanks!Yes; happy to (will be first time; but one must start somewhere).====='; 'https://github.com/tensorflow/tfjs/pull/4790Every check is green. Is there anything else I should do with this?====='; '@pyu10055 gentle ping to review the PR====='; 'The related PR has been merged ; so closing the issue. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4623"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4623"">No</a>=====']",Slow Execution,Poor Performance,Incorrect Code Logic,Operator,API,remove unnecessary code,remove unnecessary code,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.1.1,A.4
https://github.com/tensorflow/tfjs/issues/4606,Kernel 'RotateWithOffset' not registered for backend 'tensorflow',7,closed,2021-01-27T14:18:28Z,2021-09-10T11:11:55Z,"**System information**- I have written custom code- macOS Catalina 10.15.7- TensorFlow.js installed from npm- TensorFlow.js version 3.0.0- Browser version: Not in browser- Tensorflow.js Converter Version: 3.0.0**Describe the current behavior**When running project with Node backend ""tensorflow""; getting error:```(node:2424) UnhandledPromiseRejectionWarning: Error: Kernel 'RotateWithOffset' not registered for backend 'tensorflow'    at Engine.runKernel(/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3053:19)    at rotateWithOffset_ (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:20785:22)    at Object.rotateWithOffset__op [as rotateWithOffset] (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3968:29)    at HandPipeline.<anonymous> (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:161:49)    at step (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:48:23)    at Object.next (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:29:53)    at fulfilled (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:20:58)    at runMicrotasks (<anonymous>)    at processTicksAndRejections (internal/process/task_queues.js:93:5)(Use `node --trace-warnings ...` to show where the warning was created)(node:2424) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection; use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:2424) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```**Describe the expected behavior**Code works without errors**Steps to reproduce the issue**1. npm install @tensorflow/tfjs-node2. Set tf backend to 'tensorflow'3. Run standard example**Other info / logs**```(node:2424) UnhandledPromiseRejectionWarning: Error: Kernel 'RotateWithOffset' not registered for backend 'tensorflow'    at Engine.runKernel(/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3053:19)    at rotateWithOffset_ (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:20785:22)    at Object.rotateWithOffset__op [as rotateWithOffset] (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3968:29)    at HandPipeline.<anonymous> (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:161:49)    at step (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:48:23)    at Object.next (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:29:53)    at fulfilled (/Users/me/Documents/Projects/MCH/node_modules/@tensorflow-models/handpose/dist/pipeline.js:20:58)    at runMicrotasks (<anonymous>)    at processTicksAndRejections (internal/process/task_queues.js:93:5)(Use `node --trace-warnings ...` to show where the warning was created)(node:2424) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection; use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:2424) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```","[""@MiloAkerman - our tensorflow backend does not yet support handpose because handpose runs a custom operation 'rotateWithOffset' which we have so far only implemented in the 'cpu'|'webgl'|'wasm' backends.=====""; 'Alright. I got it fixed by dropping node completely and using sockets to communicate between the browser and my node process. Thank you====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4606"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4606"">No</a>====='; '@rthadur Do you have any plans to add it?Thank you====='; 'I have the same problem. any update?====='; ""same issue but with `Kernel 'Log1p'` instead of `Kernel 'RotateWithOffset'`.  I am using wasm as the backend and tfjs attempts to retrieve the threaded-simd wasm file.  I downloaded that file from the CDN and set the location of the file in my project with `setWasmPaths('/')`.  Version of `@tensorflow/tfjs-backend-wasm`; `@tensorflow/tfjs-backend-wasm` and the version of `tfjs-backend-wasm-threaded-simd.wasm` are all 3.7.0.  Are there general troubleshooting measures to take for this error?=====""; 'Does this mean it\'s currently impossible to run the `@tensorflow-models/face-landmarks-detection` model on a node server? I\'m trying to run it in an `expressJs` server to move the heavy lifting from the browser.From my package.json``` ""@tensorflow-models/face-landmarks-detection"": ""0.0.3""; ""@tensorflow/tfjs-node"": ""^3.9.0"";``````const tf = require(""@tensorflow/tfjs-node"");const faceLandmarksDetection = require(""@tensorflow-models/face-landmarks-detection"");const { Buffer } = require(""buffer"");....     try {      const model = await faceLandmarksDetection.load(        faceLandmarksDetection.SupportedPackages.mediapipeFacemesh      );      const img = dataObj.data_url.replace(        /^data:image\\/(png|jpeg);base64;/;        """"      );      const b = Buffer.from(img; ""base64"");      const tensor = tf.node.decodeImage(b; 3);      // Throws error => Error: Kernel \'RotateWithOffset\' not registered for backend \'tensorflow\'      const preds = await model.estimateFaces({        input: tensor;      });      console.log(preds);    } catch (err) {      console.log(err);    }```=====']",Reference Error,Crash,Unimplemented Operator,TF(CPU),Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4595,tf.unique failing on Node only with `Error running Unique: Neither modular kernel nor forward func passed`,4,open,2021-01-25T02:35:24Z,2021-08-16T21:03:37Z,"The code ```tsconst tf = require('@tensorflow/tfjs-node')const a = tf.tensor1d([1; 1; 2; 4; 4; 4; 7; 8; 8]);const {values; indices} = tf.unique(a);```Fails on **windows** and **mac** for Node.jsIf you switch backend to CPU it works.Here it is failing on node<img width=""1456"" alt=""Screen Shot 2021-01-24 at 8 31 28 PM"" src=""https://user-images.githubusercontent.com/997157/105654244-35b96000-5e83-11eb-8d30-90c5ba88379d.png"">But when using CPU backend it works.See https://github.com/GantMan/unique_fail_poc for failure and success reproductions with latest release of TFJS Node.  Let me know if I can help with anything.   Tested on two machines; and it fails on quite a few versions of TFJS; btw.  Lots of stuff works just fine; it's just tf.unique that seems angry.","[""@GantMan thank you for detailed report ; tf.unique is only supported in 'cpu' and 'webgl' for now ; this would be a feature request to add it to Node. =====""; 'cc @jinjingforever ====='; ""I think it would be wise to add some kind of notation or caveat to the docs where functions are unsupported.  A small note on the [docs](https://js.tensorflow.org/api/latest/#unique) would identify this is not a bug.Secondly; as another feature; better error messages would be a good feature as well.  It took me a while to find out what the issue was.  I'll make a note of this and try to contribute back if this issue is still outstanding when I get some cycles.=====""; 'Hello; any news relating to this feature? I think getting the unique values in a tensor in a core method and should be also implemented for node=====']",Reference Error,Crash,Unimplemented Operator,TF(CPU),Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4592,Minor typo (I believe),4,closed,2021-01-23T23:08:26Z,2021-02-02T18:51:33Z,```Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: [path]\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v7\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source' to rebuild the native addon module.```> Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source'should be> Please run command 'npm rebuild @tensorflow/tfjs-node-gpu --build-addon-from-source'if I'm not mistaken...I'd make a Pull Request myself if I knew which file to change.,"['Here is the file https://github.com/tensorflow/tfjs/blob/master/tfjs-node/src/index.ts#L43 ; where you can submit a PR. Thank you ====='; ""Oh; is that file used for both the GPU and non-GPU versions? Would the best solution be to add a conditional like```js// error message`'npm rebuild @tensorflow/tfjs-node`+(String(bindingPath).indexOf('@tensorflow\\tfjs-node-gpu\\') > 0 ? `-gpu` : ``) +` --build-addon-from-source' to ` +// error message```?=====""; 'cc @pyu10055 @lina128 ====='; '@EFHIII looks good; I will suggest to just check for string `tfjs-node-gpu` instead to avoid dealing with file separators.=====']",Build & Install Failure,Build & Initialization Failure,Improper Exception Handling,TF(CPU),Backend,change error message,change error message,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Untimely Update""
  }
}
```",C,A.7
https://github.com/tensorflow/tfjs/issues/4580,ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from tensorflowjs),2,closed,2021-01-20T11:41:56Z,2021-01-20T20:49:46Z,I am following the NMT with attention tutorial from tensorflow docs. My requirement is to port the same model to tfjs. the tensorflow/tfjs repository readme.md file has a note saying -_Note: Session bundle format have been deprecated in TensorFlow.js 1.0. Please use the TensorFlow.js 0.15.x backend to convert session bundle; available in tfjs-converter 0.8.6._...as well as..._7. I have a model formatted as a Session bundle. How do I convert it to TensorFlow.js?_.....`pip install tensorflowjs==0.8.6`following the readme.md; i created a venv and tried installing tensorflowjs 0.8.6.i got the following error -```Collecting tensorflowjs==0.8.6  Using cached tensorflowjs-0.8.6-py3-none-any.whl (39 kB)ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from tensorflowjs)ERROR: No matching distribution found for tensorflow==1.13.1```how do i install tensorflowjs 0.8.6 and convert my session bundle to tfjs?,"['Lets track this issue at one place here https://github.com/tensorflow/tfjs/issues/4566 ; thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4580"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4580"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,,Backend,change dependency version,Modifying dependency configuration,framework,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Version Requirement Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4542,Error: The dict provided in model.execute(dict) has keys: [ToFloat] that are not part of graph,3,open,2021-01-15T18:14:54Z,2021-10-13T15:13:21Z,This template is for miscellaneous issues not covered by the other issue categories.For questions on how to work with TensorFlow.js; or support for problems that are not verified bugs in TensorFlow.js; please go to [StackOverflow](https://stackoverflow.com/tags/tensorflow.js).hi all; I have got the captioned error when running the following scripts for Google AutoML multiple object detection. Is there any clue ?[index.txt](https://github.com/tensorflow/tfjs/files/5822253/index.txt)tfjs:17 Uncaught (in promise) Error: The dict provided in model.execute(dict) has keys: [ToFloat] that are not part of graph    at e.t.checkInputs (tfjs:17)    at e.<anonymous> (tfjs:17)    at u (tfjs:17)    at Generator._invoke (tfjs:17)    at Generator.forEach.e.<computed> [as next] (tfjs:17)    at Um (tfjs:17)    at o (tfjs:17)    at tfjs:17    at new Promise (<anonymous>)    at e.<anonymous> (tfjs:17),['@startonggithub is it possible to share the model which you are using ?====='; 'hi rthadur; here is the model. Thanks. [model.zip](https://github.com/tensorflow/tfjs/files/5845242/model.zip)====='; 'having same issue please i need help====='],Data & Model Error,Crash,Unknown,Model API,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,E
https://github.com/tensorflow/tfjs/issues/4539,Regresssion: WASM fails to load due to _scriptDir not defined,4,closed,2021-01-15T13:10:08Z,2021-01-20T13:21:12Z,This is a regression in `tfjs` 2.8.4 compared to 2.8.3  Setting backend to `wasm` on the main thread results in error:```worker.js onmessage() captured an uncaught exception: ReferenceError: _scriptDir is not defined  threadPrintErr @ e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1  onmessage @ e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1ReferenceError: _scriptDir is not defined  WasmBackendModuleThreadedSimd (a0196e5e-0cae-4d78-90e3-7f0714096dfc:1)  onmessage (e67a52fd-9fdd-4d80-87b0-5a39d7cd567d:1)```No issues running TFJS 2.8.3 or older.This is likely related to pull request #4528,"['Hi Vladimir; Thanks for reporting! This is a little bit strange because we did test it in 2.8.4. See this codepin: https://codepen.io/jing-jin/pen/BaLqJoE. If you click ""Settings""; you can see that the program is loading tfjs core and wasm backend @ 2.8.4.Could you please provide more context/sample code about this issue? Thanks!====='; ""Ok; this is related to both #4392 and #4528.  Originally `tfjs-backend-wasm` was not fully minification safe when running through a bundler - minify whitespace and minify syntax were ok; but minify identifiers was not. Then #4392 fixed it and everything was fine for a while.Now #4528 (i assume its this one since i don't see other differences) broke it once more and it's even worse since not even minify syntax works anymore.  If I disable minification completely; then yes `wasm` works in 2.8.4 - but that is not realistic for production code.So yes; it's a regression.  If you want to see a simple project; it's at <https://github.com/vladmandic/stocks> and bundler runs via `server/build.js`=====""; 'closing as fixed in tfjs 2.8.5.  thank you!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4539"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4539"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Dependency Error,Wasm,Backend,change code order,Adjust API invocation sequence,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4534,@tensorflow/tfjs-node package.json file missing license value,1,closed,2021-01-14T18:57:26Z,2021-01-15T17:48:17Z,"Hi; I'm trying to get tfjs-node added into our corporate repository; but our process requires the package.json file to have the license header included.@tensorflow/tfjs can be added easily; because it has the value ""Apache-2.0"" in the license header; but tfjs-node doesn't.Is it intended that tfjs-node would not have the license header within its package.json file?Thanks in advance! ",['Added license filed in tfjs-node package.json====='],Build & Install Failure,Build & Initialization Failure,Misconfiguration,TF(CPU),Backend,configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4525,wasm backend returns incorrect values as result,7,closed,2021-01-13T15:11:44Z,2021-02-24T01:10:42Z,I have a very simple float32 model (link provided in the code below) that takes image with values in range of 0..255 as input and returns a single float32 value.  Works perfectly with `webgl` backend; but values explode using `wasm` backend.  E.g.; instead of returning value in range of 1-99; it returns values like 700+.  This is just one more case that looks like incorrect cast inside `wasm` backend; likely related to #4326 and #4311 that I've reported earlier.  simple reproduction code:```jsconst resizeT = tf.image.resizeBilinear(image; [64; 64]; false); // image is any picture of a faceconst normalizeT = tf.mul(resizeT; [255.0]);resizeT.dispose();const model = await tf.loadGraphModel('https://vladmandic.github.io/human/models/age-ssrnet-imdb.json');const ageT = await model.predict(normalizeT);normalizeT.dispose();const age = ageT.dataSync();ageT.dispose();console.log(age);```Environment: TFJS 2.8.3 on Chrome 87 / Windows 10,"['@rthadur @annxingyuan @pyu10055 @lina128 sorry for the spam;  but i have 3 issues filled (#4525; #4326; #4311) for `wasm` backend (with full reproduction code)  that make `wasm` pretty much unusable.  and all likely have the same root cause; just symptoms are different.but none received any updates since they were filed (oldest was 2.5 months ago).====='; 'Hi; VladimirThank you for your patience!I took a look at this and found that: if I disable simd; the result from WASM backend matches the webgl result. If simd is enabled (with or without multi-threading support); the result is wrong; like you mentioned. Our code uses different flags/options to build WASM module for different combinations of simd+multi-threading support. At this point it looks like that there are some potential bugs in xnnpack/simd. I will follow up with my colleagues and will update here if more comes up.Thanks! ====='; '@jinjingforever thanks for the update====='; 'BTW we further noticed that the wrong results come from some of the ""div"" nodes. With SIMD enabled; those nodes produce the results with the numerator and the denominator swapped.. I am asking folks from the xnnpack project to investigate further. Thanks!====='; ""@jinjingforever any update from xnnpack team?i've chedked xnnpack git repository and there is nothing simmilar mentioned there.=====""; '(sorry; I was on vacation last week)Today we figured out that the error was caused by out-of-date xnnpack+emscripten. I am in the process of updating our toolchain to the latest xnnpack and emscripten. Hopefully it can be done this week. Thanks!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4525"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4525"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Dependency Error,Wasm,Backend,change dependency version,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",D,B.2
https://github.com/tensorflow/tfjs/issues/4524,tfjs-backend-wasm-threaded-simd.wasm is not found starting from 2.8.1,2,closed,2021-01-12T20:18:40Z,2021-01-14T19:38:35Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below): 2.8.1- Browser version:- Tensorflow.js Converter Version: 2.8.1**Describe the current behavior**Reproduce steps:1. Turn on simd flag by going to chrome://flag and enable simd option2. Run this simple codepen example: https://codepen.io/na-li/pen/XWXqgdM3. Error message in developer console: Initialization of backend wasm failed. RuntimeError: abort(both async and sync fetching of the wasm failed). Build with -s ASSERTIONS=1 for more info.**Describe the expected behavior**Expect simd wasm to load successfully. Change above codepen's dependency from 2.8.1 to 2.8.0 to see the normal behavior.For reference; diff between 2.8.0 and 2.8.1: https://github.com/tensorflow/tfjs/releases/tag/tfjs-v2.8.1,"['https://github.com/tensorflow/tfjs/pull/4528 fixed it; it has been released in 2.8.4.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4524"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4524"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,Wasm,Backend,type/env checker,Fix environment adaptability,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": """"
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.4
https://github.com/tensorflow/tfjs/issues/4514,Unsupported system error when installing on M1 / Apple Silicon,84,closed,2021-01-09T02:59:29Z,2021-12-21T19:03:30Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 11.0.1 (20B29)- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 2.8.3**Describe the problem**Installation fails due to a missing precompiled `libtensorflow` for darwin / arm64. Apple has [their own fork](https://github.com/apple/tensorflow_macos) of TensorFlow supporting their chips-- is there a way to tell the installation script to use their `libtensorflow.dylib`?**Provide the exact sequence of commands / steps that you executed before running into the problem**```npm install @tensorflow/tfjs-node --save```**Any other info / logs**```npm ERR! code 1npm ERR! path /Users/nicklee/Documents/Development/<redacted>/src/<redacted>/node_modules/@tensorflow/tfjs-nodenpm ERR! command failednpm ERR! command sh -c node scripts/install.jsnpm ERR! CPU-darwin-2.8.3.tar.gznpm ERR! * Downloading libtensorflownpm ERR! /Users/nicklee/Documents/Development/<redacted>/src/<redacted>/node_modules/@tensorflow/tfjs-node/scripts/install.js:100npm ERR!     throw new Error(`Unsupported system: ${libType}-${platform}-${os.arch()}`);npm ERR!           ^npm ERR! npm ERR! Error: Unsupported system: cpu-darwin-arm64npm ERR!     at getPlatformLibtensorflowUri (/Users/nicklee/Documents/Development/<redacted>/src/<redacted>/node_modules/@tensorflow/tfjs-node/scripts/install.js:100:11)npm ERR!     at downloadLibtensorflow (/Users/nicklee/Documents/Development/<redacted>/src/<redacted>/node_modules/@tensorflow/tfjs-node/scripts/install.js:134:7)npm ERR!     at async run (/Users/nicklee/Documents/Development/<redacted>/src/<redacted>/node_modules/@tensorflow/tfjs-node/scripts/install.js:197:5)npm ERR! A complete log of this run can be found in:npm ERR!     /Users/nicklee/.npm/_logs/2021-01-09T02_53_09_673Z-debug.log```,"['@nickplee thank you for reporting this; can you help to test it out following the instruction of custom TF buildhttps://github.com/tensorflow/tfjs/tree/master/tfjs-node#optional-build-optimal-tensorflow-from-source@yhwang any suggestions on this? thanks.====='; ""since the tfjs-node@2.8.3 is using tensorflow shared libs v1.15.0; I think it's better to build the dependent shared libs by using the link @pyu10055 posted [above](https://github.com/tensorflow/tfjs/tree/master/tfjs-node#optional-build-optimal-tensorflow-from-source). However; the tricky part would be tweaking the tensorflow to build on darwin arm64. It's okay by using ubuntu with arm64 to build tensorflow shared libs. But I am not sure about darwin with arm64.For the tensorflow apple provides; you can verify whether you can find all files in [this tarball](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-1.15.0.tar.gz) from the binaries that apple provides. If yes; you can just put them under `node_modules/@tensorflow/tfjs-node/lib/deps` where you install the tfjs-node npm package. Then you can run `npm run build-addon-from-source` under `node_modules/@tensorflow/tfjs-node`. It will build the node binding for you. If everything goes well; you can try to use `tfjs-node`. Again; seems apple provides newer version then 1.15.0; I don't know if you can run tfjs-node without any issue even you can build the node binding successfully.=====""; '@pyu10055 a side question: any idea of supporting more platforms and architectures officially?====='; ""@yhwang @pyu10055 I actually already tried subbing in the Apple library and recompiling the binding. Once you install it; it includes the dylib as well as the headers.I was hopeful; but ran in to two issues:- `TF_StringDecode`; `TF_StringEncode`; and `TF_StringEncodedSize` are used by the binding and were deprecated/removed from the TensorFlow C API in 2.4.0- After removing the usages of those functions (and replacing them with what I think is the right alternative implementation) the compiled binding crashed in weird ways. I don't know enough about TensorFlow to be able to explain in more detail unfortunately.=====""; '@yhwang It would definitely be useful to official support other platforms officially; but no plan as this moment; might be a good project for the SIG; any suggestions are welcome.@nickplee Do you mind contriubting your fix attempt as an PR?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; ""Perhaps with https://github.com/tensorflow/tensorflow/pull/45404 this is more feasible? Has anyone been able to get tfjs-node to work on an M1 device?We're extremely interested in this as well. If anyone has been able to get it working; we would very; very much appreciate some pointers :)=====""; 'Closing as stale. Please @mention us if this needs more attention.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4514"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4514"">No</a>====='; 'Can this be reopened or someone point us in the direction of where we should go to help? Support for the new M1 macs is essential for any applications running edge TF models.====='; '@patrickhulce reopened====='; ""Thank you @rthadur! Is there anything that we can help with? My current understanding based on https://github.com/tensorflow/tfjs/issues/4514#issuecomment-758334433 is that there a few patches (possibly that @nickplee already has written?) to work with the 2.4 API *or* attempt to rebuild 2.3 using the methods described in https://github.com/tensorflow/tensorflow/pull/45404.My M1 rig is arriving this week; so I'll be able to provide more hands-on assistance then if I know where I can be helpful :)=====""; 'Gentle ping for @pyu10055 ====='; ""Just chiming in - I wasn't able to get tfjs/node working on my M1. The moment I import '@tensorflow/tfjs-node'; node just quits out with no error message.=====""; 'Any update on this issue by any chance? Experiencing the exact same dylib error.====='; 'I\'ve just successfully built and ran my models with tfjs-node 3.5.0 with TensorFlow 2.5.0-rc1 on my M1 Macbook Air. From what I could see; the yet-to-be released TF 2.5.0 is where Apple Silicon is supported; but **pre-release** 2.5.0-rc1 is already fully usable.**Caveat:** I didn\'t keep exact track of all the steps; this was written in ""hindsight"" so I maybe I\'m missing something.```# 1) set the build directory at will and clone Tensorflowexport BUILDDIR=~/tf-buildmkdir $BUILDDIR && cd $BUILDDIRgit clone -b v2.5.0-rc1 https://github.com/tensorflow/tensorflow tensorflowcd tensorflow# 2) you\'ll need Bazel 3.7.2 to build TF from source# I\'ve used bazelisk installed with brew under Rosetta# note that the binaries are built for ARM64 regardlessarch -x86_64 bashbrew install bazelisk# 3) the following takes a couple hours aprox on my Macbook air# set the number of jobs to a higher value to speed up the build# or lower to have it in the backgroundUSE_BAZEL_VERSION=3.7.2 bazel build --jobs 4 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow.dylibUSE_BAZEL_VERSION=3.7.2 bazel build --jobs 4 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow_framework.dylib# 4) now install/bind JS libs with NPM# I\'m assuming you have NVM to manage Node versions# brew install nvm if you don\'t have it yetexit # <---------- we leave the Rosetta bash prompt herecd $BUILDDIRmkdir tfjs-test && cd tfjs-testnvm install 16  # <---- I\'ve actually built with 15.8.0; but it\'s working under v16 toonpm init -ynpm i node-pre-gypnpm i @tensorflow/tfjs-node@latest --ignore-scriptscd node_modules/@tensorflow/tfjs-node/mkdir -p deps/lib deps/include/tensorflowcp -r $BUILDDIR/tensorflow/tensorflow/c deps/include/tensorflow/cp -r $BUILDDIR/tensorflow/tensorflow/core deps/include/tensorflow/cp -r $BUILDDIR/tensorflow/bazel-bin/tensorflow/*.dylib deps/lib/npx node-pre-gyp rebuild```That\'s it.You should be able to import/require the Tensorflow JS modules and train/predict without issues. I\'m still testing this; but so far so good. Training speed is ~5x or faster compared my 2015 Intel i7 Macbook Air and 2-3x compared to a 16 core 4GHz Intel Xeon E5 server.You can install the @tensorflow/tfjs-node directory by copying it around; repeating the steps or just doing this in a global context (`-g`; not recommended).====='; 'Thank you @rodrigolive ; @patrickhulce @nickplee can you please confirm if this is resolved on M1 chip with latest release.====='; ""The issue as described is not resolved with @rodrigolive's workaround. Manually hacking `@tensorflow/tfjs-node` to compile has been workable since https://github.com/tensorflow/tensorflow/pull/45404. Could this issue track working by default? i.e. the `@tensorflow/tfjs-node/scripts/install.js` script should not throw a fatal error when run on M1; it should download an arm64 compatible binary instead :)https://github.com/tensorflow/tfjs/blob/b7859539fcc148cfadbefafb08e7cb0f4597022b/tfjs-node/scripts/install.js#L87-L103I might be able to take some time next weekend to give a stab at it if no one else has the bandwidth to do so; but closing would be unfortunate as there's still work to be done in this repo.=====""; ""Note that I've used a release candidate version. I'd say we still need to wait for TensorFlow 2.5.x to be released; that would be the first GA version to fully support Apple Silicon. Then; yes; we can publish the binary to the storage location and update the install scripts to handle `darwin-arm64`. As I understand from the Release Notes for `2.5.0-rc1`; apparently there aren't any breaking changes that would affect TFJS.I'm also not sure about the build flags that should be used; `--config=macos_arm64` seems to fold the correct configuration options. Here are some build configs in case they actually enable M1 features that could improve performance to make sure we publish a fully optimized M1 lib.```\t--config=mkl         \t# Build with MKL support.\t--config=mkl_aarch64 \t# Build with oneDNN support for Aarch64.\t--config=monolithic  \t# Config for mostly static monolithic build.\t--config=numa        \t# Build with NUMA support.\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.```Source: https://github.com/tensorflow/tensorflow/pull/45404=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '@tensorflow-butler please do not close; it is awaiting 2.5.0 release to be completed; but still important.====='; 'Has there been any progress on this? any fixes recently?====='; 'https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0 released 3 days ago.====='; '@all can you please confirm if this is resolved after 2.5 release ? Thank you ====='; 'Not working for me yet; but that is probably due to my own incompetence.Will try some other things after work today. If anyone finds a step by stepway please share.On Mon; May 17; 2021 at 11:23 AM Rajeshwar Reddy T ***@***.***>wrote:> @ALL <https://github.com/ALL> can you please confirm if this is resolved> after 2.5 release ? Thank you>> —> You are receiving this because you commented.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4514#issuecomment-842459788>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ANDJPC75TWVPMWGMTLTK5CLTOE7IHANCNFSM4V3HOZSQ>> .>====='; 'I basically followed what @rodrigolive mentioned above cloning this time the v2.5.0 tag. Tensorflow built based on Bazel 4.1.0 rc4; bazel version I built on ARM- and tested this rebuilt `tfjs-node` dependency with danfo.js; it works.====='; 'So when I run the command: npm install --save @tensorflow/tfjs-node I get this giant error ***@***.***: Please upgrade to @mapbox/node-pre-gyp: thenon-scoped node-pre-gyp package is deprecated and only the @mapbox scopedpackage will recieve updates in the futurenpm ERR! code 1npm ERR! path /Users/devintripp/Desktop/Current***@***.***/tfjs-nodenpm ERR! command failednpm ERR! command sh -c node scripts/install.jsnpm ERR! CPU-darwin-3.6.1.tar.gznpm ERR! node-pre-gyp install failed with error: Error: Command failed:node-pre-gyp install --fallback-to-buildnpm ERR! node-pre-gyp info it worked if it ends with oknpm ERR! node-pre-gyp info using ***@***.***npm ERR! node-pre-gyp info using ***@***.*** | darwin | x64npm ERR! node-pre-gyp WARN Using request for node-pre-gyp https downloadnpm ERR! node-pre-gyp info check checked for""/Users/devintripp/Desktop/Current***@***.***/tfjs-node/lib/napi-v8/tfjs_binding.node""(not found)npm ERR! node-pre-gyp http GEThttps://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.6.1/CPU-darwin-3.6.1.tar.gznpm ERR! node-pre-gyp http 404https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.6.1/CPU-darwin-3.6.1.tar.gznpm ERR! node-pre-gyp WARN Tried to download(404):https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.6.1/CPU-darwin-3.6.1.tar.gznpm ERR! node-pre-gyp WARN Pre-built binaries not found for@***@***.*** and ***@***.*** (node-v83 ABI; unknown)(falling back to source compile with node-gyp)npm ERR! node-pre-gyp http 404 status code downloading tarballhttps://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.6.1/CPU-darwin-3.6.1.tar.gznpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using ***@***.***npm ERR! gyp info using ***@***.*** | darwin | x64npm ERR! gyp info oknpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using ***@***.***npm ERR! gyp info using ***@***.*** | darwin | x64npm ERR! gyp info find Python using Python version 3.8.8 found at""/Users/devintripp/miniforge3/envs/tf/bin/python3""npm ERR! gyp info spawn /Users/devintripp/miniforge3/envs/tf/bin/python3npm ERR! gyp info spawn args [npm ERR! gyp info spawn args\'/usr/local/lib/node_modules/npm/node_modules/node-gyp/gyp/gyp_main.py\';npm ERR! gyp info spawn args   \'binding.gyp\';npm ERR! gyp info spawn args   \'-f\';npm ERR! gyp info spawn args   \'make\';npm ERR! gyp info spawn args   \'-I\';npm ERR! gyp info spawn args   \'/Users/devintripp/Desktop/Current***@***.***/tfjs-node/build/config.gypi\';npm ERR! gyp info spawn args   \'-I\';npm ERR! gyp info spawn args\'/usr/local/lib/node_modules/npm/node_modules/node-gyp/addon.gypi\';npm ERR! gyp info spawn args   \'-I\';npm ERR! gyp info spawn args\'/Users/devintripp/Library/Caches/node-gyp/14.17.0/include/node/common.gypi\';npm ERR! gyp info spawn args   \'-Dlibrary=shared_library\';npm ERR! gyp info spawn args   \'-Dvisibility=default\';npm ERR! gyp info spawn args\'-Dnode_root_dir=/Users/devintripp/Library/Caches/node-gyp/14.17.0\';npm ERR! gyp info spawn args\'-Dnode_gyp_dir=/usr/local/lib/node_modules/npm/node_modules/node-gyp\';npm ERR! gyp info spawn args\'-Dnode_lib_file=/Users/devintripp/Library/Caches/node-gyp/14.17.0/<(target_arch)/node.lib\';npm ERR! gyp info spawn args\'-Dmodule_root_dir=/Users/devintripp/Desktop/Current***@***.***/tfjs-node\';npm ERR! gyp info spawn args   \'-Dnode_engine=v8\';npm ERR! gyp info spawn args   \'--depth=.\';npm ERR! gyp info spawn args   \'--no-parallel\';npm ERR! gyp info spawn args   \'--generator-output\';npm ERR! gyp info spawn args   \'build\';npm ERR! gyp info spawn args   \'-Goutput_dir=.\'npm ERR! gyp info spawn args ]npm ERR! gyp info oknpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using ***@***.***npm ERR! gyp info using ***@***.*** | darwin | x64npm ERR! gyp info spawn makenpm ERR! gyp info spawn args [ \'BUILDTYPE=Release\'; \'-C\'; \'build\' ]npm ERR! clang: error: no such file or directory:***@***.***/tfjs-node/deps/include\'npm ERR! make: *** [Release/obj.target/tfjs_binding/binding/tfjs_backend.o]Error 1npm ERR! gyp ERR! build errornpm ERR! gyp ERR! stack Error: `make` failed with exit code: 2npm ERR! gyp ERR! stack     at ChildProcess.onExit(/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:194:23)npm ERR! gyp ERR! stack     at ChildProcess.emit (events.js:376:20)npm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit(internal/child_process.js:277:12)npm ERR! gyp ERR! System Darwin 20.3.0npm ERR! gyp ERR! command ""/usr/local/bin/node""""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js""""build"" ""--fallback-to-build"" ""--module=/Users/devintripp/Desktop/Current***@***.***/tfjs-node/lib/napi-v8/tfjs_binding.node""""--module_name=tfjs_binding""""--module_path=/Users/devintripp/Desktop/Current***@***.***/tfjs-node/lib/napi-v8""""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8""""--node_napi_label=napi-v8""npm ERR! gyp ERR! cwd /Users/devintripp/Desktop/Current***@***.***/tfjs-nodenpm ERR! gyp ERR! node -v v14.17.0npm ERR! gyp ERR! node-gyp -v v7.1.2npm ERR! gyp ERR! not oknpm ERR! node-pre-gyp ERR! build errornpm ERR! node-pre-gyp ERR! stack Error: Failed to execute\'/usr/local/bin/node/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js build--fallback-to-build --module=/Users/devintripp/Desktop/Current***@***.***/tfjs-node/lib/napi-v8/tfjs_binding.node--module_name=tfjs_binding --module_path=/Users/devintripp/Desktop/Current***@***.***/tfjs-node/lib/napi-v8--napi_version=8 --node_abi_napi=napi --napi_build_version=8--node_napi_label=napi-v8\' (1)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.<anonymous>(/Users/devintripp/Desktop/CurrentWork/algo-art/backend/node_modules/node-pre-gyp/lib/util/compile.js:83:29)npm ERR! node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:376:20)npm ERR! node-pre-gyp ERR! stack     at maybeClose(internal/child_process.js:1055:16)npm ERR! node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit(internal/child_process.js:288:5)npm ERR! node-pre-gyp ERR! System Darwin 20.3.0npm ERR! node-pre-gyp ERR! command ""/usr/local/bin/node""""/Users/devintripp/Desktop/CurrentWork/algo-art/backend/node_modules/.bin/node-pre-gyp"" ""install""""--fallback-to-build""npm ERR! node-pre-gyp ERR! cwd /Users/devintripp/Desktop/Current***@***.***/tfjs-nodenpm ERR! node-pre-gyp ERR! node -v v14.17.0npm ERR! node-pre-gyp ERR! node-pre-gyp -v v0.14.0npm ERR! node-pre-gyp ERR! not oknpm ERR! * Downloading libtensorflownpm ERR!npm ERR! * Building TensorFlow Node.js bindingsnpm ERR! A complete log of this run can be found in:npm ERR!     /Users/devintripp/.npm/_logs/2021-05-18T01_28_12_433Z-debug.logI have been able to install it before but it still didn\'t work so Irestarted the installation process and I can\'t even get past this point now.On Mon; May 17; 2021 at 5:17 PM Damien ***@***.***> wrote:> I basically followed what @rodrigolive <https://github.com/rodrigolive>> mentioned above cloning this time the v2.5.0 tag. Tensorflow built based on> Bazel 4.1.0 rc4; bazel version I built on ARM- and tested this rebuilt> tfjs-node dependency with danfo.js; it works.>> —> You are receiving this because you commented.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4514#issuecomment-842678252>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ANDJPC6M6GEJAZZ32OC3MXDTOGIZBANCNFSM4V3HOZSQ>> .>I was able to build using bazel and I have the .tar file and I was able to put it in before; but I no longer have the tfjs-node directory and when I run the npm install command it doesn\'t create it anymore. How do I get the tfjs-node directory back? and will I probably need to rebuild the .tar file with bazel again I thought I switched to the 2.5 branch but perhaps I didn\'t.====='; 'I keep getting the following error when running a node.js file (node test.js) that imports the tfjs-node on M1. Tried what you suggested above.```dyld: lazy symbol binding failed: Symbol not found: _TF_NewStatusExpected in: flat namespace```I tried to do the install with the 2.5 branch; and it compiles and the steps that @rodrigolive suggest run correctly; the only thing I changed is to pull the 2.5 TF branch; and changed the Bazel version to 4.1.0 like following:```USE_BAZEL_VERSION=4.1.0 bazel build --jobs 4 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow.dylib--action_env PYTHON_BIN_PATH=/usr/bin/python  USE_BAZEL_VERSION=4.1.0 bazel build --jobs 6 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow_framework.dylib --action_env PYTHON_BIN_PATH=/usr/bin/python```The `npx node-pre-gyp rebuild` builds correctly aswell.====='; 'I\'ve just reran my own steps again; but now with GA version `a4dfb8d1 v2.5.0` pulled from the Tensorflow Git repository. Everything working as expected. Both a simple `node -e ""tf = require(\'@tensorflow/tfjs-node\')""` and my CNN model training session with 12 epochs and 888286 parameters worked.@kzka90 make sure you are pulling the correct version; `git checkout v2.5.0` which is a **tag**; not a branch!https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0My personal versions:MacOS=`Big Sur 11.2.3`Bazel=`3.7.2` (through bazelisk)Clang=`Apple clang version 12.0.0 (clang-1200.0.32.29)`  (clang --version)MacOS SDK include dir=`MacOSX.sdk/usr/include/c++/4.2.1` (gcc --version)Node=`16.2.0`NPM=`7.13.0`tfjs=`3.6.0` (from node_modules/@tensorflow/tfjs/package.json)tfjs-node=`3.6.1` (from node_modules/@tensorflow/tfjs-node)node-pre-gyp=`0.17.0` (from node_modules/node-pre-gyp)====='; 'Just reran and I keep getting the same error.Here is exactly what I did:1. Downloaded https://github.com/tensorflow/tensorflow/releases/tag/v2.5.02. Put it inside `$BUILDDIR/tensorflow`3. Ran the following commands```cd $BUILDDIR/tensorflowarch -x86_64 bashbrew install bazeliskUSE_BAZEL_VERSION=4.1.0 bazel build --jobs 6 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow.dylib --action_env PYTHON_BIN_PATH=/usr/bin/pythonUSE_BAZEL_VERSION=4.1.0 bazel build --jobs 6 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow_framework.dylib --action_env PYTHON_BIN_PATH=/usr/bin/python```Needed to add `PYTHON_BIN_PATH=/usr/bin/python` because it wasn\'t finding my Python. And I\'m using the 4.1.0 of Bazel.This compiling throws the following successful messages:First one:```INFO: Found 1 target...Target //tensorflow:libtensorflow.dylib up-to-date:  bazel-bin/tensorflow/libtensorflow.dylibINFO: Elapsed time: 3965.162s; Critical Path: 240.83sINFO: 9095 processes: 67 internal; 9028 local.INFO: Build completed successfully; 9095 total actions```Second one:```INFO: Found 1 target...Target //tensorflow:libtensorflow_framework.dylib up-to-date:  bazel-bin/tensorflow/libtensorflow_framework.dylibINFO: Elapsed time: 3.620s; Critical Path: 0.23sINFO: 2 processes: 1 internal; 1 local.INFO: Build completed successfully; 2 total actions```So I can tell Bazel does it work correctly.4. Existed the `arch -x86_64 bash` with `exit `5. Then ran the following```cd $BUILDDIRmkdir tfjs-test && cd tfjs-testnvm use 16.2.0npm init -ynpm i node-pre-gypnpm i @tensorflow/tfjs-node@latest --ignore-scriptscd node_modules/@tensorflow/tfjs-node/mkdir -p deps/lib deps/include/tensorflowcp -r $BUILDDIR/tensorflow/tensorflow/c deps/include/tensorflow/cp -r $BUILDDIR/tensorflow/tensorflow/core deps/include/tensorflow/cp -r $BUILDDIR/tensorflow/bazel-bin/tensorflow/*.dylib deps/lib/npx node-pre-gyp rebuild```This throws the following message:```gyp info spawn args   \'-Dnode_engine=v8\';gyp info spawn args   \'--depth=.\';gyp info spawn args   \'--no-parallel\';gyp info spawn args   \'--generator-output\';gyp info spawn args   \'build\';gyp info spawn args   \'-Goutput_dir=.\'gyp info spawn args ]gyp info ok node-pre-gyp info This Node instance does not support builds for N-API version 8gyp info it worked if it ends with okgyp info using node-gyp@7.1.2gyp info using node@15.8.0 | darwin | x64gyp info spawn makegyp info spawn args [ \'BUILDTYPE=Release\'; \'-C\'; \'build\' ]  CXX(target) Release/obj.target/tfjs_binding/binding/tfjs_backend.o  CXX(target) Release/obj.target/tfjs_binding/binding/tfjs_binding.o  SOLINK_MODULE(target) Release/tfjs_binding.nodeld: warning: ignoring file /Users/user/tf-build/tfjs-test/node_modules/@tensorflow/tfjs-node/deps/lib/libtensorflow_framework.2.dylib; building for macOS-x86_64 but attempting to link with file built for macOS-arm64ld: warning: ignoring file /Users/user/tf-build/tfjs-test/node_modules/@tensorflow/tfjs-node/deps/lib/libtensorflow.2.dylib; building for macOS-x86_64 but attempting to link with file built for macOS-arm64POSTBUILD(tfjs_binding) Adjust libtensorflow load pathPOSTBUILD(tfjs_binding) Adjust libtensorflow_framework load path  COPY /Users/user/tf-build/tfjs-test/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node  TOUCH Release/obj.target/action_after_build.stampgyp info ok node-pre-gyp info ok ```But when I run a test; e.g. `node -e ""tf = require(\'@tensorflow/tfjs-node\')""` still getting:```dyld: lazy symbol binding failed: Symbol not found: _TF_NewStatus  Referenced from: /Users/user/tf-build/tfjs-test/node_modules/@tensorflow/tfjs-node/lib/napi-v8/tfjs_binding.node  Expected in: flat namespacedyld: Symbol not found: _TF_NewStatus  Referenced from: /Users/user/tf-build/tfjs-test/node_modules/@tensorflow/tfjs-node/lib/napi-v8/tfjs_binding.node  Expected in: flat namespacezsh: abort      node -e ""tf = require(\'@tensorflow/tfjs-node\')""```My versions:MacOS=`Big Sur 11.0`Bazel=`4.1.0`Clang=`12.0.5`MacOS SDK incldue dir=`MacOSX.sdk/usr/include/c++/4.2.1`Node=`16.2.0`NPM=`7.13.0`tfjs-node=`3.6.1`node-pre-gyp=`0.17.0`=====']",Build & Install Failure,Build & Initialization Failure,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,D.1
https://github.com/tensorflow/tfjs/issues/4508,Support Reciprocal in wasm backend,1,open,2021-01-08T14:54:32Z,2021-06-03T16:16:31Z,**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link):  2.8.3- TensorFlow.js version (use command below): 2.8.3- Browser version:  Chrome 87- Tensorflow.js Converter Version: 2.7**Describe the current behavior**When i run my efficientdet model in the WASM backend i have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. Possibly related to https://github.com/tensorflow/tfjs/issues/4480i initialize model as follows```const init = async () => {  await tf.setBackend('wasm');  await tf.ready();  prepareEfficentDetModel();};init();```,"[""@rafallukasik123 thanks for adding the extra info about the backend you are using. The WASM backend does not yet implement Reciprocal. You could try this with the webgl backend and see if it works.I'm going to update this bug to be a feature request for adding reciprocal to the wasm backend.=====""]",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4500,Implement step op in WASM backend (needed for training ReLU networks),1,closed,2021-01-07T16:09:56Z,2021-01-08T02:18:50Z,**System information**- TensorFlow.js version (you are using): 2.8.2- Are you willing to contribute it (Yes/No): YES**Describe the feature and the current behavior/state.** Hi;I am currently using `tfjs-backend-wasm` to implement some RL algorithms. For that I am trying to train a perceptron with ReLU activations in a web worker. Unfortunately not all ops required for this are implemented. I understand that your priorities are on inference.However; it seems that the only thing missing to make ReLU training work; is the gradient function of the ReLU activation (training works fine with sigmoid whose gradient only needs sub/mul ops). More specifically; the gradient fails due to the absence of the `step` op.Are you guys planning on implementing the `step` op in the WebAssembly backend soon? **Will this change the current api? How?**No. New op.**Who will benefit with this feature?**Anyone who trains using ReLU activation and WebAssembly backend.**Any Other info.**If you are not planning on implementing this; I would love to help! Let me know.,['cc @annxingyuan ====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4484,Update tfjs-models/body-pix dependencies to @tensorflow/tfjs 2.x,5,closed,2021-01-05T08:38:44Z,2021-02-26T00:54:01Z,**System information**- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 1.5.2- Tensorflow.js Converter Version: 1.5.2**Describe the current behavior**The latest release of @tensorflow-models/body-pix depends on @tensorflow/tfjs-core 1.x; ref: https://github.com/tensorflow/tfjs-models/blob/c08dae1739f8cc42502c1770ce4f314052ece2e8/body-pix/package.json#L17@tensorflow/tfjs-core before 2.5.0 depends on a vulnerable version of node-fetch. This vulnerability was fixed via https://github.com/tensorflow/tfjs/issues/3931.**Describe the expected behavior**There should be a version of @tensorflow-models/body-pix that does not depend on vulnerable node-fetch version.**Standalone code to reproduce the issue**Run:```npm install @tensorflow-models/body-pixnpm audit```There should be no vulnerability listed.,"['@tafsiri can we update tfjs-core version to latest ?====='; ""Yeah no reason why we shouldn't be able to. I think we have PRs for blazeface and face-landmarks-detection. =====""; '@tafsiri submitted a PR ; please review ====='; 'Package.json is updated with latest version ; this should be fixed now.Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4484"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4484"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Operator,API,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4483,tfjs-react-native: Any way to make models load faster? (outside of Metro),3,closed,2021-01-05T04:39:36Z,2021-08-12T13:29:37Z,I load three graph models when my app boots. On my new Galaxy S20+; this takes ~3 seconds with Metro (served over HTTP right?) and ~9 seconds with a release bundle (loaded with react-native-fs I believe). These numbers are fine (although I'd be interested to know why it takes 3x as long when the file is already on-device _edit: I realize now this is because over HTTP we use fetch as binary; whereas with fs we cycle between bytes/base64 multiple times_ ); but now that my app is getting ready for release; I'm seeing load times of 30 seconds or more on some older devices. This is really pushing the limit of what I'm comfortable with.Is there any recourse here? Each of my models is a tfjs model with 6 or 7 weights shards; 4mb each. ~75mb total on file.,"[""I think I know what's going on here.I found this in react-native-fs:```InputStream inputStream = getInputStream(filepath);byte[] inputData = getInputStreamBytes(inputStream);String base64Content = Base64.encodeToString(inputData; Base64.NO_WRAP);promise.resolve(base64Content);```And this in BundleResourceHandler:```base64Weights = await RNFS.readFileRes(fileName)...const weightData = util.encodeString(base64Weights; 'base64').buffer```So the flow is:RNFS reads bytes > RNFS converts to base64 > TFJS receives base64 > TFJS converts to bytesIt looks like this is a package out there that can fix this cycle:https://www.npmjs.com/package/rn-fetch-blobSpecifically the `readStream` method: https://github.com/joltup/rn-fetch-blob/wiki/File-System-Access-API#readstreampath-encoding-buffersize-interval-promisernfbreadstream=====""; ""@tafsiri Check this out! I think I'm on to something here. I know this code is pretty raw (mixing Expo FS and RNFS...) but it actually works! And I can use this exact same code to load buffers in Metro or in release.I was able to reduce my shards load time from 9-12 seconds to 3-3.5 seconds!Flow:1. Get asset uris from expo-asset2. Copy files from cache into document directory3. use react-native-static-server to serve doc directory over HTTP4. use axios to grab the .bin files from the serverI also think it's definitely possible to skip the cache>docDir copies; but it's late and I will try tomorrow...```async function loadAllShards() {  const t1 = Date.now()  const [family; genus; species] = await Asset.loadAsync([    require('../../assets/family_shards.bin');    require('../../assets/genus_shards.bin');    require('../../assets/species_shards.bin')  ])  console.log('loaded assets in'; Date.now() - t1; 'ms')  if (!family.localUri || !genus.localUri || !species.localUri) {    throw new Error('shards file is missing a localUri')  }  await FileSystem.copyAsync({    from: family.localUri;    to: FileSystem.documentDirectory + 'family_shards.bin'  })  await FileSystem.copyAsync({    from: genus.localUri;    to: FileSystem.documentDirectory + 'genus_shards.bin'  })  await FileSystem.copyAsync({    from: species.localUri;    to: FileSystem.documentDirectory + 'species_shards.bin'  })  console.log('starting server'; RNFS.DocumentDirectoryPath; Date.now() - t1)  const server = new StaticServer(0; RNFS.DocumentDirectoryPath)  const baseURL = await server.start()  console.log('serving at'; baseURL; Date.now() - t1)  const axios = Axios.create({    baseURL  })  const buffers = await Promise.all(    ['family_shards.bin'; 'genus_shards.bin'; 'species_shards.bin'].map(      async file => {        const response = await axios.get<ArrayBuffer>(file; {          responseType: 'arraybuffer'        })        return response.data      }    )  )  console.log('got all buffers in'; Date.now() - t1; 'ms')}```=====""; 'Ran into the same issue when loading a single model with four weight shards; totalling to ~13MB. The tf.loadGraphModel method resolves reasonably quickly on iOS (iPhone7: 3s when developing locally; 9s when installed from testflight) but the load times on Android devices running built release apk are **much slower** leaving the app UI completely unresponsive in the meantime 😕 loadGraphModel taking: 24s on Redmi 7; 60s on nexus 5x; 78s on samsung a10 😖 @zholmes1  Could you elaborate on how you ended up using the _buffers_ array? I don’t see them used in any wayin the posted code and wanted to attempt try your version of the code. Running it I’m also running into an issue where axios returns undefined for response.data if ""arraybuffer"" responseType is specified.=====']",Slow Execution,Poor Performance,Device Incompatibility,Device,Device,change device,Changing device/browser,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,D.1
https://github.com/tensorflow/tfjs/issues/4481,Use tfjs 2.8.2 executeAsync show 'length' undefined error,4,closed,2021-01-04T12:35:17Z,2021-01-06T00:31:41Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.8.2- Browser version: Google Chrome. Version 87.0.4280.88**Describe the current behavior**I use a train model by myself. When I use Tensorflow.js 2.7.0; it was ok. But; when I use the latest Version Tensorflow.js 2.8.2(script link: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs); use code **model.executeAsync**; it show an error: **Uncaught (in promise) TypeError: Cannot read property 'length' of undefined****Describe the expected behavior**I just want to use as Tensorflow.js 2.7.0 as well**Standalone code to reproduce the issue**Here is the code:```const MODEL_URL = ""./niu_web_model/model.json"";const model = await tf.loadGraphModel(MODEL_URL);const cat = document.getElementById(""cat"");model   .executeAsync(tf.browser.fromPixels(cat).expandDims(0))   // error when execute to here   .then((modelres) => {                             console.log(modelres[6].dataSync();modelres[7].dataSync())    });```error show:```Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at Vz (tfjs:17)    at Object.kernelFunc (tfjs:17)    at p (tfjs:17)    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.runKernelFunc (tfjs:17)    at e.t.runKernel (tfjs:17)    at topk_ (tfjs:17)    at topk__op (tfjs:17)    at tfjs:17```It 's ok when use in Tensorflow.js 2.7.0.","[""I got a similar error; but when running tf.topk() on my prediction results instead.What seemed to fix it was running .print() on the inference output tensor.This works:```const prediction = model.predict(x);prediction.print();const topPreds = tf.topk(prediction; 3; true);````But this throws an error```const prediction = model.predict(x);const topPreds = tf.topk(prediction; 3; true);Unhandled Promise rejection: Cannot read property 'length' of undefined ; Zone: <root> ; Task: Promise.then ; Value: TypeError: Cannot read property 'length' of undefined    at topKImpl (TopK_impl.js:22)    at Object.topK [as kernelFunc] (TopK.js:24)    at kernelFunc (engine.js:440)    at engine.js:504    at Engine.scopedRun (engine.js:333)    at Engine.runKernelFunc (engine.js:502)    at Engine.runKernel (engine.js:391)    at topk_ (topk.js:57)    at Module.topk__op (operation.js:44)```No clue why the error is being thrown without the print though.=====""; '@zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?====='; '> @zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?See here:https://codepen.io/miksvasiljevs/pen/VwKxpGKLet me know if there are any issues with the pen or anything.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4481,Use tfjs 2.8.2 executeAsync show 'length' undefined error,4,closed,2021-01-04T12:35:17Z,2021-01-06T00:31:41Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.8.2- Browser version: Google Chrome. Version 87.0.4280.88**Describe the current behavior**I use a train model by myself. When I use Tensorflow.js 2.7.0; it was ok. But; when I use the latest Version Tensorflow.js 2.8.2(script link: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs); use code **model.executeAsync**; it show an error: **Uncaught (in promise) TypeError: Cannot read property 'length' of undefined****Describe the expected behavior**I just want to use as Tensorflow.js 2.7.0 as well**Standalone code to reproduce the issue**Here is the code:```const MODEL_URL = ""./niu_web_model/model.json"";const model = await tf.loadGraphModel(MODEL_URL);const cat = document.getElementById(""cat"");model   .executeAsync(tf.browser.fromPixels(cat).expandDims(0))   // error when execute to here   .then((modelres) => {                             console.log(modelres[6].dataSync();modelres[7].dataSync())    });```error show:```Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at Vz (tfjs:17)    at Object.kernelFunc (tfjs:17)    at p (tfjs:17)    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.runKernelFunc (tfjs:17)    at e.t.runKernel (tfjs:17)    at topk_ (tfjs:17)    at topk__op (tfjs:17)    at tfjs:17```It 's ok when use in Tensorflow.js 2.7.0.","[""I got a similar error; but when running tf.topk() on my prediction results instead.What seemed to fix it was running .print() on the inference output tensor.This works:```const prediction = model.predict(x);prediction.print();const topPreds = tf.topk(prediction; 3; true);````But this throws an error```const prediction = model.predict(x);const topPreds = tf.topk(prediction; 3; true);Unhandled Promise rejection: Cannot read property 'length' of undefined ; Zone: <root> ; Task: Promise.then ; Value: TypeError: Cannot read property 'length' of undefined    at topKImpl (TopK_impl.js:22)    at Object.topK [as kernelFunc] (TopK.js:24)    at kernelFunc (engine.js:440)    at engine.js:504    at Engine.scopedRun (engine.js:333)    at Engine.runKernelFunc (engine.js:502)    at Engine.runKernel (engine.js:391)    at topk_ (topk.js:57)    at Module.topk__op (operation.js:44)```No clue why the error is being thrown without the print though.=====""; '@zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?====='; '> @zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?See here:https://codepen.io/miksvasiljevs/pen/VwKxpGKLet me know if there are any issues with the pen or anything.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4481,Use tfjs 2.8.2 executeAsync show 'length' undefined error,4,closed,2021-01-04T12:35:17Z,2021-01-06T00:31:41Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.8.2- Browser version: Google Chrome. Version 87.0.4280.88**Describe the current behavior**I use a train model by myself. When I use Tensorflow.js 2.7.0; it was ok. But; when I use the latest Version Tensorflow.js 2.8.2(script link: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs); use code **model.executeAsync**; it show an error: **Uncaught (in promise) TypeError: Cannot read property 'length' of undefined****Describe the expected behavior**I just want to use as Tensorflow.js 2.7.0 as well**Standalone code to reproduce the issue**Here is the code:```const MODEL_URL = ""./niu_web_model/model.json"";const model = await tf.loadGraphModel(MODEL_URL);const cat = document.getElementById(""cat"");model   .executeAsync(tf.browser.fromPixels(cat).expandDims(0))   // error when execute to here   .then((modelres) => {                             console.log(modelres[6].dataSync();modelres[7].dataSync())    });```error show:```Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at Vz (tfjs:17)    at Object.kernelFunc (tfjs:17)    at p (tfjs:17)    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.runKernelFunc (tfjs:17)    at e.t.runKernel (tfjs:17)    at topk_ (tfjs:17)    at topk__op (tfjs:17)    at tfjs:17```It 's ok when use in Tensorflow.js 2.7.0.","[""I got a similar error; but when running tf.topk() on my prediction results instead.What seemed to fix it was running .print() on the inference output tensor.This works:```const prediction = model.predict(x);prediction.print();const topPreds = tf.topk(prediction; 3; true);````But this throws an error```const prediction = model.predict(x);const topPreds = tf.topk(prediction; 3; true);Unhandled Promise rejection: Cannot read property 'length' of undefined ; Zone: <root> ; Task: Promise.then ; Value: TypeError: Cannot read property 'length' of undefined    at topKImpl (TopK_impl.js:22)    at Object.topK [as kernelFunc] (TopK.js:24)    at kernelFunc (engine.js:440)    at engine.js:504    at Engine.scopedRun (engine.js:333)    at Engine.runKernelFunc (engine.js:502)    at Engine.runKernel (engine.js:391)    at topk_ (topk.js:57)    at Module.topk__op (operation.js:44)```No clue why the error is being thrown without the print though.=====""; '@zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?====='; '> @zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?See here:https://codepen.io/miksvasiljevs/pen/VwKxpGKLet me know if there are any issues with the pen or anything.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">No</a>=====']",Regression,Poor Performance,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4481,Use tfjs 2.8.2 executeAsync show 'length' undefined error,4,closed,2021-01-04T12:35:17Z,2021-01-06T00:31:41Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 2.8.2- Browser version: Google Chrome. Version 87.0.4280.88**Describe the current behavior**I use a train model by myself. When I use Tensorflow.js 2.7.0; it was ok. But; when I use the latest Version Tensorflow.js 2.8.2(script link: https://cdn.jsdelivr.net/npm/@tensorflow/tfjs); use code **model.executeAsync**; it show an error: **Uncaught (in promise) TypeError: Cannot read property 'length' of undefined****Describe the expected behavior**I just want to use as Tensorflow.js 2.7.0 as well**Standalone code to reproduce the issue**Here is the code:```const MODEL_URL = ""./niu_web_model/model.json"";const model = await tf.loadGraphModel(MODEL_URL);const cat = document.getElementById(""cat"");model   .executeAsync(tf.browser.fromPixels(cat).expandDims(0))   // error when execute to here   .then((modelres) => {                             console.log(modelres[6].dataSync();modelres[7].dataSync())    });```error show:```Uncaught (in promise) TypeError: Cannot read property 'length' of undefined    at Vz (tfjs:17)    at Object.kernelFunc (tfjs:17)    at p (tfjs:17)    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.runKernelFunc (tfjs:17)    at e.t.runKernel (tfjs:17)    at topk_ (tfjs:17)    at topk__op (tfjs:17)    at tfjs:17```It 's ok when use in Tensorflow.js 2.7.0.","[""I got a similar error; but when running tf.topk() on my prediction results instead.What seemed to fix it was running .print() on the inference output tensor.This works:```const prediction = model.predict(x);prediction.print();const topPreds = tf.topk(prediction; 3; true);````But this throws an error```const prediction = model.predict(x);const topPreds = tf.topk(prediction; 3; true);Unhandled Promise rejection: Cannot read property 'length' of undefined ; Zone: <root> ; Task: Promise.then ; Value: TypeError: Cannot read property 'length' of undefined    at topKImpl (TopK_impl.js:22)    at Object.topK [as kernelFunc] (TopK.js:24)    at kernelFunc (engine.js:440)    at engine.js:504    at Engine.scopedRun (engine.js:333)    at Engine.runKernelFunc (engine.js:502)    at Engine.runKernel (engine.js:391)    at topk_ (topk.js:57)    at Module.topk__op (operation.js:44)```No clue why the error is being thrown without the print though.=====""; '@zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?====='; '> @zhuyingyan @MiksVasiljevs can either of you provide a codepen example for this failure?See here:https://codepen.io/miksvasiljevs/pen/VwKxpGKLet me know if there are any issues with the pen or anything.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4481"">No</a>=====']",Regression,Poor Performance,Incorrect Code Logic,WebGL,Backend,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4480,Error running LinSpace,10,closed,2021-01-04T08:14:59Z,2021-04-26T00:34:09Z,"When I run **[tf.linspace](https://js.tensorflow.org/api/2.8.2/#linspace)** in the 2.8.2  API documentation.I got this ""An error occured on line: 1 Error running LinSpace: Neither modular kernel nor forward func passed"".![image](https://user-images.githubusercontent.com/422362/103514521-23658b00-4eb0-11eb-9284-8197ab47d166.png)The 2.7.0 vsersion of tf.linspace works but 2.8.0; 2.8.1; 2.8.2  are not working for now.**System information**- TensorFlow.js version: 2.8.0; 2.8.1; 2.8.2- OS Platform and Distribution (MacOS 10.15; Windows 10):","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; 'Re-opening as this will need a new release to publish the fix.For context this will only happen if linspace is the first op called. As a workaround you call `tf.ready()` or `await tf.ready()` before calling any ops.====='; '@tafsiri  your workground doesn\'t work for me. When i call tf.ready() before calling any ops i have error in one of kernel func. More specifically in fromPixels.js and the error content is ""backend.makeTensorInfo is not a function""====='; '@rafallukasik123 could you post a reprodcution (maybe in codepen form) so that we can take a look.====='; 'Unfortunately I can\'t share all the code. Here is a small snipe of code https://codepen.io/rafallukasik123/pen/LYRBpVj When i remove tf.ready() from the beginning i have error similar to you ""Error running Round: Neither modular kernel nor forward func passed"".If my code contain tf.ready() i have mentioned error ""backend.makeTensorInfo is not a function"" in @tensorflow/tfjs-backend-webgl/dist/kernels/FromPixels.js====='; 'in the latest version 2.8.3 I have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. should I create a new issue?====='; ""@rafallukasik123 Yes I think a new issue would be best. In terms of reproduction; you don't have to post your whole code; ideally it would be the smallest snippet of code that shows the error. It can even just load the library and directly call tf.reciprocal or tf.browser.fromPixels. In the codepen you posted I get a syntax error so the code doesn't run.=====""; 'Fixed in 2.8.3 https://js.tensorflow.org/api/2.8.3/#linspace====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; ""In the latest version (3.5.0) I've seen:```Error: Kernel 'LinSpace' not registered for backend 'wasm'```Is this a related issue?=====""]",Reference Error,Crash,Incorrect Code Logic,Operator,API,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4480,Error running LinSpace,10,closed,2021-01-04T08:14:59Z,2021-04-26T00:34:09Z,"When I run **[tf.linspace](https://js.tensorflow.org/api/2.8.2/#linspace)** in the 2.8.2  API documentation.I got this ""An error occured on line: 1 Error running LinSpace: Neither modular kernel nor forward func passed"".![image](https://user-images.githubusercontent.com/422362/103514521-23658b00-4eb0-11eb-9284-8197ab47d166.png)The 2.7.0 vsersion of tf.linspace works but 2.8.0; 2.8.1; 2.8.2  are not working for now.**System information**- TensorFlow.js version: 2.8.0; 2.8.1; 2.8.2- OS Platform and Distribution (MacOS 10.15; Windows 10):","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; 'Re-opening as this will need a new release to publish the fix.For context this will only happen if linspace is the first op called. As a workaround you call `tf.ready()` or `await tf.ready()` before calling any ops.====='; '@tafsiri  your workground doesn\'t work for me. When i call tf.ready() before calling any ops i have error in one of kernel func. More specifically in fromPixels.js and the error content is ""backend.makeTensorInfo is not a function""====='; '@rafallukasik123 could you post a reprodcution (maybe in codepen form) so that we can take a look.====='; 'Unfortunately I can\'t share all the code. Here is a small snipe of code https://codepen.io/rafallukasik123/pen/LYRBpVj When i remove tf.ready() from the beginning i have error similar to you ""Error running Round: Neither modular kernel nor forward func passed"".If my code contain tf.ready() i have mentioned error ""backend.makeTensorInfo is not a function"" in @tensorflow/tfjs-backend-webgl/dist/kernels/FromPixels.js====='; 'in the latest version 2.8.3 I have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. should I create a new issue?====='; ""@rafallukasik123 Yes I think a new issue would be best. In terms of reproduction; you don't have to post your whole code; ideally it would be the smallest snippet of code that shows the error. It can even just load the library and directly call tf.reciprocal or tf.browser.fromPixels. In the codepen you posted I get a syntax error so the code doesn't run.=====""; 'Fixed in 2.8.3 https://js.tensorflow.org/api/2.8.3/#linspace====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; ""In the latest version (3.5.0) I've seen:```Error: Kernel 'LinSpace' not registered for backend 'wasm'```Is this a related issue?=====""]",Reference Error,Crash,Incorrect Code Logic,Operator,API,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4480,Error running LinSpace,10,closed,2021-01-04T08:14:59Z,2021-04-26T00:34:09Z,"When I run **[tf.linspace](https://js.tensorflow.org/api/2.8.2/#linspace)** in the 2.8.2  API documentation.I got this ""An error occured on line: 1 Error running LinSpace: Neither modular kernel nor forward func passed"".![image](https://user-images.githubusercontent.com/422362/103514521-23658b00-4eb0-11eb-9284-8197ab47d166.png)The 2.7.0 vsersion of tf.linspace works but 2.8.0; 2.8.1; 2.8.2  are not working for now.**System information**- TensorFlow.js version: 2.8.0; 2.8.1; 2.8.2- OS Platform and Distribution (MacOS 10.15; Windows 10):","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; 'Re-opening as this will need a new release to publish the fix.For context this will only happen if linspace is the first op called. As a workaround you call `tf.ready()` or `await tf.ready()` before calling any ops.====='; '@tafsiri  your workground doesn\'t work for me. When i call tf.ready() before calling any ops i have error in one of kernel func. More specifically in fromPixels.js and the error content is ""backend.makeTensorInfo is not a function""====='; '@rafallukasik123 could you post a reprodcution (maybe in codepen form) so that we can take a look.====='; 'Unfortunately I can\'t share all the code. Here is a small snipe of code https://codepen.io/rafallukasik123/pen/LYRBpVj When i remove tf.ready() from the beginning i have error similar to you ""Error running Round: Neither modular kernel nor forward func passed"".If my code contain tf.ready() i have mentioned error ""backend.makeTensorInfo is not a function"" in @tensorflow/tfjs-backend-webgl/dist/kernels/FromPixels.js====='; 'in the latest version 2.8.3 I have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. should I create a new issue?====='; ""@rafallukasik123 Yes I think a new issue would be best. In terms of reproduction; you don't have to post your whole code; ideally it would be the smallest snippet of code that shows the error. It can even just load the library and directly call tf.reciprocal or tf.browser.fromPixels. In the codepen you posted I get a syntax error so the code doesn't run.=====""; 'Fixed in 2.8.3 https://js.tensorflow.org/api/2.8.3/#linspace====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; ""In the latest version (3.5.0) I've seen:```Error: Kernel 'LinSpace' not registered for backend 'wasm'```Is this a related issue?=====""]",Regression,Poor Performance,Incorrect Code Logic,Operator,API,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Kernel Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4480,Error running LinSpace,10,closed,2021-01-04T08:14:59Z,2021-04-26T00:34:09Z,"When I run **[tf.linspace](https://js.tensorflow.org/api/2.8.2/#linspace)** in the 2.8.2  API documentation.I got this ""An error occured on line: 1 Error running LinSpace: Neither modular kernel nor forward func passed"".![image](https://user-images.githubusercontent.com/422362/103514521-23658b00-4eb0-11eb-9284-8197ab47d166.png)The 2.7.0 vsersion of tf.linspace works but 2.8.0; 2.8.1; 2.8.2  are not working for now.**System information**- TensorFlow.js version: 2.8.0; 2.8.1; 2.8.2- OS Platform and Distribution (MacOS 10.15; Windows 10):","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; 'Re-opening as this will need a new release to publish the fix.For context this will only happen if linspace is the first op called. As a workaround you call `tf.ready()` or `await tf.ready()` before calling any ops.====='; '@tafsiri  your workground doesn\'t work for me. When i call tf.ready() before calling any ops i have error in one of kernel func. More specifically in fromPixels.js and the error content is ""backend.makeTensorInfo is not a function""====='; '@rafallukasik123 could you post a reprodcution (maybe in codepen form) so that we can take a look.====='; 'Unfortunately I can\'t share all the code. Here is a small snipe of code https://codepen.io/rafallukasik123/pen/LYRBpVj When i remove tf.ready() from the beginning i have error similar to you ""Error running Round: Neither modular kernel nor forward func passed"".If my code contain tf.ready() i have mentioned error ""backend.makeTensorInfo is not a function"" in @tensorflow/tfjs-backend-webgl/dist/kernels/FromPixels.js====='; 'in the latest version 2.8.3 I have error `Error running Reciprocal: Neither modular kernel nor forward func passed`. should I create a new issue?====='; ""@rafallukasik123 Yes I think a new issue would be best. In terms of reproduction; you don't have to post your whole code; ideally it would be the smallest snippet of code that shows the error. It can even just load the library and directly call tf.reciprocal or tf.browser.fromPixels. In the codepen you posted I get a syntax error so the code doesn't run.=====""; 'Fixed in 2.8.3 https://js.tensorflow.org/api/2.8.3/#linspace====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4480"">No</a>====='; ""In the latest version (3.5.0) I've seen:```Error: Kernel 'LinSpace' not registered for backend 'wasm'```Is this a related issue?=====""]",Regression,Poor Performance,Incorrect Code Logic,Operator,API,change code order,Adjust API invocation sequence,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Kernel Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4473,wasm error when simd enable,4,closed,2020-12-31T03:30:18Z,2021-08-02T04:28:07Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code : yes - OS Platform and Distribution : windows 10- TensorFlow.js installed from : npm - TensorFlow.js version : tfjs-backend-wasm 2.8.1- Browser version: Chrome  87.0.4280.88- Tensorflow.js Converter Version:**Describe the current behavior**enable `WebAssembly SIMD support.` and `WebAssembly threads support` in `Chrome://flags`; and ` tf.setBackend(""wasm"")` ;it will get `tfjs-backend-wasm-threaded-simd.wasm`;then throw errori found that the default of `WebAssembly threads support` is enable;so it throw error when enable `WebAssembly SIMD support.`it works when i disable `WebAssembly threads support` and enable `WebAssembly SIMD support.`;it will get `tfjs-backend-wasm-simd.wasm`![image](https://user-images.githubusercontent.com/25682169/103392773-65ae6580-4b5a-11eb-9a26-a655b2f51346.png)**Describe the expected behavior**it works**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ 901527c4-dc2e-4033-b14b-51186ce336e1:150b93f8c-628e-42d5-9177-5f795c2d1243:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ 50b93f8c-628e-42d5-9177-5f795c2d1243:1901527c4-dc2e-4033-b14b-51186ce336e1:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (36a83941-1c7e-4692-bf4f-3068289f6167:1)    at onmessage (901527c4-dc2e-4033-b14b-51186ce336e1:1)threadPrintErr @ 901527c4-dc2e-4033-b14b-51186ce336e1:1dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1eaaa06c0-b063-44fb-857c-4a0e09673796:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ eaaa06c0-b063-44fb-857c-4a0e09673796:150b93f8c-628e-42d5-9177-5f795c2d1243:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (3ca13380-4414-4f88-bee2-e405e9632241:1)    at onmessage (50b93f8c-628e-42d5-9177-5f795c2d1243:1)threadPrintErr @ 50b93f8c-628e-42d5-9177-5f795c2d1243:1dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (acc831de-3880-49c3-900e-ae7d04186613:1)    at onmessage (dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1)threadPrintErr @ dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1eaaa06c0-b063-44fb-857c-4a0e09673796:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (1d084c92-9a0e-4bd2-8b1b-7755455debd7:1)    at onmessage (eaaa06c0-b063-44fb-857c-4a0e09673796:1)threadPrintErr @ eaaa06c0-b063-44fb-857c-4a0e09673796:12e24e056-724c-4680-ad63-30f01caa3cd7:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ 2e24e056-724c-4680-ad63-30f01caa3cd7:12e24e056-724c-4680-ad63-30f01caa3cd7:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (a3db88cd-bec1-4a62-aa33-84d6917c3585:1)    at onmessage (2e24e056-724c-4680-ad63-30f01caa3cd7:1)threadPrintErr @ 2e24e056-724c-4680-ad63-30f01caa3cd7:1chunk-vendors.80ce6414e29e3dbe5783.js:2 pthread sent an error! blob:http://seed.dev.mindflow.com.cn/901527c4-dc2e-4033-b14b-51186ce336e1:1: Uncaught ReferenceError: A is not definedg.onerror @ chunk-vendors.80ce6414e29e3dbe5783.js:2chunk-vendors.80ce6414e29e3dbe5783.js:2 pthread sent an error! blob:http://seed.dev.mindflow.com.cn/50b93f8c-628e-42d5-9177-5f795c2d1243:1: Uncaught ReferenceError: A is not definedg.onerror @ chunk-vendors.80ce6414e29e3dbe5783.js:2chunk-vendors.80ce6414e29e3dbe5783.js:2 pthread sent an error! blob:http://seed.dev.mindflow.com.cn/dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1: Uncaught ReferenceError: A is not definedg.onerror @ chunk-vendors.80ce6414e29e3dbe5783.js:2chunk-vendors.80ce6414e29e3dbe5783.js:2 pthread sent an error! blob:http://seed.dev.mindflow.com.cn/eaaa06c0-b063-44fb-857c-4a0e09673796:1: Uncaught ReferenceError: A is not definedg.onerror @ chunk-vendors.80ce6414e29e3dbe5783.js:2chunk-vendors.80ce6414e29e3dbe5783.js:2 pthread sent an error! blob:http://seed.dev.mindflow.com.cn/2e24e056-724c-4680-ad63-30f01caa3cd7:1: Uncaught ReferenceError: A is not definedg.onerror @ chunk-vendors.80ce6414e29e3dbe5783.js:22d31cc95-07c6-4147-bfe5-038312c33235:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ 2d31cc95-07c6-4147-bfe5-038312c33235:12d31cc95-07c6-4147-bfe5-038312c33235:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (dc4a82ea-9a0f-4fd9-aca3-43ba5da94f7c:1)    at onmessage (2d31cc95-07c6-4147-bfe5-038312c33235:1)threadPrintErr @ 2d31cc95-07c6-4147-bfe5-038312c33235:1901527c4-dc2e-4033-b14b-51186ce336e1:1 Uncaught ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (36a83941-1c7e-4692-bf4f-3068289f6167:1)    at onmessage (901527c4-dc2e-4033-b14b-51186ce336e1:1)50b93f8c-628e-42d5-9177-5f795c2d1243:1 Uncaught ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (3ca13380-4414-4f88-bee2-e405e9632241:1)    at onmessage (50b93f8c-628e-42d5-9177-5f795c2d1243:1)dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1 Uncaught ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (acc831de-3880-49c3-900e-ae7d04186613:1)    at onmessage (dad1bba9-1979-4bb8-bdf2-8106f1724d3b:1)eaaa06c0-b063-44fb-857c-4a0e09673796:1 Uncaught ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (1d084c92-9a0e-4bd2-8b1b-7755455debd7:1)    at onmessage (eaaa06c0-b063-44fb-857c-4a0e09673796:1)2e24e056-724c-4680-ad63-30f01caa3cd7:1 Uncaught ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (a3db88cd-bec1-4a62-aa33-84d6917c3585:1)    at onmessage (2e24e056-724c-4680-ad63-30f01caa3cd7:1)a8d87a3c-bec1-4ce8-95b6-fcc20bf2163b:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ a8d87a3c-bec1-4ce8-95b6-fcc20bf2163b:1a8d87a3c-bec1-4ce8-95b6-fcc20bf2163b:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (92342e3e-49f3-4473-b5af-973837a70056:1)    at onmessage (a8d87a3c-bec1-4ce8-95b6-fcc20bf2163b:1)threadPrintErr @ a8d87a3c-bec1-4ce8-95b6-fcc20bf2163b:135d0c31b-9e47-49e0-a567-a6335908d791:1 worker.js onmessage() captured an uncaught exception: ReferenceError: A is not definedthreadPrintErr @ 35d0c31b-9e47-49e0-a567-a6335908d791:135d0c31b-9e47-49e0-a567-a6335908d791:1 ReferenceError: A is not defined    at WasmBackendModuleThreadedSimd (d09c4f78-f39f-49ab-a607-13d107a468b4:1)    at onmessage (35d0c31b-9e47-49e0-a567-a6335908d791:1)```","['This issue has been addressed [here](https://github.com/tensorflow/tfjs/pull/4121) ; please use latest version ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4473"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4473"">No</a>====='; '> This issue has been addressed [here](https://github.com/tensorflow/tfjs/pull/4121) ; please use latest versionit error again;when Chrome is 91 and tfjs and tfjs-wasm is 3.8.0====='; 'cc @mattsoulanille @pyu10055 =====']",Initialization Faliure,Build & Initialization Failure,Misconfiguration,Wasm,Backend,build/install configuration,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4449,Unnormalized scores,1,closed,2020-12-29T00:41:52Z,2021-01-02T13:52:27Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows10- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):2.7.0- Browser version: Google Chrome. Version 87.0.4280.88 (Official Build) (x86_64)**Describe the current behavior**In [speech-commands](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands) model; when I use `listen` or `recognize` function; _sometimes_ I get unnormalized [scores](https://github.com/tensorflow/tfjs-models/blob/master/speech-commands/src/types.ts#L408) **Describe the expected behavior**excepted to always get normalized [scores](https://github.com/tensorflow/tfjs-models/blob/master/speech-commands/src/types.ts#L408) when using `listen` or `recognize` function**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful toExamples of unnormalized scores: `9.917678198689828e-7`; `3.500647105525445e-9`I noticed that I see `unnormalized` scores when the score is supposed to be around `0`And I see more unnormalized scores with recognize function than with listen function,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4449"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4449"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Unknown,Model API,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/4442,[wasm] realDivide not yet implemented,3,closed,2020-12-23T17:01:19Z,2021-01-05T09:58:27Z,<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 2.7.0- Are you willing to contribute it (Yes/No): Yes**Describe the feature and the current behavior/state.**When using *BodyPix 2.0* with the WASM backend this error appears:```Uncaught (in promise) Error: 'realDivide' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen```The `realDivide` operation is used by BodyPix `preprocessInput` function but is not implemented in the wasm backend.**Who will benefit with this feature?**While BodyPix with the WebGL backend is fine on PC it is way too slow for real-time applications on mobile devices.**Any Other info.**I'll happily implement the feature myself if there's no plan for adding it.,['cc @annxingyuan ====='; 'yeah I had this same exact problem. What should I do to fix it?====='; 'This seems to be working now with tf version 2.8.2Closing====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4434,[wasm] Add Round kernel. Unblocks efficientdet models.,2,closed,2020-12-21T13:13:33Z,2021-01-05T16:37:12Z,"**System information**- TensorFlow.js version (you are using): 2.7.0**Describe the feature and the current behavior/state.**There are plans to add an implementation for efficientdet models in the wasm backend ? It is related to the issue https://github.com/tensorflow/tfjs/issues/4408.When i use webgl backend i  have a problem with precision (my device doesn't support float32) I took the advice from this issue. I tried to use the wasm backend but i have exception ""Error: 'round' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen""",['cc @annxingyuan ====='; 'XNNPACK implementation: https://github.com/google/XNNPACK/tree/master/src/f32-vrnd====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,add support for operator,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4429,tfjs 2.8.1 change in behavior in tf.reshape causes errors,13,closed,2020-12-18T01:54:11Z,2020-12-22T21:33:34Z,upon upgrade from 2.7.0 to 2.8.1; my existing app fails with:```Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3    at Object.inferFromImplicitShape (util_base.ts:328)    at Object.reshape76 [as kernelFunc] (Reshape.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)    at Engine.runKernelFunc (engine.ts:657)    at Engine.runKernel (engine.ts:522)    at reshape_ (reshape.ts:60)    at reshape__op (operation.ts:51)```this may be desired behavior; but it breaks several existing models published on `tfhub`!e.g; i'm using `blazeface` where `reshape` is failing on results of model inference; so not like that can be changed easily:```jsconst [contours; confidence; contourCoords] = this.meshDetector.predict(face);const coordsReshaped = tf.reshape(contourCoords; [-1; 3]);let rawCoords = coordsReshaped.arraySync();```,"['Which backend has this behavior?====='; 'webgl backend====='; '@lina128 I am also receiving this error after upgrading to 2.8.1 with webgl.====='; 'Same thing here!====='; ""Same here! When trying to fetch face landmarks :` const predictions = await model.estimateFaces({ input: video }) `Browser returns `util_base.ts:328 Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3`=====""; 'cc @pyu10055 @annxingyuan @tafsiri ====='; ""Yup; I'm getting the same error as well! I still don't understand why demos like these https://mediapipe-facemesh-3js-tf2.glitch.me/ work. I tried changing the version of my facemesh model but that doesn't do anything=====""; ""@dewball345 because it's hard coded to load tfjs 2.4.0. anything up to 2.7.0 works fine; 2.8.0 and 2.8.1 don't.=====""; 'yeah i realized that now. A temporary fix would be to just use the older version of tensorflow?====='; ""I'm staying on 2.7.0 until this is resolved.=====""; 'Hi @lina128 - I ran a git bisect and it seems to be traceable to this PR: https://github.com/tensorflow/tfjs/pull/4187====='; 'Thanks for the bug report; we will release the fix in 2.8.2 next week.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">No</a>=====']",Data & Model Error,Crash,Inconsistent Modules,WebGL,Backend,condition replacer,condition replacer,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.2
https://github.com/tensorflow/tfjs/issues/4429,tfjs 2.8.1 change in behavior in tf.reshape causes errors,13,closed,2020-12-18T01:54:11Z,2020-12-22T21:33:34Z,upon upgrade from 2.7.0 to 2.8.1; my existing app fails with:```Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3    at Object.inferFromImplicitShape (util_base.ts:328)    at Object.reshape76 [as kernelFunc] (Reshape.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)    at Engine.runKernelFunc (engine.ts:657)    at Engine.runKernel (engine.ts:522)    at reshape_ (reshape.ts:60)    at reshape__op (operation.ts:51)```this may be desired behavior; but it breaks several existing models published on `tfhub`!e.g; i'm using `blazeface` where `reshape` is failing on results of model inference; so not like that can be changed easily:```jsconst [contours; confidence; contourCoords] = this.meshDetector.predict(face);const coordsReshaped = tf.reshape(contourCoords; [-1; 3]);let rawCoords = coordsReshaped.arraySync();```,"['Which backend has this behavior?====='; 'webgl backend====='; '@lina128 I am also receiving this error after upgrading to 2.8.1 with webgl.====='; 'Same thing here!====='; ""Same here! When trying to fetch face landmarks :` const predictions = await model.estimateFaces({ input: video }) `Browser returns `util_base.ts:328 Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3`=====""; 'cc @pyu10055 @annxingyuan @tafsiri ====='; ""Yup; I'm getting the same error as well! I still don't understand why demos like these https://mediapipe-facemesh-3js-tf2.glitch.me/ work. I tried changing the version of my facemesh model but that doesn't do anything=====""; ""@dewball345 because it's hard coded to load tfjs 2.4.0. anything up to 2.7.0 works fine; 2.8.0 and 2.8.1 don't.=====""; 'yeah i realized that now. A temporary fix would be to just use the older version of tensorflow?====='; ""I'm staying on 2.7.0 until this is resolved.=====""; 'Hi @lina128 - I ran a git bisect and it seems to be traceable to this PR: https://github.com/tensorflow/tfjs/pull/4187====='; 'Thanks for the bug report; we will release the fix in 2.8.2 next week.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">No</a>=====']",Data & Model Error,Crash,Inconsistent Modules,WebGL,Backend,condition replacer,condition replacer,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.3.1,A.2
https://github.com/tensorflow/tfjs/issues/4429,tfjs 2.8.1 change in behavior in tf.reshape causes errors,13,closed,2020-12-18T01:54:11Z,2020-12-22T21:33:34Z,upon upgrade from 2.7.0 to 2.8.1; my existing app fails with:```Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3    at Object.inferFromImplicitShape (util_base.ts:328)    at Object.reshape76 [as kernelFunc] (Reshape.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)    at Engine.runKernelFunc (engine.ts:657)    at Engine.runKernel (engine.ts:522)    at reshape_ (reshape.ts:60)    at reshape__op (operation.ts:51)```this may be desired behavior; but it breaks several existing models published on `tfhub`!e.g; i'm using `blazeface` where `reshape` is failing on results of model inference; so not like that can be changed easily:```jsconst [contours; confidence; contourCoords] = this.meshDetector.predict(face);const coordsReshaped = tf.reshape(contourCoords; [-1; 3]);let rawCoords = coordsReshaped.arraySync();```,"['Which backend has this behavior?====='; 'webgl backend====='; '@lina128 I am also receiving this error after upgrading to 2.8.1 with webgl.====='; 'Same thing here!====='; ""Same here! When trying to fetch face landmarks :` const predictions = await model.estimateFaces({ input: video }) `Browser returns `util_base.ts:328 Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3`=====""; 'cc @pyu10055 @annxingyuan @tafsiri ====='; ""Yup; I'm getting the same error as well! I still don't understand why demos like these https://mediapipe-facemesh-3js-tf2.glitch.me/ work. I tried changing the version of my facemesh model but that doesn't do anything=====""; ""@dewball345 because it's hard coded to load tfjs 2.4.0. anything up to 2.7.0 works fine; 2.8.0 and 2.8.1 don't.=====""; 'yeah i realized that now. A temporary fix would be to just use the older version of tensorflow?====='; ""I'm staying on 2.7.0 until this is resolved.=====""; 'Hi @lina128 - I ran a git bisect and it seems to be traceable to this PR: https://github.com/tensorflow/tfjs/pull/4187====='; 'Thanks for the bug report; we will release the fix in 2.8.2 next week.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">No</a>=====']",Regression,Poor Performance,Inconsistent Modules,WebGL,Backend,condition replacer,condition replacer,framework,Data Processing,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3""
  }
}
```",A.2,A.2
https://github.com/tensorflow/tfjs/issues/4429,tfjs 2.8.1 change in behavior in tf.reshape causes errors,13,closed,2020-12-18T01:54:11Z,2020-12-22T21:33:34Z,upon upgrade from 2.7.0 to 2.8.1; my existing app fails with:```Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3    at Object.inferFromImplicitShape (util_base.ts:328)    at Object.reshape76 [as kernelFunc] (Reshape.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)    at Engine.runKernelFunc (engine.ts:657)    at Engine.runKernel (engine.ts:522)    at reshape_ (reshape.ts:60)    at reshape__op (operation.ts:51)```this may be desired behavior; but it breaks several existing models published on `tfhub`!e.g; i'm using `blazeface` where `reshape` is failing on results of model inference; so not like that can be changed easily:```jsconst [contours; confidence; contourCoords] = this.meshDetector.predict(face);const coordsReshaped = tf.reshape(contourCoords; [-1; 3]);let rawCoords = coordsReshaped.arraySync();```,"['Which backend has this behavior?====='; 'webgl backend====='; '@lina128 I am also receiving this error after upgrading to 2.8.1 with webgl.====='; 'Same thing here!====='; ""Same here! When trying to fetch face landmarks :` const predictions = await model.estimateFaces({ input: video }) `Browser returns `util_base.ts:328 Uncaught (in promise) Error: The implicit shape can't be a fractional number. Got 266 / 3`=====""; 'cc @pyu10055 @annxingyuan @tafsiri ====='; ""Yup; I'm getting the same error as well! I still don't understand why demos like these https://mediapipe-facemesh-3js-tf2.glitch.me/ work. I tried changing the version of my facemesh model but that doesn't do anything=====""; ""@dewball345 because it's hard coded to load tfjs 2.4.0. anything up to 2.7.0 works fine; 2.8.0 and 2.8.1 don't.=====""; 'yeah i realized that now. A temporary fix would be to just use the older version of tensorflow?====='; ""I'm staying on 2.7.0 until this is resolved.=====""; 'Hi @lina128 - I ran a git bisect and it seems to be traceable to this PR: https://github.com/tensorflow/tfjs/pull/4187====='; 'Thanks for the bug report; we will release the fix in 2.8.2 next week.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4429"">No</a>=====']",Regression,Poor Performance,Inconsistent Modules,WebGL,Backend,condition replacer,condition replacer,framework,Data Processing,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3""
  }
}
```",B.3.1,A.2
https://github.com/tensorflow/tfjs/issues/4419,`npm install` succeeds despite errors due to not having Python (in a container),3,closed,2020-12-16T20:56:51Z,2021-02-17T16:27:00Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No.- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux container (docker) node:14- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ^2.8.0- Tensorflow.js Converter Version: N/A**Describe the current behavior**1. Running `npm install` in a `docker build step`; with Dockerfile:	  ```	  FROM node:14-slim	  WORKDIR /usr/src/app	  COPY package*.json ./	  RUN npm install --only=production	  COPY . ./	  CMD [ ""node""; ""index.js"" ]	  ```2. It succeeds and exits with 0.3. However; the build log shows errors saying it cannot find Python.**Describe the expected behavior**If it can't find python; `npm install` **should fail** and fail the whole build process in turn.**Standalone code to reproduce the issue****Other info / logs** Logs from docker build:```docker build -t gcr.io/ahmetb-demo/tfjs-cloudrun .Sending build context to Docker daemon  55.85MBStep 1/6 : FROM node:14-slim ---> 30b595bd6403Step 2/6 : WORKDIR /usr/src/app ---> Using cache ---> aa273d65905cStep 3/6 : COPY package*.json ./ ---> 95c5ad63c38fStep 4/6 : RUN npm install --only=production ---> Running in ba6ed059f4a1npm WARN read-shrinkwrap This version of npm is compatible with lockfileVersion@1; but package-lock.json was generated for lockfileVersion@2. I'll try to do my best with it!> @tensorflow/tfjs-node@2.8.0 install /usr/src/app/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-linux-2.8.0.tar.gz* Downloading libtensorflow* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https downloadnode-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/2.8.0/CPU-linux-2.8.0.tar.gznode-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@2.8.0 and node@14.15.1 (node-v83 ABI; glibc) (falling back to source compile with node-gyp)gyp ERR! find Pythongyp ERR! find Python Python is not set from command line or npm configurationgyp ERR! find Python Python is not set from environment variable PYTHONgyp ERR! find Python checking if ""python"" can be usedgyp ERR! find Python - ""python"" is not in PATH or produced an errorgyp ERR! find Python checking if ""python2"" can be usedgyp ERR! find Python - ""python2"" is not in PATH or produced an errorgyp ERR! find Python checking if ""python3"" can be usedgyp ERR! find Python - ""python3"" is not in PATH or produced an errorgyp ERR! find Pythongyp ERR! find Python **********************************************************gyp ERR! find Python You need to install the latest version of Python.gyp ERR! find Python Node-gyp should be able to find and use Python. If not;gyp ERR! find Python you can try one of the following options:gyp ERR! find Python - Use the switch --python=""/path/to/pythonexecutable""gyp ERR! find Python   (accepted by both node-gyp and npm)gyp ERR! find Python - Set the environment variable PYTHONgyp ERR! find Python - Set the npm configuration variable python:gyp ERR! find Python   npm config set python ""/path/to/pythonexecutable""gyp ERR! find Python For more information consult the documentation at:gyp ERR! find Python https://github.com/nodejs/node-gyp#installationgyp ERR! find Python **********************************************************gyp ERR! find Pythongyp ERR! configure errorgyp ERR! stack Error: Could not find any Python installation to usegyp ERR! stack     at PythonFinder.fail (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:307:47)gyp ERR! stack     at PythonFinder.runChecks (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:136:21)gyp ERR! stack     at PythonFinder.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:179:16)gyp ERR! stack     at PythonFinder.execFileCallback (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/find-python.js:271:16)gyp ERR! stack     at exithandler (child_process.js:315:5)gyp ERR! stack     at ChildProcess.errorhandler (child_process.js:327:5)gyp ERR! stack     at ChildProcess.emit (events.js:315:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12)gyp ERR! stack     at onErrorNT (internal/child_process.js:465:16)gyp ERR! stack     at processTicksAndRejections (internal/process/task_queues.js:80:21)gyp ERR! System Linux 4.19.121-linuxkitgyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7"" ""--napi_version=7"" ""--node_abi_napi=napi"" ""--napi_build_version=7"" ""--node_napi_label=napi-v7""gyp ERR! cwd /usr/src/app/node_modules/@tensorflow/tfjs-nodegyp ERR! node -v v14.15.1gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute '/usr/local/bin/node /usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js configure --fallback-to-build --module=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7/tfjs_binding.node --module_name=tfjs_binding --module_path=/usr/src/app/node_modules/@tensorflow/tfjs-node/lib/napi-v7 --napi_version=7 --node_abi_napi=napi --napi_build_version=7 --node_napi_label=napi-v7' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/usr/src/app/node_modules/node-pre-gyp/lib/util/compile.js:83:29)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:315:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1048:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:288:5)node-pre-gyp ERR! System Linux 4.19.121-linuxkitnode-pre-gyp ERR! command ""/usr/local/bin/node"" ""/usr/src/app/node_modules/.bin/node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd /usr/src/app/node_modules/@tensorflow/tfjs-nodenode-pre-gyp ERR! node -v v14.15.1node-pre-gyp ERR! node-pre-gyp -v v0.14.0node-pre-gyp ERR! not ok> core-js@3.8.1 postinstall /usr/src/app/node_modules/core-js> node -e ""try{require('./postinstall')}catch(e){}""Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!The project needs your help! Please consider supporting of core-js on Open Collective or Patreon:> https://opencollective.com/core-js> https://www.patreon.com/zloirockAlso; the author of core-js ( https://github.com/zloirock ) is looking for a good job -)npm WARN object-detection@ No descriptionnpm WARN object-detection@ No repository field.npm WARN object-detection@ No license field.added 182 packages from 140 contributors and audited 182 packages in 12.534s5 packages are looking for funding  run `npm fund` for detailsfound 0 vulnerabilitiesRemoving intermediate container ba6ed059f4a1 ---> d20320f6d3f5Step 5/6 : COPY . ./ ---> c79f02866ef4Step 6/6 : CMD [ ""node""; ""index.js"" ] ---> Running in fa4cbd173895Removing intermediate container fa4cbd173895 ---> dab5876e0308Successfully built dab5876e0308Successfully tagged gcr.io/ahmetb-demo/tfjs-cloudrun:latest```It shouldn't succed. It clearly says `node-pre-gyp ERR! not ok` but `npm install` succeeds.","['We had issues with 2.8.0 ; which was rolled back ; can you please try with 2.8.1 ?====='; ""The issue was present even on 1.4.0; I don't think it's a recently occurred issue. =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4419"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4419"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Improper Exception Handling,TF(CPU),Backend,add exception handling,add exception handling,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2"",
    ""specific_type"": ""C.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1""
  }
}
```",C,A.7
https://github.com/tensorflow/tfjs/issues/4418,tfjs 2.8.0 is broken and introduces regressions in tf.image.cropAndResize,10,closed,2020-12-16T19:29:47Z,2020-12-18T02:02:27Z,As subject line says; TFJS 2.8.0 unfortunately seems like a broken version.First; default parameter in `cropAndResize` was unintentionally removed (and apparently just re-added via #4407) which causes quite a lot of errors in existing appsSecond; even when specifying resize method as `bilinear`; `cropAndResize` FAILS on `WebGL` baclend:```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.ts:103)    at GPGPUContext.createProgram (gpgpu_context.ts:280)    at compileProgram (gpgpu_math.ts:93)    at backend_webgl.ts:858    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:902)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:857)    at Object.cropAndResize3 [as kernelFunc] (CropAndResize.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)```With WebGL code dump highlighting error in```204 setOutput(float(undefined));```IMO; this is is blocking bug - once fixed; TFJS packages should be republished!(and I'm not sure how this passed any level of testing?)Btw; same code confirmed working after downgrade to tfjs 2.7.0.,"['cc @annxingyuan @lina128 for visibility - sorry for spam; but this is bad====='; 'cc @tafsiri ====='; ""Hi @vladmandic - thanks for reporting this issue - just wanted to let you know that you can avoid the fragment shader compile error by passing both `method` and `extrapolationValue` into `cropAndResize`; e.g.: `tf.image.cropAndResize(image; boxes; boxInd; [5;5]; 'bilinear'; 0 /* extrapolation value */) // note the last argument` =====""; ""@annxingyuan thanks for that - but I have `cropAndResize` all over my code; I'll just wait until this is fixed and tfjs is republished - in the meantime i'm sticking with tfjs 2.7.0.=====""; 'Hi @vladmandic; we are going to release the fix in 2.8.1 very soon.====='; ""Since `2.8.0` is tagged as `latest`; the whole package looks deprecated now; see:https://www.npmjs.com/package/@tensorflow/tfjsI'd recommend to tag `2.7.0` as `latest` until you have a fixed version to remove the warning from the package npm page:```npm dist-tag add @tensorflow/tfjs@2.7.0 latest```=====""; ""Thanks for the tip @mgol! We've tagged 2.7.0 as the latest npm version for all packages that were in the 2.8.0 release. =====""; '2.8.1 is released. It should fix the bug.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">No</a>====='; 'tfjs 2.8.1 fixes this issue; but exposes another issue in #4429=====']",Browser & Device Error,Crash,Incorrect Code Logic,WebGL,Backend,parameter modifier,Modify API Parameter usage,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",A.4,A.4
https://github.com/tensorflow/tfjs/issues/4418,tfjs 2.8.0 is broken and introduces regressions in tf.image.cropAndResize,10,closed,2020-12-16T19:29:47Z,2020-12-18T02:02:27Z,As subject line says; TFJS 2.8.0 unfortunately seems like a broken version.First; default parameter in `cropAndResize` was unintentionally removed (and apparently just re-added via #4407) which causes quite a lot of errors in existing appsSecond; even when specifying resize method as `bilinear`; `cropAndResize` FAILS on `WebGL` baclend:```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.ts:103)    at GPGPUContext.createProgram (gpgpu_context.ts:280)    at compileProgram (gpgpu_math.ts:93)    at backend_webgl.ts:858    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:902)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:857)    at Object.cropAndResize3 [as kernelFunc] (CropAndResize.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)```With WebGL code dump highlighting error in```204 setOutput(float(undefined));```IMO; this is is blocking bug - once fixed; TFJS packages should be republished!(and I'm not sure how this passed any level of testing?)Btw; same code confirmed working after downgrade to tfjs 2.7.0.,"['cc @annxingyuan @lina128 for visibility - sorry for spam; but this is bad====='; 'cc @tafsiri ====='; ""Hi @vladmandic - thanks for reporting this issue - just wanted to let you know that you can avoid the fragment shader compile error by passing both `method` and `extrapolationValue` into `cropAndResize`; e.g.: `tf.image.cropAndResize(image; boxes; boxInd; [5;5]; 'bilinear'; 0 /* extrapolation value */) // note the last argument` =====""; ""@annxingyuan thanks for that - but I have `cropAndResize` all over my code; I'll just wait until this is fixed and tfjs is republished - in the meantime i'm sticking with tfjs 2.7.0.=====""; 'Hi @vladmandic; we are going to release the fix in 2.8.1 very soon.====='; ""Since `2.8.0` is tagged as `latest`; the whole package looks deprecated now; see:https://www.npmjs.com/package/@tensorflow/tfjsI'd recommend to tag `2.7.0` as `latest` until you have a fixed version to remove the warning from the package npm page:```npm dist-tag add @tensorflow/tfjs@2.7.0 latest```=====""; ""Thanks for the tip @mgol! We've tagged 2.7.0 as the latest npm version for all packages that were in the 2.8.0 release. =====""; '2.8.1 is released. It should fix the bug.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">No</a>====='; 'tfjs 2.8.1 fixes this issue; but exposes another issue in #4429=====']",Browser & Device Error,Crash,Incorrect Code Logic,WebGL,Backend,parameter modifier,Modify API Parameter usage,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.2] Inconsistent Modules in TF.js""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4418,tfjs 2.8.0 is broken and introduces regressions in tf.image.cropAndResize,10,closed,2020-12-16T19:29:47Z,2020-12-18T02:02:27Z,As subject line says; TFJS 2.8.0 unfortunately seems like a broken version.First; default parameter in `cropAndResize` was unintentionally removed (and apparently just re-added via #4407) which causes quite a lot of errors in existing appsSecond; even when specifying resize method as `bilinear`; `cropAndResize` FAILS on `WebGL` baclend:```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.ts:103)    at GPGPUContext.createProgram (gpgpu_context.ts:280)    at compileProgram (gpgpu_math.ts:93)    at backend_webgl.ts:858    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:902)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:857)    at Object.cropAndResize3 [as kernelFunc] (CropAndResize.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)```With WebGL code dump highlighting error in```204 setOutput(float(undefined));```IMO; this is is blocking bug - once fixed; TFJS packages should be republished!(and I'm not sure how this passed any level of testing?)Btw; same code confirmed working after downgrade to tfjs 2.7.0.,"['cc @annxingyuan @lina128 for visibility - sorry for spam; but this is bad====='; 'cc @tafsiri ====='; ""Hi @vladmandic - thanks for reporting this issue - just wanted to let you know that you can avoid the fragment shader compile error by passing both `method` and `extrapolationValue` into `cropAndResize`; e.g.: `tf.image.cropAndResize(image; boxes; boxInd; [5;5]; 'bilinear'; 0 /* extrapolation value */) // note the last argument` =====""; ""@annxingyuan thanks for that - but I have `cropAndResize` all over my code; I'll just wait until this is fixed and tfjs is republished - in the meantime i'm sticking with tfjs 2.7.0.=====""; 'Hi @vladmandic; we are going to release the fix in 2.8.1 very soon.====='; ""Since `2.8.0` is tagged as `latest`; the whole package looks deprecated now; see:https://www.npmjs.com/package/@tensorflow/tfjsI'd recommend to tag `2.7.0` as `latest` until you have a fixed version to remove the warning from the package npm page:```npm dist-tag add @tensorflow/tfjs@2.7.0 latest```=====""; ""Thanks for the tip @mgol! We've tagged 2.7.0 as the latest npm version for all packages that were in the 2.8.0 release. =====""; '2.8.1 is released. It should fix the bug.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">No</a>====='; 'tfjs 2.8.1 fixes this issue; but exposes another issue in #4429=====']",Regression,Poor Performance,Incorrect Code Logic,WebGL,Backend,parameter modifier,Modify API Parameter usage,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.4,A.4
https://github.com/tensorflow/tfjs/issues/4418,tfjs 2.8.0 is broken and introduces regressions in tf.image.cropAndResize,10,closed,2020-12-16T19:29:47Z,2020-12-18T02:02:27Z,As subject line says; TFJS 2.8.0 unfortunately seems like a broken version.First; default parameter in `cropAndResize` was unintentionally removed (and apparently just re-added via #4407) which causes quite a lot of errors in existing appsSecond; even when specifying resize method as `bilinear`; `cropAndResize` FAILS on `WebGL` baclend:```Uncaught (in promise) Error: Failed to compile fragment shader.    at createFragmentShader (webgl_util.ts:103)    at GPGPUContext.createProgram (gpgpu_context.ts:280)    at compileProgram (gpgpu_math.ts:93)    at backend_webgl.ts:858    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:902)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:857)    at Object.cropAndResize3 [as kernelFunc] (CropAndResize.ts:35)    at kernelFunc3 (engine.ts:590)    at engine.ts:660    at Engine.scopedRun (engine.ts:453)```With WebGL code dump highlighting error in```204 setOutput(float(undefined));```IMO; this is is blocking bug - once fixed; TFJS packages should be republished!(and I'm not sure how this passed any level of testing?)Btw; same code confirmed working after downgrade to tfjs 2.7.0.,"['cc @annxingyuan @lina128 for visibility - sorry for spam; but this is bad====='; 'cc @tafsiri ====='; ""Hi @vladmandic - thanks for reporting this issue - just wanted to let you know that you can avoid the fragment shader compile error by passing both `method` and `extrapolationValue` into `cropAndResize`; e.g.: `tf.image.cropAndResize(image; boxes; boxInd; [5;5]; 'bilinear'; 0 /* extrapolation value */) // note the last argument` =====""; ""@annxingyuan thanks for that - but I have `cropAndResize` all over my code; I'll just wait until this is fixed and tfjs is republished - in the meantime i'm sticking with tfjs 2.7.0.=====""; 'Hi @vladmandic; we are going to release the fix in 2.8.1 very soon.====='; ""Since `2.8.0` is tagged as `latest`; the whole package looks deprecated now; see:https://www.npmjs.com/package/@tensorflow/tfjsI'd recommend to tag `2.7.0` as `latest` until you have a fixed version to remove the warning from the package npm page:```npm dist-tag add @tensorflow/tfjs@2.7.0 latest```=====""; ""Thanks for the tip @mgol! We've tagged 2.7.0 as the latest npm version for all packages that were in the 2.8.0 release. =====""; '2.8.1 is released. It should fix the bug.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4418"">No</a>====='; 'tfjs 2.8.1 fixes this issue; but exposes another issue in #4429=====']",Regression,Poor Performance,Incorrect Code Logic,WebGL,Backend,parameter modifier,Modify API Parameter usage,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,A.4
https://github.com/tensorflow/tfjs/issues/4397,JS Error in demo,1,closed,2020-12-13T14:46:21Z,2020-12-14T21:08:02Z,**System information**Demo URL mentioned in the link (https://github.com/tensorflow/tfjs-models/tree/master/qna)https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html is not working!**Describe the expected behavior**I didn't get answers to any question. Attaching the error here.![demoerror](https://user-images.githubusercontent.com/2010921/102015121-af880500-3d7f-11eb-9f09-eef093a6cbaa.JPG),['There is a similar issue [here](https://github.com/tensorflow/tfjs/issues/3861) for tracking ; will close this and track it at one place.====='],Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4386,Tensorflow.js with wasm backend is trying to use unsupported function 'resizeNearestNeighbor'; even though I used bilinear interpolation in all the places,6,closed,2020-12-10T16:30:16Z,2020-12-17T11:25:42Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu 18.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): pip install tensorflowjs- TensorFlow.js version (use command below): 2.7.0- Browser version: Google Chrome Version 84.0.4147.89 (Official Build) (64-bit)- Tensorflow.js Converter Version: v2.4.0**Describe the current behavior**Tensorflow.js with wasm backend is trying to use unsupported function 'resizeNearestNeighbor'; even though I used bilinear interpolation in all the places. I get the following error when trying to run inference in the browser ""Error: 'resizeNearestNeighbor' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen"".Code runs smoothly but slowly when switching to ""cpu"" backend.**Describe the expected behavior**I would like the inference to run without errors and use Bilinear interpolation which is supported on all the backends.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.I can not share the trained weights; but the rest of inference code is here. You can see the model.json file; which does not contain any nearest neighbor interpolation. https://github.com/martun/tensorflow_js_issue**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","[""Related to https://github.com/tensorflow/tfjs/issues/4340 and https://github.com/tensorflow/tfjs/issues/4212The underlying issue is the UpSampling2D layer (used in your model0 directly calls `resizeNearestNeighbor` and doesn't seem handle the bilinear interpolation option. @pyu10055 @caisq; should this be an enhancement to the UpSampling2D layer?=====""; ""@tafsiri I agree this should be considered as an enhancement. It is currently unimplemented is most likely lack of support by tfjs-core. So it's possible that the nearest-neighbor resampling support is still unavailable from tfjs-core and that'll need to be addressed first. =====""; '@caisq tfjs-core has both resizeNearestNeighbour (currently called) and resizeBilinear; so we should be able to switch on that unless resizeBilinear does not support something required by upsampling2d.====='; '@caisq Actually looks like its been added!====='; 'Thanks guys; it started to work after I updated to tfjs version: 2.8.0====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4386"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4386"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Layer API,API,add support for option,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/4378,GPU memory leak of tf.signal.stft,8,closed,2020-12-09T06:37:36Z,2021-01-21T04:03:56Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 10.15.7- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0""> </script>- TensorFlow.js version (use command below): 2.7.0- Browser version: Chrome 87.0.4280.88- Tensorflow.js Converter Version: N/A**Describe the current behavior**There is GPU memory leak after use tf.signal.stft. If use it continuously; it leads to crash the Chrome GPU process.  **Describe the expected behavior**There is no GPU memory leak after use tf.signal.stft.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.```jsfunction hanningWindow(N) {  return tf.tidy(() => {    const window = new Float32Array(N);    for (let i = 0; i < N - 1; ++i) {      window[i] = 0.5*(1 - Math.cos(6.283185307179586*i/(N-1)));    }    return tf.sqrt(window);  });}console.log(tf.engine().memory());// e.g. {unreliable: false; numBytesInGPU: 0; numBytesInGPUAllocated: 0; numBytesInGPUFree: 0; numTensors: 0; …}const N = 100; // Reproduce the GPU process crash by setting large number; e.g. 10000 for (let i = 0; i < N; ++i) {  const win = 320;  const fft = 320;  const hop = 160;  const input = tf.zeros([1760]);  const output = tf.signal.stft(input; win; hop; fft; hanningWindow);  input.dispose();  output.dispose();}console.log(tf.engine().memory());// e.g. {unreliable: false; numBytesInGPU: 2560000; numBytesInGPUAllocated: 2694080; numBytesInGPUFree: 134080; numTensors: 0; …}```The GPU memory leak can be reproduced by using WebGL backend. The CPU backend doesn't have this issue.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['@huningxin thanks for reporting; the intermediate tensors seem to have be disposed; but the bytesInGPU has increased; @annxingyuan can you confirm this is normal? Is the numBytesInGPUAllocated counting the texture size?====='; ""Thanks @pyu10055 for your comments. If you'd like to reproduce the Chrome GPU process crash; you can simply set the `N` to a large number; say 100000; in above [sample code](https://github.com/tensorflow/tfjs/issues/4378#issue-760048997).=====""; '@huningxin I notice that the `hanningWindow` output is tight to the size `N`; which is also not disposed after stft call.The other thing you can choose to do to set the texture dispose threshold flag `WEBGL_DELETE_TEXTURE_THRESHOLD`; here is modified code:```javascript    function hanningWindow(N) {      const window = new Float32Array(N);      for (let i = 0; i < N - 1; ++i) {        window[i] = 0.5 * (1 - Math.cos(6.283185307179586 * i / (N - 1)));      }      return tf.sqrt(window);    }    console.log(tf.engine().memory());    // e.g. {unreliable: false; numBytesInGPU: 0; numBytesInGPUAllocated: 0; numBytesInGPUFree: 0; numTensors: 0; …}    // aggressive gpu texture disposal starts at 100M    tf.env().set(""WEBGL_DELETE_TEXTURE_THRESHOLD""; 100 * 1024 * 1024);    const N = 10000; // Reproduce the GPU process crash by setting large number; e.g. 10000     for (let i = 0; i < N; ++i) {      const win = 320;      const fft = 320;      const hop = 160;      const input = tf.zeros([1760]);      //const ident = (length) => tf.ones([length]).as1D();      const output = tf.tidy(() => {        return tf.signal.stft(input; win; hop; fft; hanningWindow);      })      input.dispose();      output.dispose();    }    console.log(tf.engine().memory());```====='; ""Thanks for your reply; @pyu10055 !> @huningxin I notice that the `hanningWindow` output is tight to the size `N`; which is also not disposed after stft call.I understand the output of `hanningWindow` is used by `tf.signal.stft` internally. I suppose it would be `tf.singla.stft`'s responsibility to dispose it. Is that correct?> The other thing you can choose to do to set the texture dispose threshold flag `WEBGL_DELETE_TEXTURE_THRESHOLD`; here is modified code:I tried the modified code. The `numBytesInGPU` can still go higher than `WEBGL_DELETE_TEXTURE_THRESHOLD`; 100 * 1024 * 1024. e.g. after run the loop several times; the output of `console.log(tf.engine().memory());` is:```{unreliable: false; numBytesInGPU: 130560000; numBytesInGPUAllocated: 130560000; numBytesInGPUFree: 0; numTensors: 0;\xa0…}numBytes: 0numBytesInGPU: 130560000numBytesInGPUAllocated: 130560000numBytesInGPUFree: 0numDataBuffers: 0numTensors: 0unreliable: false__proto__: Object```=====""; '@huningxin I think you are right that; the op is wrapped in the `ENGINE.startScope` and `endScope`; intermediate tensor should have been disposed; which is also verified through the tensor count from the `memory()`result.I curious about the size exceeding the limit as well; possibly there are some large intermediate tensor get allocated; which cause the memory to exceed the limit; maybe the disposal of texture is done before the tensor creation. @annxingyuan ====='; ""I'm investigating. This is the first bad commit: https://github.com/tensorflow/tfjs/commit/c537c813ba8234a909d1e0849abfd95998a8fb5b=====""; 'I verified with tf.js 2.8.5 release that the memory leak issue has been fixed. Thanks much @pyu10055 and @annxingyuan !====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,WebGL,Backend,memory management,Add API usage for memory management,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/4362,Memory Leak with GPU & tf.node.decodeImage,10,open,2020-12-06T23:08:18Z,2021-12-18T18:19:00Z,Windows 10 20H2; Cuda 10.0; Cudnn v7.6.5; TFJS v2.7.0**Issue**Running the following code fills my 1080s memory causing issues when performing any actual useful code```const { node: { decodeImage }; tidy; engine } = require('@tensorflow/tfjs-node-gpu')const fs = require('fs')async function main () {  engine().startScope()  const buffer = fs.readFileSync('./test.jpg')  const tensor = tidy(() => decodeImage(Buffer.from(buffer)).toFloat().expandDims())  tensor.dispose()  engine().endScope()  setTimeout(() => {  }; 5000)}main()```I tried without scopes & tidy. Same result. GPU usage goes back to normal after the timeout ends & the script exists.,"['you can use [tf.node.decodeJpeg ]( https://js.tensorflow.org/api_node/2.7.0/#node.decodeJpeg ) to decode the image ; please try====='; '> you can use [tf.node.decodeJpeg ](https://js.tensorflow.org/api_node/2.7.0/#node.decodeJpeg) to decode the image ; please tryExact same result. ====='; 'cc @pyu10055 @tafsiri ====='; ""@Nuzzlet Trying to isolate if this is a tfjs-node or GPU only issue; since we don't have a windows machine with CUDA. Have you tried the same setting in tfjs-node cpu? does it behave the same?=====""; ""> @Nuzzlet Trying to isolate if this is a tfjs-node or GPU only issue; since we don't have a windows machine with CUDA. Have you tried the same setting in tfjs-node cpu? does it behave the same?It is isolated to tfjs-node-gpu. Does not happen with tfjs-node or tfjs webGL=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'same issue on tf-js-node cpu====='; ""I'm having the same problem with the `tfjs-node` CPU backend; but only for certain JPEGs shot on iPhones. Some cause a memory leak; some don't.If it helps; here's the header of a leaky image:```FF D8 FF E1 00 22 45 78 69 66 00 00 4D 4D 00 2A 00 00 00 08 00 01 01 12 00 03 00 00 00 01 00 01 00 00 00 00 00 00 FF ED 00 38 50 68 6F 74 6F 73 68 6F 70 20 33 2E 30 00 38 42 49 4D 04 04 00 00 00 00 00 00 38 42 49 4D 04 25 00 00 00 00 00 10 D4 1D 8C D9 8F 00 B2 04 E9 80 09 98 EC F8 42 7E FF E2 02 34 49 43 43 5F 50 52 4F 46 49 4C 45 00 01 01 00 00 02 24 61 70 70 6C 04 00 00 00 6D 6E 74 72 52 47 42 20 58 59 5A 20 07 E1 00 07 00 07 00 0D 00 16 00 20 61 63 73 70 41 50 50 4C 00 00 00 00 41 50 50 4C 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 F6 D6 00 01 00 00 00 00 D3 2D 61 70 70 6C CA 1A 95 82 25 7F 10 4D 38 99 13 D5 D1 EA 15 82 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0A 64 65 73 63 00 00 00 FC 00 00 00 65 63 70 72 74 00 00 01 64 00 00 00 23 77 74 70 74 00 00 01 88 00 00 00 14 72 58 59 5A 00 00 01 9C 00 00 00 14 67 58 59 5A 00 00 01 B0 00 00 00 14 62 58 59 5A 00 00 01 C4 00 00 00 14 72 54 52 43 00 00 01 D8 00 00 00 20 63 68 61 64 00 00 01 F8 00 00 00 2C 62 54 52 43 00 00 01 D8 00 00 00 20 67 54 52 43 00 00 01 D8 00 00 00 20 64 65 73 63 00 00 00 00 00 00 00 0B 44 69 73 70 6C 61 79 20 50 33 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 74 65 78 74 00 00 00 00 43 6F 70 79 72 69 67 68 74 20 41 70 70 6C 65 20 49 6E 63 2E 2C 20 32 30 31 37 00 00 58 59 5A 20 00 00 00 00 00 00 F3 51 00 01 00 00 00 01 16 CC 58 59 5A 20 00 00 00 00 00 00 83 DF 00 00 3D BF FF FF FF BB 58 59 5A 20 00 00 00 00 00 00 4A BF 00 00 B1 37 00 00 0A B9 58 59 5A 20 00 00 00 00 00 00 28 38 00 00 11 0B 00 00 C8 B9 70 61 72 61 00 00 00 00 00 03 00 00 00 02 66 66 00 00 F2 A7 00 00 0D 59 00 00 13 D0 00 00 0A 5B 73 66 33 32 00 00 00 00 00 01 0C 42 00 00 05 DE FF FF F3 26 00 00 07 93 00 00 FD 90 FF FF FB A2 FF FF FD A3 00 00 03 DC 00 00 C0 6E FF C0 00 11 08 0F C0 0B D0 03 01 22 00 02 11 01 03 11 01 FF C4 00 1F 00 00 01 05 01 01 01 01 01 01 00 00 00 00 00 00 00 00 01 02 03 04 05 06 07 08 09 0A 0B FF C4 00 B5 10 00 02 01 03 03 02 04 03 05 05 04 04 00 00 01 7D 01 02 03 00 04 11 05 12 21 31 41 06 13 51 61 07 22 71 14 32 81 91 A1 08 23 42 B1 C1 15 52 D1 F0 24 33 62 72 82 09 0A 16 17 18 19 1A 25 26 27 28 29 2A 34 35 36 37 38 39 3A 43 44 45 46 47 48 49 4A 53 54 55 56 57 58 59 5A 63 64 65 66 67 68 69 6A 73 74 75 76 77 78 79 7A 83 84 85 86 87 88 89 8A 92 93 94 95 96 97 98 99 9A A2 A3 A4 A5 A6 A7 A8 A9 AA B2 B3 B4 B5 B6 B7 B8 B9 BA C2 C3 C4 C5 C6 C7 C8 C9 CA D2 D3 D4 D5 D6 D7 D8 D9 DA E1 E2 E3 E4 E5 E6 E7 E8 E9 EA F1 F2 F3 F4 F5 F6 F7 F8 F9 FA FF C4 00 1F 01 00 03 01 01 01 01 01 01 01 01 01 00 00 00 00 00 00 01 02 03 04 05 06 07 08 09 0A 0B FF C4 00 B5 11 00 02 01 02 04 04 03 04 07 05 04 04 00 01 02 77 00 01 02 03 11 04 05 21 31 06 12 41 51 07 61 71 13 22 32 81 08 14 42 91 A1 B1 C1 09 23 33 52 F0 15 62 72 D1 0A 16 24 34 E1 25 F1 17 18 19 1A 26 27 28 29 2A 35 36 37 38 39 3A 43 44 45 46 47 48 49 4A 53 54 55 56 57 58 59 5A 63 64 65 66 67 68 69 6A 73 74 75 76 77 78 79 7A 82 83 84 85 86 87 88 89 8A 92 93 94 95 96 97 98 99 9A A2 A3 A4 A5 A6 A7 A8 A9 AA B2 B3 B4 B5 B6 B7 B8 B9 BA C2 C3 C4 C5 C6 C7 C8 C9 CA D2 D3 D4 D5 D6 D7 D8 D9 DA E2 E3 E4 E5 E6 E7 E8 E9 EA F2 F3 F4 F5 F6 F7 F8 F9 FA FF DB 00 43 00 04 04 04 04 04 04 06 04 04 06 09 06 06 06 09 0C 09 09 09 09 0C 0F 0C 0C 0C 0C 0C 0F 12 0F 0F 0F 0F 0F 0F 12 12 12 12 12 12 12 12 15 15 15 15 15 15 19 19 19 19 19 1C 1C 1C 1C 1C 1C 1C 1C 1C 1C FF DB 00 43 01 04 05 05 07 07 07 0C 07 07 0C 1D 14 10 14 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D FF DD 00 04 00 BD```And the header of a non-leaky image:```FF D8 FF E1 00 16 45 78 69 66 00 00 4D 4D 00 2A 00 00 00 08 00 00 00 00 00 00 FF E2 02 34 49 43 43 5F 50 52 4F 46 49 4C 45 00 01 01 00 00 02 24 61 70 70 6C 04 00 00 00 6D 6E 74 72 52 47 42 20 58 59 5A 20 07 E1 00 07 00 07 00 0D 00 16 00 20 61 63 73 70 41 50 50 4C 00 00 00 00 41 50 50 4C 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 F6 D6 00 01 00 00 00 00 D3 2D 61 70 70 6C CA 1A 95 82 25 7F 10 4D 38 99 13 D5 D1 EA 15 82 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0A 64 65 73 63 00 00 00 FC 00 00 00 65 63 70 72 74 00 00 01 64 00 00 00 23 77 74 70 74 00 00 01 88 00 00 00 14 72 58 59 5A 00 00 01 9C 00 00 00 14 67 58 59 5A 00 00 01 B0 00 00 00 14 62 58 59 5A 00 00 01 C4 00 00 00 14 72 54 52 43 00 00 01 D8 00 00 00 20 63 68 61 64 00 00 01 F8 00 00 00 2C 62 54 52 43 00 00 01 D8 00 00 00 20 67 54 52 43 00 00 01 D8 00 00 00 20 64 65 73 63 00 00 00 00 00 00 00 0B 44 69 73 70 6C 61 79 20 50 33 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 74 65 78 74 00 00 00 00 43 6F 70 79 72 69 67 68 74 20 41 70 70 6C 65 20 49 6E 63 2E 2C 20 32 30 31 37 00 00 58 59 5A 20 00 00 00 00 00 00 F3 51 00 01 00 00 00 01 16 CC 58 59 5A 20 00 00 00 00 00 00 83 DF 00 00 3D BF FF FF FF BB 58 59 5A 20 00 00 00 00 00 00 4A BF 00 00 B1 37 00 00 0A B9 58 59 5A 20 00 00 00 00 00 00 28 38 00 00 11 0B 00 00 C8 B9 70 61 72 61 00 00 00 00 00 03 00 00 00 02 66 66 00 00 F2 A7 00 00 0D 59 00 00 13 D0 00 00 0A 5B 73 66 33 32 00 00 00 00 00 01 0C 42 00 00 05 DE FF FF F3 26 00 00 07 93 00 00 FD 90 FF FF FB A2 FF FF FD A3 00 00 03 DC 00 00 C0 6E FF DB 00 84 00 09 06 07 10 10 10 1F 10 10 10 15 1A 20 20 10 0F 17 10 10 10 1E 28 20 16 1A 20 0D 28 1E 20 28 28 28 1A 28 2F 31 20 30 20 29 31 0E 1E 25 38 25 26 2E 40 41 2B 2C 28 25 2C 2E 3C 30 34 29 27 41 21 01 0A 0A 0A 0F 10 0D 0D 0D 15 11 2B 0D 15 15 29 2D 31 3B 2A 36 31 2D 33 29 33 34 25 37 3A 2D 35 32 33 2A 25 21 2B 29 31 30 36 2C 33 36 32 2F 36 37 2C 31 26 33 2C 33 25 2C 31 31 2E 26 2F 30 24 39 FF C0 00 11 08 03 6C 03 6C 03 01 22 00 02 11 01 03 11 01 FF C4 00 1C 00 00 03 01 01 01 01 01 01 00 00 00 00 00 00 00 00 00 01 02 03 04 05 06 07 08 FF C4 00 3C 10 00 02 02 01 03 02 04 05 02 05 04 01 04 02 03 01 00 01 02 11 03 04 21 31 12 41 05 51 61 81 13 22 71 91 A1 32 B1 06 14 42 C1 F0 52 D1 E1 F1 62 72 82 92 C2 23 33 15 43 A2 16 FF C4 00 18 01 01 01 01 01 01 00 00 00 00 00 00 00 00 00 00 00 00 01 02 03 04 FF C4 00 1D 11 01 01 01 01 00 03 01 01 01 00 00 00 00 00 00 00 00 01 11 02 12 21 31 41 03 61```Seems like it could have something to do with Exif data?=====""; '@Nuzzlet @sudo-carson @MahdiNicoo ; please help provide test images so that we can reproduce or a codepen/glitch example with images. Thank you ====='; ""I have a leaky image here.`tf.memory()` doesn't report any increase in size; but memory use still goes up fast.I'm using the CPU backend.![tf-leak](https://user-images.githubusercontent.com/3341858/146651768-225e4dc6-21eb-47ab-bbc0-a10cab911398.jpg)=====""]",Memory Leak,Poor Performance,Unknown,TF(GPU),Backend,,,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,E
https://github.com/tensorflow/tfjs/issues/4347,WebGL backend should include an environment variable to toggle failIfMajorPerformanceCaveat value.,5,open,2020-12-04T03:12:57Z,2021-02-16T21:15:12Z,"I've run into an interesting situation.Recently; when Firefox upgraded from 82 to 83; performance on [my addon](https://github.com/wingman-jr-addon/wingman_jr) dropped sharply for myself and some other users as well. I traced the issue to the WebGL backend failing to load due to `failIfMajorPerformanceCaveat` exhibiting a change in behavior between the two versions. As this was negatively affecting my users; I quickly added in the WASM backend as an option. However; while this was [faster for some users](https://github.com/wingman-jr-addon/wingman_jr/issues/75#issuecomment-734822117) - thank you TF.js team for your awesome work! - for others like myself it seems to still be a blocking issue.When I [did a bisection and filed a bug with Mozilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1679671); their initial response is that it is likely that `failIfMajorPerformanceCaveat` had a bug in FF 82 but that has now been fixed; and that the context creation *should* be failing by the spec.So this puts me in an interesting spot. With the evidence I have so far; I believe there are a significant number of users that would still benefit from the WebGL backend - even with ""major performance caveats"" - over the WASM backend. Unfortunately; it appears that the `failIfMajorPerformanceCaveat` is [hardcoded](https://github.com/tensorflow/tfjs/blob/c537c813ba8234a909d1e0849abfd95998a8fb5b/tfjs-backend-webgl/src/canvas_util.ts#L27) rather than relying on say an environment variable or something I can tweak to allow for a different user experience.So; I have a couple followup questions:1. Is the use of `failIfMajorPerformanceCaveat` solely about performance for the end user; or is there a correctness concern as well?2. If it is solely about performance; would the TF.js team consider making this controllable via; say; an environment variable?Maybe I'm missing something about the overall situation; but I've been battling this for about a week and a half now and I'm curious your perspective on the matter.Thanks!","['@wingman-jr-addon I believe you can create your own webgl backend using your own gl context.similar to how we register webgl backend for wechat mini program ([link](https://github.com/tensorflow/tfjs-wechat/blob/master/src/plugin/utils/wechat_platform.ts#L127))You can set the failIfMajorPerformanceCaveat to false if you prefer.====='; '@pyu10055 Thanks for the tip - as per my first question; do you know if there are any correctness concerns if I do that?====='; ""Hi @wingman-jr-addon - I don't believe there would be correctness concerns with creating your own WebGL backend. Per the [Khronos documentation](http://www.khronos.org/registry/webgl/specs/latest/1.0/#5.2) of this attribute; it should only be relevant to performance.I think it would be reasonable to add a WebGL environment variable to toggle this value - thanks for the suggestion! I've renamed this issue to reflect.=====""; 'Thanks @annxingyuan - and that was also a good link to the spec; I learned a few things about some of the additional flags; too!====='; '@annxingyuan and @pyu10055 I think you may be interested to see the direction Firefox has taken now regarding this flag in the linked bugs [1684475](https://bugzilla.mozilla.org/show_bug.cgi?id=1684475) and [1678652](https://bugzilla.mozilla.org/show_bug.cgi?id=1678652). =====']",Browser Hangs,Poor Performance,Browser Incompatibility,Browser,Platform,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",B.1.2,D.2
https://github.com/tensorflow/tfjs/issues/4340,UpSampling2D(interpolation=bilinear) gives significant difference when converted using tensorflowjs_converter,1,open,2020-12-02T23:40:40Z,2021-06-03T16:16:30Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): close to stock example- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Ubuntu 18.04- TensorFlow.js version (use command below): ""@tensorflow/tfjs-node"": ""^2.1.0""- Tensorflow.js Converter Version: tensorflowjs 2.7.0**Describe the current behavior**1. Added a layer`UpSampling2D` with interpolation as `bilinear` (tensorflow python model).`model.add(tf.keras.layers.UpSampling2D(size=(2;2); interpolation='bilinear'))`2. Converted the model (to be used in js) using tensorflowjs_converter`tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --saved_model_tags=serve --signature_name=serving_default --skip_op_check --strip_debug_ops=True --weight_shard_size_bytes=4194304 model_path output_path`3. Now; the output in tensorflow js (converted model) is significantly different than tensorflow python.**Note:** The outputs are same when we use `interpolation=nearest` (js vs python)**Describe the expected behavior**There should not be significant difference when a model with a particular layer is converted from python to js.","[""I am having the same issue.I am using an `UpSampling2D` layer to generate images using a Generative Adversarial Network. In Python I set `tf.keras.layers.UpSampling2D(size=2; interpolation='bilinear')`; but the resulting generated image in TensorFlow.js looks as though it was switched to `interpolation='nearest'`.=====""]",Incorrect Functionality,Incorrect Functionality,Inconsistent Modules,Layer API,API,add support for option,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,A.2
https://github.com/tensorflow/tfjs/issues/4337,tf.stridedSlice is not a function,3,closed,2020-12-02T16:42:15Z,2020-12-02T17:48:01Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):NA- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.7.0- Browser version:Google Chrome is up to dateVersion 87.0.4280.66 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Uncaught (in promise) TypeError: points.stridedSlice is not a function**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Earlier I was working with 2.4.0 version and it was working with that. Update to 2.7.0 has caused the this. ====='; '@hkhoont Can you please provide reproduction code and model if possible.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">No</a>=====']",Reference Error,Crash,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,E
https://github.com/tensorflow/tfjs/issues/4337,tf.stridedSlice is not a function,3,closed,2020-12-02T16:42:15Z,2020-12-02T17:48:01Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):NA- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.7.0- Browser version:Google Chrome is up to dateVersion 87.0.4280.66 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Uncaught (in promise) TypeError: points.stridedSlice is not a function**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Earlier I was working with 2.4.0 version and it was working with that. Update to 2.7.0 has caused the this. ====='; '@hkhoont Can you please provide reproduction code and model if possible.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">No</a>=====']",Reference Error,Crash,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/4337,tf.stridedSlice is not a function,3,closed,2020-12-02T16:42:15Z,2020-12-02T17:48:01Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):NA- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.7.0- Browser version:Google Chrome is up to dateVersion 87.0.4280.66 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Uncaught (in promise) TypeError: points.stridedSlice is not a function**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Earlier I was working with 2.4.0 version and it was working with that. Update to 2.7.0 has caused the this. ====='; '@hkhoont Can you please provide reproduction code and model if possible.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">No</a>=====']",Regression,Poor Performance,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,E
https://github.com/tensorflow/tfjs/issues/4337,tf.stridedSlice is not a function,3,closed,2020-12-02T16:42:15Z,2020-12-02T17:48:01Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):NA- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version (use command below):2.7.0- Browser version:Google Chrome is up to dateVersion 87.0.4280.66 (Official Build) (64-bit)- Tensorflow.js Converter Version:**Describe the current behavior**Uncaught (in promise) TypeError: points.stridedSlice is not a function**Describe the expected behavior****Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Earlier I was working with 2.4.0 version and it was working with that. Update to 2.7.0 has caused the this. ====='; '@hkhoont Can you please provide reproduction code and model if possible.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4337"">No</a>=====']",Regression,Poor Performance,Unknown,Operator,API,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",B.3.1,E
https://github.com/tensorflow/tfjs/issues/4327,[Codelab]: Making Predictions from 2D Data,1,closed,2020-12-01T14:42:23Z,2020-12-03T17:35:55Z,"Found inconsistencyhttps://www.tensorflow.org/js/tutorials/setup - script uses tfjs@2.0.0https://codelabs.developers.google.com/codelabs/tfjs-training-regression/index.html#1 - script uses tfjs@1.0.0Also; I see ""55 mins remaining"" on top. Can I not be rushed? :(",['It is fixed now; thanks for reporting.====='],Document Error,Document Error,Confused Document,Operator,API,change document,change document,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",E,B.4
https://github.com/tensorflow/tfjs/issues/4325,missing ops in kernel cause misleading error message,10,closed,2020-12-01T12:20:13Z,2021-01-20T13:11:13Z,If an op is missing in kernel; error messages printed are not helpful at all.I have a simple home-made `tf.sequential` model that works with `webgl` and `cpu` backends; but fails with `wasm` backend:```logbackend.ts:665 Uncaught (in promise) Error: 'step' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.ts:665)    at BackendWasm.step (backend.ts:418)    at step.ts:48    at engine.ts:625    at engine.ts:433    at Engine.scopedRun (engine.ts:444)    at Engine.tidy (engine.ts:431)    at kernelFunc3 (engine.ts:625)    at engine.ts:639    at Engine.scopedRun (engine.ts:444)```Finally I've traced it to `recurrentActivation` set to `relu` inside a layer instead of default `hardSigmoid` - you'd never guess that based on the message!Also; I understand that some ops are not implemented in all kernels; but is there a full matrix of ops per kernel?Environment: TFJS 2.7.0 in Chrome 87,"['cc @pyu10055 @lina128 ====='; 'linking to #4500====='; 'update: tested with **tfjs 2.8.3**; error message has changed but result is still a failure:```engine.js:487 Uncaught (in promise) Error: Error running Step: Neither modular kernel nor forward func passed  runKernelFunc\t@\tengine.js:487  runKernel\t@\tengine.js:391  step_\t@\tstep.ts:47  step__op\t@\toperation.ts:51  x\t@\tRelu_grad.ts:29  (anonymous)\t@\ttape.js:128  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  (anonymous)\t@\tengine.js:843  backpropagateGradients\t@\ttape.js:128  (anonymous)\t@\tengine.js:841  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  gradients\t@\tengine.js:837  variableGrads\t@\tgradients.js:252  computeGradients\t@\toptimizer.js:82  minimize\t@\toptimizer.js:38  (anonymous)\t@\ttraining.js:1118  (anonymous)\t@\ttraining_tensors.js:197  (anonymous)\t@\tengine.js:323  scopedRun\t@\tengine.js:333  tidy\t@\tengine.js:322  tidy\t@\tglobals.js:175  fitLoop\t@\ttraining_tensors.js:188```@pyu10055 this looks related to your work in #4511 ?====='; '@vladmandic Step op was added to WASM in the latest (2.8.3); can you give it another try; see the error message makes more sense now? thanks====='; ""@pyu10055 i've noticed it was added and i've tried; but i can't make sense of the new error message:```engine.js:487 Uncaught (in promise) Error: Error running Step: Neither modular kernel nor forward func passed```note that same code works just fine with webgl backend.=====""; ""Hi @vladmandic; the missing op is implemented and merged into our codebase; but hasn't been released yet. I will be in 2.8.4; likely happen later this week.=====""; ""@lina128 thanks - i thought it was released in tfjs 2.8.3; my bad. i'll wait for 2.8.4=====""; 'No problem; @vladmandic. You can track what is released using our release notes: https://github.com/tensorflow/tfjs/releases/tag/tfjs-v2.8.3====='; 'closing as fixed in tfjs 2.8.5====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4325"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4325"">No</a>=====']",Reference Error,Crash,Unimplemented Operator,Wasm,Backend,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.1
https://github.com/tensorflow/tfjs/issues/4324,Inconsistent creation of tensor6d with nested Uint8Arrays,2,open,2020-12-01T01:45:29Z,2021-09-10T17:24:14Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):n/a- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows 10; Version 1909 (OS Build 18363.1198)- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:n/a- TensorFlow.js installed from (npm or script link):npm- TensorFlow.js version:2.7.0- Browser version:n/a- Tensorflow.js Converter Version:unknown**Describe the current behavior**TLDR: It appears the type definitions for tensor1-5d and tensor6d have inconsistent nested Uint8Array bracket counts. Please see the source at the bottom of this bug.Full Story:When creating tensors from nested Uint8Arrays; the results are not consistent across every supported rank (1-6). Specifically; rank 6 does not act like ranks 1-5._Regarding depth; I will consider Uint8Array[] as one-dimensional in this write-up; as I would with boolean[] or number[]. It's confusing with typed arrays._- If you pass a 6d array of Uint8Array into tensor6d(); an error is thrown.- If you pass the same 6d array into the generic tensor(); a rank 7 tensor is returned.- If you pass a 5d array into tensor6d(); a rank 6 tensor is returned.These cases may be perfectly valid (although they seem inconsistent). However; the behavior of the third case changes when applied to lower ranks.- If you attempt to pass a 4d array into tensor5d(); a TypeScript error is produced; showing that 'Uint8Array[][][][] not assignable to TensorLike5D'.It appears that one of these problems exists:- The type for 6d Uint8Array is missing a pair of brackets.OR- The types for 1-5d Uint8Array each have an extra pair of brackets._NOTE: I'd be happy to fix this one; but I'm not certain which behavior is preferred. Should tensor5() accept 4d or 5d arrays of Uint8Array? Should the generic tensor() ever produce a rank 7; or should that throw?_**Describe the expected behavior**The generic call tensor() and the specific calls tensor<1-6>d() should return properly ranked tensors given identical Uint8Array input; and there should be no TypeScript errors.**Standalone code to reproduce the issue**```import * as TF from '@tensorflow/tfjs-node';const UI8_4: Uint8Array[][][][]     = [[[[new Uint8Array(3)]]]];const UI8_5: Uint8Array[][][][][]   = [[[[[new Uint8Array(3)]]]]];const UI8_6: Uint8Array[][][][][][] = [[[[[[new Uint8Array(3)]]]]]];try {	// using the generic tensor() call	const TENSOR_6_MADE_FROM_6D_AUTO = TF.tensor(UI8_6); // rank 7; shape [1; 1; 1; 1; 1; 1; 3]	const TENSOR_6_MADE_FROM_5D_AUTO = TF.tensor(UI8_5); // rank 6; shape [1; 1; 1; 1; 1; 3]	// sending n-1 dimensions to the tensor<n>d() calls	const TENSOR_6_MADE_FROM_5D = TF.tensor6d(UI8_5); // rank 6; shape: [1; 1; 1; 1; 1; 3]	const TENSOR_5_MADE_FROM_4D = TF.tensor5d(UI8_4); // rank 5; shape: [1; 1; 1; 1; 3]; TypeScript: ""Argument of type 'Uint8Array[][][][]' is not assignable to parameter of type 'TensorLike5D'. ts(2345)""	// sending n dimensions to the tensor<n>d() calls	const TENSOR_6_MADE_FROM_6D = TF.tensor6d(UI8_6); // crash: ""Error: tensor6d() requires values to be number[][][][][][] or flat/TypedArray""	const TENSOR_5_MADE_FROM_5D = TF.tensor5d(UI8_5); // crash: ""Error: tensor5d() requires values to be number[][][][][] or flat/TypedArray""}catch (e) {	console.log(e);}```**Other info / logs**This is the source in question (*.\tfjs-core\src\types.ts; lines 136-159*):```/** @docalias TypedArray|Array */export type TensorLike =    TypedArray|number|boolean|string|RecursiveArray<number|number[]|TypedArray>|    RecursiveArray<boolean>|RecursiveArray<string>|Uint8Array[];export type ScalarLike = number|boolean|string|Uint8Array;/** @docalias TypedArray|Array */export type TensorLike1D = TypedArray|number[]|boolean[]|string[]|Uint8Array[];/** @docalias TypedArray|Array */export type TensorLike2D = TypedArray|number[]|number[][]|boolean[]|boolean[][]|    string[]|string[][]|Uint8Array[]|Uint8Array[][];/** @docalias TypedArray|Array */export type TensorLike3D = TypedArray|number[]|number[][][]|boolean[]|    boolean[][][]|string[]|string[][][]|Uint8Array[]|Uint8Array[][][];/** @docalias TypedArray|Array */export type TensorLike4D = TypedArray|number[]|number[][][][]|boolean[]|    boolean[][][][]|string[]|string[][][][]|Uint8Array[]|Uint8Array[][][][];/** @docalias TypedArray|Array */export type TensorLike5D =    TypedArray|number[]|number[][][][][]|boolean[]|boolean[][][][][]|string[]|    string[][][][][]|Uint8Array[]|Uint8Array[][][][][];/** @docalias TypedArray|Array */export type TensorLike6D =    TypedArray|number[]|number[][][][][][]|boolean[]|boolean[][][][][][]|    string[]|string[][][][][][]|Uint8Array[]|Uint8Array[][][][][];```","[""I think the inconsistencies start from `ScalarLike` types. `ScalarLike` includes Uint8Array as one of its type definitions. But should Uint8Array be part of a scalar? Because Uint8Array is a collection of int data types; it doesn't fit the scalar mathematical definition.=====""; ""I'm not sure whether arrays belong in ScalarLike; but I don't believe that affects this issue. It's purely the implementation of 1-5 and 6 being done differently; regardless of type.To me; it's pretty clearly a typo. Somebody just left out the last set of brackets on rank 6.________________________________From: Raffi Zulvian Muzhaffar ***@***.***>Sent: Thursday; September 9; 2021 03:28To: tensorflow/tfjs ***@***.***>Cc: joemullenix-ks ***@***.***>; Author ***@***.***>Subject: Re: [tensorflow/tfjs] Inconsistent creation of tensor6d with nested Uint8Arrays (#4324)I think the inconsistencies start from ScalarLike types. ScalarLike includes Uint8Array as one of its type definitions. But should Uint8Array be part of a scalar? Because Uint8Array is a collection of int data types; it doesn't fit the scalar mathematical definition.—You are receiving this because you authored the thread.Reply to this email directly; view it on GitHub<https://github.com/tensorflow/tfjs/issues/4324#issuecomment-916091747>; or unsubscribe<https://github.com/notifications/unsubscribe-auth/AFDBCENNHZXGPBUSMI523WLUBCY7ZANCNFSM4UILGAPA>.Triage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Operator,API,variable replacer,variable replacer,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4313,tf.tensor([ 17378479 ]).print();returns 17378480,2,open,2020-11-27T01:19:45Z,2021-11-05T19:00:11Z,Thanks for all tensorflow jobs!I'm trying to modify image with using tensorflow.js.and i found that some integer will change via tensorflow object.But I know that it is not critical for machine learning.So; I want to know ...Is it a important bug or inevitable spec for tensorflow.js?**System information**- Have I written custom code    e.g.)   tf.tensor([ 17378479 ]).print(); // => 17378480  tf.tensor1d([ 17378479 ]; 'int32').print() // => 17378479  tf.tensor1d([ 17378479 ]; 'float32').print() // => 17378480  tf.tensor1d([ 17378479 ]; 'int32').slice([0]; [1]).print() // => 17378479 (In some case; changes to 17378480)- OS Platform and Distribution:  macOS 11.0.1 (20B29)- Mobile device:  MacBookAir9;1- TensorFlow.js version:  v2.6.0 / v2.7.0- Browser version:  Chrome 87.0.4280.67（Official Build）beta （x86_64）  MSEdge 87.0.664.47 (Official Build) (x86_64)  Or another,"[""By default tensors will be created at float32 tensors; which will have some degree of numerical imprecision; so i'm not particularly surprised with your first three lines; however could you say more about the last line.`tf.tensor1d([ 17378479 ]; 'int32').slice([0]; [1]).print() // => 17378479 (In some case; changes to 17378480)`In particular you say in some cases it changes; Could you elaborate on when that happens? Does that happen when repeatedly running the same line over and over again in the same environment?=====""; ""@tafsiri Thanks for your reply.Not by a repeatedly running. Run the code below. You can see the value change via .slice( ) method.```jslet x = 127;tf.tensor1d(new Array(x).fill(0).concat(17378479); 'int32').slice(0; x + 1).dataSync()[x]; // => 17378480```And the case below; the matter is not occured.```jslet x = 127;tf.tensor1d(new Array(x).fill(0).concat(17378479); 'int32').dataSync()[x]; // => 17378479``````jslet x = 126;tf.tensor1d(new Array(x).fill(0).concat(17378479); 'int32').slice(0; x + 1).dataSync()[x]; // => 17378479```=====""]",Incorrect Functionality,Incorrect Functionality,Unknown,Operator,API,,,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/4311,big difference in inference results between webgl and wasm backends,3,closed,2020-11-26T15:11:35Z,2021-02-24T01:10:45Z,there is a **big** difference in inference results between `webgl` and `wasm` backends.and this makes `wasm` model completely unsuitable for usage with some models.i've noticed this with several models; but biggest difference is with MediaPipe HandPose model: <https://github.com/tensorflow/tfjs-models/tree/master/handpose>to be specific; in <https://github.com/tensorflow/tfjs-models/blob/master/handpose/src/hand.ts>:```jsbatchedPrediction = this.model.predict(normalizedInput) as tf.Tensor3D;...const prediction: tf.Tensor2D = batchedPrediction.squeeze();...const scores: tf.Tensor1D = tf.tidy(() => tf.sigmoid(tf.slice(prediction; [0; 0]; [-1; 1])).squeeze());```for high (>0.99) and low scores (<0.1); both `webgl` and `wasm` values are simmilar.but for medium scores values; `webgl` has a gradient - you can get scores of any value while `wasm` behaves almost like a binary function - it's either very high or very low confidence and nothing inbetween.*it's almost like a wrong cast in a `wasm` backend so details are lost.*for performance reasons; i'm using `tfjs-backend-wasm-threaded-simd.wasm` after enabling `WebAssembly SIMD support` in <chrome://flags>environment: tfjs 2.7.0 on chrome 87,"['@rthadur @annxingyuan @pyu10055 @lina128 sorry for the spam;  but i have 3 issues filled (#4525; #4326; #4311) for `wasm` backend (with full reproduction code)  that make `wasm` pretty much unusable.  and all likely have the same root cause; just symptoms are different.but none received any updates since they were filed (oldest was 2.5 months ago).====='; 'thanks for your patience ; cc more folks for better visibility @mattsoulanille @jinjingforever ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4311"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4311"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Dependency Error,Wasm,Backend,change dependency version,Modifying dependency configuration,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",D,B.2
https://github.com/tensorflow/tfjs/issues/4310,Error: The Node.js native addon module (tfjs_binding.node) can not be found,6,closed,2020-11-26T12:00:50Z,2020-12-02T17:39:58Z,"I have installed TFJS Node GPU ; it gets stucked at end ; i tried this mutlipe of times .![bug](https://user-images.githubusercontent.com/45932883/100348141-83912380-300c-11eb-9ff7-5335bb329149.PNG) i tried reinstalling ; rebuilding ; looked for similar issues but no replay contained solution.now I can 't compile with GPU backend; For some reason CPU backend works.following error occurs every time I try to compile with it with GPU with following import```jsconst tf=require(""@tensorflow/tfjs-node-gpu"")``````Node version 15.0.1``` ```C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\dist\index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +    ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\lib\napi-v6\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node build-addon-from-source' to rebuild the native addon module.If you have problem with building the addon module; please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (C:\Users\shive\Desktop\Test\nodeapp\node_modules\@tensorflow\tfjs-node-gpu\dist\index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1083:30)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1112:10)    at Module.load (node:internal/modules/cjs/loader:948:32)    at Function.Module._load (node:internal/modules/cjs/loader:789:14)    at Module.require (node:internal/modules/cjs/loader:972:19)    at require (node:internal/modules/cjs/helpers:88:18)    at Object.<anonymous> (C:\Users\shive\Desktop\Test\nodeapp\index.js:1:10)    at Module._compile (node:internal/modules/cjs/loader:1083:30)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1112:10)npm ERR! code 1npm ERR! path C:\Users\shive\Desktop\Test\nodeappnpm ERR! command failednpm ERR! command C:\Windows\system32\cmd.exe /d /s /c ""node index.js""npm ERR! A complete log of this run can be found in:npm ERR!     C:\Users\shive\AppData\Local\npm-cache\_logs\2020-11-26T11_46_53_847Z-debug.log```","['Can you please try delete node-modules folder and rebuild ?====='; 'I tried reinstalling nothing worked ;Tried cleaning modules nothing worked;Tried creating new node project nothing worked;Tried Rebuilding with `npm rebuild @tensorflow/tfjs-node-gpu --build-from-source`Then i downgraded to Node 12.20 from v15 now post install script worked but still i am not able to compile to GPU```jstf=require(""@tensorflow/tfjs-node-gpu"")```Not able to import like this; it throws napi error;I will post error in couple of minutes.====='; 'Error```Error: The specified module could not be found.\\\\?\\C:\\Users\\shive\\Desktop\\Test\\nodeapp\\node_modules\\@tensorflow\\tfjs-node-gpu\\lib\\napi-v6\\tfjs_binding.node    at Object.Module._extensions..node (internal/modules/cjs/loader.js:1057:18)    at Module.load (internal/modules/cjs/loader.js:863:32)    at Function.Module._load (internal/modules/cjs/loader.js:708:14)    at Module.require (internal/modules/cjs/loader.js:887:19)    at require (internal/modules/cjs/helpers.js:74:18)    at Object.<anonymous> (C:\\Users\\shive\\Desktop\\Test\\nodeapp\\node_modules\\@tensorflow\\tfjs-node-gpu\\dist\\index.js:58:16)    at Module._compile (internal/modules/cjs/loader.js:999:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)    at Module.load (internal/modules/cjs/loader.js:863:32)    at Function.Module._load (internal/modules/cjs/loader.js:708:14)```====='; 'please take a look at similar issue [here](https://github.com/tensorflow/tfjs/issues/4171) ; thank you====='; 'It did .Thanks ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4310"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4310"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(GPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Native Addon Module Missing""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4309,Facemesh/Face-Landmarks-Detection model crashes on WebGL Backend,3,open,2020-11-26T09:45:39Z,2021-04-26T14:02:16Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):iOS 14.2.1- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:iPhone 12 Pro Max; iPhone 11 Pro Max- TensorFlow.js installed from (npm or script link):https://unpkg.com/@tensorflow/tfjs-core@2.7.0/dist/tf-core.jshttps://unpkg.com/@tensorflow/tfjs-converter@2.7.0/dist/tf-converter.jshttps://unpkg.com/@tensorflow/tfjs-backend-webgl@2.7.0/dist/tf-backend-webgl.jshttps://unpkg.com/@tensorflow-models/face-landmarks-detection@0.0.2/dist/face-landmarks-detection.js- TensorFlow.js version (use command below):- Browser version: Safari 14.0- Tensorflow.js Converter Version: 2.7.0**Describe the current behavior**Page refreshes after approx 15-18min of usage or 18;000 requestAnimationFrames. Perhaps this is a memory leak within the Face-landmarks-detection library?**Describe the expected behavior**Page does not crash when using WASM backend with Face-landmarks-detection.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.https://glitch.com/edit/#!/facemesh-bug**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,['Any update on this issue? I have since updated to version 2.8.4 with the same results. Is #4405 still relevant to this issue? Or perhaps #4407 ? ====='; '@rthadur could you please look again on that issue? We have problems with it too =( ====='; '@pyu10055 It will be great if you could check it too =)====='],Memory Leak,Poor Performance,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2.1] Memory Leak""
  }
}
```",B.2.1,E
https://github.com/tensorflow/tfjs/issues/4295,Customed Object Detection Model Crash When there is no target in video,2,open,2020-11-23T04:32:40Z,2020-12-04T09:42:25Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): True- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Mac - Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: - TensorFlow.js installed from (npm or script link): npm - TensorFlow.js version (use command below): 2.7.0- Browser version: Chrome 87.0.4280.67- Tensorflow.js Converter Version: 2.7.0**Describe the current behavior**works well when there is target object in video but crash when there is nothing.**Describe the expected behavior**works well for video with/without target.**Standalone code to reproduce the issue**first I trainned my model and convert it to tfjs format. [model.json.zip](https://github.com/tensorflow/tfjs/files/5581107/model.json.zip)Then I integrated it to tfjs-models demo code using tfconv.loadGraphModel and executeAsync with webGL backend.```// pseudo code to reproduce the problemboundingBoxDetector = tfconv.loadGraphModel(HANDGDETECTIONV2_MODEL_PATH);const detResult = await boundingBoxDetector.executeAsync(image) as tf.Tensor[];````I noticed the PR https://github.com/tensorflow/tfjs/pull/4122 but it does not fix my issue.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Error Stack.shader_compiler.ts:721 Uncaught (in promise) TypeError: Cannot read property '0' of undefined    at getPackedSampler2D (shader_compiler.ts:721)    at getPackedSamplerFromInInfo (shader_compiler.ts:118)    at getInputSamplingSnippet (shader_compiler.ts:131)    at shader_compiler.ts:56    at Array.map (<anonymous>)    at Object.makeShader (shader_compiler.ts:56)    at Object.compileProgram (gpgpu_math.ts:90)    at backend_webgl.ts:2384    at MathBackendWebGL.getAndSaveBinary (backend_webgl.ts:2428)    at MathBackendWebGL.runWebGLProgram (backend_webgl.ts:2383)Following is the gpu code I found that actually cause crash.""PadPackedProgram_0;4_undefined_false100;4_100;4_false_      const ivec2 start = ivec2(0;0);      const ivec2 end = ivec2(0;4);      void main() {        ivec2 outputLoc = getOutputCoords();        vec4 result = vec4(0.);                ivec2 rc = outputLoc;        if (any(lessThan(rc; start)) || any(greaterThanEqual(rc; end))) {          result[0] = float(0);        } else {          ivec2 source = rc - start;          result[0] = getChannel(getX(source.x;source.y); vec2(source.x;source.y));        }              rc.y += 1;       if(rc.y < 4) {              if (any(lessThan(rc; start)) || any(greaterThanEqual(rc; end))) {          result[1] = float(0);        } else {          ivec2 source = rc - start;          result[1] = getChannel(getX(source.x;source.y); vec2(source.x;source.y));        }              }       rc = outputLoc;       rc.x += 1;       if(rc.x < 100) {        if (any(lessThan(rc; start)) || any(greaterThanEqual(rc; end))) {          result[2] = float(0);        } else {          ivec2 source = rc - start;          result[2] = getChannel(getX(source.x;source.y); vec2(source.x;source.y));        }                rc.y += 1;         if(rc.y < 4) {        if (any(lessThan(rc; start)) || any(greaterThanEqual(rc; end))) {          result[3] = float(0);        } else {          ivec2 source = rc - start;          result[3] = getChannel(getX(source.x;source.y); vec2(source.x;source.y));        }      }}        setOutput(result);      }    """,['Hi @luoyuxuan0528 ; does the model run with cpu backend or wasm backend? Also; can you share the model with us?====='; '>  does the model run with cpu backend or wasm backend? Also; can you share the model with us?Hi; @lina128:   I did not test the model on cpu/wasm backend because that is not our target. The model is attached and input of that model should be a 224x224x3 Int Tensor.    Looking forward to your replay ^_^.[224x224_det_graph_ssd_small_v5.zip](https://github.com/tensorflow/tfjs/files/5626277/224x224_det_graph_ssd_small_v5.zip)====='],Reference Error,Crash,Unknown,WebGL,Backend,,,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,E
https://github.com/tensorflow/tfjs/issues/4290,Adam on  trainable = true/false net part,4,open,2020-11-20T23:17:20Z,2020-12-03T05:27:45Z,Hello.I'm a beginner trying to train a GAN network. Found an example https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3In python; the example works fine; but I need to run it under NodeJS.I rewrote it in JS - https://pastebin.com/RZt6AtZcAnd I get the error(node: 3988) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 3Message: Incompatible shapes: [784;1024] vs. [100;256]I began to understand; under the debugger I realized that the error occurs when the optimizer is running. I changed the optimizer to sgd and the code started executing without error.It looks like the problem is that trainable = false / true is changing for a part of the network.In @tensorflow \tfjs-core\src\optimizers\adam_optimizer.ts in line 67; only the index is used; but the problem is that the turkeys are the same and the layer names are different.,"['Hi @lexmulya; would you like to contribute a PR to fix this?====='; 'I have not dealt with this problem yet; there was no time. If I have a solution to this question I will do PR.====='; ""Got it. No problem; @lexmulya. We'll put this bug fix in our queue; but it will likely take some time. If you need it sooner; feel free to submit a fix PR any time.=====""; 'Got it. Thanks!=====']",Data & Model Error,Crash,Incorrect Code Logic,Operator,API,parameter modifier,Modify API Parameter usage,framework,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.4
https://github.com/tensorflow/tfjs/issues/4284,Inconsistent ”Could not get context for WebGL version 2” error message,8,open,2020-11-20T08:53:20Z,2021-11-02T09:46:55Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution: macOS 10.15.7 and iOS 14.1- Mobile device: iPhone 11- TensorFlow.js installed from (npm or script link): `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0`- TensorFlow.js version: `2.7.0`- Browser version:   - Safari 14.0 on iOS:    `Mozilla/5.0 (iPhone; CPU iPhone OS 14_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML; like Gecko) Version/14.0 Mobile/15E148 Safari/604.1`  - Safari 14.0.1 on macOS:    `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML; like Gecko) Version/14.0.1 Safari/605.1.15`- Tensorflow.js Converter Version: ?**Describe the current behavior**Steps to reproduce:1. Load https://js.tensorflow.org/api/latest/#add2. Click the first Run button3. Console log prints:```Could not get context for WebGL version 2Tensor    [11; 22; 33; 44]```4. Click Run again5. Console log prints:```Tensor    [11; 22; 33; 44]```**Describe the expected behavior**Expected the log messages in steps 3 and 5 to be identical; including the WebGL error message.**Standalone code to reproduce the issue**Reproducible test case: https://js.tensorflow.org/api/latest/#add**Other info / logs**The latest stable Safari 14 ships with WebGL 2.0 disabled by default. Make sure WebGL 2.0 is disabled via Safari > Experimental WebKit Features to be able to reproduce this issue.,['@annxingyuan Can you help to confirm? The WebGL 2 check happens on the first API call; once it is identified missing; we will be using WebGL 1 instead. The following API calls would not trigger the warning message. ====='; 'i am having same issue on chrome @pyu10055 ![image](https://user-images.githubusercontent.com/40135431/105950950-4f31f600-6095-11eb-864e-62e67fda0fe4.png)====='; 'Better documentation on the warning message; people understand that this is the case for safari.====='; 'Thank you for dealing with this issue; but is there a temporary solution for this? For instance; would assigning the backend to WebGL version 1 helps? If yes; could you tell us how to do it; please?====='; 'any updates on this issue ?====='; 'Hi Guys; Not sure of the root cause for this bug; but one of our client reported such issue on some Windows Machine with Chrome. The workaround was to change a flag in Chrome to not use D3D11 as a backend for ANGLE but to force OpenGL.As this issue only occurred on few computers it might be related to a combination of Drivers + CPU/GPU platforms. This has only be reported on our side for integrated GPU.====='; 'I am getting this error when trying to use tensorflow js in safari; with an ionic app.====='; '> I am getting this error when trying to use tensorflow js in safari; with an ionic app.Change to wasm backend====='],Browser & Device Error,Crash,Incorrect Code Logic,WebGPU,Backend,change backend,changing backend,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.4,A.4
https://github.com/tensorflow/tfjs/issues/4271,[tfjs-core] Error: The output # of rows (11.4) must be an integer when running pool test,5,closed,2020-11-19T12:42:07Z,2020-11-26T07:17:40Z,"**System information**- TensorFlow.js version (use command below):""@tensorflow-core"": ""^2.7.0"" **Describe the current behavior**When running [this pool test](https://brucedai.github.io/report-bug/unset_roundingMode_param.html); error happened **Describe the expected behavior**None error; and the output # of rows is an integer (11) computed with below Python code of [AVERAGE_POOL_2D test](https://android.googlesource.com/platform/frameworks/ml/+/refs/tags/android-cts-11.0_r1/nn/runtime/test/specs/V1_0/avg_pool_float_2.mod.py) from NNAPI  CTS.```pythonrow = 52std = 5flt = 100pad = 50output_row = (row + 2 * pad - flt + std) // std #L35```**Standalone code to reproduce the issue**Here's a [test case](https://brucedai.github.io/report-bug/unset_roundingMode_param.html).**Other info / logs** Include any logs or source code that would be helpful toWhen calling function `conditionalRound`; none value (undefined) was passed to the second **roundingMode** paramter; so as the first **value** paramter being not an integer (11.4); the value wasn't rounded. ```ts/** * Rounds a value depending on the rounding mode * @param value * @param roundingMode */function conditionalRound(    value: number; roundingMode?: 'floor'|'round'|'ceil') {  if (!roundingMode) {    return value;  }  switch (roundingMode) {    case 'round':      // used for Caffe Conv      return Math.round(value);    case 'ceil':      // used for Caffe Pool      return Math.ceil(value);    case 'floor':      return Math.floor(value);    default:      throw new Error(`Unknown roundingMode ${roundingMode}`);  }}```Traced that when calling function **conv_util.computePool2DInfo** of [tfjs-core/src/ops/pool.ts#L85](https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/ops/pool.ts#L85); none value (undefined) was passed to the sixth **roundingMode** parameter .```ts  const convInfo = conv_util.computePool2DInfo(      x4D.shape; windowShape; strides; dilations; pad);```According to [**computePool2DInfo** definitions](https://github.com/tensorflow/tfjs/blob/4d81b29a13cb67a0fae67833890cac326ca4d720/tfjs-core/src/ops/conv_util.ts#L119):```tsexport function computePool2DInfo(    inShape: [number; number; number; number];    filterSize: [number; number]|number; strides: number|[number; number];    dilations: number|[number; number]; pad: 'same'|'valid'|number;    roundingMode?: 'floor'|'round'|'ceil';    dataFormat: 'channelsFirst'|'channelsLast' = 'channelsLast')```","['might be related issue [here](https://github.com/tensorflow/tfjs/issues/1055) ====='; 'Hi @BruceDai; thank you for the bug report! Do you want to contribute a fix PR? ====='; ""@lina128 OK; I'll submit a PR for it.=====""; '@lina128 Please take a look this PR #4282; thanks.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4271"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4271"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,Operator,API,parameter modifier,Modify API Parameter usage,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.5] Training Argument Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4270,"Unable to resolve ""@react-native-community/async-storage"" from ""node_modules\@tensorflow\tfjs-react-native\dist\async_storage_io.js""",8,open,2020-11-19T09:33:23Z,2021-06-16T09:06:43Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device : Honor7x- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 2.7.0Description:Importing fetch from @tensorflow/tfjs-react-native resulting in above issue.I am getting this issue when I import only. I did follow the exact steps as in docs.Please see the screenshot below:![WhatsApp Image 2020-11-19 at 2 57 16 PM](https://user-images.githubusercontent.com/49977739/99647365-af922f00-2a77-11eb-9683-8cebf1c2b4cd.jpeg)Package  Dependencies : ```javascript ""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.13.2"";    ""@tensorflow-models/mobilenet"": ""^2.0.4"";    ""@tensorflow/tfjs"": ""^2.7.0"";    ""@tensorflow/tfjs-react-native"": ""^0.5.0"";    ""expo"": ""~39.0.2"";    ""expo-camera"": ""^9.0.0"";    ""expo-constants"": ""^9.2.0"";    ""expo-gl"": ""^9.1.1"";    ""expo-gl-cpp"": ""^9.1.2"";    ""expo-image-picker"": ""^9.1.1"";    ""expo-permissions"": ""^9.3.0"";    ""expo-status-bar"": ""~1.0.2"";    ""jpeg-js"": ""^0.4.2"";    ""react"": ""16.13.1"";    ""react-dom"": ""16.13.1"";    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-39.0.4.tar.gz"";    ""react-native-fs"": ""^2.16.6"";    ""react-native-web"": ""~0.13.12""  };``` Please help me how to resolve this issue.","['Your dependencies have ""@react-native-async-storage/async-storage"": ""^1.13.2"". But you need to have ""@react-native-community/async-storage"" installed. It looks like the async-storage packaged moved to a new org recently so we would need to update our dependency in a future release; but at the moment you should be able to load it with the older package name. Let me know if you run into problems with that.I\'ll leave this open to track updating the dependency on our end.====='; ""I'm sorry ... but I need the current package(https://github.com/react-native-async-storage/async-storage); and installing both creates conflict. Can it be updated for the package considered indicated?=====""; 'Is there any update regarding this? It is difficult to start a new project because of how far back in versioning for several libraries you have to go back to support this particular piece.====='; 'Since Expo SDK 41 dropped the support of old package name; we cannot upgrade to SDK 41 as long as we use tfjs on an expo project.====='; ""@tafsiri This issue has been open for over 6 months. Would love it if you can make this your priority. Can't use BundleResourceIO and load models at all because this dependency hasn't been updated yet=====""; ""Same issue here; app doesn't even build. =====""; '> Same issue here; app doesn\'t even build.If you want to build the app. Go to the file the error takes you and replace the dependency manually to the `""@react-native-async-storage/async-storage""`. I\'m not sure if the async storage functionalities still work though. I worked around them. ====='; '> > Same issue here; app doesn\'t even build.> > If you want to build the app. Go to the file the error takes you and replace the dependency manually to the `""@react-native-async-storage/async-storage""`. I\'m not sure if the async storage functionalities still work though. I worked around them.Yes that works; but this issue is already open for 6+ months. I think monkey patching is not the solution.=====']",Initialization Faliure,Build & Initialization Failure,Dependency Error,Mobile,Platform,change dependency version,Modifying dependency configuration,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4257,tensorflow.dll is missing from tfjs-node and I can't build it following the instructions,5,closed,2020-11-18T15:18:40Z,2020-11-23T19:42:00Z,"I've tried just about every potential fix I could find on mitigating this issue from this very own GitHub Repo and other sources. I'm going to apologize upfront; because I plan on detailing all the results here.OS: Windows 10 Enterprise (OS Build 19042.572)CPU: Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz   2.90 GHzNode: v14.15.1npm: v6.14.5python: v2.7node-gyp: @7.1.2Tensorflow:   @tensorflow/tfjs-node"": ""^2.7.0""The origin of the issue is as follows: - Removed the entirety of node_modules from the project- Deleted package-lock.json- `npm install`- Creating a main.ts which has the following:```import * as tf from '@tensorflow/tfjs-node';async function get_model() {  const model = tf.sequential();  model.add(tf.layers.dense({ inputShape: [7]; units: 124; activation: 'relu'; kernelInitializer: 'leCunNormal' }));  model.add(tf.layers.dense({ units: 64; activation: 'relu' }));  model.add(tf.layers.dense({ units: 32; activation: 'relu' }));  model.add(tf.layers.dense({ units: 1; activation: ""sigmoid"" }));  model.summary();  return model;}get_model();```- tsc - running the main.ts; produces```Error: The specified module could not be found.\\?\c:\workspace\my_project\node_modules\@tensorflow\tfjs-node\lib\napi-v6\tfjs_binding.node```When I peek into that directory ""c:\workspace\my_project\node_modules\@tensorflow\tfjs-node\lib\napi-v6\"" I notice the .dll that should be created is missing.At this point; I should note that this project **WAS WORKING** up until I did a recent windows update; node version update; and I changed the version of Tensorflow in package.json from '^1.3.2' to '^2.7.0'.Separately; I do have a working tfjs-examples/baseball-node project that I built before the windows update. The contents of its tfjs-node directory (C:\workspace\ml_examples\tfjs-examples\baseball-node\node_modules\@tensorflow\tfjs-node\lib\napi-v5) DOES have a tensorflow.dll.  Its package.json has:  ""@tensorflow/tfjs-node"": ""^1.3.2""The difference between napi-v5 and napi-v6 hasn't escaped me; but I'm a novice when it comes to compiling these c++ libraries to js.Now; back to my project.I then tried doing the suggested instructions from other issues.First; I navigate into ""c:\workspace\my_project\node_modules\@tensorflow\tfjs-node"" and run ```npm install --build-from-source```It runs but no .dll is created.  Not sure if the following means anything```> core-js@3.7.0 postinstall C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\node_modules\core-js> node -e ""try{require('./postinstall')}catch(e){}""```Next; from the same ""c:\workspace\my_project\node_modules\@tensorflow\tfjs-node"" dir; I try;```npm run build-addon-from-source  ```which produces```> @tensorflow/tfjs-node@2.7.0 build-addon-from-source C:\workspace\my_project\node_modules\@tensorflow\tfjs-node> node-pre-gyp install --build-from-sourcenode-pre-gyp WARN Using needle for node-pre-gyp https download Building the projects in this solution one at a time. To enable parallel build; please add the ""/m"" switch.  tfjs_backend.cc  tfjs_binding.cc  win_delay_load_hook.cc     Creating library C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\build\Release\tfjs_binding.lib and object C:\workspace\my_project\nod  e_modules\@tensorflow\tfjs-node\build\Release\tfjs_binding.exp  tfjs_binding.vcxproj -> C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\build\Release\\tfjs_binding.node  tfjs_binding.vcxproj -> C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\build\Release\tfjs_binding.pdb (Full PDB)  Copying C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\build\Release\/tfjs_binding.node to C:/workspace/my_project/node_modules/@tensorf  low/tfjs-node/lib/napi-v6\tfjs_binding.node          1 file(s) copied.```Yet; no .dll is created. And same issue as above happens when I try to run:```Error: The specified module could not be found.\\?\c:\workspace\my_project\node_modules\@tensorflow\tfjs-node\lib\napi-v6\tfjs_binding.node```Per the WINDOWS_TROUBLESHOOTING.md readme; I've run```node-gyp configure --verbose```Which does appear to have an issue and thus me creating this issue.```gyp info it worked if it ends with okgyp verb cli [gyp verb cli   'C:\\Program Files\\nodejs\\node.exe';gyp verb cli   'C:\\Users\\me\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js';gyp verb cli   'configure';gyp verb cli   '--verbose'gyp verb cli ]gyp info using node-gyp@7.1.2gyp info using node@14.15.1 | win32 | x64gyp verb command configure []gyp verb find Python Python is not set from command line or npm configurationgyp verb find Python checking Python explicitly set from environment variable PYTHONgyp verb find Python - process.env.PYTHON is ""C:\Python27\python.exe""gyp verb find Python - executing ""C:\Python27\python.exe"" to get executable pathgyp verb find Python - ""C:\Python27\python.exe"" is not in PATH or produced an errorgyp verb find Python checking if ""python3"" can be usedgyp verb find Python - executing ""python3"" to get executable pathgyp verb find Python - ""python3"" is not in PATH or produced an errorgyp verb find Python checking if ""python"" can be usedgyp verb find Python - executing ""python"" to get executable pathgyp verb find Python - executable path is ""C:\Program Files (x86)\Python27\python.exe""gyp verb find Python - executing ""C:\Program Files (x86)\Python27\python.exe"" to get versiongyp verb find Python - version is ""2.7.0""gyp info find Python using Python version 2.7.0 found at ""C:\Program Files (x86)\Python27\python.exe""gyp verb get node dir no --target version specified; falling back to host node version: 14.15.1gyp verb command install [ '14.15.1' ]gyp verb install input version string ""14.15.1""gyp verb install installing version: 14.15.1gyp verb install --ensure was passed; so won't reinstall if already installedgyp verb install version is already installed; need to check ""installVersion""gyp verb got ""installVersion"" 9gyp verb needs ""installVersion"" 9gyp verb install version is goodgyp verb get node dir target node version installed: 14.15.1gyp verb build dir attempting to create ""build"" dir: C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\buildgyp verb build dir ""build"" dir needed to be created? C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\buildgyp verb find VS msvs_version not set from command line or npm configgyp verb find VS VCINSTALLDIR not set; not running in VS Command Promptgyp verb find VS checking VS2017 (15.9.28307.1300) found at:gyp verb find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""gyp verb find VS - found ""Visual Studio C++ core features""gyp verb find VS - found VC++ toolset: v141gyp verb find VS - found Windows SDK: 10.0.17763.0gyp info find VS using VS2017 (15.9.28307.1300) found at:gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""gyp info find VS run with --verbose for detailed informationgyp verb build/config.gypi creating config filegyp verb build/config.gypi writing out config file: C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\build\config.gypigyp verb config.gypi checking for gypi file: C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\config.gypigyp verb common.gypi checking for gypi file: C:\workspace\my_project\node_modules\@tensorflow\tfjs-node\common.gypigyp verb gyp gyp format was not specified; forcing ""msvs""gyp info spawn C:\Program Files (x86)\Python27\python.exegyp info spawn args [gyp info spawn args   'C:\\Users\\me\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\gyp\\gyp_main.py';gyp info spawn args   'binding.gyp';gyp info spawn args   '-f';gyp info spawn args   'msvs';gyp info spawn args   '-I';gyp info spawn args   'C:\\workspace\\my_project\\node_modules\\@tensorflow\\tfjs-node\\build\\config.gypi';gyp info spawn args   '-I';gyp info spawn args   'C:\\Users\\me\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\addon.gypi';gyp info spawn args   '-I';gyp info spawn args   'C:\\Users\\me\\AppData\\Local\\node-gyp\\Cache\\14.15.1\\include\\node\\common.gypi';gyp info spawn args   '-Dlibrary=shared_library';gyp info spawn args   '-Dvisibility=default';gyp info spawn args   '-Dnode_root_dir=C:\\Users\\me\\AppData\\Local\\node-gyp\\Cache\\14.15.1';gyp info spawn args   '-Dnode_gyp_dir=C:\\Users\\me\\AppData\\Roaming\\npm\\node_modules\\node-gyp';gyp info spawn args   '-Dnode_lib_file=C:\\\\Users\\\\me\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\14.15.1\\\\<(target_arch)\\\\node.lib';  gyp info spawn args   '-Dmodule_root_dir=C:\\workspace\\my_project\\node_modules\\@tensorflow\\tfjs-node';gyp info spawn args   '-Dnode_engine=v8';gyp info spawn args   '--depth=.';gyp info spawn args   '--no-parallel';gyp info spawn args   '--generator-output';gyp info spawn args   'C:\\workspace\\my_project\\node_modules\\@tensorflow\\tfjs-node\\build';gyp info spawn args   '-Goutput_dir=.'gyp info spawn args ]gyp: Undefined variable module_name in binding.gyp while trying to load binding.gypgyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (C:\Users\me\AppData\Roaming\npm\node_modules\node-gyp\lib\configure.js:351:16)gyp ERR! stack     at ChildProcess.emit (events.js:315:20)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:277:12)gyp ERR! System Windows_NT 10.0.19042gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\me\\AppData\\Roaming\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--verbose""gyp ERR! cwd C:\workspace\my_project\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v14.15.1gyp ERR! node-gyp -v v7.1.2gyp ERR! not ok```All of the above have been run from Adminstrative Powershell so I don't suspect any permissions issues.In the interest of keeping this ""short"". I've also downloaded tfjs into its own project and followed all the steps above. which more or less produce the same issue.  No .dll is being created (I think is the real issue).","['So it appears that this previous issue #2619 has a potential workaround. the built .dll can be found in the deps/lib.  I moved into lib dir and it worked.`In the lib/napi-v4 folder; it should have tfjs_binding.node file and tensorflow.dll file. tensorflow.dll file is moved from deps/lib folder during installation. So it seems like the installation script failed to move it. Can you fork this repo; then go to tfjs-node folder and run npm install or yarn? Then paste the installation script and what is inside lib\\napi-v4 folder?`====='; '@quickdrawmcballs can you try to install the 2.7.0 again; I have uploaded precompiled bindings for napi-v6; seems with that being downloadable; the dll is copied properly.====='; '@pyu10055 I tried as you requested.- Removed the entirety of node_modules from the project- Deleted package-lock.json- npm installYet; I still don\'t see the .dll under the tfjs-node/lib/napi-v6/ dir.  I still have to move it from deps/libI\'ve tried with both:`""@tensorflow/tfjs-node"": ""^2.7.0""`and`""@tensorflow/tfjs-node"": ""2.7.0""`====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4257"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4257"">No</a>====='; ""@pyu10055; for what its worth; I'm on win64. And last I checked over the weekend; this is still broke.=====""]",Initialization Faliure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Missing Dependency""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/4256,Error: Argument 'x' passed to 'expandDims' must be a Tensor or TensorLike; but got 'Promise',2,closed,2020-11-18T14:40:19Z,2020-11-18T16:23:31Z,"**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10 Desktop - TensorFlow.js installed from (npm or script link): 2.3.0- TensorFlow.js version (use command below): 2.3.0- Browser version: Version 86.0.4240.198 (Official Build) (64-bit)- Tensorflow.js Converter Version: 2.3.0I am new to Tensor flow and coding as well. I have retrained EfficientDet-D0 model with a custom dataset in colab and now trying to use the saved model after conversion with tfjs-convertor in Angular appThe conversion script used was:```!tensorflowjs_converter --input_format=tf_saved_model --output_node_names='MobilenetV1/Predictions/Reshape_1' --saved_model_tags=serve ""./12112020_savedmodel/fine_tuned_model/saved_model"" ""./12112020_savedmodel/web_model""```After successful conversion and importing the model.json and shard files into into my angular app folder; i'm trying to run inference on a single custom input image(.jpg) after image is converted into a 4D tensor but shows error mentioned in the title. My code is as follows:```import { Component; OnInit; AfterViewInit; ViewChild; ElementRef } from '@angular/core';import * as tf from '@tensorflow/tfjs';import {loadGraphModel} from '@tensorflow/tfjs-converter';export interface DetectedObject {  bbox: [number; number; number; number];  class: string;  score: number;}@Component({  selector: 'app-home';  templateUrl: 'home.page.html';  styleUrls: ['home.page.scss'];})export class HomePage implements AfterViewInit{  @ViewChild('video') video: ElementRef;  @ViewChild('canvas') canvas: ElementRef;  @ViewChild('image') image: ElementRef;  private model;  constructor() {}  ngAfterViewInit() {    this.predictWithModel();   }async predictWithModel() {    // Load Model     tf.setBackend('webgl');    const modelURL = '../assets/web/model.json';    this.model = await loadGraphModel(modelURL);    console.log('model loaded');    // Prediction    const imagetensor = this.getTensorFromImage(this.image.nativeElement);    const result = await this.model.executeAsync(imagetensor);    console.log(result);  }     // Converting input image into 4D tensor  getTensorFromImage(imageSrc): tf.Tensor {    const tfimage = tf.browser.fromPixels(imageSrc);    const tfcast = tf.cast(tfimage; 'int32');    const expanded = tf.expandDims(tfcast; 0);    console.log('Image:'; expanded.size; 'bytes with shape:'; expanded.shape);    tf.dispose(tfimage);    return expanded;    }}```And we get the following in console:```model loadedhome-home-module.js:99286 Image: 750000 bytes with shape: Array(4)0: 1 1: 500 2: 500 3: 3 length: 4 __proto__: Array(0)vendor.js:39108 ERROR Error: Uncaught (in promise): Error: Argument 'x' passed to 'expandDims' must be a Tensor or TensorLike; but got 'Promise'Error: Argument 'x' passed to 'expandDims' must be a Tensor or TensorLike; but got 'Promise'    at convertToTensor (home-home-module.js:71910)    at expandDims_ (home-home-module.js:48521)    at Module.expandDims (home-home-module.js:54832)    at Module.executeOp (home-home-module.js:23938)    at home-home-module.js:27202    at home-home-module.js:30652    at Engine.scopedRun (home-home-module.js:30662)    at Engine.tidy (home-home-module.js:30651)    at Module.tidy (home-home-module.js:31822)    at home-home-module.js:27202    at resolvePromise (polyfills.js:3904)    at polyfills.js:3811    at rejected (vendor.js:112961)    at ZoneDelegate.invoke (polyfills.js:3470)    at Object.onInvoke (vendor.js:62348)    at ZoneDelegate.invoke (polyfills.js:3469)    at Zone.run (polyfills.js:3229)    at polyfills.js:3963    at ZoneDelegate.invokeTask (polyfills.js:3505)    at Object.onInvokeTask (vendor.js:62336)defaultErrorLogger @ vendor.js:39108```When i try to use 'predict' instead of 'executeAsync' as follows```const result = await this.model.predict(imagetensor);``` the following error occurs after model is loaded and tensor is generated :```model loadedhome.page.ts:78 Image: 750000 bytes with shape: (4) [1; 500; 500; 3]core.js:4197 ERROR Error: Uncaught (in promise): Error: This execution contains the node 'StatefulPartitionedCall/Preprocessor/ResizeToRange/cond/output/_707'; which has the dynamic op 'Merge'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [StatefulPartitionedCall/Preprocessor/ResizeToRange/Shape_2]Error: This execution contains the node 'StatefulPartitionedCall/Preprocessor/ResizeToRange/cond/output/_707'; which has the dynamic op 'Merge'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [StatefulPartitionedCall/Preprocessor/ResizeToRange/Shape_2]    at GraphExecutor.compile (graph_executor.js:121)    at GraphExecutor.execute (graph_executor.js:156)    at GraphModel.execute (graph_model.js:264)    at HomePage.<anonymous> (home.page.ts:65)    at Generator.next (<anonymous>)    at fulfilled (tslib.es6.js:71)    at ZoneDelegate.invoke (zone-evergreen.js:364)    at Object.onInvoke (core.js:27437)    at ZoneDelegate.invoke (zone-evergreen.js:363)    at Zone.run (zone-evergreen.js:123)    at resolvePromise (zone-evergreen.js:798)    at zone-evergreen.js:705    at fulfilled (tslib.es6.js:71)    at ZoneDelegate.invoke (zone-evergreen.js:364)    at Object.onInvoke (core.js:27437)    at ZoneDelegate.invoke (zone-evergreen.js:363)    at Zone.run (zone-evergreen.js:123)    at zone-evergreen.js:857    at ZoneDelegate.invokeTask (zone-evergreen.js:399)    at Object.onInvokeTask (core.js:27425)```Similarly for 'execute' instead of 'executeAsync' the following error occurs```model loadedhome.page.ts:78 Image: 750000 bytes with shape: (4) [1; 500; 500; 3]0: 11: 5002: 5003: 3length: 4__proto__: Array(0)core.js:4197 ERROR Error: Uncaught (in promise): Error: This execution contains the node 'StatefulPartitionedCall/Preprocessor/ResizeToRange/cond/output/_707'; which has the dynamic op 'Merge'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [StatefulPartitionedCall/Preprocessor/ResizeToRange/Shape_2]Error: This execution contains the node 'StatefulPartitionedCall/Preprocessor/ResizeToRange/cond/output/_707'; which has the dynamic op 'Merge'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [StatefulPartitionedCall/Preprocessor/ResizeToRange/Shape_2]    at GraphExecutor.compile (graph_executor.js:121)    at GraphExecutor.execute (graph_executor.js:156)    at GraphModel.execute (graph_model.js:264)    at HomePage.<anonymous> (home.page.ts:65)    at Generator.next (<anonymous>)    at fulfilled (tslib.es6.js:71)    at ZoneDelegate.invoke (zone-evergreen.js:364)    at Object.onInvoke (core.js:27437)    at ZoneDelegate.invoke (zone-evergreen.js:363)    at Zone.run (zone-evergreen.js:123)    at resolvePromise (zone-evergreen.js:798)    at zone-evergreen.js:705    at fulfilled (tslib.es6.js:71)    at ZoneDelegate.invoke (zone-evergreen.js:364)    at Object.onInvoke (core.js:27437)    at ZoneDelegate.invoke (zone-evergreen.js:363)    at Zone.run (zone-evergreen.js:123)    at zone-evergreen.js:857    at ZoneDelegate.invokeTask (zone-evergreen.js:399)    at Object.onInvokeTask (core.js:27425)```I'm not sure if the problem is with Conversion method/code of saved model or with Image to Tensor conversion or incompatibility of the EfficientDet Model with TFJS based implementation.Please help.Thanks.","['I found the problem. The tensorflowjs converter (python) was of version 2.7.0 and i was using tfjs 2.3.0 in angular and the problem resolved on updating it to 2.7.0. Thanks.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4256"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4256"">No</a>=====']",Data & Model Error,Crash,Inconsistent Modules,Operator,API,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.2,A.2
https://github.com/tensorflow/tfjs/issues/4253,"The package ""@tensorflow/tfjs-backend-wasm"" is not minifier-safe",8,closed,2020-11-17T20:59:03Z,2020-12-16T00:01:07Z,"I'm working on [a bug](https://github.com/evanw/esbuild/issues/538) filed by a user of this library against [esbuild](https://github.com/evanw/esbuild); a bundler I work on. The `@tensorflow/tfjs-backend-wasm` package breaks when bundled and minified. This problem is not specific to esbuild; it happens with a normal Webpack production build too with all default options. However; this is a problem that needs to be solved in the library itself instead of by the bundler; so I figured I should file an issue here about this.**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS 10.15.7- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ?- Browser version: Chrome 80- Tensorflow.js Converter Version: N/A**Describe the current behavior**The `@tensorflow/tfjs-backend-wasm` package sometimes breaks when minified. This is the case with Webpack and other bundlers.The specific reason for the breakage is described here: https://github.com/evanw/esbuild/issues/538#issuecomment-729199631. At a high level; this library converts a function to a string and then converts that string back to a function. This essentially ""rips"" it out of its local scope and injects it into another scope and then re-binds all identifiers. That function accesses a local variable called `_scriptDir` and expects that variable to keep the same name in the function source code.Some ways of fixing this:* This could be fixed by placing the code that should not be minified inside a string instead of using a function expression to contain the code. This will ensure that the minifier will not transform it in any way.* This could be fixed by not using the name of the variable to implement this binding. It would be best to pass the value of this variable as an additional parameter to the function so the function doesn't reach for anything outside of its scope.* There is an alternative hack using direct `eval` if doing this is not possible. This hack is also described in https://github.com/evanw/esbuild/issues/538#issuecomment-729199631. Using direct `eval` is an anti-pattern so I'd only do this if other options don't work.**Describe the expected behavior**The `@tensorflow/tfjs-backend-wasm` package should not behave differently when minified.**Standalone code to reproduce the issue**There is a full reproduction case here: https://github.com/evanw/esbuild/issues/538#issuecomment-728939332. Note that to hit the issue; you need to have both `WebAssembly SIMD support` and `WebAssembly threads support` enabled in `chrome://flags/`.","['cc @tafsiri @annxingyuan ====='; 'I use @tensorflow/tfjs-backend-wasm with webpack and I have no problems. Webpack 4. I can give you my webpack config if you need.====='; ""@annxingyuan i can repro this issue (or at least one that seems related) with the sample webpack project in the repo. For the code mentioned in the issue; it this part of emscripten's output?=====""; ""@tafsiri Yes; the code is part of emscripten's output. However we prepend code to it when converting it to a blob.=====""; '@tafsiri Do you think this needs the release blocker tag since it is not specific to master?====='; ""Hmm I was thinking so because the example in the repo doesn't work in current form? What do you think? I think with a closer look we can decide whether we want to this to block the next release (for example if we have to file an upstream bug with emscripten). But thought we should at least investigate before doing another release.=====""; 'Work-in-progress PR: https://github.com/emscripten-core/emscripten/pull/12832====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4253"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4253"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,Wasm,Backend,change code order,Adjust API invocation sequence,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Minification Incompatibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5959,image-classification/react-native-cli example fails to build,1,open,2021-12-21T16:34:30Z,2021-12-23T19:21:02Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Mac OS 12.1; XCode 13.2.1 (13C100)- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): yarn via package.json at https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification/react-native-cli- TensorFlow.js version (use command below): 3.9.0- Browser version: n/a- Tensorflow.js Converter Version: n/a**Describe the current behavior**Running `pod install` on the [`image-classification`](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification/react-native-cli) react native CLI example fails with: ```undefined method `react_native_post_install' for #<Pod::Podfile:0x00007fa755bc7240>```**Describe the expected behavior**pod install should complete successfully. **Standalone code to reproduce the issue**https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification/react-native-cli----From https://stackoverflow.com/a/67136582/4965 it looks like the version of react-native in the [example](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/image-classification/react-native-cli) needs to be bumped one minor release to 0.64. I was able to verify this; and can make a PR. Is there any CI on the react examples repo? Maybe I can contribute for that too.```react-native-cli (master)> cd ios && pod installAuto-linking React Native modules for target `reactnativecli`: RNCAsyncStorage and RNFSAnalyzing dependenciesFetching podspec for `DoubleConversion` from `../node_modules/react-native/third-party-podspecs/DoubleConversion.podspec`Fetching podspec for `Folly` from `../node_modules/react-native/third-party-podspecs/Folly.podspec`Fetching podspec for `glog` from `../node_modules/react-native/third-party-podspecs/glog.podspec`Downloading dependenciesInstalling CocoaAsyncSocket (7.6.5)Installing DoubleConversion (1.1.6)Installing FBLazyVector (0.63.2)Installing FBReactNativeSpec (0.63.2)Installing Flipper (0.41.5)Installing Flipper-Boost-iOSX (1.76.0.1.11)Installing Flipper-DoubleConversion (1.1.7)Installing Flipper-Fmt (7.1.7)Installing Flipper-Folly (2.6.10)Installing Flipper-Glog (0.3.6)Installing Flipper-PeerTalk (0.0.4)Installing Flipper-RSocket (1.4.3)Installing FlipperKit (0.41.5)Installing Folly (2020.01.13.00)Installing OpenSSL-Universal (1.1.1100)Installing RCTRequired (0.63.2)Installing RCTTypeSafety (0.63.2)Installing RNCAsyncStorage (1.15.8)Installing RNFS (2.14.1)Installing React (0.63.2)Installing React-Core (0.63.2)Installing React-CoreModules (0.63.2)Installing React-RCTActionSheet (0.63.2)Installing React-RCTAnimation (0.63.2)Installing React-RCTBlob (0.63.2)Installing React-RCTImage (0.63.2)Installing React-RCTLinking (0.63.2)Installing React-RCTNetwork (0.63.2)Installing React-RCTSettings (0.63.2)Installing React-RCTText (0.63.2)Installing React-RCTVibration (0.63.2)Installing React-callinvoker (0.63.2)Installing React-cxxreact (0.63.2)Installing React-jsi (0.63.2)Installing React-jsiexecutor (0.63.2)Installing React-jsinspector (0.63.2)Installing ReactCommon (0.63.2)Installing Yoga (1.14.0)Installing YogaKit (1.18.1)Installing boost-for-react-native (1.63.0)Installing glog (0.3.5)Installing libevent (2.1.12)Generating Pods project[!] An error occurred while processing the post-install hook of the Podfile.undefined method `react_native_post_install' for #<Pod::Podfile:0x00007fa755bc7240>~/tfjs-examples/react-native/image-classification/react-native-cli/ios/Podfile:29:in `block (3 levels) in from_ruby'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-core-1.11.2/lib/cocoapods-core/podfile.rb:196:in `post_install!'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:945:in `run_podfile_post_install_hook'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:933:in `block in run_podfile_post_install_hooks'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/user_interface.rb:149:in `message'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:932:in `run_podfile_post_install_hooks'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:331:in `block (2 levels) in create_and_save_projects'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer/xcode/pods_project_generator/pods_project_writer.rb:61:in `write!'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:330:in `block in create_and_save_projects'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/user_interface.rb:64:in `section'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:309:in `create_and_save_projects'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:301:in `generate_pods_project'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:180:in `integrate'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/installer.rb:167:in `install!'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/command/install.rb:52:in `run'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/claide-1.0.3/lib/claide/command.rb:334:in `run'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/lib/cocoapods/command.rb:52:in `run'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/gems/cocoapods-1.11.2/bin/pod:55:in `<top (required)>'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/bin/pod:23:in `load'/usr/local/Cellar/cocoapods/1.11.2_1/libexec/bin/pod:23:in `<main>'```,"['Upgrading react-native to 0.64 (here: https://github.com/tensorflow/tfjs-examples/blob/master/react-native/image-classification/react-native-cli/package.json#L22) resolves this error and build succeed; but produces a fatal runtime error (I believe this is the one referenced in https://github.com/tensorflow/tfjs-examples/pull/608; which I now see noted that react-native > 0.65 would produce a crash):``` WARN  The ""UMNativeModulesProxy"" native module is not exported through NativeModules; verify that @unimodules/react-native-adapter\'s native code is linked properly ERROR  TypeError: null is not an object (evaluating \'NativeUnimoduleProxy.viewManagersNames\')```Possibly related to https://github.com/tensorflow/tfjs/issues/5698?=====']",Build & Install Failure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change react-native version,change react-native version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Pod Install Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,D.3
https://github.com/tensorflow/tfjs/issues/5897,React Native - Back Camera Issue + Unhandled promise rejection: TypeError: null is not an object,5,closed,2021-11-25T01:01:41Z,2021-12-09T20:39:58Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Copied from [Here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection)- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):macOS Big Sur 11.2- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone SE- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): ""^0.8.0""- Browser version: Chrome 96- Tensorflow.js Converter Version: ""^3.9.0""**Describe the current behavior**I am still having issues with the Back camera not placing landmarks correctly. The Front camera works great. On Back camera; they place when no person is in the camera view and off to the side on a ghost pose when a person is in view. I had the same issue with version 0.6.0. I've taken out useCustomShadersToResize={false}; cameraTextureWidth{OUTPUT_TENSOR_WIDTH} and cameraTextureHeight={OUTPUT_TENSOR_HEIGHT} but it is still requesting these.**Describe the expected behavior**Expected behavior would be for the landmarks to place correctly.I am getting this error currently in VS Code:![Screen Shot 2021-11-27 at 3 11 29 PM](https://user-images.githubusercontent.com/47552416/143723008-cdb87926-38f7-4bbc-b20b-572b0b823373.png)**Standalone code to reproduce the issue**```import React; { useEffect; useState; useRef } from 'react';import { StyleSheet; Text; View; Dimensions; Platform } from 'react-native';import { Camera } from 'expo-camera';import * as tf from '@tensorflow/tfjs';import * as posedetection from '@tensorflow-models/pose-detection';import * as ScreenOrientation from 'expo-screen-orientation';import { cameraWithTensors } from '@tensorflow/tfjs-react-native';import Svg; { Circle } from 'react-native-svg';import { ExpoWebGLRenderingContext } from 'expo-gl';// tslint:disable-next-line: variable-nameconst TensorCamera = cameraWithTensors(Camera);const IS_ANDROID = Platform.OS === 'android';const IS_IOS = Platform.OS === 'ios';// Camera preview size.//// From experiments; to render camera feed without distortion; 16:9 ratio// should be used fo iOS devices and 4:3 ratio should be used for android// devices.//// This might not cover all cases.const CAM_PREVIEW_WIDTH = Dimensions.get('window').width;const CAM_PREVIEW_HEIGHT = CAM_PREVIEW_WIDTH / (IS_IOS ? 9 / 16 : 3 / 4);// The score threshold for pose detection results.const MIN_KEYPOINT_SCORE = 0.3;// The size of the resized output from TensorCamera.//// For movenet; the size here doesn't matter too much because the model will// preprocess the input (crop; resize; etc). For best result; use the size that// doesn't distort the image.const OUTPUT_TENSOR_WIDTH = 180;const OUTPUT_TENSOR_HEIGHT = OUTPUT_TENSOR_WIDTH / (IS_IOS ? 9 / 16 : 3 / 4);// Whether to auto-render TensorCamera preview.const AUTO_RENDER = false;export default function App() {  const cameraRef = useRef(null);  const [tfReady; setTfReady] = useState(false);  const [model; setModel] = useState<posedetection.PoseDetector>();  const [poses; setPoses] = useState<posedetection.Pose[]>();  const [fps; setFps] = useState(0);  const [orientation; setOrientation] =    useState<ScreenOrientation.Orientation>();  useEffect(() => {    async function prepare() {      // Set initial orientation.      const curOrientation = await ScreenOrientation.getOrientationAsync();      setOrientation(curOrientation);      // Listens to orientation change.      ScreenOrientation.addOrientationChangeListener((event) => {        setOrientation(event.orientationInfo.orientation);      });      // Camera permission.      await Camera.requestPermissionsAsync();      // Wait for tfjs to initialize the backend.      await tf.ready();      // Load movenet model.      // https://github.com/tensorflow/tfjs-models/tree/master/pose-detection      const model = await posedetection.createDetector(        posedetection.SupportedModels.MoveNet;        {          modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING;          enableSmoothing: true;        }      );      setModel(model);      // Ready!      setTfReady(true);    }    prepare();  }; []);    const handleCameraStream = async (    images: IterableIterator<tf.Tensor3D>;    updatePreview: () => void;    gl: ExpoWebGLRenderingContext  ) => {    const loop = async () => {      // Get the tensor and run pose detection.      const imageTensor = images.next().value as tf.Tensor3D;      const startTs = Date.now();      const poses = await model!.estimatePoses(        imageTensor;        undefined;        Date.now()      );      const latency = Date.now() - startTs;      // setFps(Math.floor(1000 / latency));      setPoses(poses);      tf.dispose([imageTensor]);      // Render camera preview manually when autorender=false.      if (!AUTO_RENDER) {        updatePreview();        gl.endFrameEXP();      }      requestAnimationFrame(loop);    };        loop();  };    const renderPose = () => {    if (poses != null && poses.length > 0) {      const keypoints = poses[0].keypoints        .filter((k) => (k.score ?? 0) > MIN_KEYPOINT_SCORE)        .map((k) => {          // Flip horizontally on android.          const x = IS_ANDROID ? OUTPUT_TENSOR_WIDTH - k.x : k.x;          const y = k.y;          const cx =            (x / getOutputTensorWidth()) *            (isPortrait() ? CAM_PREVIEW_WIDTH : CAM_PREVIEW_HEIGHT);          const cy =            (y / getOutputTensorHeight()) *            (isPortrait() ? CAM_PREVIEW_HEIGHT : CAM_PREVIEW_WIDTH);          return (            <Circle              key={`skeletonkp_${k.name}`}              cx={cx}              cy={cy}              r='4'              strokeWidth='2'              fill='#00AA00'              stroke='white'            />          );        });      return <Svg style={styles.svg}>{keypoints}</Svg>;    } else {      return <View></View>;    }  };  const renderFps = () => {    return (      <View style={styles.fpsContainer}>        <Text>FPS: {fps}</Text>      </View>    );  };  const isPortrait = () => {    return (      orientation === ScreenOrientation.Orientation.PORTRAIT_UP ||      orientation === ScreenOrientation.Orientation.PORTRAIT_DOWN    );  };  const getOutputTensorWidth = () => {    // On iOS landscape mode; switch width and height of the output tensor to    // get better result. Without this; the image stored in the output tensor    // would be stretched too much.    //    // Same for getOutputTensorHeight below.    return isPortrait() || IS_ANDROID      ? OUTPUT_TENSOR_WIDTH      : OUTPUT_TENSOR_HEIGHT;  };  const getOutputTensorHeight = () => {    return isPortrait() || IS_ANDROID      ? OUTPUT_TENSOR_HEIGHT      : OUTPUT_TENSOR_WIDTH;  };  const getTextureRotationAngleInDegrees = () => {    // On Android; the camera texture will rotate behind the scene as the phone    // changes orientation; so we don't need to rotate it in TensorCamera.    if (IS_ANDROID) {      return 0;    }    // For iOS; the camera texture won't rotate automatically. Calculate the    // rotation angles here which will be passed to TensorCamera to rotate it    // internally.    switch (orientation) {      // Not supported on iOS as of 11/2021; but add it here just in case.      case ScreenOrientation.Orientation.PORTRAIT_DOWN:        return 180;      case ScreenOrientation.Orientation.LANDSCAPE_LEFT:        return 270;      case ScreenOrientation.Orientation.LANDSCAPE_RIGHT:        return 90;      default:        return 0;    }  };  if (!tfReady) {    return (      <View style={styles.loadingMsg}>        <Text>Loading...</Text>      </View>    );  } else {    return (      // Note that you don't need to specify `cameraTextureWidth` and      // `cameraTextureHeight` prop in `TensorCamera` below.      <View        style={          isPortrait() ? styles.containerPortrait : styles.containerLandscape        }      >        <TensorCamera           ref={cameraRef}          style={styles.camera}          autorender={AUTO_RENDER}          type={Camera.Constants.Type.front}          // tensor related props          resizeWidth={getOutputTensorWidth()}          resizeHeight={getOutputTensorHeight()}          resizeDepth={3}          rotation={getTextureRotationAngleInDegrees()}          onReady={handleCameraStream}                  />        {renderPose()}        {renderFps()}       </View>    );  }}const styles = StyleSheet.create({  containerPortrait: {    position: 'relative';    width: CAM_PREVIEW_WIDTH;    height: CAM_PREVIEW_HEIGHT;    marginTop: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2;  };  containerLandscape: {    position: 'relative';    width: CAM_PREVIEW_HEIGHT;    height: CAM_PREVIEW_WIDTH;    marginLeft: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2;  };  loadingMsg: {    position: 'absolute';    width: '100%';    height: '100%';    alignItems: 'center';    justifyContent: 'center';  };  camera: {    width: '100%';    height: '100%';    zIndex: 1;  };  svg: {    width: '100%';    height: '100%';    position: 'absolute';    zIndex: 30;  };  fpsContainer: {    position: 'absolute';    top: 10;    left: 10;    width: 80;    alignItems: 'center';    backgroundColor: 'rgba(255; 255; 255; .7)';    borderRadius: 2;    padding: 8;    zIndex: 20;  };});```Here is my package.json:```{  ""name"": ""mobile"";  ""version"": ""1.0.0"";  ""main"": ""node_modules/expo/AppEntry.js"";  ""scripts"": {    ""start"": ""expo start"";    ""android"": ""expo start --android"";    ""ios"": ""expo start --ios"";    ""web"": ""expo start --web"";    ""eject"": ""expo eject""  };  ""dependencies"": {    ""@mediapipe/pose"": ""^0.5.1635988162"";    ""@react-native-async-storage/async-storage"": ""^1.15.14"";    ""@react-navigation/core"": ""^6.1.0"";    ""@react-navigation/drawer"": ""^6.1.8"";    ""@react-navigation/material-bottom-tabs"": ""^6.0.9"";    ""@react-navigation/native"": ""^6.0.6"";    ""@react-navigation/stack"": ""^6.0.11"";    ""@tensorflow-models/pose-detection"": ""0.0.6"";    ""@tensorflow/tfjs"": ""^3.11.0"";    ""@tensorflow/tfjs-react-native"": ""^0.8.0"";    ""axios"": ""^0.24.0"";    ""expo"": ""~43.0.2"";    ""expo-camera"": ""~12.0.3"";    ""expo-font"": ""^10.0.3"";    ""expo-gl"": ""~11.0.3"";    ""expo-gl-cpp"": ""~11.0.1"";    ""expo-status-bar"": ""~1.1.0"";    ""formik"": ""^2.2.9"";    ""react"": ""17.0.1"";    ""react-dom"": ""17.0.1"";    ""react-native"": ""0.64.3"";    ""react-native-animatable"": ""^1.3.3"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-gesture-handler"": ""^1.10.2"";    ""react-native-paper"": ""^4.10.1"";    ""react-native-reanimated"": ""^2.2.4"";    ""react-native-safe-area-context"": ""^3.3.2"";    ""react-native-svg"": ""^12.1.1"";    ""react-native-vector-icons"": ""^9.0.0"";    ""react-native-web"": ""0.17.1"";    ""react-navigation"": ""^4.4.4"";    ""react-redux"": ""^7.2.6"";    ""redux"": ""^4.1.2"";    ""redux-thunk"": ""^2.4.1""  };  ""devDependencies"": {    ""@babel/core"": ""^7.12.9"";    ""@types/react"": ""~17.0.21"";    ""@types/react-native"": ""~0.64.12"";    ""typescript"": ""~4.3.5""  };  ""private"": true}```Example video of landmarks:https://user-images.githubusercontent.com/47552416/144524649-b1c1239f-edc2-4971-928d-03c7016506a6.mp4**Additional Issue**I am also dealing with issues when changing screens. Anytime I exit the screen with the pose detection or even close down the app; I receive this error:![Screen Shot 2021-11-27 at 5 46 24 PM](https://user-images.githubusercontent.com/47552416/143725958-ed60743b-d27c-4051-b176-b56128525c5b.png)On occasion I get his error with it:![Screen Shot 2021-11-27 at 5 46 32 PM](https://user-images.githubusercontent.com/47552416/143725950-c357d0ab-c5ae-4af4-80e5-9cc7a2ec64cf.png)","['Hi @eledahl;I need to look more on those errors (please ignore them for now). For the back camera; I think you need to flip the X coordinates on iOS when back camera is being used. Please see this [draft PR](https://github.com/tensorflow/tfjs-examples/pull/637) where I added a button to switch between front and back camera (still work in progress; e.g. back camera landscape mode not working correctly).Tested on iPhone 13.Thanks!====='; '@jinjingforever thank you! The changes in your draft fixed the issue for the back camera. Please let me know if you find anything about the unmounting error!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5897"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5897"">No</a>====='; 'Hi @eledahl I updated the demo with back camera support and I also added logic to properly clean up update loop (cancelAnimationFrame) when the app is unmounted.I took a look at the ""null is not an object"" error. Looks like it is caused by weird interactions between expo\'s ""fast refresh"" and the app that I don\'t quite understand yet. I tried unmounting the app/camera component in a regular app and didn\'t see this error. For now please ignore that error. Thanks!====='; 'Thank you @jinjingforever! I am not seeing the unmounting error anymore with the updates.=====']",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,add data postprocess,Add data processing,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[A.2] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",D,D.1
https://github.com/tensorflow/tfjs/issues/5889,tfjs-examples/chrome-extension needs to migrate to manifest v3 (phase out begins Jan 17 2022),1,open,2021-11-24T17:47:15Z,2021-12-11T21:18:21Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):N/A; any OS (I'm using Ubuntu 20.04)- TensorFlow.js installed from (npm or script link):Any version used in the tfjs-examples/chrome-extension with  yarn / yarn build.- Browser version:Anything at or above Chrome 88**Describe the current behavior**This tfjs-example uses manifest v2: https://github.com/tensorflow/tfjs-examples/tree/master/chrome-extensionThis blog post: https://developer.chrome.com/blog/mv2-transition/ states that the phase out for manifest V2 begins on January 17; 2022 when developers will no longer be able to submit extensions using manifest V2.**Describe the expected behavior**The tfjs-example/chrome-extension is migrated to manifest V3 and continues to work so that developers can continue to use it as a reference to build their own extensions that utilize TFJS. Note; this may require non-trivial work because manifest V3 deprecates background pages and requires service workers (which do not allow for the use of the global: ""window""). **Standalone code to reproduce the issue**See this file which states ""manifest_version"": 2. This now needs to read ""manifest_version"": 3 since V2 is phasing out starting on January 17; 2022. https://github.com/tensorflow/tfjs-examples/blob/master/chrome-extension/dist/manifest.jsonProvide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.See: https://github.com/tensorflow/tfjs-examples/tree/master/chrome-extension**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.See the phase out plan announced in this blog post: https://developer.chrome.com/blog/mv2-transition/","[""This would be very helpful. I'm trying to integrate tfjs in a chrome extension but manifest v3 doesn't allow using cdn scripts. It would be very helpful if there was a guide for using tfjs in real chrome extensions.=====""]",Build & Install Failure,Build & Initialization Failure,Browser Incompatibility,Browser,Platform,change maninfest,change maninfest,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1 Others""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Configuration & Dependency Error""
  }
}
```",C,D.2
https://github.com/tensorflow/tfjs/issues/5743,[tf-core] Cannot pass ImageBitmap to model in worker,8,closed,2021-10-19T10:54:10Z,2021-10-28T18:20:13Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 7- TensorFlow.js installed from (npm or script link): https://www.jsdelivr.com/package/npm/@tensorflow/tfjs-core- TensorFlow.js version (use command below): 3.9.0- Browser version: Chrome  94.0.4606.81- Tensorflow.js Converter Version: 3.9.0Model: face-landmarks-detection v. 0.0.4; with wasm-only backend.**Preconditions**1. Put all face-landmarks-detection code into web-worker2. Pass current ImageBitmap**Describe the current behavior**`ReferenceError: document is not defined`.**Describe the expected behavior**ImageBitmap gets processed.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.https://github.com/AlexShafir/Sensoria/blob/tfjs5743Message passed [here](https://github.com/AlexShafir/Sensoria/blob/tfjs5743/src/index.js#L176); received [here](https://github.com/AlexShafir/Sensoria/blob/tfjs5743/src/worker.js#L25)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```Uncaught (in promise) ReferenceError: document is not defined    at fromPixels_ (:8887/fld/tf-core.js:8911)    at Object.fromPixels__op [as fromPixels] (:8887/fld/tf-core.js:5406)    at :8887/fld/face-landmarks-detection.js:1350    at :8887/fld/tf-core.js:4394    at Engine.scopedRun (:8887/fld/tf-core.js:4404)    at Engine.tidy (:8887/fld/tf-core.js:4393)    at Object.tidy (:8887/fld/tf-core.js:10072)    at FaceMesh.<anonymous> (:8887/fld/face-landmarks-detection.js:1348)    at step (:8887/fld/face-landmarks-detection.js:81)    at Object.next (:8887/fld/face-landmarks-detection.js:62)```Probably related to #4218,"[""This is not a bug in TFJS; this is how web workers work in general - there is no DOM in a worker; so you cannot rely on any DOM objects. TFJS could print more informative error message; but root cause is the same.What you can do is pass pixel data (as value or actually transfer ownership of an array to worker) such as `ImageData.data` (which is just an array so that can be used by web worker) that you've already prepared in the main thread.=====""; '@vladmandic - Sure; web workers do not have access to `document`. - Thanks for tip with `ImageData.data`! I ended up using `ImageData.data.buffer` as ArrayBuffer is transferable.====='; '@AlexShafir yup; thats the best approach for web workers...and inside web worker you can either reconstruct `ImageData` and then use `fromPixels` or just create tensor directly (but then you need to slice off alpha channel manually since `ImageData` is in RGBA format)====='; 'Thank you @vladmandic @AlexShafir closing this issue as this is not a bug and more support issue. Feel free to @mention to reopen.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>====='; '@rthadur if API says that one can enter ImageBitmap; you pass it (in worker context) and it crashes; I expect this behavior is called a bug?In any case; what is the process for this kind of problems?====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5743"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Operator,API,add support for datatype,Add unsupported operator,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5603,Installation failed for tfjs-node 3.9.0 on windows with Node 12,3,closed,2021-09-10T12:09:18Z,2021-09-13T12:56:55Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 3.9.0- CUDA/cuDNN version: NAnpm install of tfjs-node fails on windows**Provide the exact sequence of commands / steps that you executed before running into the problem**npm i @tensorflow/tfjs-nodeLogs> @tensorflow/tfjs-node@3.9.0 install C:\Users\xxxxxxxxxxxxxxxxxxxx\node_modules\@tensorflow\tfjs-node> node scripts/install.jsCPU-windows-3.9.0.zip* Downloading libtensorflowhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.5.0.zip[==============================] 8649874/bps 100% 0.0s* Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnode-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.9.0/CPU-windows-3.9.0.zipnode-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@3.9.0 and node@12.22.5 (node-v72 ABI; unknown) (falling back to source compile with node-gyp)node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/3.9.0/CPU-windows-3.9.0.zipgyp WARN install got an error; rolling back installgyp ERR! configure errorgyp ERR! stack Error: self signed certificate in certificate chaingyp ERR! stack     at TLSSocket.onConnectSecure (_tls_wrap.js:1502:34)gyp ERR! stack     at TLSSocket.emit (events.js:314:20)gyp ERR! stack     at TLSSocket._finishInit (_tls_wrap.js:937:8)gyp ERR! stack     at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:711:12)gyp ERR! System Windows_NT 10.0.18363gyp ERR! command ""C:\\Users\\xxxxxxxxxxxxx\\node.exe"" ""C:\\Users\\txxxxxxxxxxx\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\Users\\xxxxxxxxxxxxxx\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=C:\\Users\\xxxxxxxxxxx\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8"" ""--napi_version=8"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""gyp ERR! cwd C:\Users\xxxxxxxx\node_modules\@tensorflow\tfjs-nodegyp ERR! node -v v12.22.5gyp ERR! node-gyp -v v5.1.0gyp ERR! not oknode-pre-gyp ERR! build errornode-pre-gyp ERR! stack Error: Failed to execute 'C:\Users\xxxxxxxxx\node.exe C:\Usersxxxxxxxxxx\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=C:\Users\xxxxxxxxx\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=C:\Users\xxxxxxxx\node_modules\@tensorflow\tfjs-node\lib\napi-v8 --napi_version=8 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (C:\Users\xxxxx\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:314:20)node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:1022:16)node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:287:5)node-pre-gyp ERR! System Windows_NT 10.0.18363node-pre-gyp ERR! command ""C:\\Users\\xxxxxxxx\\node.exe"" ""C:\\Users\\xxxxxxxxnode_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""node-pre-gyp ERR! cwd C:\Users\xxxxxxxxx\node_modules\@tensorflow\tfjs-nodenode-pre-gyp ERR! node -v v12.22.5node-pre-gyp ERR! node-pre-gyp -v v1.0.4node-pre-gyp ERR! not ok","[""The 404 on the prebuilt binary is actually expected since I don't believe we ship Node-API version 8 binaries yet. This should not matter though; since the install should fall back to compiling from source; and it looks like that's where it failed.This looks like it could be related to certificates; since it's complaining about a self-signed certificate in the chain (`gyp ERR! stack Error: self signed certificate in certificate chain`). Apparently; this can be caused by being behind a corporate firewall. Certificates are outside my area of expertise; but you might find an answer on [this issue](https://github.com/nodejs/node-gyp/issues/695#issuecomment-177260446).=====""; ""Hello; thanks for the link where I find a work around to install for the first time the tensorflow nodejs librariries.Using set NODE_TLS_REJECT_UNAUTHORIZED=0 before launching the install. Didn't succeed with the solution of default certificates  up to now.Thank you=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5603"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5603"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5336,WASM throws memory access out of bounds in Chrome but not Firefox/Edge,6,closed,2021-07-16T13:02:48Z,2021-08-12T18:35:06Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):Yes; at least as pertains to the web part.- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):Windows 10- TensorFlow.js installed from (npm or script link):importScripts(""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"");importScripts(""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"");- Browser version:Chrome 91.0.4472.164**Describe the current behavior**When running the ProGan model on a website in web workers; when using WASM backend; Firefox and Edge will use tfjs-backend-wasm-simd.wasm while Chrome will use tfjs-backend-wasm-threaded-simd.wasm. The code will execute fine on Firefox and Edge but fail on Chrome with unclear exceptions. Best guess; multithreaded only works on the main js module and fails through workers.However; when setting WASM_HAS_MULTITHREAD_SUPPORT to false; Chrome uses the same tfjs-backend-wasm-simd.wasm file and throws a memory access out of bounds exception. Again; this works fine with the other browsers. This is the same with WASM_HAS_SIMD_SUPPORT set to false as well.**Describe the expected behavior**Chrome should load the model instead of throwing the error.NOTE: I'm running wasm in worker threads not only to achieve multithreading on browsers other than Chrome; but primarily to not block the UI thread. This is for benchmarking; not meant to be real-time.**Standalone code to reproduce the issue**https://github.com/tauber/proganYou can run the demo from http://objectcoded.com/progan/progan.htmlSelect any number of threads and WASM for the backend. Approx. ~100MB model.Exception details:Uncaught (in promise) RuntimeError: memory access out of bounds    at tfjs-backend-wasm-simd.wasm:0xa8b    at Pa (tfjs-backend-wasm-simd.wasm:0x1aaa2)    at BackendWasm.move (backend_wasm.ts:83)    at BackendWasm.write (backend_wasm.ts:54)    at e.t.makeTensor (engine.js:813)    at Sk (tensor_ops_util.js:75)    at Tk (tensor.js:56)    at Ak (io_utils.js:224)    at e.t.loadSync (graph_model.js:160)    at e.<anonymous> (graph_model.js:134)(anonymous) @ tfjs-backend-wasm-simd.wasm:0xa8bPa @ tfjs-backend-wasm-simd.wasm:0x1aaa2BackendWasm.move @ backend_wasm.ts:83BackendWasm.write @ backend_wasm.ts:54t.makeTensor @ engine.js:813Sk @ tensor_ops_util.js:75Tk @ tensor.js:56Ak @ io_utils.js:224t.loadSync @ graph_model.js:160(anonymous) @ graph_model.js:134c @ runtime.js:63(anonymous) @ runtime.js:293(anonymous) @ runtime.js:118bv @ runtime.js:747o @ runtime.js:747async function (async)render_gan_image @ render-gan.js:73","[""Hi @tauber;Thanks for the report! Looks like the problem is version mismatch. The `progan/modules/tfjs-backend-wasm-simd.wasm` file in your [repo](https://github.com/tauber/progan) is from 3.7.0; but the `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js` is loading 3.8.0 (which was released 23 days ago; 2 days before your repo was created). I replaced `tfjs-backend-wasm-simd.wasm` with the one from 3.8.0 and your app works without problems (great app btw!). I am not entirely sure why things work on other browsers though.. One thing to do to help with this is to call the following in your worker script (maybe after the two `importScripts` calls). This will ask the wasm backend to load the wasm binary files from jsdelivr so you don't need to copy them manually. I tried it and it seems to [work](https://drive.google.com/file/d/1utcV8WEiG6AQlkdYJbV7dCyazt1jCyML/view?usp=sharing).```jstf.wasm.setWasmPaths(    'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/');```Thanks!=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5336"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5336"">No</a>====='; ""Great work! Thank you! I wanted to use the setWasmPaths api but couldn't because I didn't find any documentation that uses the wasm object. Existing blog posts call the function directly from tf; which did not work.=====""; 'Thank you @tauber ; did you mean to say the documentation here https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/README.md#using-bundlers  is not clear ?====='; ""@rthadur Looks like the doc didn't mention what to do when importing the wasm backend through the script tag. I will update it. Thanks!=====""; ""Now I see the comment: '// or tf.wasm.setWasmPaths when using <script> tags.'I don't know why I missed this. Making it more obvious would definitely help exhausted developers :)Thank you again for your help with this issue.=====""]",Data & Model Error,Crash,Browser Incompatibility,Browser,Platform,change import(script) path,Fix import confusion in program,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.5] Training Argument Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.2,D.2
https://github.com/tensorflow/tfjs/issues/5257,Posenet Camera Demo is Broken,5,closed,2021-06-28T18:17:37Z,2021-06-30T21:51:01Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Debian 5.10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): N/A- TensorFlow.js version (use command below): N/A- Browser version: Chrome 91- Tensorflow.js Converter Version: N/A**Describe the current behavior**The Posenet live camera demo displays ""this browser does not support video capture;or this device does not have a camera"" instead of opening the camera. Other models' live camera demos work fine on my machine.**Describe the expected behavior**The posenet live camera demo predicts poses from the computer's webcam.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.Click the demo from the [Posenet readme](https://github.com/tensorflow/tfjs-models/tree/master/posenet). Here's a [direct link](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html) to the demo.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['@mattsoulanille the demo [link](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html) which you provided is working for me without any issues (on my PC - Windows 10 pro x64 | Chrome 91.0.4472.124). Later; I re-checked whether the issue persists on different systems; & found that my MacBook is also facing the same issue (with Chrome).====='; ""This is strange. The demo is now working for me with no issues on the same computer I used in my first test. I don't believe I've made any changes to the setup (haven't restarted Chrome). I also tested on a 2015 macbook pro 10.14.6 chrome 91.0.4472.114 with no issues. I'll keep this issue open since you've been able to reproduce it. =====""; '@mattsoulanille I would love to add an update to my reply.I was facing the same issue (on my MacBook Air) which you mentioned; later I figure out that doing a **hard refresh** fixes the issue! I think; I was able to reproduce the issue because of the cache of the ResNet50 model from one of my personal projects; which might be creating an issue when a new model (MobileNetv1) was loading up.====='; ""Thanks for the additional info; @Neilblaze. I was running some other demos that use ResNet50 locally; so maybe that caused the same cache problems you had. I thought I had done an **empty cache and hard reload**; but I guess I actually didn't. I'll close this issue; since it seems like that was the problem.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5257"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5257"">No</a>=====']",Browser & Device Error,Crash,Browser Incompatibility,Browser,Platform,empty cache,empty cache,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] Camera Access Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.4,D.2
https://github.com/tensorflow/tfjs/issues/5191,[Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'graph.versions.producer')],2,open,2021-06-08T14:00:01Z,2021-06-09T19:48:05Z,I have used graph-model while loading the loading i m getting this error   [Unhandled promise rejection: TypeError: undefined is not an object (evaluating 'graph.versions.producer')]Code Referenceasync componentDidMount() {    await tf.ready()    this.setState({      isTfReady: true     })      const modelJson = require('./assets/model/model.json');      const modelWeights = require('./assets/model/final.bin');            this.model = await tf.loadGraphModel(bundleResourceIO(modelJson; modelWeights));      console.log('loadGraphModel'; this.model)            this.setState({ isModelReady: true })    // await this.getPermissionAsync()  },"['Can you share full log and reproducible code ;and also looking at the error message this is related to exported model not tfjs ; please check for a related issue here https://github.com/tensorflow/tfjs/issues/5122====='; '[<img width=""1440"" alt=""Screenshot 2021-06-09 at 2 38 42 PM"" src=""https://user-images.githubusercontent.com/84903872/121326855-8f04c600-c930-11eb-8143-1f1d8d601755.png"">](url)import React from \'react\'import {  StyleSheet;  Text;  View;  ActivityIndicator;  StatusBar;  Image;  TouchableOpacity} from \'react-native\'import * as tf from \'@tensorflow/tfjs\'import * as jpeg from \'jpeg-js\'import * as ImagePicker from \'expo-image-picker\'import Constants from \'expo-constants\'import * as Permissions from \'expo-permissions\'import * as FileSystem from \'expo-file-system\';import {bundleResourceIO} from ""@tensorflow/tfjs-react-native"";class App extends React.Component {  state = {    isTfReady: false;    isModelReady: false;    predictions: null;    image: null  }  async componentDidMount() {    await tf.ready()    this.setState({      isTfReady: true     })      const modelJson = require(\'./assets/model/model.json\');      const modelWeights = require(\'./assets/model/final.bin\');            this.model = await tf.loadGraphModel(bundleResourceIO(modelJson; modelWeights));      console.log(\'loadGraphModel\'; this.model)            this.setState({ isModelReady: true })    // await this.getPermissionAsync()  }  getPermissionAsync = async () => {    if (Constants.platform.ios) {      const { status } = await Permissions.askAsync(Permissions.CAMERA_ROLL)      if (status !== \'granted\') {        alert(\'Sorry; we need camera roll permissions to make this work!\')      }    }  }  imageToTensor(rawImageData) {    const TO_UINT8ARRAY = true    const { width; height; data } = jpeg.decode(rawImageData)    // Drop the alpha channel info for mobilenet    const buffer = new Uint8Array(width * height * 3)    let offset = 0 // offset into original data    for (let i = 0; i < buffer.length; i += 3) {      buffer[i] = data[offset]      buffer[i + 1] = data[offset + 1]      buffer[i + 2] = data[offset + 2]      offset += 4    }    return tf.tensor3d(buffer; [height; width; 3])  }  classifyImage = async () => {    try {      const imageAssetPath = Image.resolveAssetSource(this.state.image)      // const response = await fetch(imageAssetPath.uri; {}; { isBinary: true })      const imgB64 = await FileSystem.readAsStringAsync(imageAssetPath.uri; {        encoding: FileSystem.EncodingType.Base64;      });      const imgBuffer = tf.util.encodeString(imgB64; \'base64\').buffer;      // const rawImageData = await response.arrayBuffer()      const raw = new Uint8Array(imgBuffer)      // const imageTensor = jpeg.decode(raw);      const imageTensor = this.imageToTensor(raw).resizeBilinear([224;224]).reshape([1;224;224;3]);      // const predictions = await this.model.classify(imageTensor)      // const zeros = tf.zeros([1; 224; 224; 3]);      // model.predict(zeros).print();       const res = this.model.predict(imageTensor);      this.setState({ res })      console.log(res; \'success\')    } catch (error) {      console.log(error; \'failure\')    }  }  selectImage = async () => {    try {      let response = await ImagePicker.launchImageLibraryAsync({        mediaTypes: ImagePicker.MediaTypeOptions.All;        allowsEditing: true;        aspect: [4; 3]      })      if (!response.cancelled) {        const source = { uri: response.uri }        this.setState({ image: source })        this.classifyImage()      }    } catch (error) {      console.log(error)    }  }  renderPrediction = prediction => {    return (      <Text key={prediction.className} style={styles.text}>        {prediction.className}      </Text>    )  }  render() {    const { isTfReady; isModelReady; predictions; image } = this.state    console.log(predictions; \'predictionspredictionspredictionspredictions\')    return (      <View style={styles.container}>        <StatusBar barStyle=\'light-content\' />        <View style={styles.loadingContainer}>          <Text style={styles.text}>            TFJS ready? {isTfReady ? <Text>✅</Text> : \'\'}          </Text>          <View style={styles.loadingModelContainer}>            <Text style={styles.text}>Model ready? </Text>            {isModelReady ? (              <Text style={styles.text}>✅</Text>            ) : (              <ActivityIndicator size=\'small\' />            )}          </View>        </View>        <TouchableOpacity          style={styles.imageWrapper}          onPress={isModelReady ? this.selectImage : undefined}>          {image && <Image source={image} style={styles.imageContainer} />}          {isModelReady && !image && (            <Text style={styles.transparentText}>Tap to choose image</Text>          )}        </TouchableOpacity>        <View style={styles.predictionWrapper}>          {isModelReady && image && (            <Text style={styles.text}>              Predictions: {predictions ? \'\' : \'Predicting...\'}            </Text>          )}          {isModelReady &&            predictions &&            predictions.map(p => this.renderPrediction(p))}        </View>      </View>    )  }}const styles = StyleSheet.create({  container: {    flex: 1;    backgroundColor: \'#171f24\';    alignItems: \'center\'  };  loadingContainer: {    marginTop: 80;    justifyContent: \'center\'  };  text: {    color: \'#ffffff\';    fontSize: 16  };  loadingModelContainer: {    flexDirection: \'row\';    marginTop: 10  };  imageWrapper: {    width: 280;    height: 280;    padding: 10;    borderColor: \'#cf667f\';    borderWidth: 5;    borderStyle: \'dashed\';    marginTop: 40;    marginBottom: 10;    position: \'relative\';    justifyContent: \'center\';    alignItems: \'center\'  };  imageContainer: {    width: 250;    height: 250;    position: \'absolute\';    top: 10;    left: 10;    bottom: 10;    right: 10  };  predictionWrapper: {    height: 100;    width: \'100%\';    flexDirection: \'column\';    alignItems: \'center\'  };  transparentText: {    color: \'#ffffff\';    opacity: 0.7  };  footer: {    marginTop: 40  };  poweredBy: {    fontSize: 20;    color: \'#e69e34\';    marginBottom: 6  };  tfLogo: {    width: 125;    height: 70  }})export default App=====']",Reference Error,Crash,Inconsistent Modules,Mobile,Platform,add support for layers,Add unsupported operator,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5145,Model using `WebGL` backend returns nonsense results while working fine with other backends,2,closed,2021-05-30T15:45:38Z,2021-05-31T11:04:18Z,I have a custom MobileNet-v3/CenterNet model trained on COCO dataset at <https://github.com/vladmandic/mb3-centernet>  and it works perfectly using `tfjs-node` in NodeJS or using `tfjs-backend-cpu` in browser  However; when using `tfjs-backend-webgl` in browser; results are **all fixed-value nonsense**  Input is `[1; 512; 512; 3]`; using pixel range 0..255Output is array of `[x1; y1; x2; y2; score; class]` pre-sorted by score  Example results when using `tfjs-node` or `tfjs-backend-cpu`: There are different boxes marked around different classes```js[  [46.219512939453125; 111.64230346679688; 205.2839813232422; 507.42388916015625; 0.4410598874092102; 0];  [111.56916809082031; 245.57809448242188; 166.20912170410156; 411.6197814941406; 0.35047441720962524; 27];  [75.27127075195312; 411.9134521484375; 110.96424865722656; 507.50360107421875; 0.31512993574142456; 40];  ...]```Example result for the same input when using `tfjs-backend-webgl`:All of the boxes and classes are fixed values!```js[  [174.25; 2.09375; 228.125; 58.3125; 0.6572265625; 64];  [174.25; 2.09375; 228.125; 58.3125; 0.53369140625; 64];  [174.25; 2.09375; 228.125; 58.3125; 0.44091796875; 64];  ...]```Just as a test; I've tried with different input normalizations as well as different versions of weights quantizations - no changes.I cannot test with `tfjs-backend-wasm` due to missing `Mod` op; see #5110  But given it works with `tfjs-backend-cpu`; it's definitely an issue with `webgl` backend  Environment: TFJS 3.6.0 on Ubuntu 20.04 and Chrome 91,"[""my fault; issue was due to model clipping - i forgot i had `tf.ENV.set('WEBGL_FORCE_F16_TEXTURES'; true)` set globally.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5145"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5145"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,Operator,API,change env flag,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,C.2
https://github.com/tensorflow/tfjs/issues/5122,Why I can't load a Keras Model with tfjs-react-native?,2,closed,2021-05-25T09:37:06Z,2021-05-26T09:09:42Z,"I like to load a ML Model in a React-Native App.I converted Keras model model.h5 to model.json and binary weight files using this command.`tensorflowjs_converter --input_format=keras /tmp/model.h5 /tmp/tfjs_model`App Code```import * as tf from ""@tensorflow/tfjs""import { bundleResourceIO } from ""@tensorflow/tfjs-react-native""const modelJson = require(""model.json"")const modelWeights = require(""weights.bin"")await tf.ready().then(async () => {  try {    const modelBundle = bundleResourceIO(modelJson; modelWeights)    console.log(""modelBundle""; modelBundle)    // Output    /**     * {""modelJson"": {""convertedBy"": ""TensorFlow.js Converter v3.6.0""; ""format"": ""layers-model"";     * ""generatedBy"": ""keras v2.4.0""; ""modelTopology"": {""backend"": ""tensorflow"";     * ""keras_version"": ""2.4.0""; ""model_config"": [Object]}; ""weightsManifest"": [[Object]]};     * ""modelWeightsId"": 4}     */    await tf      .loadLayersModel(bundleResourceIO(modelJson; modelWeights))      .then((layersModel) => console.log(""layersModel""; layersModel))    // Output    /**     * [Error: Unknown activation: swish. This may be due to one of the following reasons:      * 1. The activation is defined in Python; in which case it needs to be ported to TensorFlow.js or your JavaScript code.     * 2. The custom activation is defined in JavaScript; but is not registered properly with tf.serialization.registerClass().]    */    await tf      .loadGraphModel(bundleResourceIO(modelJson; modelWeights))      .then((graphModel) => console.log(""graphModel""; graphModel))    // Output    /**     * [TypeError: undefined is not an object (evaluating 'graph.versions.producer')]     */  } catch (error) {    console.log(""error""; error)  }})```I tried changing `modelTopology.model_config.class_name` in `model.json` from Functional to Model still the same result.","['The issue was related to the exported model.json and not related to tfjs and tfjs-react-native. Sorry.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5122"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5122"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Mobile,Platform,add support for layers,Add unsupported operator,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/5040,The Node.js native addon module (tfjs_binding.node) can not be found,4,closed,2021-05-05T17:33:54Z,2021-05-07T02:42:29Z,"**System information**- OS Platform and Distribution: Windows 10 version 1909- TensorFlow.js installed from: `npm i @tensorflow/tfjs-node`- TensorFlow.js version: none- tfjs-node version: `3.6.1`**Describe the problem**I successfully installed tfjs-node after installing `windows-build-tools@5.2.2` (2017) and Python v2.7.2. When I run the app; it says that the native addon module `tfjs_binding.node` can not be found. I've tried install `@mapbox/node-pre-gyp@1.0.4` globally but it doesn't help. When I run rebuild from source; it returns errors as below. I've also read and tried all solutions for previous relative issues but none works; including copying `deps/libs` to `lib/napi-v7`.**Any other info / logs**- `node app.js````node-pre-gyp info This Node instance does not support builds for N-API version 8node-pre-gyp info This Node instance does not support builds for N-API version 8C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\dist\index.js:49    throw new Error(""The Node.js native addon module (tfjs_binding.node) can not "" +    ^Error: The Node.js native addon module (tfjs_binding.node) can not be found at path: C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node.Please run command 'npm rebuild @tensorflow/tfjs-node --build-addon-from-source' to rebuild the native addon module.If you have problem with building the addon module; please check https://github.com/tensorflow/tfjs/blob/master/tfjs-node/WINDOWS_TROUBLESHOOTING.md or file an issue.    at Object.<anonymous> (C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\dist\index.js:49:11)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)    at Module.load (node:internal/modules/cjs/loader:973:32)    at Function.Module._load (node:internal/modules/cjs/loader:813:14)    at Module.require (node:internal/modules/cjs/loader:997:19)    at require (node:internal/modules/cjs/helpers:92:18)    at Object.<anonymous> (C:\Users\User1\Desktop\mnist\app.js:2:1)    at Module._compile (node:internal/modules/cjs/loader:1108:14)    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1137:10)```- `npm rebuild @tensorflow/tfjs-node build-addon-from-source````...npm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-buildnpm ERR! node-pre-gyp info it worked if it ends with oknpm ERR! node-pre-gyp info using node-pre-gyp@0.14.0npm ERR! node-pre-gyp info using node@15.5.0 | win32 | x64npm ERR! node-pre-gyp WARN Using needle for node-pre-gyp https downloadnpm ERR! node-pre-gyp info This Node instance does not support builds for N-API version 8npm ERR! node-pre-gyp info check checked for ""C:\Users\User1\Desktop\mnist\node_modules\@tensorflow\tfjs-node\lib\napi-v7\tfjs_binding.node"" (not found)npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp http 404 https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp WARN Tried to download(404): https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! node-pre-gyp WARN Pre-built binaries not found for @tensorflow/tfjs-node@3.6.1 and node@15.5.0 (node-v88 ABI; unknown) (falling back to source compile with node-gyp)npm ERR! node-pre-gyp http 404 status code downloading tarball https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v7/3.6.1/CPU-windows-3.6.1.zipnpm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@7.1.2npm ERR! gyp info using node@15.5.0 | win32 | x64npm ERR! gyp info oknpm ERR! node-pre-gyp info This Node instance does not support builds for N-API version 8npm ERR! gyp info it worked if it ends with oknpm ERR! gyp info using node-gyp@7.1.2npm ERR! gyp info using node@15.5.0 | win32 | x64npm ERR! gyp info find Python using Python version 2.7.2 found at ""C:\Python27\python.exe""npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS msvs_version was set from command line or npm confignpm ERR! gyp ERR! find VS - looking for Visual Studio version 2017npm ERR! gyp ERR! find VS VCINSTALLDIR not set; not running in VS Command Promptnpm ERR! gyp ERR! find VS checking VS2017 (15.9.28307.1500) found at:npm ERR! gyp ERR! find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools""npm ERR! gyp ERR! find VS - found ""Visual Studio C++ core features""npm ERR! gyp ERR! find VS - found VC++ toolset: v141npm ERR! gyp ERR! find VS - missing any Windows SDKnpm ERR! gyp ERR! find VS could not find a version of Visual Studio 2017 or newer to usenpm ERR! gyp ERR! find VS looking for Visual Studio 2015npm ERR! gyp ERR! find VS - not foundnpm ERR! gyp ERR! find VS not looking for VS2013 as it is only supported up to Node.js 8npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS valid versions for msvs_version:npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VSnpm ERR! gyp ERR! configure errornpm ERR! gyp ERR! stack Error: Could not find any Visual Studio installation to usenpm ERR! gyp ERR! stack     at VisualStudioFinder.fail (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16npm ERR! gyp ERR! stack     at VisualStudioFinder.findVisualStudio2013 (C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:351:14)npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:372:16npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7npm ERR! gyp ERR! stack     at C:\Program Files\nodejs\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16npm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:340:5)npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:376:20)npm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1063:16)...```","['@uahnbu The root cause seems to be related to your visual studio installation:npm ERR! gyp ERR! find VS **************************************************************npm ERR! gyp ERR! find VS You need to install the latest version of Visual Studionpm ERR! gyp ERR! find VS including the ""Desktop development with C++"" workload.npm ERR! gyp ERR! find VS For more information consult the documentation at:npm ERR! gyp ERR! find VS https://github.com/nodejs/node-gyp#on-windowsnpm ERR! gyp ERR! find VS **************************************************************====='; ""@pyu10055 There's no instruction to install VS on `tfjs-node` gitpage; which is really obstructive. The `node-gyp` link instructed that **_VS Build 2017_** with **_Visual C++ build tools_** is enough as a replacement for **_Desktop development with C++_**. I've tried installing both **_VS Build_** and **_VS Community latest 2019_** version from external website for 3GB instead of `npm install` above but I can't install `tfjs-node` at all with them. I've currently installed the following components:![image](https://user-images.githubusercontent.com/27907396/117342345-560aa900-aecd-11eb-88ce-0bfca8f6beef.png)The error says that Windows SDK is missing and I don't really know what it means.=====""; ""Hmm; I think that I have to restart the computer to make VS Build Tools (2017) work. That's weird but it works now. The 2019 versions doesn't work still although it has forced a restart.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5040"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5040"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,TF(CPU),Backend,,,framework,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Native Addon Module Not Found""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/tensorflow/tfjs/issues/5013,TFJS converter mismanages EfficientDet models 2,6,closed,2021-04-29T14:16:17Z,2021-06-08T19:05:56Z,"Efficientdet  can't be used ; it relates to #3903; but seems to be a completely different oneFor this  [model](https://tfhub.dev/tensorflow/efficientdet/d0/1)## Converted the following way```tensorflowjs_converter \    --input_format=tf_hub  \    --output_format=tfjs_graph_model \    --signature_name=serving_default \    --saved_model_tags=serve \    ./efficientdet_d0_1 ./web_model_tfhub```## Loaded and executed as follows in a browser```const model = await tf.loadGraphModel(""web_model_tfhub/model.json"");t = tf.browser.fromPixels(img).expandDims(0)let output = await model.executeAsync(t)```## Results in```Uncaught Error: FusedConv2d and DepthwiseConv2d with BiasAdd must have one extra argument: bias.    at UO (tfjs@2.7.0:17)    at tfjs@2.7.0:17    at tfjs@2.7.0:17    at tfjs@2.7.0:17    at e.t.scopedRun (tfjs@2.7.0:17)    at e.t.tidy (tfjs@2.7.0:17)    at lx (tfjs@2.7.0:17)    at tfjs@2.7.0:17    at KO (tfjs@2.7.0:17)    at p (tfjs@2.7.0:17)```Same when the model is taken from the official repo.Tried both tfjs version 2.7.0 ; which was released just after #3903 was resolved ; and 3.6.0; the result is the same ","['@partus I was able to convert and run the TFJS model without errors. Please make sure your tensorflowjs pip is the same version (3.6.0) as the tfjs library. ====='; 'What versions of other packages are relevant? : I am running it on Arch linux; hence the most recent tensorflow + python as well; e.g. tensorflow with python 3.9... is not officially supported. Which are the important dependencies so that I will I will put it into a cantainer; Hopefully  veryify that it’s fine. Then with the next step we might verify the issue in a container with upgraded versions ====='; ""@partus Have you resolved this issue; I've countered the same one. Change tfjs version and tensorflowjs won't work```Uncaught Error: FusedConv2d and DepthwiseConv2d with BiasAdd must have one extra argument: bias.```=====""; '@Linh0704 ; @pyu10055  I was able to convert and run the tfhub version with python 3.8 instead of 3.9; and tfjs 3.6.0 ; but I was unable to run the model from the official repo. Did not dive into it further as decided to run the model on a server.====='; 'Closing the issue ; please mention@ if you need further help to reopen. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5013"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5013"">No</a>=====']",Reference Error,Crash,Inconsistent Modules,Operator,API,change framework version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.2
https://github.com/tensorflow/tfjs/issues/4975,Build fails when upgrading @tensorflow/tfjs-react-native from 0.2.1 to 0.5.0,2,closed,2021-04-23T06:00:11Z,2021-04-28T06:42:01Z,"<em>This is a build/installation issue. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): macOS Big Sur 11.2.1- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy M10- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version:  1.5.1- CUDA/cuDNN version:**Steps executed before running into the problem**Installed @tensorflow/tfjs-react-native in my project according to the [docs](https://www.npmjs.com/package/@tensorflow/tfjs-react-native) provided.The project did run successfully.After that; I tried to update the version of @tensorflow/tfjs-react-native from 0.2.1 to 0.5.0 when these errors occurred.![1](https://user-images.githubusercontent.com/8373729/115825224-bbb75980-a426-11eb-88bd-26e5ae276cda.png)module ""expo-camera"" not found. I installed it.![2](https://user-images.githubusercontent.com/8373729/115825300-d38edd80-a426-11eb-91b3-2f98d4b5af2b.png) module @tensorflow/tfjs-backend-webgl not found. I installed it.Then the build failed with this error message.![3](https://user-images.githubusercontent.com/8373729/115825398-00db8b80-a427-11eb-9189-8e2caadde1ed.jpeg)Please help me in solving this issue.Thank you.","['@farhan2056 You are using an older version of TFJS (TensorFlow.js version: 1.5.1); try to upgrade the TFJS version to the latest (3.5.0)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4975"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4975"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Inconsistent Modules,Mobile,Platform,change framework version,Changing version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Missing dependencies""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.2
https://github.com/tensorflow/tfjs/issues/4965,React Native: Crash on tf.ready() when running on Android's JS Engine,8,closed,2021-04-21T22:50:00Z,2021-09-27T21:08:02Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Mac OS Big Sur 11.2.3- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Any Android device. I have tested on OnePlus 5 and Samsung Galaxy S10- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs-react-native - TensorFlow.js version (use command below): 3.4.0- Browser version: The problem happens when JS is run on the phone- Tensorflow.js Converter Version: The problem happens on initialization of tensor flow**Describe the current behavior**We have integrated TFJS into a react native app. It works fine with remote debugging as the JS is running on the computer in chrome. However; when we try to run on the phone itself it crashes the app as soon as tf.ready() is called. Following is the log from logcatA/libc: Fatal signal 7 (SIGBUS); code 1 (BUS_ADRALN); fault addr 0xfe8aa0003f3 in tid 25679 (mqt_js)The above only happens for Android. We have also tried with another JS engine https://www.npmjs.com/package/react-native-v8 but that also fails with a crashIt works fine on iPhone both in remote debugging and running on the phone itself.**Describe the expected behavior**TensorflowJS should initialize**Standalone code to reproduce the issue**Add tensorflowjs imports to App.js in a react native app and call await tf.ready()**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.A/libc: Fatal signal 7 (SIGBUS); code 1 (BUS_ADRALN); fault addr 0xfe8aa0003f3 in tid 25679 (mqt_js),"[""Sorry Matt; re-assigning this for you now since I don't have an Android device:) Thanks!=====""; 'Here is the package.json. This may provide an insight of the environment tfjs is running in[package.json.zip](https://github.com/tensorflow/tfjs/files/6368794/package.json.zip)====='; ""I'm also having this issue right now; any solution?I'm using tfjs 3.7.0 and react native 0.62.3=====""; 'i am facing same issue. @jinjingforever  @mattsoulanille my app crash when await tf.ready() called.i am using react native cli.""dependencies"": {    ""@react-native-async-storage/async-storage"": ""^1.15.7"";    ""@tensorflow/tfjs"": ""^3.8.0"";    ""@tensorflow/tfjs-react-native"": ""^0.6.0"";    ""expo-camera"": ""^11.2.2"";    ""expo-constants"": ""^11.0.1"";    ""expo-gl"": ""^10.4.2"";    ""expo-gl-cpp"": ""^10.4.1"";    ""react"": ""17.0.2"";    ""react-native"": ""0.65.1"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-unimodules"": ""^0.14.6""  };====='; 'I am also facing the same issue```    ""@react-native-async-storage/async-storage"": ""^1.15.8"";    ""@tensorflow/tfjs"": ""^3.9.0"";    ""@tensorflow/tfjs-react-native"": ""^0.7.0"";    ""expo-camera"": ""^11.2.2"";    ""expo-gl"": ""^10.4.2"";    ""expo-gl-cpp"": ""^10.4.1"";    ""react"": ""17.0.2"";    ""react-native"": ""0.65.1"";    ""react-native-ble-manager"": ""^7.6.1"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-svg"": ""^12.1.1"";    ""react-native-unimodules"": ""^0.14.9"";    ""react-native-vector-icons"": ""^8.1.0""; ```A screenshot of my device logs![Screenshot 2021-09-26 at 14 51 19](https://user-images.githubusercontent.com/2452503/134808787-ca121a0f-fa1a-4ded-991a-6014258b2ae8.png)====='; 'Hi @wvanooijen92 @AhmedRazaVerge; I recently checked in some tfjs react native example apps [here](https://github.com/tensorflow/tfjs-examples/tree/master/react-native). Please take a look. One thing to note is that react native 0.65.1 does not work well with expo-gl 10.4.2 (would crash Android as you guys mentioned). Please use react native 0.63.2. Thank you!====='; '@jinjingforever thanks for the update; I was just about to leave you a message. That is exactly what I did today; I was able to get it running. Fyi I am on 0.63.4 now and it seems to be working. Thank you!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4965"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4965"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Cross-platform App Framework Incompatibility,Mobile,Platform,change react-native version,change react-native version,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",C,D.3
https://github.com/tensorflow/tfjs/issues/4902,Memory Leak in TensorCamera when remounting repeatedly,1,open,2021-04-04T02:53:54Z,2021-04-09T06:32:04Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): React Native 0.63.4- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 8 Plus- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 3.0.0- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**I have the [TensorCamera](https://js.tensorflow.org/api_react_native/0.5.0/#cameraWithTensors) for React Native mounted on a tab in my [Tab Navigator](https://reactnavigation.org/docs/bottom-tab-navigator/). Switching between tabs causes the total memory usage to increase every time TensorCamera is remounted. I've narrowed the memory leak down to the Tensorcamera as replacing it with Expo Camera caused no memory leaks when switching between tabs. I've also commented out the onReady function so it shouldn't be related to undisposed tensors.**Describe the expected behavior**Re-mouting the TensorCamera shouldn't cause memory leaks**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.```const TensorCamera = cameraWithTensors(Camera);```...```               <TensorCamera                  ref={cameraRef}                  style={styles.camera}                  type={Camera.Constants.Type.back}                  zoom={0}                  cameraTextureHeight={textureDims.height}                  cameraTextureWidth={textureDims.width}                  resizeHeight={tensorDims.height}                  resizeWidth={tensorDims.width}                  resizeDepth={3}                  onReady={handleCameraStream}                  autorender={true}                />```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.![IMG_2767](https://user-images.githubusercontent.com/44853346/113497214-e40b0280-94cf-11eb-8042-7723f8b6c887.PNG)![IMG_2770](https://user-images.githubusercontent.com/44853346/113497220-e79e8980-94cf-11eb-85bb-5e5af8ef86c0.PNG),"[""I face the same issue; i opened a ticket here : https://github.com/tensorflow/tfjs/issues/4846I then tracked down the leak to expo-gl's method createCameraTextureAsync() ( used in TensorCamera ). I wrote a ticket there too : https://github.com/expo/expo/issues/12468=====""]",Memory Leak,Poor Performance,Cross-platform App Framework Incompatibility,Mobile,Platform,memory management,Add API usage for memory management,framework,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,D.3
https://github.com/tensorflow/tfjs/issues/4871,running some models using webgl is 10x slower than using nodejs,16,closed,2021-03-27T13:03:32Z,2021-11-26T16:45:18Z,Model executes in NodeJS using `tensorflow` backend in ~100ms; but above 1sec in browser using `WebGL` backendThat is 10x difference in performance between NodeJS and `WebGL`  I've tried to figure out why a model is executing an order of magnitude slower than expected using `WebGL` backend;  but profiler info for it makes no sense```const t0 = performance.now();const res = await tf.profile(() => model.executeAsync(input));const t1 = performance.now();const wallTime = t1 - t0;const kernelTime = res.kernels.reduce((a; b) => a += b.kernelTimeMs; 0);```wallTime is 900-1200ms  kernelTime is ~20ms  I'm re-running inference on the same input twice and looking at second run to allow for warmup time of `WebGL` backend (shader compile; etc.)  I even tried `tf.enableDebugMode()` and I still don't see anything that gets even close to overall wall time  And I have no idea where is time spent?  Model in question: <https://github.com/vladmandic/nanodet>Environment: TFJS 3.3.0 on Ubuntu 20.10 and Chrome 89,"['@vladmandic Have you tried with our [benchmark tool](https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html); I tried the file in model-tfjs-graph-m/ directory; it runs around 79ms on my mac pro 2018; but the profile time is around 1sec.The profile time is different from the real inference time; since it is wait for GL time query for each kernel.====='; ""haven't used that tool before; will try.what confuses me that i have two variations of the model `model-tfjs-graph-m` and `model-tfjs-graph-g`  they both have comparable execution time in `nodejs`; but; *g-variation* executes 10x faster than *m-variation* using `webgl` backend - and i can't figure out why the *m-variation* would be so slow in browser and totally fine otherwise.=====""; ""Using benchmark tool; setting backend to `webgl` and leaving everything else at default values:Model URL: <https://raw.githubusercontent.com/vladmandic/nanodet/main/model-tfjs-graph-m/nanodet.json>Result: Subsequent average(5 runs)\t114.8 msModel URL: <https://raw.githubusercontent.com/vladmandic/nanodet/main/model-tfjs-graph-g/nanodet.json>Result: Subsequent average(5 runs)\t65.6 msSo difference is ~2x which is expected and not even close to ~10x that is seen in reality  I still don't know why the model inference for m-variation is that slow - about 5x slower than expected  =====""; 'Another model with same symptoms: <https://github.com/vladmandic/efficientpose>Looking at simplest ""I-Lite"" variation:<https://raw.githubusercontent.com/vladmandic/efficientpose/main/model-tfjs-graph-i-lite/efficientpose.json>Wall times (measured in JS exactly before and after call to `executeAsync()`):- **NodeJS**: ~150 ms- **WASM**: ~350 ms- **WebGL**: ~500 msAnd what does benchmark tool report?- **WASM**: subsequent average = 300 ms    So only slight overhead between benchmark and actual wall time - completely within reason- **WebGL**: subsequent average = 30 ms    So where does the 15x difference between benchmark and wall time come from???  And even if we say that kernel benchmarking is just wrong;    there is no chance that model should execute this slowly with `WebGL` backendThere is something really wrong with WebGL when executing specific models====='; '@vladmandic The profile() method has much more overhead in WebGL comparing to WASM. It aims to measure kernel speed; no to measure inference speed. It adds GL time query at each kernel execution. The average inference time in benchmark is the more accurate measurement for deployment.====='; ""@pyu10055 I only brought up `profile()` method because I was trying to figure out why some models are  5-10x slower for inference using `WebGL` backend than using NodeJS `tensorflow` backend (that should definitely not be the case; especially how low-profile models are) - that is the key question  And when possible; I've tried using same models through other frameworks - for example; inference time using NodeJS with `tensorflow` backend is comparable to TF2 when used directly via Python or even with PyTorch - so no issues other than `WebGL`!But given they profile shows total kernel time 15x lower than inference time; how can I get to bottom of this?I've posted here two very simple models; `NanoDet` and `EfficientDet` that exhibit that behavior  =====""; '@vladmandic Can you clarify on how are you benchmarking WebGL that give you 15x slowness?For example what is the total kernel time you are referring to?and how are you measuring the inference time?====='; ""inference time is simple:in browser:```jsconst t0 = performance.now();const res = await model.executeAsync(input));const t1 = performance.now();const inferenceTime = Math.round(t1 - t0); // elapsed time in ms```and in nodejs:```jsconst t0 = process.hrtime.bigint();const res = await model.executeAsync(input)); // also can be model.predict(input);const t1 = process.hrtime.bigint();const inferenceTime = Math.round((t1 - t0) / 1000 /1000); // to convert to ms```and i'm only measuring subsequent runs; skipping first run so webgl has time to warmup (compile&upload shaders)for most models; webgl is fast and it's the best option (since node-gpu doesn't work due to obsolete cuda dependency from tf1).however; for some models (i've provided examples); webgl is 10x slower than nodejs.but running `profile()` shows nothing useful.=====""; "">  (since node-gpu doesn't work due to obsolete cuda dependency from tf1).Note that tfjs-node-gpu [will soon be updated to use TF 2](https://github.com/tensorflow/tfjs/pull/4810); and it can be used with TF 1.15.=====""; ""@DanielRuf yup; i'm monitoring that. still need to fix whatever is wrong with `webgl`=====""; '@vladmandic based on my test the inference time on webgl (79ms) for nanodet is faster than node.js (100ms).I am still confused about your claim that webgl is 10x slower than nodejs.To reconciling the conceptual difference; can you provide the exact inference time you getting and how are you measure that for a single model? thanks====='; ""i've re-converted the models while changing some resource types  it seems there was some serious clipping in the converted model (original implementation heavily uses `int64`)  which was causing trouble for `webgl` inference as the model was branching differently and doing far more work  right now; results are as expected - `webgl` is faster than `nodejs` in pretty much all cases  sorry for the false leads and time wasted; this took me a while to figure out and outputs of `profile()` were throwing me off  =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4871"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4871"">No</a>====='; '@vladmandic @pyu10055  any comments on this issue https://github.com/tensorflow/tfjs/issues/4907. Do you notice any problems between benchmark tool and own profiling for first run?====='; ""> i've re-converted the models while changing some resource types it seems there was some serious clipping in the converted model (original implementation heavily uses `int64`) which was causing trouble for `webgl` inference as the model was branching differently and doing far more workHey @vladmandic ; I think I am facing similar issues. I'm trying to do some inference with an EfficientNet model; and I get slower inference time with the webgl backend. Could you please give some details on how you re-converted the models to make it work ? Thank you!=====""; ""@OlivierMns I never stated I saw the problem with EfficientNet; I spoke about EfficientDet. But in general; issue in my case was clipping due to quantization - try unquantized model first. Also; EfficientNet is notoriously slow for warmup; I suggest to enable WebGL uniforms which speeds up warmup 2-4x (no difference on actual inference)> tf.ENV.set('WEBGL_USE_SHAPES_UNIFORMS'; true);=====""]",Slow Execution,Poor Performance,Improper Model Attribute,WebGL,Backend,re-converter model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",B.1.1,C.2
https://github.com/tensorflow/tfjs/issues/4870,Cannot debug model containing Infinity as tensor value,2,closed,2021-03-27T12:33:41Z,2021-03-27T20:09:01Z,Original issue in #4764 was that models containing **Infinity** value could not run.Work by @mattsoulanille in #4779 and #4783 fixed that issue.However; if TFJS debug mode is enabled using `tf.enableDebugMode();`;  TFJS still runs checks and prevents model from running:```logUncaught (in promise) Error: A tensor of type float32 being uploaded contains -Infinity.    at checkConversionForErrors (util_base.ts:477)    at toTypedArray (util.ts:50)    at makeTensor (tensor_ops_util.ts:73)    at tensor (tensor.ts:56)    at Object.decodeWeights (io_utils.ts:225)    at GraphModel.loadSync (graph_model.ts:160)    at GraphModel.load (graph_model.ts:134)    at async loadGraphModel (graph_model.ts:440)    at async load12 (nanodet.ts:15)    at async Human.load (human.ts:243)```Model in question is at <https://github.com/vladmandic/nanodet>Environment: TFJS 3.3.0 using WebGL backend on Chrome 89 on Windows 10,"[""closing this as it turns out; this is not an incompatibility with INF; it's the INF that results due to overflow when executing model quantized to float16.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4870"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4870"">No</a>=====']",Data & Model Error,Crash,Improper Model Attribute,WebGL,Backend,re-converter model,Changing model,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3"",
    ""specific_type"": ""D.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,C.2
https://github.com/tensorflow/tfjs/issues/4809,[tfjs-react-native] tf.sparseToDense is broken,3,closed,2021-03-12T15:41:21Z,2021-03-12T17:43:12Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Android 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Nexus 7 Emulator- TensorFlow.js installed from (npm or script link): https://www.npmjs.com/package/@tensorflow/tfjs-react-native- TensorFlow.js version (use command below): (core@3.2.0; react-native@0.5.0)- Browser version: n/a- Tensorflow.js Converter Version: n/a**Describe the current behavior**```jsconst indices = tf.tensor1d([4; 5; 6; 1; 2; 3]; 'int32');const values = tf.tensor1d([10; 11; 12; 13; 14; 15]; 'float32');const shape = [8];tf.sparseToDense(indices; values; shape).print();// outputs: [0; 1; 0; 0; 0; 0; 0; 0]```**Describe the expected behavior**Expected output: `[0; 13; 14; 15; 10; 11; 12; 0]`It works flawlessly with the CPU backend.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"[""I've just found that my emulator doesn't support webgl2. This can be the cause of the issue. I'll reopen it once I'll confirm that it doesn't work on a device.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4809"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4809"">No</a>====='; 'Resolved.For if anyone will stumble upon this. `tfjs-react-native` requires GLES 3 / WebGL 2. You can enable it in Android SDK Emulator via:```echo ""GLESDynamicVersion = on"" >> ~/.android/advancedFeatures.ini```I tested this only under Linux. Your host GPU should support at least OpenGL ES 3.2; you can check if it does via:```$ sudo apt install mesa-utils$ glxinfo | grep \'version.*OpenGL\'```=====']",Incorrect Functionality,Incorrect Functionality,Device Incompatibility,Device,Device,enable webgl in device,Modifying the value of environment variable,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Incorrect Output""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",D,D.1
https://github.com/tensorflow/tfjs/issues/4415,examples: tfjs-examples/firebase-object-detection-node fails to run,3,open,2020-12-15T22:54:34Z,2021-01-14T19:47:46Z,Sample code in https://github.com/tensorflow/tfjs-examples/tree/master/firebase-object-detection-node not working with Node v15.4.0; `/predict` endpoint fails with error:```(node:13194) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(); Buffer.allocUnsafe(); or Buffer.from() methods instead.(Use `node --trace-deprecation ...` to show where the warning was created)/Users/ahmetb/workspace/junk/tfjs-examples/firebase-object-detection-node/functions/index.js:59    const buf = req.files.file[0].buffer;                                  ^TypeError: Cannot read property 'buffer' of undefined    at Busboy.<anonymous> (/Users/ahmetb/workspace/junk/tfjs-examples/firebase-object-detection-node/functions/index.js:59:35)    at Busboy.emit (node:events:388:22)    at Busboy.emit (/Users/ahmetb/workspace/junk/tfjs-examples/firebase-object-detection-node/functions/node_modules/busboy/lib/main.js:37:33)    at /Users/ahmetb/workspace/junk/tfjs-examples/firebase-object-detection-node/functions/node_modules/busboy/lib/types/multipart.js:304:17    at processTicksAndRejections (node:internal/process/task_queues:75:11)```,"['@ahmetb can you try with latest 2.8.1 v ? thank you====='; ""Doesn't work. I had to refactor the code and remove some statements like `busboy.end` for this to work. Highly recommend reproing it yourself. Overall code quality also isn't great. =====""; ""If you're interested in seeing the patch that made it work for me on Node 15.x; take a look at here (I removed Firebase Functions boilerplate; as I was converting it to a container image for Cloud Run).https://www.diffchecker.com/AZUp1iPX I know no Node.js at all; but I figured the choices around storing values and flowing data between closures were quite odd. So I took a stab at that; too.=====""]",Reference Error,Crash,Dependency Error,TF(CPU),Backend,change npm/node version,Changing version,framework,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,B.2
https://github.com/tensorflow/tfjs/issues/4343,frequent getaddrinfo ENOTFOUND tfhub.dev on models,15,closed,2020-12-03T11:54:34Z,2021-01-29T22:27:07Z,"**System information**this is just to do with loading models; basic TF js code.**Describe the current behavior**frequently when loading models; I'll get an error like the below:```Uncaught Promise Rejection FetchError: request to https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1/group1-shard1of7?tfjs-format=file failed; reason: getaddrinfo ENOTFOUND tfhub.dev    at ClientRequest.<anonymous> (/Users/dc/dev/nlp/TfClassifier/node_modules/node-fetch/lib/index.js:1461:11)    at ClientRequest.emit (events.js:314:20)    at TLSSocket.socketErrorListener (_http_client.js:469:9)    at TLSSocket.emit (events.js:314:20)    at emitErrorNT (internal/streams/destroy.js:100:8)    at emitErrorCloseNT (internal/streams/destroy.js:68:3)    at processTicksAndRejections (internal/process/task_queues.js:80:21) {  type: 'system';  errno: 'ENOTFOUND';  code: 'ENOTFOUND'}```**Describe the expected behavior**Ideal would be a way to download the models locally for both speed and reliability.otherwise; less network fails!**Standalone code to reproduce the issue**```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"";await sentenceEncoder.load()```but this is hard to repro as it seems to be a networking issues**Other info / logs** Include any logs or source code that would be helpful tosee above","['@dcsan can you please share code snippet and model which you are trying to load ?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '@rthadur there is a code snippet above```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"";await sentenceEncoder.load()```around here is where it times out.is there any way to grab/cache the models locally rather than a network import every runtime?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Not stale botOn Thu; Dec 17; 2020; 8:47 PM tensorflow-butler[bot] <notifications@github.com> wrote:> This issue has been automatically marked as stale because it has not had> recent activity. It will be closed in 7 days if no further activity occurs.> Thank you.>> —> You are receiving this because you were mentioned.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4343#issuecomment-747865182>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AAD5PUXGKEODVNGRP62AC3DSVLNGXANCNFSM4UL3Z5YQ>> .>====='; 'you can save the model using model.save(""localstorage://model"") locally. ====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Actually I was asking about the encoder not my own models; as from the example above.```import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"";await sentenceEncoder.load()```I don\'t think `sentenceEncoder.save()` will work in the same way as for a model. I do of course use that already for saving and caching models.====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'not stale :D====='; 'May i know from which location you are trying to load tfhub models ?====='; 'import * as sentenceEncoder from ""@tensorflow-models/universal-sentence-encoder"";like written in the issue above====='; 'some links does not work in china ; I would like to know if you are accessing from china and one more thing we will not be supporting custom save function in universal-sentence-encoder in near future.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4343"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4343"">No</a>====='; ""No I'm not accessing from China.So there is no way to pull the model down locally; it has to be re-downloaded for every run?meaning this function:`await sentenceEncoder.load()`pulls a large model over the network every time I run my process?=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,WebGL,Backend,change model path,Modifying model file path/extension,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Fetch Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/tensorflow/tfjs/issues/4293,AutoML Vision Tensorflowjs -  Cannot read property 'reduce' of undefined,4,closed,2020-11-22T01:22:38Z,2020-11-23T22:42:58Z,"When using the latest model created by automl for tensorflowjs for image classification or object detection; getting the following error when running:`const model = await tf.automl.loadImageClassification('/model/model.json');`Getting the following error:```Uncaught (in promise) TypeError: Cannot read property 'reduce' of undefined    at e.t.transformGraph (tfjs:17)    at e.t.loadSync (tfjs:17)    at e.<anonymous> (tfjs:17)    at u (tfjs:17)    at Generator._invoke (tfjs:17)    at Generator.forEach.e.<computed> [as next] (tfjs:17)    at Um (tfjs:17)    at o (tfjs:17)```Using```""@tensorflow/tfjs"": ""^2.7.0"";""@tensorflow/tfjs-automl"": ""^1.0.0""```Full code```<script src=""https://unpkg.com/@tensorflow/tfjs""></script><script src=""https://unpkg.com/@tensorflow/tfjs-automl""></script><img id=""daisy"" src=""test2.jpg""><script>    async function run() {        const model = await tf.automl.loadImageClassification('/model/model.json');        const image = document.getElementById('daisy');        const predictions = await model.classify(image);        console.log(predictions);        // Show the resulting object on the page.        const pre = document.createElement('pre');        pre.textContent = JSON.stringify(predictions; null; 2);        document.body.append(pre);    }    run();</script>```","['Lets close this in and track it at single place [here](https://github.com/tensorflow/tfjs/issues/4293)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">No</a>====='; 'sorry wrong issue reference earlier ; same issue exists [here](https://github.com/tensorflow/tfjs/issues/4105) ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4293"">No</a>=====']",Reference Error,Crash,Incorrect Code Logic,Model API,API,retrain model,Changing model,framework,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/5872,[wasm] Incorrect check for 'process' in pre.js,1,closed,2021-11-19T19:25:42Z,2021-11-22T23:05:03Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version (use command below): b2f3664a32a0119a1acf93756cc8b930b8de860f- Browser version:- Tensorflow.js Converter Version:**Describe the current behavior**#5852 made it impossible to load tfjs-backend-wasm on the web. It checks for the existence of the global `process` variable incorrectly in `pre.js`:```javascriptif (process && process.listeners) {...}```This was not caught by unit tests because Karma adds a global `process` variable to the browser.**Describe the expected behavior**`pre.js` correctly checks for the existence of a `process` variable.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5872"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5872"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,Wasm,Backend,condition replacer,condition replacer,framework,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/395,Facing Error at withFaceLandmarks().,2,open,2019-08-27T06:06:22Z,2019-09-23T08:04:55Z,"Hi @justadudewhohacks ;I am really impressed on your incredible work!First of all; I am currently working on biometric login screen that using RequireJs framework. I use faceapi.js for face detection & face recognition task. I manage to setup well in RequireJs and able run basic face detection but i facing error when add `withFaceLandmarks()`. below is my codeThis my model import:```var model = 'scripts/vendors/faceapi/model';await faceapi.loadMtcnnModel(model)await faceapi.loadFaceLandmarkModel(model)await faceapi.loadFaceRecognitionModel(model)```This is my main process at `onPlay()````async function onPlay() {	const videoEl = $('#inputVideo').get(0)	if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded()){		console.log(""Model not load or webcam off"");		return setTimeout(() => onPlay())	}	const options = new faceapi.MtcnnOptions(mtcnnForwardParams)	const result = await faceapi.detectSingleFace(videoEl; options).withFaceLandmarks().withFaceDescriptor()	console.log(result);		if (result) {		const canvas = $('#overlay').get(0)		const dims = faceapi.matchDimensions(canvas; videoEl; true)		faceapi.draw.drawDetections(canvas; faceapi.resizeResults(result; dims))	}	setTimeout(() => onPlay())}```This are error from chrome console> Uncaught (in promise) TypeError: Cannot read property 'expandDims' of undefined>     at stack_ (face-api.js:100)>     at stack (face-api.js:100)>     at face-api.js:3590>     at face-api.js:100>     at t.scopedRun (face-api.js:100)>     at t.tidy (face-api.js:100)>     at Fe (face-api.js:100)>     at FaceLandmark68NetBase.postProcess (face-api.js:3575)>     at face-api.js:3603>     at face-api.js:100However when i use below code it work well without any error```const result = await faceapi.detectSingleFace(videoEl; options)//.withFaceLandmarks().withFaceDescriptor()console.log(result);```Hope you can help me.Thanks in advance!","[""`expandDims` is a method provided by tfjs-core. Since FaceLandmark68NetBase.postProcess does not call expandDims directly; this error comes from some tfjs internal code. Might be some issue with requirejs + tfjs; but not sure what's going on there.What's the last face-api.js call where the error is thrown; what code is executed in face-api.js:3590?=====""; 'This code produce that error.`await faceapi.detectSingleFace(videoEl; options).withFaceLandmarks().withFaceDescriptor()`=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.5
https://github.com/hiukim/mind-ar-js/issues/10,Issue trying new tensorflow version,7,closed,2021-01-17T20:04:48Z,2021-12-11T03:21:07Z,"I tested the new version with tensorflow; i get this error:```javascriptA-Frame Version: 1.0.4 (Date 2020-02-05; Commit #2b359246)mindar.prod.js:12858 three Version (https://github.com/supermedium/three.js): ^0.111.6mindar.prod.js:12858 WebVR Polyfill Version: ^0.10.10mindar.prod.js:12680 THREE.WebGLRenderer: WEBGL_depth_texture extension not supported.get @ mindar.prod.js:12680mindar.prod.js:12680 THREE.WebGLRenderer: OES_texture_float_linear extension not supported.get @ mindar.prod.js:12680mindar.prod.js:12658 video ready... <video autoplay muted playsinline style=​""position:​ absolute;​ top:​ 0px;​ left:​ -32px;​ z-index:​ -2;​ width:​ 384px;​ height:​ 512px;​"" width=​""480"" height=​""640"">​</video>​mindar.prod.js:10332 Could not get context for WebGL version 2mindar.prod.js:10348 1    2        precision highp float;3        precision highp int;4        precision highp sampler2D;5        varying vec2 resultUV;6        7        const vec2 halfCR = vec2(0.5; 0.5);8    9        struct ivec510       {11         int x;12         int y;13         int z;14         int w;15         int u;16       };mindar.prod.js:10348 Fragment shader compilation failed.mindar.prod.js:10348  17                                                                                                      mindar.prod.js:10348 18       struct ivec619       {20         int x;21         int y;22         int z;23         int w;24         int u;25         int v;26       };27   28       uniform float NAN;29       30         #define isnan(value) isnan_custom(value)31         bool isnan_custom(float val) {32           return (val > 0. || val < 1. || val == 0.) ? false : true;33         }34         bvec4 isnan_custom(vec4 val) {35           return bvec4(isnan(val.x); isnan(val.y); isnan(val.z); isnan(val.w));36         }37       38       39         uniform float INFINITY;40   41         bool isinf(float val) {42           return abs(val) == INFINITY;43         }44         bvec4 isinf(vec4 val) {45           return equal(abs(val); vec4(INFINITY));46         }47       48       49         int round(float value) {50           return int(floor(value + 0.5));51         }52   53         ivec4 round(vec4 value) {54           return ivec4(floor(value + vec4(0.5)));55         }56       57   58       int imod(int x; int y) {59         return x - y * (x / y);60       }61   62       int idiv(int a; int b; float sign) {63         int res = a / b;64         int mod = imod(a; b);65         if (sign < 0. && mod != 0) {66           res -= 1;67         }68         return res;69       }70   71       //Based on the work of Dave Hoskins72       //https://www.shadertoy.com/view/4djSRW73       #define HASHSCALE1 443.897574       float random(float seed){75         vec2 p = resultUV * seed;76         vec3 p3  = fract(vec3(p.xyx) * HASHSCALE1);77         p3 += dot(p3; p3.yzx + 19.19);78         return fract((p3.x + p3.y) * p3.z);79       }80   81       82   vec2 uvFromFlat(int texNumR; int texNumC; int index) {83     int texR = index / texNumC;84     int texC = index - texR * texNumC;85     return (vec2(texC; texR) + halfCR) / vec2(texNumC; texNumR);86   }87   vec2 packedUVfrom1D(int texNumR; int texNumC; int index) {88     int texelIndex = index / 2;89     int texR = texelIndex / texNumC;90     int texC = texelIndex - texR * texNumC;91     return (vec2(texC; texR) + halfCR) / vec2(texNumC; texNumR);92   }93   94       95   vec2 packedUVfrom2D(int texelsInLogicalRow; int texNumR;96     int texNumC; int row; int col) {97     int texelIndex = (row / 2) * texelsInLogicalRow + (col / 2);98     int texR = texelIndex / texNumC;99     int texC = texelIndex - texR * texNumC;100    return (vec2(texC; texR) + halfCR) / vec2(texNumC; texNumR);101  }102  103      104  vec2 packedUVfrom3D(int texNumR; int texNumC;105      int texelsInBatch; int texelsInLogicalRow; int b;106      int row; int col) {107    int index = b * texelsInBatch + (row / 2) * texelsInLogicalRow + (col / 2);108    int texR = index / texNumC;109    int texC = index - texR * texNumC;110    return (vec2(texC; texR) + halfCR) / vec2(texNumC; texNumR);111  }112  113    114  115      float sampleTexture(sampler2D textureSampler; vec2 uv) {116        return texture2D(textureSampler; uv).r;117      }118    119  120      void setOutput(vec4 val) {121        gl_FragColor = val;122      }123    124  uniform sampler2D A;125  uniform int offsetA;126  127      ivec3 getOutputCoords() {128        ivec2 resTexRC = ivec2(resultUV.yx *129                               vec2(160; 160));130        int index = resTexRC.x * 160 + resTexRC.y;131  132        int b = index / 25500;133        index -= b * 25500;134  135        int r = 2 * (index / 510);136        int c = imod(index; 510) * 2;137  138        return ivec3(b; r; c);139      }140    141  142          143      float getA(int row; int col) {144        vec2 uv = (vec2(col; row) + halfCR) / vec2(1020.0; 100.0);145        return sampleTexture(A; uv);146      }147    148          float getA(int row; int col; int depth) {149            return getA(col; depth);150          }151        152      float getAAtOutCoords() {153        ivec3 coords = getOutputCoords();154        155        return getA(coords.x; coords.y; coords.z);156      }157    158  159        ivec3 outCoordsFromFlatIndex(int index) {160          int r = index / 102000; index -= r * 102000;int c = index / 1020; int d = index - c * 1020;161          return ivec3(r; c; d);162        }163  164        void main() {165          ivec2 resTexRC = ivec2(resultUV.yx *166            vec2(160; 160));167          int index = 4 * (resTexRC.x * 160 + resTexRC.y);168  169          vec4 result = vec4(0.);170  171          for (int i=0; i<4; i++) {172            int flatIndex = index + i;173            ivec3 rc = outCoordsFromFlatIndex(flatIndex);174            result[i] = getA(rc.x; rc.y; rc.z);175          }176  177          gl_FragColor = result;178        }179      mindar.prod.js:10348 Uncaught (in promise) Error: Failed to compile fragment shader.    at mf (mindar.prod.js:10348)    at dg.createProgram (mindar.prod.js:10911)    at mindar.prod.js:11440    at mindar.prod.js:11440    at vb.getAndSaveBinary (mindar.prod.js:11440)    at vb.runWebGLProgram (mindar.prod.js:11440)    at vb.decode (mindar.prod.js:11440)    at vb.getValuesFromTexture (mindar.prod.js:11440)    at vb.readSync (mindar.prod.js:11440)    at g.readSync (mindar.prod.js:4141)```Tested with a Wiko View; with Android 7.1.2. Chrome 87.0.4280.141.","[""@kalwalt Sorry; I still couldn't replicate this error.Maybe can you try running any tensorflowjs example first? https://www.tensorflow.org/js/demos=====""; ""> @kalwalt Sorry; I still couldn't replicate this error.> > Maybe can you try running any tensorflowjs example first? https://www.tensorflow.org/js/demosi will try again also with other devices and will test the examples you suggested :slightly_smiling_face: =====""; 'Thanks a lot! appreciated! ====='; '@kalwalt Just bumped the tfjs version from 2.x to 3.6. Not sure it helps; but worth a try.====='; '> @kalwalt Just bumped the tfjs version from 2.x to 3.6. Not sure it helps; but worth a try.Thank you for the news! I will test It.====='; '@hiukim with my new Oppo A72 works fine all the examples! i will test also with my oldest device.====='; '@kalwalt good news! thanks for testing.=====']",Browser & Device Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.4,A.5
https://github.com/hiukim/mind-ar-js/issues/19,Basic example doesn't display on iPhone,18,closed,2021-03-30T10:13:56Z,2021-12-11T03:21:28Z,"Hi;When I try to see the live demo of the basic example on my iPhone (6SPlus - iOS 14.4.1);I stay on the waiting animation and if I take a look at the debug console; I got <img width=""1215"" alt=""Capture d’écran 2021-03-30 à 12 06 33"" src=""https://user-images.githubusercontent.com/10347315/112972970-1ecc0e00-9151-11eb-8079-3251804fe2f0.png"">Any idea ?","[""Probably something went wrong when running the webgl code.  Unfortunately; it's hard for me to debug without the device.If you are interested; I can give some pointers and see if we can fix it together. But it requires you running the development build and tweak some codes and locate the errors.=====""; ""Yes; I think I can help you. I'm a developper; well; not a web one rather anative application one ;o)I have basic knowledge of debugging on my iOS device (using Safari).So tell me everything !o)Le mer. 31 mars 2021 à 01:57; HiuKim Yuen ***@***.***> aécrit :> Probably something went wrong when running the webgl code. Unfortunately;> it's hard for me to debug without the device.>> If you are interested; I can give some pointers and see if we can fix it> together. But it requires you running the development build and tweak some> codes and locate the errors.>> —> You are receiving this because you authored the thread.> Reply to this email directly; view it on GitHub> <https://github.com/hiukim/mind-ar-js/issues/19#issuecomment-810653105>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ACO6GM5S26UDRECNT5MTGWDTGJQONANCNFSM42BWMCZQ>> .>=====""; ""@Brun0oO Thanks a lot. It would be greatly appreciated. First of all; you need to clone the repo and start development by running `npm run watch`. It will compile a development build of the library; i.e. `./dist-dev/mindar.js`. It will also watch for file changes and recompile continuously.Then we can start debugging. I have just created a new debug example in the repo. `./examples/debug.html`. It's a minimal example that try to detect a simple image. You can run it as static html page. If everything runs successfully; you should see some console logs like this. I guess the first step is to use your desktop browser to verify you can get something like this.![Screenshot 2021-03-31 at 11 04 01 AM](https://user-images.githubusercontent.com/459126/113190201-e55ad780-9210-11eb-8be3-e424eb2ec4e6.png)After that; we can start debugging with the problematic device. We just need to locate the exact line that is causing the error. As you can see from debug.html; the entry point is `./src/controller.js`. Just a quick tip here: It's almost certain that the issue is related to the webgl code. After you dive into the codebase; you will find that there are codes that look like:```userCode:`\tvoid main() {\t  ivec2 coords = getOutputCoords();\t  int texR = coords[0];\t  int texC = coords[1];\t  vec2 uv = (vec2(texC; texR) + halfCR) / vec2(${width}.0; ${height}.0);\t  vec4 values = ${textureMethod}(A; uv);\t  setOutput((values.r + values.g + values.b) * 255.0 / 3.0);\t}      ````Basically; these code are compiled and run in gpu. The issue is that they failed to compile. Let's try to locate the error and we'll go from there.=====""; 'I got some problems when I try to call the first ""npm install"" command in the main directory...  I understand the problem is installing ""node-sass@4.5.0"" on my computer (macbook pro with macOS Catalina). Here the complete log of ""npm install"" call (I truncated the long part which; in my opinion; does not give any relevant information):```0 info it worked if it ends with ok1 verbose cli [ \'/usr/local/bin/node\'; \'/usr/local/bin/npm\'; \'install\' ]2 info using npm@6.5.0-next.03 info using node@v11.6.04 verbose npm-session 70cb30c857581bab5 silly install runPreinstallTopLevelLifecycles6 silly preinstall mind-ar@0.3.27 info lifecycle mind-ar@0.3.2~preinstall: mind-ar@0.3.28 silly install loadCurrentTree9 silly install readLocalPackageData10 timing stage:loadCurrentTree Completed in 11ms11 silly install loadIdealTree12 silly install cloneCurrentTreeToIdealTree13 timing stage:loadIdealTree:cloneCurrentTree Completed in 1ms14 silly install loadShrinkwrap15 timing stage:loadIdealTree:loadShrinkwrap Completed in 249ms16 silly install loadAllDepsIntoIdealTree17 timing stage:loadIdealTree:loadAllDepsIntoIdealTree Completed in 351ms18 timing stage:loadIdealTree Completed in 715ms19 silly currentTree mind-ar@0.3.220 silly idealTree mind-ar@0.3.220 silly idealTree ├── @discoveryjs/json-ext@0.5.220 silly idealTree ├── @msgpack/msgpack@1.12.220 silly idealTree ├── @npmcli/move-file@1.1.220 silly idealTree ├── @tensorflow/tfjs-backend-cpu@2.8.620 silly idealTree ├── @tensorflow/tfjs-backend-webgl@2.8.620 silly idealTree ├── @tensorflow/tfjs-converter@2.8.6...9967 silly saveTree │ └── webpack-sources@2.2.09967 silly saveTree └─┬ worker-loader@2.0.09967 silly saveTree   ├─┬ loader-utils@1.4.09967 silly saveTree   │ └── json5@1.0.19967 silly saveTree   └─┬ schema-utils@0.4.79967 silly saveTree     ├── ajv-keywords@3.4.19967 silly saveTree     └── ajv@6.12.29968 verbose stack Error: node-sass@5.0.0 postinstall: `node scripts/build.js`9968 verbose stack Exit status 19968 verbose stack     at EventEmitter.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/index.js:301:16)9968 verbose stack     at EventEmitter.emit (events.js:188:13)9968 verbose stack     at ChildProcess.<anonymous> (/usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/lib/spawn.js:55:14)9968 verbose stack     at ChildProcess.emit (events.js:188:13)9968 verbose stack     at maybeClose (internal/child_process.js:978:16)9968 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)9969 verbose pkgid node-sass@5.0.09970 verbose cwd /Users/bruno/GitHub/mind-ar-js9971 verbose Darwin 19.6.09972 verbose argv ""/usr/local/bin/node"" ""/usr/local/bin/npm"" ""install""9973 verbose node v11.6.09974 verbose npm  v6.5.0-next.09975 error code ELIFECYCLE9976 error errno 19977 error node-sass@5.0.0 postinstall: `node scripts/build.js`9977 error Exit status 19978 error Failed at the node-sass@5.0.0 postinstall script.9978 error This is probably not a problem with npm. There is likely additional logging output above.9979 verbose exit [ 1; true ]```I\'ve followed the advices of this post : https://stackoverflow.com/questions/48298361/npm-install-failed-at-the-node-sass4-5-0-postinstall-script without success.  Here the paste of the console log of ""npm install node-sass@4.5.0"" (once the previous advices has been applied) :```0 ✓ bruno@BRUNO ~/GitHub/mind-ar-js $ npm i node-sass@4.5.0npm WARN deprecated request@2.88.2: request has been deprecated; see https://github.com/request/request/issues/3142npm WARN deprecated har-validator@5.1.5: this library is no longer supported> node-sass@4.5.0 install /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass> node scripts/install.jsDownloading binary from https://github.com/sass/node-sass/releases/download/v4.5.0/darwin-x64-67_binding.nodeCannot download ""https://github.com/sass/node-sass/releases/download/v4.5.0/darwin-x64-67_binding.node"": HTTP error 404 Not FoundHint: If github.com is not accessible in your location      try setting a proxy via HTTP_PROXY; e.g.       export HTTP_PROXY=http://example.com:1234or configure npm proxy via      npm config set proxy http://example.com:8080> node-sass@4.5.0 postinstall /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass> node scripts/build.jsBuilding: /usr/local/bin/node /Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=gyp info it worked if it ends with okgyp verb cli [ \'/usr/local/bin/node\';gyp verb cli   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js\';gyp verb cli   \'rebuild\';gyp verb cli   \'--verbose\';gyp verb cli   \'--libsass_ext=\';gyp verb cli   \'--libsass_cflags=\';gyp verb cli   \'--libsass_ldflags=\';gyp verb cli   \'--libsass_library=\' ]gyp info using node-gyp@3.8.0gyp info using node@11.6.0 | darwin | x64gyp verb command rebuild []gyp verb command clean []gyp verb clean removing ""build"" directorygyp verb command configure []gyp verb check python checking for Python executable ""python2"" in the PATHgyp verb `which` succeeded python2 /usr/local/bin/python2gyp verb check python version `/usr/local/bin/python2 -c ""import sys; print ""2.7.15gyp verb check python version .%s.%s"" % sys.version_info[:3];""` returned: %jgyp verb get node dir no --target version specified; falling back to host node version: 11.6.0gyp verb command install [ \'11.6.0\' ]gyp verb install input version string ""11.6.0""gyp verb install installing version: 11.6.0gyp verb install --ensure was passed; so won\'t reinstall if already installedgyp verb install version is already installed; need to check ""installVersion""gyp verb got ""installVersion"" 9gyp verb needs ""installVersion"" 9gyp verb install version is goodgyp verb get node dir target node version installed: 11.6.0gyp verb build dir attempting to create ""build"" dir: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/buildgyp verb build dir ""build"" dir needed to be created? /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/buildgyp verb build/config.gypi creating config filegyp verb build/config.gypi writing out config file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/build/config.gypigyp verb config.gypi checking for gypi file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/config.gypigyp verb common.gypi checking for gypi file: /Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/common.gypigyp verb gyp gyp format was not specified; forcing ""make""gyp info spawn /usr/local/bin/python2gyp info spawn args [ \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/gyp_main.py\';gyp info spawn args   \'binding.gyp\';gyp info spawn args   \'-f\';gyp info spawn args   \'make\';gyp info spawn args   \'-I\';gyp info spawn args   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-sass/build/config.gypi\';gyp info spawn args   \'-I\';gyp info spawn args   \'/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/addon.gypi\';gyp info spawn args   \'-I\';gyp info spawn args   \'/Users/bruno/.node-gyp/11.6.0/include/node/common.gypi\';gyp info spawn args   \'-Dlibrary=shared_library\';gyp info spawn args   \'-Dvisibility=default\';gyp info spawn args   \'-Dnode_root_dir=/Users/bruno/.node-gyp/11.6.0\';gyp info spawn args   \'-Dnode_gyp_dir=/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp\';gyp info spawn args   \'-Dnode_lib_file=/Users/bruno/.node-gyp/11.6.0/<(target_arch)/node.lib\';gyp info spawn args   \'-Dmodule_root_dir=/Users/bruno/GitHub/mind-ar-js/node_modules/node-sass\';gyp info spawn args   \'-Dnode_engine=v8\';gyp info spawn args   \'--depth=.\';gyp info spawn args   \'--no-parallel\';gyp info spawn args   \'--generator-output\';gyp info spawn args   \'build\';gyp info spawn args   \'-Goutput_dir=.\' ]ERROR:root:code for hash md5 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type md5ERROR:root:code for hash sha1 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha1ERROR:root:code for hash sha224 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha224ERROR:root:code for hash sha256 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha256ERROR:root:code for hash sha384 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha384ERROR:root:code for hash sha512 was not found.Traceback (most recent call last):  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 147; in <module>    globals()[__func_name] = __get_hash(__func_name)  File ""/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py""; line 97; in __get_builtin_constructor    raise ValueError(\'unsupported hash type \' + name)ValueError: unsupported hash type sha512Traceback (most recent call last):  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/gyp_main.py""; line 16; in <module>    sys.exit(gyp.script_main())  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py""; line 545; in script_main    return main(sys.argv[1:])  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py""; line 538; in main    return gyp_main(args)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py""; line 514; in gyp_main    options.duplicate_basename_check)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/__init__.py""; line 98; in Load    generator.CalculateVariables(default_variables; params)  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/generator/make.py""; line 79; in CalculateVariables    import gyp.generator.xcode as xcode_generator  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/generator/xcode.py""; line 7; in <module>    import gyp.xcodeproj_file  File ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/gyp/pylib/gyp/xcodeproj_file.py""; line 152; in <module>    _new_sha1 = hashlib.sha1AttributeError: \'module\' object has no attribute \'sha1\'gyp ERR! configure error gyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack     at ChildProcess.onCpExit (/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/lib/configure.js:345:16)gyp ERR! stack     at ChildProcess.emit (events.js:188:13)gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)gyp ERR! System Darwin 19.6.0gyp ERR! command ""/usr/local/bin/node"" ""/Users/bruno/GitHub/mind-ar-js/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild"" ""--verbose"" ""--libsass_ext="" ""--libsass_cflags="" ""--libsass_ldflags="" ""--libsass_library=""gyp ERR! cwd /Users/bruno/GitHub/mind-ar-js/node_modules/node-sassgyp ERR! node -v v11.6.0gyp ERR! node-gyp -v v3.8.0gyp ERR! not ok Build failed with error code: 1npm WARN sass-loader@11.0.1 requires a peer of fibers@>= 3.1.0 but none is installed. You must install peer dependencies yourself.npm WARN sass-loader@11.0.1 requires a peer of sass@^1.3.0 but none is installed. You must install peer dependencies yourself.npm WARN worker-loader@2.0.0 requires a peer of webpack@^3.0.0 || ^4.0.0-alpha.0 || ^4.0.0 but none is installed. You must install peer dependencies yourself.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! node-sass@4.5.0 postinstall: `node scripts/build.js`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the node-sass@4.5.0 postinstall script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR!     /Users/bruno/.npm/_logs/2021-04-02T07_07_05_352Z-debug.log```ƪ(ړײ)\u200eƪ\u200b\u200b====='; 'Hurrah! I managed to install by updating node to v15.13.0; clearing the cache and calling again ""npm install"". I continue following your instructions...====='; ""On my desktop; I got this :![Capture d’écran 2021-04-02 à 17 03 57](https://user-images.githubusercontent.com/10347315/113427599-9ca04b80-93d5-11eb-9b95-91c751fb3ceb.png)On my iPhone; something strange appears; It's difficult to debug remotely with Safari. The debug window disappears quite often in the development menu on my desktop Safari; I don't know why yet. The rare times I was able to access the remote debug; I could see that mindar.js was not loaded in this specific case...=====""; '@Brun0oO We are in the right track. Your desktop log is perfect. I remember that disappearing problem happens to me before. You can try this tool for remote debug: https://remotejs.com/====='; 'Using _remotejs_; on my iOS device; I got this :<img width=""1508"" alt=""Capture d’écran 2021-04-06 à 08 48 29"" src=""https://user-images.githubusercontent.com/10347315/113670492-7d473e00-96b5-11eb-855e-6c96247f816b.png"">====='; 'New attempt with useful (perhaps!) informations :<img width=""1531"" alt=""Capture d’écran 2021-04-06 à 15 34 07"" src=""https://user-images.githubusercontent.com/10347315/113719398-dc28a980-96ed-11eb-9fb2-bfac42c7bb0a.png"">====='; '> New attempt with useful (perhaps!) informations :> <img alt=""Capture d’écran 2021-04-06 à 15 34 07"" width=""1531"" src=""https://user-images.githubusercontent.com/10347315/113719398-dc28a980-96ed-11eb-9fb2-bfac42c7bb0a.png"">Just want to double check. Your iOS safari works on the example; right?  ====='; ""Oups!; it works now; I don't understand what i did different!=====""; 'Does it work on debug.html or the basic example? ====='; 'I do not understand anything : now it doesn\'t work anymore on debug.htmlI tried on example1.html without success; here the screenshots :![IMG_D666F990FF79-1](https://user-images.githubusercontent.com/10347315/113781795-870e8700-9731-11eb-8dc5-a2c1196dbacd.jpeg)<img width=""1531"" alt=""Capture d’écran 2021-04-06 à 23 36 30"" src=""https://user-images.githubusercontent.com/10347315/113781855-9beb1a80-9731-11eb-9ed2-a9a1a6f2f6df.png"">I noticed I have the same error as for displaying debug.htmlbut I didn\'t dream (at least I think); I (only) saw the debug.html page with the camera display once.What is the expected behaviour of the debug.html page; should I see the camera if all goes well?====='; ""You should see an empty page in debug.html.  Just the console.log. Anyway; I think we were expecting error; so that's fine.Can you quickly try one thing? In the `src/image-target/input-loader.js` make it use the old method `loadInput` methodBasically; just rename `_loadInput` to `loadInput` and `loadInput` to `_loadInput`https://github.com/hiukim/mind-ar-js/blob/ba310848f10acf3941bdccd949b3120b14ddda96/src/image-target/input-loader.js#L29-L48=====""; 'I\'ve reversed _loadInput and loadInput.I took the opportunity to add some traces (logs like "">0""; "">1""; ...)  :```javascript...class InputLoader {  constructor(width; height) {    console.log("">0"");    this.width = width;    this.height = height;    this.texShape = [height; width];    const context = document.createElement(\'canvas\').getContext(\'2d\');    context.canvas.width = width;    context.canvas.height = height;    this.context = context;    this.program = this.buildProgram(width; height);    const backend = tf.backend();    console.log("">1"");    console.log(backend);    this.tempPixelHandle = backend.makeTensorInfo(this.texShape; \'int32\');    // warning!!!    // usage type should be TextureUsage.PIXELS; but tfjs didn\'t export this enum type; so we hard-coded 2 here     //   i.e. backend.texData.get(tempPixelHandle.dataId).usage = TextureUsage.PIXELS;    console.log("">2"");    backend.texData.get(this.tempPixelHandle.dataId).usage = 2;    console.log("">3"");  }  // old method  loadInput(input) {    console.log("">4"");    return tf.tidy(() => {      let inputImage = tf.browser.fromPixels(input);      inputImage = inputImage.mean(2);      return inputImage;    });  }  // input is instance of HTMLVideoElement or HTMLImageElement  _loadInput(input) {    this.context.drawImage(input; 0; 0; this.width; this.height);    const backend = tf.backend();    backend.gpgpu.uploadPixelDataToTexture(backend.getTexture(this.tempPixelHandle.dataId); this.context.canvas);    const res = backend.compileAndRun(this.program; [this.tempPixelHandle]; \'float32\');    //backend.disposeData(tempPixelHandle.dataId);    return res;  } ...```Here the results :<img width=""1487"" alt=""Capture d’écran 2021-04-07 à 10 33 12"" src=""https://user-images.githubusercontent.com/10347315/113836117-e483f180-978c-11eb-95f6-1d18465f7fe6.png"">I\'ve tried to inspect the backend variable but I did not succeed :* Using console.log(JSON.stringify(backend; null; 2); I got a ""JSON.stringify cannot serialize cyclic structures."" message.* Using console.dir(backend); I got ... nothing !It is as if the tensorflow backend was not yet initialized when the InputLoader constructor was called... Do we have to wait until the tensorflow engine is ready?I noticed that the loadInput function was not called when the exception is raised.====='; 'ah.. let\'s try comment out these part of code as well. we don\'t need them for the old loadInput ```/*this.program = this.buildProgram(width; height);    const backend = tf.backend();    console.log("">1"");    console.log(backend);    this.tempPixelHandle = backend.makeTensorInfo(this.texShape; \'int32\');    // warning!!!    // usage type should be TextureUsage.PIXELS; but tfjs didn\'t export this enum type; so we hard-coded 2 here     //   i.e. backend.texData.get(tempPixelHandle.dataId).usage = TextureUsage.PIXELS;    console.log("">2"");    backend.texData.get(this.tempPixelHandle.dataId).usage = 2;    console.log("">3"");*/```====='; 'We are going forward ;o) now; loadInput is called; new screenshot :<img width=""1531"" alt=""Capture d’écran 2021-04-07 à 20 11 44"" src=""https://user-images.githubusercontent.com/10347315/113914234-9ea34980-97dd-11eb-93d3-b74caf62fbbe.png"">====='; 'seems like the tensorflow webgl backend has failed to initialize; which is also indicated in the Warning. Probably need to dig deeper into that.=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Initialization Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",C,A.4
https://github.com/infinitered/nsfwjs-mobile/issues/4,Load Local Model,3,closed,2019-09-28T00:22:04Z,2019-10-23T23:55:49Z,Currently; the model is being fetched over the network every time the app is loaded. This both expensive and inconvenient.Depends on https://github.com/infinitered/nsfwjs/issues/170,['Hi; I was trying the example. Nice work by the way and thanks for sharing the code.Can the model (since to my understanding is pre-trained) be loaded from the assets file?====='; 'It can be for web projects but since it is trying to do a network fetch the existing system will not work for loading the model locally in React Native; which is why this links to the NSFWJS ticket this issue depends on. ====='; 'Thanks! My perspective was for a React Native app. ====='],Slow Execution,Poor Performance,Incorrect Code Logic,,,parameter modifier,Modify API Parameter usage,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.1.1,A.4
https://github.com/infinitered/nsfwjs/issues/16,Bias in model,14,closed,2019-02-20T15:51:44Z,2019-03-05T17:53:55Z,There seems to be a bias of when an image contains a woman with _any_ skin showing; the model will mark those as porn; even when they clearly aren't.There's some examples in [this twitter post](https://twitter.com/_Nec/status/1098140532751118338); but I could reproduce it with different images too.,"['I am finding very similar issues as well:![image](https://user-images.githubusercontent.com/2205537/53109390-c6080100-3506-11e9-8680-3e1ada6dbb46.png)![image](https://user-images.githubusercontent.com/2205537/53109862-cead0700-3507-11e9-9553-f87c1944b0e5.png)====='; 'Agreed.  I have a plan to help fix this.  One thing to note is that faces near microphones might exacerbate this bias.  The original training data was provided by https://github.com/alexkimxyz/nsfw_data_scrapperThe input images seem to slightly off due to the subreddits that fueled the dataset. Hopefully; I can counterbalance this and contribute back to that original repo.====='; 'I tried going the opposite approach and seeing how it classified women who were predominantly covered up; I used two different types of women; two with women in traditional Muslim Garb; and one with traditional Quaker garb. I then tried out an image that (by my analysis) would be considered more ""sexy""; and the results were woefully incorrect.![collage](https://user-images.githubusercontent.com/11723093/53112657-c8218e00-350d-11e9-9f84-04f1f15effa0.png)As you can see; the predominantly covered group; are all flagged as porn with high certainty; despite being anything but. Though interestingly; they are also all flagged with a high ""Neutral"" rating as well.  VS this woman in a revealing; but technically not ""porn quality"" swimsuit.<img width=""314"" alt=""screen shot 2019-02-20 at 12 39 07 pm"" src=""https://user-images.githubusercontent.com/11723093/53112689-da033100-350d-11e9-89b7-5c919e99cf21.png"">This would be a good lesson for the ML Model; as you could use this result to test false positives; where if the ""Porn"" & ""Sexy"" categories are the two highest; it is most likely porn. Obviously; this would still allow for some false positives labeled as ""porn"" to fall through the cracks; but it\'s a step in the right direction. Similarly; if the two highest categories are ""Porn"" and ""Neutral""; then there is a good chance that it is also a false positive as well.====='; ""Great examples so far!  This perspective is very valuable.  I filed an issue on the data-repo.#### Side note:I've gotten lots of requests to share the dataset (for a bunch of reasons; not all for science).   But with some public curation of the training data; the model can significantly improve.  Adding a few hundred photos of false positives into the test/train will help.  As a call to action; I would love to crowsource-fix this problem.  Feel free to let me know you have a zip of false-positive images to add; or if you'd be interested in curation. =====""; ""> ... The original training data was provided by https://github.com/alexkimxyz/nsfw_data_scrapper@GantMan it'd be nice if you credited the original repo https://github.com/alexkimxyz/nsfw_data_scraper in both your medium blog post and this repo. Thanks.=====""; ""You read my mind @alexkimxyz - you've been added to both! Sorry for the delay=====""; 'Bearded man covered in snow:<img width=""544"" alt=""screen shot 2019-02-20 at 8 26 04 pm"" src=""https://user-images.githubusercontent.com/3649460/53138992-c677ba80-354d-11e9-8ef7-367460c99f48.png"">====='; 'My fingers ![微信图片_20190222112357.png](https://i.loli.net/2019/02/22/5c6f6b91cfcd2.png)====='; 'Hah; is there a subreddit of hands?  If so we can add it. ====='; 'Some more examples here: https://twitter.com/JustTenDads/status/1098502194196697088 (Jeffrey goldblum)====='; 'RE: Jeff Goldblum![image](https://user-images.githubusercontent.com/997157/53252118-d5608900-3683-11e9-9d82-ad1f2f5234c4.png)🤣 ====='; ""## MODEL BIAS UPDATE:**I'm working on a newly trained model.  This takes DAYS on my home computer.  This will take time**I appreciate everyone's feedback!  You'll be happy to know I'm grabbing some updated training photos to hopefully help fix this bias... except for Jeff Goldblum; that stays.   I'm going to lock this conversation for two reasons; which I hope are fair and clear.1.  I'm actively working on an update which will have new biases; so noted biases on the existing model won't help.  A new ticket can be created when the new model is in place.2. This model is trained on GIGS of data; so single examples actually don't help as much as providing an excellent subreddit to the data scraper repo.   The workflow is like so:### [NSFW Data Scraper](https://github.com/alexkimxyz/nsfw_data_scrapper) ➡️ [NSFW Model](https://github.com/gantman/nsfw_model)  ➡️  NSFW JS 🥳So if you find category bias; please contribute back to the data scraper; it's easy!  Those contributions will make it to the model; which will make it to NSFW JS.=====""; ""I'm happy to announce after a lot of tweaking; adjusting; and hours of additional training.  I've increased the test set to reflect a broader array of images; AND increased accuracy to 93% on that larger dataset.   ## While model bias is unavoidable; and accuracy is below a human's potentialI'm very happy to continue working on improving NSFW JSThank you all for your feedback; and I hope you find the following results pleasing.![image](https://user-images.githubusercontent.com/997157/53823053-97424f80-3f36-11e9-9f69-7659991dae0d.png)### I consider this more accurate than the original classification![image](https://user-images.githubusercontent.com/997157/53823080-a88b5c00-3f36-11e9-8901-58d38831c178.png)![image](https://user-images.githubusercontent.com/997157/53823108-b5a84b00-3f36-11e9-96d3-e517696d5d35.png)![image](https://user-images.githubusercontent.com/997157/53823144-c658c100-3f36-11e9-8d4f-4bcb30b49e4a.png)![image](https://user-images.githubusercontent.com/997157/53823177-d53f7380-3f36-11e9-858c-a4f09b8822b0.png)![image](https://user-images.githubusercontent.com/997157/53823197-e4262600-3f36-11e9-8fe6-d0134629f5bb.png)![image](https://user-images.githubusercontent.com/997157/53823402-51d25200-3f37-11e9-8101-2c074bf1d0b2.png)### And unfortunately; 1 false positive.   Which is of-course to be expected.  93% accuracy is not nearly as good as a human.![image](https://user-images.githubusercontent.com/997157/53823494-83e3b400-3f37-11e9-9412-136af9d43a3e.png)If someone can suggest a large set of images of people singing into microphones; I'd love to add that to the training data.  Please supply zip files with hundreds of examples in appropriate folders; if you'd like to help!## ThanksI'd like to thank everyone who helped in the spirit of making something creative and useful.  Please keep in mind this is not my full-time job.  I build things like this as a passion to help the community.This model is about to be released.  If you have old results; please clear your cache and make sure you get the latest model.=====""; ""## BONUS:   Jeff Goldblum is fixed... also I'm sad about this 😿![image](https://user-images.githubusercontent.com/997157/53824387-73343d80-3f39-11e9-9235-1261e69b75d2.png)![image](https://user-images.githubusercontent.com/997157/53824362-64e62180-3f39-11e9-9be0-411cca1122de.png)![image](https://user-images.githubusercontent.com/997157/53824415-83e4b380-3f39-11e9-84f9-e400d2d74e38.png)![image](https://user-images.githubusercontent.com/997157/53824446-9363fc80-3f39-11e9-9774-8eb3df3149c6.png)=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/324,Confused with the models - false positives,28,open,2020-05-01T11:51:46Z,2021-01-03T13:19:34Z,"Hello;I'm using these model files:https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/model( I think it's the same as https://s3.amazonaws.com/nsfwdetector/min_nsfwjs.zip )and I'm having several false positives.100% non-adult face pictures get flagged as 90%+ pornI tested also with the non-min model: https://s3.amazonaws.com/nsfwdetector/nsfwjs.zipBut; from the few examples I tried; it didn't change much at all; and some were even worse.I see that in this issue: https://github.com/infinitered/nsfwjs/issues/276You added some new ""DAG 93"" model.Is this one supposed to have better accuracy?What model is the best; and what should I use for best accuracy; please?What is the difference between using the model provided in README (first link above);and using the DAG one with { type: ""graph"" } ?Are there pros and cons?Thank you very much.","['The new one is supposed to have better accuracy.Use the DAG 93.  Can you provide some false positives here?   I can elaborate on the pros/cons; but I want to see if it fixes your issue first.And sorry; I think that s3 link is outdated.====='; 'Hello;Thank you.I\'ve just updated to the latest version of NSFWJS; and then loaded the DAG 93 model.Tested against a number of false positives; and the ""porn"" % decreased to much more acceptable values for sure.However; I tested a few images that are 100% porn and they would give 99% on the older model; but with the DAG93 they give 72.95% for one; 56.37% for another and 34.13% for the other; which doesn\'t make sense. There\'s a 4th image that I tested that it\'s not porn; but a semi-naked lady; and that one gave 89.48%.With my examples; the new model just doesn\'t seem accurate at all.Happy to give you examples; please give me your email so I can send privately to you.====='; ""Our latest model was provided by @TechnikEmpire so I'm tagging em here.My email is Gant and the domain is infinite.red=====""; '## **NSFW Post!!!**My post is NSFW because I\'m going to use some real talk here to get to the nitty gritty details of what I did differently.Alright so with the very latest model that I trained; there was a significant change.## **Changes to Training Data**I used the same training data that @GantMan used in older models; but I decided to clean it up. What I did was I loaded Yahoo Open NSFW model and I deleted all images from the `porn` category that had a classification score of `10%` or less.I then used Yahoo model to move all images out of the `sexy` category into the `porn` folder that had a score of `60%` or more.This translated into several thousand files for each category. There were thousands of files in the `sexy` category that were full blown pornography. Images of sexual penetration; just plain snapshots of porn movies; full body nudity; close up of genitalia. That\'s not `sexy`; that\'s porn.There were probably a couple thousand images in the `porn` folder of bukakke. As humans we recognize what this is; but really it\'s just a closeup of a face. To a neural network it\'s just a face; and Yahoo\'s Open NSFW model confirmed this by classifying them as non-pornographic. Those images were blown away; so I\'m really surprised that plain faces are coming up as porn.There were also a couple thousand images of extremely obscure pornography. I took a peek and some of them I was like ""this is a completely benign image""; then I\'d see a 3x3 pixel area of a penis hanging out of someone\'s pants. This is going to throw off the neural network; at least for training purposes. Those images were blown away.The `sexy` category should now only really trigger when there\'s provocative images. i.e. someone rolling around on a boat in a thong and bra. Nudity should come up in the pornography category.## **Remarks About Changing Accuracy**With regards to the scores for categories changing; this isn\'t necessarily a bad thing. The way this model is to be used is that the highest scored class wins; and that\'s how neural networks like this are scored as well. Top-1 and Top-5 accuracy. Even if a porn image is split up like so:```Porn: 21%Sexy: 20%Neutral: 20%Drawing: 20%Hentai: 19%```The model is still accurate. It\'s not even necessarily indicative of a problem in the neural network.## **Conclusion**What I believe happened here is twofold. First; I believe I\'ve overfit the model by over-training it. I didn\'t notice this before; but your remarks made me take a second glance and it seems to be a bit overfit.If you look at the posted Tensorflow output on [this issue](https://github.com/GantMan/nsfw_model/pull/56#issue-410906105); you can see that train loss is decreasing while validation loss is increasing in the final train iteration.Second; one-byte quantization would exacerbate this issue.I think I\'ll run a new training session and get a new model published with one less iteration because it seems it was that last iteration that pushed the model over the edge.Thanks for bringing this up!====='; ""Oh yeah I forgot; I also started the training session with training just the final softmax layer; then fine tuning for 5 consecutive sessions; then 2 more iterations with a highly reduced LR. I'll just train it the way I did the previous model and report back.=====""; ""@GantMan - just sent the email. Please let me know if you didn't receive.Thanks @TechnikEmpire - please keep me updated!=====""; ""I've started training so sometime this evening I'll do a PR on the model repo.=====""; ""I got the email @ghnp5  thanks so much.@TechnikEmpire - thanks for checking for overfitting!  I'll look forward to your update.=====""; 'Hey - any news? :)Thanks!====='; ""Yeah I've retrained; I'll check your submissions against the new model before I publish.I also got side tracked cause I'm not happy with this 93% ceiling fine tuning so I'm training mobilenet v3 large from scratch.=====""; ""Great work  guys!!!```Porn: 21%Sexy: 20%Neutral: 20%Drawing: 20%Hentai: 19%```I am fairly new at this and I don't know what a good confidence score for each of the categories is.I have used the saved_model.tflite  found https://github.com/GantMan/nsfw_model/releases/tag/1.1.0  on Android/IOS with impressive results. However; I still don't know how to choose a good confidence score.=====""; 'Hello;Any news about this? :)Thanks!====='; ""@ghnp5 Yeah sorry for going MIA; I got tied up with a bunch of stuff. I have trained new models; I'm just coordinating reviewing your submission against them.=====""; ""@ghnp5 If there's an urgency; you can simply revert back to this model:https://github.com/GantMan/nsfw_model/releases/download/1.1.0/nsfw_mobilenet_v2_140_224.zip=====""; ""I'm super excited to see your new model @TechnikEmpire !   Lots of people use NSFWJS and I'd love for your advanced model to be the de facto standard.=====""; ""> Great work guys!!!> > ```> Porn: 21%> Sexy: 20%> Neutral: 20%> Drawing: 20%> Hentai: 19%> ```> > I am fairly new at this and I don't know what a good confidence score for each of the categories is.> I have used the saved_model.tflite found https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 on Android/IOS with impressive results. However; I still don't know how to choose a good confidence score.Those scores were just an example. You just take the highest valued class and accept that. Don't get into thresholding the values. Simply take the neural network's prediction at face value.=====""; ""Here are the confidences on a new model for your submission @ghnp5 :For unsafe submissions:```BAD:  F: 80.39.jpg Confidence: 0.95885BAD:  F: 93.20.jpg Confidence: 0.998286BAD:  F: 93.79.jpg Confidence: 0.993065BAD:  F: 98.26.jpg Confidence: 0.671015BAD:  F: 99.07.jpg Confidence: 0.718538BAD:  F: 99.23.jpg Confidence: 0.998276BAD:  F: 99.24.jpg Confidence: 0.596281```For safe submissions:```BAD:  F: 70.99.jpg Confidence: 0.428323GOOD:  F: 96.52.jpg Confidence: 0.908221```As you know; the file `70.99.jpg` is black and white. This will throw off any neural network; which is why colorization before inference is an extensive topic with various techniques.Here's what happens to the scoring when I colorize that image:```BAD:  F: 70.99.jpg Class: 1 Confidence: 0.428323GOOD:  F: 96.52.jpg Class: 2 Confidence: 0.908221GOOD:  F: 70.99-Color.png Class: 2 Confidence: 0.827742```For future reference; it's very difficult to gauge any network with a few images. This is why we split out 10% or more of our total data set for validation. According to that split ratio (against tens of thousands of images); Tensorflow tells us that the latest model is ~92% accurate. While this is a really good accuracy; note that 8% between where we're at and perfection (100%) is quite the chasm; so you're definitely going to see false positives and false negatives.I'll be uploading the newly trained model for @GantMan to my attached PR here:https://github.com/GantMan/nsfw_model/pull/60I just need to convert it to tfjs first.=====""; 'New model is attached to that linked PR. For greater clarity; my experimentation (the scores given above) are based on a FP16 version run through OpenCV::DNN; not the web or quantized web versions of the model.====='; 'Hi; I\'m fairly new to neural nets and this library; I tried using your model and I keep on getting the same error ""nsfwjs.min.js:1 Uncaught (in promise) Error: layer: Improper config format:"".Do you know what I\'m doing wrong? I have tried loading it without the {type: ""graph""}  parameter and the other models have worked; Thanks!Here\'s how I\'m trying to load the model:```this.nsfwjs.load(this.modelURL; {type: ""graph""})```EDIT: I\'ve just realised I was using an outdated version of nsfwJS; Thanks for the model!====='; ""I'm also new to the entire subject of preventing nsfw images uploaded by users entering my app without any sort of check for porn or violence. Now; I've read through the blog post here https://shift.infinite.red/avoid-nightmares-nsfw-js-ab7b176978b1 this github readme page as well as https://github.com/GantMan/nsfw_modelHowever; I'm still in the fog on what model file is right for me and where to get it from? What's the difference among the various model files eg normal vs graph? Could someone maybe add two or three lines to the readme page because I reckon that's something people new to the subject are struggling to understand in general?! 😃=====""; ""@evdama I'm still using this one:https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/modelIn my experience; this one works the best.The thing is that you cannot 100% rely on any of the models.What we do is that if it detects 70%+; we show a popup to the user and ask them to confirm that this is not inappropriate; or otherwise don't proceed.=====""; ""ha! I was just afk making a tasty ☕️  and already got two great answeres 👍 (those guys must be in lockdown as well so... 😂)@TechnikEmpire Please; like you'd speak to a very motivated but unexperienced puppy; where do I get those files and how do I use them? Is it the entire .zip I found or just one of contained files or the entire collection of shards 🤔 ? And once I have the right file/model; I'd just put it inside my Sapper's `public/` folder and reference it with the `.load()` right? @GantMan Can you add a line or two to the readme so that puppies know what to do with regards to model files (and which one is the right one for a certain use case e.g. images vs videos)? 🤓=====""; 'Someone answered here; but seems the reply is gone. They were saying that the model I\'m using is very outdated and that there\'s a new one with 98% accuracy.Where can I find it?The README of this repository points to the model I\'m using; in the ""Host your own model"" section:https://github.com/infinitered/nsfwjs#host-your-own-model====='; ""> Someone answered here; but seems the reply is gone. They were saying that the model I'm using is very outdated and that there's a new one with 98% accuracy.Yup; it was @TechnikEmpire so I assume he'll come back with an even better answer...=====""; ""> > > @evdama I'm still using this one:> > https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo/public/model> > In my experience; this one works the best.> > The thing is that you cannot 100% rely on any of the models.> What we do is that if it detects 70%+; we show a popup to the user and ask them to confirm that this is not inappropriate; or otherwise don't proceed.I use this model in my chrome extension; from what I remember it was the best however it looks like it's trained on professional porn and not more _home-made_ as where my chrome extension is made for omegle; there are many people with bad lighting or blurry cameras and it doesn't detect them very well sadly=====""; ""The site at https://nsfwjs.com/ uses the 93% accuracy model; which I'm pretty sure I tried in the past and got worse results than I get with the model I'm currently using.=====""; ""I posted something and then thought better of it. I am the author of a closed-source program that uses such models and there are dirtbags that follow me on github and steal my oss so I'm doubly inclined not to help anyone.However; I was the guy making big overhauls to this repo and for some reason @GantMan stopped merging my pr's.https://github.com/TechnikEmpire/nsfw_modelHas stats for every kind of model I trained. Dunno if my site links are gone or not. But basically you just need to manually clean up the categories and the use the new tf2 api that leans on tfhub that I integrated and you'll hit much better numbers.=====""; 'Sorry not ""this"" repo; the model repo that drives this project.=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/336,Error: Size(200704) must match the product of shape 1;224;224;3,5,closed,2020-05-11T11:30:25Z,2020-05-29T07:48:03Z,"Hi.I'm using the module together with NodeJS; and when I run the `model.classify(image);` function it only successfully executes for a certain portion of images (roughly 50%)Sometimes; I get the error:```mdError: Size(200704) must match the product of shape 1;224;224;3```I have no control over what size the images are posted in (as the user sends them in whatever size); I can only resize them at max. ~~From my observations; it doesn't work for square or square-ish images; the rest seems to be ok; although I will continue my testings.~~ Edit: random images do that; not only square or square-ishI'm using the default model: `nsfwjs.load();`My whole code:```jsconst photoUrl = ""domain.com/photo.png""; /* Note: this is only a placeholder */const pic = await axios.get(photoUrl; { responseType: `arraybuffer` });let model = await nsfwjs.load();let image = await tf.node.decodeImage(pic.data);let predictions = model.classify(image);/* Do something with predictions */```(It's almost directly copied from the readme files on Github & npm)","[""That's strange.  It should resize the images for you.Can you assure you're using a new-ish version of TFJS?  I'm not an expert on the Node side; as that code was written by the community.   But the concepts seem simple enough.=====""; ""I'm using the latest version of both tfjs & nsfwjs module.=====""; ""Please create an open source repo of your example; I'll pull down the code exactly as you have it; and provide an image that fails in the demo.This will assure I solve the exact problem as I dig in.   I'd describe my steps; but they are going to be very ML specific.   I should have an answer of what's technically breaking and if I can fix it.=====""; ""Can't; I don't fully own it.=====""; 'I had the same issue today. I went through the react native files for [https://github.com/infinitered/nsfwjs-mobile/blob/master/App.js](NSFW-JS) and found a function called imagetotensor. I used the code from that to convert my array buffer to a 3d tensor; and it seems to work pretty well for me. I am getting slight issues; but I think they might just be false positives. If anyone sees any issue with the below code please let me know :)```javascriptvar sizeOf = require(\'buffer-image-size\'); //npm install buffer-image-sizeconst pic = await axios.get(photoUrl; { responseType: ""arraybuffer"" });let model = await nsfwjs.load();var dimensions = sizeOf(pic.data);const buffer = new Uint8Array(dimensions.width * dimensions.height * 3);let offset = 0; // offset into original datafor (let i = 0; i < buffer.length; i += 3) {    buffer[i] = pic.data[offset];    buffer[i + 1] = pic.data[offset + 1];    buffer[i + 2] = pic.data[offset + 2];    offset += 4;}const image = tf.tensor3d(buffer; [dimensions.height; dimensions.width; 3]);let predictions = model.classify(image);```=====']",Data & Model Error,Crash,Incorrect Code Logic,,,type replacer,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,A.4
https://github.com/infinitered/nsfwjs/issues/388,"Memory leaking that can't be fixed using ""img.dispose()""",5,closed,2020-08-04T11:30:43Z,2020-12-11T16:34:25Z,"I'm currently having memory issues when classifying images. Here is my code:```jstf.enableProdMode();async function classify(url){    var res = await axios.get(url; {        responseType: ""arraybuffer""    });    if(!res || !res.data) return null;     tf.engine().startScope();    var model = await nsfw.load(""file://model/""; { size: 299 });    var img = await tf.node.decodeImage(res.data; 3);    var classes = await model.classify(img);    // Attempted to use "".dispose()"" but didn't make that much of a difference    img.dispose();    var reviewed = {        sexy: {};        porn: {};        hentai: {}    };    classes.forEach(async(c) => {        if(c.className == ""Sexy"") reviewed.sexy = { name: ""explicit""; pr: c.probability };        if(c.className == ""Porn"") reviewed.porn = { name: ""pornography""; pr: c.probability };        if(c.className == ""Hentai"") reviewed.hentai = { name: ""hentai""; pr: c.probability };    });    tf.disposeVariables();    tf.engine().endScope();    return reviewed;}```After around 80 classifications; memory usage has already passed 1.8GB:![image](https://user-images.githubusercontent.com/65198941/89288367-6e534c00-d64d-11ea-8eee-e45b09b2bb45.png)![image](https://user-images.githubusercontent.com/65198941/89288400-7f9c5880-d64d-11ea-9598-13b83871ecfa.png)I have also attempted to use:```js tf.engine().startScope();// Classification code tf.engine().endScope();```But it made no difference.Anyone know how I could possibly fix this?","['I have managed to control the problem a little. I have used child processes to free the memory as soon as classification is finished.====='; ""Let me know what you use to solve this.  I haven't had that problem yet; but I want to see if it's specific to a version or something I can fix in code on my side.=====""; 'It isn\'t really a fix but it at least allows memory to be freed as soon as classification finishes.**index.js**```jsvar { fork } = require(\'child_process\');var forked = fork(\'./compute.js\');forked.send({ url: toClassify });forked.on(\'message\'; async out => {    console.log(out);    if(out.err) return console.warn(""Classification failed."");    // continue using output});```**compute.js**```jsvar tf = require(""@tensorflow/tfjs-node"");var nsfw = require(""nsfwjs"");var axios = require(""axios"");tf.enableProdMode();process.on(\'message\'; async msg => {    var res = await run(msg.url);    process.send(res);    process.exit();});async function run(url){    var res = await axios.get(url; {        responseType: ""arraybuffer""    })    .catch(async(err) => {        process.send({ err: true });        process.exit();    });    if(!res || !res.data){        process.send({ err: true });        process.exit();    }    tf.engine().startScope();    var model = await nsfw.load(""file://model/""; { size: 299 });    var img = await tf.node.decodeImage(res.data; 3);    var classes = await model.classify(img);        img.dispose();        var reviewed = {        sexy: {};        porn: {};        hentai: {};        err: false    };        classes.forEach(async(c) => {        if(c.className == ""Sexy"") reviewed.sexy = { name: ""explicit""; pr: c.probability };        if(c.className == ""Porn"") reviewed.porn = { name: ""pornography""; pr: c.probability };        if(c.className == ""Hentai"") reviewed.hentai = { name: ""hentai""; pr: c.probability };    });        tf.disposeVariables();    tf.engine().endScope();    return reviewed;}```====='; 'Can you dispose the model as well please?====='; 'I have tried using `tf.dispose(model)` but it makes little to no difference.=====']",Memory Leak,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,E
https://github.com/infinitered/nsfwjs/issues/397,Error when using a custom model,16,closed,2020-08-15T12:56:06Z,2021-01-04T19:43:28Z,"> ""classname"" and ""config"" must be set.","[""I've emailed Gantman; and please take a look at it.=====""; 'Can you provide a repo with your custom model so I can debug?====='; '> Can you provide a repo with your custom model so I can debug?I was using your newest model.> https://github.com/GantMan/nsfw_model/releases/tag/1.2.0My code was like this:```jsconst axios = require(\'axios\') //you can use any http clientconst tf = require(\'@tensorflow/tfjs-node\')const nsfw = require(\'nsfwjs\')async function fn() {  const pic = await axios.get(link_image; {responseType: \'arraybuffer\'})  const model = await nsfw.load(""file://anti-nsfw-bot/project-model/"")  const image = await tf.node.decodeImage(pic.data;3)  const predictions = await model.classify(image)  image.dispose()  console.log(predictions)}fn()```====='; 'What version of TensorFlow js?====='; '@tensorflow/tfjs-node: 2.3.0@tensorflow/tfjs: 2.3.0====='; 'I believe that\'s the issue.  I have it on my TODO to support the latest version of TFJS; but they made some breaking changes when they jumped from 1.x to 2.x and I need to come back and update it.Try ""@tensorflow/tfjs"": ""^1.7.4""====='; 'Which one? tfjs or tfjs-node?Also I tried to downgrade it; and it said version not found (404)====='; 'tfjs ====='; '![image](https://user-images.githubusercontent.com/33544674/90414713-b4df8680-e0e2-11ea-8f6b-a0e725d7add2.png)====='; 'Should I use above than 1.7.4 ?====='; ""**Update:**I uninstall tfjs (not the node one)I downgraded my node to v12.13.0Now I only used tfjs-node v2.1.0I don't know if I stuck at here.**Only HTTP(S) protocols are supported**Don't know why but I'm stuck here.=====""; 'Are you sure you cleaned out web TFJS?https://stackoverflow.com/questions/60137483/tensorflow-node-js-typeerror-only-https-protocols-are-supported====='; 'Yes; I only have tfjs-node.====='; ""@conver4yI've just had the same problemuninstall @tensorflow/tfjs-node and @tensorflow/tfjsand then npm install @tensorflow/tfjs-nodenpm install @tensorflow/tfjs@^1.7.4EXACTLY IN THIS ORDERidk why; but when I install  @tensorflow/tfjs@^1.7.4 first; it crashes with the same error you had![image](https://user-images.githubusercontent.com/44163887/94936092-778a5900-04d6-11eb-8330-b714aaf491f4.png)=====""; ""> @conver4y> I've just had the same problem> uninstall @tensorflow/tfjs-node and @tensorflow/tfjs> and then> npm install @tensorflow/tfjs-node> npm install @tensorflow/tfjs@^1.7.4> EXACTLY IN THIS ORDER> idk why; but when I install @tensorflow/tfjs@^1.7.4 first; it crashes with the same error you had> ![image](https://user-images.githubusercontent.com/44163887/94936092-778a5900-04d6-11eb-8330-b714aaf491f4.png)exactly the same dude :)=====""; ""I'll go ahead and close this issue.  If you can think of a proper place to warn others of this issue with install order; please PR the docs.=====""]",Reference Error,Crash,Incorrect Code Logic,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Custom model configuration issues""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.4
https://github.com/infinitered/nsfwjs/issues/430,Supporting model load from indexeddb,2,open,2020-10-24T21:42:36Z,2020-10-25T03:30:21Z,"https://github.com/infinitered/nsfwjs/blob/ebcd41c46087a3f42c6577f96acc53d7a934b068/src/index.ts#L68Hello; it seems; although not explicit I can save the model to different schemas by referencing the underlying ""model"" attribute in the model returned by `nsfwjs.load()` e.g. `nsfwjs.load(path).then(function (newModel) {        console.log(""path""; path);        if(newModel) {           newModel.model.save('indexeddb://' + SOME_KEY);        }       }).catch(error => {        console.error('onRejected function called: ' + error.message);      })`However; I am unable to then reload the model from say an `indexeddb` location using:nsfwjs.load('indexeddb://' + SOME_KEY) as the line above in the code just checks for a string; assumes it's a url or relative path; rather than another schema location and them appends 'model.js'.I think tf.js supports loading and saving to other schemas; it'd be cool if NSFWJS followed suit ...",['Agreed.  I think the best plan of action would be URL unless it detects something like `indexeddb:` or even `localstorage:`Would you be interested in providing the PR with updated docs/tests/code?  ====='; 'reference for whomever provides the code:   https://www.tensorflow.org/js/guide/save_load====='],Reference Error,Crash,Incorrect Code Logic,,,format checker,format checker,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Inability to load model from indexeddb""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/infinitered/nsfwjs/issues/435,bad memory leak,6,open,2020-11-07T08:48:07Z,2021-02-23T15:52:11Z,Hello; I'm having a bad memory leak in production after implementing nsfwjs. I'm talking gigs of memory.```jsconst axios = require('axios')const tf = require('@tensorflow/tfjs-node');tf.enableProdMode();const nsfw = require('nsfwjs');const modelFunc = async () => await nsfw.load();let model;setInterval(() => {    tf.disposeVariables()console.log(tf.memory()) // { unreliable: true;  numTensors: 49; numDataBuffers: 49; numBytes: 532830292 } and keeps increasing}; 5000)async function scan(url) {    if (!model) {        console.log('no model; loading.....')        model = await modelFunc();    }    const pic = await axios.get(url; {        responseType: 'arraybuffer';    });    tf.engine().startScope();    const image = await tf.node.decodeImage(pic.data; 3)    const predictions = await model.classify(image)    image.dispose()    tf.engine().endScope();    return predictions;}for (const nsfwURL of [...(alex000kim / nsfw_data_scraper_chunk_ofurls)]) { //a chunk of nsfw image urls    scan(nsfwURL).then(e => console.log(e))}```I sometimes get `TypeError: Cannot read property 'backend' of undefined`; _(@tensorflow/tfjs-core/dist/tf-core.node.js:3280:31)_Either way; the memory is never free'd.What am i doing wrong?,"[""It's a bit confusing the way this is constructed; but I see you're doing a lot of good things.The thing I'm confused by; is why are you `tf.disposeVariables()` on an interval?   Does that cause the model to have to reload?I'm not familiar with startScope/endScope.  Are you sure you should be managing the global engine like that? Do you have an open source GitHub repo you can link me to; so I can pull down the code and run it and see if I can fix it?=====""; '@GantMan It\'s actually a worker that exposes the ""scan"" function; the loop part is just to show how I\'m supplying the urls.I used tf.disposeVariables() in an interval just to see the effect it would have on memory and also the results of `tf.memory()`;I had tf.memory() return a VERY high numTensors number (over 500). tf.disposeVariables() reduced it and only few are returned now (around 50); The memory leak however wasn\'t fixed.`<image>.dispose()` ( or `tf.dispose(image)` ) had absolutely no effect sadly.As for `tf.engine().startScope()` and `tf.engine().endScope()`; I noticed online:> The way to clean any unused tensors in async code is to wrap the code that creates them between a startScope() and an endScope() call.I do admit; the two might be the cause of the error I had mentioned aboveEither way; the memory leak was never resolved.The code shown above is enough to actually reproduce this behavior on node====='; ""Hrmmm; TBH; I'm not seeing the source.  Perhaps the model is getting created over and over?   You could try `model.model.dispose()`.  But I'll have to create a downloadable runnable debuggable demo to find it.   I don't think it's the library; but I could be wrong; of course!  Strange stuff indeed.If you have a small; open source; ready to run demo I'll pull it down and throw an hour at it.If I were you; I'd comment out line by line and watch tensor memory with a log.=====""; 'Was this resolved?====='; '<img width=""447"" alt=""Screen Shot 2021-02-23 at 8 34 56 AM"" src=""https://user-images.githubusercontent.com/32657584/108858660-0563ff00-75b2-11eb-9571-f7bad6696c1d.png"">I have same problem with this too. The `image.dispose()` seems doesn\'t work.====='; 'Can I see some code?  Are you using ts-node?  =====']",Memory Leak,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,E
https://github.com/infinitered/nsfwjs/issues/459,False positive drawing,13,open,2021-01-07T13:18:53Z,2021-01-12T16:32:30Z,![SPOILER_Matou_shinji-1-67-1.png](https://user-images.githubusercontent.com/50463727/103897058-3f0ca380-50f3-11eb-9578-8fad802bdb3f.png),"[""The ai classify's this as hentai(Latest mobilenet model)=====""; ""Which model did you use?    I'm dragging the image to NSFWJS.com and can't reproduce with the 90% mode or the 93% model.![2](https://user-images.githubusercontent.com/997157/103913570-4bcad080-50ce-11eb-989b-40278c4ec53d.jpg)![1](https://user-images.githubusercontent.com/997157/103913574-4c636700-50ce-11eb-966d-63b200faf26e.JPG)=====""; 'I used the 93% oneI set the jpg quality of images to 60 before scanning them using nsfwjs====='; 'Can you provide the image you used?And yes; false positives will happen.====='; '![SPOILER_Matou_shinji-1-67-2.jpg](https://user-images.githubusercontent.com/50463727/103941650-75681400-512f-11eb-8b88-8b594031607c.jpg)====='; ""for whatever reason I don't have this false positive on the website while I do on my codemaybe its because I use tf prod mode=====""; ""From my testings; I have noticed that this issue is with the latest released model from the nsfwjs_model repowhen I used a previous release it didn't have this false positive=====""; ""@TechnikEmpire - would know what's up.https://github.com/TechnikEmpire=====""; ""@LINKdiscordd link directly to the model you pulled and I'll check it out to see if I can repro.=====""; 'Just to be clear this could very well be a false positive. Latest models were trained by me with a vastly different data set than previous models. So imo such ""bugs"" can be expected; if they\'re defined simply by comparing against previous models. There will always be a margin of error. Even whether we get 70% or 99% ""accuracy""; those numbers only apply to the dataset we trained and tested on; not the billions of images in existence.I\'m still happy to verify this though if you link to it. I\'m asking for a link so there\'s no error in verifying. ====='; 'https://github.com/GantMan/nsfw_model/releases/download/1.2.0/mobilenet_v2_140_224.1.zipIts the model from the web model folder which has this false positive====='; 'I can confirm that this is just a false positive. ====='; 'With relatively low confidence I might add; but still a false positive. 57% confidence.=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}",D,C.2
https://github.com/infinitered/nsfwjs/issues/478,False positive-parrot images,4,open,2021-02-16T06:55:06Z,2021-02-16T17:24:37Z,When using Mobilenet V2 4.2mb Model; the following images show false positives[Parrot courtship dance](https://i.imgur.com/5QDpmTG.png) - Identified as Porn - 61.18%[Parrot sleeping](https://i.imgur.com/cdsTryl.png) - Identified as Porn - 59.64%[Parrot eating](https://i.imgur.com/LtR0GFZ.png) - Identified as Porn - 68.27%[Parrot moving at high speed](https://i.imgur.com/6cCgtie.png) - Identified as Porn - 95.78%[Banana](https://i.imgur.com/E4OAJri.png) - Identified as Porn - 95.21%[Lovebird](https://i.imgur.com/pkEW2xe.png) - Identified as Porn - 61.15%[Parrot's claws](https://i.imgur.com/gBPDw93.png) - Identified as Porn - 69.66%Seems to have a strange false positives rate for parrots?,"['A few additions:[Parrot calling](https://i.imgur.com/czro6qC.png) - Identified as Porn - 70.59%[Overlooking a parrot](https://i.imgur.com/bTvAEvF.png) - Identified as Porn - 91.37%[Parrot](https://i.imgur.com/7QBFg1R.png) - Identified as Porn - 69.18%[Parrot shakes its head](https://i.imgur.com/DaygdF6.png) - Identified as Porn - 51.20%[More Parrot](https://i.imgur.com/oFxJvSa.png) - Identified as Porn - 70.51%====='; 'That is strange!   How does the 90MB model perform?====='; ""> That is strange! How does the 90MB model perform?![chrome_mc5UthqZJD](https://user-images.githubusercontent.com/45941794/108090236-5b6a0b80-70b5-11eb-9dc0-35a1fdb22319.png)The situation doesn't look promising eitherI'll get more pictures to test it out=====""; ""What a crazy result!   We'll have to add parrot pictures to the training data next round.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/493,example/manualtesting doesn't work,3,open,2021-04-03T06:12:09Z,2021-04-03T19:38:55Z,I get this error when running it: tfjs@1.7.4:2 Uncaught (in promise) TypeError: t is not a function    at tfjs@1.7.4:2    at tfjs@1.7.4:2    at t.scopedRun (tfjs@1.7.4:2)    at t.tidy (tfjs@1.7.4:2)    at h (tfjs@1.7.4:2)    at tfjs@1.7.4:2    at t.scopedRun (tfjs@1.7.4:2)    at t.runKernelFunc (tfjs@1.7.4:2)    at t.runKernel (tfjs@1.7.4:2)    at slice_ (nsfwjs.min.js:7147)(anonymous) @ tfjs@1.7.4:2,"['@YegorZaremba - can you take a look at this?====='; ""Hi @GantMan  I'll check it tommorowBTW; we have tfjs v 3.x.x in package.json https://github.com/infinitered/nsfwjs/blob/master/package.json#L27=====""; 'Correct.  The project seems to work fine with TFJS 3.x =====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/infinitered/nsfwjs/issues/513,False Positive Drawing,3,open,2021-05-07T18:58:01Z,2021-05-07T19:13:08Z,![DvYY8qbXgAAMi62.jpeg](https://user-images.githubusercontent.com/50463727/117496130-e51fcb80-af76-11eb-82be-37411380d1fb.jpeg),"['Model Used is the latest release from the nsfw model repo====='; 'Prediction ResultsLabel: HentaiProbability: 70%====='; ""Thanks for sharing.  We'll add this to the data.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/517,False Positive Neutral,2,closed,2021-06-14T07:46:05Z,2021-06-14T08:17:06Z,"![52f81f94-4a5f-43cf-8a25-7d6b8ba53fd8](https://user-images.githubusercontent.com/23108292/121856660-6a786780-ccfd-11eb-9b01-320c36d9498e.jpg)**Dependencies:**""@tensorflow/tfjs"": ""^3.7.0""""nsfwjs"": ""^2.4.0""**Prediction Results**className: ""Drawing""probability: 0.9339360594749451","['Model config`{  ""format"": ""graph-model"";  ""generatedBy"": ""2.2.0"";  ""convertedBy"": ""TensorFlow.js Converter v1.7.4r1""}`====='; 'Now i using ""Inception V3 Model""; and get better results.=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/560,False Positive: women eating banana,1,closed,2021-11-19T08:18:26Z,2021-11-19T16:01:44Z,Hi Gant; NSFW JS will fail with pictures of women eating banana!just a doing a simple google search: https://www.google.com/search?q=woman+eating+banana... and some examples:https://image.shutterstock.com/image-photo/young-woman-eating-banana-260nw-3106254.jpghttps://thumb.mp-farm.com/89598622/preview.jpghttps://st2.depositphotos.com/1012146/6473/i/950/depositphotos_64738027-stock-photo-girl-eating-a-banana.jpghttps://image.shutterstock.com/image-photo/woman-eating-banana-260nw-514825333.jpgas what I know; eating a (real) banana is not porn or nudity!,"[""I'll add these to the training data.  Thanks for the find.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/563,Demo predictions and Node JS App predictions are not the same result,2,open,2021-11-22T15:53:40Z,2021-11-22T16:42:16Z,"nsfwjs.com (load model: inceptionv3: ['/model/'; { size: 299 }]) image result:```Identified as PornPorn - 48.19%Neutral - 40.21%Sexy - 10.87%Hentai - 0.61%Drawing - 0.12%```NodeJS app (load model: inceptionv3: ['/model/'; { size: 299 }]) same image result:```[    {        ""className"": ""Neutral"";        ""probability"": 0.6607071757316589    };    {        ""className"": ""Sexy"";        ""probability"": 0.18770809471607208    };    {        ""className"": ""Porn"";        ""probability"": 0.14371605217456818    };    {        ""className"": ""Hentai"";        ""probability"": 0.004498853348195553    };    {        ""className"": ""Drawing"";        ""probability"": 0.0033697152975946665    }]```My Code:```const express = require('express')const multer = require('multer')const jpeg = require('jpeg-js')const tf = require('@tensorflow/tfjs-node')const nsfw = require('nsfwjs')const app = express()const upload = multer()let _modelconst convert = async (img) => {  // Decoded image in UInt8 Byte array  const image = await jpeg.decode(img; true)  const numChannels = 3  const numPixels = image.width * image.height  const values = new Int32Array(numPixels * numChannels)  for (let i = 0; i < numPixels; i++)    for (let c = 0; c < numChannels; ++c)      values[i * numChannels + c] = image.data[i * 4 + c]  return tf.tensor3d(values; [image.height; image.width; numChannels]; 'int32')}app.post('/nsfw'; upload.single(""image""); async (req; res) => {  if (!req.file)    res.status(400).send(""Missing image multipart/form-data"")  else {    const image = await convert(req.file.buffer)    const predictions = await _model.classify(image)    image.dispose()    res.json(predictions)  }})const load_model = async () => {  _model = await nsfw.load('https://nsfwjs.com/model/'; {size: 299})}// Keep the model in memory; make sure it's loaded only onceload_model().then(() => app.listen(8080))```I tried the model files locally; same result.",['Have you tried tfjs-node to decode the images?Just for info.  Which one of the two was correct?  I assume the website was correct; and the node was wrong?====='; 'https://github.com/infinitered/nsfwjs/discussions/540====='],Incorrect Functionality,Incorrect Functionality,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,E
https://github.com/infinitered/nsfwjs/issues/569,False positive on manga,1,open,2021-12-13T07:37:12Z,2021-12-15T19:14:57Z,## OverviewModel used: Inception v3. MobileNet models does not experience this issue.![ar0ltbti00h71](https://user-images.githubusercontent.com/32661241/145770552-f7b8c11d-7d73-43d7-a04f-97b3315c6eee.jpg)![proof](https://user-images.githubusercontent.com/32661241/145770838-b8b0fada-d9d0-437a-bbb9-88a021906cd1.png),"[""I'm thinking about making a voting layer that allows you to combine multiple models and get higher accuracy.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,C.2
https://github.com/infinitered/nsfwjs/issues/62,error TS2348,6,closed,2019-03-27T00:32:03Z,2019-04-05T01:32:24Z,any idea?```shellC:\Users\Jae Jin\Desktop\HUB\nsfwjs>yarn prepyarn run v1.15.2$ yarn && yarn build && cd example/nsfw_demo/ && yarn add ../../ && cd -[1/4] Resolving packages...success Already up-to-date.$ tsc --skipLibChecksrc/index.ts:103:47 - error TS2348: Value of type 'typeof Model' is not callable. Did you mean to include 'new'?103           this.intermediateModels[endpoint] = tf.Model({                                                  ~~~~~~~~~~104             inputs: this.model.inputs;    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~105             outputs: layer.output    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~106           })    ~~~~~~~~~~~~Found 1 error.```,"[""What version of Tensorflow do you have?  I might know what's happening.=====""; 'Tensorflow/tfjs@0.15.3====='; 'Closed for now Answer for a week.====='; ""sorry @Kadantte -  I'll be upgrading the project to Tensorflow 1.x like in ticket #61 - It might fix your issue then.  I can't recreate this bug; so I'm hoping that fixes it.=====""; 'PR to upgrade to TFJS 1.x is in #78 ====='; ""Thank's you very much.=====""]",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",C,A.5
https://github.com/infinitered/nsfwjs/issues/94,5-6 seconds - or more; before model load and client (user) can use the page - how to improve?,34,open,2019-04-23T14:41:50Z,2020-05-21T01:02:46Z,I'm currently testing this interesting nsfwjs module on a live server.I activate this code:pdModel = await nsfwjs.load('model/');via a call to an async function triggered by the onLoad event. The 6 model files are fetched from the disk and is using approximately 960ms to load (I see this using Chrome inspector - Network tab).After this I have to wait approx. 5-6 seconds before I get any response from the page. From what happens in the Memory tab in Inspector; it looks like the nsfwjs load is using this time to do something. After the 5-6 seconds. I can select an image. When that is done; this code (in another async function):var img = new Image();img.src = document.getElementById('img_url').src;var predictions = await pdModel.classify(img);console.log('Predictions'; predictions);is using about 1;5 - 2 seconds to finish. This is acceptable time for me.This time usage of about 1.5 -2 seconds - is consistent.But the usage of 5-6 seconds nsfwjs use to do something - before anything else can happen in the page (e.g. select an image to analyze); is quite a long time - for an end user which want to do something as soon as possible. So... is there a way to speed up the time nsfwjs uses to finish this part: pdModel = await nsfwjs.load('model/');and whatever it does before the page is ready for handling user triggered events in the page?Could the preparation work done by nsfwjs (these first 5-6 seconds) maybe be split up in 2 parts; and then do half the job - onLoad; and then the second part - which is triggered to happen when an image actually is selected by the user?,"['Try updating your version of Tensorflow maybe?====='; ""I'm using this:https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.4Is there a newer version available?=====""; 'You should definitely keep track of what CDN packages are available https://cdnjs.com/libraries/tensorflow====='; 'Thanks for that information. Now tested with latest tfjs  1.1.0. Same results. Any idea on how to not make the client (user) wait ""so long"" (5-6 seconds) before he can interact with the page (see suggestion in the end of thread start)?====='; ""Hrmmm. I really don't know. Is your code open source? I can take a look if so. =====""; 'Thanks for that.**You can test it from here**Explanation is in the page:[nude-test-await](http://monsanto.watch/nsfwjs/nude-test-await.html)But this should be no surprice. The same delay can be experienced on this demo page: https://nsfwjs.com/Same delay there - even if the model files are fetched from disk.Here is the relevant information on how the page was set up: `<!-- Load TensorFlow.js. This is required -->\t<script src=""nsfwjs/tfjs@1.1.0""></script>\t<!-- Load the NSFWJS library. -->\t<script src=""nsfwjs/bundle.js""></script>const nsfwjs = require(\'nsfwjs\');var pdModel;async function loadAwait() {\tpdModel = await nsfwjs.load(\'model/\');}async function analyzeImageAwait(){\t\t\t\tif (pdModel){\t\t\tvar img = new Image();\t\timg.src = document.getElementById(\'img_url\').src;\t\tvar predictions = await pdModel.classify(img);\t\tconsole.log(\'Predictions\'; predictions);        }}`in the body element I use this: onload=""loadAwait()""on the server I have these sub directories with the following files:model/files: group1-shard1of6group1-shard2of6group1-shard3of6group1-shard4of6group1-shard5of6group1-shard6of6model.jsonnsfwjs/files:bundle.jstf.min.js.maptfjs@1.1.0Except for this the main file only contain an input file field where I can select an image on the computer; and some javascript code which copy the relevant file information to the img.src with id img_url - like this:`function img_pathUrl(input){     $(\'#img_url\')[0].src = (window.URL ? URL : webkitURL).createObjectURL(input.files[0]);     setTimeout(analyzeImageAwait; 1000);}`But as allready mentioned the delay of 5-6 seconds happens before I can click the file button which activate the img_pathUrl function above.====='; ""So this is going to sound strange.  Neither NSFWJS or your page have this delay for me.  When I load your page; the first time; it downloads the model and then I was immediately able to access the UI.The second time I access the page; it reconfirms the cache for a few seconds; but the very second your loader goes away; I'm able to click.Perhaps it's a browser/computer difference?  Would you be willing to google/run some other TensorflowJS examples and see if this happens across the board for you?Perhaps it's when the model is loaded into memory?   Instant for some; slower on other computers?=====""; 'What is the load time in seconds (which shows in blue text in page after loader hides) when the model files loads from disk; when you use this page?[nude-test-await](http://www.monsanto.watch/nsfwjs/nude-test-await.html)====='; '### Cold start![image](https://user-images.githubusercontent.com/997157/56745759-906ad880-6740-11e9-98d6-af08b96bb523.png)### Second page load![image](https://user-images.githubusercontent.com/997157/56745812-ad071080-6740-11e9-9047-5873db71173d.png)For me; the spinner shows me that the UI would be inaccessible.  But the second it goes away I can click.### Side noteAs far as I can tell; this is verifying local cache and loading the model into memory.   I might train a smaller model soon and see if I can get it nearly as accurate.  If so; that will be helpful with these kinds of times.  As you can guess; training models can take a long time.====='; 'My point is that the load time takes a lot of time - and that this might make it awkward to use in a production environment. In my web application which is still under development; the user will want to interact rather quickly with the UI; not wait 5 - 20 seconds before he can select an image; or do something else in the page.The thing is; there is a huge difference between time to fetch the model files from disk; which on my computer takes only 400 - 600 milliseconds; and the total time the load stuff actually work (as long as the loader spins). After the loader spinner stops spinning (after load process finish) the page is accessible also on my computer.My question is related to if there is a way to make the load time (except for the 400 - 600 milliseconds for the files to be read from disk cache) shorter; or if possible to divide this work into e.g. two parts. If the latter (two parts) - then one part could be activated when user selects the image; and the other part when he submit the form. This way - the wait; could be non-problematic for end user - in my opinion. I say could because the time seems to vary between browsers (and platforms; e.g. mobile); and even between different page loads in the same browser.Another way to speed things up; could be to NOT analyze drawing and neutral. For my use it would be enough to test for a numeric value for Porn; Sexy and Hentai content. Is it possible in some way to force the module to a) not load the model files for Drawing and Neutral; and b) not test for these values?Or what about this idea. Open a new hidden browser window via Javascript; then load e.g. a similar html page [as this page](http://monsanto.watch/nsfwjs/nude-test-await.html); and then load the model there without blocking usage of the actual page. Then when the user click submit; or select an image in the actual page; the img src is copied into javascript in the tab with the model - then do the analyzing there; and return the answer back to the actual page.Today I also tested the page; I gave you the link to; on my Samsung Galaxy Note II - on a Wi-Fi connection. It took 121 seconds to finish the first load; and 181 seconds to finish the second load.  When selecting an image - and waiting for a couple of minutes - no predictions where output. Need to do more tests here - since I have no clue why it was so slow on second load; and why no predictions where output (UPDATE: the day after I tested this again. It actually works; but needed some more time to finish - see my next comment below).In Internet Explorer (version: 11.0.9600.17728) the model does not load. The error is from the tfjs@1.1.0 file; where debugger report this error: ""\'Symbol\' is undefined""; where the debugger highlighted this code: ![Bug-in-IE](https://user-images.githubusercontent.com/436558/56764638-7dd6ba80-67a5-11e9-909e-494209adab03.jpg)I also observe the \'Symbol\' error in IE; mentioned above - when testing the of [NSFWJS demo page](https://nsfwjs.com/) in IE.====='; 'GantManThere is something called Web Workers in Javascript. Is it possible to load the model in a web worker; and then make the model load behind the scenes; so to speak; so the user does not have to wait until the model has loaded before the user can start to interact with the web page.When I have this in the worker.js`importScripts(\'tfjs@1.1.0.js\');  importScripts(\'bundle.js\');`Then I get these errors in the console:`Uncaught Error: Could not find a global object    at engine.ts:829    at engine.ts:818    at engine.ts:836    at tfjs@1.1.0.js:2    at tfjs@1.1.0.js:2    at worker.js:13`If I click the ""at engine.ts:829; it reports error at this line:`throw new Error(\'Could not find a global object\');`So; do you know if it is possible to run the nsfwjs module from within a web worker?More about web workers here: [Multithreading Java script](https://medium.com/techtrument/multithreading-javascript-46156179cf9a)====='; ""Personally; I haven't worked with Web Workers just yet; but some searching found this discussion which I think should be noted:  https://github.com/tensorflow/tfjs/issues/102=====""; ""> Personally; I haven't worked with Web Workers just yet; but some searching found this discussion which I think should be noted: [tensorflow/tfjs#102](https://github.com/tensorflow/tfjs/issues/102)Thanks for that information; just what I needed. Have tested it; and Web Workers do work with NSFWJS (in Opera and Chrome). Which mean that the model can load in a background process; while the client can start interacting with the user interface of the web page immediately.**But could you please tell me** if it is possible in some way to not use the model for Drawing and Neutral. That is; just download (load) the model files for analyzing image for Porn; Sexy and Hentai? Thereby save time by not having to download the two mentioned model files (Drawing and Neutral) to the client; and hopefully awoid that the model.classify() use time on testing for Drawing and Neutral. **An even better solution**  would be to create separate Web Worker for analyzing if Porn; Sexy; Hentai (and the others if one want that: Drawing and Neutral). The first Web Worker could initiate the other Web Workers after the nsfwjs is created; but before loading the model for testing for Porn. This way the other workers would fetch e.g. bundle.js and tfjs@1.1.0.js from cache (second worker would then fetch model file(s) for anlyzing Sexy; the third worker would fetch file(s) for analyzing Hentai (and so on). I believe such an approach would speed up the load time drastically on mobile devices; as well as speeding up load time in general on other devices - since each worker is working in it's own separate process at the same time. The process of analyzing frames (images) from a video; should also be a lot faster - using Web Workers.**If this can be done; when can we see new model files and new other files on github - which makes it possible to load e.g. separate model for Porn; and analyze just for Porn (and same for the other types (e.g. Sexy; Hentai; etc) - in a separate Web Worker?****For this to work in the Web Worker; the Web Worker must support Offscreen Canvas.** Not all web browsers support this yet; but it would be easy to test in a web worker js file to report if the feature is supported on the client browser; and if not - do not use use web workers. Here one can see which browsers (versions) [support Offscreen Canvas](https://developer.mozilla.org/en-US/docs/Web/API/OffscreenCanvas/OffscreenCanvas#Browser_compatibility).I mentioned in my previous comment above; that I could not make the NSFWJS work in my mobile (Galaxy Note II) using Chrome. That was not correct. Did a new test yesterday; and found that the image analyzing did work on the mobile device. I did not wait long enough the first day I tried this. But it took about 3 minutes and 30 seconds just to analyze the image (get the predictions). Approximately the same time it took to load the model.=====""; ""Unfortunately the model is all in one and would not be made smaller by removing classifications. However!  I'm nearing the end of training a smaller and nearly as accurate model. Expect that soon. It should speed everything upRe: https://github.com/GantMan/nsfw_model/issues/20#issuecomment-487354103=====""; 'Look forward to test that. But; if you trained a model to recognize only Porn; Sexy and Hentai images; or just Porn; would that model (the files) not be smaller than a model trained to recognize (predict) Porn; Sexy; Hentai; Drawing and Neutral?====='; ""So the model is architected first and only the final layer is affected in size by number of possibilities.  Now; fewer possibilities could be supported by a smaller model architecture; but that would require building a smaller model and retraining.  Aka lots of time. It's hard to remove those parts without throwing the whole model out of balance. As far as I know it's not even a practice.  But it should be! That would be cool. Anyhow; best case is to get this new smaller model I'm making and do a conditional on the predictions. I fully believe model optimization will become a very real thing in the next 5 to 10 years. Just not a simple solution.  =====""; 'Is there noe way to run it non-blocking? Which means it should run async in background with a callback when it finishes...I have the same problem. I want to create a chrome extension. But waiting on every site 5-6 seconds is just annoying.====='; 'danieldaeschle:You can load the model and analyze the images in a non blocking mode. Just create the NSFWJS and load the model in a web worker - then it will run in a separate thread on the client. I have tested this; and it works in latest versions of Chrome and Opera. This way the web page UI is not blocked by the loading procedure of NSFWJS. @GantMan ; could you please confirm if the suggestion below is technically possible:After figuring out that I can load the NSFWJS module in a web worker; my next thought was if it was possible to load the data from the different model files using separate Web workers; and just send the loaded data (array; object) or whatever is loaded in a web worker; to the main web worker (the one you want to use for analyzing the images). This way the data from the model files could be loaded in parallell - and could probably minimize the total load time - which would be especially beneficial if the client is on a mobile device; but should be faster on all devices. Note: It is possible to send e.g. arrays and objects from a worker back to main script (page) and from that script to another worker. ====='; '@bongobongo like i already said; i want to create a chrome extension which does not support web workers. i just tried it :( ====='; '> @bongobongo like i already said; i want to create a chrome extension which does not support web workers. i just tried it :(@danieldaeschle ; have you looked into this:https://bugs.chromium.org/p/chromium/issues/detail?id=357664#c3====='; 'I checked both. Nothing works. They are 5 years old...bongobongo <notifications@github.com> schrieb am Di.; 30. Apr. 2019; 17:36:> @bongobongo <https://github.com/bongobongo> like i already said; i want> to create a chrome extension which does not support web workers. i just> tried it :(>> @danieldaeschle <https://github.com/danieldaeschle> ; have you looked> into this:> https://bugs.chromium.org/p/chromium/issues/detail?id=357664#c3>> —> You are receiving this because you were mentioned.> Reply to this email directly; view it on GitHub> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488001585>;> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHKTWHAKXDBRIT67JTJT5JLPTBRQNANCNFSM4HHYUUNA>> .>====='; ""@danieldaeschle ; do you know that you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker? Like this - place it first in the Web Worker JavaScript file: `if (typeof OffscreenCanvas !== 'undefined') {\t// currently tested and works in my Chrome and Opera\t    self.document = {        createElement: () => {            return new OffscreenCanvas(640; 480);        }    };    self.window = self;    self.screen = {        width: 640;        height: 480    };    self.HTMLVideoElement = function() {};    self.HTMLImageElement = function() {};    self.HTMLCanvasElement = OffscreenCanvas;}`=====""; ""Thank you; i'll try this.bongobongo <notifications@github.com> schrieb am Di.; 30. Apr. 2019; 17:46:> @danieldaeschle <https://github.com/danieldaeschle> ; do you know that> you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker?> Like this - place it first in the Web Worker JavaScript file:>> `if (typeof OffscreenCanvas !== 'undefined') {> // currently tested and works in my Chrome and Opera>> self.document = {>     createElement: () => {>         return new OffscreenCanvas(640; 480);>     }> };> self.window = self;> self.screen = {>     width: 640;>     height: 480> };> self.HTMLVideoElement = function() {};> self.HTMLImageElement = function() {};> self.HTMLCanvasElement = OffscreenCanvas;>> }`>> —> You are receiving this because you were mentioned.> Reply to this email directly; view it on GitHub> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488005415>;> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHKTWHCTQVYJE3R2BGOUHPLPTBSWDANCNFSM4HHYUUNA>> .>=====""; ""I tried it with cpu instead of gpu which does not require a canvas. Thatalso didn't work.Daniel Däschle <daniel.daeschle@gmail.com> schrieb am Di.; 30. Apr. 2019;17:48:> Thank you; i'll try this.>> bongobongo <notifications@github.com> schrieb am Di.; 30. Apr. 2019;> 17:46:>>> @danieldaeschle <https://github.com/danieldaeschle> ; do you know that>> you have to use OffscreenCanvas to use the NSFWJS module in a Web Worker?>> Like this - place it first in the Web Worker JavaScript file:>>>> `if (typeof OffscreenCanvas !== 'undefined') {>> // currently tested and works in my Chrome and Opera>>>> self.document = {>>     createElement: () => {>>         return new OffscreenCanvas(640; 480);>>     }>> };>> self.window = self;>> self.screen = {>>     width: 640;>>     height: 480>> };>> self.HTMLVideoElement = function() {};>> self.HTMLImageElement = function() {};>> self.HTMLCanvasElement = OffscreenCanvas;>>>> }`>>>> —>> You are receiving this because you were mentioned.>> Reply to this email directly; view it on GitHub>> <https://github.com/infinitered/nsfwjs/issues/94#issuecomment-488005415>;>> or mute the thread>> <https://github.com/notifications/unsubscribe-auth/AHKTWHCTQVYJE3R2BGOUHPLPTBSWDANCNFSM4HHYUUNA>>> .>>>=====""; '@danieldaeschle - did u have any progress on making the chrome extension? I have just started looking building one myself; and then saw ur comment above saying u were trying it. ====='; ""I'd be interested in seeing how this is performing on the latest TensorFlow.jsI know they've been working on the blocking UI problem.Additionally; I've really shrunk the model; so once it's loaded into memory; it should be super quick!!!1=====""; 'Is there any progress in solving the blocking UI issue?====='; ""I've found using the small model doesn't even appear to cause any complication.   I haven't tried the larger model in a while.=====""; 'Which small model? How long does it take to initialize?====='; 'By default https://nsfwjs.com/  uses the small model.  You can change it to the larger model in the dropdown.  I have a new small-medium sized model that is more accurate; as well.=====']",Slow Execution,Poor Performance,Improper Model Attribute,,,change model,change model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.1,C.2
https://github.com/justadudewhohacks/face-api.js/issues/120,getBox error on 0.15.0,3,closed,2018-10-30T02:55:06Z,2018-11-04T02:04:07Z,### Calling allFaces with SsdMobilenetv1 is giving me a getBox error.This code:```jsconst faceImage = await faceapi.fetchImage('https://i.imgur.com/GzBMITw.jpg')const faceDescript = await faceapi.allFacesSsdMobilenetv1(faceImage)```resulting in:![image](https://user-images.githubusercontent.com/997157/47692693-a732d700-dbc4-11e8-9d81-588766fb61fd.png)I have the code (in React) available upon request.  Please let me know if this issue makes sense or if I should do more.,['Apparently I forgot to clean the build folder before building and publishing; because the error originates from an old file that has been removed.====='; 'Should be resolved in 0.15.1.====='; 'it is====='],Reference Error,Crash,Incorrect Code Logic,,,change Third-party library version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/128,Failed to Compile fragment Shader,2,closed,2018-11-06T05:05:23Z,2018-11-11T19:52:13Z,"I am getting the following error suddenly since yesterday (was working fine till Nov 1) when I try to detect face using tiny-yolo. I had downloaded latest face-api.min.js on Oct 30th and it worked for a day and somehow started getting this error since yesterday.![image](https://user-images.githubusercontent.com/38608333/48043981-726be480-e1af-11e8-8d34-bd15d0189af5.png)Can you pls help on the same. I am including tfjs using  the following link.<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js""> </script>",['Seems like an issue with tf latest maybe? Try to use the tfjs-core version; that face-api.js uses; which is 0.13.2 currently. I noticed some issues with 0.13.9 + WebGL backend.====='; 'Thank you.You are right. It was an issue with tfjs as things work fine when I switchto tfjs 0.13.2.On Tue; Nov 6; 2018 at 2:03 PM justadudewhohacks <notifications@github.com>wrote:> Seems like an issue with tf latest maybe? Try to use the tfjs-core> version; that face-api.js uses; which is 0.13.2 currently. I noticed some> issues with 0.13.9 + WebGL backend.>> —> You are receiving this because you authored the thread.> Reply to this email directly; view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/128#issuecomment-436171165>;> or mute the thread> <https://github.com/notifications/unsubscribe-auth/Ak0dzURDszpL8o5YgFvTMYk4dW6CKXA_ks5usUllgaJpZM4YPwzZ>> .>====='],Browser & Device Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2""
  }
}
```",A.4,A.5
https://github.com/justadudewhohacks/face-api.js/issues/129,Webcam_face_tracking doesn't detect face when it is run locally in firefox browser on laptop.,10,closed,2018-11-06T08:59:30Z,2019-03-27T14:05:41Z,I am using 63.0.1 firefox browser on windows 10. This works https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking on both chrome & firefox.but when it is run locally by npm start. It is opening the camera; but not detecting face in firefox. It works fine in chrome. And no logs too.,"['Hi dude; any way to resolve this?====='; 'I have the same question and wonder how to resolve it.====='; 'It seems like the program stops at this line`      const result = withFaceLandmarks        ? await faceDetectionTask.withFaceLandmarks()        : await faceDetectionTask`====='; 'yes====='; 'I think there is an issue with tf.fromPixels on firefox sometimes; when the processing loop starts too soon. Not sure why this is though.====='; '@vbh25; did you find a solution to that problem?====='; 'calling videoEl.play() in videoEl.onloadedmetadata function solves it.====='; ""> calling videoEl.play() in videoEl.onloadedmetadata function solves it.I don't find videoEl.onloadedmetadata function. Which file is the function in?=====""; 'In fact; as @veetechh pointed out; moving onPlay to onloadedmetadata instead of onplay in the video element seems to solve it: `<video onloadedmetadata=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>`.Will fix this in the examples.====='; 'Checked in the fixed examples; closing here.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,Dom attribute,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Different results produced under multiple DL backends, platforms, or devices""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/133,Error on latest version of tensorflowjs,1,closed,2018-11-13T10:28:48Z,2018-11-13T21:30:33Z,"I got an error when implement the latest version of tensorflowjs in html file `<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest""></script>`> Uncaught (in promise) Error: Failed to compile fragment shader.>     at createFragmentShader (tfjs@latest:2)>     at e.createProgram (tfjs@latest:2)>     at compileProgram (tfjs@latest:2)>     at tfjs@latest:2>     at e.getAndSaveBinary (tfjs@latest:2)>     at e.compileAndRun (tfjs@latest:2)>     at e.maxPool (tfjs@latest:2)>     at ENV.engine.runKernel.x (face-api.js:23)>     at tfjs@latest:2>     at e.scopedRun (tfjs@latest:2)It looks like FaceRecognitionModel can only compatible tensorflowjs under 0.13.0.",['Answered here: #128. Always use the same tfjs-core version as face-api.js; which is 0.13.8 now.====='],Browser & Device Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.4,A.5
https://github.com/justadudewhohacks/face-api.js/issues/152,Significant drop of the accuracy for tilted face,13,open,2018-11-29T15:03:02Z,2020-07-02T13:22:23Z,I am testing the different images and I experience significant drop of accuracy if I just feed the tilted face with >30*.I am using the prepared examples in browser and node-js and effects are similar: for the face looking straight into camera I get around 0.3 - 0.4; but if I tilt the face I go beyond 0.7. When testing the same images with dlib (which is the original network used in face-api.js if I understand correctly) I get the distance of <0.5 for the tilted faces.Do I need to additionally enable the alignment with face-api.js? Maybe I am missing something here.Here the example photos; where I get high distance with face-api.js; but is accepted by dlib:![grande1](https://user-images.githubusercontent.com/14967831/49232775-e5eec380-f3f4-11e8-9891-29a812eed371.jpg)![grande2](https://user-images.githubusercontent.com/14967831/49232779-e9824a80-f3f4-11e8-9887-b2c4c94a281b.jpg)But if I just manually rotate further the first photo:![new](https://user-images.githubusercontent.com/14967831/49233071-76c59f00-f3f5-11e8-9dd6-5cda24baef0e.jpg)the face is accepted and I get very similar result to dlib (around 0.48).,"[""Thanks for reporting this! The issue is; that face-api.js alignment doesn't rotate the images; it only centers them at the moment. I guess dlib does rotate them based on the face landmarks predicted by the shape predictor right?=====""; 'Yes; I am not sure how exactly is it done to mimic it; but it is done here: http://dlib.net/dnn_face_recognition_ex.cpp.html in the function`extract_image_chip`Here even 5 landmark model is used instead of 68 point model. Maybe this 5 point model after quantization would be even smaller than the currently used model in face-api.js for face recognition :)I could try to make a PR with it; but I am normally writing in C++; someone would need to review it; thoroughly.====='; 'Maybe in future it would be beneficial to train a very small 5 point landmark model just for alignment; although the existing 68 point face landmark models should be really fast already; so not sure if thats worth it.A PR would be highly appreciated. Currently the alignment is done based on the 68 point face landmarks and is simply centering the face by some predefined ratio; that is close to how dlib does it. The rotation of the face is not considered yet; I think the main challenge would be to figure out; how to rotate the image content of an HTML canvas or image: [FaceLandmarks.ts](https://github.com/justadudewhohacks/face-api.js/blob/master/src/classes/FaceLandmarks.ts#L84-L97)====='; ""Thanks a lot! I'll try to come back with PR if I find some time for it.=====""; 'Just gonna leave this issue open to keep track of it.====='; 'Hi Jendker;You can rotate the image by using landmark position at 36 (left eye) and 45 (right eye) as begin and end point then you calculate the angle of the line compared with horizontal.For example: https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/bbtFaceLandmarkDetection.htmlat line 42:`    function redraw() {      const canvas = faceapi.createCanvasFromMedia(currentImg)      $(\'#faceContainer\').empty()      $(\'#faceContainer\').append(canvas)      faceapi.drawLandmarks(canvas; landmarks; { lineWidth: drawLines ? 2 : 4; drawLines })    }`You can edit as`    function redraw() {      var canvas = faceapi.createCanvasFromMedia(currentImg)            var dY = landmarks[\'positions\'][45][\'y\'] - landmarks[\'positions\'][36][\'y\']      var dX = landmarks[\'positions\'][45][\'x\'] - landmarks[\'positions\'][36][\'x\']      var degree = Math.atan(dY/dX)      var ctx = canvas.getContext(""2d"");      ctx.rotate(-degree);      ctx.drawImage(currentImg;0;0);      $(\'#faceContainer\').empty()      $(\'#faceContainer\').append(canvas)      faceapi.drawLandmarks(canvas; landmarks; { lineWidth: drawLines ? 2 : 4; drawLines })    }`Hope I can help.====='; 'That could be something; thanks!I was thinking about a bit different approach with the alignment to follow what is done in dlib. Davisking is using the affine transformations with some approximations; which are quite complex and if we are using the original dlib face recognition model it would be advisable to follow it to get better recognition results. On the other hand it works still well if we are not using any transformation just image cropping :) That could be an intermediate solution.The other thing is how to add it neatly into face-api.js.====='; ""Hello everyone; I'm trying to use this API on my project to detect faces but I've noticed that I have a lot of different users that try to submit a photo with their faces tilted. Has been there any update on this particular issue?=====""; ""I ended up using a solution recommended by Bajajar and it was fine for me. I didn't manage to get it working by changing the underlying face-api.js functions; my limited JavaScript skills were not sufficient. =====""; ""@Jendker thank you for your quick respose!I've noticed that the recognition API doesn't work well with tilted images but the detection is doing great!Thank you.=====""; 'https://github.com/Jack-CV/FaceKit/tree/master/PCNIf face-api.js uses such a model; it will be able to recognize faces of all angles.Face-api.js can not meet my requirements because the landmarks are messy and unmatched on a lying face or a face that is flipped up and down.====='; '@wkdhkr thanks for sharing the repo; I will look into the paper and if it looks promising.The landmark model is not trained on data of faces that are rotated like 90 degrees. You could simply rotate the image in 90 degree steps and feed them into the model; but that obviously requires more computation time.====='; '@justadudewhohacks https://github.com/siriusdemon/pytorch-PCNThere is a Pytorch version of the above model. If this is converted into a model for Tensorflow.js with ONNX.js etc.; it seems that it can be used with face-api.js. (I do not understand the problem of license well.)Alternatively; it might be possible to bridge the C implementation as a native module.By the way; Not only landmarks but also ssd mobile net can not detect inverted or rotated faces. Rather; if it is possible to detect the top and bottom of the face first; the landmark model as it is may be fine even with the current landmark model. Because processing only needs to rotate the face once.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data preprocess,Add data processing,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/154,"SOLVED: Webpack warning about not found ""fs"" module",8,closed,2018-11-30T20:55:27Z,2021-04-16T10:37:40Z,"I get the following warning when building my webapp with webpack:```[1] WARNING in ../node_modules/tfjs-image-recognition-base/build/es6/env/initialize.js[1] Module not found: Error: Can't resolve 'fs' in '/Users/username/project/node_modules/tfjs-image-recognition-base/build/es6/env'[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/env/initialize.js[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/env/index.js[1]  @ ../node_modules/tfjs-image-recognition-base/build/es6/index.js[1]  @ ../node_modules/face-api.js/build/es6/index.js[1]  @ ./src/index.tsx```I had to add the following to my webpack config to suppress the warning:```node: {   fs: ""empty""}```It's weird because the target in my webpack config is set to `web`.Am I doing something wrong? If not; this issue may help other people with the same warning.","[""Thanks for reporting your solution!It's because webpack sees the require('fs') in the code; but although it won't be required in a browser environment. face-api.js is not only a library for the browser anymore; if you are running in a nodejs environment fs will be required and used for reading model files from disk.=====""; ""Gotcha. Do you think there's a way to suppress the warning on the library side so that webpack doesn't complain?=====""; ""Thanks for your awesome work btw; it's really impressive how easy it is to implement facial detection using your library. Love it 🙏=====""; ""Hmm not sure if it's possible from the side of the library. The issue is the conditional require; since webpack cannot evaluate the condition. So I think your solution is the correct one; telling webpack; that it is safe to ignore the fs module.Edit: The only option I see; is not to require fs at all and monkeyPatch fs into the environment manually; as done with the canvas implementation etc.=====""; ""Alright; I don't have time to investigate that monkeyPatch thing so I'll just close the issue for now. Thanks again.=====""; ""to ignore the error. add this code on your webpack.mix.js`mix.webpackConfig({    node: {        fs: 'empty';    }})`=====""; 'For webpack 5 you need:```jsresolve: {    fallback: {      fs: false    }  }```This really should be fixed in this lib though. Having to do this in my webpack config is clunky.====='; '@maccuaa your solution worked for me. The warning went away. For anyone curious as to how the resolve option fixes the issue; please read the webpack docs https://webpack.js.org/configuration/resolve/#resolvefallback=====']",Build & Install Failure,Build & Initialization Failure,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Module Not Found""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/157,face-api.js Trying to Initialize NodeJS Env in Electron Renderer Process,13,open,2018-12-01T17:05:24Z,2019-02-17T15:09:13Z,"About a month ago; I opened an issue #113 which was related to my lack of necessary hardware which I have solved.However; with the latest version of `face-api.js` when using the same project structure and similar files; I am unable to load `face-api.js` the way mentioned in #113 and am getting this error when I try to load `face-api.js` in a renderer process: ![error](https://user-images.githubusercontent.com/21309909/49330716-63831280-f560-11e8-879a-88d0b658b7d4.png)Extra Info:- Loading `face-api.js` in a renderer process of my electron app.- `face-api.js` version 0.16.1- Using `face-api.js` with `electron`This is the project structure:![dir-structure](https://user-images.githubusercontent.com/21309909/49330646-5ade0c80-f55f-11e8-8196-310797f027de.png)index.html:```html<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <meta name=""viewport"" content=""width=device-width; initial-scale=1.0"">    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">    <title>Test</title></head><body>    <img id=""myImg"" src=""face.png"" alt="""">    <script src=""./node_modules/face-api.js/dist/face-api.js""></script>    <script src=""index.js""></script></body></html>```index.js:```javascriptvar img = document.getElementById(""myImg"");console.log(faceapi); // Just to check whether everything is working.(async () => {    await faceapi.loadSsdMobilenetv1Model('./models')    await faceapi.loadTinyFaceDetectorModel('./models')    await faceapi.loadMtcnnModel('./models')    await faceapi.loadFaceLandmarkModel('./models')    await faceapi.loadFaceLandmarkTinyModel('./models')    await faceapi.loadFaceRecognitionModel('./models')    // Used to work till 0.15.0 / 0.15.1 but doesn't work anymore.    var inf = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()    console.log(inf)})();```Looking into the `face-api.js` file where the error was thrown; it seems as if `face-api.js` thinks I'm using NodeJS even when I'm loading it in the browser.Thanks a lot; in advance!","['Hmm; the nodejs check is implemented [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/isNodejs.ts). If this check succeeds in the renderer process; then we will have to fix this.I will investigate in this issue.====='; '@justadudewhohacks Thank you! And it does seem like the NodeJS check succeeds in the renderer process.====='; '@justadudewhohacks Please get this fixed as soon as possible. I really need this to work for a project.Thank you.====='; ""The most straight forward solution would be to export the env inititializiation functions and call them manually: https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.tsFor now you can probably just manually monkey patch the environment back to a browser environment:```faceapi.env.monkeyPatch({    Canvas: HTMLCanvasElement;    Image: HTMLImageElement;    ImageData: ImageData;    Video: HTMLVideoElement;    createCanvasElement: () => document.createElement('canvas');    createImageElement: () => document.createElement('img')})```=====""; 'Thanks so much!====='; 'It worked!====='; 'Should be fixed now; see my answer in [this](https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3) issue.> I think the renderer process environment should now get initialized correctly; if you want to give face-api.js v0.16.2 a try.> > I also exported createBrowserEnv and createNodejsEnv; so in case there are still issues with incorrect initialization one can easily fix this by faceapi.env.setEnv(faceapi.env.createBrowserEnv()) for example.====='; 'Unfortunately; it didn’t work. I updated the module; removed the monkey patch; then tested my app; and it gave me the same error again :(====='; ""@justadudewhohacks are there any basic examples on using face-api with electron? I can't get it working and would really like to use it in a project=====""; ""@johndouglas3 You should take a look at #113 to find how I configured mine.I'm using it in my app and it works like a charm(with the monkey patch though.)=====""; '@Frixoe thank you for the quick reply; sorry I cant seem to figure it out to get it working. could you point me to a working example?is it in the render process or main process?how are you importing face-api.js?thank you!====='; ""I keep getting` Uncaught (in promise) Error: ENOENT: no such file or directory; open './models/tiny_face_detector_model-weights_manifest.json'`=====""; '@johndouglas3Is your project up on GitHub? If so; a link to it would really help.P.s. Apologies for the late reply.=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/164,Can´t get nodejs examples working,6,closed,2018-12-09T18:25:13Z,2018-12-12T18:11:57Z,Hello; can't figure out how to run examples for nodejs. Browser examples works fine.  Followed instructions in readme:>git clone https://github.com/justadudewhohacks/face-api.js.git>>cd face-api.js/examples/examples-nodejs>npm i>> tsc faceDetections>Output: https://pastebin.com/huEH2RVQ>and>ts-node faceDetection.ts>Output: https://pastebin.com/7AFfM6kiMachine is Ubuntu 16;04 on AWS.,"['Right; i have that npm not installed. How can i fix this?====='; '@ollfa Check out the [tsconfig](https://github.com/justadudewhohacks/face-api.js/blob/master/tsconfig.json) of this repo. Looks like a typescript issue.Second link looks like an issue with tfjs. Are you using the same tfjs-core version as face-api.js (0.13.8)?@persone16 npm comes with nodejs.====='; 'I think my problem connected with python version; i have installed 3+; but for this need lower than 3 version====='; ""You shouldn't need python to run the nodejs examples.You only need python (or pip for that matter) to install @tensorflow/tfjs-node; but I am also using python 3.6 and it works fine.=====""; 'It installed; then i deleted from variable path python 3.7.; and then i installed python 2.7.====='; 'Solved; thanks! Was using wrong version of tfjs-core. =====']",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Missing npm Package""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/169,Improve accuracy and stabilization?,11,closed,2018-12-14T16:26:20Z,2019-03-23T11:21:03Z,is there any easy way to improve to the accuracy and stabilize landmarks? trying with TINY_FACE_DETECTOR on mobile; sometimes landmarks go out of the face.,"['There are no parameters to tune the landmark prediction process; if that was your question.====='; 'Okay .. are you planning for any release to address this? thanks ====='; 'Can you show some screenshots please of what you mean by ""sometimes landmarks go out of the face.""?====='; '![capture3](https://user-images.githubusercontent.com/19422397/50052970-ed330280-0152-11e9-83e5-c04bd475ec77.PNG)Here you can see the landmarks are bit misplaced.====='; 'Hmm strange; they really look a bit off; are you using the default or the tiny model?Also keep in mind that the models are designed to run at realtime rather than providing state of the art accuracy.====='; 'tiny_face_detector_model-shard1 and face_landmark_68_model-shard1====='; 'I observed this usually happens when the face is near to camera.====='; 'is there any solution to this?====='; ""Also depends on the input size of the tiny face detector and how large the face appears in your image. What's the input size you are using for the face detector? Try increasing the input size a bit; maybe the faces are just too small; which will make them become blurry inputs.=====""; 'Increasing input size to 224 has improved the landmarks positions. But quite misplaced at the edges when I turn right. And the landmarks are bit shaky. The face covers more than 1/4 of camera preview.  thanks====='; 'Closing because the actual question of this issue seems to be answered.=====']",Unstable,Poor Performance,Improper Model Attribute,,,change input shape,Replace data Shape/type,Third-party library,Model Inference,"{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}",B.3.2,C.2
https://github.com/justadudewhohacks/face-api.js/issues/169,Improve accuracy and stabilization?,11,closed,2018-12-14T16:26:20Z,2019-03-23T11:21:03Z,is there any easy way to improve to the accuracy and stabilize landmarks? trying with TINY_FACE_DETECTOR on mobile; sometimes landmarks go out of the face.,"['There are no parameters to tune the landmark prediction process; if that was your question.====='; 'Okay .. are you planning for any release to address this? thanks ====='; 'Can you show some screenshots please of what you mean by ""sometimes landmarks go out of the face.""?====='; '![capture3](https://user-images.githubusercontent.com/19422397/50052970-ed330280-0152-11e9-83e5-c04bd475ec77.PNG)Here you can see the landmarks are bit misplaced.====='; 'Hmm strange; they really look a bit off; are you using the default or the tiny model?Also keep in mind that the models are designed to run at realtime rather than providing state of the art accuracy.====='; 'tiny_face_detector_model-shard1 and face_landmark_68_model-shard1====='; 'I observed this usually happens when the face is near to camera.====='; 'is there any solution to this?====='; ""Also depends on the input size of the tiny face detector and how large the face appears in your image. What's the input size you are using for the face detector? Try increasing the input size a bit; maybe the faces are just too small; which will make them become blurry inputs.=====""; 'Increasing input size to 224 has improved the landmarks positions. But quite misplaced at the edges when I turn right. And the landmarks are bit shaky. The face covers more than 1/4 of camera preview.  thanks====='; 'Closing because the actual question of this issue seems to be answered.=====']",Unstable,Poor Performance,Improper Model Attribute,,,change input shape,Replace data Shape/type,Third-party library,Model Inference,"{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/169,Improve accuracy and stabilization?,11,closed,2018-12-14T16:26:20Z,2019-03-23T11:21:03Z,is there any easy way to improve to the accuracy and stabilize landmarks? trying with TINY_FACE_DETECTOR on mobile; sometimes landmarks go out of the face.,"['There are no parameters to tune the landmark prediction process; if that was your question.====='; 'Okay .. are you planning for any release to address this? thanks ====='; 'Can you show some screenshots please of what you mean by ""sometimes landmarks go out of the face.""?====='; '![capture3](https://user-images.githubusercontent.com/19422397/50052970-ed330280-0152-11e9-83e5-c04bd475ec77.PNG)Here you can see the landmarks are bit misplaced.====='; 'Hmm strange; they really look a bit off; are you using the default or the tiny model?Also keep in mind that the models are designed to run at realtime rather than providing state of the art accuracy.====='; 'tiny_face_detector_model-shard1 and face_landmark_68_model-shard1====='; 'I observed this usually happens when the face is near to camera.====='; 'is there any solution to this?====='; ""Also depends on the input size of the tiny face detector and how large the face appears in your image. What's the input size you are using for the face detector? Try increasing the input size a bit; maybe the faces are just too small; which will make them become blurry inputs.=====""; 'Increasing input size to 224 has improved the landmarks positions. But quite misplaced at the edges when I turn right. And the landmarks are bit shaky. The face covers more than 1/4 of camera preview.  thanks====='; 'Closing because the actual question of this issue seems to be answered.=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change input shape,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",B.3.2,C.2
https://github.com/justadudewhohacks/face-api.js/issues/169,Improve accuracy and stabilization?,11,closed,2018-12-14T16:26:20Z,2019-03-23T11:21:03Z,is there any easy way to improve to the accuracy and stabilize landmarks? trying with TINY_FACE_DETECTOR on mobile; sometimes landmarks go out of the face.,"['There are no parameters to tune the landmark prediction process; if that was your question.====='; 'Okay .. are you planning for any release to address this? thanks ====='; 'Can you show some screenshots please of what you mean by ""sometimes landmarks go out of the face.""?====='; '![capture3](https://user-images.githubusercontent.com/19422397/50052970-ed330280-0152-11e9-83e5-c04bd475ec77.PNG)Here you can see the landmarks are bit misplaced.====='; 'Hmm strange; they really look a bit off; are you using the default or the tiny model?Also keep in mind that the models are designed to run at realtime rather than providing state of the art accuracy.====='; 'tiny_face_detector_model-shard1 and face_landmark_68_model-shard1====='; 'I observed this usually happens when the face is near to camera.====='; 'is there any solution to this?====='; ""Also depends on the input size of the tiny face detector and how large the face appears in your image. What's the input size you are using for the face detector? Try increasing the input size a bit; maybe the faces are just too small; which will make them become blurry inputs.=====""; 'Increasing input size to 224 has improved the landmarks positions. But quite misplaced at the edges when I turn right. And the landmarks are bit shaky. The face covers more than 1/4 of camera preview.  thanks====='; 'Closing because the actual question of this issue seems to be answered.=====']",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change input shape,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/194,Blob is not defined fetching local images,10,open,2019-01-15T15:07:34Z,2020-08-24T16:07:41Z,"Hi; i testing face-api.js and trying loading a local image for make face similarity and is throw me ""Blob is not defined"" in my nodeJS application; here the function where i try computing descriptors:```face.env.monkeyPatch({ fetch; Blob });const threshold = 0.6;var descriptors = { desc1: null; desc2: null };function computarDescriptor(aDesc; aUri) {  // ~statics/pics/001_tmp.jpg  face.fetchImage(aUri).then(function(result) {    console.log('fecthImage: ' + result);    descriptors['desc' + aDesc] = face.computeFaceDescriptor(result);  }).catch(err => {    console.log(err);  })}```node-fetch and blob is installed and monkeyPatched and referenceErros is still there.Thanks","[""Just added const Blob = require('node-fetch'); before bufferToImage function located at build/commonjs/dom/bufferToImage.js and worked fine..=====""; ""Ahh; currently face.env.monkeyPatch doesn't patch Blob; thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?`global.Blob = require('blob');`=====""; ""> Ahh; currently face.env.monkeyPatch doesn't patch Blob; thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> `global.Blob = require('blob');`After i define as global i get > TypeError: Right-hand side of 'instanceof' is not an object> tfjs-image-recognition-base\\src\\dom\\bufferToImage.ts:5:14=====""; ""> > Ahh; currently face.env.monkeyPatch doesn't patch Blob; thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> > `global.Blob = require('blob');`> > After i define as global i get> > > TypeError: Right-hand side of 'instanceof' is not an object> > > tfjs-image-recognition-base\\src\\dom\\bufferToImage.ts:5:14@justadudewhohacks Hi; I'm getting the same error as well. the fetchImage function isnt working after monkey patching fetch=====""; ""Hi @enecumene; @justadudewhohacks I had a similar issue a few minutes ago. The only difference is that I was trying to fetch the image from the cloud (cloudinary).I solved it by using the 'loadImage' function from the node 'canvas' package i.eimport { loadImage } from 'canvas';const referenceImage = await loadImage(refImageUrl);It might work for local images. Hope this helps=====""; 'Hi! I\'m working on API REST (Node v12.9.1 and Express 4.17.1) and have the same problem ""Blob is not defined"" when trying to fetch images from URL.I have tried all suggestions in this issue comments:**1.**  `global.Blob = require(\'blob\'); `and got same as @hanjeahwan > TypeError: Right-hand side of \'instanceof\' is not an object**2.** `const referenceImage = await loadImage(refImageUrl);` as @EziamakaNV suggested; but this doesn\'t work when I try to use faceapi.detectSingleFace() because is not HTMLImageElement; even though I already monkeypatched Image with Canvas...`detection = await faceapi.detectSingleFace(referenceImage).withFaceLandmarks().withFaceDescriptor();`> Argument of type \'Image\' is not assignable to parameter of type \'TNetInput\'. Type \'Image\' is not assignable to type \'HTMLImageElement\'.If i add //@ts-ignore ; then i get:> Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D; or to be an element idI also tried creating new Image() like this:```const canvas = require(\'canvas\');const img = new Image()const canvasCreated = canvas.createCanvas(200; 200)const ctx = canvasCreated.getContext(\'2d\')img.onload = () => ctx.drawImage(img; 0; 0)img.onerror = err => { throw err }img.src = URL_TO_IMAGE```Result: > UnhandledPromiseRejectionWarning: TypeError: media.addEventListener is not a functionIf you have any other suggestion I would be happy to try! Thanks in advance for your amazing work @justadudewhohacks ====='; 'Same scenario with @sebacampos currently I am building rest api with node. then I encountered this. ====='; ""> Ahh; currently face.env.monkeyPatch doesn't patch Blob; thanks for pointing this out. Maybe as for now you could simply try to expose Blob to the global scope?> `global.Blob = require('blob');`I am using this library in react and getting same problem with fetchImage function; could you please help me on this?=====""; 'I did this long time ago so I don\'t remember very well; but this was my working implementation: **monkeypatching fetch AND canvas classes:**```tsimport { loadImage; Canvas; Image; ImageData } from \'canvas\';// @ts-ignorefaceapi.env.monkeyPatch({ Canvas; Image; ImageData; fetch });export const detect = async (imageUrl: string): Promise<WithFaceDescriptor<any>> => {  let canvasImage;    try {    canvasImage = await loadImage(imageUrl);        if (!canvasImage) {      throw new Error(""Canvas not loaded."");    }  } catch (error) {     throw new Error(""Could not load image."");  }    const faces = await detectFaces(canvasImage);     return faces[0];} const detectFaces = (image: any): any => faceapi.detectAllFaces(image).withFaceLandmarks().withFaceDescriptors();```Hope it helps====='; 'Actually the issue is i am fetching my labeled faces ; from node api and Iwant to fetch that images with fetchImage ; and it\'s not working;ThanksOn Mon; 24 Aug 2020; 7:29 p.m. Sebastian Campos; <notifications@github.com>wrote:> I did this long time ago so I don\'t remember very well; but this was my> working implementation: *monkeypatching fetch AND canvas classes:*>> import { loadImage; Canvas; Image; ImageData } from \'canvas\';> // @ts-ignorefaceapi.env.monkeyPatch({ Canvas; Image; ImageData; fetch });>> export const detect = async (imageUrl: string): Promise<WithFaceDescriptor<any>> => {>   let canvasImage;>>   try {>     canvasImage = await loadImage(imageUrl);>>     if (!canvasImage) {>       throw new Error(""Canvas not loaded."");>     }>   } catch (error) {>      throw new Error(""Could not load image."");>   }>>   const faces = await detectFaces(canvasImage);>>   return faces[0];}> const detectFaces = (image: any): any => faceapi.detectAllFaces(image).withFaceLandmarks().withFaceDescriptors();>> Hope it helps>> —> You are receiving this because you commented.> Reply to this email directly; view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/194#issuecomment-679142698>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AHIVDUMFBRQ7HH6TUMZNWVTSCJW2LANCNFSM4GQETFOQ>> .>=====']",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/197,TypeError: Illegal constructor.,9,open,2019-01-21T15:32:05Z,2021-12-06T10:46:08Z,"Hi; I get the following error:> TypeError: Illegal constructor.when I do the following:`const detection = await faceapi.detectSingleFace(video);`video is basically an HTML element (document.getElementById(""video-stream""))I also tried passing an id to the faceapi.detectSingleFace; same error.I am using **node v10.15.0**; webpack; babel and React.I had an error about tensorflow saying that fs module could not be resolved; but I guess this is not related. Just in case...Here is the full code:```javascriptasync function detectFaces(video; canvas; width; height) {	window.logger.log(""detectFaces"");	const detection = await faceapi.detectSingleFace(video);	console.log(detection);}function loadVideo(video; canvas; width; height) {	window.logger.log(""loadVideo"");	navigator.mediaDevices.getUserMedia({		video: {			width : width;			height: height		}	}).then(function(stream) {		video.srcObject = stream;		video.play();		video.onloadedmetadata = function() {			detectFaces(video; canvas; width; height);		}	}).catch(function(error) {		console.log(error);	});}```Thank you.","['Hmm; can you post the full stacktrace please.====='; 'Hi ; could you tell me how you solve the problem ;pleaseeI have the same error====='; 'same error here in an electron-vue app:I\'m using the import statement `import * as faceapi from ""face-api.js"";`then ```await faceapi.loadSsdMobilenetv1Model(""static/models"");let detections = await faceapi.detectAllFaces(this.images[i]);```stacktrace:```C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10 Uncaught (in promise) TypeError: Illegal constructor    at createCanvasElement (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10)    at createCanvas (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:10)    at Object.createCanvasFromMedia (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:22)    at C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:37    at Array.forEach (<anonymous>)    at new NetInput (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:22)    at Object.<anonymous> (C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\toNetInput.js:53)    at step (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:133)    at Object.next (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:114)    at fulfilled (C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:104)createCanvasElement @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\env\\createNodejsEnv.js:10createCanvas @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:10createCanvasFromMedia @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\createCanvas.js:22(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:37NetInput @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\NetInput.js:22(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tfjs-image-recognition-base\\build\\commonjs\\dom\\toNetInput.js:53step @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:133(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:114fulfilled @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:104Promise rejected (async)step @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:106(anonymous) @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:107__awaiter @ C:\\Projects\\face-blur\\node_modules\\tslib\\tslib.js:103ComposableTask.then @ C:\\Projects\\face-blur\\node_modules\\face-api.js\\build\\commonjs\\globalApi\\ComposableTask.js:8VideoPage.vue?10d6:134 Uncaught (in promise) TypeError: Cannot read property \'detectAllFaces\' of undefined    at VueComponent._callee4$ (webpack-internal:///./node_modules/babel-loader/lib/index.js!./node_modules/vue-loader/lib/index.js?!./src/renderer/components/VideoPage.vue?vue&type=script&lang=js&:202:75)    at tryCatch (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:62:40)    at Generator.invoke [as _invoke] (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:296:22)    at Generator.prototype.(anonymous function) [as next] (webpack-internal:///./node_modules/regenerator-runtime/runtime.js:114:21)    at step (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:17:30)    at eval (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:35:14)    at new Promise (<anonymous>)    at new F (webpack-internal:///./node_modules/core-js/library/modules/_export.js:36:28)    at eval (webpack-internal:///./node_modules/babel-runtime/helpers/asyncToGenerator.js:14:12)    at VueComponent.detectFaces (webpack-internal:///./node_modules/babel-loader/lib/index.js!./node_modules/vue-loader/lib/index.js?!./src/renderer/components/VideoPage.vue?vue&type=script&lang=js&:223:10)_callee4$ @ VideoPage.vue?10d6:134tryCatch @ runtime.js?96cf:62invoke @ runtime.js?96cf:296prototype.(anonymous function) @ runtime.js?96cf:114step @ asyncToGenerator.js?0f75:17(anonymous) @ asyncToGenerator.js?0f75:35F @ _export.js?63b6:36(anonymous) @ asyncToGenerator.js?0f75:14detectFaces @ VideoPage.vue?10d6:134_callee$ @ VideoPage.vue?10d6:64tryCatch @ runtime.js?96cf:62invoke @ runtime.js?96cf:296prototype.(anonymous function) @ runtime.js?96cf:114step @ asyncToGenerator.js?0f75:17(anonymous) @ asyncToGenerator.js?0f75:28Promise rejected (async)step @ asyncToGenerator.js?0f75:27(anonymous) @ asyncToGenerator.js?0f75:28Promise resolved (async)step @ asyncToGenerator.js?0f75:27(anonymous) @ asyncToGenerator.js?0f75:35F @ _export.js?63b6:36(anonymous) @ asyncToGenerator.js?0f75:14(anonymous) @ VideoPage.vue?10d6:62emitTwo @ events.js:131emit @ events.js:214```seems it tries to use the nodejs version; but it should use the browser version====='; ""Yes ; I put this in my code and it works:faceapi.env.monkeyPatch({    Canvas: HTMLCanvasElement;    Image: HTMLImageElement;    ImageData: ImageData;    Video: HTMLVideoElement;    createCanvasElement: () => document.createElement('canvas');    createImageElement: () => document.createElement('img')}) =====""; ""@ciobanudan97 thanks; that solved my problem. For me the problem only occurred after monkey patching the 'fetch' method. Patching `createCanvasElement` as described above solved the issue for me.=====""; 'i am facing a similar error . can\'t seem to resolve it with the above mentioned  suggestions . here\'s the code`import { Component; Input; ViewChild; ElementRef; AfterViewInit; Inject } from \'@angular/core\';import * as faceapi from \'face-api.js\';import {WebcamImage; WebcamInitError; WebcamUtil} from \'ngx-webcam\';import {Subject;Observable} from \'rxjs\';import * as canvas from \'canvas\';import * as $ from \'jquery\';import { DOCUMENT } from \'@angular/common\'; faceapi.env.monkeyPatch({  Canvas: HTMLCanvasElement;  Image: HTMLImageElement;  ImageData: ImageData;  Video: HTMLVideoElement;  createCanvasElement: () => document.createElement(\'canvas\');  createImageElement: () => document.createElement(\'img\')  })@Component({  selector: \'app-root\';  templateUrl: \'./app.component.html\';  styleUrls: [\'./app.component.css\']})export class AppComponent implements AfterViewInit {  @Input() cameraName:string = """";  @ViewChild(\'myImage\';{ static: false }) imageInput: ElementRef;  title = \'imageRecognition\'; private doc:Documentconstructor(@Inject(DOCUMENT) document) {  this.doc=document;}ngOnInit() {this.loadModels();}ngAfterViewInit() {  } async loadModels() {  // load the models  const MODEL_URL = \'./assets/models/\'  await faceapi.loadSsdMobilenetv1Model(MODEL_URL)  await faceapi.loadFaceLandmarkModel(MODEL_URL)  await faceapi.loadFaceRecognitionModel(MODEL_URL)  const input = this.imageInput.nativeElement  let fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()   }   }`====='; 'this is reason : https://medium.com/@andreas.schallwig/do-not-laugh-a-simple-ai-powered-game-3e22ad0f8166@ciobanudan97 for him answer.====='; ""@step4  Like the problems I havewe need to manually monkey patch the environment back to a browser environment.add this after import face-api.js// configure face APIfaceapi.env.monkeyPatch({  Canvas: HTMLCanvasElement;  Image: HTMLImageElement;  ImageData: ImageData;  Video: HTMLVideoElement;  createCanvasElement: () => document.createElement('canvas');  createImageElement: () => document.createElement('img')});and it works=====""; 'You must set options for that like tinyModel or Yolyo=====']",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.6""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/200,nodejs examples can't working,7,closed,2019-01-23T08:08:35Z,2019-01-28T09:40:56Z,"macos    node v10.7.0     npm 6.6.0     tfjs-core ""0.13.8""  git clone https://github.com/justadudewhohacks/face-api.js.gitcd face-api.js/examples/examples-nodejsnpm its-node faceDetection.tslocalhost:examples-nodejs hwanpenn$ ts-node faceDetection.ts2019-01-23 15:48:46.382968: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPUsupports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend(node:33079) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function    at new Tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)    at Function.Tensor.make (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:430:12)    at Object.tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/tensor_ops.ts:99:17)    at _loop_1 (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/io_utils.ts:129:15)    at Object.decodeWeights (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/dist/io/io_utils.js:132:9)    at /Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:221:13    at Array.forEach (<anonymous>)    at /Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:216:22    at Array.forEach (<anonymous>)    at Object.<anonymous> (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/io/weights_loader.ts:197:25)(node:33079) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:33079) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.**similar with Can´t get nodejs examples working #164  ;but my tfjs-core version is right; i dont know how to fix it  ;Browser examples works fine.**","['Try to clean your node_modules and start over with a fresh npm install. The error message does not come from face-api.js; but from tfjs-core.====='; 'Some error====='; '**thanks @justadudewhohacks  ; love you ^_^; with your suggest i fix that;but。。。**TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(3;16): error TS2354: This syntax requires an imported helper but module \'tslib\' cannot be found.faceDetection.ts(5;26): error TS2339: Property \'loadFromDisk\' does not exist on type \'SsdMobilenetv1\'.faceDetection.ts(10;23): error TS2339: Property \'createCanvasFromMedia\' does not exist on type \'typeof ""/Users/hwanpenn/Downloads/hwanpenn/face01/src/index""\'.faceDetection.ts(11;11): error TS2339: Property \'drawDetection\' does not exist on type \'typeof ""/Users/hwanpenn/Downloads/hwanpenn/face01/src/index""\'.environment ：ts-node v7.0.1    node v11.6.0    typescript v2.8.4**same with Could not run faceDetection.ts #142now im trying fix this**====='; ""**cd   /face-api.js/  and npm install  ;now we will hava tslib^1.9.3it seem work  ;no “TSError: ⨯ Unable to compile TypeScript:” again;**but new error  （）：2019-01-28 13:43:14.462031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPUsupports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend(node:88557) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function    at new Tensor (/Users/hwanpenn/Downloads/hwanpenn/face01/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)**Isn't that my first mistake?   Wrapped around a large circle ; fuck...**=====""; ""**i want to die ; i fix the problem upstairs;and i have a new one again。。。**2019-01-28 15:40:11.349408: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:228    return new TSError(diagnosticText; diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:../../src/ssdMobilenetv1/outputLayer.ts(73;7): error TS2345: Argument of type '[number; number | undefined]' is not assignable to parameter of type 'number[] | [number; number; number] | [number; number; number; number] | [number; number] | [number] | [number; number; number; number; number] | [number; number; number; number; number; number]'.  Type '[number; number | undefined]' is not assignable to type '[number; number]'.    Type 'number | undefined' is not assignable to type 'number'.      Type 'undefined' is not assignable to type 'number'.    at createTSError (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:228:12)    at getOutput (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:334:40)    at Object.compile (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:367:11)    at Module.m._compile (/Users/hwanpenn/.nvm/versions/node/v10.7.0/lib/node_modules/ts-node/src/index.ts:413:43)    at Module._extensions..js (internal/modules/cjs/loader.js:700:10)=====""; ""Hold on for a second. > (node:33079) UnhandledPromiseRejectionWarning: TypeError: trackerFn(...).nextTensorId is not a function> at new Tensor (/Users/hwanpenn/Documents/hwanpenn/face-api.js/node_modules/@tensorflow/tfjs-core/src/tensor.ts:415:27)I think this is an issue with tfjs-node@0.1.19; which is the current in the package.json of the nodejs examples. It should actually be tfjs-node@0.1.21 (same as in the root package.json).> TSError: ⨯ Unable to compile TypeScript:> ../../src/ssdMobilenetv1/outputLayer.ts(73;7): error TS2345: Argument of type '[number; number | undefined]' is not assignable to parameter of type 'number[] | [number; number; number] | [number; number; number; number] | [number; number] | [number] | [number; number; number; number; number] | [number; number; number; number; number; number]'.> Type '[number; number | undefined]' is not assignable to type '[number; number]'.> Type 'number | undefined' is not assignable to type 'number'.> Type 'undefined' is not assignable to type 'number'.This is a typescript 3 specific error; which indicates that you are not using ts-node v7.0.1 anymore. I am currently updating the dependencies of face-api.js including upgrading the typescript version to latest.=====""; '1.git clone https://github.com/justadudewhohacks/face-api.js.git2.examples/examples-nodejs/package.json ""@tensorflow/tfjs-node"": ""^0.1.19""-->""^0.1.21""3.cd face-api.js    npm install    （fix tslib）    npm audit fix       （add karma-typescript@3.0.13）4.cd face-api.js/examples/examples-nodejs    npm install    ts-node faceDetection.ts **you are right @justadudewhohacks ;It should be tfjs-node@0.1.21 ;it works ;now i love you more** =====']",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/201,Web examples don't work on firefox,2,closed,2019-01-23T14:18:28Z,2019-01-24T12:44:33Z,Hi !I noticed that web example doesn't work on firefox.More precisely; the asynchronous functions detectSingleFace and detectAllFaces never end.I can't explain why and honestly; I don't even know why I tried this package on Firefox since every single complex project I run into doesn't seem to work on that browser.But maybe you could specify it on README's package ?Regards,"['Are you referring to the same issue as #129? Otherwise should work fine on firefox.====='; ""Yep; that's exactly the same issue ! Sorry; I should have check better. I am closing this one.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,Dom attribute,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.2] JS Variable Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/210,Error running node example,6,open,2019-02-03T10:33:50Z,2020-08-14T23:12:00Z,"I followed the directions on the README and I get the following error:```npm ERR! path /Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/face-api.js-8127d704/node_modules/@babel/code-framenpm ERR! code ENOENTnpm ERR! errno -2npm ERR! syscall renamenpm ERR! enoent ENOENT: no such file or directory; rename '/Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/face-api.js-8127d704/node_modules/@babel/code-frame' -> '/Users/codydaig/dev/face-api.js/examples/examples-nodejs/node_modules/.staging/@babel/code-frame-dce0a722'npm ERR! enoent This is related to npm not being able to find a file.npm ERR! enoent npm ERR! A complete log of this run can be found in:npm ERR!     /Users/codydaig/.npm/_logs/2019-02-03T10_27_19_451Z-debug.log```I tried changing the version in package.json of ""@tensorflow/tfjs-node"" to ""0.1.21"" and performing a fresh install and get the same error. I've tried this on two different computers. Primary computer:Mac OS X 10.14.2. Node v 11.1.0npm v6.5.0Any suggestions? Thanks in advance!","['Hi; @codydaig!I got the same error.To fix this I have just changed version of **tensorflow/tfjs-node** to 0.1.9:`""@tensorflow/tfjs-node"": ""^0.1.9"";`====='; 'Try cleaning your node_modules and reinstall them.====='; 'I’ve tried both of those several times and same error. ====='; ""@justadudewhohacks Circling back to this. I've tried multiple times on multiple different computers; operating systems; etc... and am unable to get any of the examples to run. Any suggestions?=====""; 'Same error here on Windows or Linux====='; 'I was having this issue. I rolled back to 0.1.9 as @NikolayYakovenko suggested; deleted my node_modules folder and package-lock; reinstalled with npm install; and restarted my development server- and this fixed it.=====']",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Missing file or directory during npm install""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/222,FaceLandmarks68: left and right eyes/brows are reversed.,10,open,2019-02-21T07:23:01Z,2021-01-15T06:46:18Z,Hello;`FaceLandmarks68.getLeftEye()` returns points corresponding to the right eye; and `FaceLandmarks68.getRightEye()` returns points corresponding to the left eye.Same issue for the `FaceLandmarks68.getLeftEyeBrow()` and `FaceLandmarks68.getRightEyeBrow()`.,"['Hi; thanks for reporting. I would highly appreciate PRs for fixing this. :)====='; '@a-bronx would you like to share your case so we can help?====='; 'Here is a quick test that every point of the right eye/brow has a correct horizontal position relative to the left eye/brow; given that ""right"" is closer to the origin than ""left"":```javascriptconst result = await faceLandmark68Net.detectLandmarks(imgElRect) as FaceLandmarks68expect(    Math.max(...result .getRightEyeBrow().map(_=>_.x)) <     Math.min(...result .getLeftEyeBrow().map(_=>_.x))).toBe(true);expect(    Math.max(...result .getRightEye().map(_=>_.x)) <     Math.min(...result .getLeftEye().map(_=>_.x))).toBe(true);```To fix it just rename FaceLandmarks68.getLeftXXX() to FaceLandmarks68.getRightXXX() and vice versa; it is just a labeling issue; all geometry is correct.NOTE: chances a low; but it still may be a breaking change for someone who do calculations based on landmark points.Sending you PR; but I cannot build the library and run unit tests on my Windows box without Python (and I have no interest to install it); so it will be a ""blind"" fix and it is up to you to verify and probably correct it.====='; ""That was a reason I asked to share your case because we can't swap names for one single case. There are others that were successful getting the right pointer results.=====""; 'My case is a face liveness check based on certain statistic of relative movements of landmark points in a video stream. Before making the check; I validate the face geometry to ensure it is not distorted due to fast movement; glasses etc (which sometime happens); so these garbled ""Picasso-like"" frames do not affect the statistics. As part of this sanity check I verify that the right eye is correctly positioned relative to the left eye. The assumption is that in a normal (non-mirrored) photo/video frame the subjects\' right eye will be closer to the left border of the frame than the left one. But the `FaceLandmarks68` does not support this assumption; my code was not working and I had to debug to find out that method names are misleading.====='; 'So I think based on your example I see your solution is based on a non-mirrored image. Maybe to have an option or parameter passed will solve problem. Will try; test and let you know.====='; 'By default source images and source video streams come non-mirrored; so it should be a baseline. If I need to flip photo/video; I use CSS on the whole canvas (e.g. `transform: scaleX(-1);`) instead of processing the source frame; which is much cheaper; especially for a web camera video stream.But here is a catch: if I mirror the video in a browser using CSS ; then I cannot use the `drawDetection()` anymore because score text under the bounded box becomes mirrored too and not readable. It is a non-issue for me as I draw a custom face boundary anyway; so just letting you know :)  But if you decide to make `drawDetection()` support drawing over a mirrored representation and flip text labels accordingly; then you need a parameter here.====='; 'Yes; Flip of the stream (mirroring) is something important (must have) when doing face recognition using a webcam. Hope we can get its support soon. ====='; '> By default source images and source video streams come non-mirrored; so it should be a baseline. If I need to flip photo/video; I use CSS on the whole canvas (e.g. `transform: scaleX(-1);`) instead of processing the source frame; which is much cheaper; especially for a web camera video stream.> > But here is a catch: if I mirror the video in a browser using CSS ; then I cannot use the `drawDetection()` anymore because score text under the bounded box becomes mirrored too and not readable. It is a non-issue for me as I draw a custom face boundary anyway; so just letting you know :) But if you decide to make `drawDetection()` support drawing over a mirrored representation and flip text labels accordingly; then you need a parameter here.After playing around a bit; I found a work around. First you need to create a second canvas that is not flipped. And then you need to use the customised draw function: `drawBox.draw` and `drawText.draw`.  You need to find the correct X coordinate; which is `canvas_new.width - box.x - box.width`====='; '> Yes; Flip of the stream (mirroring) is something important (must have) when doing face recognition using a webcam. Hope we can get its support soon.Just wondering if this option was ever added to the api=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data postprocess,Add data processing,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/224,Build without tfjs,4,closed,2019-02-24T20:45:08Z,2019-03-01T11:56:44Z,I use faceapi on a web page that already imports tfjs using their CDN.Is there a build of faceapi that doesn’t include tfjs so that it’ll be smaller and no warning about tfjs having been included twice?,"['You mean a bundled face-api.js script? No there is no such build. There would also probably be conflicts regarding the tfjs-core version; if there is a mismatch.====='; 'Right; something like Google does when you use their CDN bundles.For example; when I want to use body-pix; I simply have to do:```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix""></script>```It\'d be nice to be able to add face-api.js below in order to use body-pix and face-api on the same webapp.Laurent====='; ""Why are you pulling all the scripts from CDNs and don't bundle tfjs; body-pix and face-api.js together?=====""; 'Because it’s easy to try things in pure JavaScript without having to setup a dev environment that needs TypeScript.But I should try.=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Multiple Imports Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/226,Performance issue on the firefox at low laptop of ThinkPad X201.,1,open,2019-02-26T00:56:14Z,2019-03-01T08:45:46Z,"There is no landmark trace on the face when testing ""Webcam Face Landmark Detection"" on the laptop of ThinkPad X201 as well as ""Video Face Tracking"" and the video play is very slow.So my question is that any performance description for the project? or any hardware requirement able support face-api.js run well?Thanks & Best RegardsSui  ",['First of all; I would check whether tfjs is running correctly on your ThinkPad; e.g. whether the WebGL backend is registered correctly. Try to run some of the tfjs demos on your Pad first.====='],Slow Execution,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,E
https://github.com/justadudewhohacks/face-api.js/issues/226,Performance issue on the firefox at low laptop of ThinkPad X201.,1,open,2019-02-26T00:56:14Z,2019-03-01T08:45:46Z,"There is no landmark trace on the face when testing ""Webcam Face Landmark Detection"" on the laptop of ThinkPad X201 as well as ""Video Face Tracking"" and the video play is very slow.So my question is that any performance description for the project? or any hardware requirement able support face-api.js run well?Thanks & Best RegardsSui  ",['First of all; I would check whether tfjs is running correctly on your ThinkPad; e.g. whether the WebGL backend is registered correctly. Try to run some of the tfjs demos on your Pad first.====='],Slow Execution,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",D,E
https://github.com/justadudewhohacks/face-api.js/issues/226,Performance issue on the firefox at low laptop of ThinkPad X201.,1,open,2019-02-26T00:56:14Z,2019-03-01T08:45:46Z,"There is no landmark trace on the face when testing ""Webcam Face Landmark Detection"" on the laptop of ThinkPad X201 as well as ""Video Face Tracking"" and the video play is very slow.So my question is that any performance description for the project? or any hardware requirement able support face-api.js run well?Thanks & Best RegardsSui  ",['First of all; I would check whether tfjs is running correctly on your ThinkPad; e.g. whether the WebGL backend is registered correctly. Try to run some of the tfjs demos on your Pad first.====='],Incorrect Functionality,Incorrect Functionality,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,E
https://github.com/justadudewhohacks/face-api.js/issues/226,Performance issue on the firefox at low laptop of ThinkPad X201.,1,open,2019-02-26T00:56:14Z,2019-03-01T08:45:46Z,"There is no landmark trace on the face when testing ""Webcam Face Landmark Detection"" on the laptop of ThinkPad X201 as well as ""Video Face Tracking"" and the video play is very slow.So my question is that any performance description for the project? or any hardware requirement able support face-api.js run well?Thanks & Best RegardsSui  ",['First of all; I would check whether tfjs is running correctly on your ThinkPad; e.g. whether the WebGL backend is registered correctly. Try to run some of the tfjs demos on your Pad first.====='],Incorrect Functionality,Incorrect Functionality,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,E
https://github.com/justadudewhohacks/face-api.js/issues/228,Trying to get this to work in Angular,8,closed,2019-02-27T23:19:34Z,2019-03-27T14:07:06Z,"I have been trying to get this to work in an Angular component; without success; and would love a bit of help/direction. Await dies on the vine and I'm stumped as to what to do next beyond throwing more darts at the wall. Is there a debug flag somewhere?**[package.json]**""face-api.js"": ""^0.18.0"";**[component.html]**< div style=""position: relative"" class=""margin"">  < video id=""videoEl"" autoplay muted>< /video>  < canvas id=""overlay"">< /canvas>< /div>**[component.ts]**import { Component; OnInit } from '@angular/core';import * as $ from 'jquery';// implements nodejs wrappers for HTMLCanvasElement; HTMLImageElement; ImageDataimport * as canvas from 'canvas';import * as faceapi from 'face-api.js';import { CPU_ENVS } from '@tensorflow/tfjs-core/dist/test_util';// patch nodejs environment; we need to provide an implementation of// HTMLCanvasElement and HTMLImageElement; additionally an implementation// of ImageData is required; in case you want to use the MTCNNconst { Canvas; Image; ImageData } = canvas;faceapi.env.monkeyPatch({ Canvas; Image; ImageData });const TINY_FACE_DETECTOR = 'tiny_face_detector';@Component({  selector: 'app-facetrack';  templateUrl: './facetrack.component.html';  styleUrls: ['./facetrack.component.css']})export class FacetrackComponent implements OnInit {  // tiny_face_detector options  inputSize = 512;  scoreThreshold = 0.5;  withFaceLandmarks = false;  withBoxes = true;  videoEl;  stream;  constructor(  ) {  }  ngOnInit() {    this.loadModels();  }  // ======================================================================  // BEGIN: startTracking  // ======================================================================  async startTracking() {    let cnvs;    const vEl = <HTMLCanvasElement>document.getElementById('videoEl');    const vElwidth = vEl.width;    const vElheight = vEl.height;    console.log('start tracking...');    const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 320; scoreThreshold: 0.5 });    const ts = Date.now();    setTimeout(function () {      console.log('this.videoEl = ' + vEl);      // const detections = faceapi.detectSingleFace(vEl).withFaceLandmarks();      const detections = faceapi.detectSingleFace(vEl; options);      console.log('detections');      console.log(detections);      // resize the detected boxes in case your displayed image has a different size then the original      const detectionsForSize = faceapi.resizeResults(detections; { width: vElwidth; height: vElheight });      console.log('detectionsForSize');      console.log(detectionsForSize);      // draw them into a canvas      cnvs = document.getElementById('overlay');      // this.canvas.getContext('2d');      canvas.width = vEl.width;      canvas.height = vEl.height;      faceapi.drawDetection(cnvs; detectionsForSize; { withScore: true });    }; 5000);  }  // ======================================================================  // END: startTracking  // ======================================================================  // ======================================================================  // BEGIN: cameraConnect  // ======================================================================  async cameraConnect() {    // try to access users webcam and stream the images    // to the video element    this.stream = await navigator.mediaDevices.getUserMedia({ video: {} });    this.videoEl = $('#videoEl').get(0);    this.videoEl.srcObject = this.stream;    if (this.videoEl.srcObject.active === true) {      this.startTracking();    } else {      console.log('Camera not connected...');    }  }  // ======================================================================  // END: cameraConnect  // ======================================================================  // ======================================================================  // BEGIN: loadModels  // ======================================================================  async loadModels() {    // load the models    await faceapi.loadTinyFaceDetectorModel('./assets/weights/');    await faceapi.loadFaceRecognitionModel('./assets/weights/');    this.cameraConnect();  }  // ======================================================================  // END: loadModels  // ======================================================================}","['The output for ""console.log(detections);"" is: DetectSingleFaceTask {input: video#videoEl; options: TinyFaceDetectorOptions}input: video#videoEloptions: TinyFaceDetectorOptions {_name: ""TinyFaceDetectorOptions""; _inputSize: 320; _scoreThreshold: 0.5}__proto__: DetectFacesTaskBase====='; 'The output for ""console.log(detectionsForSize);"" is:DetectSingleFaceTask {input: video#videoEl; options: TinyFaceDetectorOptions}input: video#videoEloptions: TinyFaceDetectorOptions {_name: ""TinyFaceDetectorOptions""; _inputSize: 320; _scoreThreshold: 0.5}__proto__: DetectFacesTaskBase====='; 'You forgot the await keyword: `const detections = await faceapi.detectSingleFace(vEl; options)`; detectSingleFace returns a Promise. Therefore your function also has to be async.====='; 'When I use ""await faceapi.detectSingleFace(vEl; options)"" nothing happens. Is there a debug flag or a way to get some console output?====='; ""I had the same problem but instead of giving to detectSingleFace the video to eat I gave it a canvas I in wich I draw the video's frames. Seems to be working. I would yet be interested to know why feeding the video to the function doesn't work in this context.=====""; '@Getsuren l have the same problem; but l open a new tab: Live Demos page; my app  seems to be working. ====='; 'I think you are facing the same issue as #129. I added a comment how to fix this. I still have to fix the example code.====='; 'Closing here. If the issue still persists after following the advice in my comment above; feel free to reopen.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,Dom attribute,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/252,Error: TypeError: trackerFn is not a function at new e (tf-core.esm.js:17),4,open,2019-03-24T17:42:48Z,2019-12-28T18:30:09Z,Hi; I'm getting the following error when I tried to load a loadFaceLandmarkModel. I don't believe that is an error that is directly associated with face-api.js but an incompatibility of packages versions. I already created a question on stackoverflow to seek help for the comunity:https://stackoverflow.com/questions/55326589/error-typeerror-trackerfn-is-not-a-function-at-new-e-tf-core-esm-js17Can you please help me solve this problem?Thank you!,"["">  I don't believe that is an error that is directly associated with face-api.js but an incompatibility of packages versions.That's my guess as well. Which version of tfjs-core are you using?=====""; 'I have a similar problem! I have used tfjs version >1.0.0 ====='; 'same problem here====='; 'I was having the same problem. I resolved by inverting the package installation order: npm i @tensorflow/tfjs-node canvas face-api.jsThus; the version of tensorflow-core installed inside face-api is now 1.4.0. Otherwise version 1.2.9 had been installed.=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/254,Update to work with latest @tensorflow/tfjs-node,3,closed,2019-03-26T14:49:09Z,2020-12-05T03:42:44Z,When testing using the latest @tensorflow/tfjs-node it does not load correctly and you end up with this error:`============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================`Processing images then takes a good 10 secondsI think face-api.js is only compatible with older versions of @tensorflow/tfjs-node as it references ^0.2.3. After downgrading to this version my image processing time drops down to 0.4 seconds per image. Let me know if there is a way to run the latest @tensorflow/tfjs-node with face-api.jsThanks,"[""> I think face-api.js is only compatible with older versions of @tensorflow/tfjs-node as it references ^0.2.3. That's correct; face-api.js currently uses tfjs-core version 0.14.2; thus you need to install a compatible tfjs-node version.PR for upgrading to latest tfjs-core version is already up #246. Will upgrade as soon as possible.=====""; 'Thanks for the prompt reply; just wanted to raise as when trying to run the code initially in NodeJS I hit this issue and it was unclear how to resolve it.Look forward to the update for the tfjs-core.Thanks====='; 'What are the best versions of face-api; tfjs-core and tfjs-node to run together?=====']",Regression,Poor Performance,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Compatibility Issues""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",B.3.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/255,version of @tensorflow/tfjs-node,1,closed,2019-03-27T10:03:55Z,2019-03-27T10:10:11Z,"when i use newest version of  @tensorflow/tfjs-node; ""faceapi.detectSingleFace()"" will become like 100 times slower than using v0.2.3",['duplicate of #254 ====='],Regression,Poor Performance,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",B.3.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/277,Mobile browser getting stuck,4,open,2019-04-26T07:59:08Z,2020-06-09T06:02:34Z,Thanks for the cool library. I'm experiencing issues on mobile and would like to know whether there are any limitations or known issues for mobile? What happens in my case is that it appears as if tensor flow or another internal component gets stuck sometimes on the live face tracking and then the expression detection also gets stuck unless I manually position my phone so that my face fits in the stationary bound box. I'm using your demo link here https://justadudewhohacks.github.io/face-api.js/webcam_face_expression_recognition and viewing it on a Samsung S9+ running Chrome 73.0.3683.90. I've attached to my mobile via the Chrome debugger and no errors are being reported when I'm experiencing this issue.,"[""I believe I'm seeing a related issue on an iPhone 7 running iOS 12.3.1. The same demo link @die-rooikat  references above freezes on a single frame from my camera after I allow the use of the camera. The UI that is supposed to show over the camera feed never loads and the camera feed never updates. There are no errors in the debugger when connecting to my laptop.=====""; 'Same issue here @nsbingham. Any update?====='; 'Did you find a solution for your question? ====='; 'No update from my side issue still persists.=====']",Browser Hangs,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.2,E
https://github.com/justadudewhohacks/face-api.js/issues/280,@tensorflow/tfjs-node is not correctly recognised when running examples,7,open,2019-04-28T12:43:01Z,2021-09-17T03:14:54Z,I am trying to run the NodeJS examples. The examples run; but seem to not properly utilize the TensorFlow C++ binding.### Steps to reproduce```cd face-api.js/examples/examples-nodejsnpm itsc faceDetection.tsnode faceDetection.js```### Expected outcome:Examples runs without warnings.### Actual outcome:```============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================```### EnvironmentNode: `v11.14.0`face-api.js: `0.19.0`@tensorflow/tfjs-node: `1.0.2`### Additional InformationsI tried running the examples with older versions to exclude the possibility of a problem with my setup. I managed to run the examples for face-api.js versions `0.17.0` and `0.17.1` with face-api.js picking up the TensorFlow bindings correctly.,"[""Strange; can you provide the output of the following:``` javascriptimport tfnode from '@tensorflow/tfjs-node';console.log(tfnode.version)```=====""; '> ```js> import tfnode from \'@tensorflow/tfjs-node\';> console.log(tfnode.version)> ```fails with ```faceDetection.ts:5:8 - error TS1192: Module \'""<some path>/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-node/dist/index""\' has no default export.```I instead ran ```jsimport * as tfjsnode from \'@tensorflow/tfjs-node\';console.log(tfjsnode.version)```Which yields```{ \'tfjs-core\': \'1.1.2\';  \'tfjs-data\': \'1.1.2\';  \'tfjs-layers\': \'1.1.2\';  \'tfjs-converter\': \'1.1.2\';  tfjs: \'1.1.2\';  \'tfjs-node\': \'1.1.2\' }```**Edit:** I had to rerun the **Steps to reproduce** and apparently it now installs a more recent version of `tfjs` (`1.0.2` -> `1.1.2`). However; the problem (`Hi there 👋....`) remains even with version `1.1.2`.====='; 'Hmm; how about explicitly installing the correct tfjs-node version? `npm i @tensorflow/tfjs-node@1.0.2`====='; ""Ok; after ```npm i @tensorflow/tfjs-node@1.0.2```the example runs correctly; without warning. The version output is:```{ 'tfjs-core': '1.0.4';  'tfjs-data': '1.0.4';  'tfjs-layers': '1.0.4';  'tfjs-converter': '1.0.4';  tfjs: '1.0.4';  'tfjs-node': '1.0.2' }```However; now I am completely lost; what is happening here 🤔 Does this mean I need to make sure `face-api.js` and the example use the exact same version of `tfjs-node`?=====""; 'face-api.js does not use tfjs-node at all. face-api.js is depending on tfjs-core. So in order to run face-api.js with the tfjs-node backend; you have to install a tfjs-node version; which is compatible with the tfjs-core version that face-api.js uses.====='; 'Sorry for the confusion.I Time Machine restored my repo to the state I had when I opened this issue. Now; I get```{ \'tfjs-core\': \'1.1.0\';  \'tfjs-data\': \'1.1.0\';  \'tfjs-layers\': \'1.1.0\';  \'tfjs-converter\': \'1.1.0\';  tfjs: \'1.1.0\';  \'tfjs-node\': \'1.1.0\' }============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================done; saved results to out/faceDetection.jpg```Aha; so I actually used `@tensorflow/tfjs-node@1.1.0`; not `@tensorflow/tfjs-node@1.0.2` as said in OP. Apologies! So; it looks like the problem arises when using `@tensorflow/tfjs-node>=1.1.0`. Indeed after ```diff@@ -2;7 +2;7 @@   ""author"": ""justadudewhohacks"";   ""license"": ""MIT"";   ""dependencies"": {-    ""@tensorflow/tfjs-node"": ""^1.0.2"";+    ""@tensorflow/tfjs-node"": ""~1.0.2"";     ""canvas"": ""^2.0.1"";     ""face-api.js"": ""../../""   }```and```npm install```It now runs correctly 🎉 ```{ \'tfjs-core\': \'1.0.4\';  \'tfjs-data\': \'1.0.4\';  \'tfjs-layers\': \'1.0.4\';  \'tfjs-converter\': \'1.0.4\';  tfjs: \'1.0.4\';  \'tfjs-node\': \'1.0.3\' }cpu backend was already registered. Reusing existing backenddone; saved results to out/faceDetection.jpg```====='; '> face-api.js does not use tfjs-node at all. face-api.js is depending on tfjs-core. So in order to run face-api.js with the tfjs-node backend; you have to install a tfjs-node version; which is compatible with the tfjs-core version that face-api.js uses.Ok; I think I got it now. However; this is valuable information and might be important for newcomers that try to get started with face-api.js in a Node.js environment. #### SuggestionShould we ...1. pin the `tfjs-node` version in the Node.js example; such that the example will run correctly on a freshly cloned repo and2. add a small comment somewhere in the Node.js example explaining that a compatible `tfjs-node` version is needed?If no objections; I am happy to send PR for the above mentioned.=====']",Incorrect Functionality,Incorrect Functionality,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Version Compatibility Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",D,A.5
https://github.com/justadudewhohacks/face-api.js/issues/302,Low face recognition accuracy for asian guys,3,open,2019-05-23T09:20:51Z,2020-03-16T01:14:10Z,We tried with our company employees (Asian faces) but sometimes the distance between 2 different faces are quite low (below 0.3); we can easily separate them with eyes but looks the face-api pre-trained model could not separate them apart; we tried some western faces and it can work well. So I am thinking if there are not so many Asian face samples in the training dataset so it may not perform very well towards them? If I have some Asian face dataset and want to train a new model; would you please advice how should I do the retrain? Thanks.,"[""Yes your assumption is correct; the author of dlib (the project where the model is from) also states somewhere in his blog I think; that there is a bias towards western faces. I think you can retrain the model using the corresponding dlib example; but afterwards the model weights have to be converted to a the format that tensorflow uses. But I can help you with the latter one; if you really want to retrain the model.Other than that; I am planning to train an own face recognition model as well since I want to have a more web friendly and more efficient model for face-api.js. But I can't make any promises when that model will be included.=====""; ""> Yes your assumption is correct; the author of dlib (the project where the model is from) also states somewhere in his blog I think; that there is a bias towards western faces. I think you can retrain the model using the corresponding dlib example; but afterwards the model weights have to be converted to a the format that tensorflow uses. But I can help you with the latter one; if you really want to retrain the model.> > Other than that; I am planning to train an own face recognition model as well since I want to have a more web friendly and more efficient model for face-api.js. But I can't make any promises when that model will be included.My project also encountered this problem and I had to use a traditional C/C++ face API. So I'm really looking forward to have this feature in the future. Also please please consider to bring liveness detection to face-api; so I can make sure it is a living person standing in front of a cam not a picture.=====""; ""@dr1llc4t ; hi; I am also interested in retrain the model with a new dataset; would you mind telling me how far have you gone? Or in which direction have you been working on? my email address is sxtgwzz@163.com. ( I don't know what language should I use here; so I just use the common one )Thanks!=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/312,Slow in nodejs project,8,closed,2019-06-05T14:21:21Z,2020-03-14T19:53:09Z,Hello!I am trying to setup simple face detection in nodejs env. Out of the box everything run fine with the exception of speed. It takes 12+ seconds for a single face detection.`require('@tensorflow/tfjs-node')`is included!On execution i see:> cpu backend was already registered. Reusing existing backendthen> Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up > dramatically; install our node backend; which binds to TensorFlow C++; by running npm i > @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call > require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit > https://github.com/tensorflow/tfjs-node for more details.When i install older versions of the two packages:`npm install @tensorflow/tfjs-node@0.3.0 face-api.js@0.14.3`there is significant speed improvement; BUT i need age and gender detection ; which is not present below face-api.js < 0.20.0I also tryed:@tensorflow/tfjs-node version 1.0.2but then i get:> (node:29723) UnhandledPromiseRejectionWarning: TypeError: tfc.ENV.findBackend is not a > function>    at nodeBackend (/home/pi/testproject/node_modules/@tensorflow/tfjs-> node/dist/ops/op_utils.js:25:28)How can i use age and gender recognition and get faster speed under nodejs?**Environment**Raspberry PiNode: v8.10.0face-api.js: 0.20.0 / 0.14.3@tensorflow/tfjs-node: 1.1.2 / 1.0.2 / 0.3.0,"['@tensorflow/tfjs-node 1.0.2 + @tensorflow/tfjs-core 1.0.3 should work with face-api.js 0.20.0. This is the setup that the nodejs examples use. The error message you posted might indicate; that you are not using tfjs-core 1.0.3.====='; 'Hi @justadudewhohacks; I am also facing the same issue. I have run the face-api.js on browser for facial expression and landmark detection; which was comparatively very fast; but in nodejs; it was damn too slow; (i.e approx 1 frame in 5-6sec). Please help me out how I can do to fix this issue.====='; 'As I answered above; make sure the package versions line up correctly.====='; 'I also had the problem of agreeing and spent about 17 seconds in nodejs. Please see if my reference package is correct.![image](https://user-images.githubusercontent.com/24919652/59317744-dc74db00-8cf6-11e9-8787-d12070eab440.png)====='; 'Thank you for the help  @justadudewhohacks!More than 10 times faster when > @tensorflow/tfjs-node 1.0.2 + @tensorflow/tfjs-core 1.0.3====='; ""I'm getting following error; please help to resolvecpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 4module.js:549    throw err;    ^Error: Cannot find module 'D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v3\\tfjs_binding.node'    at Function.Module._resolveFilename (module.js:547:15)    at Function.Module._load (module.js:474:25)    at Module.require (module.js:596:17)    at require (internal/module.js:11:18)    at Object.<anonymous> (D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules\\@tensorflow\\tfjs-node\\dist\\index.js:44:16)    at Module._compile (module.js:652:30)    at Object.Module._extensions..js (module.js:663:10)    at Module.load (module.js:565:32)    at tryModuleLoad (module.js:505:12)    at Function.Module._load (module.js:497:3)=====""; ""> I'm getting following error; please help to resolve> > cpu backend was already registered. Reusing existing backend factory.> Platform node has already been set. Overwriting the platform with [object Object].> node-pre-gyp info This Node instance does not support builds for N-API version 4> node-pre-gyp info This Node instance does not support builds for N-API version 4> module.js:549> throw err;> ^> > Error: Cannot find module 'D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules@tensorflow\\tfjs-node\\lib\\napi-v3\\tfjs_binding.node'> at Function.Module._resolveFilename (module.js:547:15)> at Function.Module._load (module.js:474:25)> at Module.require (module.js:596:17)> at require (internal/module.js:11:18)> at Object. (D:\\bench_work_space\\GCH\\face-expressions\\face-expressions\\facetest\\node_modules@tensorflow\\tfjs-node\\dist\\index.js:44:16)> at Module._compile (module.js:652:30)> at Object.Module._extensions..js (module.js:663:10)> at Module.load (module.js:565:32)> at tryModuleLoad (module.js:505:12)> at Function.Module._load (module.js:497:3)Same issue here. Impossible to include @tensorflow. It installs without errors. Tried many versions. Still not able to include.=====""; '@0x6c23 Did you find any solution for this error? =====']",Regression,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",B.3.1,E
https://github.com/justadudewhohacks/face-api.js/issues/32,Improve speed,9,closed,2018-06-29T19:56:15Z,2021-11-16T13:46:59Z,"Hello;in the beginning; I would like to say that this library is beautiful; and I'm really impressed by how good it works. I mean how precise. I have an issue with the speed of face and landmark detection. I feel that with this quality of precision it's hard to make it faster; but I'm really interested in slightly better speed of detecting. I'm using this library with the stream from my camera; and I get very low results (sth around 1 FPS in face detection)So Is there any known factor or something that I could change which will increase the speed of face detecting? I'm aware that the precision probably will decrease but I don't care much about this.This is my code for detecting face landmarks using camera stream:That's my code for drawing landmarks on camera stream```<!DOCTYPE html><html><head>    <script src=""face-api.js""></script>    <script src=""commons.js""></script>    <link rel=""stylesheet"" href=""styles.css"">    <link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css"">    <script type=""text/javascript"" src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>    <script src=""https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js""></script></head><body><video id=""video"" autoplay width=""480"" height=""360""></video><canvas id=""overlay"" width=""480"" height=""360""></canvas><script>    const video = document.getElementById('video');    const canvas = document.getElementById('overlay')    canvas.width = 480;    canvas.height = 360;    navigator.mediaDevices.getUserMedia({audio: true; video: true}).then(        stream => {            video.srcObject = stream;            video.play();            video.muted = true;            run()        }    ).catch(e => console.warn(e));    const minConfidence = 0.6    const maxResults = 1    async function run() {        await faceapi.loadFaceLandmarkModel('/');        await faceapi.loadFaceDetectionModel('/');        requestAnimationFrame(processFrame)    }    async function processFrame() {                const detections = await faceapi.locateFaces(video; minConfidence; maxResults)        const faceTensors = await faceapi.extractFaceTensors(video; detections)        let landmarksByFace = await Promise.all(faceTensors.map(t => faceapi.detectLandmarks(t)))        faceTensors.forEach(t => t.dispose())        if(landmarksByFace.length > 0){            landmarksByFace = landmarksByFace.map((landmarks; i) => {                const box = detections[i].forSize(480; 360).getBox()                return landmarks.forSize(box.width; box.height).shift(box.x; box.y)            })            canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)            faceapi.drawLandmarks(canvas; landmarksByFace[0]; {drawLines: true})        }        requestAnimationFrame(processFrame)    }</script></body></html>```and this code is only for detecting face (1 FPS on chrome)```  navigator.mediaDevices.getUserMedia({audio: true; video: true}).then(        stream => {            video.srcObject = stream;            video.play();            video.muted = true;            run()        }    ).catch(e => console.warn(e));    const minConfidence = 0.6    const maxResults = 1    async function run() {        await faceapi.loadFaceLandmarkModel('/');        await faceapi.loadFaceDetectionModel('/');        requestAnimationFrame(processFrame)    }    async function processFrame() {        const detections = await faceapi.locateFaces(video; minConfidence; maxResults)        const detectionsForSize = detections.map(det => det.forSize(480; 360)        canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)        faceapi.drawDetection(canvas; detectionsForSize)        requestAnimationFrame(processFrame)    }```","[""Hi;I know; face detection is currently the bottleneck. Probably the main reason for that is that the net works on 512x512 sized images. I was planning on providing an option to make it alternatively run on 256x256 sized images; first measurements turned out to provide a speedup of 4x - 6x; which atleast on my machine was achieving realtime speed.However; this change requires to train an additional model on top of the current model. I am currently working on an api to make the nets conveniently trainable; such that I can hopefully refine some of the models.Apart from that; looking at your code I assume you are trying to get the face landmark positions of a users face from a webcam? Probably it is also possible to pass the frames directly into the landmark net if you only have a single face shown; but you would probably have to play some tricks to get a rough estimation of the bounding box; or atleast crop the frame to a square. Not sure if that's gonna work out sufficiently well though.Atleast the forwarding time of the face landmark net should be much faster. On my machine it's about 60 - 80 fps; even on my shitty laptop it runs at about 30fps.=====""; 'Super; so what should I do to pass frames directly into the landmark net? It will be very good if I could reach 30 FPS 👍 ====='; 'I just released v0.8.0; which implements MTCNN as an alternative face detector. I have played around with live webcam detection on my laptop a bit and I can achieve about realtime performance with MTCNN; in case you want to give it a try. You can also find a MTCNN webcam example [here](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/views/mtcnnFaceDetectionWebcam.html).====='; ""Wow; it's really better than my previous experiments 👍 It's a pity; that the precision isn't so good; but fps are great. Well; It's not ideal face tracking; but I see a big progress.Looking forward to hearing about another version with improvements. Kind regards =====""; 'If you only want to detect a single face (webcam); I suggest you try to create your own mobilenet that has as output:\xa0\xa0\xa0 - face: sigmoid [classificacion]\xa0\xa0\xa0 - bounding-box: 4values; linear [regression]You can use a loss combined with the detection of the face by using a binary_crossentropy and bounding-box using mean_square_error.If not; you can opt for already existing methods like: viola & jones or this (it is an algorithm similar to viola & jones but replaces the haar features with a comparison between two pixels): https://github.com/tehnokv/picojsKeep in mind that you will lose accuracy; but you will earn a lot of FPS.====='; ""The problem is; that mobilenet itself is already slower than mtcnn; atleast from what I measured from PC; laptop and mobile.But it's probably faster than SSD; if you only have to predict a single bounding box; sure.=====""; ""I will soon publish a new face detector (tiny yolo v2; using depthwise separable convolutions). This one is much faster than MTCNN on mobile (I can get processing times around ~100ms / 4 - 5 fps on mobile android) and seems to be about as fast as MTCNN on my desktop and laptop; but it's much more stable.You can already try it out under the link I posted here: #52. I will close this issue here.=====""; 'Is there any way to improve speed and stabilization ?Trying 0.15 version; webfacetracking example with tiny face detector; 128 input size.====='; ""I'm using other ML models with fcae.api; the video in my webCam is lagging a bit  when I add face.api in my project; any way to smoothen the video that the user see's in their laptop?Thanks!=====""]",Slow Execution,Poor Performance,Improper Model Attribute,,,retrain model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",B.1.1,C.2
https://github.com/justadudewhohacks/face-api.js/issues/354,angular; ng serve always get error。only happend when face-api.js>0.20.1; which with 'draw'.,3,closed,2019-07-18T03:47:15Z,2019-07-22T08:27:37Z,"This is the wrong message I got.  ```ERROR in node_modules/@tensorflow/tfjs-core/dist/io/browser_files.d.ts(24;34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/http.d.ts(23;31): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/indexed_db.d.ts(31;34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@tensorflow/tfjs-core/dist/io/local_storage.d.ts(41;34): error TS1039: Initializers are not allowed in ambient contexts.node_modules/@types/offscreencanvas/index.d.ts(10;22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(11;22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(12;22): error TS2304: Cannot find name 'CanvasImageSource'.node_modules/@types/offscreencanvas/index.d.ts(17;43): error TS2304: Cannot find name 'ImageBitmapSource'.node_modules/@types/offscreencanvas/index.d.ts(18;43): error TS2304: Cannot find name 'ImageBitmapSource'.node_modules/@types/offscreencanvas/index.d.ts(27;53): error TS2304: Cannot find name 'CanvasState'.node_modules/@types/offscreencanvas/index.d.ts(27;66): error TS2304: Cannot find name 'CanvasTransform'.node_modules/@types/offscreencanvas/index.d.ts(27;83): error TS2304: Cannot find name 'CanvasCompositing'.node_modules/@types/offscreencanvas/index.d.ts(28;9): error TS2304: Cannot find name 'CanvasImageSmoothing'.node_modules/@types/offscreencanvas/index.d.ts(28;31): error TS2304: Cannot find name 'CanvasFillStrokeStyles'.node_modules/@types/offscreencanvas/index.d.ts(28;55): error TS2304: Cannot find name 'CanvasShadowStyles'.node_modules/@types/offscreencanvas/index.d.ts(28;75): error TS2304: Cannot find name 'CanvasFilters'.node_modules/@types/offscreencanvas/index.d.ts(28;90): error TS2304: Cannot find name 'CanvasRect'.node_modules/@types/offscreencanvas/index.d.ts(29;9): error TS2304: Cannot find name 'CanvasDrawPath'.node_modules/@types/offscreencanvas/index.d.ts(29;25): error TS2304: Cannot find name 'CanvasText'.node_modules/@types/offscreencanvas/index.d.ts(29;54): error TS2304: Cannot find name 'CanvasImageData'.node_modules/@types/offscreencanvas/index.d.ts(29;71): error TS2304: Cannot find name 'CanvasPathDrawingStyles'.node_modules/@types/offscreencanvas/index.d.ts(30;9): error TS2304: Cannot find name 'CanvasTextDrawingStyles'.node_modules/@types/offscreencanvas/index.d.ts(30;34): error TS2304: Cannot find name 'CanvasPath'.node_modules/@types/offscreencanvas/index.d.ts(42;53): error TS2304: Cannot find name 'CanvasRenderingContext2DSettings'.node_modules/tfjs-image-recognition-base/build/commonjs/dom/awaitMediaLoaded.d.ts(1;115): error TS2304: Cannot find name 'unknown'.```  this is my package.json . ``` {  ""name"": ""face-api"";  ""version"": ""0.0.0"";  ""scripts"": {    ""ng"": ""ng"";    ""start"": ""ng serve"";    ""build"": ""ng build"";    ""test"": ""ng test"";    ""lint"": ""ng lint"";    ""e2e"": ""ng e2e""  };  ""private"": true;  ""dependencies"": {    ""@angular/animations"": ""^6.1.0"";    ""@angular/common"": ""^6.1.0"";    ""@angular/compiler"": ""^6.1.0"";    ""@angular/core"": ""^6.1.0"";    ""@angular/forms"": ""^6.1.0"";    ""@angular/http"": ""^6.1.0"";    ""@angular/platform-browser"": ""^6.1.0"";    ""@angular/platform-browser-dynamic"": ""^6.1.0"";    ""@angular/router"": ""^6.1.0"";    ""@tensorflow/tfjs-node"": ""^1.2.3"";    ""canvas"": ""^2.5.0"";    ""core-js"": ""^2.5.4"";    ""draw"": ""0.0.0"";    ""face-api.js"": ""^0.20.1"";    ""rxjs"": ""~6.2.0"";    ""zone.js"": ""~0.8.26""  };  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.8.0"";    ""@angular/cli"": ""~6.2.3"";    ""@angular/compiler-cli"": ""^6.1.0"";    ""@angular/language-service"": ""^6.1.0"";    ""@types/jasmine"": ""~2.8.8"";    ""@types/jasminewd2"": ""~2.0.3"";    ""@types/node"": ""~8.9.4"";    ""codelyzer"": ""~4.3.0"";    ""jasmine-core"": ""~2.99.1"";    ""jasmine-spec-reporter"": ""~4.2.1"";    ""karma"": ""~3.0.0"";    ""karma-chrome-launcher"": ""~2.2.0"";    ""karma-coverage-istanbul-reporter"": ""~2.0.1"";    ""karma-jasmine"": ""~1.1.2"";    ""karma-jasmine-html-reporter"": ""^0.2.2"";    ""protractor"": ""~5.4.0"";    ""ts-node"": ""~7.0.0"";    ""tslint"": ""~5.11.0"";    ""typescript"": ""^2.9.2""  }}```   Is it because my typescript version can't match?  Can someone help me see what's wrong?","['the edition of typescript is not matched.   should us v-3.xxx with face-api.js-0.20.1====='; ""@929853209 ; Is your issue got resolved ?I'm also facing exactly same issue.=====""; '> @929853209 ; Is your issue got resolved ?> I\'m also facing exactly same issue.""face-api.js"": ""^0.20.1"";""typescript"": ""~3.4.3""this is my version in package.json ;there are some new typescript api in face-api.js-0.20.1;i have to update my angular version which can use typescript-3.4.3   if you want to use typescript-2.xxx;  must with face-api.js-0.19xxx; which without \'faceapi.draw\'=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change ui framework,change ui framework,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/355,Tiny Face Detector inputSize has strange effect on detect result?,1,closed,2019-07-18T08:31:24Z,2019-10-12T11:23:34Z,"I run some test on demo site: https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detectionAnd found that when I choose different inputSize; some detected a face; some(224) didn't.Is it a bug?<img width=""472"" alt=""faceapi-160"" src=""https://user-images.githubusercontent.com/388222/61441841-0619c580-a979-11e9-8634-5eaf3f49109a.png""><img width=""462"" alt=""faceapi-224"" src=""https://user-images.githubusercontent.com/388222/61441912-29dd0b80-a979-11e9-8e46-141242fcba56.png""><img width=""451"" alt=""faceapi-320"" src=""https://user-images.githubusercontent.com/388222/61441942-33667380-a979-11e9-80bd-fef6a256568d.png""><img width=""462"" alt=""faceapi-416"" src=""https://user-images.githubusercontent.com/388222/61441971-3eb99f00-a979-11e9-8eb4-3e0645e76d14.png"">Tested on MacBook Pro 2017 13'. Macos 10.14.5. Chrome 75.0.3770.142(64bit). ",['Not really a bug; it just happens for some scales with the current model. If in doubt and you can afford it; run the prediction on different input sizes for better scalability.====='],Unstable,Poor Performance,Improper Model Attribute,,,change input size,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Inconsistent results with different input sizes""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",B.3.2,C.2
https://github.com/justadudewhohacks/face-api.js/issues/36,Error: Tensor is disposed,6,closed,2018-07-02T19:38:13Z,2018-07-22T16:38:20Z,Hi @justadudewhohacks ;I'm getting the following after performing the below steps:1. load models using `faceapi.loadModels('assets/models/') `(this is successful)2. invoke `faceapi.allFaces(input; this.minConfidence).then(..` (where `input` is the id to a image element.  This fails with below error).> core.js:1449 ERROR Error: Uncaught (in promise): Error: Tensor is disposed.Error: Tensor is disposed.    at t.throwIfDisposed (face-api.min.js:1)    at t.asType (face-api.min.js:1)    at t.toFloat (face-api.min.js:1)    at face-api.min.js:1    at Array.map (<anonymous>)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at rc (face-api.min.js:1)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at t.throwIfDisposed (face-api.min.js:1)    at t.asType (face-api.min.js:1)    at t.toFloat (face-api.min.js:1)    at face-api.min.js:1    at Array.map (<anonymous>)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at rc (face-api.min.js:1)    at face-api.min.js:1    at t.tidy (face-api.min.js:1)    at c (polyfills.js:3)    at c (polyfills.js:3)    at polyfills.js:3    at t.invokeTask (polyfills.js:3)    at Object.onInvokeTask (core.js:4751)    at t.invokeTask (polyfills.js:3)    at r.runTask (polyfills.js:3)    at o (polyfills.js:3)    at e.invokeTask (polyfills.js:3)    at i.isUsingGlobalCallback.invoke (polyfills.js:3)Did I forget to do sth? Sorry; not familiar with how tensors work so maybe its a stupid question.Thanks,"['Just realized this.I was using `tfjs` for another library in my app. After removing it; `faceapi.allFaces` works!What could be wrong?> <script src=""https://unpkg.com/@tensorflow/tfjs"" defer></script>====='; 'So the error occurs when you are additionally including the tfjs script to face-api.js? Does it work with tfjs-core?Not sure if there are issues; when you are using tfjs additionally; because someone seemed to have issues with this in #9. In case tfjs-core is enough for your application; you can also access the bundled version via `faceapi.tf.`====='; '> So the error occurs when you are additionally including the tfjs script to face-api.js? Does it work with tfjs-core?yes and yes> you can also access the bundled version via faceapi.tfCan you please point me to how to use it? Sorry tf noob speaking :)====='; '> yes and yesSo it works with tfjs-core but not with tfjs?> Can you please point me to how to use it? Sorry tf noob speaking :)Well not quite sure what you mean. You can simply use tf via `faceapi.tf`. You could simply bind tf to the window object for convenience: `window.tf = faceapi.tf` and then use tf as you would normally.====='; ""> So it works with tfjs-core but not with tfjs?Correct> You could simply bind tf to the window object for convenience: window.tf = faceapi.tf and then use tf as you would normally.I see. I'll try that. Closing. Thanks for the prompt responses.=====""; '> So it works with tfjs-core but not with tfjs?@justadudewhohacks  No. It doesnot work with both (tfjs & tfjs-core). It throws ""Tensor disposed Error"" for tfjs-core.![image](https://user-images.githubusercontent.com/16360498/43047855-82badb32-8dfb-11e8-811a-58da23b956ff.png)=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/376,tsc compilation error,5,closed,2019-08-08T03:29:56Z,2019-08-21T01:50:09Z,"Hi All;Just got an error on running node program:```yarn run v1.16.0$ D:\Github\Minazuki\node\node_modules\.bin\ts-node detection.ts ../images/pos1.jpgcpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 42019-08-08 11:12:39.517916: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:245    return new TSError(diagnosticText; diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:utils/faceDetection.ts:19:10 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'SsdMobilenetv1' have no overlap.19   return net === faceapi.nets.ssdMobilenetv1            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~utils/faceDetection.ts:21:7 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'TinyFaceDetector' have no overlap.21     : net === faceapi.nets.tinyFaceDetector         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~utils/faceDetection.ts:26:60 - error TS2345: Argument of type 'TinyFaceDetector' is not assignable to parameter of type 'NeuralNetwork<any>'.  Types of property 'extractParamsFromWeigthMap' are incompatible.    Type '(weightMap: NamedTensorMap) => { params: TinyYolov2NetParams; paramMappings: ParamMapping[]; }' is not assignable to type '(weightMap: NamedTensorMap) => { params: any; paramMappings: ParamMapping[]; }'.      Types of parameters 'weightMap' and 'weightMap' are incompatible.        Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.          Index signatures are incompatible.            Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.              Types of property 'flatten' are incompatible.                Type '() => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>' is not assignable to type '() => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                    Types of property 'asScalar' are incompatible.                      Type '() => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>' is not assignable to type '() => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                          Types of property 'as2D' are incompatible.                            Type '(rows: number; columns: number) => import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' is not assignable to type '(rows: number; columns: number) => import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' is not assignable to type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                                Types of property 'asType' are incompatible.                                  Type '<T extends import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; dtype: ""string"" | ... 3 more ... | ""complex64"") => T' is not assignable to type '<T extends import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; dtype: ""string"" | ... 3 more ... | ""complex64"") => T'.                                    The 'this' types of each signature are incompatible.                                      Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                        Property 'bytes' is missing in type 'import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' but required in type 'import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/Github/Minazuki/node/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.26 export const faceDetectionOptions = getFaceDetectorOptions(faceDetectionNet)                                                              ~~~~~~~~~~~~~~~~  node_modules/@tensorflow/tfjs-core/dist/tensor.d.ts:374:5    374     bytes(): Promise<Uint8Array[] | Uint8Array>;            ~~~~~    'bytes' is declared here.    at createTSError (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:245:12)    at reportTSError (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:249:19)    at getOutput (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:362:34)    at Object.compile (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:395:32)    at Module.m._compile (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:473:43)    at Module._extensions..js (module.js:663:10)    at Object.require.extensions.(anonymous function) [as .ts] (D:\Github\Minazuki\node\node_modules\ts-node\src\index.ts:476:12)    at Module.load (module.js:565:32)    at tryModuleLoad (module.js:505:12)    at Function.Module._load (module.js:497:3)error Command failed with exit code 1.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```The root cause of this error maybe because I just upgrade typescript from 3.5.2 to 3.5.3 and @tensorflow/tfjs-node from 1.2.3 to 1.2.7Anyone here have this error and solutions?Thanks","['Same error====='; 'I had the same error.It is mainly the @tensorflow/tfjs-node version 1.2.7 issues.I think face-api.js dependent on the @tensorflow/tfjs-node  version 1.2.2 which you can read the package.json in root of this repos.You can uninstall @tensorflow/tfjs-node and reinstall face-api.js then it will be fine.====='; 'I already fix the error:* Clear npm modules directory and cache* I user typescript and I add this line to tsconfig.json -> ""skipLibCheck"": true;* npm iAnd all is ok====='; 'Hi @xperiafan13-rom Can you share your package.json and tsconfig.json ? ThanksI just try and get the same error again.My those two files are:```  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.2.7"";    ""canvas"": ""^2.6.0"";    ""face-api.js"": ""^0.20.1""  };  ""devDependencies"": {    ""@types/node"": ""^12.7.2"";    ""prettier"": ""^1.18.2"";    ""ts-node"": ""^8.3.0"";    ""typescript"": ""^3.5.3""  }``````{  ""compilerOptions"": {    ""skipLibCheck"": true  };  ""exclude"": [    ""node_modules"";    ""build"";    ""doc"";    ""scripts"";    ""acceptance-tests"";    ""webpack"";    ""jest"";    ""src/setupTests.ts"";    ""coverage""  ]}```====='; '`{  ""name"": ""rem_services"";  ""version"": ""0.0.1"";  ""description"": ""REM REST Services"";  ""main"": ""dist/server.js"";  ""scripts"": {    ""build"": ""tsc"";    ""dev"": ""ts-node lib/server.ts"";    ""start"": ""nodemon ./dist/server.js"";    ""prod"": ""npm run build && npm run start""  };  ""author"": ""xperiafan13"";  ""license"": ""ISC"";  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.0.2"";    ""@types/express"": ""^4.17.0"";    ""@types/node"": ""^12.7.1"";    ""awesome-qr"": ""^1.2.0"";    ""axios"": ""^0.19.0"";    ""ba64"": ""^3.0.9"";    ""body-parser"": ""^1.19.0"";    ""canvas"": ""^2.5.0"";    ""cloudinary"": ""^1.14.0"";    ""dateformat"": ""latest"";    ""ethers"": ""^4.0.20"";    ""express"": ""^4.17.1"";    ""face-api.js"": ""^0.19.0"";    ""file-encryptor"": ""^0.1.1"";    ""firebase-admin"": ""latest"";    ""http"": ""0.0.0"";    ""image-hash"": ""^3.3.9"";    ""int-encoder"": ""^1.1.1"";    ""inversify"": ""^5.0.1"";    ""inversify-express-utils"": ""^6.3.2"";    ""jsonpack"": ""^1.1.5"";    ""jsonwebtoken"": ""^8.5.1"";    ""lodash"": ""latest"";    ""mongodb"": ""latest"";    ""mongoose"": ""^5.6.9"";    ""mustache-express"": ""^1.2.8"";    ""node-gd"": ""^1.6.0"";    ""node-schedule"": ""^1.3.2"";    ""nodemailer"": ""latest"";    ""oidc-provider"": ""^5.5.6"";    ""p-iteration"": ""^1.1.8"";    ""rand-token"": ""latest"";    ""reflect-metadata"": ""^0.1.13"";    ""socket.io"": ""^2.2.0"";    ""speakeasy"": ""^2.0.0"";    ""ts-node"": ""^8.3.0"";    ""typescript"": ""^3.5.3"";    ""web3"": ""^1.2.1"";    ""web3-eth-accounts"": ""^1.2.1""  }}``{  ""compilerOptions"": {    ""module"": ""commonjs"";    ""moduleResolution"": ""node"";    ""experimentalDecorators"": true;    ""skipLibCheck"": true;    ""pretty"": true;    ""sourceMap"": true;    ""resolveJsonModule"": true;    ""noImplicitAny"": false;    ""allowJs"": true;    ""declaration"": false;    ""target"": ""es6"";    ""outDir"": ""./dist"";    ""baseUrl"": ""./lib""  };  ""include"": [    ""lib/**/*.ts"";    ""lib/**/*.js""  ];  ""exclude"": [    ""node_modules""  ]}`=====']",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,build/install configuration,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/381,Ml5.js and face-api.js are not available at the same time.,2,closed,2019-08-10T17:12:59Z,2019-10-12T11:20:41Z,"I wanted to try face-api.js and installed it. But when I import both ml5.js and face-api.js at the same time; I have a problem. So I cloned https://github.com/ml5js/ml5-library/ And I tried to build; install and use it via yarn build myself but I get the same problem.<img width=""800"" alt=""problem"" src=""https://user-images.githubusercontent.com/31428960/62824778-87155700-bbdd-11e9-99e7-69f0084d67b6.PNG"">",['This error comes from ml5.js so I am not sure why this error occurs. My guess is that the tfjs-core versions of your ml5.min.js script and the version used by face-api.js are conflicting?====='; 'Closing because of inactivity.====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/409,withFaceLandmark slow in Slackware,4,open,2019-09-12T04:48:22Z,2019-09-15T09:14:17Z,Hi @justadudewhohacks! I have a problem with this API. At first; I tried this API in intel nuc system unit with ubuntu os and it works perfectly. It can detect faces and create face landmarks less than a second. After; I change the os to Slackware. I transfer all the codes to Slackware with the same system unit and the code works but the thing here is that it is slower than the usual. It can detect the face fast but the withFaceLandMark was not. It can create face landmark about 10s. I don't know what is the cause of this problem.  Maybe with the os?,"['Nodejs or browser? Which backend are you using on both machines (cpu / webgl / node gpu)?====='; 'Thank you for the response @justadudewhohacks. I\'m using a browser. I\'m sorry I don\'t know what backend I\'m using. Here is my code.<!DOCTYPE html><html>\t<head>                <title>Biometric Facial Recognition</title>                <link rel=""SHORTCUT ICON"" href=""/images/gecko.ico"">\t\t<meta name=""viewport"" content=""width=device-width; initial-scale=1"">  \t\t<link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css"">  \t\t<link rel=""stylesheet"" type=""text/css"" href=""/scripts/face-rec-models/style.css"">  \t\t<script src=""https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js""></script>  \t\t<script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js""></script>  \t\t<script src=""/scripts/face-api2.js""></script>        </head>               <div id=""body"" >\t\t\t<div id=""parent1"">      \t\t\t\t<div class=""margin"" style=""position: relative; float:left; margin-left: 10px; margin-top: 50px; border: 3px solid black;"">      \t\t \t \t\t<video id=""vidDisplay"" style=""height: 568px; width: 640px; display: inline-block; vertical-align: baseline;"" onloadedmetadata=""onPlay(this)"" autoplay=""true""></video>        \t\t\t\t<canvas id=""overlay"" style=""position: absolute; top: 0; left: 0;"" width = ""640px"" height = ""568px""/>      \t\t\t\t</div>    \t\t\t</div>\t        </body><script>  \t$(""#parent1"").hide();  \tPromise.all([    \t\tfaceapi.nets.faceRecognitionNet.loadFromUri(\'/scripts/face-rec-models/models\');    \t\tfaceapi.nets.faceLandmark68Net.loadFromUri(\'/scripts/face-rec-models/models\');    \t\tfaceapi.nets.ssdMobilenetv1.loadFromUri(\'/scripts/face-rec-models/models\');    \t\tfaceapi.nets.tinyFaceDetector.loadFromUri(\'/scripts/face-rec-models/models\')  \t]).then(start);  \tasync function start() {    \t\t$(\'#parent1\').show();        \t\trun();  \t}  \tasync function onPlay() {      \t\tconst videoEl = $(\'#vidDisplay\').get(0)      \t\tif(videoEl.paused || videoEl.ended ){        \t\treturn setTimeout(() => onPlay());\t\t}        \tconst canvas = $(\'#overlay\').get(0);         \tconst options = getFaceDetectorOptions();        \tconst result = await faceapi.detectSingleFace(videoEl; options).withFaceLandmarks();\t\tif (result) {            \t\t$(""#overlay"").show();            \t\tconst dims = faceapi.matchDimensions(canvas; videoEl; true);            \t\tdims.height = 568;            \t\tdims.width = 640;            \t\tcanvas.height = 568;            \t\tcanvas.width = 640;            \t\tconst resizedResult = faceapi.resizeResults(result; dims);            \t\tfaceapi.draw.drawDetections(canvas; resizedResult);        \t}             \telse{            \t\t$(""#overlay"").hide();       \t\t}      \t\tsetTimeout(() => onPlay());    \t} \t async function run() {      \t\tconst stream = await navigator.mediaDevices.getUserMedia({ video: {} });      \t\tconst videoEl = $(\'#vidDisplay\').get(0);      \t\tvideoEl.srcObject = stream;  \t}    \t// tiny_face_detector options  \tlet inputSize = 160;  \tlet scoreThreshold = 0.4;\tfunction getFaceDetectorOptions() {    \t\treturn  new faceapi.TinyFaceDetectorOptions({ inputSize; scoreThreshold });  \t}\t</script></html>====='; '[code.txt](https://github.com/justadudewhohacks/face-api.js/files/3609496/code.txt)I tried the demo of face-api.js webcam face tracking with face landmark in slackware and unfortunately; it will create the box after 10 seconds. I tried also the demo to ubuntu on the same system unit and the box was created less than 1 second====='; ""You can run `console.log(tf.getBackend())` or `console.log(faceapi.tf.getBackend())` on both machines to verify whether the webgl backend is utilized. If it outputs 'cpu' then that's the reason why it's running so slow.=====""]",Slow Execution,Poor Performance,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.1.1,E
https://github.com/justadudewhohacks/face-api.js/issues/415,Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor',8,open,2019-09-16T20:40:25Z,2021-03-07T18:11:13Z,"HI; I use this library in a project Angular; I'm create a library that there is problem Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' and no build project; I'm try use 0.20.1 version I am trying to use the 0.20.1 version that was the one with which my project was built but currently it does not workthis is the problem shown by consoleBUILD ERRORnode_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.        Index signatures are incompatible.          Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.            Types of property 'flatten' are incompatible.              Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Types of property 'asScalar' are incompatible.                    Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Types of property 'as2D' are incompatible.                          Type '(rows: number; columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/nod...' is not assignable to type '(rows: number; columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/type...'.                            Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Types of property 'asType' are incompatible.                                Type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-rec...' is not assignable to type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; d...'.                                  The 'this' types of each signature are incompatible.                                    Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                      Property 'bytes' is missing in type 'Tensor<Rank.R2>' but required in type 'Tensor<Rank.R2>'.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceProcessor<TExtractorParams>' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyXception' is not assignable to the same property in base type 'NeuralNetwork<TinyXceptionParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'AgeGenderNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceRecognitionNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'Mtcnn' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'SsdMobilenetv1' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceDetector' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyYolov2' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.Error: node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<FaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.        Index signatures are incompatible.          Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.            Types of property 'flatten' are incompatible.              Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Types of property 'asScalar' are incompatible.                    Type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognit...' is not assignable to type '() => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Types of property 'as2D' are incompatible.                          Type '(rows: number; columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/nod...' is not assignable to type '(rows: number; columns: number) => import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/type...'.                            Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-ba...' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                              Types of property 'asType' are incompatible.                                Type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-rec...' is not assignable to type '<T extends import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; d...'.                                  The 'this' types of each signature are incompatible.                                    Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                      Property 'bytes' is missing in type 'Tensor<Rank.R2>' but required in type 'Tensor<Rank.R2>'.node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceProcessor<TExtractorParams>' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyXception' is not assignable to the same property in base type 'NeuralNetwork<TinyXceptionParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'AgeGenderNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceFeatureExtractor' is not assignable to the same property in base type 'NeuralNetwork<TinyFaceFeatureExtractorParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'FaceRecognitionNet' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'Mtcnn' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'SsdMobilenetv1' is not assignable to the same property in base type 'NeuralNetwork<NetParams>'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyFaceDetector' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10;15): error TS2416: Property 'extractParamsFromWeigthMap' in type 'TinyYolov2' is not assignable to the same property in base type 'TinyYolov2'.  Type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }' is not assignable to type '(weightMap: import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }'.    Types of parameters 'weightMap' and 'weightMap' are incompatible.      Type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.    at Object.<anonymous> (/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/ng-packagr/lib/ngc/compile-source-files.js:65:19)    at Generator.next (<anonymous>)    at fulfilled (/home/edison/Documents/repositorio zinobe/IATeam/zinobe-front-libraries-biometrics/node_modules/ng-packagr/lib/ngc/compile-source-files.js:4:58)","['Looks like an issue with your typescript setup. Try use the latest typescript version. face-api.js 0.21.0 is compiled with typescript 3.6.3; so that should work.====='; ""Same problem here. I am using angular compiler v8.2.8 (which is the latest at the moment). This compiler requires typescript between 3.4.0 and 3.6.0; so version 3.6.3 can not work just 3.5.3.I've tried to decrease the version numbers for ngx-face-api; and face api as well to test which one is compiled with 3.5.3; but I'm getting always the same errors. So any other ideas?=====""; ""Isn't angular supposed to support latest typescript versions? I would upgrade your compiler.=====""; 'I get the same error on 20.1 now; it seems that base module uses 1.2.9 tfjs and the api uses 1.2.2For some reason npm installs tfjs-image-recognition-base 0.6.2 instead of 0.6.1====='; '> ¿No se supone que angular admite las últimas versiones de mecanografiado? Actualizaría tu compilador.The latest version of the angular compiler uses 3.6 at most; is there any definitive solution to use this library? ERROR in The Angular Compiler requires TypeScript >=3.4.0 and <3.6.0 but 3.6.4 was found instead.====='; 'Got the following error while updating angular v8 from v7 and modules are at following versions -  ""face-api.js"": ""^0.21.0"" -  ""typescript"": ""3.5.3""-  ""@tensorflow/tfjs-core"": ""^1.6.0""-  ""@angular/cli"": ""~8.3.25""-  ""@angular/compiler-cli"": ""^8.2.14"" -  ""@angular/core"": ""^8.2.14"";```ERROR in node_modules/face-api.js/build/commonjs/ageGenderNet/AgeGenderNet.d.ts(20;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'AgeGenderNet\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/ageGenderNet/types"").NetParams; paramMappings: import(""...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceFeatureExtractor\' is not assignable to the same property in base type \'IFaceFeatureExtractor<FaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").FaceFeatureExtractorParams...\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/FaceFeatureExtractor.d.ts(9;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceFeatureExtractor\' is not assignable to the same property in base type \'NeuralNetwork<FaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").FaceFeatureExtractorParams...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.            Index signatures are incompatible.              Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>\'.                Types of property \'flatten\' are incompatible.                  Type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\' is not assignable to type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\'.                    Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>\'.                      Types of property \'asScalar\' are incompatible.                        Type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\' is not assignable to type \'() => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\'.                               Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>\'.                            Types of property \'as2D\' are incompatible.                              Type \'(rows: number; columns: number) => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\' is not assignable to type \'(rows: number; columns: number) => import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Ra...\'.                                Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>\'.                                  Types of property \'asType\' are incompatible.                                    Type \'<T extends import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; dtype: ""string"" | ... 3 more ... | ""complex64"") => T\' is not assignable to type \'<T extends import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(this: T; dtype: ...\'.                                      The \'this\' types of each signature are incompatible.                                        Type \'T\' is not assignable to type \'Tensor<Rank.R2>\'.                                          Type \'Tensor<Rank.R2>\' is missing the following properties from type \'Tensor<Rank.R2>\': divNoNan; relu6          node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceFeatureExtractor\' is not assignable to the same property in base type \'IFaceFeatureExtractor<TinyFaceFeatureExtractorParams>\'.           Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").TinyFaceFeatureExtractorPa...\'.    node_modules/face-api.js/build/commonjs/faceFeatureExtractor/TinyFaceFeatureExtractor.d.ts(9;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceFeatureExtractor\' is not assignable to the same property in base type \'NeuralNetwork<TinyFaceFeatureExtractorParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceFeatureExtractor/types"").TinyFaceFeatureExtractorPa...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceProcessor/FaceProcessor.d.ts(19;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceProcessor<TExtractorParams>\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceProcessor/types"").NetParams; paramMappings: import(...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/faceRecognitionNet/FaceRecognitionNet.d.ts(10;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'FaceRecognitionNet\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/faceRecognitionNet/types"").NetParams; paramMappings: im...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/mtcnn/Mtcnn.d.ts(17;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'Mtcnn\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/mtcnn/types"").NetParams; paramMappings: import(""D:/am w...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.d.ts(18;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'SsdMobilenetv1\' is not assignable to the same property in base type \'NeuralNetwork<NetParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/types"").NetParams; paramMappings: import...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/tinyFaceDetector/TinyFaceDetector.d.ts(9;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyFaceDetector\' is not assignable to the same property in base type \'TinyYolov2\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/types"").TinyYolov2NetParams;...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/tinyYolov2/TinyYolov2.d.ts(10;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyYolov2\' is not assignable to the same property in base type \'TinyYolov2\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/tfjs-image-recognition-base/build/commonjs/tinyYolov2/types"").TinyYolov2NetParams;...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.    node_modules/face-api.js/build/commonjs/xception/TinyXception.d.ts(10;15): error TS2416: Property \'extractParamsFromWeigthMap\' in type \'TinyXception\' is not assignable to the same property in base type \'NeuralNetwork<TinyXceptionParams>\'.      Type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { ...; }\' is not assignable to type \'(weightMap: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap) => { params: import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/build/commonjs/xception/types"").TinyXceptionParams; paramMappings: imp...\'.        Types of parameters \'weightMap\' and \'weightMap\' are incompatible.          Type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\' is not assignable to type \'import(""D:/am workspace/codemeet-frontend-web - Angular Update/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap\'.```====='; 'Same problem in Angular 10 (TypeScript 3.9.7). Did anyone find a solution or workaround for this issue?====='; '😩Facing the same issue in Angular 11 and TypeScript 4.1.2Any workaround for this issue?=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change typescript version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/428,face not being detected on latest mater build,2,open,2019-09-30T14:11:38Z,2019-11-16T13:47:18Z,Hi!I'm having a problem having the examples work in my Windows dev machine. I was able to make the sample work in Mac; and was able to updated code with my own code. When I've moved it into my windows machine my code wasn't able to detect faces anymore.The sample code in https://justadudewhohacks.github.io/face-api.js/webcam_face_expression_recognition works; but when running it via node it doesn't seem to work.,"['Can you please give me the exact hardware information of the computer it is not working for (CPU; GPU; OS).There still seems to be some issues with Intel Graphics Cards / (CPUs?).====='; '**Fixed with setting tf-core version to 1.2.9**Same here. Works perfectly in vue-cli/dev; but doesn\'t work in production (nginx). If i\'m using tensors as input; gives this response: `Negative size values should be exactly -1 but got -112 for the slice() size at index 0.` somewhere at faceapi.detectAllFaces.If i use images from canvas; it gives me `Uncaught (in promise) DOMException: Failed to execute \'getImageData\' on \'CanvasRenderingContext2D\': The source height is 0.` though the image is captured correctly (the height and width are in console just before the faceapi call and are non-zero.It has been working in summer; then i\'ve added a minor tweak (to the GUI; not the face recognition process); and it all went south. Maybe some tf update or smth. I\'m using: tfjs 1.2.9faceapi 0.21.Tried to use older version which worked; and it used the following dependencies:""core-js"": ""2.6.5"";""face-api.js"": ""0.21.0""The output of `faceapi.detectAllFaces` on vue dev is:<img width=""291"" alt=""Screenshot 2019-11-09 at 10 06 30"" src=""https://user-images.githubusercontent.com/11751592/68524454-2d095800-02d8-11ea-99c3-8ce221beed7d.png"">the very same code/image on vue/prod: <img width=""560"" alt=""Screenshot 2019-11-09 at 10 07 55"" src=""https://user-images.githubusercontent.com/11751592/68524474-617d1400-02d8-11ea-8453-e388034da45f.png""><img width=""286"" alt=""Screenshot 2019-11-09 at 10 08 14"" src=""https://user-images.githubusercontent.com/11751592/68524475-617d1400-02d8-11ea-9943-db91e08e85d8.png"">it detects 419 faces (sic!) and their height/width is really <0=====']",Incorrect Functionality,Incorrect Functionality,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",D,E
https://github.com/justadudewhohacks/face-api.js/issues/436,Android app face-api,8,open,2019-10-12T01:41:27Z,2020-02-06T12:10:28Z,Hi everyone. Do anyone had tried using this API in android? If yes; how did you include face-api.js in java? Thank you for the response.,"['Are you talking about react native? Tfjs recently launched react native support; but I am not sure how to integrate tfjs / libraries built on top of tfjs with react native.====='; ""Have you figured anything out RE: integrating tfjs / libraries built on top of tfjs within React Native? Currently trying to figure this out now so I can implement face-api in a RN app – would be great if you've discovered anything / have any pointers!=====""; ""Hi I really need help too on this issue. I hope there's an implementation to it. Thank you very much=====""; ""I've already implemented this api in android=====""; '@Mactacs Can you please share the details/steps regarding the implementation for android. I am a newbie in android kindly share the implementation details.====='; 'Hi;In react-native i recommend use [expo-face-detector](https://docs.expo.io/versions/latest/sdk/facedetector/) it use tf-lite and it is very smooth and no require load models :)====='; 'Hi;     We are going with native android build. We have a node server which does the recognition using face api but would like to reduce the latency time by doing all the process in the app itself. Kindly let us know how the same can be achieved.====='; 'Hi;can anyone share the details/steps; how to implement face-api in android.Thanks in advance.=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,change device,Changing device/browser,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error"",
    ""specific_type"": ""E.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""Cross-platform App Framework Incompatibility""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/438,Unable to integrate it with react native,26,open,2019-10-12T07:30:37Z,2021-04-12T14:45:38Z,tried to integrate the face api in react native but tfjs-image-recognition is in fs format. is there any way to resolve issue. Really need help as this is a school project . Thanks,"['What is a fs format? Can describe the issue more precisely?====='; ""when i did an npm i face-api.js and import * as faceapi from 'face-api.js' into my page ; it threw me an error fs module does not exist in module haste mapDid a check online and found the tjfs image recognition which is in fs format can only be work in web browser.So im wondering is that anyway I can solve this error and make it run on mobile? I really need help on these issues . thanks.=====""; '> Did a check online and found the tjfs image recognition which is in fs format can only be work in web browser.I am maintaing tfjs-image-recognition base; but I still don\'t know what ""fs format"" should be. A more detailed error message would be helpful.====='; 'Just a quick ask is it possible for faceapi to be used on react native currently?====='; '<img width=""839"" alt=""Screenshot 2019-11-07 at 9 19 10 AM"" src=""https://user-images.githubusercontent.com/52208748/68352141-7aeb5800-0140-11ea-97f3-c5e2c791810d.png"">I suspect this maybe where the error come from since react native cannot read \'fs\' file format====='; ""I too am facing the same problem in React-NativeI have installed face-api.js and then imported it as `import * as faceapi from 'face-api.js';`Then I get the following errors``` bundling failed: Error: Unable to resolve module `fs` from `node_modules/tfjs-image-recognition-base/build/commonjs/env/createFileSystem.js`: fs could not be found within the project.If you are sure the module exists; try these steps: 1. Clear watchman watches: watchman watch-del-all 2. Delete node_modules: rm -rf node_modules and run yarn install 3. Reset Metro's cache: yarn start --reset-cache 4. Remove the cache: rm -rf /tmp/metro-*    at ModuleResolver.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph/ModuleResolution.js:186:15)    at ResolutionRequest.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph/ResolutionRequest.js:52:18)    at DependencyGraph.resolveDependency (/root/React-native/faceapi/node_modules/metro/src/node-haste/DependencyGraph.js:282:16)    at Object.resolve (/root/React-native/faceapi/node_modules/metro/src/lib/transformHelpers.js:267:42)    at /root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:426:31    at Array.map (<anonymous>)    at resolveDependencies (/root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:423:18)    at /root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:275:33    at Generator.next (<anonymous>)    at asyncGeneratorStep (/root/React-native/faceapi/node_modules/metro/src/DeltaBundler/traverseDependencies.js:87:24)```=====""; 'Hi @sedaplaksa @wulforr;It could very much be possible that `tfjs-image-recognition-base` is currently incompatible with React Native...Node is only used by the packager to serve/compile your app bundle; in other words; React Native apps don\'t run in the node environment.""fs"" stands for file system; and in order to access it in React Native; you would have to use something like [react-native-fs](https://www.npmjs.com/package/react-native-fs/v/1.2.0) or [rn-nodeify](https://www.npmjs.com/package/rn-nodeify) in order to bridge and talk to each platform\'s native APIs (the iOS/Android platforms; in this case). If you could find a way to integrate these into `tfjs-image-recognition-base` for your specific app use-case; then it should work.@justadudewhohacks please correct me if I\'m wrong in anything that I\'ve stated above.====='; ""I think its posible fix; you add 'fs empty' in [webpack](https://github.com/webpack-contrib/css-loader/issues/447#issuecomment-285598881) and create custom fetch in faceapi.env.monkeyPatch; but i dont know if this working in [react-native](https://github.com/react-boilerplate/react-boilerplate/issues/2279#issuecomment-405824492)=====""; '> Hi @sedaplaksa @wulforr;> > It could very much be possible that `tfjs-image-recognition-base` is currently incompatible with React Native...> > Node is only used by the packager to serve/compile your app bundle; in other words; React Native apps don\'t run in the node environment.> > ""fs"" stands for file system; and in order to access it in React Native; you would have to use something like [react-native-fs](https://www.npmjs.com/package/react-native-fs/v/1.2.0) or [rn-nodeify](https://www.npmjs.com/package/rn-nodeify) in order to bridge and talk to each platform\'s native APIs (the iOS/Android platforms; in this case). If you could find a way to integrate these into `tfjs-image-recognition-base` for your specific app use-case; then it should work.> > @justadudewhohacks please correct me if I\'m wrong in anything that I\'ve stated above.i tried this method but it still did not work sadly  :(====='; 'Hi does anyone have any other different solutions? Or is it just not compatible at the moment ?====='; 'Well i removed face-api from react native and used it in node and sent my request from react native to node.====='; 'Do u mind sharing with me how u did it? Is there any latency during detection ?====='; 'Hi guys; @wulforr can you give more details of you solution; im  with the same problem; but i can turn it to server-side; i have to solve this issue on mobile app.====='; 'Did anyone try integrate face api with this adapter?https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native====='; ""> Did anyone try integrate face api with this adapter?https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native@artem-kushal may be one way to do this work; i didn't konw tfjs-react-native; i will try use it; please comment bellow if you got success...=====""; '> Hi guys; @wulforr can you give more details of you solution; im with the same problem; but i can turn it to server-side; i have to solve this issue on mobile app.Sure @deividsoncs; I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.In server this image is compared to all the images in the database.If you wanna know more or anything in specific do let me know.====='; '> > Hi guys; @wulforr can you give more details of you solution; im with the same problem; but i can turn it to server-side; i have to solve this issue on mobile app.> > Sure @deividsoncs;> I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.> In server this image is compared to all the images in the database.> > If you wanna know more or anything in specific do let me know.Thanks @wulforr ; but my problem is that a have to solve in a mobile app to send only the features extracted from the image captured in a mobile camera; because the image is taking so long time to uploado to the servers; im live at Brazil and the mobile internet connection is so slow in many places here. So; i wish do this at app; could you help on this? ====='; ""> > Hi guys; @wulforr can you give more details of you solution; im with the same problem; but i can turn it to server-side; i have to solve this issue on mobile app.> > Sure @deividsoncs;> I used face-api.js to find similarities between two images. I had images stored in the database and sent my image from mobile app to server.> In server this image is compared to all the images in the database.> > If you wanna know more or anything in specific do let me know.Hi @wulforr; by any chance is your repo of this project public; if yes can you please tell me the name to check it out; I have been looking for examples of how to use face-api.js and I'm a little confused; thank you =====""; 'I assume the library can be implemented in a react native webview====='; ""If anyone manage to make face-api work with react-native; please share your solution.I'm trying to figure out as well.=====""; 'use https://www.npmjs.com/package/react-native-fs instead of fs====='; '@yasahmed have you got face-api.js to work with react native ? can you post an example ?====='; ""as said before; you can use https://www.npmjs.com/package/react-native-fs instead of fs; but it's not enough.you'll need to install https://www.npmjs.com/package/@tensorflow/tfjs-react-native.use `faceapi.setEnv` to initialize; face-api is excpecting an Environment interface```{  Canvas: typeof HTMLCanvasElement  CanvasRenderingContext2D: typeof CanvasRenderingContext2D  Image: typeof HTMLImageElement  ImageData: typeof ImageData  Video: typeof HTMLVideoElement  createCanvasElement: () => HTMLCanvasElement  createImageElement: () => HTMLImageElement  fetch: (url: string; init?: RequestInit) => Promise<Response>}```I was using react-native-canvas for the canvas elementshere is a partial implementation```var RNFS = require('react-native-fs');import Canvas; {  Image;  ImageData;  CanvasRenderingContext2D;} from 'react-native-canvas';import * as faceapi from 'face-api';import { cameraWithTensors } from '@tensorflow/tfjs-react-native';import { Camera } from 'expo-camera';const TensorCamera = cameraWithTensors(Camera);const createCanvasElement = function () {  if (Canvas) {    return new Canvas();  }  throw new Error(    'createCanvasElement - missing Canvas implementation for RN environment';  );};const createImageElement = function () {  if (Image) {    return new Image();  }  throw new Error(    'createImageElement - missing Image implementation for RN environment';  );};const readFile = function (filePath: string) {  try {    return RNFS.readFile(filePath);  } catch (e) {    console.error('error reading file'; e);  }};const fileSystem = {  readFile;};const _fetch =  fetch ||  function () {    throw new Error(      'fetch - missing fetch implementation for browser environment';    );  }; let env = {    Canvas: Canvas;    CanvasRenderingContext2D: CanvasRenderingContext2D;    Image: Image;    ImageData: ImageData;    Video: TensorCamera;    createCanvasElement;    createImageElement;    fetch: _fetch;    ...fileSystem;  };  faceapi.env.setEnv(env);```for loading the models i uploaded them to external storage (AWS S3)and used ` await faceapi.nets.tinyFaceDetector.loadFromUri('https://your-bucket-urls3.amazonaws.com/models');`and I used this example to get a proper input that I can pass to the faceDetector: https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsxEverything haw worked without any errors but I couldn't get any results; also the detection process was very slow compared to web browser.Good Luck; and please let us know if you have any breakthrough   =====""; '@0xori im trying to do step that you mention; but im still getting error; did u manage to finish that code ?====='; 'I am facing the same issue. But somehow I managed to run the application without any errors up to the detection process following @0xori instruction except for the ""expo-camera"" I used ""react-native-camera"". But when I pass the camera ref  as a parameter to the faceapi.detectSingleFace() function; I\'m getting the following error; media.addEventListener(""load""; ""onLoad"") is not a function!====='; 'Is there anybody having the solution for using face-api.js in RN ? I am trying to  use some solutions above but it seems not to work TT =====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,change platform,Changing device/browser,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Compatibility issues with React Native""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/44,Face API can support and using in server-side?,4,closed,2018-07-09T02:40:13Z,2018-07-16T02:25:34Z,I have some questions; is it possible to use `face-api` in server side? for example: I need use it to process some image in db. If it is; possible; could it work correctly without environment config like on browser?Use it like other package? And now; I can not setup it in server-side! :<```javascriptimport faceApi from 'face-api'// faceApi is undefined```Thanks!,"[""If you are using typescript: `import * as faceapi from 'face-api'` since it has no default export.The neural network implementations could probably also run on server side; but the package as is right now probably won't work with nodejs; as it uses quite some stuff from the dom api. For face recognition with nodejs you can also give [face-recognition.js](https://github.com/justadudewhohacks/face-recognition.js) a try.=====""; '@justadudewhohacks what is `dom api` using in package now? we can convert it and run on nodejs? If you want I can help you convert it to nodejs because face detect some using in server.thanks for answers!====='; 'Probably possible to convert it to nodejs. If you want to investigate in that; sure. However; I am not sure what the state of tfjs + node is atm. If the code is running on the cpu in node; it would probably be very slow.====='; 'thanks @justadudewhohacks ; I will learn more about tfjs 👍 =====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,change platform,Changing device/browser,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Functionality not working as expected in server-side""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/440,Which size  images  gives the best performance?,2,open,2019-10-14T13:06:55Z,2019-10-26T12:18:20Z,hi Vincent;im used many input size images ( images > 150 *150   ;  it you told in orther issuse) ! but i dont know Which size gives the best performance?,['The face-recognition net has an input size of 150x150; all other classification nets use an input size of 112x112; the size of the face after face extraction should be around the same size. The smaller they are the more blurrier they will be after upsampling and thus give worse results.So it depends on how large the face appear in your input image.====='; 'oh thanks for your anwser ^^====='],Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change input shape,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/450,Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA,6,open,2019-10-24T05:38:54Z,2021-01-03T02:50:39Z,"Logs: ```cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].node-pre-gyp info This Node instance does not support builds for N-API version 4node-pre-gyp info This Node instance does not support builds for N-API version 42019-10-24 13:34:37.471423: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA/usr/local/lib/node_modules/ts-node/src/index.ts:245    return new TSError(diagnosticText; diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:commons/faceDetection.ts:19:10 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'SsdMobilenetv1' have no overlap.19   return net === faceapi.nets.ssdMobilenetv1            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~commons/faceDetection.ts:21:8 - error TS2367: This condition will always return 'false' since the types 'NeuralNetwork<any>' and 'TinyFaceDetector' have no overlap.21     : (net === faceapi.nets.tinyFaceDetector          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~commons/faceDetection.ts:27:60 - error TS2345: Argument of type 'SsdMobilenetv1' is not assignable to parameter of type 'NeuralNetwork<any>'.  Types of property 'extractParamsFromWeigthMap' are incompatible.    Type '(weightMap: NamedTensorMap) => { params: NetParams; paramMappings: ParamMapping[]; }' is not assignable to type '(weightMap: NamedTensorMap) => { params: any; paramMappings: ParamMapping[]; }'.      Types of parameters 'weightMap' and 'weightMap' are incompatible.        Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor_types"").NamedTensorMap'.          Index signatures are incompatible.            Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank>'.              Types of property 'flatten' are incompatible.                Type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-imag...' is not assignable to type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                  Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R1>'.                    Types of property 'asScalar' are incompatible.                      Type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-imag...' is not assignable to type '() => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                        Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R0>'.                          Types of property 'as2D' are incompatible.                            Type '(rows: number; columns: number) => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-...' is not assignable to type '(rows: number; columns: number) => import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core...'.                              Type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...' is not assignable to type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>'.                                Types of property 'asType' are incompatible.                                  Type '<T extends import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs...' is not assignable to type '<T extends import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>>(...'.                                    The 'this' types of each signature are incompatible.                                      Type 'T' is not assignable to type 'Tensor<Rank.R2>'.                                        Property 'relu6' is missing in type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/types"").Rank.R2>' but required in type 'import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor"").Tensor<import(""/Users/yicheng/opencvTest/face-api.js/examples/examples-nodejs/node_modules/face-api.js/node_modules/tfjs-image-reco...'.27 export const faceDetectionOptions = getFaceDetectorOptions(faceDetectionNet)                                                              ~~~~~~~~~~~~~~~~  node_modules/face-api.js/node_modules/tfjs-image-recognition-base/node_modules/@tensorflow/tfjs-core/dist/tensor.d.ts:540:5    540     relu6<T extends Tensor>(this: T): T;            ~~~~~    'relu6' is declared here.    at createTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:245:12)    at reportTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:249:19)    at getOutput (/usr/local/lib/node_modules/ts-node/src/index.ts:357:34)    at Object.compile (/usr/local/lib/node_modules/ts-node/src/index.ts:415:32)    at Module.m._compile (/usr/local/lib/node_modules/ts-node/src/index.ts:493:43)    at Module._extensions..js (internal/modules/cjs/loader.js:700:10)    at Object.require.extensions.(anonymous function) [as .ts] (/usr/local/lib/node_modules/ts-node/src/index.ts:496:12)    at Module.load (internal/modules/cjs/loader.js:599:32)    at tryModuleLoad (internal/modules/cjs/loader.js:538:12)    at Function.Module._load (internal/modules/cjs/loader.js:530:3)```","['What version of typescript are you using? face-api.js is compiled with TS 3.6.3; so upgrade your TS compiler.====='; 'I\'m using the latest version v3.6.4.<img width=""110"" alt=""螢幕快照 2019-10-29 上午9 54 15"" src=""https://user-images.githubusercontent.com/11001914/67731140-30782600-fa32-11e9-81ee-88bea35b2dd6.png"">====='; 'Me too...====='; 'Me too 3.7.4====='; 'I am on 3.9.3 and still get the same issue``` Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-07-21 16:00:24.813860: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1071ef570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-07-21 16:00:24.813925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host; Default Version====='; 'I am using Typescript 4.1.3 and still same message> Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2; cpu backend was already registered. Reusing existing backend factory.=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change typescript version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/451,Improve face match accuracy,1,closed,2019-10-27T17:36:39Z,2019-11-21T10:51:07Z,Hi; faceapi.js is amazing; I currently use it in my login system for 3 months; registered ~ 50 people; and they logged in 4 times a day; I use maxDistanceDescriptor equal to 0.45-0.5 in FaceMatcher; each person has 1-3 descriptorsDuring this time; the match has been wrong 4 times; at least those that I have been able to detect.Example:Petter's photo-> Its detected as Mike with distance of 0.44Snapshots from : - Fixed tablet camera login; it have resolution its from 2MP and size of 640x480 - Mobile Camera - resolution dynamic (each people)How can I improve or verify that the matching is correct? Add more descriptors? How many? Update descriptors each month? I will planning register ~ 1000 people :)faceMatcher:``    faceMatcher = new faceapi.FaceMatcher (        labeledDescriptors;        maxDescriptorDistance      );``Thank you,['Improved little; I add 10 photos per people and working with maxDescriptors 0.45====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data preprocess,Add data processing,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/455,Issue loading: TypeError: Nt.makeTensor is not a function,24,closed,2019-10-29T23:11:35Z,2021-06-20T16:43:10Z,I just started and in the latest version I am getting the following error in the browser:```tf-core.esm.js:17 Uncaught (in promise) TypeError: Nt.makeTensor is not a function    at Sn (tf-core.esm.js:17)    at kn (tf-core.esm.js:17)    at o (tf-core.esm.js:17)    at Fh (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at tf-core.esm.js:17```I installed `face-api.js` with `npm install face-api.js`. Now using it as follows:```import * as faceapi from 'face-api.js';await faceapi.nets.tinyFaceDetector.loadFromUri('/models');```As soon as I try to load the model; I get the type error above. Any idea's where it is going wrong? I saw this https://github.com/tensorflow/tfjs/issues/2194#issuecomment-546187719 but couldn't see a solution to that. Thanks!,"['Hi; I just got the same error(tfjs-core@1.3.1) and I downgraded to tfjs-core@1.3.0; and this error disappeared; however; a new error rendered. Anyway; you could have a try.====='; 'When downgrading tfjs-core to 1.2.9 I get this error when trying to use the librarySsdMobilenetv1.js:26 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (SsdMobilenetv1.js:26)    at SsdMobilenetv1.<anonymous> (SsdMobilenetv1.js:81)    at step (tslib.es6.js:196)    at Object.next (tslib.es6.js:127)    at fulfilled (tslib.es6.js:80)here is the lock file of a project where this library still works.[yarnlockfile.txt](https://github.com/justadudewhohacks/face-api.js/files/3787177/yarnlockfile.txt)====='; '@Spodeopieter; I think your last error has to do with not asynchronously loading in your model. Are you sure it is being loaded in correctly before you try to use it?====='; '@luucv  this is how I am loading the models : `  loadModels = async () => {`    `const MODEL_URL = ""/models"";`   ` await faceapi.loadSsdMobilenetv1Model(MODEL_URL);`   ` await faceapi.loadFaceLandmarkModel(MODEL_URL);`   ` await faceapi.loadFaceRecognitionModel(MODEL_URL);` ` };`====='; ""@luucv have the same error right now. I installed face-api.js via yarn. I retried it with `npm install face-api.js` and it worked without any problem.I'm using webpack-encore with Symfony.=====""; 'Have the same problem.Try to delete package-lock.json; node_modules. Then install the latest version (currently 0.21.0)====='; 'The issue is most certainly due to your package managers installing a tfjs core version other than 1.2.9 (which is the one the latest face-api.js version uses). May be an issue with yarn?Try deleting your yarn locks / package locks and do a clean installation. If for some reason the issue still remains; try explicitly installing `npm i @tensorflow/tfjs-core@1.2.9`.====='; '@justadudewhohacks Thanks for the Tip!I\'ve solved my problem :DYou do not just have to say `""@tensorflow/tfjs-core"": ""1.2.9""` in the dependencies-section of the package.json; you also have to prevent _tfjs-image-recognition-base_ from getting a newer version:```""resolutions"": {    ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""}```====='; ""I've solved by taking these steps:1.  delete tensorflow node module2. `npm i @tensorflow/tfjs-node@1.2.9`3. `npm i @tensorflow/tfjs-code@1.2.9`The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.=====""; 'I have same problem.```""@tensorflow/tfjs-core"": ""1.2.9"";""@tensorflow/tfjs-node"": ""1.2.9"";""face-api.js"": ""^0.21.0"";```5 days ago; It was operating normally. ====='; '> @justadudewhohacks Thanks for the Tip!> I\'ve solved my problem :D> > You do not just have to say `""@tensorflow/tfjs-core"": ""1.2.9""` in the dependencies-section of the package.json; you also have to prevent _tfjs-image-recognition-base_ from getting a newer version:> > ```> ""resolutions"": {>     ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""> }> ```This helped. looks like `tfjs-image-recognition-base` was secretly installing the latest tfjs behind our backs.====='; 'have same problem; resolutions no helped====='; 'installing face-api with npm instead of yarn seems to solve the problem for me.====='; ""> installing face-api with npm instead of yarn seems to solve the problem for me.I'm running into the same issue with yarn; installing with npm solved me problem as well.=====""; '> looks like `tfjs-image-recognition-base` was secretly installing the latest tfjs behind our backs.I added the `resolutions` section as suggested above and by [this help doc](https://yarnpkg.com/lang/en/docs/selective-version-resolutions/) but it was apparently ignored by yarn cuz it resolved to 1.3.1 anyway per yarn.lock.  I\'ve got a yarn monorepo so I\'m sure there\'s either some logical reason or some bug as to why that was the case.I was able to get around it by **manually editing** the `yarn.lock` to change the section for `""@tensorflow/tfjs-core@^1.2.9"":` to be identical to the section for `""@tensorflow/tfjs-core@1.2.9"":` (no caret).  But that seems super hacky and tenuous.  If this is working in npm but not yarn that seems like a yarn bug to me.====='; ""> I've solved by taking these steps:> > 1. delete tensorflow node module> 2. `npm i @tensorflow/tfjs-node@1.2.9`> 3. `npm i @tensorflow/tfjs-code@1.2.9`> > The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.This solved the same issue I was having. Hope this helps somebody. =====""; 'So for me nothing worked what did work was this**FOR YARN USERS**1. add ""resolutions"": {    ""tfjs-image-recognition-base/@tensorflow/tfjs-core"": ""<=1.2.9""}2. in the package add these lines  ""dependencies"": {    ""@tensorflow/tfjs-core"": ""1.2.9"";    ""face-api.js"": ""^0.21.0"";    ""next"": ""9.1.4"";    ""react"": ""16.12.0"";    ""react-dom"": ""16.12.0"";    ""react-stickynode"": ""^2.1.1"";    ""styled-components"": ""^4.4.1"";    ""tfjs-image-recognition-base"": ""^0.6.2""  }actually installing tfjs-image-recognition-base worked!====='; ""I have this same issue. I get the error: ![image](https://user-images.githubusercontent.com/39976117/69595561-c44afc80-0fb4-11ea-8a2a-4819275fe322.png)when I try to load any model. I have the models in `public/models` and am trying to access them with ``` await faceapi.nets.ssdMobilenetv1.loadFromUri('/models') ```I've tried all the fixes mentioned above and I don't think it's an issue with versioning; or npm. Is there another way to load the models? Thanks!=====""; 'Also having the same issue; though my error message reads:`engine_1.ENGINE.makeTensor is not a function`Tried some of the above fixes with no luck. Using tfjs 1.2.9 also.====='; ""We need to upgrade to TFJS 1.3.x for other reasons.  Any idea what's causing this problem with newer TFJS packages?  I don't see `makeTensor` anywhere in the code here or in `tfjs-image-recognition-base`I'm guessing maybe the models need to be regenerated for TF 1.3.x?=====""; ""> I've solved by taking these steps:> > 1. delete tensorflow node module> 2. `npm i @tensorflow/tfjs-node@1.2.9`> 3. `npm i @tensorflow/tfjs-code@1.2.9`> > The 1.2.9 is the last one supported by latest face-api.js version as @justadudewhohacks said.It solved my problem.=====""; 'I upgraded the tfjs-core version the latest release. Also I decided to move tfjs-image-recognition-base to face-api.js; so yarn should not have any reason to install different versions of tfjs-core anymore unless you or one if your dependencies is explicitly installing another version.====='; 'I\'m getting a ""Kt.makeTensor is not a function""; more specifically: Uncaught (in promise) TypeError: Kt.makeTensor is not a function    at Rn (face-api.min.js:2426)    at In (face-api.min.js:2407)    at e (face-api.min.js:11444)    at Vh (face-api.min.js:11445)    at face-api.min.js:12054    at Array.forEach (<anonymous>)    at face-api.min.js:12053    at Array.forEach (<anonymous>)    at face-api.min.js:12047    at face-api.min.js:103I\'ve tried all these resolutions and none of them worked. Could anyone help please?====='; '> I\'m getting a ""Kt.makeTensor is not a function""; more specifically:> Uncaught (in promise) TypeError: Kt.makeTensor is not a function> at Rn (face-api.min.js:2426)> at In (face-api.min.js:2407)> at e (face-api.min.js:11444)> at Vh (face-api.min.js:11445)> at face-api.min.js:12054> at Array.forEach ()> at face-api.min.js:12053> at Array.forEach ()> at face-api.min.js:12047> at face-api.min.js:103> > I\'ve tried all these resolutions and none of them worked. Could anyone help please?I\'m facing the same problem. Is there any fix available for it?=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/463,TypeError: forwardFunc is not a function,2,closed,2019-11-05T13:22:15Z,2021-12-12T13:55:27Z,"Hi!  Well; my question is on the title; so here is my full code:```/////////////////////////////////////////Requirements'use strict';require('@tensorflow/tfjs-node');const tf = require('@tensorflow/tfjs');const { createCanvas; createImageData; Image } = require('canvas');const nodeFetch = require('node-fetch');const fapi = require('face-api.js');const path = require('path');const width = 640;const height = 480;fapi.env.monkeyPatch({ fetch: nodeFetch });const MODELS_URL = path.join(__dirname; '/examples/video-compositing/models');/////////////////////////////////////////Load ModelsPromise.all([    fapi.nets.tinyFaceDetector.loadFromDisk(MODELS_URL);    fapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL);    fapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL);    fapi.nets.faceExpressionNet.loadFromDisk(MODELS_URL)]).then(beforeOffer);/////////////////////////////////////////After loading models run this functionfunction beforeOffer() {    // TODO(mroberts): Is pixelFormat really necessary?    const canvas = createCanvas(width; height);    const context = canvas.getContext('2d'; { pixelFormat: 'RGBA24' });/////////////////////////////////////////Get an image and convert it into tensor 3d    async function loadLocalImage(imgPath) {        try {            let img = new Image();            img.onload = () => {                context.drawImage(img; 0; 0);            }            img.onerror = err => {                console.log(err);            }            img.src = imgPath;            const image = tf.browser.fromPixels(canvas);            return image;        } catch (e) {            console.log(e);        }    }/////////////////////////////////////////Get image tensor    async function getImage(imgPath) {        try {            const image = await loadLocalImage(imgPath);            return image;        } catch (e) {            console.log(e);        }    }/////////////////////////////////////////Show results    async function detections() {        const img = await getImage(""/home/ilya/Downloads/putin.png"");        const result = await fapi.detectAllFaces(img; new fapi.TinyFaceDetectorOptions()).withFaceLandmarks();        console.log(result);    }    detections();}```Here is an error:![ucZDChN3RKQ](https://user-images.githubusercontent.com/42372955/68211463-abb87980-ffe8-11e9-8b0c-c6e79b189cf0.jpg)When i use just `await fapi.detectAllFaces(img; new fapi.TinyFaceDetectorOptions())` without `.withFaceLandmarks()` all works fine and i get ```[  FaceDetection {    _imageDims: Dimensions { _width: 640; _height: 480 };    _score: 0.7238702505633767;    _classScore: 0.7238702505633767;    _className: '';    _box: Box {      _x: -5.652424869069339;      _y: 4.018378046097197;      _width: 76.10965370740514;      _height: 61.51865117662532    }  }]```","[""So i’ve installed tfjsnode; tfjs; tfjs-core  1.2.9 version and all works fine.```[  {    detection: FaceDetection {      _imageDims: [Dimensions];      _score: 0.9932203603310482;      _classScore: 0.9932203603310482;      _className: '';      _box: [Box]    };    landmarks: FaceLandmarks68 {      _imgDims: [Dimensions];      _shift: [Point];      _positions: [Array]    };    unshiftedLandmarks: FaceLandmarks68 {      _imgDims: [Dimensions];      _shift: [Point];      _positions: [Array]    };    alignedRect: FaceDetection {      _imageDims: [Dimensions];      _score: 0.9932203603310482;      _classScore: 0.9932203603310482;      _className: '';      _box: [Box]    }  }]```=====""; 'I got this error and the reason is because face-api only works with `@tensorflow/tfjs-node` version 1. So if you do `yarn add @tensorflow/tfjs-node@1.7.4` it should work.=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/466,TypeError: st().registerTensor is not a function,3,closed,2019-11-07T08:50:26Z,2019-12-07T14:47:56Z,![Screenshot from 2019-11-05 18-19-47](https://user-images.githubusercontent.com/53167951/68373125-cf102f80-0174-11ea-96ad-f9a59ba241c8.png)I get this error while using `withFaceLandmarks()` . Many other functions like `detectSingleFace()` `detectAllFaces()` cause the same error if I use other models that are not `tinyFaceDetector`.Can anyone help me ? Many thanks.Here is my code:```    await faceapi.nets.tinyFaceDetector.loadFromUri(modelsUri);    await faceapi.nets.faceLandmark68TinyNet.loadFromUri(modelsUri);    const camera = new Camera({      fps;      onSnapshot: processImage;    });    const cameraSettings = await camera.start();    async function processImage(image) {      const face = await faceapi                          .detectSingleFace(image; new faceapi.TinyFaceDetectorOptions({ inputSize; scoreThreshold }))                          .withFaceLandmarks(true);   }```,"[""I'm also seeing this issue. My setup is essentially the faceRecognition example to a T. I haven't been able to figure out what the problem is as of yet. Some help would be appreciated.=====""; 'Downgrading tfjs-core appears to fix it (via [this other issue](https://github.com/justadudewhohacks/face-api.js/issues/455#issuecomment-548987538) )====='; 'Same issue as #455.=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/488,I'm having a issue in running the example,1,closed,2019-11-28T14:44:43Z,2019-12-07T10:04:38Z,I'm having following issue;cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].Error: The specified module could not be found.<MyLoc>\face-api.js\examples\examples-nodejs\node_modules\@tensorflow\tfjs-node\dist\index.js:44:16)    at Module._compile (internal/modules/cjs/loader.js:1121:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1160:10)    at Module.load (internal/modules/cjs/loader.js:976:32)    at Function.Module._load (internal/modules/cjs/loader.js:884:14),"[""This error doesn't come from face-api.js; it comes from tfjs-node. Please make sure you are using the same tfjs-core and tfjs-node version as suggested in the package.json of the example.=====""]",Initialization Faliure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Backend Reinitialization Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/491,node-js examples not compiling,17,open,2019-12-02T23:44:17Z,2021-02-26T14:46:21Z,To try and run the examples I do the following with a freshly cloned repo:```cd examples/examples-nodejsnpm itsc faceDetection.ts```I get the following error:```node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 556 more ...; readonly WAIT_FAILED: number; }'; but here has type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 557 more ...; readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number; }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../usr/lib/node_modules/typescript/lib/lib.dom.d.ts:16485:13    16485 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```I have the following:tsc --version: 3.7.2node --version: 12.13.0I have tried adding skipLibCheck: true to tsconfig.json but it did not do any difference.Do you have any idea why it is not working for node?,"[""This command compile.```tsc --skipLibCheck faceDetection.ts```But then it doesn't workerror:```UnhandledPromiseRejectionWarning: TypeError: this.backend.register is not a function    at Engine.registerTensor (/home/ff/nodejs/face-api.js/node_modules/@tensorflow/tfjs-core/dist/engine.js:468:30)```=====""; ""Please check out @russellwmy's answer in this issue #376. Make sure you are using the same typescript (take a look at the package.json of the examples folder) and tfjs-core version as face-api.js.=====""; 'same error; I tried #376; seems different errors. not work. I just used the default package.json.tsc --version  Version 3.7.3node --version v12.13.1tsc --skipLibCheck faceDetection.tsthere is no error.but when I run node faceDetection.js(node:25928) UnhandledPromiseRejectionWarning: TypeError: this.backend.register is not a function    at Engine.registerTensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\engine.js:468:30)    at new Tensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\tensor.js:246:21)    at Function.Tensor.make (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\tensor.js:261:16)    at makeTensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\ops\\tensor_ops.js:98:28)    at Object.tensor (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\ops\\tensor_ops.js:55:12)    at _loop_2 (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\io_utils.js:219:36)    at Object.decodeWeights (E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\io_utils.js:223:9)    at E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\weights_loader.js:255:66    at Array.forEach (<anonymous>)    at E:\\face\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\dist\\io\\weights_loader.js:253:44(node:25928) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:25928) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.====='; 'got the reason. it seems the ""npm i"" in examples will get the tfjs 1.4.0 version; which is not compatible with face-api.js. ====='; 'Thanks! Just to make this clear for anyone else; you can fix by removng the caret (^) in the package.json for  @tensorflow/tfjs-node and getting version 1.2.3 specifically. For example in the examples folder changing package.json to:```{  ""author"": ""justadudewhohacks"";  ""license"": ""MIT"";  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.2.3"";    ""canvas"": ""^2.5.0"";    ""face-api.js"": ""../../""  }}```====='; ""just cloned the repository; with tsc --version: 3.7.3node --version: v13.3.0```cd face-api.js/examples/examples-nodejsnpm its-node  faceDetection.ts```gives:```faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js'.1 import * as faceapi from 'face-api.js';```=====""; 'Same scenario; and same results as @sbocconi.node --version: 12.4.0tsc --version: 3.7.4====='; '@sbocconi @imgh1010 What version of tfjs-node are you using? Did you change the version in package.json before installing with `npm install` as per my previous comment?====='; 'My package.json is as  currently available in the repository:```{  ""author"": ""justadudewhohacks"";  ""license"": ""MIT"";  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.4.0"";    ""canvas"": ""^2.6.0"";    ""face-api.js"": ""../../""  }}```and the error does not depend on the version of tensorflow; but on the fact that face-api.js cannot be found.I also tried with:```{  ""author"": ""justadudewhohacks"";  ""license"": ""MIT"";  ""dependencies"": {    ""@tensorflow/tfjs-node"": ""1.4.0"";    ""canvas"": ""^2.6.0"";    ""face-api.js"": ""git+https://github.com/justadudewhohacks/face-api.js.git#master""  }}```but I get the same error; even after a `npm install face-api.js`====='; 'The solution was to first run```npm i && npm run-script build```in the root directory before trying to run the examples.====='; 'however;I found :examples/examples-nodejs/package.json {  ""author"": ""justadudewhohacks"";  ""license"": ""MIT"";  ""dependencies"": {    ""face-api.js"": ""../../""  #   this will happen circulate create  }}will circulate create face-api.js fold; is usually or not? @justadudewhohacks ====='; 'when I do ""npm i"" in console.====='; '> The solution was to first run> > ```> npm i && npm run-script build> ```> > in the root directory before trying to run the examples.when I do “npm i && npm run-script build”;and I get:> @tensorflow/tfjs-node@1.7.0 install /home/dzy/Project/face-api.js/node_modules/@tensorflow/tfjs-node> node scripts/install.jsCPU-linux-1.7.0.tar.gz* Downloading libtensorflow[==============================] 15944787/bps 100% 0.0s* Building TensorFlow Node.js bindings> canvas@2.6.1 install /home/dzy/Project/face-api.js/node_modules/canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download [canvas] Success: ""/home/dzy/Project/face-api.js/node_modules/canvas/build/Release/canvas.node"" is installed via remotenpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""linux"";""arch"":""x64""})added 442 packages from 783 contributors and audited 1741 packages in 170.919s10 packages are looking for funding  run `npm fund` for detailsfound 8 low severity vulnerabilities  run `npm audit fix` to fix them; or `npm audit` for details> face-api.js@0.22.2 build /home/dzy/Project/face-api.js> rm -rf ./build && rm -rf ./dist && npm run rollup && npm run rollup-min && npm run tsc && npm run tsc-es6> face-api.js@0.22.2 rollup /home/dzy/Project/face-api.js> rollup -c rollup.config.jsloaded rollup.config.js with warnings(!) Unused external importsdefault imported from external module \'path\' but never usedsrc/index.ts → dist/face-api.js...created dist/face-api.js in 10.3s> face-api.js@0.22.2 rollup-min /home/dzy/Project/face-api.js> rollup -c rollup.config.js --environment minify:trueloaded rollup.config.js with warnings(!) Unused external importsdefault imported from external module \'path\' but never usedsrc/index.ts → dist/face-api.min.js...created dist/face-api.min.js in 14.5s> face-api.js@0.22.2 tsc /home/dzy/Project/face-api.js> tsc> face-api.js@0.22.2 tsc-es6 /home/dzy/Project/face-api.js> tsc --p tsconfig.es6.jsonAnd tsc faceDetection.ts;  get same error.====='; 'Worked fine for me:- npm install & npm run-script build (the remove commands for the directories fail on windows; remove them from package.json in the face-api.js\\package.json file)- run npm install in examples-nodejs- running tsc faceDetection.ts fails with an error (duplicate librarydefinition WebGL2 or something)- instead tsc --skipLibCheck faceDetection.ts and things run fine.====='; ""@ronaldkuip I am also that add '--skipLibCheck' ; but is fine?=====""; ""ts-node faceDetection.ts should run in examples-nodejs directory.Tsc -skipLibCheck.ts creates faceDetection.jsnode faceDetection.js then runs fine too.Van: homedawn <notifications@github.com>Verzonden: woensdag 6 mei 2020 02:52Aan: justadudewhohacks/face-api.js <face-api.js@noreply.github.com>CC: ronaldkuip <ronald.kuip@Live.nl>; Mention <mention@noreply.github.com>Onderwerp: Re: [justadudewhohacks/face-api.js] node-js examples not compiling (#491)@ronaldkuip<https://github.com/ronaldkuip> I am also that add '--skipLibCheck' ; but is fine?—You are receiving this because you were mentioned.Reply to this email directly; view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/491#issuecomment-624383634>; or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACV4DRCPYR5R362D6IYE3JDRQCYDJANCNFSM4JUN6MOA>.=====""; ""**Hi everyone getting this error while running examples-nodejs using command => tsc faceDetection.ts**commons/env.ts:5:26** - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.5 import * as faceapi from 'face-api.js';                           ~~~~~~~~~~~~~commons/faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.1 import * as faceapi from 'face-api.js';                           ~~~~~~~~~~~~~faceDetection.ts:1:26 - error TS2307: Cannot find module 'face-api.js' or its corresponding type declarations.1 import * as faceapi from 'face-api.js';                           ~~~~~~~~~~~~~node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.d.ts:61:5 - error TS2416: Property 'stridedSlice' in type 'NodeJSKernelBackend' is not assignable to the same property in base type 'KernelBackend'.  Type '<T extends Tensor<Rank>>(x: T; begin: number[]; end: number[]; strides: number[]; beginMask: number; endMask: number; ellipsisMask: number; newAxisMask: number; shrinkAxisMask: number) => T' is not assignable to type '<T extends Tensor<Rank>>(x: T; begin: number[]; end: number[]; strides: number[]) => T'.61     stridedSlice<T extends Tensor>(x: T; begin: number[]; end: number[]; strides: number[]; beginMask: number; endMask: number; ellipsisMask: number; newAxisMask: number; shrinkAxisMask: number): T;       ~~~~~~~~~~~~node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.d.ts:64:5 - error TS2416: Property 'fusedBatchMatMul' in type 'NodeJSKernelBackend' is not assignable to the same property in base type 'KernelBackend'.  Type '(a: Tensor3D; b: Tensor3D; transposeA: boolean; transposeB: boolean; bias?: Tensor<Rank>; activation?: Activation) => Tensor3D' is not assignable to type '({ a; b; transposeA; transposeB; bias; activation; preluActivationWeights }: FusedBatchMatMulConfig) => Tensor3D'.64     fusedBatchMatMul(a: Tensor3D; b: Tensor3D; transposeA: boolean; transposeB: boolean; bias?: Tensor; activation?: Activation): Tensor3D;       ~~~~~~~~~~~~~~~~node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 556 more ...; readonly WAIT_FAILED: number; }'; but here has type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 557 more ...; readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number; }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../AppData/Roaming/npm/node_modules/typescript/lib/lib.dom.d.ts:16394:13    16394 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 6 errors.=====""]",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/5,Face Detector Batch Input,1,open,2018-06-09T17:47:41Z,2020-09-23T04:56:15Z,Apply score filtering and non max suppression to all input images in the post processing layer. Currently only the first image of the input batch is returned.,"[""Hi; how close is this feature to being finished? It would be very helpful for a project I'm working on.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change input size,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/500,face-api is incompatible with TFJS 1.3,1,closed,2019-12-09T18:19:41Z,2019-12-16T08:40:41Z,Using face-api with recent TensorflowJS builds results in an error when loading models.See: https://github.com/justadudewhohacks/face-api.js/issues/455We suspect this is because the saved models using by face-api are too old and need to be re-trained or re-exported to be compatible with new TFJS releases,['They dont need to be retrained; I upgraded tfjs-core to 1.4.0. ====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/531,Face matcher is not giving the correct output while matched,1,open,2020-01-21T10:44:31Z,2020-08-03T14:33:08Z,"@justadudewhohacks thanks for the beautiful library.but this is not helpful in face matching.please check the below code for recognition as it gives wrong descriptors.**CODE**    const labels = [      {name:'Raj';img:      [        {personimg:'assets/imgs/raj1.jpg'};        {personimg:'assets/imgs/raj2.jpg'};        {personimg:'assets/imgs/raj3.jpg'}      ]     }    ]    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 0; i <= 2; i++) {          console.log('.img[i].personimg';label.img[i].personimg)          const img = await faceapi.fetchImage(label.img[i].personimg)          const detections = await faceapi.detectSingleFace(img;new faceapi.SsdMobilenetv1Options({ minConfidence: 0.9 })).withFaceLandmarks().withFaceDescriptor()          if (!detections) {            throw new Error(`no faces detected for ${label.name}`)          }          descriptions.push(detections.descriptor)        }          return new faceapi.LabeledFaceDescriptors(label.name; descriptions)      })    )}**matcher code** const labeledFaceDescriptors = await this.loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6)    // console.log(labeledFaceDescriptors;faceMatcher)    const displaySize = { width: myImg.offsetWidth; height: myImg.offsetHeight }    //Detect Faces from input IMAGE     faceapi.matchDimensions(canvas; displaySize)    const detections = await faceapi.detectAllFaces(myImg).withFaceLandmarks().withFaceExpressions().withFaceDescriptors()    console.log('detections';detections)    const resizedDetections = faceapi.resizeResults(detections; displaySize)    faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)    // const results = faceMatcher.findBestMatch(detections.descriptor) // for detectSingleFace    const results = resizedDetections.map(d =>      faceMatcher.findBestMatch(d.descriptor))    console.log('results';results)Gives wrong matchRef Img: ""https://i.ibb.co/DfP6VMf/raj2.jpg""Query Img: ""https://i.ibb.co/ZN47xBY/test1.jpg""","[' Promise.all([      faceapi.nets.tinyFaceDetector.loadFromUri(\'../../../../assets/models/\');      faceapi.nets.faceLandmark68Net.loadFromUri(\'../../../../assets/models/\');      faceapi.nets.faceRecognitionNet.loadFromUri(\'../../../../assets/models/\');      faceapi.nets.faceExpressionNet.loadFromUri(\'../../../../assets/models/\');      faceapi.nets.ageGenderNet.loadFromUri(\'../../../../assets/models/\');      faceapi.nets.ssdMobilenetv1.loadFromUri(\'../../../../assets/models/\');    ]).then((value: any) => {});loadLabelledImages = () => {    const labels = [""labels""];    return Promise.all(      labels.map(async (label) => {        const descriptions = [];        for (let i = 1; i <= 5; i++) {          const image = await faceapi.fetchImage(            `../../../../assets/labelled_images/${label}/${i}.jpg`          );          const detections = await faceapi            .detectSingleFace(image)            .withFaceLandmarks()            .withFaceDescriptor();          descriptions.push(detections.descriptor);        }        return new faceapi.LabeledFaceDescriptors(label; descriptions);      })    );  };=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add input preprocess,Add data processing,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/542,Error with Euclides number comparing 2 faces,1,open,2020-01-31T19:40:22Z,2020-02-28T08:41:40Z,"Hi there; I'm using the library from node js and it works quite good; but I have some errors comparing 2 faces. Usually it works good; but sometimes it does not recognise the same person; and sometimes returns that 2 different faces are the same...My code looks like this:```jsimport { Request; Response } from 'express';import '@tensorflow/tfjs-node';import * as faceapi from 'face-api.js';import { loadImage; Canvas; Image; ImageData } from 'canvas';import fetch from 'node-fetch';import env from '../config/environment';// @ts-ignorefaceapi.env.monkeyPatch({ Canvas; Image; ImageData; fetch });const maxDescriptorDistance = Number(env.faceMaxdescriptor);interface ResultType {  message: string;  data?: any;  success: boolean;  error?: string;};let loadedModels: boolean = false;const loadFaceApiModels = async () => {  return new Promise(async (resolve) => {    if (!loadedModels) {      await Promise.all([        faceapi.nets.ssdMobilenetv1.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`);        faceapi.nets.faceLandmark68Net.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`);        faceapi.nets.faceRecognitionNet.loadFromUri(`${process.env.AWS_URL}/faceRecognitionModels`);      ]);      loadedModels = true;      resolve();    } else {      resolve();    }  });};const detectFace = (image: any): any => faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor();export const checkFace = async (req: Request; res: Response): Promise<void> => {  const {    body: { image1; image2 };  } = req;  let result: ResultType = { success: false; message: req.t('faceRecognition.notMatch'); error: 'FACE_NOT_MATCH' };  await loadFaceApiModels();  try {    const [canvasImage1; canvasImage2] = await Promise.all([ loadImage(image1); loadImage(image2) ]);    if (canvasImage1 && canvasImage2) {      const [detection1; detection2] = await Promise.all([ detectFace(canvasImage1); detectFace(canvasImage2) ]);      if (detection1 && detection2) {        const distance = faceapi.euclideanDistance(detection1.descriptor; detection2.descriptor);        result.data = distance;        if (distance <= maxDescriptorDistance) {          result = {            success: true;            message: req.t('faceRecognition.success');            data: distance;          }        } else {          result = { success: false; message: req.t('faceRecognition.notMatch'); error: 'FACE_NOT_MATCH'; data: distance };        }      } else {        result = { success: false; message: req.t('faceRecognition.cannotDetectFace'; { image: !detection1 ? image1 : image2 }); error: 'FACE_NOT_DETECTED' };      }    } else {      result = { success: false; message: req.t('faceRecognition.noImageError'; { image: !canvasImage1 ? image1 : image2 }); error: 'NO_IMAGE' };    }  } catch (error) {    result = { success: false; message: req.t('faceRecognition.error'); error: 'INTERNAL_ERROR'; data: error };  };  res.status(200).send(result);  return;};```So this endpoint basically receive 2 images urls which are saved in a bucket in s3 (they are public)Something like:**POST** to **endpoint/check**> body: {>	""image1"": ""S3_PATH_IMAGE_1"";>	""image2"": ""S3_PATH_IMAGE_1""> }I'm using euclides number with value **0.6** as I saw in the documentation...But for example with an image; if the right person is compared; it returns an euclides number 0.35541702332548575; and a different person is compared and it returns 0.5926215395658295 (also we are testing with male and female image)Can I improve in someway the validations? I don't want to reduce euclides numbers; because in some cases the same person fails in the comparison with 0.62 or something similar; so if I reduce the number; it will fail multiple times.Thanks in advance; and thanks because the library is really useful!","[""A threshold of 0.6 is optimal yes; I won't change it. The quality of the image and the size of the face in the image after cropping also affects the accuracy. `Can I improve in someway the validations?`One thing that can be done is to match a query image to 2 or more images of reference person X and then take the mean of the distances.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data postprocess,Add data processing,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/543,Library Collision between face-api and posenet?,1,open,2020-02-03T05:14:58Z,2020-02-28T08:43:09Z,My posenet code works; but when I add `import * as faceapi from 'face-api.js';` I start to get errors like:> Error: Argument 'x' passed to 'pad' must be a Tensor or TensorLike; but got 'Tensor'> convertToTensor — modules.js:108247> pad_ — modules.js:95597> f2 — modules.js:96538> f2 — modules.js:96538> (anonymous function) — modules.js:75986> (anonymous function) — modules.js:10545> scopedRun — modules.js:10555> padAndResizeTo — modules.js:75984> (anonymous function) — modules.js:75561> step — modules.js:75414> (anonymous function) — modules.js:75389> initializePromise> Promise> (anonymous function) — modules.js:75385Is there a way to have both of these libraries?Thanks for any thoughts or help! I am using Meteor and this is running on my iphone. In a regular laptop browser it works fine. Very confused! Any advice to track this down?,['You have to make sure that you are using a posenet version; that utilizes the same tfjs-core version as face-api.js; or atleast make sure; that you are not ending up with 2 different versions of tfjs-core in your bundle.====='],Data & Model Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.2,A.5
https://github.com/justadudewhohacks/face-api.js/issues/546,Typescript compiler errors,3,open,2020-02-09T02:15:30Z,2021-03-13T20:23:52Z,"Am I dumb or am I missing something? I followed the documentation for usage with node but this produces many typescript compiler errors even tho the code functions.The following produces the errors below```const { Canvas; Image; ImageData } = canvasfaceapi.env.monkeyPatch({  Canvas;  Image;  ImageData})``````Type 'typeof Canvas' is not assignable to type '{ new (): HTMLCanvasElement; prototype: HTMLCanvasElement; }'.Types of property 'prototype' are incompatible.Type 'Canvas' is missing the following properties from type 'HTMLCanvasElement': toBlob; transferControlToOffscreen; addEventListener; removeEventListener; and 239 more.ts(2322)types.d.ts(6; 5): The expected type comes from property 'Canvas' which is declared here on type 'Partial<Environment>'```Also```const img = await canvas.loadImage('./temp/Thumbnail1.png')const fullFaceDescriptions = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors()``````Argument of type 'Image' is not assignable to parameter of type 'TNetInput'.Type 'Image' is not assignable to type 'HTMLImageElement'.ts(2345)```Why use typescript at all if I have to use // @ts-ignore everywhere to remove compiler errors? Or am I missing something? Thanks.""typescript"": ""3.7.5""","['The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs; so you might have to cast to ""any"" here and there:```faceapi.env.monkeyPatch({  Canvas;  Image;  ImageData} as any)````faceapi.detectAllFaces(img)`By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.====='; '> > > The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs; so you might have to cast to ""any"" here and there:> > ```> faceapi.env.monkeyPatch({>   Canvas;>   Image;>   ImageData> } as any)> ```> > `faceapi.detectAllFaces(img)`> > By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.Hi! I\'m trying to read/write images with the tjfs-node utility but I got an error when I run it: ![image](https://user-images.githubusercontent.com/56895143/111039780-36dc2780-8406-11eb-8a8d-07c665d7a065.png)The error occurs between lines 90-91:![image](https://user-images.githubusercontent.com/56895143/111039811-6e4ad400-8406-11eb-85d6-4bd63e584728.png)Amazing library; if you have any idea of that error and you can help me I would be grateful====='; '> > > > The thing is that utilizing node-canvas is more of ""a hack"" to make it work in nodejs; so you might have to cast to ""any"" here and there:> > ```> > faceapi.env.monkeyPatch({> >   Canvas;> >   Image;> >   ImageData> > } as any)> > ```> > > > > > `faceapi.detectAllFaces(img)`> > By the way tfjs-node provides the necessary utility to read and write images so you do not necessarily have to use node-canvas anymore.> > Hi! I\'m trying to read/write images with the tjfs-node utility but I got an error when I run it:> > ![image](https://user-images.githubusercontent.com/56895143/111039780-36dc2780-8406-11eb-8a8d-07c665d7a065.png)> > The error occurs between lines 90-91:> > ![image](https://user-images.githubusercontent.com/56895143/111039811-6e4ad400-8406-11eb-85d6-4bd63e584728.png)> > Amazing library; if you have any idea of that error and you can help me I would be gratefulSolved. I just update the face api version ^0.22.2=====']",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,change Third-party library version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/face-api.js/issues/55,How to speed it up after porting to node.js,14,closed,2018-07-16T11:35:58Z,2021-02-16T07:41:29Z,I ported it to node.js to compare faces; but the operation was slow when extracting 128-dimensional feature vectors. I replaced the latest @ tensorflow/ tfjs and used require('@tensorflow/tfjs-node'); but it is not accelerated; it is still very slow; each method call below will wait 5-20 seconds; but in the browser can be faster; what can I do to speed it up?            return tidy(function() {                var batchTensor = input.toBatchTensor(150; true);                var normalized = normalize(batchTensor);                var out = convDown(normalized; params.conv32_down);                out = maxPool(out; 3; 2; 'valid');                out = residual(out; params.conv32_1);                out = residual(out; params.conv32_2);                out = residual(out; params.conv32_3);                out = residualDown(out; params.conv64_down);                out = residual(out; params.conv64_1);                out = residual(out; params.conv64_2);                out = residual(out; params.conv64_3);                out = residualDown(out; params.conv128_down);                out = residual(out; params.conv128_1);                out = residual(out; params.conv128_2);                out = residualDown(out; params.conv256_down);                out = residual(out; params.conv256_1);                out = residual(out; params.conv256_2);                out = residualDown(out; params.conv256_down_out);                var globalAvg = out.mean([1; 2]);                var fullyConnected = matMul(globalAvg; params.fc);                return fullyConnected;            });,"['Unfortunately; I have not played around with tfjs-node yet. But as I said in another issue already; unless you can get tfjs-node to run on your native gpu (I am not sure what the current state of tfjs-node is in that regards); I would suspect it to be very slow.Edit.: If you are only interested in the resnet for computing face descriptors; you can give [face-recognition.js](https://github.com/justadudewhohacks/face-recognition.js); which exposes nodejs bindings to the dlib implementation of that net. Compiling with openblas; you should be able to get it down to realtime speeds; depending on your CPU.====='; 'I used the @tensorflow/tfjs-node to perform the multi-person detection of the posenet from the browser to the node.js; and the time to draw one frame was raised from about 2 seconds to 170-180 milliseconds.====='; 'So running posenet in nodejs was faster or slower compared to browser?====='; ""The posenet test speed node.js is 9-10 times higher than the browser. Just now; the faceapi euclideanDistance is speeded up by Node.js. The test results are as follows:    \tvar begin = new Date().getTime();    \tconst descriptor1 = await faceapi.computeFaceDescriptor(face1)//225px*225px    \tconst descriptor2 = await faceapi.computeFaceDescriptor(face2)//252px*252px    \tconst distance = faceapi.euclideanDistance(descriptor1; descriptor2)    \tconsole.log('distance='+distance);    \tif (distance < 0.6)    \t  console.log('match')    \telse    \t  console.log('no match')    \tvar end = new Date().getTime();    \tconsole.log(end - begin);browser：distance=0.46667880224688196match2628node.js:distance=0.4666788574536108match1136Although there is no speed improvement of posenet 9-10 times; but there is also a 2x improvement; only @tensorflow/tfjs-node is used for CPU acceleration.Also I modified your embedded @tensorflow/tfjs because I tried to upgrade the embedded @tensorflow/tfjs multiple times and the result could not be accelerated by @tensorflow/tfjs-node.=====""; 'Interesting; did you set your backend in the browser to CPU or WebGL? 2628 ms for 2 forward passes through the recognition net is very slow.====='; ""Thanks for your reminder; it is true that my browser does not have Open WebGL. What I want to say is that if you don't turn on acceleration in node.js; the result will be like this:node.js:distance=0.4666789108041152match432349I haven't used your face-recognition.js yet; test it tomorrow; thank you for sharing such a good project on github.=====""; 'I see; thanks for the hint. Maybe I will also try to tinker around with tfjs-node a bit soon.====='; 'heya ; trying to port it aswell ; land007 can you please help me a little bit?====='; ""I played around with face-api.js + node.js a bit now and actually it seems that you don't need to port anything. You simply have to declare HTMLImageElement; HTMLCanvasElement and HTMLVideoElement in your node environment; for example `global.HTMLImageElement = class {}`.I will look more into this and try to get better nodejs support into face-api.js soon; but right now you should already be able to use the package with nodejs.=====""; '@zobis2 If you prefer; you can use ""docker kill tfjs-html; docker rm tfjs-html; docker run -it --privileged --name tfjs-html -v ~/tfjs-html:/node -p 8080:80 land007/tfjs-html:latest"" to pull the tensorflow.js image of node.js; which provides tfjs; canvas for node.js ; ws; xmlhttprequest; node-fetch; mjpeg-server and other support; as justadudewhohacks said to simulate these objects in node.js; you can quickly port face-api.js to the node.js environment.====='; 'I am closing this now; since face-api.js now supports nodejs. Importing tfjs-node; you should be able to get pretty fast processing even on the CPU.====='; 'Ok; I will test your new program and hope it will satisfy the video processing.====='; ""I'm also experiencing slowness problem when using faceRecognition on api-faces. I use windows and my tests are taking around three seconds to identify the face.I already installed openblas on windows; but it looks like faces-api.js does not recognize it.Sorry; you are requested to use face-recognition.js and this repository is saying This package is pretty much obsolete. I recommend you to switch to face-api.js.Please help me; I'm loving this api.=====""; 'https://github.com/land007/face-kit=====']",Slow Execution,Poor Performance,Incorrect Code Logic,,,change import,Fix import confusion in program,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/57,Improving the face alignment,5,closed,2018-07-19T21:47:05Z,2018-07-24T17:14:59Z,Currently I get some faces misaligned; the issue seems to be more with the ROI of the detector. Not completely sure in the end if its a detection or alignment model issue![image](https://user-images.githubusercontent.com/590231/42971559-17537c32-8bad-11e8-92fd-6b7a0d1e2a44.png)With changing the detection ROI we can get good results![image](https://user-images.githubusercontent.com/590231/42971618-481461ce-8bad-11e8-84c0-a0fd1d77bfb2.png)I'm looking how to get overall improvement up; maybe running mtcnn and SSD side by side. I'd rather do a bit slower detection then having users fix the ROI manualy,"['The boxes drawn; are the bounding boxes returned from SSD? The 68 point landmark model we are using currently is not that accurate and could be improved; either by training an own model or looking for a pretrained model that grants better accuracy. At the moment the accuracy is sufficient to get a roughly aligned bounding box for the net computing the face descriptors.====='; ""First one is the SSD one the other is manual resizing to get good resultsSo I ended up messing around with bounding boxes. The alignment model can get good results but its very dependent on a good bounding box. I had much worse results with faces that are angled more than 75 degree; none of them had proper alignment set. It seems that alignment model likes the bounding box to start above eyes and a squared face. I'm currently testing making wider squared bounding boxes since ones generated by SSD are narrow.Going to test mtcnn later=====""; ""Anyways; I would rather have a model; that is agnostic to the face detector and which can predict the 68 face landmarks more precisely; independent of the bounding box.I played around with retraining the landmark model a while ago actually and have some intermediate models; which seem to perform better already; but I didn't investigate further yet or validated them on a larger data set.Does deep fakes require 68 face landmark points or simply a aligned bounding box by the way?=====""; ""Current deep fake projects use a 68 face landmark to determine the face cutout; you need your input face to be the closest as possible to the one you used in training to get the best quality output. Here is an example; training input; faces are determined by a function based on landmarks![image](https://user-images.githubusercontent.com/590231/43036700-9e051c68-8d06-11e8-8738-ea2890808dd4.png)Also in post processing  you do a landmark cutout of the new outer part of the face to remove the background. That's something I'm missing from my site until I can get an openCVjs compilation with drawing.Even the slight misalignment can be seen as flickers or jitters in a video; here's an example without manual fixing http://www.machine.tube/v/87 and this one doesn't even have hard align faces=====""; ""I managed to figure it out. Since a face rotated to 90 or more won't get aligned anyways the box orientations don't matter. Lowering the bounding box to 15%-12% of the height and making it a box shape improved the aligner accuracy by a lotGoing to close the issue since it seems this is the best as it gets=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,add data preprocess,Add data processing,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/59,Unstable MTCNN face landmark detection,10,open,2018-07-24T10:33:52Z,2019-08-05T07:30:44Z,Hi; we've been messing around with face-api and especially MTCNN face detection and 5 face landmarks detection. I think you guys did a great job on the lib overall 😄 However we were trying to get it work faster and more precisely. So far we've been able to get 7fps on a webcam stream.Below we recorded some gifs to visualize.![mtcnn-test-2](https://user-images.githubusercontent.com/37145307/43133009-bc9e9260-8f3d-11e8-8784-07cd2fdb0c00.gif)![mtcnn-test-1](https://user-images.githubusercontent.com/37145307/43133011-bcc64b66-8f3d-11e8-983b-e72577e84afe.gif)The questions I have regarding these are:1. What do you think could help us improve the amount of recognitions per second (the thing I called fps before) ?2. As you can see on the GIFs the recognitions are quite unstable even when the person is not moving; do you think there is some way that we could improve stability of the algorithm ?Hope I made it clear 😉,"['Hi;First of all I would try it out with better lightning. I found that the algorithm is very sensitive to lightning conditions; the shadows on your faces might make it less precise.Also could you tell me the parameters you are using; e.g. min face size and stage threshold values as well as; which browser you are using. Currenty the package works best in chrome.To get better fps; mainly increasing the min face size does help.====='; ""We are using  maxNumScales: 10;    scaleFactor: 0.709;    scoreThresholds: [0.6; 0.7; 0.7];    minFaceSize: 200And I was trying it with various min face sizes and 200 seemed to be optimal as higher values would require me to lean over the laptop so that my face is big enough. And we've been using chrome.=====""; ""We also discovered this problem with lighting; however; we can't assure the optimal lighting. We want to use the app in various conditions.  =====""; 'You could also try to increase the scaleFactor by a bit; to make the detector compute at more scales to avoid ""blind spots"". You can by the way use `mtcnn.forwardWithStats` to receive information about the scales / image sizes that have been used in stage 1; e.g:```const { results; stats } = mtcnn.forwardWithStatsconsole.log(stats.pyramid)console.log(stats.scales)```Might help you finetune parameters.May I ask what GPUs are built in your laptops? 7 fps seems to be pretty slow.====='; ""I tried to improve the results by modifying scale factor; but didn't get astonishing result; perhaps I didn't do it thoroughly  enough.I am working on a laptop with Intel HD Graphics 6000 1536  MB; it's not a graphics demon but it handles basic graphics display quite well; I even used it for video editing and I must say it worked pretty smoothly 😉 =====""; ""I see. I think there are currently still issues with tfjs + Intel that have to be resolved; not sure if this influence the results you get with MTCNN.The results of the above posted GIFs actually don't look too bad; there are simply some blind spots. =====""; '$(document).ready(function() {    run()  })        async function run() {    // load the models    await faceapi.loadTinyFaceDetectorModel(\'./models\')    await faceapi.loadMtcnnModel(\'./models\')    await faceapi.loadFaceRecognitionModel(\'./models\')      const videoEl = document.getElementById(\'inputVideo\')    console.log(""what is happening"");    navigator.getUserMedia(      { video: {} };      stream => videoEl.srcObject = stream;      err => console.error(err)    );    this.onPlay(videoEl);  }  async function onPlay(videoEl) {    // Promise.all([    //     faceapi.nets.ssdMobilenetv1.loadFromUri(\'./models\');    //        faceapi.nets.tinyFaceDetector.loadFromUri(\'./models\');    //       faceapi.nets.faceRecognitionNet.loadFromUri(\'./models\')    //      ])  const mtcnnForwardParams = {      maxNumScales: 10;    scaleFactor: 0.709;    scoreThresholds: [0.6; 0.7; 0.7];    minFaceSize: 200  };  // await faceapi.drawDetection(\'./models\')  //   await faceapi.drawLandmarks(\'./models\')//   const mtcnnResults = await faceapi.mtcnn(document.getElementById(\'inputVideo\'); mtcnnForwardParams)const overlay=document.getElementById(\'overlay\');const mtcnnResults = await faceapi.mtcnn(document.getElementById(\'inputVideo\'); mtcnnForwardParams) faceapi.drawDetection(overlay; mtcnnResults.map(res => res.faceDetection); { withScore: false }) faceapi.drawLandmarks(overlay; mtcnnResults.map(res => res.faceLandmarks); { lineWidth: 4; color: \'red\' })const options = new faceapi.MtcnnOptions(mtcnnParams) const input = document.getElementById(\'inputVideo\')const fullFaceDescriptions = await faceapi.detectAllFaces(input;options).withFaceLandmarks().withFaceDescriptors()// const alignedFaceBoxes = results.map(//     ({ faceLandmarks }) => faceLandmarks.align()//   )  //   const alignedFaceTensors = await extractFaceTensors(input; alignedFaceBoxes)  //   const descriptors = await Promise.all(alignedFaceTensors.map(//     faceTensor => faceapi.computeFaceDescriptor(faceTensor)//   ))  //   // free memory//   alignedFaceTensors.forEach(t => t.dispose())const labels = [\'face1\';\'face2\';\'praveen\']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `./${label}.jpg`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it\'s landmarks and face descriptor    const fullFaceDescription = await faceapi.detectAllFaces(img;new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }    const faceDescriptors = [fullFaceDescription.descriptor]    // console.log(label)     return new faceapi.LabeledFaceDescriptors(label; faceDescriptors)   }) )// 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; maxDescriptorDistance) //console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))/*const boxesWithText =*/ results.map((bestMatch; i) => {    const box = fullFaceDescriptions[i].detection.box    const text = bestMatch.toString()    const boxWithText = new faceapi.BoxWithText(box; text)    drawBox.draw(canvas)    return boxWithText  })  let myCanvas = document.getElementById(\'overlay\');  const context = myCanvas.getContext(\'2d\');  context.clearRect(0; 0; myCanvas.width; myCanvas.height);  faceapi.drawDetection(overlay; boxesWithText) //   faceapi.drawDetection(\'overlay\'; mtcnnResults.map(res => res.faceDetection); { withScore: false })//   faceapi.drawLandmarks(\'overlay\'; mtcnnResults.map(res => res.faceLandmarks); { lineWidth: 4; color: \'red\' })    setTimeout(() => this.onPlay(videoEl);150)  }  ====='; 'this is my code;when i run it; it showing ncaught (in promise) TypeError: faceapi.drawDetection is not a function    at onPlay (script2.js:40)onPlay @ script2.js:40async function (async)onPlay @ script2.js:39onplay @ (index):50this error. please help to solve me====='; '@thirukumars faceapi.drawDetections is not a part of faceapi anymore; please use the up to date examples.====='; 'thank you; =====']",Unstable,Poor Performance,Incorrect Code Logic,,,parameter modifier,Modify API Parameter usage,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3"",
    ""specific_type"": ""B.3.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.3.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/594,Tiny face detection not working in example or locally,3,open,2020-04-12T21:06:14Z,2020-07-14T10:10:31Z,I was struggling to get the tiny face detector model to work locally on the same images that are working in the [live demo](https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection) for face and landmark detection—it would always return an empty array for detections. So I followed the README to clone and run the examples locally and I noticed they aren't working there either.Is it possible the live-hosted version is behind the latest version in this repository and the latest version introduced a breaking change to the tiny face detector model? I noticed the demo web app looks significantly different with new examples as well.![cap](https://user-images.githubusercontent.com/555859/79079913-db7a5480-7cdf-11ea-9437-fa13578b2283.gif),"[""FWIW It looks like it was a change after 0.21.0 that broke this code...````javascriptawait faceapi.nets.tinyFaceDetector.loadFromUri('/static')const input = document.getElementById('myImg')const detection = await faceapi.detectSingleFace(  input;  new faceapi.TinyFaceDetectorOptions())console.log(detection) // undefined in version 0.22.0 and above ````=====""; 'Thank you @craigspaeth for the bug hunt! I was having the same issue and checking out version 0.21 did the trick.====='; 'Version 0.21 still undefined in ios 12.4 with ionic. Working Fine in Android.=====']",Reference Error,Crash,Incorrect Code Logic,,,change Third-party library version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/595,Around 40-50% match rate when comparing two single faces in two images,2,open,2020-04-13T14:53:32Z,2020-10-05T00:02:44Z,```exports.verifyProfile = functions.runWith({  memory: '1GB'}).https.onCall(async (data) => {  try {    console.log('====================== Cloud function called ==================')    console.log(data);    require('@tensorflow/tfjs-node');    const faceApi = require('face-api.js');    console.log('Loaded successfully face api');    const canvas = require('canvas');    const { Canvas; Image; ImageData } = canvas;    console.log('Loaded successfully canvas');    faceApi.env.monkeyPatch({ Canvas; Image; ImageData });    console.log('Loaded successfully modules');    let imgUrl = 'IMAGE_URL_1';    let imgUrl2 = 'IMAGE_URL_2';    console.log('Loading new models');    await faceApi.nets.faceRecognitionNet.loadFromDisk('./models');    await faceApi.nets.faceLandmark68Net.loadFromDisk('./models');    await faceApi.nets.ssdMobilenetv1.loadFromDisk('./models');    console.log('loaded new models');    const img = await canvas.loadImage(imgUrl);    console.log('Image converted to buffer');    const faceDescriptionOne = await faceApi      .detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();    const img2 = await canvas.loadImage(imgUrl2);    console.log('Image 2 converted to buffer');    const faceDescriptionTwo = await faceApi      .detectSingleFace(img2).withFaceLandmarks().withFaceDescriptor();    console.log('All detections called successfully');    // Similarity match    const similarityMatch = faceApi.euclideanDistance(faceDescriptionOne.descriptor; faceDescriptionTwo.descriptor);    console.log(similarityMatch);    let matched = false;    if (similarityMatch > 0.6) {      console.log('Images matched');      matched = true;    } else {      console.log('NO MATCH');    }    // Write to the database    const hashedValue = stringHash(data['uid']);    const documentReference = admin.firestore().collection('profiles').doc(hashedValue);    return documentReference.set({      'verified': matched    }; { merge: true });  }     catch (error) {    console.log('==========================Error Occured=============================');    console.log(error);    const hashedValue = stringHash(data['uid']);    const documentReference = admin.firestore().collection('profiles').doc(hashedValue);    return documentReference.set({      'verified': 'error'    }; { merge: true });  }});```I am using firebase cloud functions running with Node.js which basically have nothing related to the low similarity error between two images but please suggest if I am doing something wrong here or what can I do to improve the results because I have seen some tutorials where the match rate is around 60%  like https://www.youtube.com/watch?v=AZ4PdALMqx0&t=1156s&pbjreload=10I understand that I am passing just two images but they are of the same person. Is using euclidian distance correct or I have to use `faceapi.FaceMatcher`? Any code snippets would be really helpful.Also; what can be a safe percentage to say the two images belong to same user?  If the image quality is less the results are lower like around 30%; so what would be safe?These are two images in place of URL1 AND URL2 which gave 57% ![1](https://user-images.githubusercontent.com/34508540/79130214-34231d80-7dc4-11ea-983f-ab2f9cb3890e.jpg)![2](https://user-images.githubusercontent.com/34508540/79130385-83694e00-7dc4-11ea-8162-edfad51271ca.jpg)These are two images in place of URL1 AND URL2 which gave 45% even when they are of different gender. Am I missing something here lol?![1](https://user-images.githubusercontent.com/34508540/79133452-ad713f00-7dc9-11ea-80f0-8c95a0e9b33a.jpg)![2](https://user-images.githubusercontent.com/34508540/79133456-aea26c00-7dc9-11ea-8705-97916519f3dc.jpg),['A Euclidean distance of 0 would occur for a perfect match (e.g.; same picture).I believe the distance is based on the color values of the image. In the first example; although the two pictures are of the same person (as we know) they are of different lighting and the person has aged making for different hair color. In contrast; the second example has two faces with similar complexion skin and similar hair color. As a result; from a color value and distance point of view; the second example would reasonably have a smaller distance (0.45 as you say) than the first example (0.57).====='; 'I have same issue; I compare my pictures but the distance I get is between 0.5 to 0.6 ; which means 40 to 50 % match====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data preprocess,Add data processing,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/605,poor accuracy on macbook with master; but online demo looks good,7,open,2020-04-27T00:20:52Z,2020-05-20T08:07:38Z,"On a macbook running the examples from the master branch; I get poor accuracy on face detection (see screenshots attached). Is the online demo at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection running current code? Also would note that I get good accuracy on my linux computer working against the master branch. My linux laptop has an intel video card; and my macbook has both an nvidia and intel gpu.<img width=""877"" alt=""master_branch_example"" src=""https://user-images.githubusercontent.com/1912197/80323686-eb735780-87fa-11ea-841d-1d97657ceb59.png""><img width=""811"" alt=""online_example"" src=""https://user-images.githubusercontent.com/1912197/80323687-ec0bee00-87fa-11ea-8bd8-7386cbef2fc2.png"">","[""The big question I have is should I be able to achieve the results I see on the online demo myself? Things work great for me on that site; but I'm not able to reproduce those results locally; including the examples in the repo. Is the source code available for the server running at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/ ?=====""; ""> The big question I have is should I be able to achieve the results I see on the online demo myself? Things work great for me on that site; but I'm not able to reproduce those results locally; including the examples in the repo. Is the source code available for the server running at https://justadudewhohacks.github.io/face-api.js/face_and_landmark_detection/ ?Agree; I have the same issues and same question. Trying the example/demo site there is no indication of faces being detected at all. ![2020-04-27_17-55-27](https://user-images.githubusercontent.com/11222718/80347985-75c9c300-88b0-11ea-8039-f074adae3374.jpg)=====""; 'I did some more research on this. For me; things work well on version 0.21.0. They start to break on 0.22.0. Is that the same for you?====='; 'Can you guys guide; i need api; to compare 2 images from url and send the result back instead of using it from form.====='; 'Hi; have anyone found the issue. I also facing same with multiple records in DB; when the pic only contains clear face image and not any background/shape; the accurate results coming; else wrong matches. I am using 0.22.2 in Nodejs app====='; ""I've switched to using 0.21.0.  Far from an ideal solution. The commit to upgrade to 0.22.0 is quite large; and I haven't had the time to dig through there to look for the issue.I would be curious if the downgrade fixes things for someone else as well. If there are multiple people for whom the downgrade works; it would make me more inspired to dig into the codebase myself.=====""; 'Thanks for the suggestion; Kevin. I will try downgrading to that version and let you know.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change Third-party library version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/61,Conflict when using Tensorflow.js with face-api.js,5,closed,2018-07-27T14:58:11Z,2018-09-06T19:15:55Z,I'm working on a prototype where I need to use tensorflow.js (load the models); there seems to be a conflict between the namespace with your version (assume you're just using core) and the official. I need access to; at least; loadModel which is not available in your module. I've tried importing both but only the first one assigns itself to the globally.,"[""Yes face-api.js only uses tfjs-core. What do you mean by namespace collision? This module doesn't export tf on the global scope; so there shouldn't be any collisions.=====""; 'Importing tensor-flow like this creating error in namespace and causing problems. **(Noob in JS)**Can you have solution to import only once.```<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0""> </script>```====='; ""I see what the issue is; apparently tf can't register the WebGL backend twice.Anyways; I wouldn't include both scripts like that anyways. Just install both modules via npm and bundle them together.=====""; 'Now I see what the actual issue was here.In version 0.12.0 I bumped the tfjs-core version to latest (0.12.14); which includes the fix (last PR referenced in [this](https://github.com/tensorflow/tfjs/issues/109) issue); which allows tfjs to reuse a backend instead of registering it again; if it has already been registered from another script. If there are still issues; feel free to reopen.====='; '@joshnewnham  did you ever find a solution to this?I\'m running into a similar problem where:1) I\'d like to load and predict using a TF-JS model I trained -- using https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest2) I\'d like to detect faces -- using a different TensorFlow built-in to FaceApiDepending on the order that I use these lines; only the 1st called module with work:    <script src=""modules/tfjs.js""></script>    <script src=""./node_modules/face-api.js/dist/face-api.js""></script>=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Namespace Collision""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/615,Error: 'softmax' not yet implemented or not found in the registry. Did you forget to import the kernel?,2,closed,2020-05-05T00:42:23Z,2020-05-15T17:43:38Z,I have some error in withAgeAndGender() node.jsDoes anybody can help me?Error: 'softmax' not yet implemented or not found in the registry. Did you forget to import the kernel?,['I believe this is an error with the versions. You need to look at the package.json file and see that the @tensorflow/tfjs-node version is 1.7.0. Run the command: npm i @tensorflow/tfjs-node@1.7.0 This should fix the problem; because I had the same problem.====='; '> I believe this is an error with the versions. You need to look at the package.json file and see that the @tensorflow/tfjs-node version is 1.7.0.> > Run the command: npm i @tensorflow/tfjs-node@1.7.0> > This should fix the problem; because I had the same problem.Thank you!! It fixed this problem!====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/626,Can't run or compile nodejs examples,1,closed,2020-05-30T02:21:11Z,2020-05-31T04:41:11Z,"I'm having trouble running the nodejs examples.```node -v v12.16.3``````tsc -vVersion 3.9.3``````ts-node -vv8.10.2```After installing all dependencies; not ts-node nor tsc works. These are the results from running:**ts-node ageAndGenderRecognition.ts**```face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-backend-cpu/src/backend_cpu.ts:24const nonMaxSuppressionV3 = kernel_impls.nonMaxSuppressionV3;                                         ^TypeError: Cannot read property 'nonMaxSuppressionV3' of undefined    at Object.<anonymous> (/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-backend-cpu/src/backend_cpu.ts:24:42)    at Module._compile (internal/modules/cjs/loader.js:1133:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1153:10)    at Module.load (internal/modules/cjs/loader.js:977:32)    at Function.Module._load (internal/modules/cjs/loader.js:877:14)    at Module.require (internal/modules/cjs/loader.js:1019:19)    at require (internal/modules/cjs/helpers.js:77:18)    at Object.<anonymous> (/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs/dist/tf.node.js:25:22)    at Module._compile (internal/modules/cjs/loader.js:1133:30)    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1153:10)```Also when trying to compile the ts files by running: **tsc ageAndGenderRecognition.ts**```node_modules/@tensorflow/tfjs-backend-cpu/dist/backend_cpu.d.ts:64:111 - error TS2694: Namespace '""/Users/seba/Programming/node/face-api.js/examples/examples-nodejs/node_modules/@tensorflow/tfjs-core/dist/backends/backend_util""' has no exported member 'FusedBatchMatMulConfig'.64     fusedBatchMatMul({ a; b; transposeA; transposeB; bias; activation; preluActivationWeights }: backend_util.FusedBatchMatMulConfig): Tensor3D;                                                                                                                 ~~~~~~~~~~~~~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-cpu/dist/kernels/Max_impl.d.ts:17:20 - error TS2305: Module '""../../../tfjs-core/dist""' has no exported member 'TypedArray'.17 import { DataType; TypedArray } from '@tensorflow/tfjs-core';                      ~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-cpu/dist/kernels/Transpose_impl.d.ts:17:20 - error TS2305: Module '""../../../tfjs-core/dist""' has no exported member 'TypedArray'.17 import { DataType; TypedArray } from '@tensorflow/tfjs-core';                      ~~~~~~~~~~node_modules/@tensorflow/tfjs-backend-webgl/dist/backend_webgl.d.ts:26:10 - error TS2305: Module '""../../tfjs-core/dist""' has no exported member 'BackendValues'.26 import { BackendValues } from '@tensorflow/tfjs-core';......```It seems to me that it has something to do with the versions I'm running maybe?","[""OK I followed this instructions and ended up workinghttps://technodezi.co.za/Post/running-face-apijs-or-tfjs-node-on-a-raspberry-pi-and-nodejsSome of the steps can't be done thoug as they are for linux.=====""]",Build & Install Failure,Build & Initialization Failure,Incorrect Code Logic,,,change device,Changing device/browser,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Compilation Type Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.4
https://github.com/justadudewhohacks/face-api.js/issues/629,face-api conflicting with @tensorflow-models/coco-ssd,1,closed,2020-06-03T14:12:39Z,2020-06-03T15:21:44Z,"As you can tell by the title; I'm trying to make use of both api's on the same page.If I try using them on different pages; they work perfectly; but once together; I have issues.I'm importing the scripts like this```<script src=""node_modules/@tensorflow/tfjs/dist/tf.js""> </script><script src=""node_modules/@tensorflow-models/coco-ssd/dist/coco-ssd.js""> </script><script src=""node_modules/face-api.js/dist/face-api.js""></script>```They obviously are conflicting because depending on the order in which I include them; one will work and the other won't.With the order shown above; object recognition works thanks to coco-ssd but face recognition doesn't. I get this error:```Uncaught (in promise) TypeError: t.batchNormalization is not a function    at tf-core.esm.js:17    at engine.js:606    at engine.js:425    at Engine.scopedRun (engine.js:436)    at Engine.tidy (engine.js:423)    at kernelFunc (engine.js:606)    at engine.js:619    at Engine.scopedRun (engine.js:436)    at Engine.runKernelFunc (engine.js:616)    at Uu (tf-core.esm.js:17)```When I invert them; I get this:```Uncaught (in promise) TypeError: backend.batchNorm is not a function    at forward (batchnorm.js:95)    at tf-core.esm.js:17    at tf-core.esm.js:17    at t.scopedRun (tf-core.esm.js:17)    at t.tidy (tf-core.esm.js:17)    at f (tf-core.esm.js:17)    at tf-core.esm.js:17    at t.scopedRun (tf-core.esm.js:17)    at t.runKernelFunc (tf-core.esm.js:17)    at batchNorm_ (batchnorm.js:110)```I tried importing them in a main file and then bundling them (using webpack) like mentioned on this issue #61 but it doesn't help.Any help would be appreciated; thanks",['Newer version of Tensorflow conflicts with face-api.js.Using @tensorflow/tfjs@1.7.4 works====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/633,solved: face-api is not compatible with tfjs 2.x or tfjs 3.x,21,open,2020-06-08T14:38:39Z,2021-12-12T13:09:22Z,Now that TFJS 2.0 has been released; are there any plans to update face-api models to be compatible with it?Currently it fails due to batchNormalization being obsoleted in tfjs 2.0.I'm using tfjs with multiple models on the same image to classify/detect all different types of objects (not just faces) and since concurrently loading multiple different versions of tfjs is not possible thus I cannot upgrade entire project because of a face-api dependency on tfjs 1.x.,"['True that!```Unhandled Rejection at: TypeError: backend.batchNormalization is not a function    at engine_1.ENGINE.runKernelFunc.x (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm.js:280:27)    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3229:55    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at kernelFunc (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3229:29)    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3240:27    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.runKernelFunc (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3238:14)    at batchNorm_ (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm.js:279:31)    at Object.batchNorm (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:46:29)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:9:18    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at depthwiseConvLayer (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:7:15)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:38:19    at Array.forEach (<anonymous>)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:35:24    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at Object.mobileNetV1 (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/mobileNetV1.js:17:15)    at /var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:29:42    at /var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3075:22    at Engine.scopedRun (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3085:23)    at Engine.tidy (/var/task/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3074:21)    at Object.tidy (/var/task/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/dist/globals.js:176:28)    at SsdMobilenetv1.forwardInput (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:26:19)    at SsdMobilenetv1.<anonymous> (/var/task/node_modules/face-api.js/build/commonjs/ssdMobilenetv1/SsdMobilenetv1.js:58:35)    at step (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:139:27)    at Object.next (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:120:57)    at fulfilled (/var/task/node_modules/face-api.js/node_modules/tslib/tslib.js:110:62) {}```====='; 'Is there any solution/work-around for this?====='; 'same here; unable to upgrade to 2.0.1 version====='; 'I have same issues like @igorescobar====='; 'Check to see if you have a `node_modules/` inside the node_modules/face-api.js folder -- this can have the older version of the tensor library and then they will conflict; because the face-api will initially load this version which then conflicts with the version in the main node_modules folder...   Once I nuked this extra node_modules folder; everything worked fine w/ v2.x====='; ""> Check to see if you have a `node_modules/` inside the node_modules/face-api.js folder -- this can have the older version of the tensor library and then they will conflict; because the face-api will initially load this version which then conflicts with the version in the main node_modules folder... Once I nuked this extra node_modules folder; everything worked fine w/ v2.xCan you elaborate on that? IMO; loading any specific version of TFJS is not a problem; but if TFJS 2.0 is loaded Face-API will not work since TFJS 2.0 obsoleted some functions - primarily batchNormalization(). And that is a dependency inside models; so models need to be recompiled; it's not just a library code issue.=====""; ""Sometimes NPM is stupid; so what happens is you have this:[YourProject]-> [node_modules]---> [...Your other package.json modules...]---> [@tensorflow/ ] (All v2.x)---> [faceapis.js]---------> [node_modules]-----------------[@tensorflow/] (v1.x)So when faceapi loads tensorflow it loads the v1.0 library from its node_modules; which then when it attempts to load tsflow-node; it gets the v2.0 from the  root @tensorflow 2.0 version.   Of course v1 doesn't work with v2 and you get that nice cryptic error message.   If you NUKE/DELETE the [node_modules] under the faceapi.js folder; then when faceapi.js goes to load tensorflow it will load the 2.0 version from the root node_modules; and that 2.0 will load the tsflow-node which is also 2.0 and everything will work.  =====""; 'As I said; the issue is not forcing load of TFJS 2.0 - even if you do that; the models used by face-api.js rely on batchNormalization() function that was deprecated and removed in TFJS 2.0 - see: <https://github.com/tensorflow/tfjs/pull/3238> and <https://github.com/tensorflow/tfjs/issues/3232>Anyhow; I\'ve tried your method (and few others)...1. Nuke private tfjs within face-api.js so it tries to use tfjs 2.0 from node_modules/@tensorflow```rm -rf node_modules/face-api.js/node_modules/@tensorflow```As expected; it doesn\'t load TFJS at all; it just fails on first usage of any method since instance is undefined.```""Cannot read property \'fetch\' of undefined""```2. Update face-api.js dependencies in-place```cd node_modules/face-api.js../.bin/ncu -u  @tensorflow/tfjs-core        1.7.0  →     2.3.0  tslib                      ^1.11.1  →    ^2.0.1  @tensorflow/tfjs-node        1.7.0  →     2.3.0  @types/jasmine              ^3.5.9  →   ^3.5.12  @types/node                ^13.9.2  →  ^14.0.27  jasmine                     ^3.5.0  →    ^3.6.1  jasmine-core                ^3.5.0  →    ^3.6.0  karma                       ^4.4.1  →    ^5.1.1  karma-jasmine               ^3.1.1  →    ^4.0.1  karma-typescript            ^5.0.1  →    ^5.1.0  rollup                      ^2.1.0  →   ^2.26.2  rollup-plugin-typescript2  ^0.26.0  →   ^0.27.2  ts-node                     ^8.7.0  →   ^8.10.2  typescript                  ^3.8.3  →    ^3.9.7npm i```Result is the same as in first case.3. Download and rebuild face-api.js from sources; not npm package```git clone https://github.com/justadudewhohacks/face-api.jscd face-api.js/npm i ```This will do a recompile which may fail due to missing system dependencies; in which case install them manually. E.g.:```sudo apt install libjpeg-dev libgif-dev```Then force package update and do a full rebuild```npm i npm-check-updatesnode_modules/.bin/ncu -u  Upgrading face-api.js/package.json  @tensorflow/tfjs-core        1.7.0  →     2.3.0  tslib                      ^1.11.1  →    ^2.0.1  @tensorflow/tfjs-node        1.7.0  →     2.3.0  @types/jasmine              ^3.5.9  →   ^3.5.12  @types/node                ^13.9.2  →  ^14.0.27  jasmine                     ^3.5.0  →    ^3.6.1  jasmine-core                ^3.5.0  →    ^3.6.0  karma                       ^4.4.1  →    ^5.1.1  karma-jasmine               ^3.1.1  →    ^4.0.1  karma-typescript            ^5.0.1  →    ^5.1.0  rollup                      ^2.1.0  →   ^2.26.2  rollup-plugin-typescript2  ^0.26.0  →   ^0.27.2  ts-node                     ^8.7.0  →   ^8.10.2  typescript                  ^3.8.3  →    ^3.9.7npm inpm run build```Fails; so it\'s not just models that need work```  Error: /home/vlado/dev/face-api.js/src/dom/NetInput.ts(131;72): semantic error TS2344: Type \'Rank.R4\' does not satisfy the constraint \'Tensor<Rank>\'.```====='; 'Hi;I had initially followed the sites instructions and did this for my project:`npm i face-api.js canvas @tensorflow/tfjs-node`Here is what I\'m running:node: v12.16.3tsc: V3.8.3All I can report is what worked for me.   I got the exact same message as `Unhandled Rejection at: TypeError: backend.batchNormalization is not a function` the very first time I ran the examples; and so that is how I found this issue.  :grinning:The `npm i` Unfortunately;  creates the nested node_modules because the face-api.js `package.json` file asks for `@tensorflow/tfjs-core"": ""1.7.0""` however the npm i @tensorflow/tfjs-node part get whatever is the latest TF (which in my case was 2.1).My projects node_modules contains these versions of libraries- @tensorflow/tfjs@2.1.0- @tensorflow/tfjs-backend-cpu@2.1.0- @tensorflow/tfjs-backend-webgl@2.1.0- @tensorflow/tfjs-converter@2.1.0- @tensorflow/tfjs-core@2.1.0- @tensorflow/tfjs-data@2.1.0- @tensorflow/tfjs-layers@2.1.0- @tensorflow/tfjs-node@2.1.0- face-api.js@0.22.2- tslib@1.13.0Originally under the `node_modules/face-api.js` their was another `node_modules` with the v1.7 tensorflow module; this is what I nuked.If I take any of the node-js examples that are on this repo; build them with TS; and then run them with node they ALL work.![image](https://user-images.githubusercontent.com/850871/90366068-80a98e80-e02c-11ea-9662-26f1f8f08cf1.png)(This is my out directory)If I add code to any of the demos to output the TensorFlow version; this is what I get:![image](https://user-images.githubusercontent.com/850871/90366363-d3834600-e02c-11ea-9eaf-412b73b71b4c.png)The code I\'m using to get the version is I just add this to the very top of any of the examples to output TensorFlow version...```const tf = require(\'@tensorflow/tfjs\');console.log(`Initializing TensorFlow/JS version ${tf.version_core}`);```As you can see from the output; IT is using TensorFlow 2.1; Node is loading TF 2.1 with face-api; and face-api is using it.    Now to triple check; If I manually edit the node_modules/face-api.js/build/commonjs/index.js and add the exact same console.log right under where the face-api loads tensorflow like so:![image](https://user-images.githubusercontent.com/850871/90368687-14308e80-e030-11ea-9d3e-f85502e4d25d.png)I also get the exact same output:![image](https://user-images.githubusercontent.com/850871/90368782-3e824c00-e030-11ea-866f-e0a62d5b69cc.png)Once from the added code in the example; and once from when face-api loads tensorflow.====='; ""Ok; I dug a bit deeper...  face-api.js includes multiple models:  - **tinyFaceDetector**: This one WORKS with TFJS 2.0+. This is the model that is used in most of examples anyhow.- **ssdMobilenetv1**: This one depends on obsolete function batchNorm() so no chance to make it work with TFJS 2.0. I used this model since in my testing it seemed to have better precision than tiny (although a touch slower). - **mtcnn**:  Mostly obsolete; but still present- **tinyYolov2**: Code is there; but weights are missing since forever; so I guess this is more of an abandonware.  So if you're ok using tinyFaceDetector; TFJS 2.0+ works just fine.  You could nuke local node_modules like NathanaelA stated or you can upgrade face-api.js packages.json and make it use newer tfjs or anything else.  =====""; ""FYI; I've forked face-api.js; updated it for tfjs 2.0; cleaned up obsolete code and modernized build process.  And it's about 2.5x smaller than original.See <https://www.npmjs.com/package/@vladmandic/face-api> if you want to use it instead of original face-api.js.  There are no changes except what's noted; so original docs still apply.  =====""; '@vladmandic - You should drop a PR so that this can get fixed...  :D ====='; ""@NathanaelA I'd be happy to write a PR for minor code changes and update dependencies; but it's really up to the author to clean up models - as it is; there is code for 4 different models and only 1 is working - removing that much code is not something that should be done in a PR.Also; build process itself is quite old and targeting old standards which did cause some issues for me when loading face-api concurrently with tfjs itself in a different model.That's why I did a fork; not a PR.=====""; 'I find the accuracy of the `tinyFaceDetector` model not good for my use-case compared to the `ssdMobilenetv1` model. For those who want still to use the `ssdMobilenetv1` model; the workaround is downgrading your `tfjs` to version `1.7.0`.I hope this gets fixed soon.cc. @justadudewhohacks ====='; ""@focux I agree that ssdMobilenetv1 is generally a bit better than tinyFaceDetector and I wish author retrains model to be compatible with tfjs@2.0+ although not sure how much can be done since mobilenet v1 is a really old model. Ideally; it should be trained with either ssd & mobilenet v2 (that would be simplest as it's very close to existing model) or a newer & better alternative.i've noticed some work has been done in a separate branch <https://github.com/justadudewhohacks/face-api.js/tree/face_detection_save>; but there are no weights and it hasn't been updated in 3 months.btw; if you really need ssdMobilenetv1; use latest tfjs@1.7.4; not 1.7.0 as there are more than few bugfixes and v1.7.4 is final version in 1.x branch.i'll reopen this issue so it can be monitored...=====""; 'One more note; ssd_mobilenetv1 model actually comes from a diferent project; <https://github.com/yeephycho/tensorflow-face-detection>.  And that project has actually been updated for tfjs@2.0+ compatibility.However; latests weighs released there <https://drive.google.com/open?id=0B5ttP5kO_loUdWZWZVVrN2VmWFk> are in TF Frozen format.  They do convert nicely to TF Graph model (and optionally quantize for reduced size); ```shell> summarize_graph \\  --in_graph=""./frozen_inference_graph_face.pb"" \\  --print_structure=falseFound 4 possible outputs: (name=detection_boxes; op=Identity) (name=detection_scores; op=Identity) (name=detection_classes; op=Identity) (name=num_detections; op=Identity)> tensorflowjs_converter \\  --input_format tf_frozen_model \\  --output_format tfjs_graph_model \\  --skip_op_check \\  --strip_debug_ops=True \\  --quantize_uint16 \\  --weight_shard_size_bytes 4194304 \\  --output_node_names detection_boxes;detection_scores;num_detections \\  ./frozen_inference_graph_face.pb \\  ./converted/```but...face-api.js model-weights list to iterate and load weights; not full model.json.If someone wants to dig deeper; I\'m sure this can be used; I just don\'t have any more time for this.As it is; tinyFaceDetector works as-is and ssdMobilenetv1 can probably be made to work. Other models are obsolete.====='; 'Done!I\'ve just pushed updated package to <https://www.npmjs.com/package/@vladmandic/face-api>Both tinyFaceDetector and ssdMobileNetv1 work:```jsModel: {name: ""FaceAPI SSD/MobileNet v1""; modelPath: ""models/faceapi/""; exec: ""ssd""; score: 0.3; topK: 1; size: 416 };Result: {  age: 21.586017608642578  expressions: FaceExpressions {...}  gender: ""female""  genderProbability: 0.960713230073452  alignedRect: FaceDetection {...}  descriptor: Float32Array(128) [...]  detection: FaceDetection {...}  landmarks: FaceLandmarks68 {...}}{ name: \'FaceAPI TinyYoloDetector\'; modelPath: \'models/faceapi/\'; exec: \'yolo\'; score: 0.3; topK: 1; size: 416 };Result: {  age: 24.492658615112305  expressions: FaceExpressions {...}  gender: ""female""  genderProbability: 0.964848417788744  alignedRect: FaceDetection {...}  descriptor: Float32Array(128) [...]  detection: FaceDetection {...}  landmarks: FaceLandmarks68 {...}}```====='; ""Wow! That's awesome; great work @vladmandic; I will definitely use your package. Thank you.=====""; ""Awesome work @vladmandic thank you for sharing! Can I buy you a beer/do you have a tip jar anywhere? :D(and @justadudewhohacks too if you're still around 😃 )=====""; 'no tip jar; just having fun - enjoy!====='; 'Thank youuuuuuuuu very much 💕💕💕💕💕=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/644,Unhandled Rejection (TypeError): t.batchNormalization is not a function,4,open,2020-06-19T17:52:42Z,2020-12-07T17:31:12Z,"I am getting the following error when trying to recreate the example code for bbt. Unhandled Rejection (TypeError): t.batchNormalization is not a functionMy packages are:   ""dependencies"": {    ""@tensorflow-models/coco-ssd"": ""^0.1.1"";    ""@tensorflow/tfjs"": ""^2.0.1"";    ""@tensorflow/tfjs-core"": ""^2.0.1"";    ""@tensorflow/tfjs-data"": ""^2.0.1"";    ""@testing-library/jest-dom"": ""^4.2.4"";    ""@testing-library/react"": ""^9.5.0"";    ""@testing-library/user-event"": ""^7.2.1"";    ""bootstrap"": ""^4.5.0"";    ""capture-video-frame"": ""^0.1.3"";    ""face-api.js"": ""^0.22.2"";    ""react"": ""^16.13.1"";    ""react-bootstrap"": ""^1.0.1"";    ""react-bootstrap-range-slider"": ""^1.0.0"";    ""react-dom"": ""^16.13.1"";    ""react-player"": ""^2.3.0"";    ""react-redux"": ""^7.2.0"";    ""react-scripts"": ""3.4.1"";    ""react-thunk"": ""^1.0.0"";    ""react-webcam"": ""^5.1.0"";    ""redux"": ""^4.0.5"";    ""tslib"": ""^2.0.0"";    ""typescript"": ""^3.9.5""  };","['face-api is not yet compatible with tfjs 2.0; you have to use older 1.7.x branch.====='; '> face-api is not yet compatible with tfjs 2.0; you have to use older 1.7.x branch.It worked for me with these versions.```""dependencies"": {    ""@tensorflow/tfjs-node"": ""^1.7.0"";    ""aws-sdk"": ""2.785.0"";    ""canvas"": ""^2.6.1"";    ""face-api.js"": ""^0.22.2"";    ""gm"": ""1.23.1"";    ""sharp"": ""0.26.3"";    ""smartcrop-sharp"": ""2.0.3""  }```====='; 'since this thread is still alive; I might mention that I did a full port of `face-api` to tfjs 2.x (including updated models as well as entire toolchain and typescript and ecma standards).  if you want; check out <https://github.com/vladmandic/face-api>====='; '> since this thread is still alive; I might mention that I did a full port of `face-api` to tfjs 2.x (including updated models as well as entire toolchain and typescript and ecma standards).> > if you want; check out https://github.com/vladmandic/face-apiAwesome!! thanks=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/647,Error when detecting face,3,open,2020-06-22T16:00:26Z,2021-08-23T14:54:38Z,"Occassionally; we receive an error when attempting to detect a face in a still image; we receive the following error:Uncaught (in promise) Error: Box.constructor - expected box to be IBoundingBox | IRect; instead have {""left"":211.82223243423;""top"":188.34445656565;""right"":299.35465223232;""bottom"":null}at BoundingBox.Boxat new BoundingBoxat minBboxat FaceLandmarks68.FaceLandmarks.alignMinBbox...It appears that it's missing a value for the bottom of a bounding box; but I'm not sure why. Any suggestions?","['I was having this error when running the html served in a node environment. Running it from traditional apache works fine.====='; 'Hi @ScottDellinger Did you ever manage to find a solution for this issue?====='; ""> Hi @ScottDellinger Did you ever manage to find a solution for this issue?No... I stopped using this library because it's been abandoned and we found the models it was trained with did not perform well on Asian or Middle Eastern faces. There is a more active fork of it; though.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/66,Uncaught (in promise) Error: Error in slice4D while running MtcnnFaceRecognitionWebcam,6,closed,2018-08-08T08:19:48Z,2018-08-13T12:14:30Z,Hello;At first; the library and your tutorial are amazing. I am currently implementing a real time face recognition from webcam with nodeJS and face-api and there was an error comes up. It happens few minutes after the streaming appeared or sometimes right after the webcam stream appeared. While it's working; it detect and recognize face quite good; but it would stop detecting and recognizing after some times (the webcam stream still works).Below is the error message taken from console window from Chrome 68 after it stop detecting or recognizing.> Uncaught (in promise) Error: Error in slice4D: begin[1] + size[1] (519) would overflow input.shape[1] (480)                                                                                                                    tf-core.esm.js:17 >     at assert (tf-core.esm.js:17)>     at assertParamsValid (tf-core.esm.js:17)>     at e.slice (tf-core.esm.js:17)>     at tf-core.esm.js:17>     at e.tidy (tf-core.esm.js:17)>     at n.value (tf-core.esm.js:17)>     at extractFaceTensors.ts:43>     at Array.map (<anonymous>)>     at extractFaceTensors.ts:42>     at e.tidy (tf-core.esm.js:17)Then I tried to run your examples directly and same thing keeps happening.> (519)number here changed each time error appeared; it seemed to depends on the duration of time it can run.Whenever I reload the page; it repeats the error after some another duration of time.Am I doing wrong somewhere? (noob in JS)Thank you.,"['Hmm maybe a rounding issue at the border of the image; not sure. I will try to reproduce it. Did you adjust the parameters or does the error also occur when running the example as is?====='; 'I only changed minFaceSize from 200 to 150. And the error also occurs with the example as well. UPDATE:As I did some playing around with the minFaceSize param (increasing and decreasing) and tried to move my face closer and further to the webcam; it seemed that when my face is getting too close; the error would occur immediately. If not; it still occurs after a minute.Is there anyway that we can get rid of this?Because each time the error appear; I have to reload the page to start detecting/recognizing again.====='; ""@justadudewhohacks confirmed; I'm getting this overflow error too. It happens when I run MTCNN with `minFaceSize: 200` and when I put my face too close so that the whole face isn't seen. To replicate; run forwardpass/allfacesmtcnn with webcam videoel as input and go very close to far a couple of times.![image](https://user-images.githubusercontent.com/3739702/43987694-8148bee8-9d57-11e8-9095-938039eacb05.png) @TuanHungVU1202 you could wrap your forward pass with `try...catch` block and just log the errors. Worked for my purposes. :) =====""; ""@beatobongco Thanks. I made it with your temp solution. But I still hope we can fix this permanently because it's too laggy for my purpose.=====""; 'I could reproduce the issue; it was due to the bounding boxes reaching over the image borders. This should now be fixed.====='; '@justadudewhohacks  perfect; just tested and everything works like a charm.=====']",Data & Model Error,Crash,Incorrect Code Logic,,,add data postprocess,Add data processing,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/68,Support Loading models from external sources,3,closed,2018-08-10T18:22:28Z,2018-08-13T12:10:40Z,The Current Implementation does not support loading of models from sources other than the current domain. This could be a useful feature as it could allow for loading of models from cdns or directly from a cloud object storage instance. I have implemented the required change [here](https://github.com/bmaguireibm/face-api.js); let me know if this is desired feature and I can create a PR.P.S.Thanks for the excellent library.,"['Hi;Sure that would be awesome; if you could create a PR for that. Thank you!====='; ""Great. I've created the PR [here](https://github.com/justadudewhohacks/face-api.js/pull/69).=====""; 'Your changes are published in version 0.10.1.=====']",Reference Error,Crash,Incorrect Code Logic,,,add support for datatype,Add unsupported operator,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Same-origin Policy Restriction""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/688,posenet and face-api in the same project,4,open,2020-09-01T05:36:04Z,2021-04-19T12:23:29Z,Hi; I'm not exactly sure what's happening; but I hope someone can save me some sleuthing.I'm using both TFJS Posenet and Face-Api on different sections of my project (not at the same time).I'm finding that I'm getting errors running Face-Api:```Uncaught (in promise) TypeError: t is not a function    at engine.ts:583    at engine.ts:423```But this only occurs after I import Posenet elsewhere in my code (without even running it):`import * as posenet from '@tensorflow-models/posenet';`Is there anything I can do to mitigate this error? Ideally; I need to go back and forth between Face-Api and Posenet in the same browser based single page application.thanks,"[""Both PoseNet and Face-API import `tfjs-core` and it registers some globals; so you end up with multiple version conflict.One of the reasons why I've forked face-api; updated it and modified it so you can import `tfjs` independently and use it across different models.This is not a self-promotion; just hope it helps.  <https://www.npmjs.com/package/@vladmandic/face-api>=====""; ""@vladmandic Thanks; I'm sure it would help if I still needed it; but I just switched over to FaceMesh which I don't think was available when I started this project.=====""; '@vladmandic Have the same problem; it helps a lot!====='; ""i use facemesh (heavily modified) in my own project that i've started after `face-api` as well: <https://github.com/vladmandic/human>=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/707,examples-nodejs not compiling,2,open,2020-10-13T11:03:46Z,2020-10-20T20:48:12Z,# Steps taken:1. Clone the repo2. cd face-api.js/examples/examples-nodejs3. npm i4. tsc faceDetection.ts# Expected Result:ts should compile into js; and then I can move forward and run faceDetection.js; as the README states.# Actual Result:Error:```../../node_modules/@types/webgl2/index.d.ts:582:13 - error TS2403: Subsequent variable declarations must have the same type.  Variable 'WebGL2RenderingContext' must be of type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 556 more ...; readonly WAIT_FAILED: number; }'; but here has type '{ new (): WebGL2RenderingContext; prototype: WebGL2RenderingContext; readonly ACTIVE_ATTRIBUTES: number; readonly ACTIVE_TEXTURE: number; ... 557 more ...; readonly MAX_CLIENT_WAIT_TIMEOUT_WEBGL: number; }'.582 declare var WebGL2RenderingContext: {                ~~~~~~~~~~~~~~~~~~~~~~  ../../../../../../../../usr/local/lib/node_modules/typescript/lib/lib.dom.d.ts:16354:13    16354 declare var WebGL2RenderingContext: {                      ~~~~~~~~~~~~~~~~~~~~~~    'WebGL2RenderingContext' was also declared here.Found 1 error.```# Environment:**node version**: v10.22.0**os**: Mac OS Catalina (10.15.7),"['Fix is to make sure that you:```npm i --save @types/webgl2```Pre-requisites should definitely go into the documentation.====='; ""also; there are quite a few type mismatched is you happen to to upgrade your typescript/tslib as newer version have more in-depth type checking. so if you want to use original face-api; you're stuck with old tslib 1.1. or pick a newer port.=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,add dependency,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Type Checking Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/713,node-fetch - npm install vunerability,4,open,2020-10-19T13:50:52Z,2020-10-20T20:44:22Z,After installing face-api.js npm returns low vulnerability that can't be fixed. Output of npm audit:```Low           Denial of Service                               Package       node-fetch                                      Patched in    >=2.6.1 <3.0.0-beta.1|| >= 3.0.0-beta.9         Dependency of face-api.js                                     Path          face-api.js > @tensorflow/tfjs-core > node-fetch```Running npm audit fix fails.,"['Have you tried `npm audit fix --force`?====='; '> Have you tried `npm audit fix --force`?No; mostly because I saw node-fetch causing issues for other people with other packages when I was looking into it.====='; 'This issue is related to packages depending on `node-fetch`; to not update their modules and end up with vulnerabilities. But it makes sense because they have to read the new updates of `node-fetch` in order to maintain a stable package.I believe that using `--force` flag will get rid of the vulnerability but will create other issues with your `face-api` package.You have two options:- Wait `face-api` contributors to fix it- Fix it and open a PR (win-win situation)====='; ""I've updated all packages and switched to TFJS 2.0 branch in my fork if you want to try   <https://github.com/vladmandic/face-api>=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change dependency version,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Dependency Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/719,multiply' not yet implemented or not found in the registry,8,open,2020-10-31T09:09:37Z,2020-12-02T22:19:53Z,"Following are the versions of node-packages to reproduce the error""face-api.js"": ""^0.22.2""""@tensorflow/tfjs-backend-cpu"": ""^2.7.0""""@tensorflow/tfjs-backend-webgl"": ""^2.7.0""""@tensorflow/tfjs-converter"": ""^2.7.0""""@tensorflow/tfjs-core"": ""^2.7.0""I tried loading the tinyFaceDetectorModel; here is the code snippet`await faceapi.nets.tinyFaceDetector.loadFromUri('/models');``const input = document.querySelector(""img"");``const result = await faceapi.detectSingleFace(input;new faceapi.TinyFaceDetectorOptions());`It leads to this error:**backend.js?8b87:504 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen**","['This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issue====='; ""> This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issueI downgraded to TF 2.6.0 and now it is giving some new error here it is:`Uncaught (in promise) TypeError: Cannot read property 'length' of undefined`   ` at parseTupleParam (conv_util.js?b818:239)`    `at tupleValuesAreOne (conv_util.js?b818:386)`    `at Module.eitherStridesOrDilationsAreOne (conv_util.js?b818:390)`    `at Object.maxPool [as kernelFunc] (MaxPool.js?898d:27)`    `at kernelFunc (engine.js?6ae2:425)`    `at eval (engine.js?6ae2:477)`    `at Engine.scopedRun (engine.js?6ae2:318)`    `at Engine.runKernelFunc (engine.js?6ae2:475)`    `at cl (tf-core.esm.js?953f:17)`    `at maxPool_ (tf-core.esm.js?953f:17)`@framp I think that downgrading didn't work=====""; 'This works ok on my machine: https://gist.github.com/framp/ac13e105f0d1a4209b12047781321a7a====='; 'I am getting the excat same issue @archeelp Did you find any solution ?====='; ""for what it's worth; I also get this error when using TFJS 2.7.0```backend.js:665 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.js:665)    at MathBackendWebGL.multiply (backend.js:176)    at Kt.runKernelFunc.a (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)```But trying other 2.x releases (I tried TFJS 2.2.0; 2.3.0; 2.4.0; 2.5.0; 2.6.0) all threw this error```face-api.min.js:1 Uncaught (in promise) TypeError: t.batchNormalization is not a function    at Kt.runKernelFunc.x (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)    at Engine.runKernelFunc (engine.js:636)    at yu (face-api.min.js:1)```  **_Update_**: I've just seen that [this was already reported/known](https://github.com/justadudewhohacks/face-api.js/issues/644#issuecomment-647049700)=====""; '@dalelane I used TFJS 1.9 and it solved that error.====='; '@shailjaa downgrading to TFJS 1.* will be a workaround. But if you have other packages which have dependency requirements of TFJS version >= V2.0 it will again cause a series of errors.====='; ""it's well known that `face-api` is not compatible with tfjs 2.0+ - you just run into few issues (such as `batchNormalization` being deprecated so models need updating; not just JS code; etc.)  i've spend a lot of time to rewrite parts; if you wish you can check out <https://github.com/vladmandic/face-api>=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/719,multiply' not yet implemented or not found in the registry,8,open,2020-10-31T09:09:37Z,2020-12-02T22:19:53Z,"Following are the versions of node-packages to reproduce the error""face-api.js"": ""^0.22.2""""@tensorflow/tfjs-backend-cpu"": ""^2.7.0""""@tensorflow/tfjs-backend-webgl"": ""^2.7.0""""@tensorflow/tfjs-converter"": ""^2.7.0""""@tensorflow/tfjs-core"": ""^2.7.0""I tried loading the tinyFaceDetectorModel; here is the code snippet`await faceapi.nets.tinyFaceDetector.loadFromUri('/models');``const input = document.querySelector(""img"");``const result = await faceapi.detectSingleFace(input;new faceapi.TinyFaceDetectorOptions());`It leads to this error:**backend.js?8b87:504 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen**","['This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issue====='; ""> This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issueI downgraded to TF 2.6.0 and now it is giving some new error here it is:`Uncaught (in promise) TypeError: Cannot read property 'length' of undefined`   ` at parseTupleParam (conv_util.js?b818:239)`    `at tupleValuesAreOne (conv_util.js?b818:386)`    `at Module.eitherStridesOrDilationsAreOne (conv_util.js?b818:390)`    `at Object.maxPool [as kernelFunc] (MaxPool.js?898d:27)`    `at kernelFunc (engine.js?6ae2:425)`    `at eval (engine.js?6ae2:477)`    `at Engine.scopedRun (engine.js?6ae2:318)`    `at Engine.runKernelFunc (engine.js?6ae2:475)`    `at cl (tf-core.esm.js?953f:17)`    `at maxPool_ (tf-core.esm.js?953f:17)`@framp I think that downgrading didn't work=====""; 'This works ok on my machine: https://gist.github.com/framp/ac13e105f0d1a4209b12047781321a7a====='; 'I am getting the excat same issue @archeelp Did you find any solution ?====='; ""for what it's worth; I also get this error when using TFJS 2.7.0```backend.js:665 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.js:665)    at MathBackendWebGL.multiply (backend.js:176)    at Kt.runKernelFunc.a (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)```But trying other 2.x releases (I tried TFJS 2.2.0; 2.3.0; 2.4.0; 2.5.0; 2.6.0) all threw this error```face-api.min.js:1 Uncaught (in promise) TypeError: t.batchNormalization is not a function    at Kt.runKernelFunc.x (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)    at Engine.runKernelFunc (engine.js:636)    at yu (face-api.min.js:1)```  **_Update_**: I've just seen that [this was already reported/known](https://github.com/justadudewhohacks/face-api.js/issues/644#issuecomment-647049700)=====""; '@dalelane I used TFJS 1.9 and it solved that error.====='; '@shailjaa downgrading to TFJS 1.* will be a workaround. But if you have other packages which have dependency requirements of TFJS version >= V2.0 it will again cause a series of errors.====='; ""it's well known that `face-api` is not compatible with tfjs 2.0+ - you just run into few issues (such as `batchNormalization` being deprecated so models need updating; not just JS code; etc.)  i've spend a lot of time to rewrite parts; if you wish you can check out <https://github.com/vladmandic/face-api>=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/719,multiply' not yet implemented or not found in the registry,8,open,2020-10-31T09:09:37Z,2020-12-02T22:19:53Z,"Following are the versions of node-packages to reproduce the error""face-api.js"": ""^0.22.2""""@tensorflow/tfjs-backend-cpu"": ""^2.7.0""""@tensorflow/tfjs-backend-webgl"": ""^2.7.0""""@tensorflow/tfjs-converter"": ""^2.7.0""""@tensorflow/tfjs-core"": ""^2.7.0""I tried loading the tinyFaceDetectorModel; here is the code snippet`await faceapi.nets.tinyFaceDetector.loadFromUri('/models');``const input = document.querySelector(""img"");``const result = await faceapi.detectSingleFace(input;new faceapi.TinyFaceDetectorOptions());`It leads to this error:**backend.js?8b87:504 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen**","['This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issue====='; ""> This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issueI downgraded to TF 2.6.0 and now it is giving some new error here it is:`Uncaught (in promise) TypeError: Cannot read property 'length' of undefined`   ` at parseTupleParam (conv_util.js?b818:239)`    `at tupleValuesAreOne (conv_util.js?b818:386)`    `at Module.eitherStridesOrDilationsAreOne (conv_util.js?b818:390)`    `at Object.maxPool [as kernelFunc] (MaxPool.js?898d:27)`    `at kernelFunc (engine.js?6ae2:425)`    `at eval (engine.js?6ae2:477)`    `at Engine.scopedRun (engine.js?6ae2:318)`    `at Engine.runKernelFunc (engine.js?6ae2:475)`    `at cl (tf-core.esm.js?953f:17)`    `at maxPool_ (tf-core.esm.js?953f:17)`@framp I think that downgrading didn't work=====""; 'This works ok on my machine: https://gist.github.com/framp/ac13e105f0d1a4209b12047781321a7a====='; 'I am getting the excat same issue @archeelp Did you find any solution ?====='; ""for what it's worth; I also get this error when using TFJS 2.7.0```backend.js:665 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.js:665)    at MathBackendWebGL.multiply (backend.js:176)    at Kt.runKernelFunc.a (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)```But trying other 2.x releases (I tried TFJS 2.2.0; 2.3.0; 2.4.0; 2.5.0; 2.6.0) all threw this error```face-api.min.js:1 Uncaught (in promise) TypeError: t.batchNormalization is not a function    at Kt.runKernelFunc.x (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)    at Engine.runKernelFunc (engine.js:636)    at yu (face-api.min.js:1)```  **_Update_**: I've just seen that [this was already reported/known](https://github.com/justadudewhohacks/face-api.js/issues/644#issuecomment-647049700)=====""; '@dalelane I used TFJS 1.9 and it solved that error.====='; '@shailjaa downgrading to TFJS 1.* will be a workaround. But if you have other packages which have dependency requirements of TFJS version >= V2.0 it will again cause a series of errors.====='; ""it's well known that `face-api` is not compatible with tfjs 2.0+ - you just run into few issues (such as `batchNormalization` being deprecated so models need updating; not just JS code; etc.)  i've spend a lot of time to rewrite parts; if you wish you can check out <https://github.com/vladmandic/face-api>=====""]",Regression,Poor Performance,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/719,multiply' not yet implemented or not found in the registry,8,open,2020-10-31T09:09:37Z,2020-12-02T22:19:53Z,"Following are the versions of node-packages to reproduce the error""face-api.js"": ""^0.22.2""""@tensorflow/tfjs-backend-cpu"": ""^2.7.0""""@tensorflow/tfjs-backend-webgl"": ""^2.7.0""""@tensorflow/tfjs-converter"": ""^2.7.0""""@tensorflow/tfjs-core"": ""^2.7.0""I tried loading the tinyFaceDetectorModel; here is the code snippet`await faceapi.nets.tinyFaceDetector.loadFromUri('/models');``const input = document.querySelector(""img"");``const result = await faceapi.detectSingleFace(input;new faceapi.TinyFaceDetectorOptions());`It leads to this error:**backend.js?8b87:504 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen**","['This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issue====='; ""> This looks like a regression in tensorflowjs; downgrading to 2.6.0 fixes the issueI downgraded to TF 2.6.0 and now it is giving some new error here it is:`Uncaught (in promise) TypeError: Cannot read property 'length' of undefined`   ` at parseTupleParam (conv_util.js?b818:239)`    `at tupleValuesAreOne (conv_util.js?b818:386)`    `at Module.eitherStridesOrDilationsAreOne (conv_util.js?b818:390)`    `at Object.maxPool [as kernelFunc] (MaxPool.js?898d:27)`    `at kernelFunc (engine.js?6ae2:425)`    `at eval (engine.js?6ae2:477)`    `at Engine.scopedRun (engine.js?6ae2:318)`    `at Engine.runKernelFunc (engine.js?6ae2:475)`    `at cl (tf-core.esm.js?953f:17)`    `at maxPool_ (tf-core.esm.js?953f:17)`@framp I think that downgrading didn't work=====""; 'This works ok on my machine: https://gist.github.com/framp/ac13e105f0d1a4209b12047781321a7a====='; 'I am getting the excat same issue @archeelp Did you find any solution ?====='; ""for what it's worth; I also get this error when using TFJS 2.7.0```backend.js:665 Uncaught (in promise) Error: 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen    at notYetImplemented (backend.js:665)    at MathBackendWebGL.multiply (backend.js:176)    at Kt.runKernelFunc.a (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)```But trying other 2.x releases (I tried TFJS 2.2.0; 2.3.0; 2.4.0; 2.5.0; 2.6.0) all threw this error```face-api.min.js:1 Uncaught (in promise) TypeError: t.batchNormalization is not a function    at Kt.runKernelFunc.x (face-api.min.js:1)    at engine.js:625    at engine.js:433    at Engine.scopedRun (engine.js:444)    at Engine.tidy (engine.js:431)    at kernelFunc (engine.js:625)    at engine.js:639    at Engine.scopedRun (engine.js:444)    at Engine.runKernelFunc (engine.js:636)    at yu (face-api.min.js:1)```  **_Update_**: I've just seen that [this was already reported/known](https://github.com/justadudewhohacks/face-api.js/issues/644#issuecomment-647049700)=====""; '@dalelane I used TFJS 1.9 and it solved that error.====='; '@shailjaa downgrading to TFJS 1.* will be a workaround. But if you have other packages which have dependency requirements of TFJS version >= V2.0 it will again cause a series of errors.====='; ""it's well known that `face-api` is not compatible with tfjs 2.0+ - you just run into few issues (such as `batchNormalization` being deprecated so models need updating; not just JS code; etc.)  i've spend a lot of time to rewrite parts; if you wish you can check out <https://github.com/vladmandic/face-api>=====""]",Regression,Poor Performance,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Loading,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",B.3.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/723,Unhandled Rejection (Error): 'multiply' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen,1,closed,2020-11-04T05:16:30Z,2020-11-04T10:53:28Z,I keep getting this error while using TinyFaceDetector for face detection. I am using another tfjs model as well so it is conflicting.![Screenshot (6)](https://user-images.githubusercontent.com/62165403/98072218-8a080180-1e8b-11eb-9491-5cb358075390.png),['> I keep getting this error while using TinyFaceDetector for face detection.> I am using another tfjs model as well so it is conflicting.> > ![Screenshot (6)](https://user-images.githubusercontent.com/62165403/98072218-8a080180-1e8b-11eb-9491-5cb358075390.png)This issue is alredy raised here [https://github.com/justadudewhohacks/face-api.js/issues/719#issue-733674343](url)@shailjaa please raise your concerns there and mark this as duplicate.====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/743,MTCNN Face Detector demo won't work,1,open,2021-01-04T07:58:40Z,2021-01-05T21:59:20Z,The link address: https://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740The moment the MTCNN model is loaded; the console prints: 'mtcnn is deprecated and will be removed soon'; May I ask why it is overdue?The following two lines of code run error: faceapi.drawDetection('overlay'; mtcnnResults.map(res => res.faceDetection); { withScore: false })faceapi.drawLandmarks('overlay'; mtcnnResults.map(res => res.faceLandmarks); { lineWidth: 4; color: 'red' })Can examples in the repository provide clear examples of MTCNN?Looking forward to reply~,"[""`mtcnn` and `tinyYolov2` models are quite old and not included in latest `face-api.js` for a while now (it's possible to make them work; but why bother?).  models that are included are `mobileNetv1` and `tinyFaceDetector`.  =====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model,Changing model,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Deprecated Model Usage""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/768,TypeError: forwardFunc_1 is not a function,6,closed,2021-03-13T04:12:29Z,2021-03-25T11:32:48Z,"I am new to face-api.js/Tensorflow.js so I might be overlooking something very obvious.I installed face-api.js and Tensorflow using: `npm i face-api.js canvas @tensorflow/tfjs-node`Everything worked fine until I added `require('@tensorflow/tfjs-node');` at the top of the code.Here's the full code:```require('@tensorflow/tfjs-node');const { loadImage;Canvas; Image; ImageData;createCanvas } = require('canvas')const fs= require('fs');const faceapi = require('face-api.js');faceapi.env.monkeyPatch({ Canvas; Image; ImageData;createCanvas });Promise.all([   faceapi.nets.ssdMobilenetv1.loadFromDisk('models');   faceapi.nets.faceRecognitionNet.loadFromDisk('models');   faceapi.nets.faceLandmark68Net.loadFromDisk('models')   ]).then(async () => {    data={};    const image1= await loadImage(""test.png"");    const result = await faceapi.detectSingleFace(image1).withFaceLandmarks().withFaceDescriptor();    data[""test.png""]={};    let left=result.landmarks.getLeftEye();    data[""test.png""].left=left;    let right=result.landmarks.getRightEye();    data[""test.png""].right=right;    console.log(data);});```The code would have worked perfectly fine; had `require('@tensorflow/tfjs-node');` not been added. The error I get when I run the full code is:```2021-03-13 09:27:30.500937: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].(node:1428) UnhandledPromiseRejectionWarning: TypeError: forwardFunc_1 is not a function    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3179:55    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3002:22    at Engine.scopedRun (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3012:23)    at Engine.tidy (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3001:21)    at kernelFunc (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3179:29)    at FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3200:27    at Engine.scopedRun (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3012:23)    at Engine.runKernelFunc (FolderName\node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:3196:14)    at mul_ (FolderName\node_modules\face-api.js\node_modules\@tensorflow\tfjs-core\dist\ops\binary_ops.js:327:28)    at Object.mul (FolderName\node_modules\face-api.js\node_modules\@tensorflow\tfjs-core\dist\ops\operation.js:46:29)(Use 'node --trace-warnings ...' to show where the warning was created)(node:1428) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection; use the CLI flag '--unhandled-rejections=strict' (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:1428) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```Here's the list of packages installed: [list.txt](https://github.com/justadudewhohacks/face-api.js/files/6134141/list.txt). I have uninstalled/re-installed the node modules multiple times. Tried using the previous versions of ""@tensorflow/tfjs-node"".I am using Node.js v14.16.0","['this version of `face-api.js` is not compatible with tfjs 2.0+ or 3.0+; only obsolete 1.x.  why it worked before you added `tfjs-node`? because `face-api.js` actually includes bundled version of `tfjs-core` 1.x.  once you added `tfjs-node`; it overrode global `tf` namespace; but its a much newer version and not compatible.  now; you can either install obsolete `tfjs-node` 1.x or use newer port of `face-api.js` (changes to make it compatible are pretty significant):  <https://github.com/vladmandic/face-api>  <https://www.npmjs.com/package/@vladmandic/face-api>====='; ""> this version of `face-api.js` is not compatible with tfjs 2.0+ or 3.0+; only obsolete 1.x.> why it worked before you added `tfjs-node`? because `face-api.js` actually includes bundled version of `tfjs-core` 1.x.> once you added `tfjs-node`; it overrode global `tf` namespace; but its a much newer version and not compatible.> > now; you can either install obsolete `tfjs-node` 1.x or use newer port of `face-api.js` (changes to make it compatible are pretty significant):> https://github.com/vladmandic/face-api> https://www.npmjs.com/package/@vladmandic/face-apiSo I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.=====""; ""@TheRealTechWiz>So I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.That was the active version of TFJS at the time this package was last updated - if you check; you'll see it hasn't been updated in about a year. That's exactly why I've started a new fork - to modify `face-api` for TFJS 2.x and 3.x compatibility.And why are you marking the response with thumbs down? Just because you don't like what is said? I've offered a solution.=====""; ""> @TheRealTechWiz> > > So I have to use really old 1.x.x version of tfjs with face-api.js 0.22.2 . If so why isn't this mentioned in the official face-api node docs. I am sure there must be some other way instead of using the really old tfjs version.> > That was the active version of TFJS at the time this package was last updated - if you check; you'll see it hasn't been updated in about a year. That's exactly why I've started a new fork - to modify `face-api` for TFJS 2.x and 3.x compatibility.> > And why are you marking the response with thumbs down? Just because you don't like what is said? I've offered a solution.Thanks for the fast reply. I think the docs need to be updated. is this the fork you are referring to https://github.com/vladmandic/face-api it doesn't say forked from this repo. So i am little confused.=====""; ""> Thanks for the fast reply. I think the docs need to be updated.Yes; but if the author is no longer maintaining the package; how can they be updated?> is this the fork you are referring to https://github.com/vladmandic/face-api it doesn't say forked from this repo. So i am little confused.It says that explicitly in the readme:## NoteThis is updated **face-api.js** with latest available TensorFlow/JS as the original is not compatible with **tfjs 2.0+**.  Forked from [face-api.js](https://github.com/justadudewhohacks/face-api.js) version **0.22.2** which was released on March 22nd; 2020  Currently based on **`TensorFlow/JS` 3.3.0**  *Why?* I needed Face-API that does not cause version conflict with newer versions of TensorFlow  And since original Face-API was open-source; I've released this version as well  Changes ended up being too large for a simple pull request  and it ended up being a full-fledged version on its own  Plus many features were added since original inception  <br>## DifferencesCompared to [face-api.js](https://github.com/justadudewhohacks/face-api.js) version **0.22.2**:- Compatible with `TensorFlow/JS 2.0+ & 3.0+`- Compatible with `WebGL`; `CPU` and `WASM` TFJS Browser backends- Compatible with both `tfjs-node` and `tfjs-node-gpu` TFJS NodeJS backends- Updated all type castings for TypeScript type checking to `TypeScript 4.2`- Switched bundling from `UMD` to `ESM` + `CommonJS` with fallback to `IIFE`  Resulting code is optimized per-platform instead of being universal  Fully tree shakable when imported as an `ESM` module  Browser bundle process uses `ESBuild` instead of `Rollup`- Typescript build process now targets `ES2018` and instead of dual ES5/ES6  Resulting code is clean ES2018 JavaScript without polyfills- Removed old tests; docs; examples- Removed old package dependencies (`karma`; `jasmine`; `babel`; etc.)- Updated all package dependencies- Updated TensorFlow/JS dependencies since backends were removed from `@tensorflow/tfjs-core`- Updated mobileNetv1 model due to `batchNorm()` dependency- Added `version` class that returns JSON object with version of FaceAPI as well as linked TFJS- Added test/dev built-in HTTP & HTTPS Web server- Removed `mtcnn` and `tinyYolov2` models as they were non-functional in latest public version of `Face-API`  Which means valid models are **tinyFaceDetector** and **mobileNetv1**  *If there is a demand; I can re-implement them back.*- Added `face angle` calculations that returns `roll`; `yaw` and `pitch`- Added `typdoc` automatic API specification generation during build- Added `changelog` automatic generation during build<br>## Credits- Original project: [Face-API](https://github.com/justadudewhohacks/face-api.js)- Original model weighs: [Face-API](https://github.com/justadudewhohacks/face-api.js-models)- ML API Documentation: [Tensorflow/JS](https://js.tensorflow.org/api/latest/)=====""; ""Thanks. i'll switch from this to your fork. =====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/77,faceapi.locateFaces returns empty array,2,open,2018-08-27T02:46:52Z,2018-08-27T06:44:13Z,Following #75; I have succeeded in downloading the models.But for some reason when on my android device using faceapi.locateFaces returns an empty array.Though it works flawlessly on my browser.Any pointers on what I should check ?,"['Interestingly though; I uploaded my code to my server at https://julianalimin.com/models/app/And for some reason it works on some phones only.Crashed my iPhone SE and does not work on my Asus Zenfone 3.Are there minimum requirements to work on phone?====='; ""Looking at your app; it seems you are using the ssd mobilenetv1 model for face detection. This one doesn't run on my android as well; probably because it requires too much resources.For mobile face detection I would recommend using the new tiny yolo v2 face detector; which uses separable convs instead of regular convolutions. This model is much lighter and way faster on mobile devices than ssd mobilenetv1 and MTCNN.=====""]",Incorrect Functionality,Incorrect Functionality,Improper Model Attribute,,,change model,Changing model,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Empty Array Result""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",D,C.2
https://github.com/justadudewhohacks/face-api.js/issues/775,# dynamically changing faceMatcher for bulk photo loading,3,open,2021-04-05T20:36:40Z,2021-04-06T15:44:32Z,I'm creating a web(react) app which **bulk loads a few tens of pictures; and then identifies and recognizes all faces together** - and sort them to albums by label\person name.because of the obvious performance problem with bulk face recognition; I'm struggling to find the most efficient way for that -* should I just compare the current image descriptor to all other (saved) labeled descriptors?* or maybe is it possible to dynamically add descriptors to a big faceMatcher which will just work on every incoming picture and classify it? will it even be worth the trouble(performance-wise)? * any other offers will be appreciated.,"[""if you want to be as efficient as possible; run math as fast as possible; don't look how to wrap it in nice functions that do the same thing but with additional overhead.```jsfunction distance(embedding1; embedding2; order = 2) {  if (!embedding1 || !embedding2) return 1;  if (embedding1?.length === 0 || embedding2?.length === 0) return 1;  if (embedding1?.length !== embedding2?.length) return 1;  // general minkowski distance; euclidean distance is limited case (default) where order is 2  return embedding1    .map((val; i) => (Math.abs(embedding1[i] - embedding2[i]) ** order)) // distance squared    .reduce((sum; now) => (sum + now); 0) // sum all distances    ** (1 / order); // get root of}```=====""; ""@vladmandic true. the next level is probably to understand how the faceMatcher work from the inside; I'm just not there yet; do to my lack of experiance\\knowledge. from the little I can understand of your function; is the faceMatcher probability calculated by the average of euclidean distances between 68 face points and their counterparts from theother descriptor? could you please reference me to a some specific topics; so I can understand it better?thank you for your time!=====""; ""face descriptor or otherwise called face embedding is basically a tensor feature vector  calculated on a cropped image of a face with weights trained on several versions of faces at various angles; etc.  it's just a numerical array that describes entire face image; not related to detected face points at all  and the length of the array is how precise it is - here it's a relatively short vector- <https://www.tensorflow.org/hub/common_signatures/images>- <https://medium.com/looka-engineering/how-to-visualize-feature-vectors-with-sprites-and-tensorflows-tensorboard-3950ca1fb2c7>=====""]",Slow Execution,Poor Performance,Unknown,,,change API,change API,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.1.1,E
https://github.com/justadudewhohacks/face-api.js/issues/780,i want to load the mtcnn json and shard file from cdn but its not working!! Need help,3,open,2021-04-15T06:34:28Z,2021-04-16T11:23:57Z,,['`mtcnn` model has been removed a looong time ago; any reference to it is pretty much invalid  but if you really want it; you can download it from `https://github.com/justadudewhohacks/face-api.js-models`  (its not on any *cdn*)  ====='; 'No actually What i wanted is; we are dockerising eveything and we dont want any static files there so we moved all the static files to cdn but my question is can put mtcnn model on our own cdn and serve the model or not to js?====='; 'of course - no issues.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Model File Access Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.1] Data/Model Inaccessibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/782,TypeError: forwardFunc is not a function （React),3,closed,2021-04-19T07:20:05Z,2021-04-20T07:37:10Z,"I want to use faceapi to detect faces in webcam; but there is a problem with title 'TypeError: forwardFunc is not a function'.![截屏2021-04-19 下午3 17 20](https://user-images.githubusercontent.com/33031512/115196325-5c093780-a122-11eb-9523-3f7efb3af23f.png)here is the code```javascriptimport React; { ReactElement; useState; useEffect; useRef } from 'react'import { Row; Col; Button } from 'antd';import * as tf from '@tensorflow/tfjs';import './style.css';import * as faceapi from 'face-api.js';const TestFace = (): ReactElement => {  // state  const [displaySize; setDisplaySize] = useState<{ width: number ; height: number;}>({ width: 1280 ; height: 1270})  // refs  const videoEl = useRef<HTMLVideoElement>(null);  const canvasEl = useRef<HTMLCanvasElement>(null);  const captureEl = useRef<HTMLCanvasElement>(null);  // effects  useEffect(() => {    async function load() {      await loadFaceApiModels();      loadCam();    };    load();  }; []);  const loadFaceApiModels = async () => {    console.log('loading faceapi');    await faceapi.loadTinyFaceDetectorModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights');    await faceapi.loadSsdMobilenetv1Model('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights');    await faceapi.loadFaceLandmarkModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights');    await faceapi.loadFaceRecognitionModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights');    console.log('loaded faceapi');      }  const loadCam = () => {    navigator.mediaDevices.getUserMedia({ video: { width: displaySize.width; height: displaySize.height}})      .then(stream => {        setIsLoading(false);        if (videoEl.current) {          videoEl.current.srcObject = stream;        }      })      .catch(err => {        console.error(err);      })  }  const onPlay = async (): Promise<void> => {    if (videoEl.current) {      const detection = await faceapi.detectSingleFace(videoEl.current); // where error cames      console.log(detection);    }  }  return (    <>      <Row        align=""middle""        justify=""center""      >        <Col>          <div>              <>                <video ref={videoEl} width=""1280"" height=""760"" onPlay={onPlay} muted playsInline autoPlay />                <canvas id=""overlay"" ref={canvasEl}></canvas>                <canvas id=""capture"" width=""224"" height=""224"" ref={captureEl}></canvas>              </>          </div>        </Col>      </Row>    </>  )}export default TestFace;```","[""you are importing both tfjs and face-api:```jsimport * as tf from '@tensorflow/tfjs';import * as faceapi from 'face-api.js';```face-api has embedded version of tfjs 1.x and latest tfjs is 3.x and they are anything but compatible.  if you don't explicitly need tf; don't import it.but if you do; use newer port of face-api that is compatible with newer tfjs: <https://github.com/vladmandic/face-api>=====""; '@vladmandic  Thanks for reply! I need `tf.js` to use my own model to predict the result;  I tried `https://github.com/vladmandic/face-api` ; but in the scene to detect a face wearing a mask ;the `@vladmandic/face-api` have a worse performance than `face-api.js`. ====='; ""ah; i saw that one - those are related.let's figure that one out.=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/783,faceapi  dose not support on Angular,1,closed,2021-04-19T20:01:42Z,2021-06-20T08:10:21Z,I run hand pose API from TF and want to run faceapi.js together; I occurred error on faceapi. please support angular and support TF core .js 3 and do not embed TF core on face api js,['part of exact reasons why this port exists: <https://github.com/vladmandic/face-api>====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.3] Cross-platform App Framework Incompatibility""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/786,Face api is incompatible with new versions of tensorflow.js 2.x; 3.x,9,open,2021-04-26T10:23:10Z,2021-04-27T14:03:22Z,face-api.js requires 1.x version and it fails with newer versions like 2.x; 3.x and gives type errors because of the compatibility issues,"[""`faceapi` has not been updated for a long time; so that's main reason why this port exists:  <https://github.com/vladmandic/face-api>=====""; ""Recent posts on TensorFlow's blog make me think maybe there is another model that could be loaded up and used for fast results and more detailed info. https://blog.tensorflow.org/2020/11/iris-landmark-tracking-in-browser-with-MediaPipe-and-TensorFlowJS.html=====""; ""Oh; also; this is happening https://www.chromestatus.com/feature/5678216012365824And this https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.htmlAnd most importantly; this happened https://blog.tensorflow.org/2020/04/tensorflow-lite-core-ml-delegate-faster-inference-iphones-ipads.htmlwhich will eventually manifest as this https://webmachinelearning.github.io/webnn/and then we shouldn't have to worry about performance anymore. =====""; ""@jeffreytgilbert > Recent posts on TensorFlow's blog make me think maybe there is another model that could be loaded up and used for fast results and more detailed info.I've been working with those models (and few others) for a while now; check out: <https://github.com/vladmandic/human>> Oh; also; this is happening https://www.chromestatus.com/feature/5678216012365824Is great if all you need is to be able to authenticate user - that's what it's for; nothing else.> And this https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.htmlNot sure how that relates? It's a great high-level API for learning; but any SOTA object detection model doesn't rely on OD API.> And most importantly; this happened https://blog.tensorflow.org/2020/04/tensorflow-lite-core-ml-delegate-faster-inference-iphones-ipads.htmlA way to natively use Apple's proprietary NPU - great performance speedup on Apple's new silicone.> which will eventually manifest as this https://webmachinelearning.github.io/webnn/One more GPU/TPU/NPU accelerated backend? Just because it's committee driven; doesn't make it better than existing ones. E.g; `tfjs-node-gpu` + `cuda` already do that job if you have nVidia GPU.I'm far more interested in development and adoption of WebGPU (to be used as a next-gen WebGL).=====""; ""WebGPU is great; but that shouldn't be the only way Apple and everyone access those NPUs. My focus has been on the performance improvements using efficient non-blocking APIs. Face/object detection in the browser APIs is not feature rich; however with web standards; they start small and get better from there. That's why I'm following that spec and WebNN/OpenVino/NNAPI/etc. They aren't drop in replacements for anything here; but might be used for simple tasks in a non-blocking way. You might be thinking; but currently this code isn't blocking! Fair; but 2 years ago when i was using it i hit a GPU bottleneck that blocked main ui rendering because the integrated graphics chip couldn't do both and it's stuck with me ever since. =====""; 'Agree; `WebGPU` use-case is in-browser; not catch-all.But regarding WebNN; etc. - I don\'t see how a committee driven backend designed for high-level usage can accomplish much other than increase entry-level adoption of ML (great for quick learning). All really good models require a lot more low-level ops and that is not the goal of WebNN. E.g.; I don\'t care about pre-packaged ""person detection""; I want to be able to use best available model.Also; most money in ML research is coming from corp sponsors that develop their own backends (e.g.; FB has PyTorch; Google has Tensorflow; Alibaba has MNN; Apple has CoreML; Tenecent has NCNN; nVidia has TensorRT; etc.) - and I need to be able to run models developed in their native frameworks.Notice that Apache foundation and OpenAI are pushing for MXNet adoption; but it really hasn\'t picked up so far.Now; if I can easily convert those models to ONNX (as ""universal"" model format) and then run them using WebNN - great. But I see that as always being couple of steps behind cutting edge as new ops need to be implemented downstream. ====='; ""You're more versed in this than I am. My last touchpoint was 2019. That's not how I understood WebNN's approach as documented in their workflow hierarchy graphics and spec documentation; but I'm fully ok with being way off in my interpretation. Here; it's mentioned what the roles are:https://webmachinelearning.github.io/webnn/#programming-modelSo my understanding is that WebNN would be a common interface for loading; queuing and executing work against low level system APIs available to the native code in the browser and that the common interface would not look to be a replacement for WebGPU; WebGL; etc; but instead a standard way to access them and get async results back from work that may block a thread. There would still be a javascript framework (Tensorflow for example) which decided which models to load and which of the available APIs to choose; similar to how Canvas allows you to choose a 2D or 3D context for instance; but the dev/framework is still responsible for implementing compatible instructions. I'm not deep enough into it to have any insight into what you mentioned regarding apache and ONNX; but that sounds like it might be cool. I wonder if Kronos or OASIS standards workgroups will pick something like that up so it ends up working like Vulcan does in replacing OpenGL with a universal API for common low level GPU tasks. I'll have to read up on it more when I'm done with my backlog of other reading materials. ;)=====""; ""PS; happy you're involved and posting on these tickets! Can't make magic happen without magicians! =====""; 'I definitely don\'t see a scenario where TFJS loads a model and then passes it to WebNN for execution - why would devs maintain TFJS then? Just as a loader?TFJS would not run on top of WebNN - both of them are frameworks that implement low level functions using some CPU/GPU/TPU/NPU acceleration. And in case of browser; how do you get to underlying hardware? Via WebGL or (in the future) WebGPU (which then internally use something like OpenGL or Vulcan or D3D - that is up to browser implementation).But...WebNN cannot have a parser and loader for every model format that comes from different framework; they have to pick one. I hope they pick ONNX as that is supposed to be ""universal"" format.I can see how someone uses Tensorflow to develop a model; then converts it to ONNX and runs it using WebNN - so role of Tensorflow to develop models stays; but role of TFJS to run models at the edge diminishes *if* (and that\'s a big if) WebNN picks up.But given it\'s several steps from original model; I see it as always being several steps behind.Still; PyTorch (for example) doesn\'t have browser-native solution. Actually; none of the other frameworks do; only TF does in form of TFJS (that\'s the reason why I choose to start with TF/TFJS when I started with ML 6 months ago). So WebNN may pick up given the need to run all those other models in browser.Anyhow; I get the goals of WebNN; but if underlying interface to HW is flaky as WebGL is; they can talk about non-blocking as much as they want; but they need better way to interface with HW. And *if* WebGPU solves that (and uses Vulcan in the backend); it will solve it for TFJS as well.=====']",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Type Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/794,TypeError: t.toFloat is not a function TensorFlow with React,1,closed,2021-05-12T15:47:54Z,2021-05-24T06:59:05Z,trying to run face-api.js on react appNetInput.js:139 Uncaught (in promise) TypeError: t.toFloat is not a function    at NetInput.js:139    at Array.map (<anonymous>)    at NetInput.js:139    at engine.js:327    at Engine.scopedRun (engine.js:337)    at Engine.tidy (engine.js:326)    at Module.tidy (globals.js:175)    at NetInput../face-api.js/node_modules/tfjs-image-recognition-base/build/es6/dom/NetInput.js.NetInput.toBatchTensor (NetInput.js:123)    at SsdMobilenetv1.ts:30    at engine.js:327,"[""`* is not a function` for any tensorflow function pretty much always comes down to tensorflow backend not registered correctly. which again comes down to multiple incompatible versions of tensorflow being used.  if you're using `tfjs` 2.x or 3.x; you'll end up with this because original `face-api` has not not been updated for a long time and includes embedded `tfjs` v1 which is NOT compatible with tfjs 2.x or tfjs 3.x.that's main reason why this port exists: <https://github.com/vladmandic/face-api>=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/817,Load image take too much time,54,closed,2021-09-14T06:26:56Z,2021-09-27T05:40:42Z,Hi;I used this face-api.js (https://github.com/justadudewhohacks/face-api.js) and I have to load 10000 images when the node server is loaded for future comparison operations.This the function that I load when server is loaded; it take 0.5 second to one picture; how can I improve it ?```**// fetch first image of each class and compute their descriptorsasync function createBbtFaceMatcher() {  await faceDetectionNet.loadFromDisk(path.join(__dirname; '../weights'))  await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname; '../weights'))  await faceapi.nets.faceRecognitionNet.loadFromDisk(path.join(__dirname; '../weights'))  const labeledFaceDescriptors = await Promise.all(classes.map(    async className => {      let descriptors: any = [];      let uri = getFaceImageUri(className; 1);      const img = await canvas.loadImage(uri);      if (img) {        let descriptor = await faceapi.computeFaceDescriptor(img);        if (descriptor) {          descriptors.push(descriptor);        }      }      return new faceapi.LabeledFaceDescriptors(        className;        descriptors      )    }  )) ```       return new faceapi.FaceMatcher(labeledFaceDescriptors)}**,"[""you havent said which *backend* you're using - since it's `nodejs` solution; i'm assuming `tfjs-node`?  anyhow; *javascript* is not a platform to run compute intensive operations concurrently - and you're triggering all `faceapi` detections inside a promise - don't do that; run simple loop instead  also; note that this original version of `face-api` is using hard-coded embedded version of `tfjs` 1.7 which actually uses `tensorflow.so` version 1.15 for execution in `tfjs-node` backend which is quite old  you may want to try newer port of `face-api` that uses `tfjs` 3.9 which relies on `tensorflow.so` 2.6 which implements more accelerated functions  =====""; ""switching to loop doesn't make it faster - after all; it's the same code. it does make it use far less memory and less chances of race scenarios leading to crashes while still running with the same performance  =====""; '@vladmandicThanks for answer.How can I speed the load of 100000 images in few seconds instead of half hour now?====='; '10;000====='; ""i just checked using your code (modified to work - it would help if you post fully working code when asking question) on my home server with i5-6500TE; so very low power and it's ~75ms per image; far from 0.5sec you're seeing.what's your hardware and what's the backend you're using (`console.log(tf.getBackend())`)?```jsconst fs = require('fs');const canvas = require('canvas');const faceapi = require('./dist/face-api.node.js');async function main() {  faceapi.env.monkeyPatch({ Canvas: canvas.Canvas; Image: canvas.Image; ImageData: canvas.ImageData });  const faceDetectionNet = new faceapi.FaceDetectionNet();  await faceDetectionNet.loadFromDisk('model');  await faceapi.nets.faceLandmark68Net.loadFromDisk('model');  await faceapi.nets.faceRecognitionNet.loadFromDisk('model');  const dir = fs.readdirSync('../human/samples/people');  const descriptors = [];  const t0 = performance.now();  for (const f of dir) {    const img = await canvas.loadImage(`../human/samples/people/${f}`);    const c = canvas.createCanvas(img.width; img.height);    const ctx = c.getContext('2d');    ctx.drawImage(img; 0; 0; img.width; img.height);    const descriptor = await faceapi.computeFaceDescriptor(c);    if (descriptor) descriptors.push(descriptor);  }  const t1 = performance.now();  console.log('time:'; t1 - t0; 'average:'; (t1 - t0) / dir.length; 'descriptors:'; descriptors.length);}main();``````logtime: 1687.4630840420723 average: 76.70286745645784 descriptors: 22```=====""; ""a) what's the backend used? i'm only assuming it's the correct one `tjfs-node`b) try newer port of `faceapi` which uses tfjs 3.9.0 and tensorflow 2.6: [@vladmandic/face-api](https://github.com/vladmandic/face-api)  (i've been maintaining it for the past year since original is no longer maintained)=====""; 'a)yes""dependencies"": {    ""@tensorflow/tfjs-core"": ""^0.13.11"";    ""@tensorflow/tfjs-node"": ""^0.1.17"";====='; ""Hi;I download  it throgh npm (@vladmandic/face-api)const faceapi1 = require('../../node_modules/@vladmandic/face-api/dist/face-api.node.js');and modified the sample ?and got this error:(node:24184) UnhandledPromiseRejectionWarning: TypeError: tf8.io.weightsLoaderFactory is not a function    at FaceLandmark68Net.loadFromDisk (D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\@vladmandic\\face-api\\src\\NeuralNetwork.ts:106:31)    at ObWhat do i miss?=====""; ""remove `@tensorflow/tfjs-core` as it's already included in `tfjs-node` and install **latest** `@tensorflow/tfjs-node` - you have obsolete version 0.1.17; latest is 3.9.0=====""; ""I removed the core and installed the lates version of tenserflow and got this error:What do i miss ? * Building TensorFlow Node.js bindingsnode-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-build'node-pre-gyp' is not recognized as an internal or external command;operable program or batch file.=====""; ""that means your `nodejs` on widows is not the best - it cannot run binary bindings as part of package installation process  imo; i'd reinstall `node` and `npm` and make sure that `node-gyp` compiler is installed `npm i -g node-gyp`  and make sure it's at least node v14; but i'd recommend v16=====""; 'I installed node-pre-gyp + installed  lates version of tenserflow.====='; ""did you load `tf` before `faceapi`? new version requires tf to be loaded first as it cannot bind to binary distribution and it would be bad to do a fake bind.add `const tf = require('@tensorflow/tfjs-node');` at the start; before you load `faceapi`=====""; 'Now I uninstall both libraries and install tf before faceapi.====='; 'still same error====='; ""I said load; not just install? Can you show the code where you're loading both tf and faceapi? =====""; 'I got this error : on thus line:const descriptor = await faceapi1.computeFaceDescriptor(c);PS D:\\Camera\\Backup Face Recognition\\1\\face-recognition> npm run startDebugger attached.> face-recognition@1.0.0 start> ts-node-dev ./src/app.tsDebugger attached.[INFO] 19:00:19 ts-node-dev ver. 1.1.8 (using ts-node ver. 9.1.1; typescript ver. 4.4.2)Debugger attached.Platform node has already been set. Overwriting the platform with [object Object].cpu backend was already registered. Reusing existing backend factory.2021-09-14 19:00:22.463783: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2To enable them in other operations; rebuild TensorFlow with the appropriate compiler flags.14/09/2021 19:00:22:503 app.js - info : Server start running at http://localhost:606014/09/2021 19:00:22:975 app.js - info : Server start listening at port:606014/09/2021 19:00:22:977 app.js - info : Server is start loading bbt face matchertensorflow(node:7716) UnhandledPromiseRejectionWarning: TypeError: forwardFunc is not a function    at D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:586:31          at D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:424:20          at Engine.scopedRun (D:\\Camera\\Backup Face Recognition\\1\\face-recognition\\node_modules\\face-api.js\\node_modules\\@tensorflow\\tfjs-core\\src\\engine.ts:435:19)====='; ""you just posted the same app code - where is the load part?  e.g.; where are your `require` or `import` statements?  judging by the log; you're loading tfjs twice and causing conflict  =====""; 'why would you assign function to a global variable? and i have no idea how your project is structured and what is a *controller* you mention - but anyhow; that is not a `face-api` question  my advise? refactor your code not to use global variables ever  ====='; '1.I want to load all images when server init ; and later consume it in other routes ?How do you refactor it? I used for compare activity.I tried to used node-cache instead; I create singelton from it to share but without success.With global is working.TypeError: Method get TypedArray.prototype.length called on incompatible receiver [object Object]2.What does _distance mean ?```{     ""bestMatchPerson"": {        ""_label"": ""unknown"";        ""_distance"": 0.7806719517998486    }}```3. Do i need both ""@vladmandic/face-api"": ""^1.5.2""; + ""face-api.js"": ""^0.22.2""; in pacage json ?4.  You said:(i\'ve been maintaining it for the past year since original is no longer maintained)What do you mean face-api.js is no longer maintined ?====='; ""1. i get the goal; but dumping stuff into `global` is always a bad idea. anyhow; your choice2. distance is euclidean distance. take a look at <https://github.com/vladmandic/human/wiki/Embedding#face-similarity> on how it's calculated.3. no; only one; never both. loading both causes conflicts you've already seen.4. correct. see notes at <https://github.com/vladmandic/face-api#note>=====""; 'why are you importing with strange relative paths?```jsfaceapi = require(\'../../node_modules/@vladmandic/face-api/dist/face-api.node.js\');```can be just```jsfaceapi = require(\'@vladmandic/face-api\');```or if you want to be specific```jsfaceapi = require(\'@vladmandic/face-api/dist/face-api.node.js\');```never import installed modules with relative paths inside `node_modules`and enclose your js code in markdown code quotes so it\'s actually readable - i cannot read this\'```js...```\'> Your package is maintained ?Yes. Just check git comit history to for any package to see when is the last update and how often it receives updates.> When i removed ""face-api.js"": ""^0.22.2""; from pacage json i got this error; i do not used in code at all.> [ERROR] 11:01:21 Error: Cannot find module \'@tensorflow/tfjs-core\'Something in the code is still referencing `@tensorflow/tfjs-core`; but it\'s no longer installed  I\'d clean `node_modules` and re-run `npm install` and then check both your code and `package-json.lock` file to see where is it referenced  My `face-api` for nodejs references just `@tensorflow/tfjs-node` package> Regading Eucliean distance; when i can say us good match and when is not ?Match is never 100%; so think of Euclidean distance as `chance of match`  What is a threshold for a good match? That is a personal choice - try what values are good for you as it highly depends on input images  ====='; '2. Can you please give example of:a. When node server is loaded than load all images(10;000 is reasonable) using createBbtFaceMatcher  and insert into best data structure(not global variable as you said.b. When i register a new image is update the images data structure for opertaion in c.c. When i compare it used the images data structure for best match====='; ""3. I fixed to use: `const faceapi = require('@vladmandic/face-api')`=====""; '5.I markdowned code quotes in all comments as you asked-:)6.I used these weights wjen working with  face-api.js; i need to change it to other files ?![image](https://user-images.githubusercontent.com/1079689/133630920-d8247f23-3f86-40f0-8dfd-9a73cfef8c94.png)====='; '> If i got ""_distance"": 0.7 (above) what does it mean and if I got 0.3(below) what does it mean ?distance 0 means identical and 1 means completely different. default threshold for matching is distance < 0.6btw; just choose one model; don\'t test for both  `ssd` is typically better than `tiny` in everything; `tiny` is included only for very low power devices> Can you please give example of:> a. When node server is loaded than load all images(10;000)> (createBbtFaceMatcher ) insert into best data structure(not global variable).> b. When i register a new image is update the images data structure for opertaion in c.> c. When i compare it used the images data structure for best matchthis is a general architecture question; perhaps this discussion is close to what you\'re looking for: <https://github.com/vladmandic/human/discussions/138> and <https://github.com/vladmandic/human/discussions/145>also see notes in <https://github.com/vladmandic/face-api/issues/51> for general json saveand notes on on multi-process analysis to fully utilize available hardware <https://github.com/vladmandic/face-api/issues/20>10;000 images is not a huge number; but it\'s getting there and it\'s worth it to architect right - you don\'t want to keep 10k objects in global variable ever. and what if it grows? solution should be such that you can switch to db store if needed so there is no need to ever load all in-memory====='; '1. How do I insert the 10k object to sql db?(createBbtFaceMatcher ); I defined coulum as json typeand later insert record for all labeledFaceDescriptors or for each labeledFaceDescriptor ?2. When I register i need to add new record to db and on compare i need to load all t from db  10k records is it fast ?  and later do?```let  descriptorFromDb = go to db and bring all descriptor;let faceMatcher = new faceapi.FaceMatcher(descriptorFromDb ); const singleResult = await faceapi        .detectSingleFace(referenceImage)        .withFaceLandmarks()        .withFaceDescriptor()   const bestMatchPerson =  global.faceMatcher .findBestMatch(singleResult.descriptor)```====='; ""> How do I insert the 10k object to db?(createBbtFaceMatcher ); how i defined in db ?That is far beyond this conversation. For a proper solution; I suggest using `mongodb` module  For a prototype; storing object as on disk is a start (`fs.writeFileSync('db.json'; JSON.stringify(myObject))`)Anything is better than a) calculating them again-and-again on each startup; b) keeping in global variable> When I register i need to add new record to db and on compare i need to load all t from db 10k records is it fast ?`findBestMatcher` is simply a for loop through that runs matches for all records and returns one with lowest distance - not more and not less  So if you read records from DB yourself and pass them to match method them; get the same thing without needing to have all records in memory all the time. Of course; you don't want to read neither all records nor one-by-one; it should have some sane disk paging  But again; this is far beyond this conversation.=====""; ""@vladmandic Thank you for you answers.But I don't understand your final solution if you please elaborate ? =====""; ""architecture is a personal choice. plus solution architecture is far beyond this scope  im sharing here a short writeup; but please don't go further with architecture questions - lets limit the scope to library issues  first; i suggest using an actual database. you might start with `nedb-promises` which allows you to easily switch to `mongodb` in the future if database grows a lot  structure is at least two different object tables: images and faces (image can have more than one face; don't assume it's always just one)  on server startup1. initialize database and pass handle to other module (such as module that handle uploads) so they can share access2. enumerate image files and for each check if its in the db or if its modification time is different than one in db3. if necessary process image and store file descriptors to faces object table and map from which file it came from4. store file details in files object tablethen on how to handle uploads1. on image file upload; trigger steps 3 and 4 from above2. run face match on all records to get best match if above threshold3. based on the best match find image file and return it to clientoptionally1. run file system watcher so it automatically finds new or modified files so you don't need to restart server to process additional images  2. use worker process pool for maximum performance so you can process as many images in parallel as you have cpu cores  =====""]",Slow Execution,Poor Performance,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.1.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/837,vladmandic / face-api,2,open,2021-12-16T04:39:56Z,2021-12-17T02:38:03Z,Hi; I have a question! This repository https://github.com/vladmandic/face-apiand this repository https://github.com/justadudewhohacks/face-api.js/are **Same** ?? Which of them do I have to install in the project?,['Second one is original;but first one has a good compatibility for the new version`tfjs`.====='; 'More info in this issue [782](https://github.com/justadudewhohacks/face-api.js/issues/782)====='],Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,change framework version,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/89,Issue with parcel build,3,closed,2018-09-12T14:21:34Z,2018-09-13T15:15:54Z,"Trying to bundle face-api.js for an SPA; similar setup to this:https://github.com/tensorflow/tfjs-examples/blob/master/addition-rnn/package.json#L17We're trying to do this:```javascriptimport * as faceapi from 'face-api.js';```Getting this error on the Parcel build:```~/workspace/project/node_modules/face-api.js/build/tinyYolov2/TinyYolov2.js:1:25: Cannot resolve dependency 'tslib'> 1 | import * as tslib_1 from ""tslib"";    |                          ^  2 | import { TinyYolov2 as TinyYolov2Base } from 'tfjs-tiny-yolov2';  3 | import { FaceDetection } from '../classes';  4 | import { BOX_ANCHORS; BOX_ANCHORS_SEPARABLE; DEFAULT_MODEL_NAME; DEFAULT_MODEL_NAME_SEPARABLE_CONV; IOU_THRESHOLD; MEAN_RGB_SEPARABLE; } from './const';```I looked at the package.json; and it seems like `tslib` should be in `dependencies`; not `devDependencies`. We're working around currently by doing this:```javascriptimport * as faceapi from 'face-api.js/dist/face-api'```Seems hacky. The other option is setting `main` to the dist file.","['Probably because the build output is still using es6 imports; I think the output should be commonjs again; not sure atm why I changed that.====='; ""@justadudewhohacks Not sure why ES6 modules would cause this issue... The issue is being able to refer to `node_modules/tslib`. It's in `devDependencies`; which means that it will never be installed when we are consuming face-api.js.Tensorflow.js uses ES6 modules; and it builds fine. See `@tensorflow/tf-js` import here:https://github.com/tensorflow/tfjs/blob/master/src/index.ts#L18See here how the `@tensorflow` modules are in `dependencies`?https://github.com/tensorflow/tfjs/blob/master/package.json#L62From what I understand; TD import helpers will emit common functions to utilize `tslib` instead of duplicating functions like `__extends`; with the expectation that `tslib` will be installed as a dependency. So `tslib` defines `__extends` once and all references to it are rewritten  to call `tslib`. Doesn't necessarily cause issues in SPA or server projects when you put it into `devDependencies`; but for node modules; I think it has to be in `dependencies`.If you're still not convinced; look at the `tslib` readme. Notice the `npm install --save tslib` command; not `--save-dev`:https://github.com/Microsoft/tslib/blob/master/README.md#npmMaybe this is an oversight but that seems doubtful.What would be the drawback of including `tslib` in `dependencies`? Am I missing something?=====""; ""> @justadudewhohacks Not sure why ES6 modules would cause this issue... The issue is being able to refer to node_modules/tslib. It's in devDependencies; which means that it will never be installed when we are consuming face-api.js.Yep you are absolutely right. Sorry for the confusion here.Besides moving tslib from devDependencies to dependencies; I think the output should still be commonjs instead of es6; but that's another story.=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,build/install configuration,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Dependency Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/9,Add a configuration with external tf-js,2,closed,2018-06-15T15:02:05Z,2018-06-16T14:12:56Z,My issue is i would like to use this code as an external package with my other tfjs models.Currently you can only have one tensorflowjs script instance any other that gets loaded after the first one breaks.,"['Hmm i didnt try that yet; what would be required to make that work? Can you not bundle this package together with you own?In case it helps; tf core is also exported via facepi.tf====='; ""I'm going to close it; managed to do it by changing the types to any and removing the tf reference=====""]",Reference Error,Crash,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Failure to initialize certain DL backends (e.g., Wasm) even after successful TensorFlow.js installation""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.5
https://github.com/justadudewhohacks/face-api.js/issues/90,Preloading of the MTCNN model is very slow.,8,closed,2018-09-12T14:44:01Z,2021-09-14T05:38:38Z,"Hello;I'm using VueJS. When you open a Modal; these codes are working; which I created from your sample code. The value of ""console.time"" is quite high.**Code:**```JSconsole.time(""fullFaceDescriptions"");vm.yuz_degisken_objesi.fullFaceDescriptions = (await faceapi.allFacesMtcnn(vm.yuz_degisken_objesi.videoEl; vm.yuz_degisken_objesi.mtcnnParams))    .map(fd => fd.forSize(vm.yuz_degisken_objesi.hw.width; vm.yuz_degisken_objesi.hw.height));console.timeEnd(""fullFaceDescriptions"");console.log(""*"");```fullFaceDescriptions: **5500.3720703125ms***fullFaceDescriptions: **372.788818359375ms***fullFaceDescriptions: **257.229736328125ms***fullFaceDescriptions: **180.324951171875ms***fullFaceDescriptions: **222.89404296875ms***fullFaceDescriptions: **207.50927734375ms***How can I make it more stable?","['More stable or faster? Faster inference time of the mtcnn detector can be achieved by increasing the minFaceSize parameter for example; which will skip the detection of smaller faces.====='; 'Preloading? What can be done for this?fullFaceDescriptions: **5500.3720703125ms**====='; 'Nothing really that I know of; seems like the first time a tensor of a certain shape is uploaded to the GPU there is an initial delay; which they refer to as ""warmup"" in the mobilenet example. The MTCNN uploads tensors of N + 2 different shapes (N in stage 1; can be adjusted by the `maxNumScales` forward parameter and 1 each in stages 2 and 3). This unfortunately causes the first forward pass of MTCNN to be much longer on certain machines.I am not sure where this inital delay comes from (maybe due to allocating textures of certain sizes on the GPU; just a wild guess; I am not that familar with WebGL for GPGPU). If it concerns you; maybe you could ask at tfjs for help.PS: I would also be interested in the answer to that.====='; ""Here's a [comment](https://github.com/tensorflow/tfjs-examples/blob/02e330e43d7234bc4002a836586141812ec08ee7/mobilenet/index.js#L35) from a tfjs-example: > Warmup the model. This isn't necessary; but makes the first prediction faster.This warmup works on `computeFaceDescriptor`; but not works on `mtcnn` or `allFacesMtcnn` on my project. Maybe you can make a predict first after the model is loaded.=====""; ""I've been fighting with this for a week or so now... I'm currently looking into web workers as a way to do an initial prediction without tying up the main UI thread so that my actual predictions later on are lightning-fast. I'll let you know if I make any progress.=====""; ""Alas... no such luck. I think I'd have to understand tfjs much better than I do to convert it in such a way to get it to work in a web worker. I have gotten it so far as to attempt to load the models; but the registration of the backend failed. Some Googling tells me that webgl isn't supported in Web Workers; but cpu backend also fails to register.=====""; 'Before you are putting too much work into this; I will soon publish some new changes; including a new tiny face detector; which is much faster than ssd and mtcnn and produces much more stable detection results than mtcnn.I think at that point there will be no real reason anymore to use the mtcnn; unless there is a way to fix the warmup issue.====='; '@justadudewhohacks Awesome! I look forward to this.=====']",Slow Execution,Poor Performance,Improper Model Attribute,,,change Third-party library version,Changing version,Third-party library,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",B.1.1,C.2
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/10,Can you please use explicit tf-core dependency versioning that matches face-api,1,closed,2019-10-31T02:00:10Z,2020-03-21T13:04:45Z,It forces me to use a skipLibCheck and clutters my dependency tree since it will almost always download a higher version of tf-core then required by face-apiThis issue is a direct result of that https://github.com/justadudewhohacks/face-api.js/issues/415Is there any reason why setting an explicit version wouldn't be a good idea?,"[""> Is there any reason why setting an explicit version wouldn't be a good idea?The only real reason is that we would enforce other code bases using tfjs-image-recognition-base to use the same tfjs-core version as face-api.jsTo be honest; I do not understand; why your package manager does not install the tfjs-core version that is explicitly set in the package.json of face-api.js. Are you using yarn or npm?=====""]",Build & Install Failure,Build & Initialization Failure,Incompatibilitty between 3rd-party DL Library and TF.js,,,use one package,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Confused Document"",
    ""specific_type"": ""[E.1.1] Incorrect Dependency Instructions""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.5
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3,Illegal Constructor' exception thrown when used in Electron renderer process,8,open,Dec 8; 2018,Mar 1; 2019,"When using this as part of the face-api.js package; I get an 'Illegal Constructor' exception when trying to do a face detection in the renderer process of an Electron app (e.g. faceapi.detectSingleFace()). Looks like face detection is attempting to create a canvas element by calling new HTMLCanvasElement() instead of the required document.createElement('canvas').
The root of the issue appears to be in src/env/initialize.ts; where function createCanvasElement attempts to create a new HTMLCanvasElement directly.

I'm able to work around it by overriding createCanvasElements (and createImageElement); e.g.
","[I see the same issue is filed here: justadudewhohacks/face-api.js#157=====Yes; I think we should definitely export the initializer funcitons from here. This way the user can simply fix the environment himself in case his working enivronment is an unusual deviation of the nodejs or browser enivronment.
As for the electron renderer thread it would be optimal to fix the isNodejs() check; such that it doesn't break in the standard nodejs environment. Maybe checking for the browser environment first in initializeEnvironment would already fix this for electron.What do you think?=====I think giving the user the option to specify environment would be best; essentially what monkeyPatch does but in a less seemingly-hackish way. :)=====Hello @andrewwalters and @justadudewhohacks I am also using electron but in my case isNodejs returns true and isBrowser also returns true. I get the following error when I call detectAllFaces:
Error: pixels passed to tf.fromPixels() must be either an HTMLVideoElement; HTMLImageElement; HTMLCanvasElement or ImageData; but was Canvas I took a look at the environment setup in my electron app. isBrowser would return true since all of the browser elements are present. But initializeBrowserEnv doesn't give the option having readFile available; which would be present in electron apps as well.=====I think the renderer process environment should now get initialized correctly; if you want to give face-api.js v0.16.2 a try.
I also exported createBrowserEnv and createNodejsEnv; so in case there are still issues with incorrect initialization one can easily fix this by faceapi.env.setEnv(faceapi.env.createBrowserEnv()) for example.
Regarding readFile in the renderer process; as far as I am aware; you have to require fs via electron remote in the renderer process right? I also exported a createFileSystem helper; which lets you monkeyPatch all the fs related envs as follows: const fs = remote.require('fs') faceapi.env.monkeyPatch(faceapi.env.createFileSystem(fs))=====The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to remote as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked: const fs = require('fs'); faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));]",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3,Illegal Constructor' exception thrown when used in Electron renderer process,8,open,Dec 8; 2018,Mar 1; 2019,"When using this as part of the face-api.js package; I get an 'Illegal Constructor' exception when trying to do a face detection in the renderer process of an Electron app (e.g. faceapi.detectSingleFace()). Looks like face detection is attempting to create a canvas element by calling new HTMLCanvasElement() instead of the required document.createElement('canvas').
The root of the issue appears to be in src/env/initialize.ts; where function createCanvasElement attempts to create a new HTMLCanvasElement directly.

I'm able to work around it by overriding createCanvasElements (and createImageElement); e.g.
","[I see the same issue is filed here: justadudewhohacks/face-api.js#157=====Yes; I think we should definitely export the initializer funcitons from here. This way the user can simply fix the environment himself in case his working enivronment is an unusual deviation of the nodejs or browser enivronment.
As for the electron renderer thread it would be optimal to fix the isNodejs() check; such that it doesn't break in the standard nodejs environment. Maybe checking for the browser environment first in initializeEnvironment would already fix this for electron.What do you think?=====I think giving the user the option to specify environment would be best; essentially what monkeyPatch does but in a less seemingly-hackish way. :)=====Hello @andrewwalters and @justadudewhohacks I am also using electron but in my case isNodejs returns true and isBrowser also returns true. I get the following error when I call detectAllFaces:
Error: pixels passed to tf.fromPixels() must be either an HTMLVideoElement; HTMLImageElement; HTMLCanvasElement or ImageData; but was Canvas I took a look at the environment setup in my electron app. isBrowser would return true since all of the browser elements are present. But initializeBrowserEnv doesn't give the option having readFile available; which would be present in electron apps as well.=====I think the renderer process environment should now get initialized correctly; if you want to give face-api.js v0.16.2 a try.
I also exported createBrowserEnv and createNodejsEnv; so in case there are still issues with incorrect initialization one can easily fix this by faceapi.env.setEnv(faceapi.env.createBrowserEnv()) for example.
Regarding readFile in the renderer process; as far as I am aware; you have to require fs via electron remote in the renderer process right? I also exported a createFileSystem helper; which lets you monkeyPatch all the fs related envs as follows: const fs = remote.require('fs') faceapi.env.monkeyPatch(faceapi.env.createFileSystem(fs))=====The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to remote as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked: const fs = require('fs'); faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));]",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3,Illegal Constructor' exception thrown when used in Electron renderer process,8,open,2018-12-08T00:03:45Z,2019-03-01T06:28:36Z,When using this as part of the face-api.js package; I get an 'Illegal Constructor' exception when trying to do a face detection in the renderer process of an Electron app (e.g. `faceapi.detectSingleFace()`). Looks like face detection is attempting to create a canvas element by calling `new HTMLCanvasElement()` instead of the required `document.createElement('canvas')`.The root of the issue appears to be in src/env/initialize.ts; where `function createCanvasElement` attempts to create a new HTMLCanvasElement directly.I'm able to work around it by overriding `createCanvasElements` (and `createImageElement`); e.g.```const faceapi = require('face-api.js');faceapi.env.monkeyPatch({    createCanvasElement: () => document.createElement('canvas');    createImageElement: () => document.createElement('img')});```,"['I see the same issue is filed here: https://github.com/justadudewhohacks/face-api.js/issues/157====='; ""Yes; I think we should definitely export the initializer funcitons from [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts). This way the user can simply fix the environment himself in case his working enivronment is an unusual deviation of the nodejs or browser enivronment.As for the electron renderer thread it would be optimal to fix the isNodejs() check; such that it doesn't break in the standard nodejs environment. Maybe checking for the browser environment first in [initializeEnvironment](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts#L5) would already fix this for electron.What do you think?=====""; ""I think giving the user the option to specify environment would be best; essentially what monkeyPatch does but in a less seemingly-hackish way. :)I took a look at the environment setup in my electron app. `isBrowser` would return true since all of the browser elements are present. But `initializeBrowserEnv` doesn't give the option having `readFile` available; which would be present in electron apps as well.=====""; 'Hello @andrewwalters and @justadudewhohacks I am also using electron but in my case isNodejs returns true and isBrowser also returns true.I get the following error when I call detectAllFaces:Error: pixels passed to tf.fromPixels() must be either an HTMLVideoElement; HTMLImageElement; HTMLCanvasElement or ImageData; but was Canvas====='; ""I think the renderer process environment should now get initialized correctly; if you want to give face-api.js v0.16.2 a try.I also exported createBrowserEnv and createNodejsEnv; so in case there are still issues with incorrect initialization one can easily fix this by `faceapi.env.setEnv(faceapi.env.createBrowserEnv())` for example.Regarding readFile in the renderer process; as far as I am aware; you have to require fs via electron remote in the renderer process right? I also exported a createFileSystem helper; which lets you monkeyPatch all the fs related envs as follows:``` javascriptconst fs = remote.require('fs')faceapi.env.monkeyPatch(faceapi.env.createFileSystem(fs))```=====""; ""The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked:```const fs = require('fs');faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));```=====""; 'Interesting; ok.====='; ""> The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked:> > ```> const fs = require('fs');> faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));> ```This worked ! Awesome. Thanks ! :)=====""]",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/tfjs-image-recognition-base/issues/3,Illegal Constructor' exception thrown when used in Electron renderer process,8,open,2018-12-08T00:03:45Z,2019-03-01T06:28:36Z,When using this as part of the face-api.js package; I get an 'Illegal Constructor' exception when trying to do a face detection in the renderer process of an Electron app (e.g. `faceapi.detectSingleFace()`). Looks like face detection is attempting to create a canvas element by calling `new HTMLCanvasElement()` instead of the required `document.createElement('canvas')`.The root of the issue appears to be in src/env/initialize.ts; where `function createCanvasElement` attempts to create a new HTMLCanvasElement directly.I'm able to work around it by overriding `createCanvasElements` (and `createImageElement`); e.g.```const faceapi = require('face-api.js');faceapi.env.monkeyPatch({    createCanvasElement: () => document.createElement('canvas');    createImageElement: () => document.createElement('img')});```,"['I see the same issue is filed here: https://github.com/justadudewhohacks/face-api.js/issues/157====='; ""Yes; I think we should definitely export the initializer funcitons from [here](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts). This way the user can simply fix the environment himself in case his working enivronment is an unusual deviation of the nodejs or browser enivronment.As for the electron renderer thread it would be optimal to fix the isNodejs() check; such that it doesn't break in the standard nodejs environment. Maybe checking for the browser environment first in [initializeEnvironment](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/initialize.ts#L5) would already fix this for electron.What do you think?=====""; ""I think giving the user the option to specify environment would be best; essentially what monkeyPatch does but in a less seemingly-hackish way. :)I took a look at the environment setup in my electron app. `isBrowser` would return true since all of the browser elements are present. But `initializeBrowserEnv` doesn't give the option having `readFile` available; which would be present in electron apps as well.=====""; 'Hello @andrewwalters and @justadudewhohacks I am also using electron but in my case isNodejs returns true and isBrowser also returns true.I get the following error when I call detectAllFaces:Error: pixels passed to tf.fromPixels() must be either an HTMLVideoElement; HTMLImageElement; HTMLCanvasElement or ImageData; but was Canvas====='; ""I think the renderer process environment should now get initialized correctly; if you want to give face-api.js v0.16.2 a try.I also exported createBrowserEnv and createNodejsEnv; so in case there are still issues with incorrect initialization one can easily fix this by `faceapi.env.setEnv(faceapi.env.createBrowserEnv())` for example.Regarding readFile in the renderer process; as far as I am aware; you have to require fs via electron remote in the renderer process right? I also exported a createFileSystem helper; which lets you monkeyPatch all the fs related envs as follows:``` javascriptconst fs = remote.require('fs')faceapi.env.monkeyPatch(faceapi.env.createFileSystem(fs))```=====""; ""The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked:```const fs = require('fs');faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));```=====""; 'Interesting; ok.====='; ""> The electron renderer process can access node modules directly without going through remote (unless you've explicitly turned off node integration in your renderer process; in which case I believe you lose access to `remote` as well). I didn't try the above but instead just explicitly initialized the browser+filesystem environment; and it worked:> > ```> const fs = require('fs');> faceapi.env.setEnv(Object.assign(faceapi.env.createBrowserEnv(); faceapi.env.createFileSystem(fs)));> ```This worked ! Awesome. Thanks ! :)=====""]",Reference Error,Crash,Incorrect Code Logic,,,patch environment,Fix environment adaptability,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/ntedgi/node-efficientnet/issues/36,update playground ui add all available files format,1,open,2021-03-28T08:29:44Z,2021-04-09T03:59:32Z,"**ui changes:**<img width=""397"" alt=""Screen Shot 2021-03-28 at 11 26 59"" src=""https://user-images.githubusercontent.com/31243793/112746621-9a468780-8fb8-11eb-95b3-938203caf965.png"">under here add a new line:""Supported file types : JPG; PNG ;GIF;SVG;HEIC;WEBP""add available files format to file uploader component ",['This task block by #35 ====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,type replacer,Replace data Shape/type,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""Incorrect Programming""
  }
}
```",D,A.4
https://github.com/tensorspace-team/tensorspace/issues/179,Support configure font style in scene,6,open,2019-01-10T05:00:56Z,2019-01-29T14:08:59Z,TensorSpace use `fonts/helvetiker_regular.typeface.json` as the basic font style for `output1d` layer text information. However; it does not support some text; for example; when input `中文`; the `fonts/helvetiker_regular.typeface.json` will show `??` (this case can be reproduced in three.js docs's playground).To handle this case; add a configuration in model; user can override the default `fonts/helvetiker_regular.typeface.json` with own fonts.For example:```// user loadFont file through AJAXlet ownFont = loadFont();let model = new TSP.models.Sequential({    ...    font: ownFont;    ...});```,"[""@syt123450 So I did a bit of digging on this after setting up a local instance of tensor (fixed a bug with missing dep aswell). I found the hard coded font and saw that there is an npm module for this specific font already (https://www.npmjs.com/package/three.regular.helvetiker). The rollup config didn't have common module and ES6 external resolver support so I added that into the build aswell. That at least trims down that giant line of json into one easy import. Still looking at the best way to pass through the option without rewriting too much function calls; but looks doable.=====""; ""@Truemedia It's really a good news! I think this refactoring will highly improve the TensorSpace's source code readability. Could you send a PR related your refactoring?=====""; '@BoTime Could you give some suggestions about rollup configuration?====='; ""@syt123450 Sure; would you be able to open a feature branch/develop branch for me to do a new request on so it doesn't go into master just yet.=====""; 'Hi @Truemedia ; I created a new issue #192 and create a new branch [refactorfont](https://github.com/tensorspace-team/tensorspace/tree/refactorfont). We can work under the new issue for refactoring; separate the work of refactor and new feature ~====='; ""@syt123450 Ok cool; I won't have much time to commit any code during week but weekend can probably box off both those branches with the new features=====""]",render bug,Crash,Incorrect Code Logic,,,add function,add function,Third-party library,Data Processing,"{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Poor Text Rendering""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}",A,A.4
https://github.com/martinjm97/ENNUI/issues/130,"Keep Saying ""Loading MNIST dataset""",6,closed,2020-10-20T21:15:04Z,2020-10-21T00:18:57Z,"When I enter the webpage: https://math.mit.edu/ennui/; the message ""Loading MNIST dataset"" keeps being on the top; and nothing changes for a long time. Is there anything wrong with the system?","['Yes are in the middle of fixing this. It is a problem with the hosting.====='; 'Thanks for the quick reply! Please let me know when it is fixed====='; ""Should be all fixed now! Let me know and I'll close this issue.=====""; 'How long it is supposed to take? Unfortunately; it is still there when I open the webpage; for about 2mins already====='; 'Oh it works now! ====='; 'Great! Closing this issue. Let us know what you end up using this for! We always love to see people using Ennui 😃 =====']",Browser Hangs,Poor Performance,Unknown,,,,,Third-party library,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",B.1.2,E
https://github.com/justadudewhohacks/face-api.js/issues/418,Using ES6 module directly inside a script ?,3,open,2019-09-18T22:50:06Z,2020-03-25T03:09:29Z,"Hello;I would like to use face-api.js using only the ES6 module (without any build step; and using the fantastic [pika manager](https://www.pika.dev/)).Like so:```<script type=""module"">  import faceapi from '/web_modules/face-api.js/dist/face-api.min.js'  console.log('faceapi.nets');</script>```But it fails to load the module doing so; I can't manage to get the object. It tries to load fs; and other node specifics libs.I also get `Module {Symbol(Symbol.toStringTag): ""Module""}Symbol(Symbol.toStringTag): ""Module""`Thanks !","[""> It tries to load fs; and other node specifics libs.fs should be the only one; otherwise it does not come from face-api.js. The issue might be one of two:1. The bundler / pika manager; that you are using does not handle conditional require statements correctly. face-api.js checks your environment; if you are running in a node environment; it will require fs.2. Your environment is mistaken for a node environment; e.g. [this](https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/master/src/env/isNodejs.ts) check returns true. Could you please check what the following condition evaluates to in your environment:``` javascripttypeof global === 'object'    && typeof require === 'function'    && typeof module !== 'undefined'    // issues with gatsby.js: module.exports is undefined    // && !!module.exports    && typeof process !== 'undefined' && !!process.version```=====""; ""Yes I think it is a pika issue; as I have this message when I do pika install:But it is strange; it is the first time I encounter this problem.  ``` @pika/web installing... vue/dist/vue.esm.browser.js; http-vue-loader/src/httpVueLoader.js; remarkable/dist/esm/index.browser.js; typeit/dist/typeit.min.js; vue-beautiful-chat/dist/index.js; botui/build/botui.js; face-api.js/dist/face-api.min.js; oscilloscope/dist/oscilloscope.es.jsThe 'this' keyword is equi⠼ @pika/web installing... vue/dist/vue.esm.browser.js; http-vue-loader/src/httpVueLoader.js; remarkable/dist/esm/index.browser.js; typeit/dist/typeit.min.js; vue-beautiful-chat/dist/index.js; botui/build/botui.js; face-api.js/dist/face-api.min.js; oscilloscope/dist/oscilloscope.es.js✖ 'crypto' is imported by 'node_modules/face-api.js/dist/face-api.min.js'; but could not be resolved.  'crypto' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'node_modules/face-api.js/dist/face-api.min.js'; but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'fs' is imported by 'node_modules/face-api.js/dist/face-api.min.js'; but could not be resolved.  'fs' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'crypto' is imported by 'commonjs-external-crypto'; but could not be resolved.  'crypto' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'commonjs-external-util'; but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'fs' is imported by 'commonjs-external-fs'; but could not be resolved.  'fs' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'util' is imported by 'node_modules/node-fetch/lib/fetch-error.js'; but could not be resolved.  'util' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'node_modules/node-fetch/index.js'; but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'node_modules/node-fetch/lib/request.js'; but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'url' is imported by 'commonjs-external-url'; but could not be resolved.  'url' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'node_modules/node-fetch/index.js'; but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'node_modules/node-fetch/lib/response.js'; but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'http' is imported by 'commonjs-external-http'; but could not be resolved.  'http' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'zlib' is imported by 'node_modules/node-fetch/index.js'; but could not be resolved.  'zlib' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'zlib' is imported by 'commonjs-external-zlib'; but could not be resolved.  'zlib' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'https' is imported by 'node_modules/node-fetch/index.js'; but could not be resolved.  'https' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'https' is imported by 'commonjs-external-https'; but could not be resolved.  'https' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/node-fetch/index.js'; but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/node-fetch/lib/body.js'; but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'commonjs-external-stream'; but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/iconv-lite/lib/streams.js'; but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'stream' is imported by 'node_modules/iconv-lite/lib/extend-node.js'; but could not be resolved.  'stream' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/safer-buffer/safer.js'; but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/iconv-lite/lib/streams.js'; but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'node_modules/iconv-lite/lib/extend-node.js'; but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'buffer' is imported by 'commonjs-external-buffer'; but could not be resolved.  'buffer' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'string_decoder' is imported by 'node_modules/iconv-lite/encodings/internal.js'; but could not be resolved.  'string_decoder' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✖ 'string_decoder' is imported by 'commonjs-external-string_decoder'; but could not be resolved.  'string_decoder' is a Node.js builtin module that won't exist on the web. You can find modern; web-ready packages at https://www.pikapkg.com✔ @pika/web installed: vue/dist/vue.esm.browser.js; http-vue-loader/src/httpVueLoader.js; remarkable/dist/esm/index.browser.js; typeit/dist/typeit.min.js; vue-beautiful-chat/dist/index.js; botui/build/botui.js; face-api.js/dist/face-api.min.js; oscilloscope/dist/oscilloscope.es.js. [3.72s]```Thanks=====""; 'I believe I\'m hitting this issue as well (without pika):```<script type=""module"">    import * as faceapi from \'./face-api.min.js\'    console.log(faceapi)    // Module\xa0{Symbol(Symbol.toStringTag): ""Module""}```=====']",Initialization Faliure,Build & Initialization Failure,Dependency Error,,,change dependency version,Modifying dependency configuration,Third-party library,Environment Integration,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.3] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/199,Can't install typescript project,4,closed,2019-01-22T15:36:59Z,2019-01-28T10:00:45Z,Hi;When I try to execute the following command in root's repository;```tsc```I get this error : ```loophole@loophole:~/packages/vision/face-api.js$ tscsrc/ssdMobilenetv1/outputLayer.ts:73:7 - error TS2345: Argument of type '[number; number | undefined]' is not assignable to parameter of type 'number[] | [number; number] | [number; number; number; number] | [number; number; number; number; number] | [number; number; number] | [number] | [number; number; number; number; number; number]'.  Type '[number; number | undefined]' is not assignable to type '[number; number]'.    Type 'number | undefined' is not assignable to type 'number'.      Type 'undefined' is not assignable to type 'number'.73       [batchSize; scores.shape[1]]         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~Found 1 error.```Am I doing something wrong ? I am a total newbie to TypeScript so excuse me if my question is silly.,"['Whats the typescript version you are using? Should compile with latest.====='; 'I am using version 3.2.4; which seems to be the latest.====='; 'Oh nevermind; are you trying to compile face-api.js? Currently face-api.js is using typescript 2.8.4 to compile the project (see package.json); if you want to compile it using the locally installed typescript version simply use the npm script (npm run tsc).====='; ""Yes; that's exactly what I was trying to do ! Thanks a lot !=====""]",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change typescript version,Changing version,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2"",
    ""specific_type"": ""C.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1""
  }
}
```",C,B.2
https://github.com/justadudewhohacks/face-api.js/issues/142,Could not run faceDetection.ts,2,closed,2018-11-20T00:28:15Z,2018-12-04T20:49:09Z,"I hope you to tell me; error reason why.here wrote; environment; actions and resultthanks------------- environment -------------$ uname -rv4.4.0-139-generic #165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018$ node -vv11.2.0$ npm ls typescript ts-node -g/usr/local/lib├── ts-node@7.0.1└── typescript@3.1.6------------- environment -------------------------- tried-and-result -------------$ cd /home/tobisaki/try2$ git clone https://github.com/justadudewhohacks/face-api.js.git$ cd face-api.js/examples/examples-nodejs/$ ls -la合計 64drwxrwxr-x  4 tobisaki tobisaki  4096 11月 20 08:36 .drwxrwxr-x  6 tobisaki tobisaki  4096 11月 19 23:19 ..drwxrwxr-x  2 tobisaki tobisaki  4096 11月 19 23:19 commons-rw-rw-r--  1 tobisaki tobisaki   479 11月 19 23:19 faceDetection.ts-rw-rw-r--  1 tobisaki tobisaki   704 11月 19 23:19 faceLandmarkDetection.ts-rw-rw-r--  1 tobisaki tobisaki  1649 11月 19 23:19 faceRecognition.tsdrwxrwxr-x 88 tobisaki tobisaki  4096 11月 20 09:07 node_modules-rw-rw-r--  1 tobisaki tobisaki 28902 11月 20 08:36 package-lock.json-rw-rw-r--  1 tobisaki tobisaki   197 11月 20 08:36 package.json$ npm i$ ts-node faceDetection.ts/usr/local/lib/node_modules/ts-node/src/index.ts:261    return new TSError(diagnosticText; diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(3;16): error TS2354: This syntax requires an imported helper but module 'tslib' cannot be found.faceDetection.ts(5;26): error TS2339: Property 'loadFromDisk' does not exist on type 'SsdMobilenetv1'.faceDetection.ts(10;23): error TS2339: Property 'createCanvasFromMedia' does not exist on type 'typeof import(""/home/tobisaki/try2/face-api.js/src/index"")'.faceDetection.ts(11;11): error TS2339: Property 'drawDetection' does not exist on type 'typeof import(""/home/tobisaki/try2/face-api.js/src/index"")'.    at createTSError (/usr/local/lib/node_modules/ts-node/src/index.ts:261:12)    at getOutput (/usr/local/lib/node_modules/ts-node/src/index.ts:367:40)    at Object.compile (/usr/local/lib/node_modules/ts-node/src/index.ts:558:11)    at Module.m._compile (/usr/local/lib/node_modules/ts-node/src/index.ts:439:43)    at Module._extensions..js (internal/modules/cjs/loader.js:733:10)    at Object.require.extensions.(anonymous function) [as .ts] (/usr/local/lib/node_modules/ts-node/src/index.ts:442:12)    at Module.load (internal/modules/cjs/loader.js:620:32)    at tryModuleLoad (internal/modules/cjs/loader.js:560:12)    at Function.Module._load (internal/modules/cjs/loader.js:552:3)    at Function.Module.runMain (internal/modules/cjs/loader.js:775:12)------------- tried-and-result -------------","['So Sorry!!I overlook faild packages when npm inisatll.Vulnerability Audited package was found by command(npm audit).                       === npm audit security report ===                                                                                                        \\# Run  npm install --save-dev karma-typescript@3.0.13  to resolve 1 vulnerability...It was fixed by command(npm audit fix).But ...My environment not have GUP. Result was$ ts-node faceDetection.ts 2018-11-20 15:06:58.359383: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMAcpu backend was already registered. Reusing existing backend2018-11-20 15:07:02.048092: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.2018-11-20 15:07:02.068154: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.2018-11-20 15:07:02.082211: W tensorflow/core/framework/allocator.cc:113] Allocation of 16777216 exceeds 10% of system memory.====='; 'I\'m seeing the same problem. The suggested fix doesn\'t seem to apply in my case. This is what I get (after manually installing tslib; which it was complaining about first):```% ts-node -vts-node v7.0.1node v11.2.0typescript v3.2.1los@ssdbuntu:~/src/face-api.js/examples/examples-nodejs^master ±% ts-node faceDetection.ts/home/los/n/lib/node_modules/ts-node/src/index.ts:261    return new TSError(diagnosticText; diagnosticCodes)           ^TSError: ⨯ Unable to compile TypeScript:faceDetection.ts(5;26): error TS2339: Property \'loadFromDisk\' does not exist on type \'SsdMobilenetv1\'.faceDetection.ts(10;23): error TS2339: Property \'createCanvasFromMedia\' does not exist on type \'typeof import(""/home/los/src/face-api.js/src/index"")\'.faceDetection.ts(11;11): error TS2339: Property \'drawDetection\' does not exist on type \'typeof import(""/home/los/src/face-api.js/src/index"")\'.    at createTSError (/home/los/n/lib/node_modules/ts-node/src/index.ts:261:12)    at getOutput (/home/los/n/lib/node_modules/ts-node/src/index.ts:367:40)    at Object.compile (/home/los/n/lib/node_modules/ts-node/src/index.ts:558:11)    at Module.m._compile (/home/los/n/lib/node_modules/ts-node/src/index.ts:439:43)    at Module._extensions..js (internal/modules/cjs/loader.js:733:10)    at Object.require.extensions.(anonymous function) [as .ts] (/home/los/n/lib/node_modules/ts-node/src/index.ts:442:12)    at Module.load (internal/modules/cjs/loader.js:620:32)    at tryModuleLoad (internal/modules/cjs/loader.js:560:12)    at Function.Module._load (internal/modules/cjs/loader.js:552:3)    at Function.Module.runMain (internal/modules/cjs/loader.js:775:12)```Any help would be appreciated=====']",Build & Install Failure,Build & Initialization Failure,Dependency Error,,,change dependency version,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] TypeScript Compilation Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,B.2
https://github.com/Volcomix/virtual-background/issues/15,Can't resolve 'fs' and 'path',4,closed,2021-05-30T05:04:33Z,2021-05-30T14:38:48Z,"I'm trying to add the virtual background to an angular 11 app. When calling 'ng serve' I'm getting the following errors:```bashError: ./src/tflite/tflite.jsModule not found: Error: Can't resolve 'fs' in 'src/tflite'Error: ./src/tflite/tflite.jsModule not found: Error: Can't resolve 'path' in 'src/tflite'```Adding this to package.json resolve the issue. Is there a way to fix the errors without changing package.json or other config files?```json""browser"": {    ""fs"": false;    ""path"": false};","[""I think you could try to set a specific [target environment](https://github.com/emscripten-core/emscripten/blob/087ca39beeb6203ac838925f4a3e9c67623fb2fc/src/settings.js#L626) in tflite [`BUILD`](https://github.com/Volcomix/virtual-background/blob/main/tflite/BUILD) file:```-s ENVIRONMENT='web'```=====""; ""I'll try to build the model with the ENVIRONMENT. Shouldn't it be the default from this project?=====""; 'Could be the default indeed; it would probably reduce the js file size.====='; 'Works great. Thanks=====']",Initialization Faliure,Build & Initialization Failure,Incorrect Code Logic,,,build/install configuration,Modifying dependency configuration,Third-party library,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Module Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.4
https://github.com/tensorflow/tfjs/issues/5498,Obtain a different model by converting the tensorflow JS (tfjs) to python (keras),3,closed,2021-08-16T01:56:54Z,2021-08-16T19:34:50Z,**System information**- Have I written custom code : [download](https://drive.google.com/file/d/10alQ8HIwix_EcWYLO6X8Jt5-rw1wA6PF/view?usp=drivesdk)- OS Platform and Distribution : MacOS v11.0.1- Mobile device : None- TensorFlow.js installed from (npm or script link): script link https://unpkg.com/ml5@0.6.1/dist/ml5.min.js- TensorFlow.js version (use command below): P5 v1.4.0; ML5 v0.6.1- Browser version: Chrome v92.0.4515.107 - Tensorflow.js Converter Version: v3.7.0**Describe the current behavior**The goal is to convert the tensorflow JS (tfjs) to python (keras). However; input the same parameters to both tfjs and keras will get the different results.**Describe the expected behavior**If we input the same parameters to both tfjs and keras; the result should be the same.**Standalone code to reproduce the issue**Step1. Construct a simple model to learn the summation of two units digits by using tfjs. Then; train the model.Step2. Save the trained model; which automatically generates three files; such as “model_meta.json”; “model.json”; “model.weights.bin”.Step3. (js2h5.py)Convert three files from step2 to python; and obtained a file called “model.h5”Step4. Using python (keras) to read “model.h5”Step5. Input the same arguments to predict the result. However; the result is different between tfjs and keras. For example; input 8+4; tfjs gets 12.5162(sum.html); but keras gets 1(predict.py).To clarify this problem; my approach is to use the official released tool “tensorflowjs_converter” to convert “model.h5” to tfjs. However; it only outputs two files “group1-shard1of1.bin” and “model.json”.By using the binary compared tool; we can find that:“model.json” is different from the file “model.json” obtained from step2.“group1-shard1of1.bin” is same as the file “model.json” obtained from step2.Furthermore; if I use tfjs to read both “group1-shard1of1.bin” and “model.json”; it shows an error message:Uncaught (in promise) Error: Weight file with basename 'group1-shard1of1.bin' is not provided.,"['Hi @wirlsawyer ; we will not be supporting https://unpkg.com/ml5@0.6.1/dist/ml5.min.js as this is thrid party library ; please reach out to ml5. You can follow these below using latest tfjs version 1) train the model and save using [model.save](https://js.tensorflow.org/api/latest/#tf.LayersModel.save).2) then convert the model using tfjs-converter3) Load the model in Keras using tf.keras.load_model(...)====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5498"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5498"">No</a>====='; 'Closing this as this is not a bug or feature request for tfjs library.=====']",Incorrect Functionality,Incorrect Functionality,Unknown,,,,,Third-party library,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2""
  }
}
```",D,E
https://github.com/amandeepmittal/mobilenet-tfjs-expo/issues/1,Fetch from tensorflow js fails,9,open,2019-11-25T02:10:55Z,2020-06-11T09:27:47Z,After selecting an image the fetch request from tfjs fails `Network request failed` `- node_modules\@tensorflow\tfjs-react-native\dist\platform_react_native.js:80:49 in onerror``- node_modules\event-target-shim\lib\event-target.js:172:43 in dispatchEvent``- ... 8 more stack frames from framework internals`,"[""As correctly noted from the article comments this is caused by the fetch request from tfjs which only supports network fetch requests and not local requests`const response = await fetch(imageAssetPath.uri; {}; { isBinary: true })` will fail `const response = await fetch('https://testimage.jpg'; {}; { isBinary: true })` will succeed I will look into what can be done to fix this =====""; ""Hey @daniel-sudz this is a great. If you are able to fix this; let me know; and with your permission; I'd add the you use case as well as your solution in the post itself. I haven't played much with tfjs tbh; and am just exploring the use cases just like you. :)I'll let the issue open; until you decide to close it.=====""; 'Hi; is there an update on this ? I ran into the same error====='; '@Hasherz96 I think TFJS now supports local requests in React Native. I will be updating the article/repo soon.====='; 'any updates on this?====='; '@amandeepmittal have you been able to sucessfully request local files through the fetch function with the latest version of TFJS?====='; ""Does anyone have any idea why it's working on an iOS simulator; but it crashes on an Android mobile device (not emulator)?=====""; 'Hey @amandeepmittal; I have made some changes and opened a pull request that fixes the problem on android but I was not able to test it on ios (not having an ios device) maybe you can test it on ios and let me know if there is anything that needs to be fixed :smile: ====='; 'Hey @sarthakpranesh thanks for the PR. Let me test on iOS and get back to you.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change API,Replace API with another effective one,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Network Request Failed""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/infinitered/nsfwjs/issues/457,[React] Cannot call a class as a function error in production build,8,closed,2021-01-04T18:52:38Z,2021-01-04T22:22:39Z,GitHub Repo: https://github.com/amanvishnani/firegramLive URL: https://firegram-aman.web.app/I get the following error on the production build however not on the Development build. Not sure what is causing this issue.![image](https://user-images.githubusercontent.com/8425802/103568346-c8299d80-4e9b-11eb-926c-55c2b2de8383.png)Can someone please help me out what's the issue here?,"['@amanvishnani - is your GitHub repo private?====='; '@GantMan sorry about that; just made it public. Try now.====='; ""Couple of notes:Don't use `var` here.   It willl hoist and possibly cause issues with your useStatehttps://github.com/amanvishnani/firegram/blob/a9ac785a6ea2af9d78cd692bfde72b0bb882a007/src/hooks/useNsfw.js#L12Try to stay away from using `var` anytime you can; instead use `let`.Also; I'm not sure if my hook knowledge is limited; but what's stopping your second useEffect from firing before your first useEffect is complete?Just a few things I noticed; but I'm not sure why it's calling a class.=====""; '@GantMan the dependency array in the second useEffect tells it to only run when either `model` or `imageElement` is changed.====='; ""But doesn't that run on the first time setting it?  My useEffect knowledge is always sticky.   Hah; I usually have to fiddle for 10 minutes to get it right :D   Is the issue the model code 100%?  Have you tried switching NSFWJS out with any old object?=====""; 'Yes; although it is called for the initial values which are `null`; the if-condition will not run the code for the first time. I have tried with the v2.2.0 only. Did not try old models or previous versions. Not sure what is causing the issue when the build changes from Dev-build to Prod-build.====='; 'I just found this:   https://github.com/tensorflow/tfjs/issues/3384Take a look at the webpack.confi.js adjustment they recommend.====='; 'Downgrading tfjs to 1.7.4 worked for me. Thanks.=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,,,change framework version,Changing version,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,B.1
https://github.com/infinitered/nsfwjs/issues/522,Cannot load model from file,5,open,2021-06-21T04:41:22Z,2021-07-03T21:53:55Z,I am trying to load a model from local directory using `const model = await nsfwjs.load('file://./model/');`and it gives me error saying `TypeError: Only HTTP(S) protocols are supported`. What can I do?,"[""just use path in load. In your case relative path:const model = await nsfwjs.load('./model/');=====""; 'I tried that but it gives another error`TypeError: Only absolute URLs are supported`====='; ""@kavinplays - please create a small repo that reproduces this issue; and I'll find a fix.=====""; 'Dear **[@GantMan](https://github.com/GantMan)**; would you kindly share for community full repo of your nsfwjs.com; because not all users here have advanced skills in tensorflow====='; '@intbw - NSFWJS.com is available here:  https://github.com/infinitered/nsfwjs/tree/master/example/nsfw_demo=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,parameter modifier,Modify API Parameter usage,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/infinitered/nsfwjs/issues/524,NSFW model is loaded over `HTTP` by default; instead of `HTTPS`,13,closed,2021-06-26T14:06:31Z,2021-06-26T19:32:49Z,https://github.com/infinitered/nsfwjs/blob/5981af701f650ff57f802e3f2bd980dae9bb63c8/src/index.ts#L30This does not work on HTTPS websites due to `mixed content` errors.,"[""I'm a bit torn here:1.  It would be best if you just pull down the model and host it locally.  Rather than depend on the cloudfront version.2.  What would be a good API?Thinking out loud:`load````jsawait nsfwjs.load('/path/to/model/directory/')````load cloudfront model````jsawait nsfwjs.infiniteRedHosted()// AND this for HTTPS?await nsfwjs.infiniteRedHosted({secure: true})```Infinite Red is fine hosting the model for free; but I think sometimes people have no clue they are depending on it.  Maybe a secondary function that is explicit is smart.=====""; ""I don't think there is any reason to load it over `http` instead of `https` (https is a better default in any case)On another note; why can't we host the model on github itself; and serve it statically from `https://raw.githubusercontent.com/` ?=====""; ""Good point.Would you like to do the PR to make the default https for cloudfront?   Basically a PR adding an 's'?As for github vs cloudfront; there's definitely a speed difference; but that's a good suggestion.=====""; ""I've noticed that the CloudFront URL also throws a CORS error when you fix `http` to `https`.```Access to fetch at 'https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json' from origin 'https://www.google.com' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs; set the request's mode to 'no-cors' to fetch the resource with CORS disabled.```However; I think the CORS issue won't happen if you host the same thing on `https://raw.githubusercontent.com/`.So; adding an 's' won't really fix it on browsers (it will work in `node` though; it probably does already).=====""; 'My CORS knowledge is a bit shaky.I tested the HTTPS here:https://www.test-cors.org/#?client_method=GET&client_credentials=false&server_url=https%3A%2F%2Fd1zv2aa70wpiur.cloudfront.net%2Ftfjs_quant_nsfw_mobilenet%2Fmodel.json&server_enable=true&server_status=200&server_credentials=false&server_tabs=remoteIt seems to load with no issue.  Maybe I need more context?   ====='; 'If you use `nsfwjs` in a chrome extension; you\'ll need to load it over an origin that\'s not `d1zv2aa70wpiur.cloudfront.net`. For example; if you run the extension on `google.com`; it will try to fetch `model.json` on Google; which is not the same origin as `d1zv2aa70wpiur.cloudfront.net`; so the S3 servers will throw a CORS error.You can simulate the same thing by running a simple `fetch` in your browser\'s console on `google.com`.```jsfetch(\'https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json\').then((response) => console.log(response));```![image](https://user-images.githubusercontent.com/42958812/123518999-4a19b700-d6c6-11eb-925c-86572208939c.png)You can fix this by modifying the permissions on the S3 bucket. Once you select your bucket; go to the `Permissions` tab and scroll down to the `Cross-origin resource sharing (CORS)` section. Here you can enter a JSON to define the behaviour you want. Here\'s an example that allows all origins.```[    {        ""AllowedHeaders"": [            ""*""        ];        ""AllowedMethods"": [            ""GET""        ];        ""AllowedOrigins"": [            ""*""        ];        ""ExposeHeaders"": []    }]```![image](https://user-images.githubusercontent.com/42958812/123519102-e5129100-d6c6-11eb-9ecc-83057df45206.png)You can also use [this](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors-troubleshooting.html) for reference.If you run a fetch similar to the one above but from `raw.githubusercontent.com`; it does not throw a CORS error.```jsfetch(\'https://raw.githubusercontent.com/infinitered/nsfwjs/master/src/nsfwjs.ts\').then((response) => console.log(response));```So if you feel that changing the S3 configuration is a hassle; then you could host the model on Github. I agree that it would be slower; but it would be free (I don\'t know if it\'s free for you to host it on CloudFront/S3; but I assume it\'s not).====='; ""OK; I want you to file all GitHub tickets from now on :D  This was perfect.1.  I updated the permissions on the bucket2.  I tested with the example you gave (from Google) and it now works.I'm going to add you as a contributor to the project for this insight.   I might still move the code to GitHub; but I want to do that in a separate ticket.=====""; 'You\'ve been added.   I\'ll also add the ""code"" tag if you want to do the PR to switch to https![image](https://user-images.githubusercontent.com/997157/123519783-f0d46800-d672-11eb-8e1e-fcb2b726b37d.png)====='; ""Thanks for adding me as a contributor @GantMan ! I've made a PR with the requested change; it will be functional once the CORS issue is fixed. https://github.com/infinitered/nsfwjs/pull/526=====""; 'Please verify the CORS issue is fixed (I verified it did for me); and then you can close this ticket if successful.====='; ""Hey @GantMan ; I just tried it; I seem to be getting a CORS error still.![image](https://user-images.githubusercontent.com/42958812/123521865-f6639980-d6d6-11eb-8fa7-ded232ace9d1.png)I pasted this:```jsfetch('https://d1zv2aa70wpiur.cloudfront.net/tfjs_quant_nsfw_mobilenet/model.json').then((response) => console.log(response));```=====""; ""I had to do some extra steps: https://stackoverflow.com/questions/12358173/correct-s3-cloudfront-cors-configurationThis means there's a cache that can take a little while.  Try again and let me know.![image](https://user-images.githubusercontent.com/997157/123522692-8298a100-d684-11eb-86c9-4f3bba9c1b3b.png)=====""; ""@GantMan; it is working now. I think it was a caching issue. I'll close this ticket; thanks for your support!=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] CORS Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/103,How to use my model in face-api to recognize face,4,closed,2018-10-15T08:43:58Z,2018-10-18T01:07:49Z,The face-api.js throw error when I load my model. I found my model different with face-api's model by checked my model json .How to use my model in face-api to recognize face?,"['Which model are you using? What kind of error is thrown?====='; ""The error is ' expected weightMap[conv32_down/conv/filters] to be a Tensor4D; instead have undefined'.There is a picture for my code.![default](https://user-images.githubusercontent.com/39442460/46986537-88dbce80-d122-11e8-8fb4-e87f1866ea47.png)My model has trained to use face recognition.My model's data structure didn't same with face-api's model.It look like no some parameters in my model.![default](https://user-images.githubusercontent.com/39442460/46987068-19b3a980-d125-11e8-92a9-cae8f7f0499c.png)=====""; 'You can not simply load your own models; this library only allows you to load the provided models. If you want to load own models; use tfjs.====='; 'Thanks;I tried other palne to finish it.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change API,Replace API with another effective one,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/182,Latest build broke detection: detector never resolves,3,closed,2019-01-04T06:59:11Z,2019-01-06T07:20:37Z,The latest changes have broken face-api for us. The last commit to work is 68ff853005635c4669c6452bd7bbee6a312e3dc0. Essentially the detector promise never resolves.We setup this way (all weights are loaded):```var options = new window.faceapi.TinyFaceDetectorOptions();var detector = window.faceapi.detectSingleFace(video; options).withFaceLandmarks().withFaceDescriptor();```Then we call```detector.run().then(function(results){.... }))```This used to work before. However; if we remove `.withFaceLandmarks().withFaceDescriptor()` it **does** work with latest commit.We tried rebuilding .. to no avail.,"[""Hmm odd; I can not reproduce this issue. Are you sure this doesn't result in an unhandled Promise rejection? Did you load all the models?Maybe you could set up a small repo for me to reproduce this issue?=====""; 'Thanks so much for responding! You are amazing! Yes; all the models are loaded. I have a feeling it could be a tensorflow issue or we are not handling promise rejection properly with the manual approach vs await.Will require some serious debugging on our side. Will update this issue when we figure it out.====='; ""We figure it out!We were creating the `detector` before the `video` started streaming (it worked before; so we had no reason to assume it would break with new tensorflow image lib). Want us to help with the doc in the README so it's clear to other devs?=====""]",promise resolves failure,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.0] Detector promise never resolves""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A,A.4
https://github.com/justadudewhohacks/face-api.js/issues/494,False positive examples. How to improve matching accuracy?,2,open,2019-12-05T08:20:07Z,2019-12-06T23:33:03Z,This is a great library to detect faces; landmarks and expressions. Is it possible to improve the accuracy of face matching? I found that Margot Robbie and Jaime Pressly often produce false positive matches. I tested the same pictures using Face++ and it also produced false positive matches for them (distance is 0.42 below).![face-api-js-jaime-and-margo](https://user-images.githubusercontent.com/7564876/70215614-78f6c380-1703-11ea-88d9-c1643c153e9b.png)Elijah Woods and Daniel Radcliffe also produced false positives matches (distance is 0.38 below).![face-api-js-elijah-and-daniel](https://user-images.githubusercontent.com/7564876/70215708-b3f8f700-1703-11ea-910d-56b588cd0d96.png)They didn't score high using Face++ and all pictures of Elijah and Daniel did not match.Is it possible to use these examples to improve the CNN model(s) or the euclidean distance algorithm? What tools were used to create the TensorFlow models?Thank you.,"['The problem is not the models; you are comparing two unaligned faces with each other; which is invalid input for the face recognition model. The BBT Face Similarity example works on aligned and cropped face images as you can see from the initial images provided in the example.You first have to perform face detection + alignment; which is done by the example shown in the ""Face Recognition"" tab of the web GUI.====='; '@justadudewhohacks; I will try that out. Thanks so much.=====']",Incorrect Functionality,Incorrect Functionality,API Misuse,,,add input preprocess,Add data processing,web application,Data Processing,"{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}",D,A.3
https://github.com/justadudewhohacks/face-api.js/issues/517,Documentation confusion: It's there a way to load the models from a variable instead that from the files and use it later,3,open,2020-01-03T19:31:09Z,2021-05-05T16:23:50Z,In the documentation looks like the only way to load the models and pass it to the instance is from the files on the static folder:`await faceapi.nets.ssdMobilenetv1.loadFromUri('/models')`But i have observed that we can use a Float32Array but i can't find a way to pass it to the faceapi instance; this is what i have tried:`let net = new faceapi.TinyFaceDetector()``let weights = await faceapi.fetchNetWeights('/models/tiny_face_detector_model-shard1.weights')``console.log('weights')``console.log(weights)``net.load(weights)``console.log('net')``console.log(net)``await faceapi.nets.tinyFaceDetector.loadFromWeightMap(weights)` -> In this line fails`const detection = await faceapi.detectSingleFace(contextForFaceDetection.canvas; new faceapi.TinyFaceDetectorOptions({ scoreThreshold: configuration.FACE_SCORE_ADMISSION }))`But i get:![image](https://user-images.githubusercontent.com/16965508/71744463-b1420c80-2e3d-11ea-8850-0d64e3bb7265.png)Can someone help me to clarify this; in the documentation is not so clear how i can load the model without using the methods to access to the files?The model weights a lot and i wuold like so save it into a variable to reuse it using the localStorage or the IndexedDB,"['> await faceapi.nets.tinyFaceDetector.loadFromWeightMap(weights)Why are you calling `loadFromWeightMap` after loading the weights into your local net instance?`await faceapi.nets.tinyFaceDetector.load(weights)` should do the job for you.====='; '@lagcamgc has your problem been solved? I found `tiny_face_detector_model.bin` and `ssd_mobilenetv1_model.bin` seems to have some problems:```jsimport * as faceapi from ""face-api.js"";const float32Array_1 = await faceapi.fetchNetWeights(  ""/weights/face_landmark_68_tiny_model.bin"");console.log(float32Array_1); // [6.35521664824198e+30; -1.8255705378378978e-13; ...]const float32Array_2 = await faceapi.fetchNetWeights(  ""/weights/tiny_face_detector_model.bin"");console.log(float32Array_2); // Error: byte length of Float32Array should be a multiple of 4```![1](https://user-images.githubusercontent.com/47501692/117122964-02169c00-adc9-11eb-9fad-1578c2922c45.png)and all models will fail to load:```jsconst net = new faceapi.FaceLandmark68TinyNet();const float32Array_1 = await faceapi.fetchNetWeights(  ""/weights/face_landmark_68_tiny_model.bin"");await net.load(float32Array_1); // Error: Based on the provided shape; [1;1;32;32]; the tensor should have 1024 values but has 578```![2](https://user-images.githubusercontent.com/47501692/117122980-093daa00-adc9-11eb-8930-fe959e047b6b.png)I am using the module provided by [@vladmandic/face-api ](https://github.com/vladmandic/face-api). Even though I am using the [face-api.js](https://github.com/justadudewhohacks/face-api.js/tree/master/weights) module; I still make the above error. Am I missing something?====='; 'Hello! @awdr74100 at the end of the day i end up using a library called picojs; so short answer... no; i was not able not make it work even with the help of the previous comment=====']",Data & Model Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Model Loading,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""Document Error""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""Confused Document""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/675,sending faceMatcher as the response from node API,4,open,2020-08-03T20:36:34Z,2020-08-12T17:28:02Z,"Very Urgent: Please help!Node:require(""@tensorflow/tfjs-node"");const faceapi = require(""face-api.js"");const canvas = require(""canvas"");const path = require(""path"");const { promisify } = require(""util"");const readdir = promisify(require(""fs"").readdir);const stat = promisify(require(""fs"").stat);const router = require(""express"").Router();const loadLabelledFaceImages = async () => {  await faceapi.nets.tinyFaceDetector.loadFromDisk(    `${__dirname}/../face-models/`  );  await faceapi.nets.faceLandmark68Net.loadFromDisk(    `${__dirname}/../face-models/`  );  await faceapi.nets.faceRecognitionNet.loadFromDisk(    `${__dirname}/../face-models/`  );  await faceapi.nets.ssdMobilenetv1.loadFromDisk(    `${__dirname}/../face-models/`  );  await faceapi.nets.faceExpressionNet.loadFromDisk(    `${__dirname}/../face-models/`  );  await faceapi.nets.ageGenderNet.loadFromDisk(`${__dirname}/../face-models/`);  //joining path of directory  let directoryPath = path.join(__dirname; ""../labelled-images"");  //passsing directoryPath  const files = await readdir(directoryPath);  return Promise.all(    files.map(async (file) => {      const descriptions = [];      for (let i = 0; i <= 39; i++) {        const imageLink = `${directoryPath}/${file}/${i}.jpg`;        const image = await canvas.loadImage(imageLink);        const detections = await faceapi          .detectSingleFace(image)          .withFaceLandmarks()          .withFaceDescriptor();        descriptions.push(detections.descriptor);      }      return new faceapi.LabeledFaceDescriptors(file; descriptions);    })  );};router.get(""/getimagesdata""; async (req; res) => {  const labeledFaceDescriptors = await loadLabelledFaceImages();  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6);  res.status(200).send({ faceMatcher });});module.exports = router;Frontend Angular:faceRecognition = async () => {    this.userManagementPermissionService.getImagesData().subscribe((res) => {      const faceMatcher = res.faceMatcher;      const faceCanvas = faceapi.createCanvasFromMedia(        this.video.nativeElement      );      this.webcam.nativeElement.append(faceCanvas);      faceCanvas.classList.add('face-canvas');      const displaySize = {        width: this.video.nativeElement.width;        height: this.video.nativeElement.height;      };      faceapi.matchDimensions(faceCanvas; displaySize);      let x: any = [];      let y: any = [];      let width: any = [];      let height: any = [];      let resizedDetections: any = [];      setInterval(async () => {        const detection = await faceapi          .detectAllFaces(            this.video.nativeElement;            new faceapi.TinyFaceDetectorOptions()          )          .withFaceLandmarks()          .withFaceDescriptors();        resizedDetections = faceapi.resizeResults(detection; displaySize);        faceCanvas          .getContext('2d')          .clearRect(0; 0; faceCanvas.width; faceCanvas.height);        const results = resizedDetections.map((d: any) => {          return faceMatcher.findBestMatch(d.descriptor);        });        results.forEach((result: any; i: any) => {          const box = resizedDetections[i].detection.box;          const drawBox = new faceapi.draw.DrawBox(box; {            label: result.toString();          });          drawBox.draw(faceCanvas);        });      }; 100);I am getting error : TypeError: faceMatcher.findBestMatch is not a functionTypeError: faceMatcher.findBestMatch is not a functionPlease do help!! Thanks in advance![error](https://user-images.githubusercontent.com/25694814/89224897-e5112a80-d5f6-11ea-9a09-041d3867a6e4.png)","['Please guys; If anyone can provide the solution====='; ""my best guess is that you don't have faceapi 'loaded' or in state somewhere in the sense it is instantiated and can access it's methods. I'm in the react world and had to re-purpose react-use-faceapi to access the methods without error=====""; 'actually this is a MEAN stack project and just to load the require(""@tensorflow/tfjs-node""); i had to install node v10.16.3 as it  was not compatible and throwing error. I used   ""@tensorflow/tfjs-node"": ""^1.2.11""; to make it compatible; I have no idea what is the issue; working great with just used on angular but photos been saved to server that\'s why wanna run it on server.====='; ""if I am not wrong; you can't load the Facematcher on the server and then pass it onto the client. I don't think that will work. What you can do; is create the descriptors and save that as a json. And send the json file to the client and then load it on the client side. You would still need to initiate the faceMatcher on the client side except you don't have to create the descriptors all over again. Just load them. =====""]",Reference Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/423,Face Recognition on Webcam Implementation: Cannot read property 'findBestMatch' of null,1,closed,2019-09-23T16:37:47Z,2019-10-26T07:58:39Z,"Urgent: Facing this issue in the implementation of face recognition on the webcam camera for the browser. Getting ""Uncaught (in promise) TypeError: Cannot read property 'findBestMatch' of null"" Error even though the input of findBestMatch is a valid float32 array (see image below for details)![image](https://user-images.githubusercontent.com/40192780/65444611-f92abe00-de62-11e9-9bb8-2fb3a992a2ac.png)Anyone has a codebase of successful implementation of webcam with face-recognition? Or webcam with facial recognition and emotion detection combined would be an immense help. Thanks in advance!Attached the current codebase to share:  ```// DETECT FACES  const video = document.getElementById('video')  let faceMatcher = null;  Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri('/models');    faceapi.nets.faceLandmark68Net.loadFromUri('/models');    faceapi.nets.faceRecognitionNet.loadFromUri('/models');    faceapi.nets.faceExpressionNet.loadFromUri('/models');    faceapi.nets.ssdMobilenetv1.loadFromUri('/models')  ]).then(start)  function startVideo() {    navigator.getUserMedia(      { video: {} };      stream => video.srcObject = stream;      err => console.error(err)    )  }  async function start() {    startVideo()    console.log(""Started"")    const labeledFaceDescriptors = await loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6)    console.log(labeledFaceDescriptors)    console.log(""Mid"")    console.log(""END"")  }  video.addEventListener('play'; () => {    const canvas = faceapi.createCanvasFromMedia(video)    document.body.append(canvas)    const displaySize = { width: video.width; height: video.height }    faceapi.matchDimensions(canvas; displaySize)    setInterval(async () => {      // FACE REC      faceapi.matchDimensions(canvas; displaySize)      const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors()      const resizedDetections = faceapi.resizeResults(detections; displaySize)      const results = resizedDetections.map(d => {        console.log(d.descriptor)        faceMatcher.findBestMatch(d.descriptor)      })      results.forEach((result; i) => {        const box = resizedDetections[i].detection.box        const drawBox = new faceapi.draw.DrawBox(box; { label: result.toString() })        drawBox.draw(canvas)      })    }; 150)  })  function loadLabeledImages() {    const labels = ['Jeremy'; 'Black Widow'; 'Captain America'; 'Captain Marvel'; 'Hawkeye'; 'Jim Rhodes'; 'Thor'; 'Tony Stark']    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 1; i <= 2; i++) {          const img = await faceapi.fetchImage(`./labeled_images/${label}/${i}.jpg`)          const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()          descriptions.push(detections.descriptor)        }        return new faceapi.LabeledFaceDescriptors(label; descriptions)      })    )  }```","[""In the start method of your code you are creating a local variable of faceMatcher:> const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6)Thus your global faceMatcher instance is null. That's why you are receiveing this error.=====""]",Reference Error,Crash,Incorrect Code Logic,,,variable replacer,variable replacer,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/130,Error at faceapi.drawDetection(canvas; boxesWithText).,11,closed,2018-11-08T01:31:35Z,2020-07-13T11:26:17Z,"I am trying to implement the use case list on below sitehttps://itnext.io/realtime-javascript-face-tracking-and-face-recognition-using-face-api-js-mtcnn-face-detector-d924dd8b5740i am able to do face detection but getting issue in face recognition with some investigation it looks like boxesWithText is giving empty output which is causing drawDetection to fail.Error Uncaught (in promise) TypeError: Cannot read property ‘x’ of undefined at VM1409 face-api.min.js:1 at Array.forEach (<anonymous>) at Object.t.drawDetection (VM1409 face-api.min.js:1) at run2 (VM1410 index.js:75)JS code``` $(document).ready(function() {       run1()})async function run1() {    const MODELS = ""http://localhost:8000/Desktop/FaceID/Face%20Detection%20with%20webcam/models""; // Contains all the weights.    await faceapi.loadSsdMobilenetv1Model(MODELS)    await faceapi.loadFaceLandmarkModel(MODELS)    await faceapi.loadFaceRecognitionModel(MODELS)    // try to access users webcam and stream the images  // to the video element const videoEl = document.getElementById('inputVideo')  navigator.getUserMedia(    { video: {} };    stream => videoEl.srcObject = stream;    err => console.error(err))}async function run2() {    const mtcnnResults = await faceapi.ssdMobilenetv1(document.getElementById('inputVideo'))overlay.width = 500overlay.height = 400const detectionsForSize = mtcnnResults.map(det => det.forSize(500; 400))faceapi.drawDetection(overlay; detectionsForSize; { withScore: true })    const input = document.getElementById('inputVideo')const fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()        const labels = ['sheldon';'ravish']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `http://localhost:8000/Desktop/${label}.png`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it's landmarks and face descriptor    const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }        const faceDescriptors = [fullFaceDescription.descriptor]   // console.log(label)    return new faceapi.LabeledFaceDescriptors(label; faceDescriptors)  }))// 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; maxDescriptorDistance) //console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))const boxesWithText = results.map((bestMatch; i) => {  const box = fullFaceDescriptions[i].detection.box  const text = bestMatch.toString()  const boxWithText = new faceapi.BoxWithText(box; text)})faceapi.drawDetection(overlay; boxesWithText)}async function onPlay(videoEl) {    run2()    setTimeout(() => onPlay(videoEl))} ```","['You forgot the return statement in your map function:``` javascriptconst boxesWithText = results.map((bestMatch; i) => {  const box = fullFaceDescriptions[i].detection.box  const text = bestMatch.toString()  const boxWithText = new faceapi.BoxWithText(box; text)  return boxWithText})```Fixed the gist...====='; 'Hi; I am attempting to get this example from the blogpost at itnext.io running as well; but have a few issues. Does anyone have the full source code (HTML + JS) for this; so I can see what I am missing?====='; ""const video = document.getElementById('video')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri('/models');  faceapi.nets.faceLandmark68Net.loadFromUri('/models');  faceapi.nets.faceRecognitionNet.loadFromUri('/models');  faceapi.nets.faceExpressionNet.loadFromUri('/models');  faceapi.nets.ssdMobilenetv1.loadFromUri('/models')]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} };    stream => video.srcObject = stream;    err => console.error(err)  )}video.addEventListener('play'; async () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const labeledFaceDescriptors = await loadLabeledImages()  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6)  const displaySize = { width: video.width; height: video.height }  faceapi.matchDimensions(canvas; displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors();    const resizedDetections = faceapi.resizeResults(detections; displaySize)      canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)    if(resizedDetections.length>0){      const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))      results.forEach((result; i) => {               const box = resizedDetections[i].detection.box        const drawBox = new faceapi.draw.DrawBox(box; { label: result.label.toString() })        drawBox.draw(canvas)      })        }      // faceapi.draw.drawDetections(canvas; resizedDetections)    // faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)    // faceapi.draw.drawFaceExpressions(canvas; resizedDetections)  }; 100)})function loadLabeledImages() {   const labels = ['Black Widow'; 'Captain America'; 'Captain Marvel'; 'Hawkeye'; 'Jim Rhodes'; 'Thor'; 'Tony Stark']  return Promise.all(    labels.map(async label => {      const descriptions = []      for (let i = 1; i <= 1; i++) {        const img = await faceapi.fetchImage(`images/${label}.png`)        const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        descriptions.push(detections.descriptor)        debugger;      }      return new faceapi.LabeledFaceDescriptors(label; descriptions)    })  )}=====""; 'while running my code;this error shows;2141script.js:32 Uncaught (in promise) TypeError: faceapi.draw.AgeAndGender is not a function    at script.js:32please help me to solve this====='; 'There is no faceapi.draw.AgeAndGender. Try [this](https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/ageAndGenderRecognition.html#L160-L169).====='; 'yeah bro its working.......please tell where  to i learn and understand  the complete face-api;please suggest to work well with face-api====='; 'script.js:66 Uncaught (in promise) ReferenceError: getFaceDetectorOptions is not defined    at updateReferenceImageResults (script.js:66)    at uploadImage (script.js:50)this is my error====='; ""//const video = document.getElementById('video')const input = document.getElementById('myImg')  Promise.all([ faceapi.nets.ssdMobilenetv1.loadFromUri('/models');    faceapi.nets.tinyFaceDetector.loadFromUri('/models');   faceapi.nets.faceLandmark68Net.loadFromUri('/models');   faceapi.nets.faceRecognitionNet.loadFromUri('/models');   faceapi.nets.faceExpressionNet.loadFromUri('/models');  faceapi.nets.ageGenderNet.loadFromUri('/models')])//.then(startVideo)//  function startVideo() {//    navigator.getUserMedia(//      { video: {} };//      stream => video.srcObject = stream;//      err => console.error(err)//    )//  }// .then(load)// async function load(){//   const imgFile = document.getElementById('myFileUpload').files[0]//   // create an HTMLImageElement from a Blob//   const img = await faceapi.bufferToImage(imgFile)//   document.getElementById('myImg').src = img.src//   const canvas = faceapi.createCanvasFromMedia(document.getElementById('myImg').src)//   const displaySize = { width: input.width; height: input.height }//   faceapi.matchDimensions(canvas; displaySize)// }// async function uploadImage() { //   const detections1 = await faceapi.detectSingleFace(input).withFaceLandmarks()//   const resizedResults = faceapi.resizeResults(detections1; displaySize)//   canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)//   const detectionsWithLandmarks = await faceapi.detectSingleFace(input).withFaceLandmarks()//   faceapi.draw.drawFaceLandmarks(canvas; resizedResults)//   console.log(detections1)// }let faceMatcher=null; async function uploadImage(e) {  const imgFile = document.getElementById('myFileUpload').files[0]  // create an HTMLImageElement from a Blob  const img = await faceapi.bufferToImage(imgFile)  document.getElementById('myImg').src = img.src  updateReferenceImageResults()}async function uploadQueryImage(e) {  const imgFile = document.getElementById('queryImgUploadInput').files[0]  // create an HTMLImageElement from a Blob  const img = await faceapi.bufferToImage(imgFile)  document.getElementById('queryImg').src = img.src  updateQueryImageResults()}async function updateReferenceImageResults() {  const inputImgEl = document.getElementById('#myImg')  const canvas = document.getElementById('#refImgOverlay')  const fullFaceDescriptions = await faceapi    .detectAllFaces(inputImgEl; getFaceDetectorOptions())    .withFaceLandmarks()    .withFaceDescriptors()  if (!fullFaceDescriptions.length) {    return  }  // create FaceMatcher with automatically assigned labels  // from the detection results for the reference image  faceMatcher = new faceapi.FaceMatcher(fullFaceDescriptions)  faceapi.matchDimensions(canvas; inputImgEl)  // resize detection and landmarks in case displayed image is smaller than  // original size  const resizedResults = faceapi.resizeResults(fullFaceDescriptions; inputImgEl)  // draw boxes with the corresponding label as text  const labels = faceMatcher.labeledDescriptors    .map(ld => ld.label)  resizedResults.forEach(({ detection; descriptor }) => {    const label = faceMatcher.findBestMatch(descriptor).toString()    const options = { label }    const drawBox = new faceapi.draw.DrawBox(detection.box; options)    drawBox.draw(canvas)  })}async function updateQueryImageResults() {  if (!faceMatcher) {    return  }  const inputImgEl = document.getElementById('#queryImg')  const canvas = document.getElementById('#queryImgOverlay')  const results = await faceapi    .detectAllFaces(inputImgEl; getFaceDetectorOptions())    .withFaceLandmarks()    .withFaceDescriptors()  faceapi.matchDimensions(canvas; inputImgEl)  // resize detection and landmarks in case displayed image is smaller than  // original size  const resizedResults = faceapi.resizeResults(results; inputImgEl)  resizedResults.forEach(({ detection; descriptor }) => {    const label = faceMatcher.findBestMatch(descriptor).toString()    const options = { label }    const drawBox = new faceapi.draw.DrawBox(detection.box; options)    drawBox.draw(canvas)  })}// video.addEventListener('play'; () => {//   const canvas = faceapi.createCanvasFromMedia(video)//   document.body.append(canvas)//   const displaySize = { width: video.width; height: video.height }//   faceapi.matchDimensions(canvas; displaySize)//   setInterval(async () => {//     const detections = await faceapi.detectAllFaces(video; new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions().withAgeAndGender()//     const resizedDetections = faceapi.resizeResults(detections; displaySize)//     canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)//     faceapi.draw.drawDetections(canvas; resizedDetections)//     faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)//     faceapi.draw.drawFaceExpressions(canvas; resizedDetections)//     resizedDetections.forEach(result=>{//       const{age;gender;genderProbability}=result//       new faceapi.draw.DrawTextField([//         `${faceapi.round(age;0)} years`;//     `${gender}(${faceapi.round(genderProbability)})`];//     result.detection.box.bottomRight).draw(canvas)//     })    // resizedDetections.forEach(res=>{    //   const{age}=res    //   new faceapi.draw.DrawTextField([    //     `${faceapi.round(age;0)} years`];    //     res.detection.box.bottomLeft).draw(canvas)    //   })        // faceapi.draw.drawAgeAndGender(canvas; resizedDetections) // }; 1000)//})=====""; 'there are many comment line so;sorry for thisplease give the solution guys====='; '> You forgot the return statement in your map function:> > ```js> const boxesWithText = results.map((bestMatch; i) => {>   const box = fullFaceDescriptions[i].detection.box>   const text = bestMatch.toString()>   const boxWithText = new faceapi.BoxWithText(box; text)>   return boxWithText> })> ```> > Fixed the gist..its not the proper solution;i have the same issuedrawDetection is not a function..please give solution its frustrating====='; 'can you help my ?? thats wrong in my code ?its can detect face; write score but without recognition >(label name)```  <script>    let forwardTimes = []    let withFaceLandmarks = false    let withBoxes = true    function onChangeWithFaceLandmarks(e) {      withFaceLandmarks = $(e.target).prop(\'checked\')    }    function onChangeHideBoundingBoxes(e) {      withBoxes = !$(e.target).prop(\'checked\')    }    function updateTimeStats(timeInMs) {      forwardTimes = [timeInMs].concat(forwardTimes).slice(0; 30)      const avgTimeInMs = forwardTimes.reduce((total; t) => total + t) / forwardTimes.length      $(\'#time\').val(`${Math.round(avgTimeInMs)} ms`)      $(\'#fps\').val(`${faceapi.utils.round(1000 / avgTimeInMs)}`)    }    async function onPlay(videoEl) {      if(!videoEl.currentTime || videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())        return setTimeout(() => onPlay(videoEl))      const options = getFaceDetectorOptions()      const ts = Date.now()      const drawBoxes = withBoxes      const drawLandmarks = withFaceLandmarks      let task = faceapi.detectAllFaces(videoEl; options)      task = withFaceLandmarks ? task.withFaceLandmarks() : task      const results = await task      updateTimeStats(Date.now() - ts)      const canvas = $(\'#overlay\').get(0)      const dims = faceapi.matchDimensions(canvas; videoEl; true)      const resizedResults = faceapi.resizeResults(results; dims)      if (drawBoxes) {        faceapi.draw.drawDetections(canvas; resizedResults)      }      if (drawLandmarks) {        faceapi.draw.drawFaceLandmarks(canvas; resizedResults)      }      setTimeout(() => onPlay(videoEl))    }    async function run() {      // load face detection and face landmark models      await changeFaceDetector(TINY_FACE_DETECTOR)\t      await faceapi.loadSsdMobilenetv1Model(\'/\')\t\t      await faceapi.loadFaceRecognitionModel(\'/\')      await faceapi.loadFaceLandmarkModel(\'/\')      changeInputSize(416)      // start processing frames const labels = [\'1\';\'2\']const labeledFaceDescriptors = await Promise.all(  labels.map(async label => {    // fetch image data from urls and convert blob to HTMLImage element    const imgUrl = `picture/${label}.JPG`    const img = await faceapi.fetchImage(imgUrl)        // detect the face with the highest score in the image and compute it\'s landmarks and face descriptor    const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()        if (!fullFaceDescription) {      throw new Error(`no faces detected for ${label}`)    }        const faceDescriptors = [fullFaceDescription.descriptor]    console.log(label)    return new faceapi.LabeledFaceDescriptors(label; faceDescriptors)  }))const input = document.getElementById(\'inputVideo\')const fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()      // 0.6 is a good distance threshold value to judge// whether the descriptors match or notconst maxDescriptorDistance = 0.6const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; maxDescriptorDistance) console.log(""face matcher""+faceMatcher)const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))\tresults.forEach((bestMatch; i) => {\tconst box = fullFaceDescriptions[i].detection.box\tconst text = bestMatch.toString()\tconst drawBox = new faceapi.draw.DrawBox(box; { label: text })\tconsole.log(""last"")\t})//  results        onPlay($(\'#inputVideo\').get(0))}\t    function updateResults() {}    $(document).ready(function() {      renderNavBar(\'#navbar\'; \'video_face_tracking\')      initFaceDetectionControls()      run()    })            </script>```=====']",Reference Error,Crash,Incorrect Code Logic,,,add return,add return,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/74,getBox() Not a Function,8,closed,2018-08-22T18:54:59Z,2019-08-01T02:22:57Z,I've tried the recent and last version of the face-api.js and get the same error with both.Uncaught (in promise) TypeError: det.getBox is not a function    at face-api.js:1438    at Array.forEach (<anonymous>)    at Object.drawDetection (face-api.js:1437)    at getFace (facetest.php:41)(anonymous) @ face-api.js:1438drawDetection @ face-api.js:1437getFace @ facetest.php:41async function (async)getFace @ facetest.php:23(anonymous) @ facetest.php:20j @ jquery-3.2.1.min.js:2k @ jquery-3.2.1.min.js:2setTimeout (async)(anonymous) @ jquery-3.2.1.min.js:2i @ jquery-3.2.1.min.js:2fireWith @ jquery-3.2.1.min.js:2fire @ jquery-3.2.1.min.js:2i @ jquery-3.2.1.min.js:2fireWith @ jquery-3.2.1.min.js:2ready @ jquery-3.2.1.min.js:2S @ jquery-3.2.1.min.js:3Any help would be appreciated.,"['Could you show me your code; looks like you are passing something to drawDetections; which is not an instance of FaceDetection.====='; '`<html><head><script src=""https://code.jquery.com/jquery-3.2.1.min.js""></script><!-- <script src=""tensor.js""></script> --><script src=""face-api.js""></script><script src=""commons.js""></script><style>.face {   position: absolute;   border: 2px solid #FFF;}</style></head><body>\t<img id=""picture"" style=""""src=""facetest.jpg"">\t<canvas id=""overlay""></canvas></body><script>\t$(document).ready(function() {\t\tvar image = document.getElementById(\'picture\');\t\tgetFace();\t});\tasync function getFace() {\t\tawait faceapi.loadFaceDetectionModel(\'models/weights\')\t\tawait faceapi.loadFaceLandmarkModel(\'models/weights\')\t\tawait faceapi.loadFaceRecognitionModel(\'models/weights\')\t\tconst myImg = document.getElementById(\'picture\');\t\tconst minConfidence = 0.8\t\tconst maxResults = 10\t\tconst detections = await faceapi.allFaces(myImg; minConfidence)\t\t\t\t\t\t\t// resize the detected boxes in case your displayed image has a different size then the original\t\t//const detectionsForSize = detections.map(det => det.forSize(myImg.width; myImg.height))\t\t//console.log(detectionsForSize);\t\tconst canvas = document.getElementById(\'overlay\')\t\tcanvas.width = myImg.width\t\tcanvas.height = myImg.height\t\t\t\tfaceapi.drawDetection(canvas; detections; {withScore: true});\t\t\t}</script></html>`I\'ve tried including tensorflow; but when I do I get the following error as well.I cant use anything outside of straight .js (in terms of a framework) for the given project and your code seemed to be a perfect fit for what we need to do; (detect faces; I\'d attempt to pull the faces afterwards to be send via API to a 3rd party).(ERROR w/ TENSOR.js included)tf-core.esm.js:17 Uncaught (in promise) Error: Argument \'tensors[0]\' passed to \'concat\' must be a Tensor; but got object.    at assert (tf-core.esm.js:17)    at assertArgumentIsTensor (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at n (tf-core.esm.js:17)    at assertArgumentsAreTensors (tf-core.esm.js:17)    at e.concat (tf-core.esm.js:17)    at tf-core.esm.js:17    at e.tidy (tf-core.esm.js:17)    at n.value (tf-core.esm.js:17)====='; 'I am getting information when I console.log out the original allFaces detection.  So I know its seeing the faces (tho only 4 of the 5 in my test image).====='; 'And ignore anything commented out - I was trying to run it down and left them  Just noticed myself====='; 'Think I may have figured this out.  I appologize; seems it was me not understanding the information.  I retreived the detection from the object and send that to the drawDetection and I got a result as expected.====='; 'Ah yeah you have to get the FaceDetection objects out of the result of allFaces. If anything is still unclear; try to take a look at the what allFaces returns [here](https://github.com/justadudewhohacks/face-api.js#shortcut-functions-for-full-face-description).====='; 'script2.js:38 Uncaught (in promise) TypeError: faceapi.drawDetections is not a function    at onPlay (script2.js:38)I have this error in browser face_detection code====='; '.anyone reply ;;give the solution=====']",Reference Error,Crash,API Misuse,,,add data postprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorspace-team/tensorspace/issues/126,Functional Model predict render bug,1,closed,2018-11-20T07:45:04Z,2018-11-20T07:57:49Z,Functional Model Input and Layer render have bug.,['Fixed during the process of building Inceptionv3 model.====='],Incorrect Functionality,Incorrect Functionality,API Misuse,,,change data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.3
https://github.com/tensorspace-team/tensorspace/issues/126,Functional Model predict render bug,1,closed,2018-11-20T07:45:04Z,2018-11-20T07:57:49Z,Functional Model Input and Layer render have bug.,['Fixed during the process of building Inceptionv3 model.====='],Incorrect Functionality,Incorrect Functionality,API Misuse,,,change data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A,A.3
https://github.com/tensorspace-team/tensorspace/issues/126,Functional Model predict render bug,1,closed,2018-11-20T07:45:04Z,2018-11-20T07:57:49Z,Functional Model Input and Layer render have bug.,['Fixed during the process of building Inceptionv3 model.====='],render bug,Crash,API Misuse,,,change data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.3
https://github.com/tensorspace-team/tensorspace/issues/126,Functional Model predict render bug,1,closed,2018-11-20T07:45:04Z,2018-11-20T07:57:49Z,Functional Model Input and Layer render have bug.,['Fixed during the process of building Inceptionv3 model.====='],render bug,Crash,API Misuse,,,change data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A,A.3
https://github.com/justadudewhohacks/face-api.js/issues/834,edits,3,open,2021-12-07T07:39:33Z,2021-12-07T10:59:14Z,"can we add new densblock and aslo can do edits in models like if we want to add like this { ""name"": ""dense0/conv4/pointwise_filter""; ""shape"": [1; 1; 32; 32]; ""dtype"": ""float32""; ""quantization"": { ""dtype"": ""uint8""; ""scale"": 0.010322785377502442; ""min"": -1.4658355236053466 } };or this { ""name"": ""dense4/conv1/pointwise_filter""; ""shape"": [1; 1; 512; 512]; ""dtype"": ""float32""; ""quantization"": { ""dtype"": ""uint8""; ""scale"": 0.010322785377502442; ""min"": -1.4658355236053466 } }; here i want to add denseblock and conv when i add its shows me error...",['Hi; Can you write the error context?====='; '> Hi; Can you write the error context?![image (1)](https://user-images.githubusercontent.com/58817326/145013932-47192053-fd0e-4527-90fc-8eda7ca25c61.png)![image](https://user-images.githubusercontent.com/58817326/145013937-a15f936b-f334-40b9-a2fb-95008d318a6c.png)====='; 'Oh; I **understood finally**. this library is using tenserflow.js and TensorFlow needs parameters for that.You are not passing enough parameters to TensorFlow. Visit this link ...https://stackoverflow.com/questions/61381112/uncaught-error-based-on-the-provided-shape-1024-3-the-tensor-should-have-30====='],Data & Model Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Improper model edits leading to errors""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/838,Unhandled error in blazor has occured,1,open,2021-12-16T09:23:29Z,2021-12-16T09:25:10Z,I managed to get real time face recognition working in visual studio code however when I moved my codes over to blazor web assembly on visual studio and after setting up JsRuntime on a razor page. When I tried to run the project. I would either get something likeUncaught (in promise) Error: Based on the provided shape; [1;1;16;32]; the tensor should have 512 values but has 417orUncaught (in promise) Error: Based on the provided shape; [1;1;512;9]; the tensor should have 4608 values but has 2018After troubleshooting I still have no clue why this has happened.,['Uncaught (in promise) Error: Based on the provided shape; [1;1;16;32]; the tensor should have 512 values but has 417    at P (face-api.min.js:1)    at Rn (face-api.min.js:1)    at In (face-api.min.js:1)    at e (face-api.min.js:1)    at Vh (face-api.min.js:1)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/680,I'm having an issue with fetching the api can anyone help please.,4,open,2020-08-13T15:43:00Z,2020-09-10T14:00:46Z,"<img width=""732"" alt=""Screen Shot 2020-08-13 at 10 41 20 AM"" src=""https://user-images.githubusercontent.com/65366881/90156070-aa736480-dd51-11ea-9f70-8378f77aa7c6.png"">","[""I fixed this issue about 2 weeks ago by putting all my models in another folder and called that folder but now it's not working and I have no idea why. =====""; 'It looks that the browser does not allow accessing weight data due to CORS. Try to access the data using http:// instead of file://====='; 'pscardoso is correct. You need to be running this behind an http or https server. ====='; 'If you\'re running **face-api** from browser; you must use http or https a fetch does not provide capability to load from file.And if you\'re running **face-api** from nodejs; it can load from file; but it doesn\'t have built-in fetch method at all; so you should provide custom fetch method; something like:```jsconst fetch = require(""node-fetch"");faceapi.env.monkeyPatch({ fetch: fetch });```=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] CORS Issue""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/642,Uncaught (in promise) Error: Based on the provided shape; [25]; the tensor should have 25 values but has 21,3,open,2020-06-18T22:30:11Z,2021-03-27T10:31:30Z,"I have a problem starting the online application in that part:```Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri('./assets/lib/face-api/models');    faceapi.nets.faceLandmark68Net.loadFromUri('./assets/lib/face-api/models');    faceapi.nets.faceRecognitionNet.loadFromUri('./assets/lib/face-api/models');    faceapi.nets.faceExpressionNet.loadFromUri('./assets/lib/face-api/models');    faceapi.nets.ageGenderNet.loadFromUri('./assets/lib/face-api/models');    faceapi.nets.ssdMobilenetv1.loadFromUri('./assets/lib/face-api/models')	]).then(startVideo)```In the console returns the following error:```face-api.js:23 Uncaught (in promise) Error: Based on the provided shape; [25]; the tensor should have 25 values but has 21    at C (face-api.js:23)    at _n (face-api.js:23)    at Fn (face-api.js:23)    at o (face-api.js:23)    at cf (face-api.js:23)    at face-api.js:23    at Array.forEach (<anonymous>)    at face-api.js:23    at Array.forEach (<anonymous>)    at face-api.js:23```If I put the command `console.log (faceapi.nets)` on the console the following appears:```{ssdMobilenetv1: SsdMobilenetv1; tinyFaceDetector: TinyFaceDetector; tinyYolov2: TinyYolov2; mtcnn: Mtcnn; faceLandmark68Net: FaceLandmark68Net; …}ageGenderNet: AgeGenderNet {_name: ""AgeGenderNet""; _params: undefined; _paramMappings: Array(0); _faceFeatureExtractor: TinyXception}faceExpressionNet: FaceExpressionNet {_name: ""FaceExpressionNet""; _params: undefined; _paramMappings: Array(0); _faceFeatureExtractor: FaceFeatureExtractor}faceLandmark68Net: FaceLandmark68Net {_name: ""FaceLandmark68Net""; _params: undefined; _paramMappings: Array(0); _faceFeatureExtractor: FaceFeatureExtractor}faceLandmark68TinyNet: FaceLandmark68TinyNet {_name: ""FaceLandmark68TinyNet""; _params: undefined; _paramMappings: Array(0); _faceFeatureExtractor: TinyFaceFeatureExtractor}faceRecognitionNet: FaceRecognitionNet {_name: ""FaceRecognitionNet""; _params: undefined; _paramMappings: Array(0)}mtcnn: Mtcnn {_name: ""Mtcnn""; _params: undefined; _paramMappings: Array(0)}ssdMobilenetv1: SsdMobilenetv1 {_name: ""SsdMobilenetv1""; _params: undefined; _paramMappings: Array(0)}tinyFaceDetector: TinyFaceDetector {_name: ""TinyYolov2""; _params: undefined; _paramMappings: Array(0); _config: {…}}tinyYolov2: TinyYolov2 {_name: ""TinyYolov2""; _params: undefined; _paramMappings: Array(0); _config: {…}}__proto__: Object```I don't know where it can be wrong or what is wrong; at localhost it worked; I put it online to test it and it just returns me this error. Can you help me? Thank you","['I get exactly the same error only online not localhost====='; ""@nicolau-arbex @XavierLGLS Try this: https://github.com/justadudewhohacks/face-api.js/issues/131#issuecomment-561738466It worked for me.Also; when using `loadFromUri` to load the nets; don't use a file spec; use a uri spec; ie:`faceapi.nets.tinyFaceDetector.loadFromUri('assets/lib/face-api/models')`or`faceapi.nets.tinyFaceDetector.loadFromUri('/assets/lib/face-api/models')`If you want load the nets from files; you must use `LoadFromDisk`; check the docs for more info:https://justadudewhohacks.github.io/face-api.js/docs/index.html=====""; 'Indeed; like @nosuko said; the solution is in this comment : https://github.com/justadudewhohacks/face-api.js/issues/131#issuecomment-561738466Your shard file needs an extension (and change the name with the extension in the json file too)Worked for me !=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/618,TinyYolov2 - load model before inference,6,open,2020-05-08T10:01:11Z,2021-07-30T16:34:12Z,I am getting this error in production build in React Js and working fine when use npm start.```javascriptimport * as faceapi from 'face-api.js'export async function loadModels() {    const MODEL_URL = process.env.PUBLIC_URL + '/models'    await faceapi.loadTinyFaceDetectorModel(MODEL_URL)    await faceapi.loadFaceLandmarkTinyModel(MODEL_URL)    await faceapi.loadFaceRecognitionModel(MODEL_URL)}export async function getFullFaceDescription(blob; inputSize = 512) {    const scoreThreshold = 0.5    const OPTION = new faceapi.TinyFaceDetectorOptions({        inputSize;        scoreThreshold    })    const useTinyModel = true    const img = await faceapi.fetchImage(blob)    const fullDesc = await faceapi        .detectAllFaces(img; OPTION)        .withFaceLandmarks(useTinyModel)        .withFaceDescriptors()}```Please help me on this.,"[""I saw the same error in a situation where I wasn't even using the Yolo model; which was confusing.In the end it was because I had made a coding error; and the models I did want to use weren't actually loaded. Perhaps there is something in the FaceApiJs code where it throws a wrong error for a missing model; and ends up outputting the last name in the list of nets.=====""; 'Hi @swapanil  ;I am faced with same issue. We have same code block. Have you solve your problem? ====='; ""Hey @swapanil ;FYI; I solved my problem. I couldn't load models correctly. Can u check your models location? It should be under the public folder as follows; public/models=====""; 'I made it sure my all models are loading correctly infact my TinyYolov2 json and shard file loading correctly but still it showing me same issue; I replace it with SsdMobilenetv1Options it is working for now. I think this should be a bug.====='; 'I added this and issue resolved`faceapi.nets.tinyFaceDetector.loadFromUri(this.MODEL_URL);`====='; ""This is probably due to failure to load the models you can try loading them like this insteadload from url `export async function loadModels() {    const MODEL_URL = process.env.PUBLIC_URL + '/models'    await faceapi.nets.loadTinyFaceDetectorModel.loadFromUri(MODEL_URL)    await faceapi.nets.loadFaceLandmarkTinyModel.loadFromUri(MODEL_URL)    await faceapi.nets.loadFaceRecognitionModel.loadFromUri(MODEL_URL)}`you can also load them from the disk`await faceapi.nets.loadTinyFaceDetectorModel.loadFromDisk('./models')`  =====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/602,face-api.js ERROR: Based on the provided shape; [256;128]; the tensor should have 32768 values but has 29076,10,open,2020-04-21T19:02:47Z,2021-10-17T02:57:44Z,Hello.i have a problem with this API. I dont know why; but localhost work very well; but in production not.I uploaded the files directly to the serverThe error says:**tf-core.esm.js:17 Uncaught (in promise) Error: Based on the provided shape; [256;128]; the tensor should have 32768 values but has 29076**    at g (tf-core.esm.js:17)    at kn (tf-core.esm.js:17)    at In (tf-core.esm.js:17)    at o (tf-core.esm.js:17)    at Ph (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17i know this error is on side face-api.js; but i don't know how to fix.i use hostgator to serverBest Regards,"['Yes; I have the same issue. Works perfectly on local; but not on production.For some reason; I think the model did not load all of the content. It should load 2MB instead of bytes.<img width=""420"" alt=""Screen Shot 2020-06-23 at 19 02 59"" src=""https://user-images.githubusercontent.com/15343619/85401511-55527800-b584-11ea-8292-f09850f42b9f.png"">====='; ""i fixed. i changed the save file. I moved Model file to the root folder. Sorry; i don't know if you understand me; because i'm Brazilian. LOL=====""; ""I understand you just fine. I fixed it too; the model wasn't loading correctly because I misconfigure URL settings so it doesn't load the model. It's a bad error message making us think the error comes from the javascript program.=====""; ""Hello; if someone got here with the same problem; in my case; it was working on local but not on hosted. So; after a long time; I've found out the problem was on the ftp transfer. We must pay atention to transfer the *shard* files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.=====""; ""> Hello;> if someone got here with the same problem; in my case; it was working on local but not on hosted. So; after a long time; I've found out the problem was on the ftp transfer.> We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.You are right!=====""; ""> Hello;> if someone got here with the same problem; in my case; it was working on local but not on hosted. So; after a long time; I've found out the problem was on the ftp transfer.> We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.How to do that?=====""; ""> > Hello;> > if someone got here with the same problem; in my case; it was working on local but not on hosted. So; after a long time; I've found out the problem was on the ftp transfer.> > We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.> > How to do that?Hello; using filezilla client; transfer> transfer type > binary=====""; 'I faced same issue when I loaded models from git repository. To solve this I made archive of models and committed to git. Once I setup I extract the zip to appropriate directory.====='; ""I have the same problem with a dotnet SPA React app; deployed in AWS Elastic Beanstalk the json files are there but I get the error ' Error: Based on the provided shape; [1;1;256;24]; the tensor should have 6144 values but has 4841' =====""; ""> Hello; if someone got here with the same problem; in my case; it was working on local but not on hosted. So; after a long time; I've found out the problem was on the ftp transfer. We must pay atention to transfer the _shard_ files in binary mode. I am using filezila and it was transfering the shards in text mode. Forcing it to binary solved.thanks; it's worked=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/598,Model Loading Issue: Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0,12,open,2020-04-16T16:36:43Z,2021-06-06T18:34:35Z,"In loading the model    async faceDetectRecognize() {      await faceapi.nets.ssdMobilenetv1.loadFromUri(""@/services/face-api/weights"");    }I get this error:    asyncToGenerator.js?1da1:6 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at     position 0How to solve the problem?Looking forward to your kind help.Marco","[""Hello;I am using react and I was receiving this error when the models folder was in the src folder.A quick fix for me was putting the models folder in the public folder.This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')Maybe this helps you:)=====""; 'Hi @jhony123 thank you for your kind suggestion.Putting the weights (models) folder in the same folder of the .vue file:    /src/components/auth    -rw-r--r-- 1 marco marco  14K apr 28 18:49 Peerjs.vue    drwxr-xr-x 2 marco marco 4;0K apr  7 12:57 weightsand then updating the path:     await faceapi.nets.ssdMobilenetv1.loadFromUri(""/weights"");the problem persists.I tried also to move the the weights folder within the public folder:    public$ ls -lah    total 24K    drwxrwxr-x 3 marco marco 4;0K apr 28 18:56 .    drwxrwxr-x 6 marco marco 4;0K apr 28 17:01 ..    -rw-r--r-- 1 marco marco 4;2K feb  6 17:45 favicon.ico    -rw-r--r-- 1 marco marco 1003 apr 23 15:16 index.html    drwxr-xr-x 2 marco marco 4;0K apr  7 12:57 weightsand then updated the path:    await faceapi.nets.ssdMobilenetv1.loadFromUri(""../../../public/weights"");but the problem persistsI also tried to change the loader from .loadFromUri to .load :     await faceapi.nets.ssdMobilenetv1.load(""/weights"");    await faceapi.nets.ssdMobilenetv1.load(""../../../public/weights"");but still the problem persists====='; ""I'd recommend trying to manually go to the URL in your web browser that your JS is trying to load.When I got errors like that; it meant that either the file was missing or in the wrong place; or my local web server wasn't properly configured to serve up those static files.=====""; '@lazerwalker I solved all the issues related to the NGINX web server configuration for static files serving. Now I\'m able to access through URI all the files in the weights folder /home/marco/www/weights:![staticfile-003](https://user-images.githubusercontent.com/3073209/81922922-20deba00-95dd-11ea-9bb1-c8b864e1f374.jpg)![staticfile-004](https://user-images.githubusercontent.com/3073209/81923299-bbd79400-95dd-11ea-9b65-3196ea0aff26.jpg)In .vue file I put these lines:    async faceDetectRecognize() {        console.log(""faceDetectRecognize() called"");         await faceapi.nets.ssdMobilenetv1.load(""/home/marco/www/weights/ssd_mobilenetv1_model-    shard1"");    }and in console.log I get this error message:    faceDetectRecognize() called    asyncToGenerator.js?1da1:6 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at     position 0What do you think could be the cause of the problem?====='; ""You don't want the URL to be your local filesystem filepath; it needs to be the URL hosted by your local web server so your web browser is capable of requesting  downloading it.=====""; '@lazerwalker probably due to my lack of knowledge; I\'m not following you.What do you mean as ""URL hosted by my local web server; so my web browser is capable of requesting downloading it""? The files have to be located somewhere in the local filesystem.  So I need to indicate somehow the location of those files to the web browser.As far as I understand; this location needs to be an URL; to be defined in my local web server.Is this what I have already done as follows?    location /weights {      root /home/marco/www;      try_files $uri $uri/ =404;I tried to search within the NGINX \'s documentation but I didn\'t find; probably to my lack of knowledge; any reference.Would you be so kind in pointing me in the right direction? http://nginx.org/en/docs/====='; 'You have copies of the models/weights hosted somewhere they\'re accessible from a web browser (looks like ggc.world/weights from your screenshot). Your current JS code tries to load the weights from your local hard disk by accessing a file path on your local file system (/home/marco/www/weights). However; that JS code running your web browser doesn\'t have access to your local file system! It needs to be able to download them from a URL/web server that your web browser can resolve.Fortunately; you have one of those!Instead of `await faceapi.nets.ssdMobilenetv1.load(""/home/marco/www/weights/ssd_mobilenetv1_model-shard1"");`; you likely want something like `await faceapi.nets.ssdMobilenetv1.load(""ggc.world/weights/ssd_mobilenetv1_model-shard1"");`====='; 'Thank you @lazerwalker.Putting this :     await faceapi.nets.ssdMobilenetv1.load(""https://ggc.world/weights/"")I get this message:![staticfile-005](https://user-images.githubusercontent.com/3073209/81975994-2c0b0780-9628-11ea-8405-1fe94d36742e.jpg)But I guess; this is another story====='; ""> Hello;> I am using react and I was receiving this error when the models folder was in the src folder.> A quick fix for me was putting the models folder in the public folder.> This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')> Maybe this helps you:)Wow; thanks bro=====""; ""> Hello;> I am using react and I was receiving this error when the models folder was in the src folder.> A quick fix for me was putting the models folder in the public folder.> This is how I load the model: faceapi.nets.ssdMobilenetv1.load('/models')> Maybe this helps you:)That also did the trick for me. I am using this lib in react with Typescript project=====""; 'I am facing same issue. my model files are hosted in web app service and from local i am trying to access. but everytime I am getting this error.error : Uncaught (in promise) SyntaxError: Unexpected token T in JSON at position 0.trying to access with below url:  https://cors-anywhere.herokuapp.com/azureURL/SampleModels/====='; 'same here @Namrataijare did you fixed?=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/538,Weights not loading correctly.(Asp .net mvc),2,open,2020-01-29T02:11:03Z,2020-02-28T12:18:54Z,"__Uncaught (in promise) Error: Based on the provided shape; [3;3;32;32]; the tensor should have 9216 values but has 1842    at D (face-api.min.js:1)    at Ge (face-api.min.js:1)    at e (face-api.min.js:1)    at jc (face-api.min.js:1)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at Array.forEach (<anonymous>)    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)-_This is my Error in Console. I and trying to use face-api with asp.net._Promise.all([	faceapi.nets.faceRecognitionNet.loadFromUri(""/recognition_models"");	faceapi.nets.faceLandmark68Net.loadFromUri(""/recognition_models"");	faceapi.nets.ssdMobilenetv1.loadFromUri(""/recognition_models"")]).then(start);_I am trying to load the weights this way but it doesn't works properly.The same code runs correctly outside the asp.net project and weights load properly.I don't know what the issue is.  ","['Check the network tab and make sure; that all the shards are fetched correctly from your backend; e.g that the uris are correct and the files are not corrupted.====='; 'I just ran into the same problem today. IIS will not serve these files by default. Try adding a web.config to the folder from which these files are served containing this:```<?xml version=""1.0"" encoding=""UTF-8""?><configuration>  <system.webServer>    <staticContent>      <mimeMap fileExtension=""."" mimeType=""application/octet-stream"" />    </staticContent>  </system.webServer></configuration>=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.1] Data/Model Inaccessibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/722,Loading models in Angular,6,closed,2020-11-03T22:18:00Z,2021-02-16T05:19:35Z,"I'm trying to load the ""models"" downloaded in my project; it compiles but showing this following error in console: ![image](https://user-images.githubusercontent.com/43259638/98045958-183aa400-1e08-11eb-80c8-c34a6ee607d5.png)All examples I have found are in Vanilla or Node.js; so I have  doubts if I'm loading the models in right place of Angular class; the current code of class is: `  @ViewChild('videoPlayer') videoplayer : HTMLVideoElement;  async ngOnInit() {    Promise.all([      await faceapi.nets.tinyFaceDetector.loadFromUri('./models');      await faceapi.nets.faceLandmark68Net.loadFromUri('./models');      await faceapi.nets.faceRecognitionNet.loadFromUri('./models');      await faceapi.nets.faceExpressionNet.loadFromUri('./models')    ]).then(() => {      this.startVideo();    })  }  async startCanvas(){    // const canvas = faceapi.createCanvasFromMedia(this.videoplayer);        setInterval(async () => {      const detections = await faceapi.detectAllFaces(this.videoplayer;         new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();        console.log(detections);    }; 100)  }  startVideo(){    navigator.getUserMedia(      { video: {} };      stream => {        this.videoplayer.srcObject = stream;      };      err => console.error(err)    );  }`Current path of ""models"" folder:![image](https://user-images.githubusercontent.com/43259638/98046459-04437200-1e09-11eb-9f6c-be2e0b98e6eb.png)Am I putting the models in a wrong place or I should to change where my code load the ""models"" in page?","['Hello; try moving the models folder to **assets**.![image](https://user-images.githubusercontent.com/25928891/98683328-473aa380-233b-11eb-9a33-58c79479df99.png)====='; 'I have no idea how to implement this in angular. can you please share github of working code in angular?====='; '@thelionleo take a look in a project named ""projeto-tcc"" in my profile; I did the implementation for Angular; just configure and access localhost:800/tests to see the results====='; '> Hello; try moving the models folder to **assets**.> > ![image](https://user-images.githubusercontent.com/25928891/98683328-473aa380-233b-11eb-9a33-58c79479df99.png)I did the changes as you sugested; I had to make other changes to models loading correctly; for example; instead of calling the model loaders on init and inside a Promise.all(); I have to load them inside a async function and call it in ngAfterViewInit().If someone have difficult to load the plugin in project; see ""projeto-tcc"" in my profile; it\'s not finished yet but is possible to see this plugin working in tests page. ====='; '> @thelionleo take a look in a project named ""projeto-tcc"" in my profile; I did the implementation for Angular; just configure and access localhost:800/tests to see the resultsthere\'s too much happening there.  is it an implementation of face-api? would it be too much to ask if you can create a new github for me for angular? just the basics to get me started.====='; '@thelionleo Here is my [repo](https://github.com/imrushi/face-api-js-angular) I have implemented the Face-api.js in angular. I hope this will help you.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/692,"IONIC: faceapi.detectAllFaces ""Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.""",1,closed,2020-09-08T21:47:26Z,2020-09-09T18:41:16Z,"Hello I'm trying to detect faces in realtime using the device camera but in ionic. I replicated the example provided  [here](https://github.com/WebDevSimplified/Face-Detection-JavaScript); it works well in the browser with `ng serve`; but it fails when running in Android. It throws the following error when executing `faceapi.detectAllFaces()`Any idea of what could be happening?### Error ```E/Capacitor/Console: File: http://localhost/vendor-es2015.js - Line 39108 - Msg: ERROR Error: Uncaught (in promise): IndexSizeError: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.    Error: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source height is 0.        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:531954        at Array.map (<anonymous>)        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:531848        at http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:1801        at Object.next (http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:1906)        at n (http://localhost/default~pages-face-validation-face-validation-module~pages-identity-validation-identity-validation-module-es2015.js:4023:677)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3470:30)        at Object.onInvoke (http://localhost/vendor-es2015.js:62348:33)        at ZoneDelegate.invoke (http://localhost/polyfills-es2015.js:3469:36)        at Zone.run (http://localhost/polyfills-es2015.js:3229:47)```Here is my HTML```html<div id=""container"" #container>    <video width=""960"" height=""720"" muted autoplay id=""video""></video></div>```Heres is my TypeScript code: ```typescriptimport * as faceapi from '../../../assets/tfjs/face-api.min.js'import { Plugins; FilesystemDirectory; FilesystemEncoding } from '@capacitor/core';const { Filesystem } = Plugins;async ngOnInit() {    await this.loadModels();  }async loadModels() {    //set path to load models    let filePathRoot = 'http://localhost/assets/';    // faceapi settings    faceapi.env.monkeyPatch({      readFile: filePath =>        new Promise(resolve => {          let fileExtension = filePath.split(""?"")[0].split(""."").pop();          let fileName = filePath.split(""?"")[0].split(""/"").pop();          if (fileExtension === ""json"") {            fetch(filePathRoot + fileName)              .then((response) => {                resolve(response.text());              })          } else {           //Retrieving shard files from File system using Cordova Plugin            Filesystem.readFile({              path: fileName;              directory: FilesystemDirectory.Documents;              encoding: FilesystemEncoding.UTF16            }).then(file => {              const str = file.data;              var buf = new ArrayBuffer(str.length * 2);              var bufView = new Uint16Array(buf);              for (var i = 0; strLen = str.length; i < strLen; i++) {                bufView[i] = str.charCodeAt(i);              }              resolve(new Uint8Array(buf));            })          }        });      Canvas: HTMLCanvasElement;      Image: HTMLImageElement;      ImageData: ImageData;      Video: HTMLVideoElement;      createCanvasElement: () => document.createElement(""canvas"");      createImageElement: () => document.createElement(""img"")    });   // load models for recognition    await faceapi.nets.tinyFaceDetector.loadFromDisk(filePathRoot);    await faceapi.nets.faceRecognitionNet.loadFromDisk(filePathRoot);    await faceapi.nets.faceExpressionNet.loadFromDisk(filePathRoot);    this.startVideo();  } startVideo() {    const video = document.getElementById(""video"") as HTMLVideoElement;    const container = document.getElementById(""container"") as HTMLDivElement;    video.onplaying = () => {      console.log('Camera loaded')      this.analyze(video; container);    };    navigator.mediaDevices.getUserMedia({      audio: false;      video: {        width: { ideal: 960 };        height: { ideal: 720 };      }    }).then((stream) => {      video.srcObject = stream;    }).catch((err0r) => {      console.log(err0r)      video.src = ""assets/video-test.mp4"";      video.play();    });  } analyze(source: HTMLVideoElement | HTMLImageElement; content) {    const canvas = faceapi.createCanvasFromMedia(source);    content.append(canvas);    const displaySize = { width: source.width; height: source.height }    faceapi.matchDimensions(canvas; displaySize)    this.intervalId = setInterval(async () => {      console.log(source.height; source.width)      const detections = await faceapi.detectAllFaces(source; new      faceapi.TinyFaceDetectorOptions()).withFaceExpressions(); //FAILS HERE      console.log(JSON.stringify(detections))      const resizedDetections = faceapi.resizeResults(detections; displaySize)      canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)      faceapi.draw.drawDetections(canvas; resizedDetections)      faceapi.draw.drawFaceExpressions(canvas; resizedDetections)    }; 100)  }```","[""The problem was caused because the model file wasn't being loaded correctly. Here is the right way to load it from local file in Capacitor.```javascript Filesystem.readFile({                path: fileName;                directory: FilesystemDirectory.External              }).then(file => {                const byteCharacters = atob(file.data);                const byteNumbers = new Array(byteCharacters.length);                for (let i = 0; i < byteCharacters.length; i++) {                  byteNumbers[i] = byteCharacters.charCodeAt(i);                }                const bufView = new Uint8Array(byteNumbers);                resolve(bufView);              })```=====""]",Data & Model Error,Crash,Incorrect Code Logic,,,type replacer,Replace data Shape/type,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/673,Uncaught (in promise) Error: failed to fetch: (404) Not Found; from url: http://localhost:8601/face-api.js/models/ssd_mobilenetv1_model-weights_manifest.json,3,closed,2020-08-01T10:51:05Z,2020-08-07T00:29:06Z,"Tried using Face-api.js in a web based application; For now we have the models folder in nodemodules/face-api.js/models. but we get the above error. May i know which is the right place to have the models folder?This is the code used.const { Canvas; Image; ImageData } = canvas	faceapi.env.monkeyPatch({ Canvas; Image; ImageData })		console.log(faceapi.nets); (async () => {	await faceapi.nets.ssdMobilenetv1.loadFromUri('face-api.js/models');		const detections = await faceapi.detectAllFaces(this.video);	console.log(""face found "");	console.log(detections);	})();    let media = navigator.mediaDevices.getUserMedia({      video: true;      audio: false    });    media.then((stream) => {      this.video.srcObject = stream;    });","[""What did you figure out here @naveen-robotixedu? Dealing with a similar issue... Thanks! (I'm not using express backend)=====""; ""> What did you figure out here @naveen-robotixedu? Dealing with a similar issue... Thanks! (I'm not using express backend)Created a build folder for the website and just placed the models folder in the build folder=====""; 'wow you just saved my life. I was stressing the fu&^ out =====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/589,[node.js version] error when load models from disk,1,closed,2020-04-07T02:50:21Z,2020-04-07T03:45:49Z,I ran into this error when use face-api in node environment```Error: ENOENT: no such file or directory; open './weights/face_landmark_68_model-weights_manifest.json'```Here my folder structure:![image](https://user-images.githubusercontent.com/23404165/78624644-55f83e00-78b4-11ea-9374-67a6c4254baa.png)In `face-comparison.js`:```jsasync function faceComparison(image) {  try {     await faceapi.nets.faceLandmark68Net.loadFromDisk('./weights');     await faceapi.nets.faceRecognitionNet.loadFromDisk('./weights');     await faceDetectionNet.loadFromDisk('./weights');     // do stuff  }  catch(err) { console.log(err); }}```I tried fs.readFile to read the json file and it worked.,"[""Never mind. I use `path.join(__dirname; './weights')` and it works=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Model/Tensor Inaccessibility""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/473,Unexpected token < in JSON at position 6,1,closed,2019-11-15T12:37:41Z,2019-11-18T11:00:55Z,"```async function detectarFace(){	await faceapi.loadSsdMobilenetv1Model('/models');	if(inputRosto.files && inputRosto.files[0]){		var reader = new FileReader();		reader.readAsDataURL(inputRosto.files[0]);		reader.onload = async event => {			imagemPreview.src = event.target.result;			displayInformacoes.innerHTML = ""processando..."";			let deteccao = await faceapi.detectSingleFace(imagemPreview)										.withFaceLandmarks()										.withFaceExpressions()										.withAgeAndGender();		}	}}```![Capturar](https://user-images.githubusercontent.com/9268549/68944016-861b3500-078b-11ea-93a2-d278d41a20a1.PNG)",['See my answer in #445.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.2] JS Variable Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/468,Wont load the shards just the manifest,3,closed,2019-11-08T21:13:51Z,2020-08-02T16:25:45Z,"I'm trying to load the tinyFaceDetector and faceLandmark68TinyModel.  The manifests load just fine but I don't see the shards coming in.  In the console I'm getting an ""Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0"".here is my code```import ""./main.scss"";import * as FaceApi from ""face-api.js"";window.addEventListener(""load""; function() {  async function loadModels() {    await Promise.all([      FaceApi.nets.tinyFaceDetector.loadFromUri(""/models/"");      FaceApi.nets.faceLandmark68TinyNet.loadFromUri(""/models/"");    ]);    const image = document.querySelector("".img"");    const detection = await FaceApi.detectSingleFace(      image;      new FaceApi.TinyFaceDetectorOptions()    ).withFaceLandmarks;    console.log(detection);  }  try {    loadModels();  } catch (e) {    console.log(e);  }});```any help would be greatly appreciatedthanks!!","['I would check:1. the network tab at web inspector;  it should be doing two ""fetch""2. the JSON downloaded is complete and correctI had a similar problem in which my framework was not loading non-extension files (as the shards)====='; 'Yeah Thanks;That is what it ended up being; my bundler was not bundling the shards.====='; 'How did you resolve the bundling issue and/or the non-extension issue (did you add .weights to the ext? That did not work for me..) Thanks!=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/412,loading models in react,3,closed,2019-09-13T06:04:03Z,2019-10-12T11:14:12Z,"import React; { Component } from 'react';import * as faceapi from 'face-api.js'class camera extends Component{    constructor(props) {        super(props);        this.videoTag = React.createRef()        this.state={            video:null        }        this.detect=this.detect.bind(this);    }    componentDidMount() {        // getting access to webcam            navigator.mediaDevices            .getUserMedia({video: true})            .then(stream =>             this.videoTag.current.srcObject = stream;             this.detect()               )              .catch(console.log);             }                     detect=async ()=>{          const videoTag=document.getElementById('videoTag');          console.log(""geeting"")          const canvas=document.getElementById('myCanvas');          await faceapi.nets.ssdMobilenetv1.loadFromUri('/models')           faceapi.nets.ssdMobilenetv1.loadFromUri('./models')          const displaySize = { width: videoTag.width; height: videoTag.height }          faceapi.matchDimensions(canvas; displaySize)          setInterval(async () => {          const detections = await faceapi.detectAllFaces(videoTag)          console.log(detections.length);          const resizedDetections = faceapi.resizeResults(detections; displaySize)          canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)          faceapi.draw.drawDetections(canvas; resizedDetections)        }; 200)      }                render() {        return(             <div>                <video id=""videoTag""                        ref={this.videoTag}                        width={500}                        height={500}                        autoPlay                        ></video>                      <canvas id=""myCanvas""                                height={500}                                width={500}></canvas>                      </div>                              );    }}export default camera;while loading ssd mobilenet model in react this error is geetingUncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0","[""please check wheather i've had mistaken or missing something=====""; 'Check you have copied the models correctly. It happened to me. Maybe you copied the json incorrectly.====='; 'Also check the network tab an make sure the uris your models are fetched from are correct.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/394,Loading models in Django framework,7,closed,2019-08-26T23:38:02Z,2021-08-31T08:50:28Z,Hi Vincent;I am really impressed on your incredible work!First of all; I am currently working on real-time face detection implementation on my web application based on Django framework. I followed the steps from youtube for implementing this feature that you have provided in the description section. This implementation is perfectly working when I have implemented on VSCode with live server extension. However; somehow this implementation doesnt work on my web application with django server. There is an issue with loading models I guess.. If you are familiar with django framework; is there any configuration do i need to add? Thanks in advance! video.js:`Promise.all([  faceapi.nets.ssdMobilenetv1.loadFromUri('/models');  faceapi.nets.tinyFaceDetector.loadFromUri('/models');  faceapi.nets.faceLandmark68Net.loadFromUri('/models');  faceapi.nets.faceRecognitionNet.loadFromUri('/models');  faceapi.nets.faceExpressionNet.loadFromUri('/models')]).then(startVideo)`console log:`Uncaught (in promise) Error: failed to fetch: (404) Not Found; from url ``face-api.min.js:1 GET http://127.0.0.1:8000/models/ssd_mobilenetv1_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_landmark_68_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_recognition_model-weights_manifest.json 404 (Not Found)``face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)`If I run my web application on heroku server; Im getting this error`Failed to load resource: the server responded with a status of 404 (Not Found) /models/tiny_face_detector_model-weights_manifest.json:1 `,"['> If you are familiar with django framework; is there any configuration do i need to add?I am not familar with django sorry.> face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)Why are you loading your models from `http://127.0.0.1:8000/` when you should actually load them from the heroku server?====='; 'Closing because of inactivity.====='; ""@dytebk did you found any solution? I have currently the same problem. I've created a question there: https://stackoverflow.com/questions/58965228/app-does-not-include-some-of-the-files-on-heroku=====""; '> Hi Vincent;> > I am really impressed on your incredible work!> First of all; I am currently working on real-time face detection implementation on my web application based on Django framework. I followed the steps from youtube for implementing this feature that you have provided in the description section. This implementation is perfectly working when I have implemented on VSCode with live server extension. However; somehow this implementation doesnt work on my web application with django server. There is an issue with loading models I guess.. If you are familiar with django framework; is there any configuration do i need to add?> Thanks in advance!> > video.js:> > `Promise.all([ faceapi.nets.ssdMobilenetv1.loadFromUri(\'/models\'); faceapi.nets.tinyFaceDetector.loadFromUri(\'/models\'); faceapi.nets.faceLandmark68Net.loadFromUri(\'/models\'); faceapi.nets.faceRecognitionNet.loadFromUri(\'/models\'); faceapi.nets.faceExpressionNet.loadFromUri(\'/models\') ]).then(startVideo)`> > console log:> > `Uncaught (in promise) Error: failed to fetch: (404) Not Found; from url `> `face-api.min.js:1 GET http://127.0.0.1:8000/models/ssd_mobilenetv1_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_landmark_68_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_recognition_model-weights_manifest.json 404 (Not Found)`> `face-api.min.js:1 GET http://127.0.0.1:8000/models/face_expression_model-weights_manifest.json 404 (Not Found)`> > If I run my web application on heroku server; Im getting this error> > `Failed to load resource: the server responded with a status of 404 (Not Found) /models/tiny_face_detector_model-weights_manifest.json:1 `Hi Dytebk; in Django do you need to refenciate to static folder. I solved putting script.js in the same file index.html; and moving /modes inside /static folder:`{% load static%}<!DOCTYPE html><html lang=""en""><head>  <meta charset=""UTF-8"">  <meta name=""viewport"" content=""width=device-width; initial-scale=1.0"">  <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">  <title>Document</title>  <script src=""{% static ""js/recdata_new.js"" %}""></script>  <script src=""{% static ""js/face-api.min.js"" %}""></script><!--   <script src=""{% static ""js/script.js"" %}""> -->      <style>    body {      margin: 0;      padding: 0;      width: 100vw;      height: 100vh;      display: flex;      justify-content: center;      align-items: center;    }    canvas {      position: absolute;    }  </style></head><body>  <video id=""video"" width=""720"" height=""560"" autoplay muted></video><script type=""text/javascript"">const video = document.getElementById(\'video\')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri(\'{% static \'/models\' %}\');  faceapi.nets.faceLandmark68Net.loadFromUri(\'{% static \'/models\' %}\');  faceapi.nets.faceRecognitionNet.loadFromUri(\'{% static \'/models\' %}\');  faceapi.nets.faceExpressionNet.loadFromUri(\'{% static \'/models\' %}\');  faceapi.nets.ageGenderNet.loadFromUri(\'{% static \'/models\' %}\')]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} };    stream => video.srcObject = stream;    err => console.error(err)  )}video.addEventListener(\'play\'; () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width; height: video.height }  faceapi.matchDimensions(canvas; displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video; new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()    const resizedDetections = faceapi.resizeResults(detections; displaySize)    canvas.getContext(\'2d\').clearRect(0; 0; canvas.width; canvas.height)    faceapi.draw.drawDetections(canvas; resizedDetections)    faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)    faceapi.draw.drawFaceExpressions(canvas; resizedDetections)  }; 100)})  </script></body></html>`Regards!====='; 'Here is also working example: https://github.com/khashashin/gibbface/blob/master/frapp/templates/frapp/frapp.html====='; 'Failed to load resource: the server responded with a status of 404 (Not Found):8000/models/face_landmark_68_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found):8000/models/face_recognition_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found)face-api.min.js:1 Uncaught (in promise) Error: failed to fetch: (404) Not Found; from url: http://127.0.0.1:8000/models/tiny_face_detector_model-weights_manifest.json    at face-api.min.js:1    at face-api.min.js:1    at Object.next (face-api.min.js:1)    at n (face-api.min.js:1)(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1(anonymous) @ face-api.min.js:1n @ face-api.min.js:1:8000/models/face_expression_model-weights_manifest.json:1 Failed to load resource: the server responded with a status of 404 (Not Found)====='; ""anyone help to solve this error to load models can i give path for inside the js file? means can we used load static in js ? if yes pleas help <-------------------------------- ths is my code ----------------------------------->const video = document.getElementById('video')Promise.all([  faceapi.nets.tinyFaceDetector.loadFromUri(`{% static '/models' %}`);  faceapi.nets.faceLandmark68Net.loadFromUri(`{% static '/models' %}`);  faceapi.nets.faceRecognitionNet.loadFromUri(`{% static '/models' %}`);  faceapi.nets.faceExpressionNet.loadFromUri(`{% static '/models' %}`)]).then(startVideo)function startVideo() {  navigator.getUserMedia(    { video: {} };    stream => video.srcObject = stream;    err => console.error(err)  )}video.addEventListener('play'; () => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width; height: video.height }  faceapi.matchDimensions(canvas; displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video; new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()    const resizedDetections = faceapi.resizeResults(detections; displaySize)    canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)    faceapi.draw.drawDetections(canvas; resizedDetections)    faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)    faceapi.draw.drawFaceExpressions(canvas; resizedDetections)  }; 100)})=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.1] Data/Model Inaccessibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/349,models/face_landmark_68_model-shard1  not fount 404,1,closed,2019-07-10T12:01:06Z,2019-10-12T11:23:47Z,I when placing the files in a subpath of my server; the project can not find the file: models / face_landmark_68_model-shard1 Files with .json are localized but not others.How to solve?,['Check the network tab; e.g. the URI your client attempts to fetch the shards from. Most likely the URI is invalid; or the files are not accessible under that URI.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] 404 Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.1] Data/Model Inaccessibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/343,RangeError: attempting to construct out-of-bounds,2,closed,2019-07-04T15:20:44Z,2019-07-05T22:29:30Z,Hello;When I pass my project online I have this error (on localhost all work great online I have this error)`RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer face-api.js:100:444722`This error break the load of the models or the models isn't loaded and this error appears ?Any idea ?,['After some search I hav found something weird. I lose some octets on model files.Now I have this error after update to the latest version of face-api```RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer tf-core.esm.js:17:443843Error: Based on the provided shape; [512;2]; the tensor should have 1024 values but has 986 tf-core.esm.js:17:4452RangeError: attempting to construct out-of-bounds TypedArray on ArrayBuffer tf-core.esm.js:17:443843```====='; 'I have found the error. The error comes from Filezilla. Just choose tranfert -> binary for the tranfert of the model files and all work fine. If necessary clear the navigator cache.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change file tranfert format,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/334,extendStatics$1,2,closed,2019-06-26T14:33:58Z,2019-07-22T13:09:06Z,Hi;first of all; I would like thank you for all your works.I encounter a problem when I use images on faceDetection and others api but not with the webcam; here is the error ![image](https://user-images.githubusercontent.com/52248745/60188417-87c17c00-982f-11e9-9fed-3d8e4632eeb5.png)here is the problem p.byteLength = 81881 and not 81888 !![image](https://user-images.githubusercontent.com/52248745/60188459-99a31f00-982f-11e9-880d-919f03b4a165.png)I have been surching for a long time; can you help me please ?,['`decodeWeights` comes from tfjs-core. Looks like the weight or shard files you are trying to load are malformed somehow.====='; 'If this is still an issue feel free to reopen.====='],Data & Model Error,Crash,API Misuse,,,format replacer,format replacer,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Incorrect weight loading leading to errors""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/307,loading models in angular,2,closed,2019-05-28T15:09:47Z,2020-10-31T16:43:02Z,hi; I try to load models from my app component who located in: myproject/src/app/openCam/opencamComponent.tsI run: Promise.all([ faceapi.nets.faceLandmark68Net.loadFromUri('./models')  ]);I put the models folder in:  myproject/src/models. I got the error: ERROR Error: Uncaught (in promise): Error: failed to fetch: (404) Not Found; from url: http://localhost:4200/models/face_landmark_68_model-weights_manifest.jsonError: failed to fetch: (404) Not Found; from url: http://localhost:4200/models/face_landmark_68_model-weights_manifest.jsonmy models folder contains all files from: face-api.js/weights/ in this github repository. I know little about AI and I want to landmark only eyes.,"[""according the taturial.. I pat models folder into assets and changed the code line to:     await faceapi.nets.faceLandmark68Net.loadFromUri('assets/models');=====""; 'The complete path to your model is project/src/assets/models ?=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] 404 Not Found Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.1] Data/Model Inaccessibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/285,Error with loadSsdMobilenetv1Model,1,closed,2019-05-05T10:34:38Z,2019-05-05T15:52:28Z,I have implemented it on my Angular project and when I try localy everything is ok (detect faces; compare and get the euclidean distance) but when I try it after deployment (ng build) and I start the app; I have the following error: **Error: Uncaught (in promise): RangeError: byte length of Float32Array should be a multiple of 4**![image](https://user-images.githubusercontent.com/50211349/57192536-e5bd9b80-6f31-11e9-8f4f-bdf583162f3b.png)Could anyone help me? I think the problem is not with assets directory because the error is not a 404 not found.Thanks.,"[""Everything it's ok. It was because FileZilla transfer files ASCII by default.=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change file tranfert format,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/186,fetch implementation is missing,9,closed,2019-01-07T13:36:33Z,2020-12-22T20:25:50Z,Like the title says; fetch implemtation is missing.face-api version: 0.17.1``` typescriptimport * as faceapi from 'face-api.js';class App {    private MODEL_URL: string;    constructor() {        console.log('starting app...');        console.log(process.env.MODELS);        this.MODEL_URL = process.env.MODELS;    }    start() {        try {            this.loadModels();            console.log(`Models loadedd successfully....`);        } catch (error) {            console.error(error);        }    }    private async loadModels() {        await faceapi.loadSsdMobilenetv1Model(this.MODEL_URL)    }}```(node:25181) UnhandledPromiseRejectionWarning: Error: fetch - missing fetch implementation for nodejs environment    at fetch (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/env/createNodejsEnv.ts:24:11)    at Object.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/dom/fetchOrThrow.ts:9:21)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)    at Object.next (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:114:57)    at /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:107:75    at new Promise (<anonymous>)    at Object.__awaiter (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:103:16)    at Object.fetchOrThrow (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/build/commonjs/dom/fetchOrThrow.js:6:20)    at Object.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tfjs-image-recognition-base/src/dom/fetchJson.ts:4:17)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)(node:25181) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of anasync function without a catch block; or by rejecting a promise which was not handled with .catch(). (rejection id: 4)(node:25181) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```,"['The error message pretty much says it; you either need to use a fetch polyfill for nodejs or you can monkeyPatch it: `faceapi.env.monkeyPatch({ fetch: /*your implementation here*/ })`.====='; ""Shouldn't it be implemented by the face-api itself? If not; then is there any specific reason for it?=====""; ""In tfjs-core fetch is being used for fetching and decoding model shards; thus you won't get around this. You can polyfill fetch installing the `node-fetch` package if you really need to load models from remote uris in nodejs or you could simply load the models from local filesystem instead.=====""; 'I have the same error @justadudewhohacks; I have installed node-fetch-polyfill but still not working. Am I misinterpreting it?```import express from \'express\';import path from \'path\';import * as faceapi from \'face-api.js\';import fetch from \'node-fetch-polyfill\';var app = express();app.use(express.static(\'public\'))app.get(\'/\'; function (req; res) {   res.sendFile(path.join(__dirname + \'/index.html\'));})var server = app.listen(8081; function () {   var host = server.address().address   var port = server.address().port   console.log(""Example app listening at http://%s:%s""; host; port);})app.get(\'/test\'; async function(req; res; next){    const MODEL_URL = \'/models\';    try {        await faceapi.loadSsdMobilenetv1Model(MODEL_URL);    } catch (e) {        console.log(e);    }   console.log(dataToSendToClient);   //var dataToSendToClient = {\'labeledFaceDescriptors\': \'hello\'};   var JSONdata = JSON.stringify(dataToSendToClient);   res.send(JSONdata);})```====='; ""### BrowserLoading the models on the browser was working fine.```javascriptconst MODELS_URL = 'path/to/models';// Load the face detection modelsawait faceapi.loadSsdMobilenetv1Model(MODELS_URL);// Load the face landmark modelsawait faceapi.loadFaceLandmarkModel(MODELS_URL);// Load the face recognition modelsawait faceapi.loadFaceRecognitionModel(MODELS_URL);       ```<br>However; I faced the same issue on Node.js.<br>### Node.jsI found the solution by looking through the example code in the repository.```javascriptimport path from 'path';// Import a fetch implementation for Node.jsimport fetch from 'node-fetch';// Make face-api.js use that fetch implementationfaceapi.env.monkeyPatch({ fetch: fetch });// Try experimenting on wherever your models are located. Mine are from up one folder// You will get 'Error: Only absolute urls are supported' if you don't specify the absolute pathconst MODELS_URL = path.join(__dirname; '/../models/face-api');async function doSomething() {  // Load the face detection models  await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODELS_URL);  // Load the face landmark models  await faceapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL);  // Load the face recognition models  await faceapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL);  // Do something else  ...}```=====""; 'i have the same issue @cefjoeii but your solution is not work for me.This give me another error > face-recognition.ts(15;27): error TS2322: Type \'typeof fetch\' is not assignable to type \'(url: string; init?: RequestInit) => Promise<Response>\'.  Types of parameters \'init\' and \'init\' are incompatible.    Type \'RequestInit\' is not assignable to type \'import(""/home/duy/find_user_nodejs/node_modules/@types/node-fetch/index"").RequestInit\'.      Types of property \'body\' are incompatible.        Type \'BodyInit\' is not assignable to type \'import(""/home/duy/find_user_nodejs/node_modules/@types/node-fetch/index"").BodyInit\'.          Type \'Blob\' is not assignable to type \'BodyInit\'.            Type \'Blob\' is missing the following properties from type \'ArrayBuffer\': byteLength; [Symbol.toStringTag]====='; ""You don't need fetch nor monkeypatch. Just use the special API calls to load the model files from local storage (the tutorials don't mention those; but README at npm does); like this:```jsconst faceapi = require('face-api.js');const tf = require('@tensorflow/tfjs-node');const Canvas = require('canvas');const Image = require('canvas');const ImageData = require('canvas');const MODEL_URL = `${__dirname}/face-models/`;faceapi.nets.ssdMobilenetv1.loadFromDisk(MODEL_URL)    .then(faceapi.nets.faceLandmark68Net.loadFromDisk(MODEL_URL))    .then(faceapi.nets.faceRecognitionNet.loadFromDisk(MODEL_URL))    .catch(error => {        error.log(error);    });```=====""; '(node:12474) UnhandledPromiseRejectionWarning: Error: only absolute urls are supported    at /home/vvm/tube/node_modules/node-fetch-polyfill/index.js:47:19====='; '> ```js> path.joi> ```I did the same and now I get `Only absolute URLs are supported`Update----------I found what was my mistakeI was calling `faceapi.nets.faceRecognitionNet.loadFromUrl` instead of `faceapi.nets.faceRecognitionNet.loadFromDisk`=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,patch environment,Fix environment adaptability,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Fetch Implementation Missing""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/141,I don't have acccess to all functionality of the API,2,closed,2018-11-19T19:05:52Z,2018-11-20T11:35:11Z,"I can only load models through the loadModels() function; not the individual loading functions as described in the 'Usage' section of the repository. Any other function gives me errors such as ""faceapi.nets.ssdMobilenetv1.loadFromUri is not a function"".I got the face-api.js file from the 'dist' folder and included that globally; is this an incorrect way of accessing the API?(I am working with JS and React for the first time)",['You are probably not using the latest version. If you are using react; then you probably use a bundler such as webpack; parcel etc. and can just npm install it to get the latest version.====='; 'Turned out I was indeed not using the latest version. I apologise for the stupid question and appreciate the quick response very much. Very cool repository!====='],Reference Error,Crash,API Misuse,,,change Third-party library version,Changing version,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.3""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/131,Error: Based on the provided shape; [1;1;16;32]; and dtype float32; the tensor should have 512 values but has 93,19,closed,2018-11-12T15:50:31Z,2021-03-05T11:15:15Z,Hi! I'm having trouble making the library work on production. Everything works perfect locally.Using angular 4. Here is the code:`const detections: Array<any> = await faceapi.detectAllFaces(this.video; new faceapi.TinyFaceDetectorOptions({ inputSize: 128; scoreThreshold: 0.4 }));`> ERROR Error: Uncaught (in promise): Error: Based on the provided shape; [1;1;16;32]; and dtype float32; the tensor should have 512 values but has 93Error: Based on the provided shape; [1;1;16;32]; and dtype float32; the tensor should have 512 values but has 93    at c (scripts.208b17eda93ff84a86fa.bundle.js:1)    at new t (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Function.t.make (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Ht (scripts.208b17eda93ff84a86fa.bundle.js:1)    at o (scripts.208b17eda93ff84a86fa.bundle.js:1)    at $a (scripts.208b17eda93ff84a86fa.bundle.js:1)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at c (scripts.208b17eda93ff84a86fa.bundle.js:1)    at new t (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Function.t.make (scripts.208b17eda93ff84a86fa.bundle.js:1)    at Ht (scripts.208b17eda93ff84a86fa.bundle.js:1)    at o (scripts.208b17eda93ff84a86fa.bundle.js:1)    at $a (scripts.208b17eda93ff84a86fa.bundle.js:1)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at scripts.208b17eda93ff84a86fa.bundle.js:1    at Array.forEach (<anonymous>)    at S (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at polyfills.b05dc6a29a1cfeb080d6.bundle.js:1    at a (main.424b2f298a047510607b.bundle.js:1)    at t.invoke (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at Object.onInvoke (main.424b2f298a047510607b.bundle.js:1)    at t.invoke (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at e.run (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at polyfills.b05dc6a29a1cfeb080d6.bundle.js:1    at t.invokeTask (polyfills.b05dc6a29a1cfeb080d6.bundle.js:1)    at Object.onInvokeTask (main.424b2f298a047510607b.bundle.js:1)$ @ main.424b2f298a047510607b.bundle.js:1t.handleError @ main.424b2f298a047510607b.bundle.js:1next @ main.424b2f298a047510607b.bundle.js:1e.object.i @ main.424b2f298a047510607b.bundle.js:1e.__tryOrUnsub @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e._next @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e.next @ main.424b2f298a047510607b.bundle.js:1e.emit @ main.424b2f298a047510607b.bundle.js:1(anonymous) @ main.424b2f298a047510607b.bundle.js:1t.invoke @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1e.run @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1t.runOutsideAngular @ main.424b2f298a047510607b.bundle.js:1onHandleError @ main.424b2f298a047510607b.bundle.js:1t.handleError @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1e.runGuarded @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1t @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1n.microtaskDrainDone @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1d @ polyfills.b05dc6a29a1cfeb080d6.bundle.js:1Also the video is 351x360 pixels,"['Not sure why the code snippet shown above should make any difference in production.Looking at the error message your issue most likely resides in loading a model. Check that the model files are served correctly in your production environment.====='; 'Thanks! That solved it!====='; '@psiservices-azubizarreta Can you tell me how to solve? I have a same error. Thank you !====='; ""@psiservices-azubizarreta Hello ! I hava a same problem as you met; and it's very peremptorily; so。。Can you tell us the way to solve this situation plz ? my very thanks!=====""; 'a bit stuck here too how did you solve it on production?====='; ""For me; this problem caused by the uploading process. I'm using filezilla and it's solved by change the transfer type to binary.Sorry for commenting on closed issue.=====""; '@atmosuwiryo this helped!====='; 'I managed to solve it serving model files from a [cdn](https://gitcdn.xyz/)Use https://gitcdn.xyz/repo/justadudewhohacks/face-api.js/master/weights/ as `MODEL_URL`====='; 'That address seems invalid...====='; '@aryehrein ; well it tends to hibernate occasionally. You have to wake it up by pasting the link to `weights` folder manually on https://gitcdn.xyz/====='; 'It was actually a different problem - the json files managed to get loaded; but the non extension files (e.g.  ""age_gender_model-shard1"") weren\'t. So what I did - I used my downloaded ""weights"" folder (and not the cdn); and for each model I :1. added a fictional "".shard"" extension for the non extension file.2. opened the corresponding ""..._manifest.json"" file and changed the ""path"" property to the new file name including the "".shard"" extension.For example:1. /age_gender_model-shard1 renamed to /age_gender_model-shard1.shard2. opened age_gender_model-weights_manifest.json; and changed `""paths"":[""age_gender_model-shard1""]`to:`""paths"":[""age_gender_model-shard1.shard""]`Pay attention that some manifest.json files have 2 paths====='; '@aryehrein It tried that (adding extension in non-extension files)  and also changing the name in .json files but it is still not working !!====='; '@aryehrein i am getting this => Error: Based on the provided shape; [1;1;128;256]; the tensor should have 32768 values but has 12137 (i am using the face_landmark_68_model). ====='; 'Please help face-api.min.js:1 Uncaught (in promise) Error: Based on the provided shape; [25]; the tensor should have 25 values but has 21====='; '> 1. .shardThis solution worked for me. Thanks!But this strange why it needs an extension for production on iOS. ====='; 'Maybe it will help somebody:The error occurred when I started my local server with --spa; when removed it error gone====='; '@aryehrein Changing the file names worked for me! (adding an extension to -shard1 files). Thanks for the tip!! I hope it will help more people.I would say that in my case I was developing an Ionic 5 (angular) app with face-api. It worked with ionic when testing with ""ionic serve"" in Chrome browser. It failed (and I need to apply this fix) when I created the app and tested in a real android device.====='; 'Added .bin file extension to tiny_face_detector tfjs model. Else the file is not being served in production mode.Changed the file `""face_detector_model-shard1""` to `""tiny_face_detector_model-shard1.bin"" `and updating the path in  detector_model-weights_manifest.json file like below`""paths"":[""tiny_face_detector_model-shard1.bin""] `====='; '> Added .bin file extension to tiny_face_detector tfjs model. Else the file is not being served in production mode.> > Changed the file `""face_detector_model-shard1""` to `""tiny_face_detector_model-shard1.bin"" `and updating the path in detector_model-weights_manifest.json file like below> `""paths"":[""tiny_face_detector_model-shard1.bin""] `Thanks! that\'s the solutionsetting .bin extension makes Content-Type: application/octet-stream for response=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change file tranfert format,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/126,Error about Float32Array,8,closed,2018-11-02T22:09:54Z,2018-11-07T08:47:59Z,HiWhen I open faceAndLandmarkDetection.html and some other files; I get the following error:tf-core.esm.js:17 **Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4  at new Float32Array** (<anonymous>)    at o (tf-core.esm.js:17)    at decodeWeights (tf-core.esm.js:17)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at tf-core.esm.js:17    at Array.forEach (<anonymous>)    at Object.<anonymous> (tf-core.esm.js:17)    at tf-core.esm.js:17    at Object.next (tf-core.esm.js:17)o @ tf-core.esm.js:17decodeWeights @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17(anonymous) @ tf-core.esm.js:17i @ tf-core.esm.js:17async function (async)run @ faceAndLandmarkDetection.html:186(anonymous) @ faceAndLandmarkDetection.html:197j @ jquery-2.1.1.min.js:2fireWith @ jquery-2.1.1.min.js:2ready @ jquery-2.1.1.min.js:2I @ jquery-2.1.1.min.js:2Any comment?,"['Me too`Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4    at new Float32Array (<anonymous>)    at o (io_utils.ts:116)    at Object.decodeWeights (io_utils.ts:79)    at e.<anonymous> (frozen_model.ts:109)    at exports_regularizers.ts:47    at Object.next (exports_regularizers.ts:47)    at s (exports_regularizers.ts:47)`====='; ""The error message is a bit confusing; but it might simply be; that the url to the model shards is simply wrong. Check out the network tab and verify; that the requests don't result in 404s.=====""; ""> > > The error message is a bit confusing; but it might simply be; that the url to the model shards is simply wrong. Check out the network tab and verify; that the requests don't result in 404s.Thanks; No message in the Network.You can try yourself on my website: URL=====""; 'Everything is visible in my web now (goto url). You can see what I have in the web. I had to copy some folders to ""views"" folder to prevent some errors.====='; ""The first shard doesn't load correctly: `ssd_mobilenetv1_model-shard1 | 206 | fetch | tf-core.esm.js:17 | 1.0\xa0MB`. The size should be 4MB and not 1MB and the response code 206 is also suspicious to me. Maybe your file is corrupted or something?=====""; 'Have I put this shard file in a right place?====='; ""> > > The first shard doesn't load correctly: `ssd_mobilenetv1_model-shard1 | 206 | fetch | tf-core.esm.js:17 | 1.0 MB`. The size should be 4MB and not 1MB and the response code 206 is also suspicious to me. Maybe your file is corrupted or something?It seems that something was corrupted . I uploaded again and it is working now!=====""; 'Great!=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,reload file,reload file,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/104,Error on release version,6,closed,2018-10-17T13:22:38Z,2018-10-18T06:19:00Z,Dear when using the local version (debug version) the plugin works finewhen building a release version we get an error![error](https://user-images.githubusercontent.com/3799066/47089120-bb6fef00-d228-11e8-93fd-6f19918b5285.png)tho we installed an local certificate for ssl to run https since i thought it was a problem from the httpsregards,"['Looks like your loading of the JSON is failing. Are you sure the path to the json files for your release version is correct?====='; 'dear friend;we didn’t upload any json; just on debug mode it works fine on release it doesn’t work it has nothing to do with json could be a react version ? or a node version on release doing a problem ?On Oct 17; 2018; at 10:14 PM; ScottDellinger <notifications@github.com<mailto:notifications@github.com>> wrote:Looks like your loading of the JSON is failing. Are you sure the path to the json files for your release version is correct?—You are receiving this because you authored the thread.Reply to this email directly; view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/104#issuecomment-430753834>; or mute the thread<https://github.com/notifications/unsubscribe-auth/ADn4GuPbMcrxRg8HEgKneK0EUw5GBpV4ks5ul4GzgaJpZM4Xj599>.====='; 'Not sure what debug and release mode means for you; but this error message comes from the model loading function; meaning in your release setup the uri to your model files are different.====='; 'okay thank you so much tomorrow i will check iti’m using reactwhen building a release version (the version to be publish on a server) it looks like it’s changing something in the path i will check it back tomorrow and get back to youthank you once againhave a nice eveningOn Oct 17; 2018; at 11:51 PM; justadudewhohacks <notifications@github.com<mailto:notifications@github.com>> wrote:Not sure what debug and release mode means for you; but this error message comes from the model loading function; meaning in your release setup the uri to your model files are different.—You are receiving this because you authored the thread.Reply to this email directly; view it on GitHub<https://github.com/justadudewhohacks/face-api.js/issues/104#issuecomment-430784429>; or mute the thread<https://github.com/notifications/unsubscribe-auth/ADn4GlzQRh5GlMs4smuE9RQrDNVol1cgks5ul5g0gaJpZM4Xj599>.====='; 'Alright; if in doubt; simply open the network tab in the dev tools of your browser and you should find out the uri of the request when loading the model.====='; 'thank you guys; yes on release version it\'s loading the path directly from localhost (react) not from the folder where the published website isi will check the file and edit it it\'s better to use ""./path"" so we won\'t have the errorthank you again=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Model Usage/Design Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/88,Fail to provide models as as static assets,7,closed,2018-09-11T06:43:43Z,2019-12-03T15:52:33Z,"I've read through the post from https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07It mentioned that ""The model files can simply be provided as static assets in your web app"". So I've done two different approach and failed.1. Integration with vue.js: In this approach; then i got the following error;Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0    at eval (loadWeightMap.js?3bcc:12)    at step (tslib.es6.js?9ab4:97)    at Object.eval [as next] (tslib.es6.js?9ab4:78)    at fulfilled (tslib.es6.js?9ab4:68)2. Native approach:  A different error;loadWeightMap.js:12 Fetch API cannot load file:///Users/ian.chu/demo-face-api/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.(anonymous) @ loadWeightMap.js:12It seems to me that models can't be provided as static assets; but served by the web host. Could anyone correct me or show me how it can be done; I will be very grateful. Thanks.","['Hi;1: The issue might be due to; how assets are served in vue.js (PS: I am not familar with vue.js); e.g. the uri might simply be wrong. Open the network panel and verify; that the uri of the fetch request is correct.2: I think it should be selfexplaining; that a browser can not fetch from your filesystem.====='; '@Chuiantw1212Unexpected token < in JSON at position 0 - this issued occurred to me when I was loading all model files at the same time. Use babel-polyfill to implement async and await or use Promise function. Everything loads up; But then I got this error  Error: Constructing tensor of shape (4608) should match the length of values (876). The same code worked when using reactjs but do not know why not on vuejs. ====='; 'As I already pointed out the root of the issues with loading the models lies in requesting them from a wrong route. Investigating the outgoing request in the network panel should solve it.====='; 'I am have the same problem using Vue.  I am storing the weights in ./assets.  I am wondering if there needs to be a webpack loader added to load the weights.  How did you fix this problem?====='; '> I\'ve read through the post from https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07> > It mentioned that ""The model files can simply be provided as static assets in your web app"".> So I\'ve done two different approach and failed.> > 1. Integration with vue.js: In this approach; then i got the following error;>    Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0>    at eval (loadWeightMap.js?3bcc:12)>    at step (tslib.es6.js?9ab4:97)>    at Object.eval [as next] (tslib.es6.js?9ab4:78)>    at fulfilled (tslib.es6.js?9ab4:68)> 2. Native approach:  A different error;>    loadWeightMap.js:12 Fetch API cannot load file:///Users/ian.chu/demo-face-api/models/ssd_mobilenetv1_model-weights_manifest.json. URL scheme must be ""http"" or ""https"" for CORS request.>    (anonymous) @ loadWeightMap.js:12> > It seems to me that models can\'t be provided as static assets; but served by the web host.> Could anyone correct me or show me how it can be done; I will be very grateful. Thanks.Did you find any solution? getting the same errors in both the ways.====='; 'simply put the models into the ./public and not the ./src/assets directory of your vue project====='; 'seams reasonable =====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/71,Error: Constructing tensor of shape (4608) should match the length of values (876),6,closed,2018-08-17T11:53:43Z,2018-08-26T12:22:18Z,Hi; Getting this error on loading the models.,"['Can you paste a snippet of your model loading code? You are passing a the contents of a wrong .weights file to the load function of your model.====='; 'Iam using vuejs```<template>  <v-layout row wrap>    <v-flex xs12 sm6 offset-sm3>      <v-card>        <v-card-media>        <img id=""img1"" src=""../assets/AE.jpg"" /></v-card-media>        <v-card-title primary-title>          <div>            <h3 class=""headline mb-0"">Img-1</h3>          </div>        </v-card-title>      </v-card>    </v-flex>    <v-flex xs12 sm6 offset-sm3>      <v-card>        <v-card-media>          <img id=""img2"" src=""../assets/AE.jpg"" />        </v-card-media>        <v-card-title primary-title>          <div>            <h3 class=""headline mb-0"">Img-2</h3>          </div>        </v-card-title>      </v-card>    </v-flex>    <v-flex xs12>      <v-btn color=""primary"" @click=""recognize"">        click      </v-btn>    </v-flex>  </v-layout></template><script>import * as faceapi from \'face-api.js\';const MODEL_URL = \'/models\';export default {  name: \'HelloWorld\';  data () {    return {      msg: \'Face detection and recognition\';      minConfidence : 0.8;      maxResults : 10    }  };  mounted(){    this.initialize();  };  methods: {    initialize: async function(){        await faceapi.loadFaceDetectionModel(MODEL_URL)        await faceapi.loadFaceLandmarkModel(MODEL_URL)        await faceapi.loadFaceRecognitionModel(MODEL_URL)        let myImg1 = document.getElementById(\'img1\')        console.log(myImg1)        const fullFaceDescriptions = await faceapi.allFaces(myImg1; this.minConfidence)        fullFaceDescription.forEach((fd; i) => {          faceapi.drawDetection(myImg1; fd.detection; { withScore: true })        })    };    recognize: function(){}  }}</script>```====='; '""/models"" contain all the files from this https://github.com/justadudewhohacks/face-api.js/tree/master/weights====='; '@justadudewhohacks ====='; ""I am guessing loading the shard files failed. Have a look at the network tab; to verify that the requests for the shard files don't result in a 404.Or you are hosting the wrong files along with the manifest or some shard files are missing. Can you append a screenshot of the contents of your /models dir.=====""; '@justadudewhohacks  all files are loading successfully. I used reactjs instead of vuejs and same code loaded all model files without any error. Thank you for reply.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.3,C.1
https://github.com/Volcomix/virtual-background/issues/16,Wrong path for tflite.wasm,12,closed,2021-05-30T05:09:42Z,2021-06-12T11:14:45Z,"I'm trying to add the virtual background to an angular 11 project and getting an error when trying to load the tflite model:```GET http://localhost:4200/tflite.wasm 404 (Not Found)tflite.js:9 wasm streaming compile failed: TypeError: Failed to execute 'compile' on 'WebAssembly': HTTP status code is not oktflite.js:9 falling back to ArrayBuffer instantiationGET http://localhost:4200/tflite.wasm 404 (Not Found)failed to asynchronously prepare wasm: RuntimeError: abort(both async and sync fetching of the wasm failed). Build with -s ASSERTIONS=1 for more info.```This is how I import tflite.js:```javascriptimport createTFLiteModule from './tflite/tflite';import createTFLiteSIMDModule from './tflite/tflite-simd';```The path to tflite.wasm is hardcoded in tflite/tflite.js. Is there a config option to set the path without modifying tflite.js?```javascriptvar wasmBinaryFile=""tflite.wasm""```","[""I hope there is a way to import wasm module using ES modules like you did but I haven't found the right way to do it. So what I did in this demo was to put `tflite.js` and `tflite.wasm` in the public directory (aka `assets` in Angular); manually added the required `<script>` tags in [`index.html`](https://github.com/Volcomix/virtual-background/blob/c367b96ba0fc2213a9ff262dde06e352754b5288/public/index.html#L46) and manually declared [`createTFLiteModule`](https://github.com/Volcomix/virtual-background/blob/c367b96ba0fc2213a9ff262dde06e352754b5288/src/core/hooks/useTFLite.ts#L7) for TypeScript to be happy when calling it without any associated `import`.=====""; ""I didn't find a way to set the path yet. Maybe we can add a global variable like TFLITE_URL and TFLITE_SIMD_URL that the js file will use if exists. Not sure if relevant.=====""; ""Actually `tflite.js` and `tflite.wasm` are generated by [emscripten](https://emscripten.org/) and the glue code in the js file is overwritten each time I build tflite in wasm. I don't think that would be a good idea to edit the file manually so unless there is an existing option in emscripten to specify the wasm file URL (which I can't find); I don't see any other alternative than importing those files from outside of the bundled code.=====""; 'Relative path should work when loading tflite.js in a script tag because the glue code use [document.currentScript.url](https://developer.mozilla.org/en-US/docs/Web/API/Document/currentScript). See https://github.com/emscripten-core/emscripten/pull/5368```var _scriptDir = typeof document !== \'undefined\' && document.currentScript ? document.currentScript.src : undefined;```Angular dev server generate the html dynamically with <script src=""app.js""></script>. That\'s why document.currentScript.url points to the root path.I think on dev we can add a proxy that handles the wasm files specifically. Maybe with ""ng serve --proxy-config proxy.conf.json"".On production; as long as the .js and wasm files will be in the same path it should work because of document.currentScript.url.====='; 'Maybe we need```-s EXPORT_ES6=1 will turn the JavaScript code into an ES6 module with a default export that works with any bundler. Also requires -s MODULARIZE=1 to be set.```====='; 'Running a build with this option (https://github.com/Volcomix/virtual-background/commit/cdad50c0f67c3537854c1b9ea6b8bef498b55fcc): https://github.com/Volcomix/virtual-background/actions/runs/890576517====='; ""There is also [USE_ES6_IMPORT_META=1](https://github.com/emscripten-core/emscripten/blob/main/src/settings.js#L1166):// Use the ES6 Module relative import feature 'import.meta.url'// to auto-detect WASM Module path.// It might not be supported on old browsers / toolchainsvar USE_ES6_IMPORT_META = 1;=====""; ""I updated tflite js files in [this branch](https://github.com/Volcomix/virtual-background/tree/emscripten-es6-export). React is yelling at me when trying to import them as ES6 modules:![image](https://user-images.githubusercontent.com/7324857/120114117-ead78e00-c17d-11eb-9163-4db3e29a0313.png)But maybe could you have more luck with Angular.USE_ES6_IMPORT_META is enabled by default so I'm gonna run a new build with this option disabled.=====""; ""I'm getting a similar error with Angular 11. EXPORT_ES6=1 with import.model.url is probably the solution but react and angular tooling are not compatible with import.model yet.=====""; ""I still prefer the automatic import.model.url solution but we can also pass custom [locateFile function](https://emscripten.org/docs/api_reference/module.html#Module.locateFile):```javascriptconst createdTFLiteSIMD = await createTFLiteModule({locateFile: () => 'path/to//tflite.wasm'});```=====""; 'Still an error with `""-s USE_ES6_IMPORT_META=0""`:![image](https://user-images.githubusercontent.com/7324857/121773236-ce265780-cb7a-11eb-813e-1f61045b2b27.png)====='; 'Not sure about complication flags but we can use locateFile  to dynamically specify the model location.This is from the [Selfie Segmentation example](https://google.github.io/mediapipe/solutions/selfie_segmentation#javascript-solution-api):```javascriptconst selfieSegmentation = new SelfieSegmentation({locateFile: (file) => {  return `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}`;}});```=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change framework position,change framework position,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",A.3,C.1
https://github.com/infinitered/nsfwjs/issues/219,Using s3 bucket works but when I switch to local model I get an error,2,closed,2019-12-05T01:02:14Z,2019-12-05T19:04:46Z,"```Error: Error when checking : expected input_1 to have shape [null;299;299;3] but got array with shape [1;224;224;3].    at new t (tf-layers.esm.js?271e:17)    at checkInputData (tf-layers.esm.js?271e:17)    at t.predict (tf-layers.esm.js?271e:17)    at eval (index.js?9368:92)    at eval (tf-core.esm.js?45ef:17)    at t.scopedRun (tf-core.esm.js?45ef:17)    at t.tidy (tf-core.esm.js?45ef:17)    at Module.je (tf-core.esm.js?45ef:17)    at NSFWJS.eval (index.js?9368:91)    at step (index.js?9368:33)``````html<template>  <div>    <img ref=""img"" src=""@/assets/img.jpg"">  </div></template>``````javascript<script>import * as nsfwjs from ""nsfwjs"";export default {  async mounted () {    // const model = await nsfwjs.load();    // ""/static/model/"" is mapped as ""/model/"" using Nuxt.js    const model = await nsfwjs.load(""/model/"");    const predictions = await model.classify(this.$refs.img);    console.log(""Predictions: ""; predictions);  }};</script>```","[""I see the issue.  You're using the larger model; right?The larger model takes slightly larger images (299 pixels); so you can tell it you're using the more advanced model like so: `const model = nsfwjs.load('/path/to/different/model/'; {size: 299})`The 299 tells it to expect 299x299 tensors on the model.  Having 2 models with different sizes is the issue.  I have a note in the docs; but it's easy to miss.I hope this fixes it!=====""; 'It fixed the problem. Thanks.=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,parameter modifier,Modify API Parameter usage,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.3,C.1
https://github.com/hugozanini/realtime-semantic-segmentation/issues/3,Fail to load trained model,2,open,2020-08-05T12:40:37Z,2020-10-01T00:45:38Z,When trying to load the pre-trained model; I get the following error:```Unhandled Promise Rejection: Error: Based on the provided shape; [576]; the tensor should have 576 values but has 100```This is at this line:https://github.com/hugozanini/realtime-semantic-segmentation/blob/master/src/index.js#L24,['I want to work on this issue @hugozanini ====='; 'Hey @carrycooldude feel free to work on that. Thanks for contributing to the project :) ====='],Data & Model Error,Crash,Unknown,,,,,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,E
https://github.com/shaqian/tfjs-yolo/issues/3,l.loadModel is not a function - tfjs 1.1.2,2,closed,2019-06-14T02:01:49Z,2019-06-17T15:59:41Z,"```""@tensorflow/tfjs"": ""^1.1.2"";""tfjs-yolo"": ""^0.0.3"";```I get `l.loadModel is not a function`; same issue as https://github.com/ModelDepot/tfjs-yolo-tiny/pull/22. If I patch per that PR (use `loadLayersModel`) I get `myYolo.predict is not a function`. I actually tried pinning tfjs back to `^1.0.0`; but I get the same errors (strangely; since this project is pinned at that version..)","[""I think it's fixed in https://github.com/shaqian/tfjs-yolo/pull/2. I just published the change in v0.0.4. =====""; 'confirmed fixed; thanks!=====']",Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.3
https://github.com/kevinisbest/FrontEnd-EmotionDetection/issues/1,EmotionModel.'Predict' of undefined at onPlay,2,closed,2019-10-23T06:22:37Z,2020-07-31T11:46:00Z,I am loading the TinyfaceDetectWebcam.html; where the camera is getting stuck and I could see the below issue on console.Uncaught (in promise) TypeError: Cannot read property 'predict' of undefined    at onPlay (TinyFaceDetectWebcam.html:165)It was working before; but i don't know why its not working now. Could be reference issue which I am not sure.![image](https://user-images.githubusercontent.com/15680777/67364135-9117e680-f58c-11e9-914f-af1f742dfde8.png)EmotionModel is undefined; sometimes not.I am not sure whether it is dependent on where we host because in local WampServer I don't have any issue but when I hosted on Remote Server and enabled openSSL for secure connection I see this issue.,"[""I have the same issue and I don't know how to solve it=====""; 'Hi @OllyOli @shreeraj04;Sorry for the late reply; please pull the latest version and try again.I modified the paths of the model and the library.=====']",Reference Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.6""
  }
}
```",A.1,A.3
https://github.com/tensorspace-team/tensorspace/issues/214,Test and upgrade dependency version,1,closed,2019-02-23T07:26:34Z,2019-02-23T10:47:33Z,Test and make sure tensorspace work well with latest tfjs v1.0.0-alpha3,['loadFrozenModel now will trigger warning; caused by upgrade to 1.0.0-alpha3. Issue #212 will fix this warning.====='],Reference Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.3] Others"",
    ""specific_type"": ""[B.3.1] Regression""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.3] Untimely Update""
  }
}
```",A.1,A.3
https://github.com/tensorspace-team/tensorspace/issues/212,Align loader API with new tfjs loaders,3,closed,2019-02-22T05:09:44Z,2019-02-24T11:23:19Z,* tf.loadFrozenModel() -> tf.loadGraphModel()* tf.loadModel() -> tf.loadLayersModel(),['depend on #214 ====='; 'Change:* tf.loadGraphModel() do not need to provide weightUrl====='; 'TODO:- [x] Change `tensorflow loader` API in TensorSpace; do not need to provide weightUrl- [x] Clear up examples using `tensorflow loader`====='],Reference Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/621,How to load model in Ionic react,1,open,2020-05-20T19:29:02Z,2021-02-06T17:09:29Z,"First of all; thank you for this awesome plugin. I am using face-api.js in my Ionic react project to detect face from image clicked by user. however the detection canvas only works in browser ionic serve and not working after i compiled it into Android apk and IOS app. Please help me on this.export const loadModels = () => {  const MODEL_URL = `${process.env.PUBLIC_URL}/models`;  return Promise.all([    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);    faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL);  ]);};I also; try using **loadModel**(see below) and using **loadFromDisk**; but so far no luck    const MODEL_URL = process.env.PUBLIC_URL + ""/models"";  await faceapi.loadTinyFaceDetectorModel(MODEL_URL);  await faceapi.loadFaceLandmarkTinyModel(MODEL_URL);  await faceapi.loadFaceRecognitionModel(MODEL_URL);If anybody with similar issue and have idea how to resolve this. Can you please let me know where I am doing wrong? I appreciate your help Thank you",['Have you tried changing the filename extension of the model file and the paths value of model-weights_manifest.json to .bin?====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] API Request Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.3] Cross-platform App Framework Incompatibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/445,sample.php:191 Uncaught (in promise) SyntaxError: Unexpected token < in JSON at position 0,1,open,2019-10-21T21:08:38Z,2019-10-26T07:35:47Z,"This errors always appears when i load the script of the model<img width=""866"" alt=""Screenshot 2019-10-22 at 5 07 35 AM"" src=""https://user-images.githubusercontent.com/3221892/67243346-f2a35c80-f489-11e9-812d-9d3705aa7890.png"">or you can view and inspect the element directlyhttps://appinsg.com/facial/sample.php",['This is probably because the URL you are loading the models from is not correct; e.g. the manifest. json files are not loaded correctly.Check the network tab and make sure models are loaded from the right URL.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/281,Tensor size error,13,open,2019-04-29T08:41:34Z,2019-11-13T11:05:58Z,Hi! I have a problem while loading the model with `faceapi.nets.faceLandmark68Net.loadFromUri('/public/models')`The model is available at the `models` directory. But an error arises while loading the model selected. In this case `face_landmark_68_model-shard1` and `face_landmark_68_model-weights-manifest.json` are placed into the directory. But the following error appears:`Error: Based on the provided shape; [3;3;32;1]; the tensor should have 288 values but has 64`Can someone help me please?EDIT: This error appears if I my loading function is not `async`; if I make it `async` another error appears (`regeneratorRuntime is not defined`)Thanks for your info,"['<img width=""849"" alt=""Captura de pantalla 2019-04-30 a las 13 48 29"" src=""https://user-images.githubusercontent.com/32225295/56959728-b3d1c100-6b4e-11e9-8dca-ac5b093a41c1.png"">This is shown in the Inspector Network Tab. Apparently the GET request for the shard is not complete; but returns a 200 code. Does anyone know why is this possible?====='; 'Where do you host your application? Or does this also happen to be the case on your local dev environment (localhost)?====='; 'It happens in my local environment. I solved the error of the shape by creating an extension to the `shard1` file and in the `json` file setting the new name of the shard at the end with the extension I placed.But still having the `regeneratorRuntime` and trying to solve it.====='; ""This means that the node version you are using doesn't seem to support async functions? Try updating nodejs to a more recent version or use babel if you have to use an older version.=====""; 'Ok; my advances. I achieved to resolve it. And another problem arose 🤦\u200d♂. When using detectFaceLandmarks I am being returned a Promise. If I need to save those landmarks for the next videoFrame/Image how can I return that value outside the promise or is there any method to use faceapi ""synchronously""? **Sidenote**: if I use `faceapi.detectFaceLandmarks(image);` on full image (not just an image cropped to the face) am I doing something wrong? It returns unexpected values.====='; '> If I need to save those landmarks for the next videoFrame/Image how can I return that value outside the promise or is there any method to use faceapi ""synchronously""?I don\'t totally get what you are trying to achieve; but why don\'t you simply wait for the promise to resolve?> Sidenote: if I use faceapi.detectFaceLandmarks(image); on full image (not just an image cropped to the face) am I doing something wrong? It returns unexpected values.Yeah that\'s not going to work out; since the face landmark net expects the cropped face image as input; which is obtained after face extraction based on the face box detected from the face detector.====='; ""```async function detectLandmarks(image) {  const landmarks = await faceapi.detectFaceLandmarks(image);  return landmarks;}```> I don't totally get what you are trying to achieve; but why don't you simply wait for the promise to resolve?That was my attempt. I am trying to wait the promise to resolve; but the method I pasted above; inside it (before return) is not a Promise; is resolved; but after return; where I use it is a Promise again and I can only use its values with the `then` structure (which I don't want)=====""; '`faceapi.detectFaceLandmarks(image)` returns a Promise and `await faceapi.detectFaceLandmarks(image)` resolves the Promise. Does that solve it?====='; ""Not at all. If I log the `landmarks` inside the method:```async function detectLandmarks(image) {  const landmarks = await faceapi.detectFaceLandmarks(image);  console.log(landmarks);  return landmarks;}```It doesn't print a promise but the value of the landmarks. (Here is where I said Yay! I got it).But when using the method like:```outsideLandmarks = detectLandmarks(outsideImage);console.log(outsideLandmarks)```Which simply calls the method defined with the await inside it; prints a Promise.Maybe I'm missing something or doing something wrong.Tell me if I didn't explain myself.Thanks for your patience=====""; ""An async function always returns a Promise. Maybe I do not fully understand; what's the issue you are facing with the function returning a Promise?=====""; ""```state = initialState;computedLandmarks = computeLandmarks(detectLandmarks(image));updateState depending on the landmark computation;```This is what I'm trying to do more or less. With the promises what I have to do is:```state = initialState;computedLandmarks = detectLandmarks(image)     .then((landmarks)=>                computation = computeLandmarks(landmarks)                return computation);updateState depending on the landmark computation;```I cant do this because I cant extract the computation from the promise. So I cant update the state=====""; 'I managed to solve it serving model files from a [cdn](https://gitcdn.xyz/)Use https://gitcdn.xyz/repo/justadudewhohacks/face-api.js/master/weights/ as `MODEL_URL`====='; 'This is a serious problem for using `parcel` to do development with this library:https://github.com/elwin013/parcel-plugin-static-files-copy/issues/37https://github.com/parcel-bundler/parcel/issues/1098https://github.com/tensorflow/tfjs/issues/924=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model file extension,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/tensorspace-team/tensorspace/issues/215,Stats error in LenetTraining example,2,closed,2019-02-23T10:22:46Z,2019-02-23T10:32:06Z,Stats error in [LenetTrainingExample](https://github.com/tensorspace-team/tensorspace/tree/master/examples/trainingLeNet); caused by stats usage change in SceneInitializer.,['Fixed by checking existence `Stats` type before `import` sentence.====='; '@BoTime The usage of Stats in sceneInitializer seems a little bit weird; maybe need further discussion.====='],render bug,Crash,Incorrect Code Logic,,,type replacer,Replace data Shape/type,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.0""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A,A.4
https://github.com/tensorspace-team/tensorspace/issues/54,Fix dense clear bug,1,closed,2018-09-21T11:07:24Z,2018-09-21T12:17:28Z,If dense layer is opening; call clear will have bug.,['Fixed; caused by repeatedly setting layer status.====='],render bug,Crash,Incorrect Code Logic,,,remove unnecessary code,remove unnecessary code,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A,A.4
https://github.com/tensorspace-team/tensorspace/issues/30,Fix hover error log,1,closed,2018-09-15T13:30:19Z,2018-09-16T06:44:20Z,Hover on layer sometimes show error log; may be caused by last layer's status.,"['Caused by set isOpen status too late; and for layer1d; add a ""isTransition"" status.=====']",render bug,Crash,Incorrect Code Logic,,,remove unnecessary code,remove unnecessary code,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A,A.4
https://github.com/infinitered/nsfwjs/issues/512,Cannot load latest (v1.1) nsfw_model from local file,8,open,2021-05-07T15:34:51Z,2021-06-26T15:32:16Z,"I currently have the latest nsfw_model found at https://github.com/GantMan/nsfw_model/releases/tag/1.1.0 and am trying to load it as a local file like so:```jsconst model = await nsfw.load('file://' + __dirname + '/mobilenet_v2_140_224/web_model_quantized/');```However this results in the following error:```(node:28129) UnhandledPromiseRejectionWarning: Error: layer: Improper config format: (LARGE JSON OBJECT HERE).'className' and 'config' must set.```I tried the solution found in #397; specifically [this comment](https://github.com/infinitered/nsfwjs/issues/397#issuecomment-702773846); however when running `npm install @tensorflow/tfjs@^1.7.4` I get:```npm ERR! npm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! Found: @tensorflow/tfjs@1.7.4npm ERR! node_modules/@tensorflow/tfjsnpm ERR!   @tensorflow/tfjs@""^1.7.4"" from the root projectnpm ERR! npm ERR! Could not resolve dependency:npm ERR! peer @tensorflow/tfjs@""^3.1.0"" from nsfwjs@2.4.0npm ERR! node_modules/nsfwjsnpm ERR!   nsfwjs@""^2.4.0"" from the root projectnpm ERR! npm ERR! Fix the upstream dependency conflict; or retrynpm ERR! this command with --force; or --legacy-peer-depsnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.npm ERR! npm ERR! See /home/jon/.npm/eresolve-report.txt for a full report.npm ERR! A complete log of this run can be found in:npm ERR!     /home/jon/.npm/_logs/2021-05-07T15_30_17_117Z-debug.logcode ERESOLVE```Running `npm install @tensorflow/tfjs@^1.7.4 --force` works to install that version but then the `load` function fails to load a local file; saying only absolute URLs are supported@tensorflow/tfjs-node: ^3.6.1nsfwjs: ^2.4.0","[""Thanks for the detailed ticket. Can you make a small demo-repo and I'll pull that down and debug the issue?  That's he most effective way for me to construct a fix=====""; 'I seem to have found the issue while making the demo repo (which honestly was only going to be like 3 lines of code)I was following the ""NodeJS App"" example here https://github.com/infinitered/nsfwjs#node-js-app which only says to pass in the path to the model folderHowever with the latest nsfw_model it seems to need you to pass in `{ type: \'graph\' }` into the options? This seems to have loaded the model correctly and it does seem to classify correctly stillMaybe this should be better explained in the README and examples? It wasn\'t immediately obvious that I needed to pass in this option to get the local files to work; especially since when looking at the source of this module it _doesn\'t_ pass that option...?====='; ""Ahhh; which model are you using?  The 93% accurate one?Please feel free to contribute back to the docs.  It's easy to forget to update them; and your experience from the outside is valuable to others who come in and use the docs.=====""; '> I seem to have found the issue while making the demo repo (which honestly was only going to be like 3 lines of code)> > I was following the ""NodeJS App"" example here https://github.com/infinitered/nsfwjs#node-js-app which only says to pass in the path to the model folder> > However with the latest nsfw_model it seems to need you to pass in `{ type: \'graph\' }` into the options? This seems to have loaded the model correctly and it does seem to classify correctly still> > Maybe this should be better explained in the README and examples? It wasn\'t immediately obvious that I needed to pass in this option to get the local files to work; especially since when looking at the source of this module it _doesn\'t_ pass that option...?Hi RedDuck; I tried the following:`_model = await nsfw.load(\'.\\models\\2020-03-04 - nsfw_mobilenet_v2_140_224\\mobilenet_v2_140_224\\web_model\'; ""{ type: \'graph\' }"")`But I get this error:`Cannot create property \'size\' on string \'{ type: \'graph\' }\'`How did you pass in the `{ type: \'graph\' }` option?Thanks;Fidel====='; ""Oh; never mind :)This worked:`_model = await nsfw.load('file://' + __dirname + '/models/2020-03-04 - nsfw_mobilenet_v2_140_224/mobilenet_v2_140_224/web_model/'; { type: 'graph'; })`=====""; ""> Please feel free to contribute back to the docs. It's easy to forget to update them; and your experience from the outside is valuable to others who come in and use the docs.Sure; I can do that. How would the best way to do that be? Just make a PR for the readme to add `{ type: 'graph' }`?=====""; 'Yup!  And any additional context.====='; 'Just checking in on this ticket.  =====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,parameter modifier,Modify API Parameter usage,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.4] Attribute/Return Value Undefined""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/596,Face Recognition on Webcam Implementation: Cannot read property 'descriptor' of undefined,7,open,2020-04-15T17:35:39Z,2020-06-13T10:53:56Z,"Urgent: Facing this issue in the implementation of face recognition on the webcam camera for the browser. This error only comes when I add more than 1 name in my labels in loadLabeledImages() function. If I have one name; it works perfectly fine. Any help is appreciated. This is a part of a project that has to be completed in a week!Thank you so much!Getting ""Uncaught (in promise) TypeError: Cannot read property 'descriptor' of undefined""![image](https://user-images.githubusercontent.com/35863027/79368026-d3c8e300-7f5f-11ea-9901-5a209eadf3e0.png)This is the code for your reference:```` const video = document.getElementById('video')  Promise.all([   faceapi.nets.tinyFaceDetector.loadFromUri('/models');   faceapi.nets.faceLandmark68Net.loadFromUri('/models');   faceapi.nets.faceRecognitionNet.loadFromUri('/models');   faceapi.nets.ssdMobilenetv1.loadFromUri('/models'); ]).then(startVideo)``````function startVideo() {  navigator.getUserMedia(    { video: {} };    stream => video.srcObject = stream;    err => console.error(err)  )}this.video.addEventListener('play';() => {  const canvas = faceapi.createCanvasFromMedia(video)  document.body.append(canvas)  const displaySize = { width: video.width; height: video.height }  faceapi.matchDimensions(canvas; displaySize)  setInterval(async () => {    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors()    const resizedDetections = faceapi.resizeResults(detections; displaySize)    canvas.getContext('2d').clearRect(0; 0; canvas.width; canvas.height)    faceapi.draw.drawFaceLandmarks(canvas; resizedDetections)    this.labeledFaceDescriptors = await this.loadLabeledImages()    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors; 0.6)    const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))    results.forEach((result; i) => {      const box = resizedDetections[i].detection.box      const drawBox = new faceapi.draw.DrawBox(box; {label: result.toString()})      drawBox.draw(canvas)     })  }; 100) })function loadLabeledImages() {  try{    const labels = ['Shriya'; 'judhi']    return Promise.all(      labels.map(async label => {        const descriptions = []        for (let i = 1; i <= 3; i++) {          const img = await faceapi.fetchImage(`public/img/${label}/${i}.jpg`)          const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()          descriptions.push(detections.descriptor)        }          return new faceapi.LabeledFaceDescriptors(label; this.descriptions)      })    )  }  catch(err){    console.log(err)  }}````","[""It is because your label image doesn't detect any face. You need to ensure the images are person face=====""; '@chenchiuchi Thank you so much; this fixed my problem. The issue was that my image was of bad quality. Thank you====='; 'Sorry; I have another question. What do I do to record the timestamp of whenever the face is recognised? I am making an attendance system so I need to record the check-in; check-out time. Thank you.====='; '@idkidk-idk leave the timestamp to your backend (or DB level). I mean; once you have a successful match; you make a request to your backend to record the clock in/out.====='; 'Hi @idkidk-idk;I have a problem as above; so as your comment ""The issue was that my image was of bad quality."" I have updated my image so it still displays the error. Could you please help me share your opinion?====='; ""Heyy @ThaiNhung I was stuck with this issue for almost half a month. My problem was that I had taken a screenshot of a pretty small picture and saved it and never looked at it again. So I thought my code was wrong and I kept making changes to the code. I opened the picture and noticed its too small and hence the quality was bad and I replaced it and everything worked.So basically if your error points out to the descriptor in the loadLabledImages() function then it's because it can't find a face in the image you have stored and nothing is wrong with your code. If its not your quality; then there should be something wrong with perhaps the color or some other element that is making it difficult to recognise the face in the stored image. So I guess try changing it to a clear picture of just the face.I hope it helped! My code remained the same. Let me know if you need any help from my side! Good luck <3=====""; 'Hi @idkidk-idk ; Thank you for your quick response. As your comment; I try to change to a clear picture just the face so so it still displays this error. Besides; I take care of some elements such as color; Brightness but it not feasible. =====']",Reference Error,Crash,API Misuse,,,add input preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/260,Use withFaceLandmarks() and withFaceExpressions() on same code.,5,open,2019-03-29T21:29:49Z,2019-06-17T15:58:47Z,Hi;I'm trying to add this line on the code webcamFaceExpressionRecognition.html:const result = await faceapi.detectSingleFace(videoEl; options).withFaceLandmarks()but I get every time an error saying that I should load the module. Could you help me please.tanks; Jayme,"[""Can you give me more information?Have you loaded the models?Can you give a screenshot or error message?I understand you haven't done the preloads.[Loading the models](https://github.com/justadudewhohacks/face-api.js#loading-the-models)##### (for TinyFaceDetector) Try this:```jsawait faceapi.nets.tinyFaceDetector.load('/models');await faceapi.loadFaceRecognitionModel('/models');await faceapi.loadFaceLandmarkModel('/models');```=====""; 'Hi;this is my error:![Captura de tela de 2019-04-01 12-57-22](https://user-images.githubusercontent.com/25558078/55341945-ff287f00-547d-11e9-9d92-decbeeebfb9c.png)The only change I made on the code was adding:const result2 = await faceapi.detectSingleFace(videoEl; options).withFaceLandmarks()====='; ""OK; i got it. You didn't load the landmarks.#### Import this files:`face_landmark_68_model-weights_manifest.json``face_landmark_68_model-shard1`#### And add this code:```jsawait faceapi.loadFaceLandmarkModel('/models/')```=====""; ""> OK; i got it. You didn't load the landmarks.> > #### Import this files:> `face_landmark_68_model-weights_manifest.json`> `face_landmark_68_model-shard1`> > #### And add this code:> ```js> await faceapi.loadFaceLandmarkModel('/models/')> ```Hi; where can I see those files? Thank you.=====""; '[Here it is](https://github.com/justadudewhohacks/face-api.js/tree/master/weights)=====']",Data & Model Error,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/788,detect landmarks in node js,1,closed,2021-05-01T14:35:46Z,2021-05-03T08:53:58Z,I am trying to detect facial landmarks in node js. My approach was the following:```const { createCanvas; loadImage } = require('canvas');const MODEL_URL = `${__dirname}/public/models/`;await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODEL_URL);await faceapi.nets.faceLandmark68Net.loadFromDisk(MODEL_URL);await faceapi.nets.faceRecognitionNet.loadFromDisk(MODEL_URL);//create canvas for the faceapiconst width = wmin;const height = hmin;var images = [outputImg1; outputImg2]; // 2 images from a local foldervar fullFaceDescriptions;for (var i = 0; i < 2; i++) { //create a canvas for each image    const canvas = createCanvas(width; height);    const context = canvas.getContext('2d');    loadImage(__dirname + '/' + images[i]).then(image => {         context.drawImage(image; 0; 0); //draw image    });    fullFaceDescriptions = await faceapi.detectSingleFace(canvas).withFaceLandmarks();}```But I get the following error and I can't understand how to call the faceapi for an image stored in a local folder: Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D; or to be an element id,"[""since you're already using `canvas` package; just tell `face-api` to use it as well as there is no native canvas support in `NodeJS`:```jsconst canvas = require('canvas');const { Canvas; Image; ImageData } = canvas;faceapi.env.monkeyPatch({ Image; Canvas; ImageData })```or alternatively; convert your canvas to tensor and pass that tensor to `faceapi`; i wrote several examples how to do that recently.=====""]",Data & Model Error,Crash,API Misuse,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Media Type Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/759,two concurrent video feeds; mask rendering degrades and almost stops after 10 min..,1,closed,2021-02-16T01:21:15Z,2021-02-16T14:57:49Z,"over about 10-15minutes; the masking (with landmarks; no points) of my two concurrent; different video feeds goes from immediately reactive to slower and slower to the point it barely reacts. I've included two snippets of the performance (10 sec clip or so). It looks like `getImageData` ( *think* within `extractFaces`?) starts to drag from under 50ms to over 250ms. This happens on both a Mac laptop and a Mac iphone x. Any thoughts/ideas for how to address? -my canvas is 435x256 and the method containing the `draw` function is invoked asynchronously-my video is hidden; and is 640x480```if (canvasOverlayBuff && canvasOverlayBuff.width > 0) {      canvasOverlayBuff.width = 435;      canvasOverlayBuff.height = 256;      const drawObj = new self.window.faceapi.draw.DrawFaceLandmarks(        lastLandmarks[`${mode}Video${pId}`]?.landmarks;        {          drawLines: true;          drawPoints: false;        }      ).draw(canvasOverlayBuff);      console.log(""faces drawn "");```<img width=""431"" alt=""Screen Shot 2021-02-15 at 8 10 18 PM"" src=""https://user-images.githubusercontent.com/659092/108007119-2c776b00-6fcb-11eb-8a4b-62bfd2cd3ef0.png""><img width=""455"" alt=""Screen Shot 2021-02-15 at 8 07 54 PM"" src=""https://user-images.githubusercontent.com/659092/108007121-2da89800-6fcb-11eb-84ad-2f1c38d86af9.png"">","[""I figured out my issue(s). 1. I was invoking `setTimeout` multiple times within `useEffect`. 2. I wasn't clearing the `Timeout`fix use `setInterval` (more appropriate anyway); store the id in state; and clear the interval upon cleanup == AWESOME performancehere's the code if this helps others:```let [intervalId; setIntervalId] = useState(null);  useEffect(() => {    if (participants) {      if (!intervalId) {        intervalId = setInterval(function () {          pullAndPostFrameTimer(mode; participants);        }; 100);  // now that I'm using setInterval; and correctly;  I can really knock this render number way down      }    }    return function cleanup() {      clearInterval(intervalId);    };  });    [participants];  return <div />;```=====""]",Slow Execution,Poor Performance,Incorrect Code Logic,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.1.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/420,load model before inference,5,closed,2019-09-20T13:43:30Z,2020-08-24T16:41:37Z,"<!doctype html><html>	<head>		<meta charset=""utf-8"">		<title>title</title>		<!-- build:css css/main.min.css -->		<link rel=""stylesheet"" href=""css/main.css"">		<!-- endbuild -->	</head>	<body>		https://sun9-58.userapi.com/c638524/v638524146/57f04/76YNYJz82jY.jpg				<img id=""myImg"" src="""">				<button>get img</button>		<script src=""js/jquery.js""></script>		<script src=""js/face-api.js""></script>		<!-- build:js js/js.min.js -->		<script src=""js/main.js""></script>		<!-- endbuild -->	</body></html>main.js$(document).ready(function() {	$('button').on('click'; getImg);		async function getImg(){  		  		const image = await faceapi.fetchImage('https://sun9-58.userapi.com/c638524/v638524146/57f04/76YNYJz82jY.jpg')		console.log(image instanceof HTMLImageElement) // true		// displaying the fetched image content		const myImg = document.getElementById('myImg')		myImg.src = image.src		const detections = await faceapi.detectAllFaces(image; await new faceapi.SsdMobilenetv1Options({ minConfidence: 0.6 }));		//console.log(detections)		console.log(detections)	}});Error:face-api.js:4675 Uncaught (in promise) Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (face-api.js:4675)    at SsdMobilenetv1.<anonymous> (face-api.js:4709)    at step (face-api.js:78)    at Object.next (face-api.js:59)    at fulfilled (face-api.js:49)","[""You should call the method to load the models as stated [here](https://github.com/justadudewhohacks/face-api.js#loading-the-models) in the doc before using the 'detectAllFaces' method in your case=====""; 'Hi can you give me a code snippet how to do in react js I have tried but giving me same error?====='; 'Hi. Even I get the same error on react js.  I have included it.Promise.all([            faceapi.nets.tinyFaceDetector.loadFromUri(MODELURL);            faceapi.nets.faceLandmark68Net.loadFromUri(MODELURL);            faceapi.nets.ssdMobilenetv1.loadFromUri(MODELURL)            ]).then(console.log(""Loaded models"")).catch(e=>console.log(e))It outputs ""Loaded models\'; but it still shows the above error====='; 'Hello; for anyone looking for the solution to React js; you need to download the ""models"" folder from https://github.com/justadudewhohacks/face-api.js/tree/master/weights and add the folder to your ""public"" folder.After that; you can simply load it via Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri(""/models"");faceapi.nets.faceLandmark68Net.loadFromUri(""/models"");faceapi.nets.ssdMobilenetv1.loadFromUri(""/models"")])====='; 'Hi; I\'m getting the same error despite loading the models; I also have the models in my public folder and provided the paths correctly but still getting the same error.Script File:const MODEL_URL = ""/weights"";const net = new faceapi.SsdMobilenetv1()Promise.all([    net.loadFromUri(MODEL_URL);    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);]).then((val) => {    console.log(val)}).catch((err) => {    console.log(err)})Express Server :const express = require(""express"")const path = require(\'path\')const app = express()const hbs = require(""hbs"")const publicFolder = path.join(__dirname; \'../public\')const viewsPath = path.join(__dirname; ""../views"")app.use(express.static(publicFolder));app.set(""view engine""; ""hbs"");app.set(""views""; viewsPath);app.get(\'/\'; (req; res) => {    res.render(\'index\')})app.listen(7000; () => {    console.log(""Listening On port 7000"")})=====']",Data & Model Error,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/241,"the  ""BBT Face Matching"" functions accuracy is very low & slow in ""example-browser"" when using my own photos.",2,closed,2019-03-09T01:15:09Z,2019-03-20T11:05:52Z,"I have no idea why the accuracy is very low(10%); what I did as below.I have created 2 new folders under example/images and edited 5 png photos for each folder(one is mine); also change ""classes"" in the file of example-browser/public/js/bbt.js to be ""const classes = ['asuix';'asui']""; when I refresh the running code; the accuracy is very low.Would you please give some advises?Thanks & Best RegardsSui","['You probably forgot to perform face alignment. If you are referring to the bbt face matching example: this example expects the face images to be aligned already.====='; ""@justadudewhohacksThanks for your response.It's my fault; now the matching accuracy has been risen to be 95% and more.Thanks again.Sui=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,type replacer,Replace data Shape/type,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/205,Memory leaks with detectAllFaces,6,closed,2019-01-30T11:04:10Z,2019-09-05T00:12:41Z,"Hi;I have this coffeescript code snippet :```# Analyse the new frame    analyseFrame: (next = _.noop) ->            @testCount = 200 unless (typeof(@testCount) is 'number')            # Skip if not capturing        return unless Service.isCapturing            # get frame        _frame = Service.videoCapture.getFrame()            # get frame date        @currentFrameTime = Date.now()            # clear old faces in history        @refreshFaceHistory(@currentFrameTime)            #convert frame to a tensor        try            _data = new Uint8Array(_frame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)            _tensorFrame = tfjs.tensor3d(_data; [_frame.rows; _frame.cols; 3])        catch _err            @log.error ""Error instantiating tensor !!!""            @log.error _err.message        faceapi.detectAllFaces(_tensorFrame; @faceDetectionOptions).then (_detectedFaces) =>            @log.debug _detectedFaces                    # fill face history with detceted faces            _detectedFaces = @fillFacesHistory(_detectedFaces)                    # draw boxes on image            Service.videoCapture.drawFaceBoxes(_frame; _detectedFaces)                    # Get partial time            Service.frameDuration = Date.now() - @currentFrameTime                    # write latency on image            Service.videoCapture.writeLatency(_frame; Service.frameDuration)                    # show image            Service.faceRecoUtils.showImage(_frame)                    # Call next            _delayNextFrame = Math.max(0; 1000/@options.fps - Service.frameDuration)                    # DEBUG MEMORY DUMP every 30 sec              if @testCount > (5*30)                        @testCount = 0                  _dumpPath = path.resolve(__dirname + ""./../memoryDump/"" + @currentFrameTime + '.heapsnapshot')                heapdump.writeSnapshot _dumpPath                               setTimeout =>                # console.log ""Next frame : #{_delayNextFrame}ms - TOTAL : #{_frameDuration}ms""                @analyseFrame()            ; (_delayNextFrame)                        @testCount++```I feel like detectAllFaces is causing memory leaks. In deed; the corresponding nodejs process gains approximately 0.5 to 1% memory usage for each processed frame ! So my RAM get totally filled really quickly. I tried to get memory dump using heapdump but I don't see anything anormal (all my dumps have same size on Google Chrome inspector). So this could means that memory leaks come from the C++ size of face-api.js; right ?At first; I thought the leak could comes from combination of loops and promises or from Service.videoCapture. But I get similar results with the below code. The only difference being that RAM is filled slower. Which seems normal since the image used is smaller than my webcam's frames.```# Analyse the new frame    analyseFrame: (next = _.noop) ->        # Skip if not capturing        return unless Service.isCapturing        _currentFunctionCallTime = Date.now()        # get frame        _frame = Service.videoCapture.getFrame()        # if analyzer is ready; we can take a frame from webcam and process it        if @analyzerIsReady            @analyzerIsReady = false            # get frame date            @currentFrameTime =_currentFunctionCallTime            # get frame            _analyzedFrame = _frame            # clear old faces in history            @refreshFaceHistory(@currentFrameTime)            @log.debug ""faceHistory.length = ""; Service.faceHistory.length            _analyzedFrame = cv.imread('/home/loophole/eventbots-packages/eb/tiki-service-facedetection/models/bbt3.jpg')            #convert frame to a tensor            try                _data = new Uint8Array(_analyzedFrame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)                _tensorFrame = tfjs.tensor3d(_data; [_analyzedFrame.rows; _analyzedFrame.cols; 3])            catch _err                @log.error ""Error instantiating tensor !!!""                @log.error _err.message            faceapi.detectAllFaces(_tensorFrame; @faceDetectionOptions).then (_detectedFaces) =>                # fill face history with detceted faces                _detectedFaces = @fillFacesHistory(_detectedFaces)                # Get partial time                Service.frameDuration = Date.now() - @currentFrameTime                # Call next                _delayNextFrame = Math.max(0; 1000/@options.fps - Service.frameDuration)                setTimeout =>                    @analyzerIsReady = true                ; (_delayNextFrame)        _callDuration = Date.now() - _currentFunctionCallTime        _delayNextCall = Math.max(0; 1000/@options.callPerSec - _callDuration)        setTimeout =>                @analyseFrame()            ; (_delayNextCall)```","[""Hey; there is no C++ side of face-api.js. If you are really experiencing memory leaks; it either comes from tensorflow or opencv4nodejs; which I find very unlikely as well since I didn't experience any kind of leaks in long running applications using opencv4nodejs + face-api.js with similar code.One thing that bothers me though; I don't see you disposing the tensor `_tensorFrame`; which you create for every frame. If you do not dispose it after your done with it; e.g. `_tensorFrame.dispose()`; then your application will indeed leak memory. You can also check the tracked memory of tfjs using `tf.memory()`.=====""; ""I wasn't aware I had to dispose the tensor myself... I thought Node JS garbage collector would do it when taking care of _tensorFrame. Guess I was a little bit too demanding haha !Then again; thank you for your help :+1: =====""; 'There is still something that bothers me though... How come I don\'t have any memory leaks with this code ?```# Analyse the new frame    analyseFrame: (next = _.noop) ->        # Skip if not capturing        return unless Service.isCapturing        # get frame        _frame = Service.videoCapture.getFrame()        # get frame date        @currentFrameTime = Date.now()        # clear old faces in history        @refreshFaceHistory(@currentFrameTime)        @log.debug ""faceHistory.length = ""; Service.faceHistory.length        #convert frame to a tensor        try            _data = new Uint8Array(_frame.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)            _tensorFrame = tfjs.tensor3d(_data; [_frame.rows; _frame.cols; 3])        catch _err            @log.error ""Error instantiating tensor !!!""            @log.error _err.message        # Get partial time        Service.frameDuration = Date.now() - @currentFrameTime        # Call next        _delayNextFrame = Math.max(0; 1000/@options.fps - Service.frameDuration)        setTimeout =>            # console.log ""Next frame : #{_delayNextFrame}ms - TOTAL : #{_frameDuration}ms""            @analyseFrame()        ; (_delayNextFrame)```Is that because a copy of _tensorFrame is sent to detectAllFaces when passed as parameter (in my first snippet) ? So _tensorFrame in analyzeFrame is just replaced at each frame ?====='; 'If you do not dispose the tensors you create; you will always leak memory. Try tracking `tf.memory()`; if the number of allocated tensors is growing with each frame;  you are leaking memory.====='; 'Hi!I having similar issue with this code in a loop:```\t\timg = await commons.canvas.loadImage(data);\t\tdetections = await faceapi.detectAllFaces(img; faceDetectionOptions).withFaceLandmarks().withFaceDescriptors();```In few seconds I fill all my RAM; how can I dispose something to avoid memory leak?Thanks!====='; 'Solved!; only happend when run in debug mode; maybe never dispose because of that.Thanks!=====']",Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/203,Webcam feed on NodeJs environment,6,closed,2019-01-24T14:18:02Z,2020-01-24T23:53:28Z,Hi;I would like to perform live face detection in a nodejs environment but I don't quiet see how to do that.Initially; I was capturing frames from my webcam using opencv4nodejs:```cap = new cv.VideoCapture(videoDeviceId);(...)_frame = cap.read();```But then I would have to find a way to convert _frame to a Canvas; right ? At least if I'm using a monkey path like this:```const { Canvas; Image; ImageData } = canvasfaceapi.env.monkeyPatch({ Canvas; Image; ImageData })```Do I have other options ? Would it be possible to use a monkey patch with some opencv4nodejs' elements ?I saw this line in README.md:```Alternatively you can simply construct your own tensors from image data and pass tensors as inputs to the API.```Okay; but how can I achieve that ? If I look at detectAllFaces' signature; I can see the following:```detectAllFaces(input: TNetInput; options?: FaceDetectionOptions): DetectAllFacesTask```And if I look further; I can see that TNetInput can be a TNetInputArg; which can be a TResolvedNetInput; which can be a tf.Tensor3D or tf.Tensor4D.So I would have to retrieve data from _frame and use them to instantiate a tf.Tensor3D ?Thanks !,"['Okay; so I managed to get a small working snippet:```this.currentFrameTime = Date.now();// get frame using opencv4nodejs_frame = Service.videoCapture.getFrame();try {    // create tensor  _tensorFrame = tfjs.tensor3d(_frame.getDataAsArray());} catch (_error) {  _err = _error;  this.log.error(""Error instantiating tensor !!!"");  this.log.error(_err.message);}faceapi.detectAllFaces(_tensorFrame; this.faceDetectionOptions).then((function(_this) {  return function(_detectedFaces) {    var _delayNextFrame; _paintedCurrentFrame;    // draw face boxes on frame    _paintedCurrentFrame = Service.videoCapture.drawFaceBoxes(_frame; _detectedFaces);    // compute latency    Service.frameDuration = Date.now() - _this.currentFrameTime;    // display image    Service.faceRecoUtils.showImage(_paintedCurrentFrame);   // compute delay needed to have a constant fps    _delayNextFrame = Math.max(0; 1000 / _this.options.fps - Service.frameDuration);    return _this.log.debug(""Latency: ""; Service.frameDuration);  };})(this));// try to have a constant fpssetTimeout((function(_this) {  return function() {    return _this.analyseFrame();  };})(this); _delayNextFrame);```So my problem is; this is really really slow ! Like way slower that your web-example (from 1000ms to 3500ms for one frame). In issue #44; you said that ""If the code is running on the cpu in node; it would probably be very slow."". As a matter of fact; I am running code on cpu. I tried to run it on GPU but I still couldn\'t make it (issue with cuDNN; cuda and tensorflow versions). Anyway; that doesn\'t change anything because the production computers won\'t have GPUs.Could you explain the speed gap between node and browser ? Because if I understand correctly; you are running your code on CPU in browser examples; right ? Second question: face-api.js isn\'t designed to run on NodeJS and I better stick to face-recognition.js; right ?====='; '> In issue #44; you said that ""If the code is running on the cpu in node; it would probably be very slow."". As a matter of fact; I am running code on cpu. I tried to run it on GPU but I still couldn\'t make it (issue with cuDNN; cuda and tensorflow versions). Anyway; that doesn\'t change anything because the production computers won\'t have GPUs.Sorry for the confusion here. You can run the tfjs code on the cpu in node without using the native tensorflow backend; which is very slow. If you are installing tfjs-node (cpu or gpu) you will notice a pretty decent performance boost.> Could you explain the speed gap between node and browser? Because if I understand correctly; you are running your code on CPU in browser examples; right ?It depends. The browser examples should optimally run on the webgl backend and not on the cpu. If you have a mediocre to decent gpu running the code in the browser on the webgl backend will probably be faster than running on the tfjs-node cpu backend (native tensorflow backend).1000ms for faceDetection in the browser is pretty slow; probably the SSD Mobilenet face detector is too heavy for your machine or the webgl backend is not registered. For example ssd mobilenetv1 runs on roughly 40 fps on my gpu. You could try using the TinyFaceDetector instead; which I would recommend anyways for realtime apps; if you are tracking your face via webcam for example._frame.getDataAsArray() is probably also a bottleneck; depending on the size of your image. To convert a cv.Mat to tensor try the following; using mat.getData() instead:``` javascript// assuming img is a BGR Matconst data = new Uint8Array(img.cvtColor(cv.COLOR_BGR2RGB).getData().buffer)const imgTensor = faceapi.tf.tensor3d(data; [img.rows; img.cols; 3])```====='; ""Sorry; I think I wasn't clear: I got 1000ms in nodejs; not in brower.But anyway _frame.getDataAsArray() was the real problem here. Your code is way more efficient.With TinyFaceDetector; the detection takes from 80 to 200 ms. My tensor creation was taking from 1100 to 1500ms  while your's only takes from 10 to 30 ms !! Thanks a lot for your help.=====""; ""In case someone comes along after me looking for **exactly** the code needed.As a repo: https://github.com/mamacker/faceme```import * as tfjs from '@tensorflow/tfjs-node';import * as faceapi from 'face-api.js';import * as fs from 'fs';import * as cv from 'opencv';const minConfidence = 0.5;/*const faceDetectionNet = faceapi.nets.ssdMobilenetv1;const faceDetectionOptions = new faceapi.SsdMobilenetv1Options({minConfidence});*/const faceDetectionNet = faceapi.nets.tinyFaceDetector;const faceDetectionOptions = new faceapi.TinyFaceDetectorOptions({minConfidence});let globalFrame = null;let lastFrame = null;function processFrame() {  if (globalFrame == null) return;  if (lastFrame == globalFrame) return;  lastFrame = globalFrame;  let tFrame = faceapi.tf.tensor3d(globalFrame; [480; 640; 3])  faceapi.detectAllFaces(tFrame; faceDetectionOptions).then((faces) => {    console.log(faces);  });}setInterval(processFrame; 100);async function run() {  await faceDetectionNet.loadFromDisk('./weights');  let video = '/dev/video0';  let cap = new cv.VideoCapture(video);  let capture = () => {    cap.read(function(err; frame) {      if (frame.width() > 0) {        let data = new Uint8Array(frame.getData().buffer);        globalFrame = data;      }      setTimeout(capture; 0);    });  }  capture();}run()```=====""; ""Thank you @mamacker for the code 🙇\u200d♂️ Thanks to you I was able to remove `canvas` from my project 🙇\u200d♂️Full code: https://github.com/whyboris/extract-faces-nodeI'm using [sharp](https://github.com/lovell/sharp) for image loading (as I need other image manipulations later)Relevant piece:```tsconst sharp = require('sharp');const imgBuffer: Buffer = await sharp('./some/img/path.jpg').toBuffer();const imgTensor = tf.node.decodeJpeg(imgBuffer);const detections = await faceapi.detectAllFaces(imgTensor);```=====""; ""> Thank you @mamacker for the code 🙇\u200d♂️ Thanks to you I was able to remove `canvas` from my project 🙇\u200d♂️> > Full code: https://github.com/whyboris/extract-faces-node> > I'm using [sharp](https://github.com/lovell/sharp) for image loading (as I need other image manipulations later)> > Relevant piece:> > ```ts> const sharp = require('sharp');> const imgBuffer: Buffer = await sharp('./some/img/path.jpg').toBuffer();> const imgTensor = tf.node.decodeJpeg(imgBuffer);> const detections = await faceapi.detectAllFaces(imgTensor);> ```This the correct mode; using decodeJpeg and most faster=====""]",Slow Execution,Poor Performance,API Misuse,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.1.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/193,Speed Improvement,6,closed,2019-01-14T14:06:59Z,2019-01-18T05:39:06Z,"This is not an issue but rather question for suggestion.Currently; i'm using on nodejs with:    ""face-api.js"": ""^0.17.1"";and not @tensorflow/tfjs-node because it seems to be broken right now.My Goal is to detect all the faces on images. For this; i'm using SSNMobileNetv1Model. It gives great result. But i takes around 10-12 sec to complete the task. Is there any way i can improve it either by using multithreading or any other meathod?","['You should definitely be using @tensorflow/tfjs-node to speed things up.====='; 'I did as per your suggestion and this is what i got.Without tfjs-node:11479.518ms with tkjs-node: 11707.518msIt doesn\'t look like tfjs-node is help alot. Is it because of my hardware limitation?using: i5-7th gen; graphic cad=intel.This is complete output:``` nodejs Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm install> canvas@2.3.1 install /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/canvas> node-pre-gyp install --fallback-to-buildnode-pre-gyp WARN Using needle for node-pre-gyp https download[canvas] Success: ""/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/canvas/build/Release/canvas.node"" is installedvia remote> nodemon@1.18.9 postinstall /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/nodemon> node bin/postinstall || exit 0npm notice created a lockfile as package-lock.json. You should commit this file.npm WARN face_detection_using_faceapi.js@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""linux"";""arch"":""x64""})added 297 packages from 195 contributors and audited 2381 packages in 130.679sfound 0 vulnerabilities Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm run dev> face_detection_using_faceapi.js@1.0.0 dev /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI> nodemon --exec ts-node --require dotenv/config   ./src/index.ts[nodemon] 1.18.9[nodemon] to restart at any time; enter `rs`[nodemon] watching: *.*[nodemon] starting `ts-node --require dotenv/config ./src/index.ts`starting app...Using SSNMobileModel============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================canvas loaded...Detecting facesdefault: 11479.975msTotal number of faces: 17saving file...1.jpeg /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/out[nodemon] clean exit - waiting for changes before restart^[[A^C Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm install --save @tensorflow/tfjs-node> @tensorflow/tfjs-node@0.2.3 install /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/@tensorflow/tfjs-node> node scripts/install.js* Downloading libtensorflow[==============================] 6318482/bps 100% 0.0s* Building TensorFlow Node.js bindings> protobufjs@6.8.8 postinstall /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/protobufjs> node scripts/postinstallnpm WARN @tensorflow/tfjs-converter@0.7.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-data@0.1.7 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN @tensorflow/tfjs-layers@0.9.2 requires a peer of @tensorflow/tfjs-core@0.14.5 but none is installed. You must install peer dependencies yourself.npm WARN face_detection_using_faceapi.js@1.0.0 No repository field.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"";""arch"":""any""} (current: {""os"":""linux"";""arch"":""x64""})+ @tensorflow/tfjs-node@0.2.3added 30 packages from 13 contributors and audited 2454 packages in 51.439sfound 0 vulnerabilities Faiz \ue0b0 localhost \ue0b0 ~ \ue0b0 Organization \ue0b1 Repos \ue0b1 Image_Analysis \ue0b1 using_FaceAPI \ue0b0 🔥 \ue0b0 npm run dev> face_detection_using_faceapi.js@1.0.0 dev /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI> nodemon --exec ts-node --require dotenv/config   ./src/index.ts[nodemon] 1.18.9[nodemon] to restart at any time; enter `rs`[nodemon] watching: *.*[nodemon] starting `ts-node --require dotenv/config ./src/index.ts`starting app...Using SSNMobileModel============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require(\'@tensorflow/tfjs-node\'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================canvas loaded...Detecting facesdefault: 11707.518msTotal number of faces: 17saving file...1.jpeg /home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/out[nodemon] clean exit - waiting for changes before restart```====='; ""Hmm this indicates; that you are not using the node backend in the second run:```============================Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.============================```Not sure why that is if tfjs-node installed correctly. Probably better to ask at tfjs what went wrong here.=====""; ""What do you by I'm not using node backend in the 2nd run?On Tue 15 Jan; 2019; 2:01 PM Vincent Mühler <notifications@github.com wrote:> Hmm this indicates; that you are not using the node backend in the second> run:>> ============================>> Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically; install our node backend; which binds to TensorFlow C++; by running npm i @tensorflow/tfjs-node; or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.>> ============================>>> Not sure why that is if tfjs-node installed correctly. Probably better to> ask at tfjs-node what went wrong here.>> —> You are receiving this because you authored the thread.> Reply to this email directly; view it on GitHub> <https://github.com/justadudewhohacks/face-api.js/issues/193#issuecomment-454307196>;> or mute the thread> <https://github.com/notifications/unsubscribe-auth/AHyDee5RcE1ZcPykov4CX6U9HmI1NR-4ks5vDZHagaJpZM4Z-UN2>> .>=====""; ""Seems you are installing tfjs-node; but the backend doesn't get registered properly in your application for some reason.=====""; 'It was mistake from my side. I forgot to load the required module. It working perfectly fine now.=====']",Slow Execution,Poor Performance,Import Error,,,add import,Fix import confusion in program,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.1.1,A.6
https://github.com/justadudewhohacks/face-api.js/issues/189,SsdMobilenetv1 - load model before inference,3,closed,2019-01-10T12:26:31Z,2021-07-28T07:49:16Z,This is how i'm loading the modules.``` typescript        await this.loadTinyModel();        console.log('model loaded');        let canvas = await this.loadCanvas(filePath);        let tt = await faceapi.detectAllFaces(canvas);    private async loadTinyModel() {        await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname; 'models'));    }    private async loadCanvas(filePath: string) {        return canvas.loadImage(filePath);    }```But getting errors:``` node(node:27555) UnhandledPromiseRejectionWarning: Error: SsdMobilenetv1 - load model before inference    at SsdMobilenetv1.forwardInput (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:26:13)    at SsdMobilenetv1.<anonymous> (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:60:14)    at step (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:133:27)    at Object.next (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:114:57)    at fulfilled (/home/Faiz/Organization/Repos/Image_Analysis/using_FaceAPI/node_modules/tslib/tslib.js:104:62)    at <anonymous>(node:27555) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:27555) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```,"[""You are calling detectAllFaces; which assumes (by default) that the ssdMobilenetv1 model is loaded:`await faceapi.nets.ssdMobilenetv1.loadFromDisk(path.join(__dirname; 'models'))`.=====""; 'So `SsdMobileNetv1` always should be loaded?Could I use `tinyFaceDetector` only?UPDATE: sorry; found an answer https://github.com/justadudewhohacks/face-api.js#detecting-faces :)====='; '> So `SsdMobileNetv1` always should be loaded?> Could I use `tinyFaceDetector` only?> > UPDATE: sorry; found an answer https://github.com/justadudewhohacks/face-api.js#detecting-faces :)what you add in youe old code?i have same problem brother.can you tell mi how you solve it.=====']",Data & Model Error,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.5] Training Argument Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/163,Huge waiting time,2,closed,2018-12-09T17:35:52Z,2019-01-17T09:17:42Z,It just take 8 seconds to recognise the reference image with Tiny Face Detector... any ideas why it takes THAT long? :/,"['Because your computer sucks. ====='; ""Can you be a bit more precise about what takes so long? The tiny face detector doesn't do any face recognition; it simply detects face bounding boxes.=====""]",Slow Execution,Poor Performance,API Misuse,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.1.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/116,Reference Error on TinyFaceDetectorOptions (and other options),2,closed,2018-10-27T20:30:53Z,2018-10-28T13:53:52Z,"I'm attempting to implement your new changes; but when I try to use: `const img1Description = await faceapi.detectSingleFace(""s1""; new TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor()`I get the following error:`ReferenceError: TinyFaceDetectorOptions is not defined`I've confirmed I've got the latest JavaScript files and have tried it across all browsers with the same result. I also get the same error for MtcnnOptions and SsdMobilenetv1Options. When I don't provide the options; the call is successful. What am I missing?",['I think it should be clear; that everything is accessible via faceapi and not on the global scope:`new faceapi.TinyFaceDetectorOptions()`.Sorry if the readme was confusing; I will adjust the snippets.====='; 'I figured it was something simple. That should have occurred to me. Thanks!====='],Reference Error,Crash,API Misuse,,,syntax modifier,syntax modifier,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/110,Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array,2,closed,2018-10-25T15:20:05Z,2018-10-28T11:01:21Z,"Hi Vincent;You've done a good job with your new update.I have an array like this:```js[  0: {     className: ""unique-id-1""    descriptors: [      0: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ];      1: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ];    ]  };  1: {     className: ""unique-id-2""    descriptors: [      0: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ];      1: [        0: ""-0.092008784413338""        1: ""-0.092008784413338""        2: ""-0.092008784413338""        3: ""-0.092008784413338""        .        .        128: ""-0.092008784413338""      ];    ]  };]```I will do facial recognition using these values. That's why I applied this method:```jsconst labeledDescriptors = [  new faceapi.LabeledFaceDescriptors(    vm.trainDescriptorsByClass[0].className;    vm.trainDescriptorsByClass[0].descriptors  );  new faceapi.LabeledFaceDescriptors(    vm.trainDescriptorsByClass[1].className;    vm.trainDescriptorsByClass[1].descriptors  )];const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors);const fullFaceDescriptions = await faceapi    .detectAllFaces(vm.yuz_degiskenleri.videoEl; vm.yuz_degiskenleri.options)    .withFaceLandmarks()    .withFaceDescriptors();fullFaceDescriptions.forEach(({ detection; landmarks; descriptor }; index) => {    console.log({ detection; landmarks; descriptor }; index);    const bestMatch = faceMatcher.findBestMatch(descriptor);    console.log(bestMatch);    /*vm.yuz_degiskenleri.en_iyi_eslesen_yuz = getBestMatch(vm.trainDescriptorsB    console.log(vm.yuz_degiskenleri.en_iyi_eslesen_yuz.className);*/});```I get the following error:```Uncaught (in promise) Error: LabeledFaceDescriptors - constructor expected descriptors to be an array of Float32Array    at new t (face-api.min.js:1)    at Vue.yuz_tanima_calistir (9nPmZuxCNCQxMzQkTWsxNlRYa3pKREU9JDE4OTk5OTkwMDAwMDI1Y97wNB:1)    at setTimeout (9nPmZuxCNCQxMzQkTWsxNlRYa3pKREU9JDE4OTk5OTkwMDAwMDI1Y97wNB:1)```Where am I doing wrong?",['Hi;`vm.trainDescriptorsByClass[0].descriptors.map(desc => new Float32Array(desc))` should make it? Actually a plain array of numbers should also be ok; but as it is implemented right now the FaceMatcher expects descriptors to be Float32Arrays. ====='; 'It worked! Thanks Vincent...====='],Data & Model Error,Crash,API Misuse,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/100,Distance often quite low; regardless of faces - How to improve accuracy?,5,closed,2018-10-05T01:44:10Z,2018-10-05T18:23:04Z,I've been doing some tests today and am using the following images:![image](https://user-images.githubusercontent.com/16200100/46511671-1f4bee00-c80d-11e8-9a77-f9d455b955a8.png)The bottom 4 images are all the same person and the top 4 are different people. Comparing the bottom; third from the left with the first top image (the female) gets  0.4439...All of them are below 0.6. Many are 0.41 to 0.50.Do you have any suggestions on how to improve the accuracy?,"['Additionally; despite the bottom 4 being the same person; there is a stronger match between them with the images of other people in many cases.====='; 'I get a distance of `0.8425371094302093` compraing the top left image with the bottom third.Can you show me what you are doing (code snippet). Are you performing face detection or pass the whole image into the recognition net?Are you using an Intel GPU?====='; 'Thanks for the prompt reply!It doesn\'t seem to matter what machine I test on... iPhone 6 in Safari reports almost the same (like .01 difference here and there) as Chrome on my Windows 10 machine with an Nvidia card and Firefox on my Pixel 2.I\'m passing the whole photo in. Here\'s the snippet:`$(document).on(""click""; "".btn""; function () {                var _ = $(this);                Compare(_.data(""img1""); _.data(""img2""));            });`        `async function Compare(img1; img2) {            await faceapi.loadFaceRecognitionModel(""/Content/FaceAPIModels"");            const descriptor1 = await faceapi.computeFaceDescriptor(img1);            const descriptor2 = await faceapi.computeFaceDescriptor(img2);            const distance = faceapi.euclideanDistance(descriptor1; descriptor2);            alert(distance);        }`And here\'s the button tag for the image comparison: `<input type=""button"" value=""Compare S1 and O3"" class=""greenbutton btn"" data-img1=""s1"" data-img2=""o3"" />`If you have suggestions or a different code sample I should try; that would be fantastic.====='; 'You have to perform face detection first; dont pass the entire image to computeFaceDescriptor; that wont give you a correct face vector. Therefore you can use faceapi.allFaces as in the face recognition examples and as stated in the readme and introduction article.====='; ""Thanks... that makes sense. Got it working now. It's super quick; once the models/etc are loaded!=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,type replacer,Replace data Shape/type,web application,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",D,A.4
https://github.com/justadudewhohacks/face-api.js/issues/67,Uncaught (in promise) TypeError: faceapi.allFacesMtcnn(...).map is not a function,9,closed,2018-08-10T06:01:46Z,2019-07-31T07:02:17Z,"I'm using VueJS. How can I fix these errors?**Code** (VueJS)```onPlay: async function(videoEl) {                if(videoEl.paused || videoEl.ended || !modelLoaded)                    return false;                let minFaceSize = 200;                const { width; height } = faceapi.getMediaDimensions(videoEl);                const canvas = document.getElementById('inputVideo');                canvas.width = width;                canvas.height = height;                const mtcnnParams = {                    minFaceSize                };                const fullFaceDescriptions = (faceapi.allFacesMtcnn(videoEl; mtcnnParams)).map(fd => fd.forSize(width; height));                fullFaceDescriptions.forEach(({ detection; landmarks; descriptor }) => {                    faceapi.drawDetection('overlay'; [detection]; { withScore: false });                    faceapi.drawLandmarks('overlay'; landmarks.forSize(width; height); { lineWidth: 4; color: 'red' });                    const bestMatch = getBestMatch(trainDescriptorsByClass; descriptor);                    const text = `${bestMatch.distance < maxDistance ? bestMatch.className : 'unkown'} (${bestMatch.distance})`;                    const { x; y; height: boxHeight } = detection.getBox();                    faceapi.drawText(                    canvas.getContext('2d');                    x;                    y + boxHeight;                    text;                    Object.assign(faceapi.getDefaultDrawOptions(); { color: 'red'; fontSize: 16 })                    )                });                setTimeout(() => onPlay(videoEl); 150);            };            run: async function() {                await faceapi.loadMtcnnModel(""/img/face-api/weights"");                await faceapi.loadFaceRecognitionModel(""/img/face-api/weights/"");                trainDescriptorsByClass = initTrainDescriptorsByClass(faceapi.recognitionNet);                modelLoaded = true;                const videoEl = document.getElementById('inputVideo');                navigator.getUserMedia(                    { video: {} };                    stream => videoEl.srcObject = stream;                    err => console.error(err)                );                this.onPlay(videoEl);            }```**Code** (HTML)```<div style=""position: relative"" class=""margin"">        <video ref=""inputVideo"" onload=""onPlay(this)"" id=""inputVideo"" autoplay=""true"" muted></video>        <canvas id=""overlay"" /></div>```","['Hi;`faceapi.allFacesMtcnn` returns a Promise. You forgot the await keyword: ` (await faceapi.allFacesMtcnn(videoEl; mtcnnParams)).map(...)`.====='; '@justadudewhohacks thanks; it\'s working. But there is another problem.Uncaught (in promise)> Event\xa0{isTrusted: true; type: ""error""; target: null; currentTarget: null; eventPhase: 0;\xa0…}I guess; I forgot one more place.![photofacefun_com_1533893018](https://user-images.githubusercontent.com/18267195/43950167-4232b21e-9c98-11e8-8379-e99d38fc2619.jpg)====='; 'This could be caused by anything. Could you show me the line; where this error is thrown.====='; ""I solved this. The path to the Images folder isn't correct.Last question; How can i train recognition? I took 10 images (from web camera). But; distance value changes between 0.6 - 0.7.=====""; 'You could use multiple images to compute reference descriptors and then compute the average of the euclidean distances between all reference descriptors and a input descriptor.Usually; you should already get quite good results using a single image only. Can you share the image/s you are using as reference and the images of the faces; you are trying to recognize.====='; 'Ok. Thanks for help.====='; 'script2.js:17 Uncaught (in promise) TypeError: fd.forSize is not a function    at script2.js:17    at Array.map (<anonymous>)    at onPlay (script2.js:17)====='; ' <video onplay=""onPlay(this)"" id=""inputVideo"" autoplay muted></video>    <!-- <video  id=""inputVideo"" autoplay muted></video>  -->    <canvas id=""overlay"" />this is html code====='; 'and the script is$(document).ready(function() {    run()  })       async function onPlay(videoEl) {                      if(videoEl.paused || videoEl.ended || !modelLoaded)                          return false;                      let minFaceSize = 200;                      const { width; height } = faceapi.getMediaDimensions(videoEl);                      const canvas = document.getElementById(\'inputVideo\');                      canvas.width = width;                      canvas.height = height;                      const mtcnnParams = {                          minFaceSize                      };                      const fullFaceDescriptions =  (await faceapi.allFacesMtcnn(videoEl; mtcnnParams)).map(fd => fd.forSize(width; height));                    fullFaceDescriptions.forEach(({ detection; landmarks; descriptor }) => {                        faceapi.drawDetection(\'overlay\'; [detection]; { withScore: false });                        faceapi.drawLandmarks(\'overlay\'; landmarks.forSize(width; height); { lineWidth: 4; color: \'red\' });                        // const bestMatch = getBestMatch(trainDescriptorsByClass; descriptor);                        // const text = `${bestMatch.distance < maxDistance ? bestMatch.className : \'unkown\'} (${bestMatch.distance})`;                        // const { x; y; height: boxHeight } = detection.getBox();                        faceapi.drawText(                        canvas.getContext(\'2d\');                        x;                        y + boxHeight;                        text;                        Object.assign(faceapi.getDefaultDrawOptions(); { color: \'red\'; fontSize: 16 })                        )                    });                    setTimeout(() => onPlay(videoEl); 150);                };                    async function run() {                                          await faceapi.loadMtcnnModel(""/models"");                                          await faceapi.loadFaceRecognitionModel(""/models"");                                        //   trainDescriptorsByClass = initTrainDescriptorsByClass(faceapi.recognitionNet);                                          modelLoaded = true;                                          const videoEl = document.getElementById(\'inputVideo\');                                          navigator.getUserMedia(                                              { video: {} };                                              stream => videoEl.srcObject = stream;                                              err => console.error(err)                                          );                                          this.onPlay(videoEl);                                      }=====']",Reference Error,Crash,API Misuse,,,syntax modifier,syntax modifier,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/65,Using landmark model and don't have the expected behavior,1,closed,2018-08-07T16:38:40Z,2018-08-07T17:13:41Z,![image](https://user-images.githubusercontent.com/16566338/43789700-f7142d74-9a46-11e8-836e-0d0f9eacca7b.png)What I'm doing wrong? I just change the example to landmark model and it´s apparently not working correctly. I'm running it on Windows/Firefox and Windows/Chrome.,['You have to feed the landmark model an image of a face; e.g. just the extracted image from the bounding box; not the entire image.====='],Incorrect Functionality,Incorrect Functionality,API Misuse,,,add input preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",D,A.3
https://github.com/justadudewhohacks/face-api.js/issues/64,Compute and compare the descriptors of two face images,6,closed,2018-08-03T10:16:42Z,2021-07-02T14:43:11Z,Hi thank you a lot for your efforts; but i need some helps in the face recognition : my goal is to compare two face  images for that i compute the descriptor o each image and the euclidean Distance the same thing in the examples when i enter the same person and the same image it gives me 'match' but when  i enter for example different photographs of the same person the result is 'no match'  i really need to compare different photographs of the same person,"['Can you append some screenshots for your example; that is not working? Also some code snippets would be helpful. Are you performing face detection and alignment?====='; 'yes of course !first i detect the faces in the images than i compute their descriptors . this an example :![capture1](https://user-images.githubusercontent.com/38425517/43686560-e7c25fae-98bf-11e8-93c3-ef4e63bb3bfd.PNG) and for the code i use this : ``<script>     const mtcnnParams = { maxNumScales: 10;scaleFactor: 0.709;minFaceSize: 20;scoreThresholds: [0.6; 0.7; 0.7];}         const results = await faceapi.mtcnn(inputImgEl; mtcnnParams)       console.log(results)       if(results==\'\' ){ console.log(""no face detected try again !!""); return true;}       if (results) {          results.forEach(({ faceDetection;faceLandmarks }) => {          faceapi.drawDetection(\'overlay\'; faceDetection.forSize(width; height))          faceapi.drawLandmarks(\'overlay\'; faceLandmarks.forSize(width; height); { lineWidth: 4; color: \'red\' })        })          const descriptor1 = await faceapi.computeFaceDescriptor(inputImgEl)          console.log(descriptor1)          const descriptor2 = await faceapi.computeFaceDescriptor(inputImgEl1)          console.log(descriptor2)          const distance = faceapi.euclideanDistance(descriptor1; descriptor2)          console.log(distance)          alert(""test"")             if (distance < 0.62)              console.log(\'match\')           else              console.log(\'no match\')      } //fin de if(results)    //}        } //fin de run()           $(document).ready(function() {            run()    })</script>``====='; ""Hmm it says match; or am I missing something. Also are you computing the face bounding box for the second image as well? `descriptor1 = await faceapi.computeFaceDescriptor(inputImgEl)` looks like you are passing the entire image and not the extracted face image.You can also use `faceapi.allFaceMtcnn` to get the bounding boxes + descriptors; so you don't have to do everything on your own.=====""; ""Hi again ; you are right it's my fault you are right when i use `computeFaceDescriptor`  i pass the entire image that why it's give me error in matching !!!! thanks a lot :+1: =====""; 'I have two base64 image and both image has a single face i want to compare both faces. can any one help men====='; 'Hi;  I am new in tensorflow js. I am working detect and recognized images between realtime web cam and stored in db.Can any one Please suggest me from where i can start in react js.I intgreated this **@tensorflow-models/blazeface** modulei am able to detect the faces in real time web cam.i do not under stand how to convert image in tensor. now i am not able to understand what i do after that. I am very thankfull if any one help me.=====']",Incorrect Functionality,Incorrect Functionality,API Misuse,,,add input preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.3
https://github.com/justadudewhohacks/face-api.js/issues/39,run example failed on windows 10,3,closed,2018-07-03T09:38:51Z,2018-07-10T08:05:39Z,[npm-debug.log](https://github.com/justadudewhohacks/face-api.js/files/2158626/npm-debug.log)I cloned the repository; and run the example as the tutorial. but got error; I'm on Windows 10.,"[""The debug log doesn't help. Please tell me what you did and what kind of error was thrown.=====""; 'Below are all what I do and error message.git clone https://github.com/justadudewhohacks/face-api.js.gitcd face-api.jscd examplesnpm inpm startapp.post(\'/fetch_external_image\'; async (req; res) => {                                        ^SyntaxError: Unexpected token (    at Object.exports.runInThisContext (vm.js:53:16)    at Module._compile (module.js:513:28)    at Object.Module._extensions..js (module.js:550:10)    at Module.load (module.js:458:32)    at tryModuleLoad (module.js:417:12)    at Function.Module._load (module.js:409:3)    at Module.runMain (module.js:575:10)    at run (node.js:348:7)    at startup (node.js:140:9)    at node.js:463:3npm ERR! Windows_NT 10.0.17134npm ERR! argv ""C:\\\\Program Files\\\\nodejs\\\\node.exe"" ""C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js"" ""start""npm ERR! node v6.2.2npm ERR! npm  v3.9.5npm ERR! code ELIFECYCLEnpm ERR! @ start: `node server.js`npm ERR! Exit status 1npm ERR!npm ERR! Failed at the @ start script \'node server.js\'.npm ERR! Make sure you have the latest version of node.js and npm installed.npm ERR! If you do; this is most likely a problem with the  package;npm ERR! not with npm itself.npm ERR! Tell the author that this fails on your system:npm ERR!     node server.jsnpm ERR! You can get information on how to open an issue for this project with:npm ERR!     npm bugsnpm ERR! Or if that isn\'t available; you can get their info via:npm ERR!     npm owner lsnpm ERR! There is likely additional logging output above.npm ERR! Please include the following file with any support request:npm ERR!     D:\\Learnings\\face-api.js\\examples\\npm-debug.log====='; ""`node v6.2.2` does not support async await out of the box. You can either upgrade your node version; or rewrite that function using Promises (or alternatively comment it out; but then you won't be able to fetch images from an external url).=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change npm/node version,Changing version,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Syntax Error in Code""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.3,C.1
https://github.com/martinjm97/ENNUI/issues/109,ResNets fail on CIFAR,1,closed,2019-05-26T20:27:02Z,2019-05-26T21:52:20Z,Dimension errors.,['Fixed by adding an extra conv layer to get the number of filters standardized for the add.====='],Data & Model Error,Crash,Improper Model Attribute,,,modify model,modify model,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,C.2
https://github.com/thekevinscott/ml-classifier-ui/issues/6,Model Error,8,closed,2019-05-14T06:10:57Z,2019-05-31T16:25:24Z,start(); async function start() {const model = await tf.loadLayersModel('model.json');const example =  tf.browser.fromPixels(document.getElementById('test'));  // for exampleconst prediction = model.predict(example);    }errors.ts:48 Uncaught (in promise) Error: Error when checking : expected flatten_Flatten1_input to have 4 dimension(s); but got array with shape [640;960;3]    at new e (errors.ts:48)    at Dd (training.ts:311)    at e.predict (training.ts:1073)    at e.predict (models.ts:765)    at start ((index):31),"['It looks like you may need to resize your image. Can you provide a code sandbox link?====='; 'Yes ; I did follow your article for resizing the image ; here is my code thanks for helping out. [test.zip](https://github.com/thekevinscott/ml-classifier-ui/files/3193631/test.zip)====='; ""You are loading the wrong model. You're loading your own model and trying to use that for activations; you should be using mobilenet.Try replacing `loadTruncatedMobileNet` with the following:```async function loadTruncatedMobileNet() {    const pretrainedModelURL = 'https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_0.25_224/model.json';    const mobilenet = await tf.loadLayersModel(pretrainedModelURL);    const layer = mobilenet.getLayer('conv_pw_13_relu');    return tf.model({inputs: mobilenet.inputs; outputs: layer.output});  }  async function loadModel() {    return await tf.loadLayersModel('ml-classifier-Super_Galaxy-Super_Snow.json');  }  async function start() {      truncatedMobileNet = await loadTruncatedMobileNet();      model = await loadModel();...```You can call `model.summary()` to inspect the shape of your model. You'll see that it expects a different shape.=====""; 'The model was generated by your ml classifier====='; 'It still needs to process the images through mobilenet for the initial activation. The embeddings mobilenet produces are the things you want to pass to your model. ====='; 'OK ; many thanks!====='; ""Sure! Let me know if you run into any problems.As an FYI; you _can_ create a single model that combines the pretrained model with your own layers. I don't have any code showing how to do this; but high level you'd want to construct a sliced model from mobilenet; make sure the existing layers are frozen (aka; ensure they won't be trainable); and then concatenate your own unfrozen layers onto the model.I prefer the approach shown here; where the pretrained model exists separately from your model; because it gives more flexibility on the UX side to process data. For instance; you can activate the images through mobilenet when the user uploads them; instead of having to do it all in one big step.=====""; 'Yes understand; again big thanks for your help; I am just into tensor flow your work really helped me out; keep it up !=====']",Data & Model Error,Crash,API Misuse,,,change data preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.3
https://github.com/thekevinscott/ml-classifier-ui/issues/3,How to add a single image for prediction?,3,closed,2018-09-16T10:29:56Z,2019-05-17T17:19:14Z,I have downloaded the trained model . When i import it into my code i get few errorUncaught (in promise) Error: Based on the provided shape; [12544;100]; and dtype float32; the tensor should have 1254400 values but has 0Do you have any sample code for loading a pretrained model and run prediction just for 1 image,['Hi @technoartista; can you share the code or a repo with the bug?====='; 'Same problem here; if you can provide us a sample code of predicting image would be great! Thanks ====='; 'Check out https://thekevinscott.com/image-classification-with-javascript/ ====='],Data & Model Error,Crash,Unknown,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,E
https://github.com/BeTomorrow/ReImproveJS/issues/23,expected dense_Dense1_input to have shape [null;12] but got array with shape [1;14],1,closed,2020-04-08T13:03:33Z,2020-04-08T19:43:39Z,I am getting this error when running ReimproveJs```Uncaught (in promise) Error: Error when checking : expected dense_Dense1_input to have shape [null;12] but got array with shape [1;14].    at new ValueError (reimprove.js:1)    at checkInputData (reimprove.js:1)    at Model.predict (reimprove.js:1)    at Sequential.predict (reimprove.js:1)    at Model.predict (reimprove.js:1)    at reimprove.js:1    at Object.Tracking.tidy (reimprove.js:1)    at Agent.createTrainingDataFromMemento (reimprove.js:1)    at reimprove.js:1    at Array.map (<anonymous>)```This error is not happening directly but after 5 seconds of it working. its called by academy.stepMy part; only added inputs:```let inputs = [speed;turn;sensor1Dist;sensor2Dist;sensor3Dist];          // Need to give a number[] of your inputs for one teacher.        let result = await academy.step([               // Let the magic operate ...        {teacherName: teacher; agentsInput: inputs}        ]);```,['Defined wrong inputSize====='],Data & Model Error,Crash,API Misuse,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",A.2,A.3
https://github.com/BeTomorrow/ReImproveJS/issues/2,Example/Documentation update [TFJS expected axis NaN for dense1],4,closed,2018-06-10T06:47:49Z,2018-06-11T13:48:47Z,"It seems that the model must goes inside the Agent config:`model.compile({loss: 'categoricalCrossentropy'; optimizer: 'sgd'})const agentConfig = {    model: model;    memorySize: 5000;    batchSize: 128;    temporalWindow: temporalWindow};``await academy.step([        {teacherName: teacher; inputs: inputs}    ]);`The parameters in the interface AcademyStepInput are ""teacherName"" and ""agentsInput"".","['Indeed; thank you for noticing it ! Corrected.====='; ""The model should be like this:`const agentConfig = {\tmodel: model;\tagentConfig: {\t\tmemorySize: 1000;\t\tbatchSize: 10;\t\ttemporalWindow: temporalWindow\t}};`Also; the academy.step should be:`await academy.step([\t\t\t\t{ teacherName: teacher; agentsInput: inputs }\t\t\t])`I'm also debugging another issue with tensorflow.js but I haven't figured out why it's happen.Error: Input 0 is incompatible with layer dense_Dense1: expected axis NaN of input shape to have value=====""; 'Thank you once more; I forgot the second part of the correction...Can you provide an example to reproduce your bug ?====='; 'Please ignore the error; it was caused by another third part code overriding the Object.prototype.clone.At some point it was changing the type of the objects used by ts.js and failing in the assert.Thanks for this awesome framework 👍 =====']",Data & Model Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E1"",
    ""specific_type"": ""E1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.4""
  }
}
```",A.2,A.3
https://github.com/infinitered/nsfwjs/issues/525,Unexpected errors are thrown if image width and height is not specified,5,open,2021-06-26T15:43:38Z,2021-06-26T20:17:23Z,When you try to run `model.classify(image)`; where `image` is of type `HTMLImageElement`; if the dimensions (width and height) of the image are not specified; the function throws unexpected errors; one of which is as follows:```DOMException: Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.```This is resolved if you make a new `Image` object with a width and height that you specify (`200` in the following example); and you set `newImg.crossOrigin` to `anonymous`. A similar code snippet is also used in the chrome extension [NSFW Filter](https://github.com/nsfw-filter/nsfw-filter/); yet there is no mention of it on the README of this repo.https://github.com/nsfw-filter/nsfw-filter/blob/2abc417c4a840ed53a7b0cf060c3867d54265779/src/background/Queue/LoadingQueue.ts#L42-L50```ts// `image` is the original `HTMLImageElement` that is supposed to be classified// `newImage` is `image` but with width and height specified (to 200; 200 in this case)const newImg = new Image(200; 200);newImg.crossOrigin = 'anonymous';newImg.src = image.src;model.classify(newImg)```Without the above modification; the image classification is inconsistent; i.e; it fails sometimes and succeeds sometimes for the same exact image (I've tested this on Google Images). It might be because the images contain too many pixels; I was not able to figure out why the error occurred randomly; but adding the above snippet fixed it. There should probably be a mention of this on the README of the repo or a fix that resolves the issue entirely.,"[""Interesting.  A couple thoughts come to mind.The most likely issue is that there's a security issue where the `src` of the image is cross-origin.  By using the `Image` tag; you're effectively making a local copy and getting around `tainted canvases` issues.That doesn't explain how it sometimes works and sometimes doesn't though.   That sounds more like a race-condition.   So maybe you're calling `classify` before the image is fully loaded?  I'd have to see the code to be sure.=====""; '@thebongy could you please post the code snippet here?====='; ""I'll paste the relevant part of the code here.We basically had a content script in a chrome extension which did the following:```typescriptlet model: nsfwjs.NSFWJS;nsfwjs.load('<redacted private url>').then((loaded) => {  model = loaded;});window.onload = async () => {   const images = [...document.querySelectorAll('img')];    for (const image in images) {      try {        console.log(await filterImage(images[image]; 0.5));        await sleep(50);      } catch (err) {        console.log(err);        // Blank      }    }}export default async function filterImage(  image: HTMLImageElement; threshold: Number;): Promise<FilterResult> {  const width = image.clientWidth;  if (width < 50) {    return {      filter: false;      reason: 'The image is too small';    };  }  const newImg = new Image(200; 200);  newImg.crossOrigin = 'anonymous';  newImg.src = image.src;  const result = await scanImage(newImg);  const highest = result[0];  if (highest.className === 'Neutral' || highest.className === 'Drawing') {    return {      filter: false;      reason: 'The image is neutral.';    };  }  if (highest.probability < threshold) {    return {      filter: false;      reason: 'The image is below the given threshold.';    };  }  return {    filter: true;    reason: `The image has ${highest.className} content which is over the configured threshold.`;  };}async function scanImage(element: HTMLImageElement) {  while (!model) {    // eslint-disable-next-line no-await-in-loop    await sleep(1000);  }  return model.classify(element);}```What we observed was that the same code; when the lines```typescriptconst newImg = new Image(200; 200);  newImg.crossOrigin = 'anonymous';  newImg.src = image.src;```weren't present; and we just directly processed the input image; we would sometimes get the error posted originally on this issue (Tainted canvases may not be loaded.) . But constructing a new smaller intermediate image always made the library work.=====""; ""This seems to be a bug with TFJShttps://github.com/tensorflow/tfjs/issues/322It's also been identified here: https://stackoverflow.com/questions/57662313/requested-texture-size-0x0-is-invalid-error-when-i-am-loading-image-in-browseThis didn't used to be a bug.However; the error you're getting seems to actually be a crossDomain error.   Because you're loading an image from a different website.  So you'll need to make a local copy.You can test all 4 possible situations with this codesandbox I created here: * switch between `img1` and `img2` for local and remote. * switch line 14 and 15 for image resize on and offhttps://codesandbox.io/s/nsfwjscrossoriginnsizecheck-epu3w?file=/src/index.js=====""; ""I see; thank you for verifying!For now the  intermediate image solution works for us; we'll update on this thread if we find out anything new about the issue :)=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,setting cors,Modifying model file path/extension,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.3,C.1
https://github.com/infinitered/nsfwjs/issues/461,minimal demo not working properly; only detect first image correctly and after that showing same result for each image,3,open,2021-01-12T09:37:27Z,2021-01-13T20:39:38Z,"I am using the minimal demo. It is working properly if I refresh the web page after checking each image and showing the correct result for each image. But if I don't refresh the page and just check multiple images using image gallery then it evaluates only the first image correctly and after that shows the same result for each image.          `const img = new Image();           img.crossOrigin = ""anonymous"";                     img.src =imgpath;           var porn_rate = 0;           var sexy_rate = 0;           var drawing_rate = 0;                 nsfwjs.load().then((model) => {                model.classify(img).then((predictions) => {	              console.log(""Predictions""; predictions);	              predictions.forEach(function (arrayItem) {		      var x = arrayItem.className;		      var y = arrayItem.probability;		      		      if(x=='Porn'){		        y = y*100;		        porn_rate = y;		      }		      else if(x=='Sexy'){		        y = y*100;		        sexy_rate = y;	              }	              else if(x=='Drawing'){		        y = y*100;		        drawing_rate = y;	              }	          });	      	          console.log(""porn rate: ""+porn_rate);              console.log(""sexy rate: ""+sexy_rate);              console.log(""Drawing rate: ""+drawing_rate);      	  if(porn_rate>40 || sexy_rate>40 ){           	  alert(""It seems that you have uploaded a nude picture :-("");       	  }			});    });`I have checked all issues here but could not get any issue similar to mine. Thanks.","['Taking a look at your code; you might need to wait for the `image.onload` event to fire before accessing the image.How are you calling this more than once?  This code looks very ""one time"" to me.====='; 'https://user-images.githubusercontent.com/29511058/104450715-569bce80-55c2-11eb-9222-5baad7f5a72a.mp4@GantMan  Please have a look at this video. I hope it will help you to understand the issue. so there is no loop actually but when I select the image each time from the gallery (you saw in the video) it changes the image name. I have checked by using the console and the image name is changing each time I select a new image. The rest of the code is the same you saw above. Please guide me where I am wrong.```document.getElementById(\'setimagename\'+pointer).value  = idBRzero;var imgpath = \'<?=sUPLOAD?>\'+idBRzero;//console.log(imgpath);const img = new Image();img.crossOrigin = ""anonymous"";img.src =imgpath;......same as above code.......```====='; 'There\'s a small chance that the image is ""tearing;"" meaning that you\'re sending it to the model before it\'s finished loading.  Usually this will cause an issue with the first classification; though and even error; so that might not be the issue.  Can you load the image and await the onload event? my _guess_ is that your first image loads faster than the NSFWJS model; so it works the first time.  Once the model is loaded; each subsequent classification happens before the image is fully ready.```          const img= new Image();          img.crossOrigin = \'anonymous\';          img.src = imgpath;          img.onload = () => {            model.classify(img).then((predictions) => {              ...            }          }```=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/infinitered/nsfwjs/issues/282,Memory Leaking,2,closed,2020-03-29T10:33:29Z,2020-04-24T07:47:36Z,Hey; every image that I classify it; the more memory consumes it.Is there any way to reduce it like deleting cache or something? It's really weird.,"['You must mean on server?See this item:  https://github.com/infinitered/nsfwjs/issues/49====='; 'Yup! #49 is definitely the fix for this. I was having the same issue.Whatever object you send to "".classify()""; just run "".dispose()"" after.=====']",Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,A.4
https://github.com/infinitered/nsfwjs/issues/49,Memory issues,8,closed,2019-03-13T20:03:13Z,2020-04-24T18:49:53Z,"I'm noticing some serious memory issues when using NSFWJS with `tfjs-node`. The memory footprint increases over time. Here's some code to test and see what I mean:**./lib/nsfwjs-inputs.js**```jsconst tf = require('@tensorflow/tfjs-node')const fs = require('fs')const jpeg = require('jpeg-js')const fetch = require('node-fetch')const readImageFile = path => {	return new Promise((resolve; reject) => {		fs.readFile(path; (err; buf) => {			if (err) return reject(err)			const pixels = jpeg.decode(buf; true)			resolve(pixels)		})	})}const readImageUrl = async url => {	let res = await fetch(url)	let buf = await res.arrayBuffer()	const pixels = jpeg.decode(buf; true)	return pixels}const imageByteArray = (image; numChannels = 3) => {	const pixels = image.data	const numPixels = image.width * image.height	const values = new Int32Array(numPixels * numChannels)	for (let i = 0; i < numPixels; i++) {		for (let channel = 0; channel < numChannels; ++channel) {			values[i * numChannels + channel] = pixels[i * 4 + channel]		}	}	return values}const imageToInput = (image; numChannels = 3) => {	const values = imageByteArray(image; numChannels)	const outShape = [image.height; image.width; numChannels]	const input = tf.tensor3d(values; outShape; 'int32')	return input}const inputFromFile = async (path; numChannels = 3) => {	const image = await readImageFile(path)	const input = imageToInput(image; numChannels)	return input}const inputFromUrl = async (url; numChannels = 3) => {	const image = await readImageUrl(url)	const input = imageToInput(image; numChannels)	return input}module.exports = {	inputFromFile;	inputFromUrl;}```**./nsfwjs-resource-test.js**```jsprocess.title = 'NSFWJS Resource Test'const fs = require('fs')const nsfwjs = require('nsfwjs')const nsfwjsInputs = require('./lib/nsfwjs-inputs.js')const pathJoin = require('path').joinconst IMG_DIR = pathJoin(__dirname; 'test-imgs')try {	fs.accessSync(IMG_DIR)} catch (err) {	fs.mkdirSync(IMG_DIR)}async function main() {	let nsfw = await nsfwjs.load('file://./nsfwjs_model/')	let files = fs.readdirSync(IMG_DIR)	let outputFile = fs.createWriteStream('./analysis.txt')	let counter = 0	for (let file of files) {		const filepath = pathJoin(IMG_DIR; file)		let input = await nsfwjsInputs.inputFromFile(filepath)		let predictions = await nsfw.classify(input)		let predictionMap = predictions.reduce(			(map; prediction) => ({				...map;				[prediction.className]: prediction.probability;			});			{}		)		outputFile.write(`${file} = ${JSON.stringify(predictionMap)}\n`; 'utf8')		console.log(++counter)	}}main().catch(err => console.error(err) && process.exit(1))```You'll need to have images in a `./test-imgs` directory ready for this test. After only 100 images; I see the ""NSFWJS Resource Test"" process at 1 GB of memory. I'm noticing 5 GB at only 500 images processed. The longer the program runs; the more memory is consumed.This leads me to believe there is a memory leak somewhere. Either it's in the `nsfwjs-inputs.js` file; tensorflow; or NSWFJ.","[""Modifying the file to remove the classification code appears to not suffer memory leakage:```jsprocess.title = 'NSFWJS Resource Test'const fs = require('fs')const nsfwjs = require('nsfwjs')const nsfwjsInputs = require('./lib/nsfwjs-inputs.js')const pathJoin = require('path').joinconst IMG_DIR = pathJoin(__dirname; 'test-imgs')try {\tfs.accessSync(IMG_DIR)} catch (err) {\tfs.mkdirSync(IMG_DIR)}async function main() {\tlet nsfw = await nsfwjs.load('file://./nsfwjs_model/')\tlet files = fs.readdirSync(IMG_DIR)\tlet outputFile = fs.createWriteStream('./analysis.txt')\tlet counter = 0\tfor (let file of files) {\t\tconst filepath = pathJoin(IMG_DIR; file)\t\tlet input = await nsfwjsInputs.inputFromFile(filepath)\t\t/*let predictions = await nsfw.classify(input)\t\tlet predictionMap = predictions.reduce(\t\t\t(map; prediction) => ({\t\t\t\t...map;\t\t\t\t[prediction.className]: prediction.probability;\t\t\t});\t\t\t{}\t\t)*/\t\t//outputFile.write(`${file} = ${JSON.stringify(predictionMap)}\\n`; 'utf8')\t\tconsole.log(++counter)\t}}main().catch(err => console.error(err) && process.exit(1))```So it has to be an issue with NSFWJS.=====""; ""Hi @samholmes -  I am upgrading to TFJS 1.x here in this branch: https://github.com/infinitered/nsfwjs/pull/78Once merged; I'd love for you to check again and see if you're still having your issue.=====""; 'I solved it by using `input.dispose()`.====='; '@samholmes could you shed some more light on input.dispose?  What is input?====='; ""@camhart - as you see above where he made some functions called `imageToInput` etc.?When you convert an image to a tensor it loads it into GPU.   To garbage collect; you need to call `.dispose` on the tensor.   So once the tensor has been used; you should dispose it.If you'd like to learn more about tensors; I suggest my course:https://academy.infinite.red/p/beginning-machine-learning-with-tensorflow-js=====""; 'THANK YOUI had the same issueI disposed the input; and now the memory usage is stable :)====='; 'This!I really think this should be documented; as I was having memory leaks; having to restart the app a few times a day due to several alerts about RAM being almost full on the server!The fix is so simple; and definitely something that should be done anyway.Whatever object you send to "".classify()""; just run "".dispose()"" after.====='; ""I'd love to have a PR of where we should mention this in the docs!  Plz PR=====""]",Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.1] Memory Leak""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",B.2.1,A.4
https://github.com/tensorspace-team/tensorspace/issues/121,visualize the predictions,7,closed,2018-11-17T08:57:14Z,2019-04-20T03:03:19Z,@CharlesLiuyx Hi; I got some problems here when I'm trying to visualize the output image with my model. I even couldn't get the prediction result. May you help me with this? And the following is the javascript problems:Uncaught TypeError: Cannot read property 'dataSync' of undefined    at Sequential.updateLayerPredictVis (tensorspace.js:2548)    at Sequential.updateVis (tensorspace.js:2512)    at Sequential.predict (tensorspace.js:2269)    at Object.success (unet_sequence.html?_ijt=26969st38nf3fi6d731vk69l6f:150)    at i (jquery.min.js:2)    at Object.fireWith [as resolveWith] (jquery.min.js:2)    at A (jquery.min.js:4)    at XMLHttpRequest.<anonymous> (jquery.min.js:4),"['It seems that your pre-trained model do not compatible with TensorSpace; have you preprocessed(about TensorSpace preprocess: https://tensorspace.org/html/docs/preIntro.html) your pre-trained model properly?Can you provide more code information; such as github repo or codepen; maybe with more information; I can help position problem more precisely.====='; '@syt123450 thanks a lot! It worked! I did forget to preprocess the model and everything goes well after I did this! You guys really did an amazing job!====='; '@NextGuido You are welcome! Hope you enjoy TensorSpace~====='; '@syt123450 Hi; I tried adding a self-designed layer with keras using Lambda; but I cannot visualize the model once again! Can you solve this problem?Uncaught (in promise) Error: Unknown layer: Lambda. This may be due to one of the following reasons:1. The layer is defined in Python; in which case it needs to be ported to TensorFlow.js or your JavaScript code.2. The custom layer is defined in JavaScript; but is not registered properly with tf.serialization.registerClass().    at new t (errors.ts:48)    at deserializeKerasObject (generic_utils.ts:202)    at deserialize (serialization.ts:27)    at i (container.ts:1329)    at t.fromConfig (container.ts:1355)    at deserializeKerasObject (generic_utils.ts:235)    at deserialize (serialization.ts:27)    at models.ts:270    at index.ts:79    at Object.next (index.ts:79)====='; ""@NextGuido From the information you provided; it seems like you have the errors after model conversion. You probably need to register your custom layer before executing the converted tfjs model. If that's the issue; you can check the repo from official TensorFlow.js: https://github.com/tensorflow/tfjs-examples/tree/master/custom-layerIf you think that's still not the error source; can you provide more details about the error? 1. Which stage do you get the error? At preprocessing; building model or loading model?2. Can you provide more details about your custom layer? Like how to build it?And it will be helpful if you can provide some sample codes or links that we can have a look.=====""; ""@zchholmes Oh; I just wrote the code with keras using a custom layer and use tensorflowjs_converter to convert the model. When I did everthing as before; I met the error. I think you are right for registering my custom layer before executing the converted tfjs model. But I'm not familiar with the tfjs and javascript code. So; is there an easy way solve this? =====""; ""@NextGuido At this point; we really can't help too much. Since TensorSpace renders the model in web browser; which is required to consume a web acceptable model format - tensorspace.js model format. For now; there's no way to walk around; especially for the case involves custom layer; which requires extra description and clarification on the layer you customized.=====""]",Reference Error,Crash,Incorrect Code Logic,,,add data preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/306,Improve speed of first detection?,3,closed,2019-05-27T03:39:00Z,2020-01-09T03:14:49Z,"The first call to detect a photo takes about 10 seconds; and then takes milliseconds for all subsequent detections. I read about the speed in #32  issue; but I'd like to know how to pre-solve some bottleneck that happens only on first detection.--Is there any way to call some function to prepare before starting detection and avoid this initial delay? Taking into consideration that the user needs to do an action (click a button) to start the face detection.I am already doing the initiated boot load. According to the code.**App Init()**```const MODEL_URL = ""/static/models"";await faceapi.loadSsdMobilenetv1Model(MODEL_URL);await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);await faceapi.loadFaceDetectionModel(MODEL_URL);await faceapi.loadFaceRecognitionModel(MODEL_URL);navigator.mediaDevices  .getUserMedia({ video: { frameRate: { ideal: 10; max: 15 } } })  .then(stream => {    this.cameraPreview.srcObject = stream;    this.cameraPreview.style.display = ""block"";  })  .catch(err => {    alert(""error"");  });```**Call Detect**```start(){    configProcessFace();    detectFace();}configProcessFace() {  this.faceOptions = faceapi.SsdMobilenetv1Options({    minConfidence: 0.8;    maxResults: 1  });};async detectFace() {  const useTinyModel = false;  const fullFaceDescription = await faceapi    .detectSingleFace(this.canvasPreview; this.faceOptions)    .withFaceLandmarks(useTinyModel)    .withFaceDescriptor();  if (fullFaceDescription && fullFaceDescription.detection.box) {    .......  }  window.setTimeout(() => {    this.detectFace();  }; 400);};```","['It is actually due to all the shader programs being compiled on the first run when using the WebGL backend.> Is there any way to call some function to prepare before starting detection and avoid this initial delay? Yes; you can simply call the prediction chain you are using for an empty image or tensor initially; which will compile everything upfront. Just make sure when using a tensor; that the input size matches the input size of the face detector; for the SSD Mobilenet detector it is 512x512.====='; '@justadudewhohacks  Perfect.  I push a picture with a face; in the proportions 512x512 and did the recognition while loading the application. When the user will recognize it; it takes 1 second.For consult:```prepareFaceDetector() {      let base_image = new Image();      base_image.src = ""/static/img/startFaceDetect.jpg"";      base_image.onload = function() {        const useTinyModel = true;        const fullFaceDescription = faceapi          .detectSingleFace(base_image; new faceapi.TinyFaceDetectorOptions())          .withFaceLandmarks(useTinyModel)          .withFaceDescriptor()          .run()          .then(res => {            console.log(""--------> "" + JSON.stringify(res));          });      };}```====='; ""Hi @luisdemarchi ; I am facing the same issue for first detection; takes about 12s for me. Would you mind explaining in simpler terms how you made it faster? My code currently looks something like this:```const faceMatcher = new faceapi.FaceMatcher(FINALVECTORS;0.5)var idVar = setInterval(async () => {\tconsole.log('start capturing intervals')\tconst detections2 = await faceapi.detectSingleFace(video; new faceapi.TinyFaceDetectorOptions({inputSize:128})).withFaceLandmarks().withFaceDescriptor()\tif (detections2) {\t\tconst results = faceMatcher.findBestMatch(detections2.descriptor)\t\tif (labels.includes(results['label'])) {\t\t\tdocument.getElementById('msg').innerHTML=(`Hello; ${results['label']}!`)\t\t\tconsole.log('done')\t\t\tclearInterval(idVar)\t\t\t// setTimeout(window.close; 3000)````the bulk of the time is spent at at faceapi.detectSingleFace; how did you reduce the time a user would spend here?=====""]",Slow Execution,Poor Performance,Incorrect Code Logic,,,add warm up,add warm up,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.1,A.4
https://github.com/tensorspace-team/tensorspace/issues/138,Unsupported Ops in the model before optimization OneShotIterator; IteratorGetNext?,1,closed,2018-11-28T05:28:37Z,2019-04-20T03:03:33Z,Unsupported Ops in the model before optimization OneShotIterator; IteratorGetNext ?,"[""@yts19871111 Unfortunately; TensorSpace haven't reached pre-processing layer step. However; its really an interesting idea to add pre-processing layers to TensorSpace; if you have any idea which layers can be included; what these layers shall look like; feel free to propose it to us ~=====""]",Reference Error,Crash,Incorrect Code Logic,,,add data preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unsupported operator in model""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.4
https://github.com/justadudewhohacks/face-api.js/issues/21,Face Detection Video doesn't work,4,closed,2018-06-26T11:20:11Z,2018-07-04T06:14:16Z,I checked the Face Detection Video but it doesn't seem to work; there is an error message in the console panel; please see attached screenshot:![2018-06-26_18-18-35](https://user-images.githubusercontent.com/6857382/41908248-8666c72a-796d-11e8-879c-d72816475acb.png)Thanks;,"['Oops; I think that happens; when the video is loaded before the model is done loading. I will provide a fix for that example.====='; 'Should be fixed now.====='; ""@justadudewhohacks I don't think it is still working properly. Although; there is no error in console. But; once the video starts; it recognizes first face and draws the rectangle. After that; as soon as person1(Loenard) moves out of frame; that rectangle sticks to the screen and never moves. PFA the screen shot.![issue21_face-api](https://user-images.githubusercontent.com/25663886/42147636-5cdd6570-7dec-11e8-8422-278fdef93ad4.png)![issue21_face-api2](https://user-images.githubusercontent.com/25663886/42147638-607fbf84-7dec-11e8-881a-a7412c0de43c.png)Is it due to performance issue?P.S. I have not cloned the repo. I just downloaded the archive from github by clickng download zip button.=====""; ""Yes; that's simply because the face detector doesn't run in realtime on your machine.=====""]",Data & Model Error,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.1] Time"",
    ""specific_type"": ""[B.1.1] Slow Execution""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.2,A.4
https://github.com/martinjm97/ENNUI/issues/98,Weird Split model architecture performs bad,7,closed,2019-03-06T21:33:23Z,2019-05-04T14:26:04Z,Performs bad on tfjs but good on python.,"['![image](https://user-images.githubusercontent.com/8813660/53915254-96401980-402d-11e9-8e72-214510a0abea.png)This is random across classes when trained in tfjs; but great in python...====='; ""@Zackh1998  Did you refresh page and try again? Could've been a webgl lost context.=====""; 'I just tried this model and it works just fine on tfjs. This was definitely a WebGL lost context issue.====='; ""I tried multiple times... it's still broken https://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22;%22children_ids%22:%5B2;7%5D;%22parent_ids%22:%5B%5D;%22params%22:%7B%22dataset%22:%22mnist%22%7D;%22id%22:0;%22xPosition%22:100;%22yPosition%22:399%7D;%7B%22layer_name%22:%22Conv2D%22;%22children_ids%22:%5B3%5D;%22parent_ids%22:%5B0%5D;%22params%22:%7B%22filters%22:10;%22kernelSize%22:%5B5;5%5D;%22strides%22:%5B2;2%5D;%22activation%22:%22relu%22%7D;%22id%22:2;%22xPosition%22:250;%22yPosition%22:319%7D;%7B%22layer_name%22:%22Dense%22;%22children_ids%22:%5B6%5D;%22parent_ids%22:%5B0%5D;%22params%22:%7B%22units%22:30%7D;%22id%22:7;%22xPosition%22:285;%22yPosition%22:564%7D;%7B%22layer_name%22:%22Flatten%22;%22children_ids%22:%5B5%5D;%22parent_ids%22:%5B2%5D;%22params%22:%7B%7D;%22id%22:3;%22xPosition%22:571;%22yPosition%22:319%7D;%7B%22layer_name%22:%22Flatten%22;%22children_ids%22:%5B5%5D;%22parent_ids%22:%5B7%5D;%22params%22:%7B%7D;%22id%22:6;%22xPosition%22:527;%22yPosition%22:535%7D;%7B%22layer_name%22:%22Concatenate%22;%22children_ids%22:%5B1%5D;%22parent_ids%22:%5B6;3%5D;%22params%22:%7B%7D;%22id%22:5;%22xPosition%22:765;%22yPosition%22:444%7D;%7B%22layer_name%22:%22Output%22;%22children_ids%22:%5B%5D;%22parent_ids%22:%5B5%5D;%22params%22:%7B%7D;%22id%22:1;%22xPosition%22:900;%22yPosition%22:399%7D%5D;%22hyperparameters%22:%7B%22learningRate%22:0.1;%22batchSize%22:64;%22optimizer_id%22:%22defaultOptimizer%22;%22epochs%22:6;%22loss_id%22:%22defaultLoss%22%7D%7D=====""; 'Not a bug for me at all. ![Screenshot from 2019-03-09 16-09-54](https://user-images.githubusercontent.com/7140467/54077423-f9c48400-4285-11e9-8215-1945aecc5f0b.png)====='; 'Still not working for you?====='; 'Makes absolutely no sense; is impossible to debug; and only shows up on my computer.=====']",Slow Execution,Poor Performance,Unknown,,,,,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,E
https://github.com/martinjm97/ENNUI/issues/65,If a parameter for a layer is a decimal; the python generation does not take anything after decimal point,2,closed,2019-02-18T19:31:06Z,2019-02-19T16:42:01Z,You can see this behavior in the newLayers branch,['You can also see this behavior by editing Conv filters to 10.1. The python code will show 10 instead.====='; 'Solution is in the newLayers branch====='],Incorrect Functionality,Incorrect Functionality,Unknown,,,,,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,E
https://github.com/martinjm97/ENNUI/issues/59,Activations are never removed from layers when deleted while attached,1,closed,2019-02-17T21:22:34Z,2019-02-24T22:35:41Z,,"[""The issue was that when activations were being added in model_templates; the layer was not also being added to that activation; this deleting the activation didn't update the corresponding layer.=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3"",
    ""specific_type"": ""D.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/issues/1,Console error expected dense_Dense1 to have shape [;57;10],18,open,2018-07-14T21:18:26Z,2019-02-21T22:10:27Z,I've got an error trying to run your code;Uncaught (in promise) Error: Error when checking target: expected dense_Dense1 to have shape [;57;10]; but got array with shape [1;1960;1].,"[""Same; I've tried changing a lot of parameters and modifying the data structure; but I can't get rid of it.The error appears in model.fit (and also when testing model.evaluate) at line 105 in 2b_cnn.js=====""; ""I would love a thorough explanation for why this happens in this case in particular. I've been banging my head against it for a while now and the head is losing.=====""; 'I implement this in Python and Js; same error with data shapes in Dense Layer.====='; ""glad I'm not the only one seeing this error.  =====""; ""I'm afraid it's my fault - I wrote the original (and as yet uncompleted) code! The CNN wasn't fully implemented yet!**NINJA EDIT - 9.30.18**: Very sorry all please see the comment added below. Also; forgive the excessive `!`'s. Thanks!=====""; 'no fix ?====='; 'did anyone manage to figure this out... ====='; ""There are major errors with the code. It is not in a functioning state and that makes me wonder it was even tested before the video was recorded? Firstly; **`data.highs` is mapped to epoch times** here:https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2a_cnn.js#L35then comes the reshaping part. Why not reshape it prior when the dates are being converted to epoch? https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L94That does not make sense.  ( https://js.tensorflow.org/api/0.12.5/#reshape )and then the part that left me speechless was this... https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L95Why would you map a price series that way? Not even sure what was the goal... Could someone enlighten me. I'm not sure how the reshaped list should look like. =====""; ""It wasn't tested; it's just an idea but that wasn't made very clear in the video.I've played around with it a lot and gotten it to a state where it works fairly well with a simple NN consisting only of dense layers. Next step is to get my head around formatting the data as matrices and implementing a CNN; but I'm learning as I'm going and some of the maths is hard.=====""; 'http://oracle.gigil.berlin/====='; 'Same problems here!====='; 'Same here; kinda disappointment.====='; '@llSourcell Hi; do you notice this issue?====='; ""> There are major errors with the code. It is not in a functioning state and that makes me wonder it was even tested before the video was recorded?> > Firstly; **`data.highs` is mapped to epoch times** here:> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2a_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2a_cnn.js#L35)> > Line 35 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  dates.push(new Date(data[i]['Date'] + 'T00:00:00.000').getTime()) > then comes the reshaping part. Why not reshape it prior when the dates are being converted to epoch?> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2b_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L94)> > Line 94 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  tdates.reshape([1; 1960; 1]); > That does not make sense. ( https://js.tensorflow.org/api/0.12.5/#reshape )> > and then the part that left me speechless was this...> > [Financial_Forecasting_with_TensorflowJS/public/scripts/2b_cnn.js](https://github.com/llSourcell/Financial_Forecasting_with_TensorflowJS/blob/3244c20286dcb020c24fc948a3c408a6bf03b705/public/scripts/2b_cnn.js#L95)> > Line 95 in [3244c20](/llSourcell/Financial_Forecasting_with_TensorflowJS/commit/3244c20286dcb020c24fc948a3c408a6bf03b705)> >  thighs.reshape([1; 1960; 1]); { > Why would you map a price series that way? Not even sure what was the goal...> > Could someone enlighten me. I'm not sure how the reshaped list should look like.Hi All! I'm @Thoughtscript - I wanted to apologize about the confusion this has caused! Yikes! First; let me be very clear that I'm not in any way affiliated with @llSourcell.This code was **NOT PRODUCTION WORTHY** and **INCOMPLETE** - as noted on the original repo listed [here](https://github.com/Thoughtscript/x_team_tensorflow_js) - it's a **WORK IN PROGRESS**.Actually; I'm not sure how it ended up in a video or attracting any interest whatsoever. The code was being completed for an article to be posted on the top-ranked consulting firm [X-Team](https://x-team.com/blog/)'s blog. Unfortunately; the company decided to move in another direction with its writing and I've since moved over to Microsoft (though I remain affiliated with X-Team's great community).Every other repo listed under my @Thoughtscript account without a *WIP* flag should be fully functional (with any issues; if they exist; noted). This particular project (which you'll note bears my name on in the pictures); is actually no longer maintained. Any questions; feel free to send me an email at adam.gerard@gmail.com. Sorry for any confusion this has caused. Cheers!=====""; ""PS - maybe try:1. Normalizing the values1. Correcting the shapes1. Correctly configuring the convolutional neural net1. Leveraging parallel processing and WebGL since that shit's gonna break on single-threaded JS=====""; 'Hi @Thoughtscript ; thanks for your kind explanation and mostly for noticing this issue!Now the big question is; what was the working demo which @llSourcell showed us in his Live stream.https://youtu.be/5Uw1iSwvHH8?t=47m53sAnyway; I think this is not a proper learning material after all.====='; ""> Anyway; I think this is not a proper learning material after all.exactly - this is generating a lot of confusion. for beginners this whole subject is hard to grasp and it is made even harder with all the other backend stuff going on.siraj makes stuff look really easy by skipping a lot of difficult and crucial steps and this repo here is the perfect example i wasn't even aware that this is not his code - but maybe i just didn't pay attention¯\\_(ツ)_/¯=====""; ""I tried to use the code but it didn't worked for me. So the closest working solution that I got was this one: https://towardsdatascience.com/stock-price-prediction-system-using-1d-cnn-with-tensorflow-js-machine-learning-easy-and-fun-fe5323e68ffb Hope that helps.=====""]",Data & Model Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",A.2,A.3
https://github.com/mwdchang/tfjs-gan/issues/3,GPU memory crash,1,open,2020-02-14T21:31:52Z,2020-02-20T20:35:19Z,Hi @mwdchang; thank you for the awesome repo!I modified your example to train with some black and white images of hand drawn curves; just for my own learning purposes.My images are 50x50; so they are a bit bigger than the MNIST images. I'm running into a problem where Chrome's GPU process crashes after a couple of training iterations.Looking at the Chrome task manager; I noticed that the GPU process retains memory and never garbage collects after training.Do you have any suggestions? I was thinking about caching the weights but I was not sure how to do that.Thanks again,['@freeman-g you might be able to alleviate some of the memory pressure by reducing the sizes of the hidden layers. Having said that; this was created as a proof-of-concept/toy-example and not meant for heavy-duty usage.====='],Abnormal GPU Memory/Utilization,Poor Performance,Incorrect Code Logic,,,modify model,modify model,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[B] Poor Performance"",
    ""subcategory"": ""[B.2] Memory"",
    ""specific_type"": ""[B.2.3] Abnormal GPU Memory/Utilization""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",B.2.3,A.4
https://github.com/justadudewhohacks/face-api.js/issues/789,create NetInput from base64 string,1,open,2021-05-04T07:57:45Z,2021-05-04T13:58:41Z,"Hi; I'm trying do create a webservice to do face matching; so I'm writting code with textscript and express; I can get the picture in base64 but I'm trying to do a face matching but in the middle I don't know how to create a NetInput from base 64 string.I tryied to create an Image but when I run I get this error: (node:10663) UnhandledPromiseRejectionWarning: Error: Image given has not completed loadingthis is my code:       `const imgRef = faceapi.env.getEnv().createImageElement();         imgRef.src = 'image/jpeg;base64;'+rostros.referencia;         imgRef.width = 393;         imgRef.height = 574;         console.log(""imgRef complete"");         console.log(imgRef.complete);         console.log(""about detectAllFaces"");        const results = await faceapi.detectAllFaces(imgRef)            .withFaceLandmarks().withFaceDescriptors();`","[""it's exactly what it says 'image has not completed loading' - assigning `imgRef.src` is ok; but then execute actual detection inside event that triggers when image is loaded; something like `imgRef.onload = () => faceapi.detectAllFaces(...)`=====""]",Data & Model Error,Crash,Incorrect Code Logic,,,change code order,Adjust API invocation sequence,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.4
https://github.com/justadudewhohacks/face-api.js/issues/524,getting error in node js while rendering the locally saved user images,2,open,2020-01-16T07:54:21Z,2020-01-28T10:40:45Z,UnhandledPromiseRejectionWarning: TypeError: Only absolute URLs are supported const img = await faceapi.fetchImage(`./labeled_images/${label}/${i}.jpg`),['faceapi.fetchImage uses the fetch API under the hood; which does not allow you to read images from disk / access file system directly; it requests an image over the network.To read images from disk in nodejs use the canvas package; as shown in the examples or use the filesystem API of tfjs-node.====='; 'Thanks  for the response @justadudewhohacks .Could u pls tell me how to stream a rtsp video for face recognition.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change API,Replace API with another effective one,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/485,Server Implementation Not Working,2,open,2019-11-25T07:35:08Z,2020-01-20T23:37:39Z,"Hello community;This might not be the issue with the faceapi but with my implementation; i am trying to do the face recognition in a real time. The frame comes from my webcam via socket as a base 64 string; all i am trying is to get it matched with my preloaded images.```function prepareCanvas(data) {  return new Promise((resolve; reject) => {    try {      const canvas = createCanvas(parseInt(data.width); parseInt(data.height))      var ctx = canvas.getContext(""2d""; { pixelFormat: ""RGBA24"" });      var image = new Image();      image.src = data.base64      ctx.drawImage(image; 0;0; parseInt(data.width); parseInt(data.height));      resolve(tf.browser.fromPixels(canvas));    } catch (err) {      console.log(""prepareCanvas error===>""; err)      reject(err)    }  })}``````async function matchWithVideoStream(data; classifierType) {  try {    let frame = await prepareCanvas(data)    const detections = await faceapi.detectAllFaces(frame; classifierType).withFaceLandmarks().withFaceExpressions().withFaceDescriptors();    frame.dispose();    const resizedDetections = faceapi.resizeResults(detections; { width: 800; height: 800 })    const results = resizedDetections.map((d) => {      return faceMatcher.findBestMatch(d.descriptor)    })    console.log('resuts'; results)  } catch (err) {    console.log(""matchWithVideoStream error""; err)  }}```","['On first sight I can not spot anything obvious causing your example to not work. Did you try to display the image to verify that you are indeed receiving the data / constucting the image tensor correctly?====='; ""I may have found a simpler solution to all this! Simply convert your image into _base64_ and store that as the _src_ property of the image!Imports include [node-canvas](https://github.com/Automattic/node-canvas):```jsconst canvas = require('canvas');const { Canvas; Image } = canvas;faceapi.env.monkeyPatch({ Canvas; Image });```Code:```js  const imgstring = await image2base64('./img.jpg');  const data = new Image();  data.src = new Buffer(imgstring; 'base64');  data.width = 480;  data.height = 360;  const detections = await faceapi.detectAllFaces(data);```In my case I'm using [sharp](https://github.com/lovell/sharp) to load the image:```js  const lol = await sharp(imgPath).toBuffer();  const data = new Image();  data.src = lol;  data.width = 480;  data.height = 360;  const detections = await faceapi.detectAllFaces(data);```🙇\u200d♀ Thank you _face-api.js_ 🙇 =====""]",Data & Model Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/329,Blob is not defined NodeJS,1,open,2019-06-19T16:39:57Z,2019-06-24T16:29:01Z,Hey Vincent; getting an error of 'Blob is not defined'. What I'm trying to accomplish:Receive an image using multer and reading the face detections in realtime; then storing the face detections in a database.What I've tried to solve the issue:I've tried using node-fetch to retrieve the image and load it into faceapi detections; but node-fetch requires a URL.Tried using multiple packages to convert the image to a blob and use faceapi.BufferToImage to convert it to an HTMLImageElement;  but the packages don't do the job.Appreciate your help; and hopefully an answer will help anyone else having the same issue.I'll attach snippets of my code belowuserRouter.js where multer exists; usercontroller.js is fully imported here![Screen Shot 2019-06-19 at 11 32 02 AM](https://user-images.githubusercontent.com/50146137/59783791-9edbf900-9286-11e9-8fe7-5b2d45208007.png)userController.js where I implement the getDetections function![Screen Shot 2019-06-19 at 11 31 28 AM](https://user-images.githubusercontent.com/50146137/59783919-db0f5980-9286-11e9-84ba-ba3370d2863c.png)The error i receive (as you can see the file upload is typeof Object![Screen Shot 2019-06-19 at 11 25 25 AM](https://user-images.githubusercontent.com/50146137/59783961-f24e4700-9286-11e9-913d-810b422814bf.png),"['First; you need to monkey patch faceapi env.```const canvas = require(""canvas"");const { Canvas; Image; ImageData } = canvas;faceapi.env.monkeyPatch({ Canvas; Image; ImageData });```Then; you need to load image with canvas module by giving it uploaded image buffer.```const getDetections = async (imageSource) => {    const img = await canvas.loadImage(imageSource.buffer);    const detections = await faceapi      .detectSingleFace(img; new faceapi.TinyFaceDetectorOptions())      .withFaceLandmarks()      .withFaceDescriptor()      .withFaceExpressions();    return detections;  }```=====']",Reference Error,Crash,API Misuse,,,patch environment,Fix environment adaptability,web application,Data Processing,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.3
https://github.com/justadudewhohacks/face-api.js/issues/827,Analysing images hosted on other domains,2,closed,2021-11-10T02:00:55Z,2021-11-10T19:59:49Z,I'm getting `SecurityError: The operation is insecure.` because I'm trying to analyse an image that's on my asset server (another domain). Is this the browser throwing the error or a limitation of `face-api.js`? :),"['when you load an image from an external domain; browser marks canvas as tainted and doesnt allow any user app to read canvas data  workaround is to setup a proxy to load such images or find a browser plugin that overrides browser setting (it cannot be done in user-mode apps; but plugins have near-root access so it can be done)  ====='; ""Thanks @vladmandic; very helpful. I've decided to go down the route of a small node express app on my server that can process images locally and store the metadata for use later. I've switched over to your `Human` library. Very nice work there :)=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,use proxy,use proxy,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/824,Bug-UnhandledPromiseRejectionWarning - OOM when allocating tensor with shape[1;256;256;64],21,closed,2021-09-24T21:02:28Z,2021-09-26T18:12:20Z,Hi;I tried to insert 10000 record to DB in loop using this link and got this error after 300 records.https://github.com/vladmandic/face-api/blob/master/demo/node-image.js```2021-09-24 23:59:17.493963: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[1;256;256;64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu(node:21876) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 8Message: OOM when allocating tensor with shape[1;256;256;64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu    at Object.<anonymous> (<anonymous>)```,"[""In production I will have same problem ? What can I do ?This the relevant code:``` const tensor = tf.tidy(() => { // create tensor from image data    const data = tf.tensor(Array.from(imageData?.data || []); [canvas.height; canvas.width; 4]; 'int32'); // create rgba image tensor from flat array and flip to height x width    const channels = tf.split(data; 4; 2); // split rgba to channels    const rgb = tf.stack([channels[0]; channels[1]; channels[2]]; 2); // stack channels back to rgb    const reshape = tf.reshape(rgb; [1; canvas.height; canvas.width; 3]); // move extra dim from the end of tensor and use it as batch number instead    return reshape;  });```=====""; ""you're using explicit functionality from <https://github.com/vladmandic/face-api>  so create issues there.tensor should never have shape `[1;256;256;64]` (and that is massive; thus OOM); it should be `[1;width;height;4]`. which means that your input `imageData` is somehow corrupt before it reaches this part. cant know for sure if i don't know how it was created up to that point.=====""; ""At least two major things- you're writing to file and not waiting for write to complete before later reading it. Use writeFileSync- you're recreating faceapi class and reloading weights on each image. Not only extremely costly; but likely will cause issues sooner or later. On a major note; for diagnostics learn to check your interim results before posting issue. E.g.; what is your canvasObj.width and height? Is it even valid. What about imageData? Etc. And for production; *always* check if any interim output is valid before passing it on to next function. You're basically running blind and when error happens; you don't know where it all started.=====""; ""And a major suggestion - since you don't need canvas to draw anything; so how about avoid using it completely. Take a look at another demo that does native JPG decoding to tensor directly and without external library. =====""; 'And why are you writing to file and then reading it back when you already have buffer in memory.There is sooo much wrong with this.====='; '> And why are you writing to file and then reading it back when you already have buffer in memory.> > There is sooo much wrong with this.I need to keep this file for other system to show this file.====='; ""> * you're recreating faceapi class and reloading weights on each image. Not only extremely costly; but likely will cause issues sooner or later.How I can avoid it ?=====""; ""A) you can keep it; but you don't have to re-read it immediately.B) how can you avoid recreating and reloading weights? By doing that during app startup and keeping it. That's as basic as it gets. Entire section 2 should be once per app; not once per image. =====""; ""> On a major note; for diagnostics learn to check your interim results before posting issue. E.g.; what is your canvasObj.width and height? Is it even valid. What about imageData? Etc.> > And for production; _always_ check if any interim output is valid before passing it on to next function. You're basically running blind and when error happens; you don't know where it all started.Fixed:)=====""; ""> A) you can keep it; but you don't have to re-read it immediately.> B) how can you avoid recreating and reloading weights? By doing that during app startup and keeping it. That's as basic as it gets. Entire section 2 should be once per app; not once per image.Only pass Entire section 2  to startup; where i keep it ? can you show me how?=====""; ""No; NOT like this.And I can't show you how. That is basic app state management for JavaScript. I don't want to write the app for you; learn basics. From now on; I'll only reply to specific questions regarding Faceapi and only if diagnostics info has been provided; not just code copy&paste.=====""; ""Sorry I understood you now.To create  this file and to called it from startup and from each register or compare? is it```const tf = require('@tensorflow/tfjs-node');const path = require('path');const canvas = require('canvas');const faceapi = require('@vladmandic/face-api');async function loadModals() {      faceapi.env.monkeyPatch({ Canvas: canvas.Canvas; Image: canvas.Image; ImageData: canvas.ImageData });      const faceDetectionNet = new faceapi.FaceDetectionNet();      const modelPath = '../../node_modules/@vladmandic/face-api/model';      await faceDetectionNet.loadFromDisk(path.join(__dirname; modelPath));      await faceapi.nets.ssdMobilenetv1.loadFromDisk(path.join(__dirname; modelPath));      await faceapi.nets.ageGenderNet.loadFromDisk(path.join(__dirname; modelPath));      await faceapi.nets.faceLandmark68Net.loadFromDisk(path.join(__dirname; modelPath));      await faceapi.nets.faceRecognitionNet.loadFromDisk(path.join(__dirname; modelPath));      await faceapi.nets.faceExpressionNet.loadFromDisk(path.join(__dirname; modelPath));}module.exports = { loadModals}```and called it from start up:app.listen(config.serverPort; async () => {          //.Load models  await  loadModals();}and same from register and compare .=====""; ""> And a major suggestion - since you don't need canvas to draw anything; so how about avoid using it completely. Take a look at another demo that does native JPG decoding to tensor directly and without external library.Which demo ? I looked on your demo and didn't see- please provide a link:)=====""; ""A) No. Take a step back and think. Look what is CONSTANT vs what is CHANGING and should be inside `register`. In your case; `optionsSSDMobileNet` that gets created in step 5 should really be part of step 2 and entire step 2 should be be placed somewhere where its executed ONCE on app startup. Only thing that matters is that `optionsSSDMobileNet` is visible from within `register` method so it can be used from there. DO NOT RECREATE IT FOR EVERY IMAGE.B) You did not look. Function is even marked with comments:> // read image from a file and create tensor to be used by faceapi> // this way we don't need any monkey patchesNext time when you have  a question; step back  Think about it and research it a bit  Search for solutions before posting issues on GitHub  Post issues in a correct repository  GitHub Issues is for issues; not for learning how to code - Only if its an issue; then post  Youre close of being blocked  =====""; ""Hi;Thank a lot for your answers.It is very appreciated here :)I fixed all your remarks; and run again the app from vs code with 1000 and after 827 inserts it crash better than before 300 inserts. I insert **same** image every time.You said:tensor should never have shape [1;32;32;512] (and that is massive; thus OOM); it should be [1;width;height;4]. which means that your input imageData is somehow corrupt before it reaches this part. cant know for sure if i don't know how it was created up to that point.> > > 26/09/2021 07:51:15:647 faceRecognition.ts - info : register: Create tensor done successfully(tensor shape:1;403;716;3; tensor size:865644> > 2021-09-26 07:51:15.809551: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[1;32;32;512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu> > (node:12320) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 8> > Message: OOM when allocating tensor with shape[1;32;32;512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu> >     at Object.<anonymous> (<anonymous>)![image](https://user-images.githubusercontent.com/1079689/134794241-8d080f92-a3b2-48e2-adde-f327859555fa.png)=====""; 'no actual information provided.====='; 'I used this code to register new faces into DB:https://github.com/vladmandic/face-api/blob/master/demo/node-image.jsMy installed ram is 16;GB ; window 10 ;Processor: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz   2.71 GHz.I used `node-cashe` to keep the descriptors.====='; ""a) you DON'T USE my code - don't just provide link to it!!! you have your own code based on my code. so provide that.b) there is no diagnostic info provided that i mentioned several times before! what are interim states of variables. LOG!!!=====""; ""STILL NO DIAG OUTPUT - LOG YOUR OPERATIONS. sorry for yelling; but i've said this how many times so far?What are the values of `tensor` or `imageData`And what is method `image.getImageData`? It's not here. Post full code on **GIST** not inline in issue as it's impossible to track after a while.And post LOG there as well.And when reporting error; execute as simply as possible. You're noting that `VSCode` runs out of memory. Don't - it impacts how NodeJS performs garbage collection. Make sure this error is reproducible from NodeJS directly.=====""; '1.Logs:26/09/2021 19:37:18:616 faceRecognition.ts - info : register: Load models done successfully26/09/2021 19:37:18:620 faceRecognition.ts - info : register: Create image data done successfully(image path:D:\\images\\2021.09.26\\12838.jpeg; width:135; height:24026/09/2021 19:37:18:635 faceRecognition.ts - info : register: Create tensor done successfully(tensor shape:1;240;135;3; tensor size:972002.imageData`const imageData = image.getImageData(canvasObj); // read decoded image data from canvas     if(!imageData){       let createImageDataMessage = ""register: Create image data failed"";       utils.logger.log({         level: \'error\';         message: createImageDataMessage;         label: \'faceRecognition.ts\'       });       return { success: false; error:createImageDataMessage }     }3. I will test this code NodeJS directly.====='; 'Step back. Do something else. Come back tomorrow. Re-read the thread and POST RELEVANT INFORMATION.What are the values of `tensor` or `imageData` ? Log them during execution and POST LOG!And still no idea what is `image.getImageData(canvasObj)` Where is code for that?Use GIST.=====']",Out of Mermory,Poor Performance,Misconfiguration,,,change input shape,Replace data Shape/type,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B,B.1
https://github.com/justadudewhohacks/face-api.js/issues/693,canvas can't detected with jsmpeg and face-api.js,2,closed,2020-09-09T09:40:15Z,2020-09-11T02:56:26Z,"(English is not my first language; sorry)I use ""rtsp-to-stream"" which is use to transcoding and jsmpeg to play my camera.```""node-rtsp-stream"": ""0.0.9"";```and I use ""videoFaceTracking.html"" ; this examples to do experiment.this is my Html code:```html<div style=""position: relative"" class=""margin"">      <!-- <video src=""bbt.mp4"" id=""inputVideo"" autoplay muted loop playsinline></video> -->      <canvas id=""myPlayer-min-left"" class=""my-player-min"" ></canvas>      <canvas id=""overlay""></canvas> </div>```and my jsmpeg code```js$(document).ready(function() {      renderNavBar('#navbar'; 'video_face_tracking')      // jsmpeg start      const myPlayerL = document.getElementById('myPlayer-min-left')      var urlL = 'ws://localhost:9530'      this.playerL = new JSMpeg.Player(urlL; { canvas: myPlayerL })      initFaceDetectionControls()      run()    })```and I change `onPlay()` args like this:```js // start processing framesonPlay(document.getElementById('myPlayer-min-left'))```then I want to get some info by console ; so I change `onPlay`  body like this```js      if (drawBoxes) {        if (resizedResults && resizedResults.length !== 0) {          // here is my change          console.log('detect face success....';  results)        }        faceapi.draw.drawDetections(canvas; resizedResults)      }      if (drawLandmarks) {        faceapi.draw.drawFaceLandmarks(canvas; resizedResults)      }```next is my question.I find that it doesn't have any output unless I set my-Player-min to ""display: none""```css<style>    .my-player-min {      display: none;    }  </style>```So I wonder why this is happening","[""The problem is solved! The reason is that the 'getContext(' webGL ')' mode is used by default when drawing canvas in Jsmpeg; while the 'getContext(' 2D ')' mode of the same canvas is needed in face-APi.js; resulting in conflicts. However; face-APi.js does not throw any error exception; which makes it difficult to find the problem=====""; 'sorry. Forget to add the solution. Just add one attribute `disableGl`  in the JSMPEG```jsthis.playerL = new JSMpeg.Player(urlL; { canvas: myPlayerL;  disableGl: true  })```=====']",render bug,Crash,Import Error,,,parameter modifier,Modify API Parameter usage,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Output issue due to context conflict""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A,A.6
https://github.com/justadudewhohacks/face-api.js/issues/408,expected media to be of type HTMLImageElement,4,closed,2019-09-11T00:18:38Z,2019-09-15T19:24:31Z,"```const faceapi = require('face-api.js');require('@tensorflow/tfjs-node');const fetch = require('node-fetch');const path = require('path');faceapi.env.monkeyPatch({ fetch: fetch });const MODELS_URL = path.join(__dirname; '/models/');const jsdom = require(""jsdom"");const { JSDOM } = jsdom;global.window = new JSDOM(`<!DOCTYPE html><img id=""myImg"" src=""image.png"" />`; { resources: ""usable""; url: ""file:///"" + __dirname + ""/"" }).window;global.document = window.document;global.HTMLElement = window.HTMLElement;`async function detectFaceCommand(message) {    await faceapi.nets.ssdMobilenetv1.loadFromDisk(MODELS_URL);    await faceapi.nets.faceLandmark68Net.loadFromDisk(MODELS_URL);    await faceapi.nets.faceRecognitionNet.loadFromDisk(MODELS_URL);    image = document.getElementById('myImg');    let fullFaceDescriptions = await faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor();}```This just gives me `Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D; or to be an element id`Outputting `image` to the console shows that it is an HTMLImageElement yet it still says its not.","[""Might be that the image element returned by JSDOM is not a valid HTMLImageElement; e.g. `img instanceof HTMLImageElement` returns false. If that's the case you could try to monkey patch HTMLImageElement somehow.=====""; ""I messed around with it for a bit and `image instanceof HTMLImageElement` returns true. In the code snippet I provided I realized `global.HTMLElement = window.HTMLElement` was supposed to be `global.HTMLImageElement = window.HTMLImageElement` but I still get `expected media to be of the type HTMLImageElement`I got around this issue by adding `Image: window.HTMLImageElement` to monkeypatch but now I get `TypeError: canvas.getContext is not a function` in getContext2dOrThrow.js so I'm assuming I need to do similar to `env.Canvas` with monkeyPatch but I cant figure out what I need to set it to.If I set it to window.HTMLCanvasElement it dosen't work because in /build/commonjs/index.js line 38 tries `new Canvas()` but Canvas is an HTMLCanvasElement so what is env.Canvas supposed to be?=====""; ""> I got around this issue by adding Image: window.HTMLImageElement to monkeypatch but now I get TypeError: canvas.getContext is not a function in getContext2dOrThrow.js so I'm assuming I need to do similar to env.Canvas with monkeyPatch but I cant figure out what I need to set it to.Exactly; check out the nodejs examples that utilize the canvas node package to monkey patch Image; Canvas and ImageData.I am actually not sure in what kind of environment you are running your code; but you also wanna try monkey patching the constructors of Image and Canvas:``` javascriptfaceapi.env.monkeyPatch({   createCanvasElement: () => document.createElement('canvas');   createImageElement: () => document.createElement('img') })```     =====""; ""Thanks a ton this worked:```faceapi.env.monkeyPatch({    fetch: fetch;    Canvas: window.HTMLCanvasElement;    Image: window.HTMLImageElement;    ImageData: canvas.ImageData;    createCanvasElement: () => document.createElement('canvas');    createImageElement: () => document.createElement('img')});```=====""]",Data & Model Error,Crash,API Misuse,,,patch environment,Fix environment adaptability,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Expected media type error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/388,Module For Face/Age/gender Detection  Not Working in NODE.JS /Cordova Camera Upload Using Multer,3,closed,2019-08-19T10:58:29Z,2019-10-26T08:00:01Z,"Module For Face/Age/gender Detection  Not Loading In Browser from a URIIssue :I am not able to load the module correctly for Age/gender detection using Http request. I am using express in the backend and i cannot use 15mb of file all the time. However i only need 1 model for my requirements . Due to less documentation i am not able to identify which model to load on my front end for gender/age detection to minimise bandwidth usage and speed up app.Please suggest.Note : I am not using TypeScript . By Making your project in TypeScript is has made transition even harder as we do not require TS for anything. and the validation messsages might have worked when conversion oj Object at JS but due to TS it has made life harder here. We hoped to use Face APi but cannot continue lie this. Error Message with Multer```Image File :{ fieldname: 'imageverify';  originalname: '1567030996427.jpg';  encoding: '7bit';  mimetype: 'image/jpeg';  public_id: 'gender/imageverify-1567030997917';  version: 1567031005;  signature: '30f09b6a4ea72e2409dd8d0b15821f97644dfa4c';  width: 4032;  height: 3024;  format: 'jpg';  resource_type: 'image';  created_at: '2019-08-28T22:23:25Z';  tags: [];  bytes: 411900;  type: 'upload';  etag: 'fd1291756909ab991c5afa3988391422';  placeholder: false;  url: 'http://res.cloudinary.com/hookup/image/upload/v1567031005/gender/imageverify-1567030997917.jpg';  secure_url: 'https://res.cloudinary.com/hookup/image/upload/v1567031005/gender/imageverify-1567030997917.jpg';  original_filename: 'file' }{ lastModified: 1567031006333;  lastModifiedDate: 2019-08-28T22:23:26.333Z;  type: 'image/jpeg';  size: 411900;  name: '1567030996427.jpg' }Promise { <pending> }(node:34199) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): Error: toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D; or to be an element id(node:34199) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.```Made Some Other Changes But does not work```log = console.log.bind(console);async function execute(imgPath) {        // Trying To Convert To File Imput Image Upload Format        var obj = {};        obj.lastModified = Date.now();        obj.lastModifiedDate = new Date();        obj.type = imgPath.mimetype        obj.size = imgPath.bytes;        obj.name = imgPath.originalname;        log(obj);        var returnValue = await faceapi.detectSingleFace(obj; new faceapi.SsdMobilenetv1Options()).withAgeAndGender();        log(returnValue);        return returnValue;    };        app.post('/get/picture/validator'; mutlerUpload.single(""imageverify""); function(req; resp; next) {        log('/get/picture/validator');        var image = req.files || req.file;        //var encoded = image.buffer.toString('base64');        log(""Image File :"");        log(image);        var returnValue = execute(image.url);        log(returnValue);        resp.send();    });```If possible kindly look into it.","[""> await faceapi.detectSingleFace(obj; new faceapi.SsdMobilenetv1Options()).withAgeAndGender();The `obj` variable you are passing into the api is not a valid input; it should either be a HTMLImageElement or a Tensor. You are just passing some self assembled javascript object in there in your code example. That's why you receive the error.=====""; 'If possible can you pls help me out with creating a Obj which can be captured via device camera so that i can upload it to backend via REST API. some documentation would be appreciated for Apache Cordova based system. This Project is such a GEM . We are having limited knowledge in ML and has tried all methods to make it work. Maybe some leads would be appreciated====='; 'If your goal is to fetch an image from an URI; take a look at [this](https://github.com/justadudewhohacks/face-api.js#fetch-and-display-images-from-an-url) codesnippet provided by the README.=====']",Data & Model Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.3
https://github.com/justadudewhohacks/face-api.js/issues/168,Uncaught (in promise) DOMException: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source width is 0.,6,closed,2018-12-13T23:58:19Z,2019-04-08T05:25:46Z,My loop was running happily; but consistently dies on this one image.Should getContext2dOrThrow also be checking for 0 width canvas?```Uncaught (in promise) DOMException: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The source width is 0.    at http://localhost:8080/face-api.js:2145:55    at Array.map (<anonymous>)    at http://localhost:8080/face-api.js:2141:53    at step (http://localhost:8080/face-api.js:326:27)    at Object.next (http://localhost:8080/face-api.js:307:57)    at http://localhost:8080/face-api.js:300:75    at new Promise (<anonymous>)    at __awaiter$1 (http://localhost:8080/face-api.js:296:16)    at extractFaces (http://localhost:8080/face-api.js:2110:16)    at ComputeAllFaceDescriptorsTask.<anonymous> (http://localhost:8080/face-api.js:5318:54)```,"['Check the rectangles that you are passing to extractFaces; seems they are not correct.====='; 'I am passing them directly from     let fullFaceDescriptions = await faceapi.detectAllFaces(                    smallCanvas;                    new faceapi.SsdMobilenetv1Options({maxResults: 100})                ).withFaceLandmarks().withFaceDescriptors();Is it possible that `faceapi.detectAllFaces()` is returning a bad rectangle?====='; 'Hmm that shouldnt be the case. Can you log the rectangle dimensions and your Image dimensions. Also could provide a repo for me to reproduce the issue?Edit.: Sorry just saw in the stacktrace that its occuring inside the task chain. Could you first of all share the image for which this issue occurs?====='; ""I can't share it publicly; sorry; don't know all the people in the picture.  But if I can get it to repro on a picture that I have full rights to; will do so.=====""; 'Closing; if this is still an issue feel free to reopen and share some information about how to reproduce this.====='; ""It's a Cross-domain issue with WebGL textures.=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,setting cors,Modifying model file path/extension,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/105,WebGL Warning when using this API,2,closed,2018-10-17T19:08:16Z,2018-10-23T17:46:24Z,"Hello there;I'm quite new to using Javascript and this API; and having read the tutorials and examples multiple times I'm having trouble getting this API to work. When running certain methods such as faceapi.tinyYolov2; the code fails and gives the following error:`Error: WebGL warning: texImage2D: Element is write-only; thus cannot be uploaded. (source: face-api.js: line 23: col 132152)`I also get this error (same location) when running the method faceapi.locateFaces (and possibly during some more methods).   This is my code:```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>Face Recognition Test</title>    <script src=""../../modulesExtern/face-api/dist/face-api.js""></script></head><body><div id=""container""><img id=""image"" src=""https://cdn-images-1.medium.com/max/1000/1*5XojINtKrkHFT3nOx6oW1g.jpeg""><canvas id=""myCanvas"" width=""200"" height=""100""></canvas><script type=module>    const MODEL_URL = '../../modulesExtern/face-api/weights';    let scoreThreshold = 0.5;    let sizeType = 'lg';    async function main() {        await faceapi.loadTinyYolov2Model(MODEL_URL);        const inputImgEl = document.getElementById(""image"");        const { width; height } = inputImgEl;        const canvas = document.getElementById(""myCanvas"");        canvas.width = width;        canvas.height = height;        const forwardParams = {            inputSize: sizeType;            scoreThreshold        };        const detections = await faceapi.tinyYolov2(inputImgEl; forwardParams)        // faceapi.drawDetection('myCanvas'; detections.map(det => det.forSize(width; height)))    }    main();</script></div></body></html>```Does anyone know what I'm doing wrong and/or how to solve it?","['`<img id=""image"" src=""https://cdn-images-1.medium.com/max/1000/1*5XojINtKrkHFT3nOx6oW1g.jpeg"">`You\'re trying to load an image from an external source; which is a security problem. The way I got around that was to convert the remote image to a data uri and use that as the src of the image tag.====='; 'This indeed was the issue. Thank you very much for your help! :) =====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,setting cors,Modifying model file path/extension,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/45,External image sources,10,closed,2018-07-10T08:46:38Z,2021-07-30T14:05:43Z,"Hi;First of all thank you for your work; it's excellent and very simple to use.I have a small problem; when I use the face recognition; the api doesn't seem to work with external image sources. I would like to work with a data base and I'm receiving this error message : ""Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.""Do you know how can I overcome this problem ?","['Hmm; can you show some example code of what you are doing? The error message indicates that whatever you are trying to pass into the net is either not a tensor or a valid media element or the media content is not loaded yet.====='; 'html : ```<div class=""footballer_images_container"">     <img class=""it_will_work_for_this_image"" src=""public/amy1.png"" alt="""">     <img class=""it_will_not_work_for_this_image"" src=""https://upload.wikimedia.org/wikipedia/commons/e/e4/Karim_Benzema_2018.jpg"" alt="""">   </div>```js`     const $imgToCompare = footballerImageContainer.querySelectorAll(""img"")`        //compare image    const compareImage = async ()=>{        //load models        await faceapi.loadModels(MODEL_URL)        //vector the custom image         const descriptor1 = await faceapi.computeFaceDescriptor($customImage)               //compare all the images to the custom image        $imgToCompare.forEach(img => {            //compare both image            faceSimilarities(img; descriptor1)          });    }    //compare faces    const faceSimilarities = async (image; descriptor1)=>{        //vector the images to compare        const descriptor2 = await faceapi.computeFaceDescriptor(image)        //compare the ressemblance of the faces        const distance = faceapi.euclideanDistance(descriptor1; descriptor2)        //transform the distance in %        const similarityPercentage = Math.round(((1 - distance) * 100) * 100) / 100         // console.log(similarityPercentage + ""%"")         //put the value in the array        imgScore.push(similarityPercentage)                //if it\'s the last image        if($imgToCompare[$imgToCompare.length - 1] === image){            compareScore()                    }    }So this will calculate the euclidian distance between a custom image; and all the image in the ""footballer_images_container"" div. When it\'s finished; it will pick the image with the most similarities with the function compareScore(). The problem is it will work for ""public/amy1.png"" because the source is in local but not for ""https://upload.wikimedia.org/wikipedia/commons/e/e4/Karim_Benzema_2018.jpg"" because the source is external.I hope i\'m clear.====='; 'You may get blocked by the browser because of the CORS on the external site====='; 'Yeah @seranus is right; that simply wont work. Look at the example how I implemented external images fetching; I proxy the request via an express server====='; 'Ok thank you !====='; 'Facing similar problem for External Video sources.any idea?====='; '@sathyamoorthyrr have you found a solution for videos? ====='; ""@lpsBetty have you found solution; any idea? I'm have same problem=====""; '> @sathyamoorthyrr have you found a solution for videos?@lpsBetty I think I got settled with an internal video; as per this; https://github.com/sathyarr/video-face-emotion-detection-analysis/blob/master/README.md#how-to-runI have lost the context altogether. been so long worked on this.====='; '@IgorGomesFATEC yes it worked with the ""newer"" npm package: https://www.npmjs.com/package/@vladmandic/face-api=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,use proxy,use proxy,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4"",
    ""specific_type"": ""A.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2""
  }
}
```",A.3,C.1
https://github.com/infinitered/nsfwjs/issues/375,Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.,2,closed,2020-07-20T02:50:40Z,2020-07-24T00:01:29Z,"Package works for me in development; but not on a production server. This is the error message I got and I don't know much more about it than what it says.`Failed to execute 'texImage2D' on 'WebGL2RenderingContext': Tainted canvases may not be loaded.`Here is my code. It is basically the simple example on the readme but with a couple of small changes:```<script src=""https://unpkg.com/@tensorflow/tfjs@1.2.8"" type=""text/javascript""></script><script src=""https://unpkg.com/nsfwjs@2.1.0"" type=""text/javascript""></script><script>    const img = new Image()    // image is hosted on the same domain as this page; so no crossDomain attribute is required    img.src = ""{{$url}}""    // Load the model.    nsfwjs.load(        'my/path/to/model/directory';        { size: 299 }    ).then(model => {        // Classify the image.        model.classify(img).then(predictions => {             // changed from console log to document write            document.write(JSON.stringify(predictions));        })    })</script><body></body>```","['It sounds to me like you might have CORS or some other cross origin protection in place.Try setting `img.crossOrigin = ""anonymous"";`====='; 'Looks like you were right on that one. I had too many problems with CORS settings; especially when handling images from unknown sites. So switching to a Node.js configuration made it a lot more predictable=====']",Fetch Failure,Crash,Data/Model Inaccessibility,,,parameter modifier,Modify API Parameter usage,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] WebGL Context Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",A.3,C.1
https://github.com/reiinakano/arbitrary-image-stylization-tfjs/issues/24,Larger images,1,closed,2019-05-05T07:37:36Z,2019-05-05T07:44:08Z,"I've tried to increase the size of the source image (changed index.html) to produce larger stylised images but I can't seem to get beyond 1000x1000.  I get an error: ""GL ERROR :GL_OUT_OF_MEMORY : glBufferData: cannot allocate more than 1GB"".  Any suggestions?","[""Your browser just ran out of memory. It *is* a resource-constrained environment after all. I don't think there's a solution for this other than using another implementation that does not run on the browser. I suggest the original Magenta implementation in Python https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization=====""]",Out of Mermory,Poor Performance,Misconfiguration,,,change input size,Replace data Shape/type,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B,B.1
https://github.com/justadudewhohacks/face-api.js/issues/534,Error GPU usage(CUDNN_STATUS_INTERNAL_ERROR),1,closed,2020-01-25T01:00:10Z,2020-01-25T02:09:05Z,"@tensorflow/tfjs-node-gpu: ""^1.4.0""""face-api.js"": ""0.22.0""Driver Version: 440.48.02    CUDA Version: 10.2GeForce GTX 1650Ubuntu 18.04I try run Win10 and have some problem` nodemon --exec ./node_modules/.bin/ts-node -- ./src/index.ts[nodemon] 1.19.4[nodemon] to restart at any time; enter `rs`[nodemon] watching dir(s): *.*[nodemon] watching extensions: ts;json[nodemon] starting `./node_modules/.bin/ts-node ./src/index.ts`2020-01-25 00:51:57.358207: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-01-25 00:51:57.388648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz2020-01-25 00:51:57.390138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ab0c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-01-25 00:51:57.390202: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host; Default Version2020-01-25 00:51:57.391997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.12020-01-25 00:51:57.811615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.811823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.695pciBusID: 0000:01:00.02020-01-25 00:51:57.811958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.02020-01-25 00:51:57.812750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.02020-01-25 00:51:57.813434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.02020-01-25 00:51:57.813587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.02020-01-25 00:51:57.814517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.02020-01-25 00:51:57.815262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.02020-01-25 00:51:57.817569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-25 00:51:57.817619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.817815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.817973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 02020-01-25 00:51:57.817988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.02020-01-25 00:51:57.856422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:2020-01-25 00:51:57.856459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      02020-01-25 00:51:57.856464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N2020-01-25 00:51:57.856537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.856729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.856907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1); but there must be at least one NUMA node; so returning NUMA node zero2020-01-25 00:51:57.857088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3548 MB memory) -> physical GPU (device: 0; name: GeForce GTX 1650; pci bus id: 0000:01:00.0; compute capability: 7.5)cpu backend was already registered. Reusing existing backend factory.Platform node has already been set. Overwriting the platform with [object Object].raven@2.6.4 alert: no DSN provided; error reporting disabledmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soonmtcnn is deprecated and will be removed soon2020-01-25 00:52:20.197202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.72020-01-25 00:52:20.917478: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR2020-01-25 00:52:20.921141: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR(node:5734) UnhandledPromiseRejectionWarning: Error: Invalid TF_Status: 2Message: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize; so try looking to see if a warning log message was printed above.    at NodeJSKernelBackend.executeSingleOutput (/home/user/aa/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:193:43)    at NodeJSKernelBackend.conv2d (/home/user/aa/node_modules/@tensorflow/tfjs-node-gpu/dist/nodejs_kernel_backend.js:744:21)    at engine_1.ENGINE.runKernelFunc.x (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/conv.ts:205:25)    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:586:31    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at kernelFunc (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:586:20)    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:599:23    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.runKernelFunc (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:596:10)    at conv2d_ (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/conv.ts:204:22)    at Object.conv2d (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/ops/operation.ts:45:24)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/pointwiseConvLayer.ts:12:18    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at Object.pointwiseConvLayer (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/pointwiseConvLayer.ts:10:13)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/mobileNetV1.ts:37:15    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at Object.mobileNetV1 (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/mobileNetV1.ts:34:13)    at /home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:35:24    at /home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:424:20    at Engine.scopedRun (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:435:19)    at Engine.tidy (/home/user/aa/node_modules/@tensorflow/tfjs-core/src/engine.ts:422:17)    at Object.tidy (/home/user/aa/node_modules/face-api.js/node_modules/@tensorflow/tfjs-core/src/globals.ts:182:17)    at SsdMobilenetv1.forwardInput (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:31:15)    at SsdMobilenetv1.<anonymous> (/home/user/aa/node_modules/face-api.js/src/ssdMobilenetv1/SsdMobilenetv1.ts:62:14)    at step (/home/user/aa/node_modules/tslib/tslib.js:136:27)    at Object.next (/home/user/aa/node_modules/tslib/tslib.js:117:57)    at fulfilled (/home/user/aa/node_modules/tslib/tslib.js:107:62)    at processTicksAndRejections (internal/process/task_queues.js:94:5)(node:5734) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block; or by rejecting a promise which was not handled with .catch(). (rejection id: 1)(node:5734) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future; promise rejections that are not handled will terminate the Node.js process with a non-zero exit code`",['Helped `export TF_FORCE_GPU_ALLOW_GROWTH=true` ====='],Build & Install Failure,Build & Initialization Failure,Misconfiguration,,,change env flag,Modifying the value of environment variable,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.5] Training Argument Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,B.1
https://github.com/justadudewhohacks/face-api.js/issues/148,CreateCanvas crashing in react js,3,closed,2018-11-26T05:29:50Z,2019-01-17T09:21:28Z,Uncaught (in promise) TypeError: Illegal constructor    at eval (index.js:19)    at createCanvas (createCanvas.js:15)    at createCanvasFromMedia (createCanvas.js:26)    at eval (NetInput.js:43)    at Array.forEach (<anonymous>)    at new NetInput (NetInput.js:28)    at eval (toNetInput.js:59)    at step (tslib.es6.js:117)    at Object.eval [as next] (tslib.es6.js:98)    at fulfilled (tslib.es6.js:88),"['Could you share a repo for me to reproduce this?Also:- in which browser does this occur?- seems you are not using the latest version of face-api.js; which version are you using?====='; ""Am not sure that whether am using it correctly .I have reproduced it in [issue148](https://github.com/ashithrahul/issue148)when i add this import '@tensorflow/tfjs-node';am getting this errorUncaught TypeError: util_1.promisify is not a function    at Object.eval (file_system.js:42)    at eval (file_system.js:297)    at Object../node_modules/@tensorflow/tfjs-node/dist/io/file_system.js (index_bundle.js:877)    at __webpack_require__ (index_bundle.js:725)    at fn (index_bundle.js:102)    at eval (index.js:5)    at Object../node_modules/@tensorflow/tfjs-node/dist/index.js (index_bundle.js:865)    at __webpack_require__ (index_bundle.js:725)    at fn (index_bundle.js:102)    at eval (App.js:23)if remove this import  am getting above mentioned errorAm using chrome Version 70.0.3538.110 (Official Build) (64-bit)=====""; ""Oh I see. You are not supposed to use @tensorflow/tfjs-node nor node-canvas in a react application; since your app won't run in a nodejs environment; but in the browser.Remove those dependencies and the faceapi.env.monkeyPatch call from your App.js.=====""]",Data & Model Error,Crash,API Misuse,,,patch environment,Fix environment adaptability,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.2,A.3
https://github.com/tensorspace-team/tensorspace/issues/177,Failed to fetch local files when running examples,1,closed,2019-01-09T00:49:29Z,2019-01-18T12:10:33Z,- IssueSee screenshot below:![image](https://user-images.githubusercontent.com/25629006/50868376-11ee5480-1364-11e9-834c-d1b1d850a704.png)- Debugging:Tried to open this example from WebStorm and it works. But if I open it directly(File -> Open File) from the browser; the error above shows. So I am guessing fetch() api requires a static file server.,['TensorSpace must run in web environment. Directly open example files from browser will have errors.====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] Local File Fetch Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",A.3,C.1
https://github.com/justadudewhohacks/face-api.js/issues/608,Angular Universal - Platform browser has already been set. Overwriting the platform with [object Object].,4,open,2020-04-28T21:10:01Z,2020-11-27T07:37:14Z,```Platform browser has already been set. Overwriting the platform with [object Object].```This is happening to me lately in Angular Universal; the difference with angular that this executes processes at the server level; using Node.js; so how do I make it work; since I am overwriting the platform with this function:```e.setPlatform | @ | face-api.min.js:1-- | -- | --  | ./node_modules/@tensorflow/tfjs-core/dist/tf-core.esm.js | @ | tf-core.esm.js:17  | __webpack_require__ | @ | bootstrap:79  | ./node_modules/face-api.js/build/es6/index.js | @ | index.js:1  | __webpack_require__ | @ | bootstrap:79  | ./src/app/components/verificacion/verificacion-facial/verificacion-facial.component.ts | @ | verificacion-dactilar.component.ts:31  | __webpack_require__ | @ | bootstrap:79  | ./src/app/app-routing.module.ts | @ | app-config.ts:60  | __webpack_require__ | @ | bootstrap:79  | ./src/app/app.module.ts | @ | app.component.ts:10  | __webpack_require__ | @ | bootstrap:79  | ./src/main.ts | @ | environment.ts:18  | __webpack_require__ | @ | bootstrap:79  | 0 | @ | main.ts:14  | __webpack_require__ | @ | bootstrap:79  | checkDeferredModules | @ | bootstrap:45  | webpackJsonpCallback | @ | bootstrap:32  | (anonymous) | @ | main.js:1```,"['I had same trouble when i used tfjs-react-native package on my device. My app just had closes when i tried use something from this package. In console was only warning ""Platform browser has already been set. Overwriting the platform with [object Object]"".This was solved by setting backend before used tfjs-react-native. Maybe it will help you too.``` javascriptawait tf.setBackend(\'cpu\');```====='; '@fomin-max does face-api can be used in react-native ?====='; '@gravestar i think yes; you cani suggest you read tutorials from https://github.com/justadudewhohacks/face-api.js/and i found already ready solution on android with react-native https://github.com/aboozaid/react-native-facerecognitioni have similar working project with blazeface and posenet tensorflow-models https://github.com/fomin-max/react-native-tfjsin RealTimeDetection.tsx component you can see how models are appliedhope it will help for you!====='; '@fomin-max well; my goal is to get face recognition from this; from what i read; face-api store data from descriptors; while what im getting from blazeface / facedetector is X;Y;and im still confused about it; how to store the data and predict itim trying using KNN to store X;Y data; and predict but the result not makes me happy xD=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,use one package,Changing version,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Unknown Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,A.6
https://github.com/justadudewhohacks/face-api.js/issues/512,Error using GPU,1,open,2019-12-26T03:49:09Z,2020-01-28T10:58:37Z,Hi!When I using GPU I get this error:E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERRORI can found a solution; but don't know how to implement:https://github.com/tensorflow/tensorflow/issues/24496`I did try compiling from source; but ran into the same issue. I was finally able to fix my problem was setting config.gpu_options.allow_growth = True.`,['This error does not come from face-api.js; probably from tfjs-node-gpu?====='],Build & Install Failure,Build & Initialization Failure,Misconfiguration,,,change env flag,Modifying the value of environment variable,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.4] Browser & Device Error"",
    ""specific_type"": ""[A.4.1] GPU Initialization Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/5936,Use bodypix model on react native,2,closed,2021-12-11T17:05:22Z,2021-12-12T15:08:08Z,Hello. I am trying to use tensorflow bodypix model on my app on react native; I want to use it for segment a body person from a local image. Could anyone say me how can I do it? Thank you so much,['You can use movenet instead of bodypix ; please check out this demo https://github.com/tensorflow/tfjs-examples/tree/master/react-native/pose-detection here ; thank you====='; 'Thank you so much; I will try it====='],Reference Error,Crash,Improper Model Attribute,,,change model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.3""
  }
}
```",A.1,C.2
https://github.com/tensorflow/tfjs/issues/5843,Batch Normalization for 5D tensors (NxDxHxWxC) to handle 3D data (e.g.; medical images),2,open,2021-11-14T09:52:21Z,2021-11-15T16:59:09Z,"**System information**- TensorFlow.js version (you are using): 3.11.0- Are you willing to contribute it (Yes/No): No**Describe the feature and the current behavior/state.**Batch Normalization (BN) in 5D tensors (NxDxHxWxC) is needed to handle models dealing with single- or multi-channel gridded 3D data; like 3D CT or MR medical images.Currently BN is limited up to 4D tensors; and with 5D ones it fails with the following error in <a href=""https://github.com/tensorflow/tfjs/blob/bec2196c8861470f929a3afee74eef1f10a2258c/tfjs-layers/src/layers/normalization.ts#L66"">normalization.ts</a>:`Error: batchNormalization is not implemented for array of rank 5 yet`This is basically a re-open of the closed issue #2504; which said: > Ssupport for 5D tensors/3D ops has been added to several components recently (#767 #1035 #1985); so I am filing this hoping that someone with free time will consider adding 5D tensor support for BatchNorm too.**Will this change the current api? How?**This will **not** change the current API.**Who will benefit with this feature?**Anyone willing to use 3D tensors in their models (e.g.; medical imaging).**Any Other info.**At first vision of the source code; to me it wouldn't take too long to implement; but I can't do it right now because I have problems in rebuilding the whole repo.",['@dibenedetto do you need this support for training (layers API) or inference (Graph model execution)? ====='; '> @dibenedetto do you need this support for training (layers API) or inference (Graph model execution)?@pyu10055  at this time for inference.right now i bypassed the 5D BN by reshaping the tensor to 3D and then back to 5D; and this hack seems to work; but having a working 5D BN would avoid keeping track of tensor shapes in model definition.====='],Reference Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.0""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/5546,tf.fused.conv2d doesn't support 'NCHW' data format,1,open,2021-08-28T02:03:40Z,2021-08-28T12:37:06Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: N/A- TensorFlow.js installed from (npm or script link): https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.8.0- TensorFlow.js version (use command below): 3.8.0- Browser version:  Google Chrome  92.0.4515.159- Tensorflow.js Converter Version: N/A**Describe the current behavior**This issue could be reproduced by following code snippet```jslet x = tf.ones([1; 2; 5; 5]);  // `NCHW`let filter = = tf.ones([3; 3; 2; 3]); // `HWIO`let y = tf.fused.conv2d({x; filter; strides: 1; pad: 'same'; dataFormat: 'NCHW'});```The error will be thrown:```Uncaught Error: Error in conv2d: depth of input (5) must match input depth for filter 2.```**Describe the expected behavior**No error is thrown and the result is expected to be same as tf.conv2d.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.See above code snippet.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.See error message above.cc @pyu10055 . Thanks.,"[""Just my $0.02; almost none of TFJS ops support any other format than *NHWC* (which is defacto a standard for TF in general); not just `Conv2D`. And it's easy (and fast) to transpose tensor before using it. Even in TF itself (using Python bindings); very few ops support anything other than *NHWC* (although more than in TFJS).=====""]",Reference Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/5410,Argument 'tensors[0]' passed to 'concat' must be a Tensor or TensorLike; but got 'null',10,closed,2021-08-01T08:23:19Z,2021-09-20T13:25:15Z,"I am trying to predict the results from the converted python keras model into tensorflow js; I have tried a lot of possible combinations to get the answer but it keeps throwing this error. The input shape it expects is [null;7] which I am providing it but still not working :-( Any help would be appreciated. Below is model and it's summary.```model = Sequential()        model.add(Embedding(self.vocab_length; embed_dim; input_length=self.maxlen))model.add(Flatten())model.add(Dense(32;activation=""relu""))model.add(Dense(self.output_dim;activation=""softmax""))  # compile the model      model.compile(loss='categorical_crossentropy'; optimizer=tf.keras.optimizers.SGD(    learning_rate=0.01; momentum=0.0; nesterov=False; name='SGD'); metrics=['accuracy'])model.summary()Model: ""sequential_2""_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================embedding_2 (Embedding)      (None; 7; 20)             1320      _________________________________________________________________flatten_2 (Flatten)          (None; 140)               0         _________________________________________________________________dense_4 (Dense)              (None; 32)                4512      _________________________________________________________________dense_5 (Dense)              (None; 10)                330       =================================================================Total params: 6;162Trainable params: 6;162Non-trainable params: 0```","['In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks!  ====='; 'Hello @rthadur ! Thanks for replying to my issue. Please find this [repo](https://github.com/RJ-1998/react-chatbot-nlp) containing all the code and model.json file whch I am using.====='; 'I see this error while running the above code ![image](https://user-images.githubusercontent.com/43972606/128782258-c9949d81-e7f0-424c-bbbf-bfee74c970d2.png)can you please resolve this and also load the model from local folder like this `$( document ).ready(async function () {\t$(\'.progress-bar\').show();    console.log( ""Loading model..."" );    model = await tf.loadGraphModel(\'model/model.json\');    console.log( ""Model loaded."" );\t$(\'.progress-bar\').hide();});`====='; '@rthadur I am using a cors extension in my browser and turning CORS extension on solves this error. You can use this  [cors](https://addons.mozilla.org/en-US/firefox/addon/cors-everywhere/) extension in firefox====='; 'Same error please check below ![image](https://user-images.githubusercontent.com/43972606/128928195-213bc4b4-be8f-4c67-9d5a-eb5d9bc6b81e.png)====='; '@rthadur Please check the following steps:-1. In the parent directory run **npm install** it has libraries to run express server2. Make sure you are able to open http://localhost:4001/static/model.json3. Turn on cors.If still not working please find below snapshot of the main error in tensorflow ![Screenshot from 2021-08-11 22-31-11](https://user-images.githubusercontent.com/37022742/129071899-58421545-5945-4ffb-97fa-297aae1f6b1f.png)====='; '@rthadur ; @lina128  Hey Guys. Any updates on the issue?====='; ""Hi @RJ-1998 ; why don't you pass in model from loadLayersModel directly in this line? https://github.com/RJ-1998/react-chatbot-nlp/blob/bf8680b120452979c2c7ff450feb7549864a4070/chatbot/src/App.js#L15=====""; 'Thanks @lina128 it worked!! Actually I was following one article that suggested the other way. I wish I knew it before.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5410"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5410"">No</a>=====']",Data & Model Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.3
https://github.com/tensorflow/tfjs/issues/5349,Is there some way to surpress this warning message?,8,closed,2021-07-19T20:15:35Z,2021-07-23T15:17:36Z,<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:feature_template</em>**System information**- TensorFlow.js version (you are using): 3.8.0- Are you willing to contribute it (Yes/No): maybe but likely no**Describe the feature and the current behavior/state.**👋 from the VS Code teamWe use tfjs in VS Code and see this warning:![image](https://user-images.githubusercontent.com/2644648/126220844-e3eb21e8-85d8-4728-a9ec-9413d0ff2896.png)It would add a lot of extra complexity to include tfjs-node in VS Code so we'd like to not do what it's suggesting and instead would like to suppress this warning.Is this possible?**Will this change the current api? How?**No**Who will benefit with this feature?**People who don't want to use tfjs-node**Any Other info.**,"[""The 1st warning is also a little strange... it's something inside of tfjs causing that.=====""; ""@TylerLeonhardt what environment are you running in? seems both of the flags IS_BROWSER and IS_NODE are true.```export function isBrowser(): boolean {  return (typeof window !== 'undefined' && window.document != null) ||      //@ts-ignore      (typeof WorkerGlobalScope !== 'undefined');}/** Whether we are in a browser (as versus; say; node.js) environment. */ENV.registerFlag(    'IS_NODE';    () => (typeof process !== 'undefined') &&        (typeof process.versions !== 'undefined') &&        (typeof process.versions.node !== 'undefined'));```=====""; ""@pyu10055 within VS Code. My guess is that since it's an electron app... both return true.![image](https://user-images.githubusercontent.com/2644648/126249367-22179365-f7df-48bf-a612-40f7b4251ed8.png)You can test this by opening up the dev tools in VS Code:* Open the command palette and type: `Dev Tools`* then choose this one:![image](https://user-images.githubusercontent.com/2644648/126249326-bf9b28e1-7886-4d88-98f8-c4b5a34a717d.png)=====""; '@TylerLeonhardt in this case; you can just use webgl backend by not including tfjs-backend-cpu package.in you deps; you can add only tfjs-core; tfjs-backend-webgl; tfjs-converter packages instead of tfjs union package.Or if you prefer cpu backend; you can swap webgl dep with cpu in above list.====='; '@pyu10055 thanks for the pointer! I believe you said CPU is best in our use-case in the previous issue; so I\'m trying that. Here\'s what I tried:* Removing `@tensorflow/tfjs` from package.json* Adding these to package.json:  *  `""@tensorflow/tfjs-backend-cpu"": ""^3.8.0"";`  *  `""@tensorflow/tfjs-converter"": ""^3.8.0"";`  * `""@tensorflow/tfjs-core"": ""^3.8.0"";`* Changing this:```import { GraphModel; io; loadGraphModel; Rank; setBackend; tensor; Tensor } from \'@tensorflow/tfjs\';```to```import { Rank; tensor; Tensor; io; setBackend } from \'@tensorflow/tfjs-core\';import { GraphModel; loadGraphModel } from \'@tensorflow/tfjs-converter\';import \'@tensorflow/tf-backend-cpu\';```Unfortunately; this still yields the logs in the original screenshot.For your reference; the code in question is found here: https://github.com/Microsoft/vscode-languagedetectionand is then loaded into vscode. The test in the vscode-languagedetection repo shows the ""hi there 👋 "" log.====='; ""@TylerLeonhardt can you try to set `IS_NODE` to false before you load the model?```tf.env().set('IS_NODE'; false);```=====""; '@pyu10055 That gets rid of the ""hi there 👋 "" log; great!The last one that shows up is:```Platform browser has already been set. Overwriting the platform with [object Object].```which only shows up when running in VS Code... (in electron; I guess)... in my mocha tests in the vscode-languagedetection repo I do not see it.====='; 'Thanks for this @pyu10055! =====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,change env flag,Modifying the value of environment variable,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": ""[D.4.1] Warning Messages""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/4466,Failure to Convert Tensorflow Saved Model to model.json ValueError: cannot create an OBJECT array from memory buffer,11,open,2020-12-30T07:44:43Z,2021-03-08T21:50:00Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No - OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Mac OS Catalina- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: No- TensorFlow.js installed from (npm or script link): pip installed-  keras 2.2.4-tf- tensorflow 2.1.0- TensorFlow.js version (use command below): 2.8.1- Browser version: chrome 87.0.4280.88- Tensorflow.js Converter Version: 2.8.1**Issue**_**ValueError: cannot create an OBJECT array from memory buffer**_**Describe the current behavior**I have trained a model in Tensorflow and have saved the model. I want to convert the tf_saved_model to model.json Model is created using the following code:```num_steps_2=200classifier2 = tf.estimator.DNNClassifier(    feature_columns=create_feature_columns(input_features; feature_spec);    hidden_units=[128; 64; 32])classifier2.train(train_inpf; steps=num_steps_2)#saving the modelimport shutilinputFn = tf.estimator.export.build_parsing_serving_input_receiver_fn(  tf.feature_column.make_parse_example_spec(feature_columns_needed))OUTDIR = module_pathshutil.rmtree(OUTDIR; ignore_errors = True) # start fresh each timemodelBasePath = os.path.join(OUTDIR; ""model"")modelPath = classifier2.export_saved_model(module_path; inputFn)print(""model is saved at :""; modelPath)```I used the following command within my notebook:`!tensorflowjs_converter --input_format=tf_saved_model --skip_op_check --saved_model_tags=serve --signature_name=serving_default --strip_debug_ops=True --weight_shard_size_bytes=4194304  ""/Users/data/data/ckd/models/ckd_Dec-29-2020-00-00-00/1609297437/"" ""/Users/data/data/ckd/models/output/""`I get the following error:`2020-12-29 19:05:03.076396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2020-12-29 19:05:03.092798: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8ee0ce94d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-12-29 19:05:03.092818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host; Default VersionWARNING: Logging before flag parsing goes to stderr.W1229 19:05:03.280285 4582796736 deprecation.py:506] From /Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.W1229 19:05:04.476931 4582796736 meta_graph.py:436] Issue encountered when serializing global_step.Type is unsupported; or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.W1229 19:05:04.477355 4582796736 meta_graph.py:436] Issue encountered when serializing variables.Type is unsupported; or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.W1229 19:05:04.477463 4582796736 meta_graph.py:436] Issue encountered when serializing trainable_variables.Type is unsupported; or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.to_proto not supported in EAGER mode.2020-12-29 19:05:04.477655: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8; compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)2020-12-29 19:05:04.477732: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session2020-12-29 19:05:04.486826: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node dnn/input_from_feature_columns/input_layer/age_indicator_1/age_lookup/hash_table/table_init/LookupTableImportV2 doesn't exist in graphW1229 19:05:04.489723 4582796736 deprecation.py:323] From /Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:388: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.Instructions for updating:This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.W1229 19:05:04.827945 4582796736 deprecation.py:323] From /Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:393: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.convert_variables_to_constants`W1229 19:05:04.828099 4582796736 deprecation.py:323] From /Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.Instructions for updating:Use `tf.compat.v1.graph_util.extract_sub_graph`2020-12-29 19:05:05.153410: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize2020-12-29 19:05:05.153433: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   debug_stripper: debug_stripper did nothing. time = 0.022ms.2020-12-29 19:05:05.153437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 502 nodes (-8); 597 edges (-8); time = 6.245ms.2020-12-29 19:05:05.153441: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 498 nodes (-4); 593 edges (-4); time = 30.05ms.2020-12-29 19:05:05.153444: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (-226); 593 edges (0); time = 9.77ms.2020-12-29 19:05:05.153448: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 4.877ms.2020-12-29 19:05:05.153451: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0); 593 edges (0); time = 2.654ms.2020-12-29 19:05:05.153509: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0); 593 edges (0); time = 9.233ms.2020-12-29 19:05:05.153514: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 6.764ms.2020-12-29 19:05:05.153517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 4.599ms.2020-12-29 19:05:05.153520: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   debug_stripper: debug_stripper did nothing. time = 0.66ms.2020-12-29 19:05:05.153524: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0); 593 edges (0); time = 2.362ms.2020-12-29 19:05:05.153527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0); 593 edges (0); time = 9.562ms.2020-12-29 19:05:05.153707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 6.718ms.2020-12-29 19:05:05.153713: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 4.525ms.2020-12-29 19:05:05.153716: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   model_pruner: Graph size after: 272 nodes (0); 593 edges (0); time = 2.69ms.2020-12-29 19:05:05.153719: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 272 nodes (0); 593 edges (0); time = 9.015ms.2020-12-29 19:05:05.153723: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 6.161ms.2020-12-29 19:05:05.153869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 272 nodes (0); 593 edges (0); time = 4.296ms.2020-12-29 19:05:05.217058: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize2020-12-29 19:05:05.217086: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   remapper: Graph size after: 265 nodes (-7); 586 edges (-7); time = 2.129ms.2020-12-29 19:05:05.217091: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 265 nodes (0); 586 edges (0); time = 7.309ms.2020-12-29 19:05:05.217095: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 265 nodes (0); 586 edges (0); time = 5.159ms.2020-12-29 19:05:05.217099: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 265 nodes (0); 586 edges (0); time = 3.959ms.2020-12-29 19:05:05.217102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   remapper: Graph size after: 265 nodes (0); 586 edges (0); time = 2.478ms.2020-12-29 19:05:05.217106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 265 nodes (0); 586 edges (0); time = 8.296ms.2020-12-29 19:05:05.217193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   arithmetic_optimizer: Graph size after: 265 nodes (0); 586 edges (0); time = 6.055ms.2020-12-29 19:05:05.217205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   dependency_optimizer: Graph size after: 265 nodes (0); 586 edges (0); time = 4.521ms.Traceback (most recent call last):  File ""/Users/taposh/anaconda3/bin/tensorflowjs_converter""; line 8; in <module>    sys.exit(pip_main())  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 813; in pip_main    main([' '.join(sys.argv[1:])])  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 817; in main    convert(argv[0].split(' '))  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 804; in convert    weight_shard_size_bytes; metadata_map)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py""; line 533; in _dispatch_converter    metadata=metadata_map)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 652; in convert_tf_saved_model    metadata=metadata)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 210; in optimize_graph    initializer_graph_def; metadata=metadata)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 266; in extract_weights    global_manifest = extract_const_nodes(graph_def.node)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py""; line 231; in extract_const_nodes    'data': graph_rewrite_util.values_from_const(const)  File ""/Users/taposh/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/graph_rewrite_util.py""; line 62; in values_from_const    tensor_value = tensor_util.MakeNdarray(input_tensor)  File ""/Users/taposh/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py""; line 598; in MakeNdarray    dtype=dtype).copy().reshape(shape))ValueError: cannot create an OBJECT array from memory buffer`Added this on StackOverflow for more eyes:https://stackoverflow.com/questions/65501730/tensorflowjs-converter-giving-valueerror-cannot-create-an-object-array-from-mem**Describe the expected behavior**expected model.json and weight files to be created in output location","['Any work around for this? ====='; '@taposh Thank you for reporting this issue. It would be very helpful if you can provide the full code to allow us to reproduce the problem. If you can provide a colab or repl.it example would be great. thanks.====='; '@pyu10055  here is my code in colab https://colab.research.google.com/drive/1cPmtlV7LMGAJ16WHBEeRko37zBNwWlda?usp=sharing====='; '@pyu10055  were you able to review this?====='; '@pyu10055  @rthadur any update on this bug !====='; 'Thank you for sharing the colab; I think the input dataset is missing:```---------------------------------------------------------------------------FileNotFoundError                         Traceback (most recent call last)<ipython-input-10-6f3b48cf182c> in <module>()      6 csv_path = \'sample_data/train.csv\'      7 ----> 8 data = pd.read_csv(csv_path)      9 # Set the column in the dataset you wish for the model to predict     10 label_column = \'class\'4 frames/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self; src; **kwds)   2008         kwds[""usecols""] = self.usecols   2009 -> 2010         self._reader = parsers.TextReader(src; **kwds)   2011         self.unnamed_cols = self._reader.unnamed_cols   2012 pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source()FileNotFoundError: [Errno 2] No such file or directory: \'sample_data/train.csv\'```====='; 'The uploaded file gets removed on Colab when runtime is recycled. I will add code to read from my drive. ====='; '@pyu10055  code in colab is updated to read file directly from github. You will need to pass to tensorflow converter the file name manually since its on command/shell as opposed to in python.====='; '@pyu10055 any update====='; '@pyu10055  any update?====='; '@taposh Sorry for the delay; I took a close look at your colab; I think the problem is caused by use tf.example as the input of the inference graph; which is not supported by TFJS right now. You need to create a signature that has input with the node after the tf.example has been processed. Here are some example code that might help you to create custom input function:```def _tfjs_serving_input_fn(feature_spec):  """"""Creates an input function for TFJS-node serving.""""""  def raw_transforming_serving_input_fn():    placeholder_features = {}    for name; value in feature_spec.items():      batch_dim = tf.TensorShape([None])      placeholder_features[name] = tf.compat.v1.placeholder(          dtype=value.dtype;          shape=batch_dim.concatenate(value.shape);          name=name)    return tf.estimator.export.ServingInputReceiver(placeholder_features;                                                    placeholder_features)  return raw_transforming_serving_input_fnexport_path = classifier.export_saved_model(""trained-model""; _tfjs_serving_input_fn(tf.feature_column.make_parse_example_spec(my_feature_columns)))```=====']",Reference Error,Crash,Improper Model Attribute,,,change data preprocess,Add data processing,web application,Model Conversion,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.2] Data & Model Error"",
    ""specific_type"": ""[A.2.1] Tensor Shape/Type/Value Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[C] Data/Model Error"",
    ""subcategory"": ""[C.2] Improper Model/Tensor Attribute""
  }
}
```",A.1,C.2
https://github.com/tensorflow/tfjs/issues/4437,AutoML Object Detection: Argument 'x' passed to 'unstack' must be numeric tensor; but got string tensor,4,closed,2020-12-22T01:35:29Z,2020-12-29T02:22:12Z,"With the latest version of AutoML object detection on edge; we are getting this error using the out-of-box object detection model.```tfjs:17 Uncaught (in promise) Error: Argument 'x' passed to 'unstack' must be numeric tensor; but got string tensor    at Qg (tfjs:17)    at $g (tfjs:17)    at unstack_ (tfjs:17)    at unstack__op (tfjs:17)    at tfjs:17    at tfjs:17    at e.t.scopedRun (tfjs:17)    at e.t.tidy (tfjs:17)    at lx (tfjs:17)    at e.<anonymous> (tfjs:17)```Model here: [model.zip](https://github.com/tensorflow/tfjs/files/5727267/model.zip)Below is the html code:```<script src=""https://unpkg.com/@tensorflow/tfjs""></script><script src=""https://unpkg.com/@tensorflow/tfjs-automl""></script><img id=""img"" src=""test/test.jpg""><script>    async function run() {        const model = await tf.automl.loadObjectDetection('/model/model.json');        const img = document.getElementById('img');        const options = { score: 0.5; iou: 0.5; topk: 20 };        console.log(model.dictionary);        const predictions = await model.detect(img; options);        console.log(predictions);        // Show the resulting object on the page.        const pre = document.createElement('pre');        pre.textContent = JSON.stringify(predictions; null; 2);        document.body.append(pre);    }    run();</script>```Library Versions```""@tensorflow/tfjs"": ""^2.8.1"";""@tensorflow/tfjs-automl"": ""^1.0.0"";```","['@tafsiri @lina128 ====='; ""Hi @ghafran; I've submitted a PR to fix this. It will be released next week in 2.8.2. For context; the reason this error shows up  in the latest library is that; we started supporting hashtables in models. In this particular model; the hashtable is the label dictionary with string values; so we also need to add string support to underlying ops.=====""; '@lina128 Thanks for the update. Looking forward to the fix. Would be good to automate the QA for automl to ensure models work with latest tensorflowjs. Happy to help with that.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4437"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4437"">No</a>=====']",Data & Model Error,Crash,Improper Model Attribute,,,add support for datatype,Add unsupported operator,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.2,C.2
https://github.com/tensorflow/tfjs/issues/5941,Multi output model history validation accuracy problem.,3,closed,2021-12-14T14:37:45Z,2021-12-16T22:31:42Z,"**System information**- Have I written custom code: Yes- OS Platform and Distribution: Windows 10;  and whatever codesandox.io runs on- TensorFlow.js installed from (npm or script link): npm @tensorflow/tfjs-node@3.12.0 and @tensorflow/tfjs-node-gpu@3.12.0**Describe the current behavior**`fitDataset` of `tf.model` with multiple (3) outputs show invalid history entry for validation accuracy of 3rd output; on both `tfjs-node` and `tfjs-node-gpu`.Also there's an type error when trying to pass output layers into array of model outputs.**Describe the expected behavior**Show proper metrics for outputs **Standalone code to reproduce the issue**code to reproduce ```import {  layers;  train;  model;  SymbolicTensor;  tensor2d;  tensor1d;  data} from ""@tensorflow/tfjs-node"";const { flatten; dense; input; conv1d; maxPooling1d } = layers;(async () => {  const inputs = input({    shape: [8; 4];    name: ""input331""  });  const conv1D0 = conv1d({ filters: 32; kernelSize: 4 }).apply(inputs);  const maxPooling1D0 = maxPooling1d({ poolSize: 4; padding: ""valid"" }).apply(    conv1D0  );  const flatten0 = flatten().apply(maxPooling1D0);  const dense00 = dense({ units: 512; activation: ""relu"" }).apply(flatten0);  const dense10 = dense({ units: 512; activation: ""relu"" }).apply(flatten0);  const dense20 = dense({ units: 512; activation: ""relu"" }).apply(flatten0);  const dense01 = dense({ units: 256; activation: ""relu"" }).apply(dense00);  const dense11 = dense({ units: 256; activation: ""relu"" }).apply(dense10);  const dense21 = dense({ units: 256; activation: ""relu"" }).apply(dense20);  const y1 = dense({ units: 1; name: ""y1"" }).apply(dense01);  const y2 = dense({ units: 1; name: ""y2"" }).apply(dense11);  const y3 = dense({ units: 1; name: ""y3"" }).apply(dense21);  let tfModel = model({    inputs: inputs;    // @ts-ignore here is also a bug with types...    outputs: [y1; y2; y3];    name: ""1""  });  tfModel.compile({    optimizer: train.adam(0.0001);    loss: {      y1: ""meanSquaredError"";      y2: ""meanSquaredError"";      y3: ""meanSquaredError""    };    metrics: {      y1: ""accuracy"";      y2: ""accuracy"";      y3: ""accuracy""    }  });  tfModel.summary();  const array = [    [2; 3; 1; 2];    [2; 4; 2; 3];    [3; 5; 2; 4];    [4; 5; 3; 3];    [3; 6; 3; 5];    [5; 7; 4; 5];    [5; 7; 4; 6];    [6; 7; 3; 4]  ];  const tensor0 = tensor2d(array).expandDims(0);  const tensor1 = tensor2d(array.reverse()).expandDims(0);  const labels0 = [tensor1d([0.01]); tensor1d([0.02]); tensor1d([0.03])];  const labels1 = [tensor1d([0.1]); tensor1d([0.09]); tensor1d([0.08])];  const ds1 = data.array([tensor0; tensor1]);  const ds2 = data.array([labels0; labels1]);  const ds3 = data.zip([ds1; ds2]);  const dataSet = ds3.map((x) => {    return { xs: x[0]; ys: x[1] };  });  const history = await tfModel.fitDataset(dataSet; {    epochs: 20;    verbose: 0;    validationData: dataSet  });  console.log(history);  const predictions = await tfModel.predict(tensor1);  // @ts-ignore  predictions.forEach((tensor) => {    tensor.print();  });})();```https://codesandbox.io/s/typescript-node-tfjs-dunsw?file=/src/index.ts:2214-2224**Other info / logs** Example output from codesanbox.io; see `val_y3_acc` as it somehow doesn't show correct value; but the actual predictions are good.```History {  validationData: null;  params: {    epochs: 15;    initialEpoch: null;    samples: null;    steps: 2;    batchSize: null;    verbose: 0;    doValidation: true;    metrics: [      'loss';        'y1_loss';      'y2_loss';     'y3_loss';      'y1_acc';      'y2_acc';      'y3_acc';      'val_loss';      'val_y1_loss'; 'val_y2_loss';      'val_y3_loss'; 'val_y1_acc';      'val_y2_acc';  'val_y3_acc'    ]  };  epoch: [     0;  1;  2; 3;  4;  5;     6;  7;  8; 9; 10; 11;    12; 13; 14  ];  history: {    val_loss: [      0.06740279495716095;[... omitted for readability];      0.00021780715906061232    ];    val_y1_loss: [      0.08847129344940186;[... omitted for readability];      0.0015886977780610323    ];    val_y2_loss: [      0.10580939054489136;[... omitted for readability];      0.003424264956265688    ];    val_y3_loss: [      0.06740279495716095;[... omitted for readability];      0.00021780715906061232    ];    val_y1_acc: [      0.021068500354886055;      0.010667561553418636;      0.02188786491751671;      0.022847648710012436;      0.013655520044267178;      0.004707253538072109;      0.0018774932250380516;      0.0036717751063406467;      0.005106504075229168;      0.0037899017333984375;      0.0013052645372226834;      0.000026645591788110323;      0.0004705819010268897;      0.001332651823759079;      0.0013708906481042504    ];    val_y2_acc: [      0.01733810268342495;      0.0026979746762663126;      0.016361301764845848;      0.020113449543714523;      0.01138104498386383;      0.002396896481513977;      0.0001758798462105915;      0.003242085687816143;      0.005758211947977543;      0.004670063033699989;      0.0017483194824308157;      0.00013128435239195824;      0.0006708669243380427;      0.001789348665624857;      0.0018355674110352993    ];    val_y3_acc: [      0; 0; 0; 0; 0; 0;      0; 0; 0; 0; 0; 0;      0; 0; 0    ];    loss: [      0.34463465213775635;[... omitted for readability];      0.003640228882431984    ];    y1_loss: [      0.1969234198331833;[... omitted for readability];      0.00024854816729202867    ];    y2_loss: [      0.07084406167268753;[... omitted for readability];;      0.0014955892693251371    ];    y3_loss: [      0.07686717063188553;[... omitted for readability];      0.0018960912711918354    ];    y1_acc: [      0; 0; 0; 0; 0; 0;      0; 0; 0; 0; 0; 0;      0; 0; 0    ];    y2_acc: [      0; 0; 0; 0; 0; 0;      0; 0; 0; 0; 0; 0;      0; 0; 0    ];    y3_acc: [      0; 0; 0; 0; 0; 0;      0; 0; 0; 0; 0; 0;      0; 0; 0    ]  }}```","['@tuturis The `acc` metric counts how many time the output is the same as label throughout the training; it targets classification (with fixed label). In your case your output value is float;  the training results will never be exactly the same as the labels. So you will see `0` values in the metric.You can try other metrics listed [here](https://js.tensorflow.org/api/latest/#Metrics) ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5941"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5941"">No</a>====='; ""> @tuturis The `acc` metric counts how many time the output is the same as label throughout the training; it targets classification (with fixed label). In your case your output value is float;  the training results will never be exactly the same as the labels. > So you will see `0` values in the metric.> You can try other metrics listed [here](https://js.tensorflow.org/api/latest/#Metrics) Thanks for the reply; noticed it was an regression problem and accuracy only partially makes sense for it. But I think there's still a bug because some of the accuracy metrics still report some values and only the third one is faulty. I would still consider this a bug.=====""]",Incorrect Functionality,Incorrect Functionality,API Misuse,,,change data,change data,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.3
https://github.com/tensorflow/tfjs/issues/5891,[Codelab]: TensorFlowJS Comment Spam Detection,1,closed,2021-11-24T19:36:06Z,2021-11-29T21:31:38Z,At the end of step 9; when I'm supposed to see 2 numbers printed in the console; I don't see any numbers. I do see the comment I post in the console. (I'm using Glitch)There are however 2 errors in the console. > 20:28:04.953 cdn.glitch.me/group1-shard1of1.bin?v=1637779856621:1 Failed to load resource: the server responded with a status of 404 (Not Found)> > 20:28:04.962 io_utils.js:169 Uncaught (in promise) RangeError: byte length of Float32Array should be a multiple of 4>     at new Float32Array (<anonymous>)>     at yw (io_utils.js:169)>     at oT (models.js:334)>     at models.js:316>     at u (runtime.js:45)>     at Generator._invoke (runtime.js:274)>     at Generator.forEach.t.<computed> [as next] (runtime.js:97)>     at Wm (runtime.js:728)>     at o (runtime.js:728)>,['It seems your reference to the binary file is incorrect - you have a 404. Please double check the notes in the codelab which explain how to change the model.json file so it works on Glitch CDN - my guess is that this is why you are getting a 404.Instructions I am referring to are here: https://developers.google.com/codelabs/tensorflowjs-comment-spam-detection#7If you are still having issues after that; please check difference between my working version which can be found here:https://glitch.com/edit/#!/comment-spam-detection-complete====='],Fetch Failure,Crash,Data/Model Inaccessibility,,,change model patch,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.3] Inf/None/Null Results"",
    ""specific_type"": ""[D.3.1] None Results""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",A.3,C.1
https://github.com/tensorflow/tfjs/issues/5876,Processing locally stored image for custom object detection in tensorflow/tfjs-react-native,12,closed,2021-11-22T15:37:33Z,2021-12-16T16:46:24Z,"I am building custom object detection on locally stored images into a React Native app using Tensorflow.js and would be **grateful for help processing the image to a tensor before I call model.predict()**. My code (shown below) to access a local image; process it to a tensor; and perform inference has worked nicely for a custom _classification_ model (MobileNetV2). However; I am now getting a dtype error when I apply the same code to my custom _object detection_ model (SSD MobileNet V2 FPNLite 640x640).> Error: The dtype of dict['input_tensor'] provided in model.execute(dict) must be int32; but was float32```useEffect(() => {    (async () => {      await tf.ready();      const modelJson = require(""../assets/VisModels/model_m.json"")      const modelWeight = require(""../assets/VisModels/model_m_weights.bin"")      const model = await tf.loadLayersModel(bundleResourceIO(modelJson;modelWeight)) //for classification      //const model = await tf.loadGraphModel(bundleResourceIO(modelJson;modelWeight)) //for object detection      setModel(model)    })();}; []);const fileUri = **uri of locally stored jpeg image**       const imgB64 = await FileSystem.readAsStringAsync(fileUri; { encoding: FileSystem.EncodingType.Base64; });const imgBuffer = tf.util.encodeString(imgB64; 'base64').buffer;const imageData = new Uint8Array(imgBuffer);const IMGSIZE = 640;const imageTensor = decodeJpeg(imageData).expandDims().resizeBilinear([IMGSIZE;IMGSIZE]).div(tf.scalar(255)).reshape([1;IMGSIZE;IMGSIZE;3])const prediction = await model.predict(imageTensor).data()```Changing the tensor to int32 produces this error: > Error: This execution contains the node 'StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow/Where'; which has the dynamic op 'Where'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow/Reshape]]Using model.executeAsync() produces this error:> TypeError: model.executeAsync(imageTensor).data is not a function.This is a managed expo (v43.02) workflow with react-native v0.64.3; @tensorflow/tfjs v3.11; and @tensorflow/tfjs-react-native v0.8.0. **Any help in properly setting up the imageTensor would be greatly appreciated!**","['Is there a more appropriate place to ask this question? Thanks!====='; 'Hi @brianspiesman; sorry for the delay.. I have been busy with other projects. `executeAsync` returns a `Promise` of `Tensor` (if input is a single tensor); so you probably need to do the following:```jsconst result = await model.executeAsync(imageTensor);const prediction = result.dataSync();```Please give it a try. Thanks! ====='; ""@jinjingforever thank you so much for your suggestion. However; I am back to the initial error: > Unhandled promise rejection: Error: The dtype of dict['input_tensor'] provided in model.execute(dict) must be int32; but was float32I am processing the raw image for the object detection model in the same way as for my classification model. Should the images be processed differently depending on whether they will be used for object detection or classification? Thank you for any further suggestions.=====""; 'Good question. Is it possible to share your object detection model? I can take a closer look. Thanks!====='; 'Sure; what is the best way to get it to you? The model is about 450kb and the weights are about 12mb.====='; 'Maybe a github repo (along with your code would be great)? Thank you====='; '@jinjingforever: Ah yes; of course. [Here is a link to a repository with code for a test version of the app.](https://github.com/brianspiesman/objectDetection) The model is in the assets/VisModels directory and the code for importing the OD model and running inference is in components/ImagePicker.js. Thank you for your patience as I am new to using github.====='; 'Hi @brianspiesman;I took a look at the model and it is indeed has [int input](https://user-images.githubusercontent.com/8752427/144651811-b8c099cb-d077-4332-89db-73380086c04b.png). (I am using [netron](https://github.com/lutzroeder/netron) to visualize and inspect the model). I am not super familiar with the training and model conversion process; but our own [coco-ssd model](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd) also has int input.So to make your code work; you can simply add ""toInt"" to convert the input tensor to int type:```jsconst imageTensor = decodeJpeg(imageData).expandDims().resizeBilinear([IMGSIZE;IMGSIZE])    .div(tf.scalar(255)).reshape([1;IMGSIZE;IMGSIZE;3])    .toInt(); // <----```Also; the return value of `model.executeAsync` in this case will be an array of `tf.Tensor` because the model has multiple output tensors. You will need to process each one individually. Something like: ```jsconst result = await model.executeAsync(imageTensor);console.log(result[0].dataSync(); result[1].dataSync());```You can take a look at how our model processes the result [here](https://github.com/tensorflow/tfjs-models/blob/master/coco-ssd/src/index.ts#L125-L133). Our model probably has different outputs from yours; due to some [extra optimization steps](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd#technical-details-for-advanced-users).Thank you! Let me know if you have questions.====='; '@jinjingforever I think this is working. Thank you! However; as you say; I receive an output of 8 tensors and I am not sure which ones to use. I would like extract the prediction value; the box coordinates; and the class (however; there is only 1 class in this OD model). Here is an example of the output:[Output array of 8 tensors](https://github.com/brianspiesman/objectDetection/blob/main/Output%20array)[tensor0](https://github.com/brianspiesman/objectDetection/blob/main/Tensor0) shape: [1; 100][tensor1](https://github.com/brianspiesman/objectDetection/blob/main/Tensor1) shape: [1; 100; 4][tensor2](https://github.com/brianspiesman/objectDetection/blob/main/Tensor2) shape: [1; 100][tensor3](https://github.com/brianspiesman/objectDetection/blob/main/Tensor3) shape: [1; 100; 2][tensor4](https://github.com/brianspiesman/objectDetection/blob/main/Tensor4) shape: [1; 100][tensor5](https://github.com/brianspiesman/objectDetection/blob/main/Tensor5) shape: [1][tensor6](https://github.com/brianspiesman/objectDetection/blob/main/Tensor6) shape: [1; 51150; 4][tensor7](https://github.com/brianspiesman/objectDetection/blob/main/Tensor7) shape: [1; 51150; 2]In your link above; I see your model processing results in 2 tensors: [0] the scores and [1] the boxes. Can you tell which of the tensors in my output correspond to scores and boxes? Thank you for any further guidance.====='; ""Hi @brianspiesman:I think one way to do it is to look at the doc of the original model you converted this tfjs model from. For example; this TF object detection [model](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1). If you scroll to the bottom; you can see what different output tensors mean. If you don't know the original model; I think the model I linked might give you some ideas. Here is a [full list of object detection models](https://tfhub.dev/tensorflow/collections/object_detection/1) I found on tfhub.In this case; I think tensor1 is probably the bounding box. One of the tensor0; tensor2; and tensor4 is probably the class index. Please give them a try.Thanks!   =====""; '@jinjingforever this is perfect! Thank you for all your help on this. It is very much appreciated.====='; 'Closing this ; please @mention to reopen if issue still persists. Thank you =====']",Data & Model Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.3
https://github.com/tensorflow/tfjs/issues/5856,results of `nonMaxSuppression` are incorrect for non-clamped boxes,2,closed,2021-11-16T14:43:12Z,2021-11-16T15:54:30Z,"some models return non-clamped outputs (example is very popular `blazeface` detector) and in that case; running `nonMaxSuppression` will produce incorrect results *if* box coordinates are bellow 0.below are example input image and two boxes that are returned as over `iouThreshold`  ![in](https://user-images.githubusercontent.com/57876960/142005818-9d62f380-5fa2-43d1-8bdf-aa64e05dc3ca.jpg)![out](https://user-images.githubusercontent.com/57876960/142005826-a289e6bf-86f1-421a-940d-f2e82baa6dd4.jpg)reducing `iouThreshold` to any value does not help as `nonMaxSuppression` calculates it as `0` while its clear that first box is almost completely overlapped with second boxcapturing values inside `tfjs-core/src/backends/non_max_suppression_impl.ts:intersectOverUnion`:```json{  ""iCoord"": { ""0"": 115.93872833251953; ""1"": 42.05706787109375; ""2"": 172.44512939453125; ""3"": 98.56344604492188 };  ""jCoord"": { ""0"": -25.357688903808594; ""1"": -12.717769622802734; ""2"": 97.00607299804688; ""3"": 109.66532897949219 };  ""areaI"": 3192.9720676520374;  ""areaJ"": 14975.256338182517;  ""intersectionYmin"": 115.93872833251953;  ""intersectionYmax"": 97.00607299804688;  ""intersectionXmin"": 42.05706787109375;  ""intersectionXmax"": 98.56344604492188;  ""intersectionArea"": 0}```shows that `intersectionArea` is incorrectly calculatedenvironment: tfjs 3.11backend or os are not relevant since `nms` implementation is in `tfjs-core`","['ignore; my fault for incorrect box transformation before nms.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5856"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5856"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data postprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5751,console.warn Platform browser has already been set. Overwriting the platform with [object Object].,10,closed,2021-10-20T08:13:39Z,2021-10-21T10:59:08Z,I am receiving this warning multiple times during my tests. I am using React `17.0.2` with jest and react-testing-library `11.2.5`. I was using `@tensorflow/tfjs` version `3.2.0` and I upgraded to `3.9.0` because I saw there is a way to suppress this error but still I get this in the console multiple times![Screenshot from 2021-10-20 11-10-54](https://user-images.githubusercontent.com/14216599/138054116-47c2bd9b-8443-46b7-9368-c350709bc7d2.png)I tried using this command `tf.env().set('PROD'; true);` as described here but there are no instructions where to place it or use it. Should I use it inside `beforeEach`; `afterEach`. It would be nice to have this in the documentation.I tried using it on `setupTests.js` and at each test file individually but with no luck. Could you please advice where to use the command to suppress this error message?Thanks in advance.,"[""> I was using @tensorflow/tfjs version 3.2.0 and I upgraded to 3.9.0 because I saw there is a way to suppress this error but still I get this in the console multiple timesYou don't want to suppress the warning; you want to fix it - and the core issue is that you're importing `tfjs` multiple times.TFJS should only be imported once and on import it registers its backends and runtime environment. So if you import `tfjs` again; it will throw a warning that it's already been registered.=====""; 'Thanks a lot for the answer. The project I am working has some legacy code which I have not developed and I am not familiar with this package so I did not know that detail. Right now the import takes place in multiple React components. If I have to import the library only once how can I use it in multiple places (as this is the case right now) ? Or this is something that is not recommended?====='; ""there are many ways to achieve that - simplest is to place tfjs on a global object after initial import:```jsimport * as tf from ...window['tf'] = tf;```i don't like using global namespace; but it is simplest - so any component can now access tf as a global namespace.=====""; 'I agree that is the reason I asked. I think ""polluting"" the window object is not the way to go but if there is no other way I will have to follow it as currently my testing console is full of error messages and I am not able to work comfortably while testing. ====='; 'you can create a component that loads tfjs and then everywhere else reference that component - definitely cleaner.====='; 'I get the exact same error but tf is not  imported more than once?  The warning does not happen in the browser only in tests?====='; ""yes; it is - review the tests - and check for all variations of duplicates  for example: `tfjs` already includes `tfjs-core` so that if both are included that's a duplicate. or `tfjs-node` already includes `tfjs` and its a common mistake to require them both for node projects. or how `tfjs` bundles `tfjs-backend-webgl`  need to know which module already includes which one so not to include it twice  =====""; 'I have reviewed the tests - the only line that imports tensor flow in the whole of my application is the component under test and it has one line:import * as tf from ""@tensorflow/tfjs"";====='; "" I created a TensorFlow component as you advised ```import { useEffect } from 'react';import * as tf from '@tensorflow/tfjs';export default function TensorFlow() {  useEffect(() => {    window.tf = tf;  }; []);  return null;}```and tried importing it both only on `App.js` and on each comp that uses `tf` and then accessing it via `window.tf` but `window.tf `is always `undefined`. The issue is that there are two lets say utils files that use `tf` and then what is exported from them is used inside 4 or 5 components. Therefore; inside utils it seems that `tf `is not defined=====""; ' I managed to resolve it finally after many efforts. The issue eventually was that a function inside the utils that were importing `tf` was used to initialize a variable in a redux reducer and that seems to cause importing multiples times the `tf` object and was generating the error in the testing console. When I moved that function away from that file the error stopped being generated. I would like to thank you @vladmandic for your useful observations and suggestions.=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,use one package,Changing version,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.4] Others"",
    ""specific_type"": """"
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5729,Cannot create custom build due to issues in Operations Mapper,2,closed,2021-10-15T17:42:20Z,2021-10-15T18:51:59Z,trying a create a custom build and bundle of `tfjs`; got most of it working  but fails on `tf.loadGraphModel` with error:```textUncaught (in promise) TypeError: Cannot read properties of undefined (reading 'tfOpName')```traced it down to:  `tfjs-converter/src/operations/operation_mapper.ts`: -> `class OperationMapper` -> `constructor````tsconst ops = [  arithmetic; basicMath; control; convolution; creation; dynamic;  evaluation; graph; hashTable; image; logical; matrices; normalization;  reduction; sliceJoin; sparse; spectral; string; transformation];const mappersJson: OpMapper[] = [].concat(...ops.map(op => op.json)); // <-- this makes no sense```*how is this supposed to ever work?*  `ops` is a constant array of strings; so reading `op.json` property will **always** return `undefined` unless the array itself is somewhere overwritten illegally?*environment: tfjs from main branch dated 10-15-2021*,"['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5729"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5729"">No</a>====='; 'my bad; its a strange import of json files without specifying what it is. sorted it out.=====']",Reference Error,Crash,Import Error,,,change import,Fix import confusion in program,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.1"",
    ""specific_type"": ""C.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.1,A.6
https://github.com/tensorflow/tfjs/issues/5713,Uncaught (in promise) TypeError: model.predict(...).data is not a function,3,closed,2021-10-11T11:55:14Z,2021-10-12T05:03:54Z,"I am trying to implement EAST text detection using tensorflowjs and after reading an image when I try to predict using the below code :```$(""#predict-button"").click(async function () {        let image = $(""#selected-image"").get(0);        let tensor = tf.browser.fromPixels(image)            .resizeNearestNeighbor([640; 320])            .expandDims();    tnsr = tf.cast(tensor; 'float32')    let predictions = await model.predict(tnsr).data();    const text  = Array.from(predictions)    console.log(text)```i am getting the above-mentioned error but when i remove the .data() part from```let predictions = await model.predict(tnsr).data();```then code is working fine with output```(2) [e; e]```but the output is not what I am expecting; I am expecting the probabilities.Can anyone help me with this?Thanks.",['your model outputs TWO tensors; not one.```jsconst [output1; output2] = await model.predict(tnsr);const data1 = await output1.data()const data2 = await output2.data();```which one is *probabilities* and what is the other one (maybe *classes* so its probability-per-class); that is up to your model.btw; imo; this type of a question is better suited for <https://stackoverflow.com/questions/tagged/tensorflow.js>====='; '@vladmandic you are correct ; please refer at tfjs example for similar implementation. https://github.com/tensorflow/tfjs-examples/blob/master/mobilenet/index.js====='; 'Thanks you very much @vladmandic ====='],Reference Error,Crash,API Misuse,,,variable replacer,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/5691,Tensorflow JS Converter Can't Find Expected FusedBatchNorm Input,16,closed,2021-10-04T16:03:37Z,2021-11-05T20:25:23Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes. I have used the following script to convert a graph model to a .pb format as I could not find official documentation on how to do this:  ```import tensorflow as tfmeta_path = 'test.meta' # Your .meta fileoutput_node_names = ['pose/locref_pred/block4/BiasAdd'; 'pose/part_pred/block4/BiasAdd']    # Output nodeswith tf.compat.v1.Session() as sess:    # Restore the graph    saver = tf.compat.v1.train.import_meta_graph(meta_path)    # Load weights    saver.restore(sess;tf.compat.v1.train.latest_checkpoint('checkpoint_folder'))    # Freeze the graph    frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(        sess;        sess.graph_def;        output_node_names; variable_names_blacklist=[n.name for n in sess.graph_def.node if ""FusedBatchNorm"" in n.name])    # Save the frozen graph    with open('_graph.pb'; 'wb') as f:      f.write(frozen_graph_def.SerializeToString())```- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):  Kubuntu 21.04- TensorFlow.js installed from (npm or script link):  Installed from pip - Tensorflow.js Converter Version:  3.9.0**Describe the current behavior**When using this command :```tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='pose/locref_pred/block4/BiasAdd';'pose/part_pred/block4/BiasAdd' output_graph.pb output --skip_op_check```I get```2021-10-04 11:07:07.150459: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory2021-10-04 11:07:07.150484: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.2021-10-04 11:07:13.137106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize  debug_stripper: debug_stripper did nothing. time = 0.369ms.  model_pruner: Graph size after: 1130 nodes (-530); 1201 edges (-530); time = 29.391ms.  constant_folding: Graph size after: 1088 nodes (-42); 1147 edges (-54); time = 105.765ms.  arithmetic_optimizer: Graph size after: 1088 nodes (0); 1147 edges (0); time = 52.367ms.  dependency_optimizer: Graph size after: 1064 nodes (-24); 1123 edges (-24); time = 23.379ms.  model_pruner: Graph size after: 1064 nodes (0); 1123 edges (0); time = 10.789ms.  constant_folding: Graph size after: 1064 nodes (0); 1123 edges (0); time = 42.342ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 41.601ms.  dependency_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 26.46ms.  debug_stripper: debug_stripper did nothing. time = 2.391ms.  model_pruner: Graph size after: 1064 nodes (0); 1123 edges (0); time = 15.009ms.  constant_folding: Graph size after: 1064 nodes (0); 1123 edges (0); time = 40.475ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 33.552ms.  dependency_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 20.708ms.  model_pruner: Graph size after: 1064 nodes (0); 1123 edges (0); time = 12.907ms.  constant_folding: Graph size after: 1064 nodes (0); 1123 edges (0); time = 44.195ms.  arithmetic_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 37.797ms.  dependency_optimizer: Graph size after: 1064 nodes (0); 1123 edges (0); time = 25.408ms.WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/FusedBatchNormV3'```The warnings here appear to be problematic. When loading the files into a browser; I get ``tf.loadGraphModel('http://127.0.0.1:5000/download/model.json')Promise {<pending>}util.ts:105 Uncaught (in promise) Error: Based on the provided shape; [3;3;512;512]; the tensor should have 2359296 values but has 2300067``**Describe the expected behavior**Either the warnings should not exist (i.e. the FusedBatchNorm is converted properly); or the model should be able to load in the browser without the batch norm layers**Standalone code to reproduce the issue**https://github.com/ByrdOfAFeather/tfjs_bug",,Data & Model Error,Crash,API Misuse,,,reload file,,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.3] Inf/None/Null Results"",
    ""specific_type"": ""[D.3.1] Warning about FusedBatchNorm input and inconsistent tensor values""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.2,A.3
https://github.com/tensorflow/tfjs/issues/5664,Error: Requested texture size [-1536x512] is invalid,2,open,2021-09-25T20:24:22Z,2021-09-27T15:29:27Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows - TensorFlow.js installed from (npm or script link): script link- TensorFlow.js version (use command below): 3.9.0- Browser version: Chrome 94.0.4606.54- Tensorflow.js Converter Version:  Most recent**Describe the current behavior**I am running two object detection models. The first locates where a sudoku board is in an image; and the second model locates where each grid is on the sudoku board. The first model works fine. However; when I call executeAsync to the model to find where each grid is on sliced tensor (This comes from the predictions made from the first model) I get an error. So the first object detection models works; but upon calling the second object detection model I get the error :tf.min.js:17 Uncaught (in promise) Error: Requested texture size [-1536x512] is invalid.    at tf.min.js:17    at jX (tf.min.js:17)    at tf.min.js:17    at e.t.createFloat32MatrixTexture (tf.min.js:17)    at e.t.acquireTexture (tf.min.js:17)    at t.n.acquireTexture (tf.min.js:17)    at t.n.uploadToGPU (tf.min.js:17)    at t.n.runWebGLProgram (tf.min.js:17)    at Object.kernelFunc (tf.min.js:17)    at n (tf.min.js:17)**Describe the expected behavior**It is should work exactly how the first model works. Its weird because the first model is able to make predictions but for the second model it fails.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible; please share a link to Colab/CodePen/any notebook.--This is the image I was loading to the html input![test_img](https://user-images.githubusercontent.com/20917134/134791114-2461b4bf-0692-41fe-afae-be9d12b858c3.png)**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.Code:HTML:```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>TF JS example</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.js""></script></head><body><form method=""POST"" action=""/upload"" enctype=""multipart/form-data"">  <p><input type=""file"" id=""file1"" name=""file""></p></form><canvas style=""border: 10px solid black "" id=""canvas""  >  <img id =""display"" width = ""...""  height=""..."" ></canvas><script src=""main.js""></script>    </body></html>```main.js:`````async function load() {  const model = await tf.loadGraphModel('https://big-g.s3.us-east.cloud-object-storage.appdomain.cloud/model.json');    return model;  };async function load_s(){  const model_s = await tf.loadGraphModel('https://small-g.s3.us-east.cloud-object-storage.appdomain.cloud/model.json');  return model_s}async function load_class(){  const model_class = await tf.loadGraphModel(""https://classy.s3.us-east.cloud-object-storage.appdomain.cloud/model.json"");  return model_class}var model = load();var model_s = load_s();//var model_class =load_class();console.log(""laoded"")document.querySelector('input').addEventListener('change'; function() {  console.log(""change"")  var reader = new FileReader();  const fileInput = document.getElementById(""file1"").files[0];  console.log(fileInput);  readerDim = new FileReader();  readerDim.readAsDataURL(fileInput);  readerDim.onload = function (e) {    //Initiate the JavaScript Image object.    var image = new Image();    //Set the Base64 string return from FileReader as source.    image.src = e.target.result;    //Validate the File Height and Width.    image.onload = function () {      var height = this.height;      var width = this.width;      console.log(width;height);//width;height        var a = tf.browser.fromPixels(image);      //convert image into 3 channel grayscale      var g_scale =a.mean(2).expandDims(-1);      var a = tf.image.grayscaleToRGB(g_scale).cast(""int32"");      a=a.expandDims();      model.then(model => {            async function pred() {        console.log('Start');        const result = await model.executeAsync(a);        console.log('End')        //console.log( await result[6].array());        //const classes = await result[2].array();        //const accuracy = await result[4].array();        const boxes = await result[6].array(); //y;x;height;width                //Time to draw         const canvas = document.querySelector('#canvas');        const ctx = canvas.getContext('2d');        //resize        canvas.height = image.height;        canvas.width= image.width;         // ctx.drawImage(image;0;0);       tf.browser.toPixels(a.squeeze();canvas);        const [y;x;height;width] =boxes[0][0];        //console.log(y);        console.log(""drawing..."");        const xc = Math.round(x*image.width);        const yc = Math.round(y*image.height);        const x2c = Math.round(width*image.width);        const y2c = Math.round(height*image.height);        console.log(xc;yc;x2c ;y2c);//x1; y1; x2; y2; not height or width        //ctx.rect(xc; yc; x2c-xc; y2c-yc);        //ctx.stroke();        console.log(a.shape)        sliced = a.slice([0;yc;xc;0];[1;y2c-yc;x2c-xc;3] );        console.log(sliced)        //await new Promise(r => setTimeout(r; 1000));        //ctx.clearRect(0; 0; canvas.width; canvas.height)        tf.browser.toPixels(sliced.squeeze();canvas);        model_s.then(model_s =>{          async function small_g(sliced){            console.log(""predicting small"";sliced);            const result_s = await model_s.executeAsync(sliced);            return result_s          }          const result_s = small_g(sliced);          console.log(result_s);        });        };      console.log('Predicting')      var box =  pred();      });    };  };}; false);`````Logs:changemain.js:27 File {name: 'test_img.png'; lastModified: 1631819149502; lastModifiedDate: Thu Sep 16 2021 15:05:49 GMT-0400 (Eastern Daylight Time); webkitRelativePath: ''; size: 259253; …}main.js:45 1268 784main.js:104 Predictingmain.js:56 Startmain.js:58 Endmain.js:76 drawing...main.js:82 117 81 660 624main.js:85 (4) [1; 784; 1268; 3]main.js:87 e {kept: false; isDisposedInternal: false; shape: Array(4); dtype: 'int32'; size: 884547; …}main.js:95 predicting small e {kept: false; isDisposedInternal: false; shape: Array(4); dtype: 'int32'; size: 884547; …}main.js:100 Promise {<pending>}tf.min.js:17 Uncaught (in promise) Error: Requested texture size [-1536x512] is invalid.    at tf.min.js:17    at jX (tf.min.js:17)    at tf.min.js:17    at e.t.createFloat32MatrixTexture (tf.min.js:17)    at e.t.acquireTexture (tf.min.js:17)    at t.n.acquireTexture (tf.min.js:17)    at t.n.uploadToGPU (tf.min.js:17)    at t.n.runWebGLProgram (tf.min.js:17)    at Object.kernelFunc (tf.min.js:17)    at n (tf.min.js:17)(anonymous) @ tf.min.js:17jX @ tf.min.js:17(anonymous) @ tf.min.js:17t.createFloat32MatrixTexture @ tf.min.js:17t.acquireTexture @ tf.min.js:17n.acquireTexture @ tf.min.js:17n.uploadToGPU @ tf.min.js:17n.runWebGLProgram @ tf.min.js:17kernelFunc @ tf.min.js:17n @ tf.min.js:17(anonymous) @ tf.min.js:17t.scopedRun @ tf.min.js:17t.runKernelFunc @ tf.min.js:17t.runKernel @ tf.min.js:17stridedSlice_ @ tf.min.js:17stridedSlice__op @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17t.scopedRun @ tf.min.js:17t.tidy @ tf.min.js:17qI @ tf.min.js:17(anonymous) @ tf.min.js:17pW @ tf.min.js:17p @ tf.min.js:17t.processStack @ tf.min.js:17(anonymous) @ tf.min.js:17c @ tf.min.js:17(anonymous) @ tf.min.js:17(anonymous) @ tf.min.js:17bv @ tf.min.js:17o @ tf.min.js:17async function (async)small_g @ main.js:96(anonymous) @ main.js:99Promise.then (async)pred @ main.js:93async function (async)pred @ main.js:57(anonymous) @ main.js:105Promise.then (async)image.onload @ main.js:53load (async)readerDim.onload @ main.js:42load (async)(anonymous) @ main.js:33Show 2 more frames","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5664"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5664"">No</a>====='; ""Any tensor I use even a random one gives this error. I tried recreating the model from the tfjs-converter but that didn't work. I think the error is that this is a center-net model. so I will just use a different model=====""]",Data & Model Error,Crash,Improper Model Attribute,,,change model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.4] WebGL Limits""
  }
}
```",A.2,C.2
https://github.com/tensorflow/tfjs/issues/5631,Can't run electron application - Getting Uncaught TypeError error message,4,closed,2021-09-16T16:01:27Z,2021-09-27T17:43:48Z,"Hi Team;Kindly help me. I'm developing chatbot application using Angular10 and convert to desktop application using ElectronJS.For offline chatbot experience; I used tensorflow/tfjs. it working in browser but we loading electron application i'm getting the below attached error message. Note: Enabled NodeIntegration (true) in electron application.Please let me know; if need any other information.![tensorflow error](https://user-images.githubusercontent.com/67852635/133645276-03f2a5ef-50bd-4af7-ad82-20d2ee1e002f.PNG)![tensorflow platform](https://user-images.githubusercontent.com/67852635/133645279-fd5dde84-f0ab-4e65-914d-585b7c99d20f.PNG)I'm using the below mentioned dependencies (package.json)""dependencies"": {    ""@angular/animations"": ""~10.0.6"";    ""@angular/cdk"": ""^11.2.5"";    ""@angular/common"": ""~10.0.6"";    ""@angular/compiler"": ""~10.0.6"";    ""@angular/core"": ""~10.0.6"";    ""@angular/flex-layout"": ""^10.0.0-beta.32"";    ""@angular/forms"": ""~10.0.6"";    ""@angular/localize"": ""~10.0.6"";    ""@angular/material"": ""^8.2.3"";    ""@angular/platform-browser"": ""~10.0.6"";    ""@angular/platform-browser-dynamic"": ""~10.0.6"";    ""@angular/router"": ""~10.0.6"";    ""@angular/service-worker"": ""~10.0.6"";    ""@ng-bootstrap/ng-bootstrap"": ""^8.0.0"";    ""@ng-idle/core"": ""^10.0.0-beta.1"";    ""@ng-select/ng-select"": ""^6.1.0"";    ""@tensorflow/tfjs"": ""^3.9.0"";    ""@types/webgl2"": ""0.0.6"";    ""angularx-qrcode"": ""^10.0.12"";    ""bootstrap"": ""^4.6.0"";    ""compute-cosine-similarity"": ""^1.0.0"";    ""core-js"": ""^2.5.4"";    ""font-awesome"": ""^4.7.0"";    ""hammerjs"": ""^2.0.8"";    ""jquery"": ""^3.5.1"";    ""jszip"": ""^3.5.0"";    ""jszip-utils"": ""^0.1.0"";    ""lodash.merge"": ""^4.6.2"";    ""material-design-icons"": ""^3.0.1"";    ""moment"": ""^2.29.1"";    ""net"": ""^1.0.2"";    ""ng-connection-service"": ""^1.0.4"";    ""ng2-file-upload"": ""^1.3.0"";    ""ngx-color-picker"": ""^11.0.0"";    ""ngx-electron"": ""^2.2.0"";    ""ngx-toastr"": ""^10.0.4"";    ""node-jose"": ""^2.0.0"";    ""popper.js"": ""^1.14.3"";    ""rxjs"": ""~6.5.5"";    ""rxjs-compat"": ""^6.3.3"";    ""sockjs-client"": ""^1.5.1"";    ""stompjs"": ""^2.3.3"";    ""stopwords"": ""0.0.9"";    ""tslib"": ""^2.1.0"";    ""uuid"": ""^8.3.2"";    ""wink-lemmatizer"": ""^3.0.1"";    ""wink-tokenizer"": ""^5.2.3"";    ""zone.js"": ""~0.10.3""  };  ""devDependencies"": {    ""@angular-devkit/build-angular"": ""~0.1000.5"";    ""@angular-devkit/build-ng-packagr"": ""~0.1000.8"";    ""@angular/cli"": ""~10.0.5"";    ""@angular/compiler-cli"": ""~10.0.6"";    ""@angular/language-service"": ""~9.0.0"";    ""@types/jasmine"": ""~3.5.0"";    ""@types/jasminewd2"": ""~2.0.3"";    ""@types/lodash.merge"": ""^4.6.6"";    ""@types/node"": ""^12.19.15"";    ""@types/node-jose"": ""^1.1.5"";    ""@types/uuid"": ""^8.3.0"";    ""codelyzer"": ""^6.0.0"";    ""csscomb"": ""^4.3.0"";    ""electron"": ""^11.2.3"";    ""electron-packager"": ""^15.2.0"";    ""husky"": ""^4.3.8"";    ""jasmine-core"": ""~3.5.0"";    ""jasmine-spec-reporter"": ""~5.0.0"";    ""karma"": ""~5.0.0"";    ""karma-chrome-launcher"": ""~3.1.0"";    ""karma-coverage-istanbul-reporter"": ""~3.0.2"";    ""karma-jasmine"": ""~3.3.0"";    ""karma-jasmine-html-reporter"": ""^1.5.0"";    ""ng-packagr"": ""^10.0.0"";    ""protractor"": ""~7.0.0"";    ""ts-node"": ""~8.3.0"";    ""tslint"": ""~6.1.0"";    ""typescript"": ""~3.9.5""  } ","['Did you get chance to check this [link](https://stackoverflow.com/questions/65562209/this-util-textencoder-is-not-a-constructor-only-in-electron-app-works-in-chrome) ? if it does not help please provide a minimal reproduction code.Thanks Rajesh ====='; ""The stackoverflow link is not helped me.While loading application; tensorflow try to set env().setPlatform('node'; new PlatformNode());Here require('util') return null and doesn't have TextEncoder function. It seems; require('util') not refer @type/node component.export class PlatformNode {    constructor() {        // tslint:disable-next-line:no-require-imports        this.util = require('util');               // According to the spec; the built-in encoder can do only UTF-8 encoding.        // https://developer.mozilla.org/en-US/docs/Web/API/TextEncoder/TextEncoder        this.textEncoder = new this.util.TextEncoder();    }=====""; 'Hi @gsvin99; I am not super familiar with electron; but I think you probably need to turn off node integration in the renderer process so it can use browser js bundle instead of nodejs bundle. Also; it might help checking out this [tfjs electron example](https://github.com/tensorflow/tfjs-examples/tree/master/electron) (it has not been updated for the newer tfjs versions but its setup might help). It shows you how to use tfjs in renderer process and main process. Hope these can help you. Thanks!====='; 'Thanks @jinjingforever and @rthadur Electron application working; after upgrading newer version; disable node integration and enable context isolation. =====']",Build & Install Failure,Build & Initialization Failure,API Misuse,,,change env flag,Modifying the value of environment variable,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",C,A.3
https://github.com/tensorflow/tfjs/issues/5607,TFJS TFLite installation with script tags not working,3,closed,2021-09-11T10:23:53Z,2021-09-13T21:02:42Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**I have tried to use the ""import via script tag"" section from https://www.npmjs.com/package/@tensorflow/tfjs-tflite/v/0.0.1-alpha.2 in a very basic index.html as follows:```<!DOCTYPE html><html><head>	<meta charset=""utf-8"">	<title>TFJS TFLite</title></head><body>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>	<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite""></script></body></html>```but the `tfjs-tflite` script raises an error:```Request URL: https://cdn.jsdelivr.net/npm/@tensorflow/tflite_web_api_cc_simd.jsRequest Method: GETStatus Code: 404 ```**Any other info / logs**See https://replit.com/@ClementWalter/tfjs-tflite#index.html","[""This looks like a typical problem with relative path references. If the file loads another file; it needs to be loaded via a fully resolved URL; which in this case is https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.6/dist/tf-tflite.min.js or https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.min.js if you don't request a specific version. Then; the requested relative path will be correctly resolved to https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.6/dist/tflite_web_api_cc_simd.js=====""; '@MartinKolarik thats correct ; please also refer to note here https://github.com/tensorflow/tfjs/tree/master/tfjs-tflite#via-a-script-tag ; thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5607"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5607"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,API Misuse,,,change import(script) path,Fix import confusion in program,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Script Loading Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.3
https://github.com/tensorflow/tfjs/issues/5574,How to convert microphone input(arraybuffer/blob/audio in javascript) to tensor for inference at browser?,2,closed,2021-09-02T12:14:36Z,2021-09-07T17:41:49Z,Hi tensors;I have recently started working on TensorflowJS. Agenda is to make web app for speech model.But I am confused and getting stuck; How to preprocess speech(microphone) input?In python; we have packages to preprocess the input. But how can I convert microphone input ( from browser; it is ArrayBuffer or Blob) into tensors. Some suggestions will be very appreciated. Thanks.,"['please check this codelab https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab#0 ; Thank you.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5574"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5574"">No</a>=====']",Reference Error,Crash,API Misuse,,,add data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""E"",
    ""subcategory"": ""E.1"",
    ""specific_type"": ""E.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/5566,Latest released @tensorflow/tfjs-core 3.9.0 missed some files.,4,closed,2021-09-01T06:09:26Z,2021-09-02T02:52:36Z,Current @tensorflow/tfjs-core 3.9.0 only have following ```shnode_modules/@tensorflow/tfjs-core$ lsdist  package.json  README.md```while @tensorflow/tfjs-core 3.8.0 had```shnode_modules/@tensorflow/tfjs-core$ lsBUILD.bazel  cloudbuild.yml  development  dist  package.json  README.md  scripts  setup_test_bazel.ts  src  test.html  tsconfig.test.json```,"['Link https://github.com/webmachinelearning/webnn-polyfill/issues/114====='; ""Thanks for the bug report! This is actually intentional as part of the Bazel build migration. We omitted TS source files (along with other files) in the most recent release because they are not used by downstream packages (although it seems like your package is an exception).In the issue you linked; I noticed you're using `@tensorflow/tfjs-core/src/ops/conv_util`. Could you instead import `@tensorflow/tfjs-core/dist/ops/conv_util`; or does this not work for your use case?=====""; 'Thanks much @mattsoulanille. It works for my case. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5566"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5566"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,change import(script) path,Fix import confusion in program,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[E] Document Error"",
    ""subcategory"": ""[E.1] Invalid Documentation"",
    ""specific_type"": ""[E.1.1] Missing Files in Release Note""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5530,tflite: tfjs-backend-cpu: Uncaught TypeError: Cannot read property 'whereImpl' of undefined,6,closed,2021-08-25T11:04:17Z,2021-08-26T17:07:26Z,"**System information**- OS Platform and Distribution: Ubuntu 20.04- TensorFlow.js version: Latest as of writing- Browser version: Chrome 92The ""Via a script tag"" instructions from [the tfjs-tflite readme](https://www.npmjs.com/package/@tensorflow/tfjs-tflite/v/0.0.1-alpha.2) don't work:```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite""></script><script>  (async function() {    const tfliteModel = await tflite.loadTFLiteModel('https://tfhub.dev/tensorflow/lite-model/mobilenet_v2_1.0_224/1/metadata/1');  })();</script>```![image](https://user-images.githubusercontent.com/1167575/130779524-4ba81071-3195-40d1-8190-549cd57afff6.png)But this works fine:```html<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.8.0/dist/tf.min.js""></script><script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.4/dist/tf-tflite.min.js""></script><script>  (async function() {    const tfliteModel = await tflite.loadTFLiteModel('https://tfhub.dev/tensorflow/lite-model/mobilenet_v2_1.0_224/1/metadata/1');  })();</script>```","['@josephrocca thanks for reporting ; I see there is a older copy on npm [here](https://www.npmjs.com/package/@tensorflow/tfjs-tflite/v/0.0.1-alpha.2) ; @jinjingforever do we need to push latest to npm ?====='; 'Nice catch @josephrocca! I think the order is wrong in the README file; (tfjs-core should be before tfjs-backend-cpi); also the tfjs-tflite import is also wrong:) I will send a PR to fix this. Thank you! ====='; 'I just realized that we do have the correct instructions in the README file in the master branch. I will publish a new version to npm soon (maybe in the next 1-2 weeks). Thank you! Closing this issue for now.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5530"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5530"">No</a>====='; ""@jinjingforever Ah; cool; thanks! Also; can I unrelatedly suggest that you add an initial console warning if people are using a script tag linked to the latest version? Perhaps by just checking `document.currentScript.src` for a version string (if the runtime has the `document` global). Either that; or use versioned links in the readme? That prevents newbies (who don't realise the problem with using the link straight from the readme) from having their app break when there's a breaking change in tfjs.=====""; 'Good ideas! Thanks @josephrocca. Will update README with versioned links for now. =====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,change import,Fix import confusion in program,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5515,dynamic input sizes are not working with movenet multipose model and tfjs-node,7,closed,2021-08-20T12:28:40Z,2021-11-19T22:59:14Z,"[MoveNet MultiPose model](https://storage.googleapis.com/movenet/MoveNet.MultiPose%20Model%20Card.pdf) was released and published on [TFHub](https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1)  model is advertised as working with dynamic input sizes; but that results in error during inference using `tfjs-node`:```logError: Invalid TF_Status: 3Message: output dimensions must be positive    at NodeJSKernelBackend.executeSingleOutput (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-node@3.8.0/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:211:43)    at Object.kernelFunc (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-node@3.8.0/node_modules/@tensorflow/tfjs-node/dist/kernels/ResizeBilinear.js:43:27)    at kernelFunc (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.8.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4672:32)    at /home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.8.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4733:27```editing input in `model.json` to use fixed size resolves the issue: `""tensorShape"":{""dim"":[{""size"":""1""};{""size"":""256""};{""size"":""256""};{""size"":""3""}]}}`(original `model.json`:)```json  ""signature"": {     ""inputs"": { ""input"": {""name"":""input:0"";""dtype"":""DT_INT32"";""tensorShape"":{""dim"":[{""size"":""1""};{""size"":""-1""};{""size"":""-1""};{""size"":""3""}]}} };    ""outputs"": { ""output_0"": {""name"":""Identity:0"";""dtype"":""DT_FLOAT"";""tensorShape"":{""dim"":[{""size"":""1""};{""size"":""6""};{""size"":""56""}]}} }  };```no issues running model using `tfjs-backend-webgl`environment: tfjs 3.8.0 on ubuntu 21.10","[""@vladmandic I tested with the following code and got no issues; is there anything different than what you are running? I used tfjs-node 3.8.0`const tf = require('@tensorflow/tfjs-node');` `const model = await tf.loadGraphModel(    'https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1';    {fromTFHub: true});` `const output =    model.predict(tf.zeros([1; 256; 256; 3]; 'int32'));``  console.log(output.dataSync());`=====""; ""@ahmedsabie try using any real JPG file; using synthetic input short-circuits model execution and is rarely useful when testing models.```jsconst fs = require('fs');const tf = require('@tensorflow/tfjs-node');async function main() {  const buf = fs.readFileSync('inputs/model1.jpg');  const tensor = tf.node.decodeJpeg(buf; 3).expandDims(0);  const model = await tf.loadGraphModel('https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1'; { fromTFHub: true });  const res = model.predict(tensor);  const data = await res.data();  console.log(data);}main();``````textError: Invalid TF_Status: 3Message: Incompatible shapes: [1;60;40;64] vs. [1;59;40;64]    at NodeJSKernelBackend.executeSingleOutput (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-node@3.11.0/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:211:43)    at Object.kernelFunc (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-node@3.11.0/node_modules/@tensorflow/tfjs-node/dist/kernels/Add.js:28:24)    at kernelFunc (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4541:32)    at /home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4602:27    at Engine.scopedRun (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4406:23)    at Engine.runKernelFunc (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4598:14)    at Engine.runKernel (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4470:21)    at add_ (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-converter@3.11.0_@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:11734:19)    at add__op (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-converter@3.11.0_@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:11577:29)    at executeOp$j (/home/vlado/dev/movenet/node_modules/.pnpm/@tensorflow+tfjs-converter@3.11.0_@tensorflow+tfjs-core@3.11.0/node_modules/@tensorflow/tfjs-converter/dist/tf-converter.node.js:25858:21)```=====""; ""@vladmandic I retested with the following 256x256 image: https://bit.ly/3kQAQUH and the following code:```js  const buf = fs.readFileSync('man.jpg');  const tensor = tf.cast(tf.image.resizeBilinear(tf.node.decodeJpeg(buf; 3); [256; 256]); 'int32').expandDims(0);  console.log(tensor.shape);  const model = await tf.loadGraphModel('https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1'; { fromTFHub: true });  const res = model.predict(tensor);  const data = await res.data();  console.log(data);}main();```No errors; can you try with the same image? I am wondering if the issue is with a specific image or with a specific platform.=====""; '256x256 is the only resolution thar works; any other resolution causes error. But model is published with variable resolution as input - so either fix the model to actually work with variable resolution or change model input definition to accept only 256!====='; ""@vladmandic The documentation asserts that input size must be 1xHxWx3; where H and W need to be a multiple of 32; so it does not support all shapes; nor does the model json being published with variable resolution indicate it taking any input size. I tested with the following code which asserts multiples of 32 work as expected.```js  for (let h = 32; w <= 512; h+=32) {    for (let w = 32; w <= 512; w+=32) {    const buf = fs.readFileSync('man.jpg');    const tensor = tf.cast(tf.image.resizeBilinear(tf.node.decodeJpeg(buf; 3); [h; w]); 'int32').expandDims(0);    console.log(tensor.shape);    const model = await tf.loadGraphModel('https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1'; { fromTFHub: true });    const res = model.predict(tensor);  const data = await res.data();  console.log(data);    }  }```=====""; 'That\'s new - comment on ""multiple of 32"" was not there when I created this issue; now its in **BOLD** :)  Given that its documented; its ok to close this issue  ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5515"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5515"">No</a>=====']",Data & Model Error,Crash,API Misuse,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.2,A.3
https://github.com/tensorflow/tfjs/issues/5513,[tfjs-react-native] react-native-unimodules is deprecated,2,closed,2021-08-20T07:12:07Z,2021-08-21T02:40:06Z,"Here: https://github.com/expo/expo/tree/master/packages/react-native-unimodulesAnd actually when I installed `react-native-unimodules: 0.14.6` with `react-native: 0.64.2` on Android; I got this:``` WARN  The ""UMNativeModulesProxy"" native module is not exported through NativeModules; verify that @unimodules/react-native-adapter's native code is linked properly ERROR  TypeError: null is not an object (evaluating 'NativeUnimoduleProxy.viewManagersNames') ERROR  Invariant Violation: Module AppRegistry is not a registered callable module (calling runApplication). A frequent cause of the error is that the application entry file path is incorrect.       This can also happen when the JS bundle is corrupt or there is an early initialization error when loading React Native. ERROR  Invariant Violation: Module AppRegistry is not a registered callable module (calling runApplication). A frequent cause of the error is that the application entry file path is incorrect.       This can also happen when the JS bundle is corrupt or there is an early initialization error when loading React Native.```","[""Fixed.It seems like I need to config react-native-unimodules manually:https://docs.expo.dev/bare/installing-unimodules/Because I built my project with bare rn; instead of expo.And don't forget to install/config android ndk in the right place.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5513"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5513"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Import Error,,,install dependency,Modifying dependency configuration,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5445,[Codelab]: Making Predictions from 2D Data,3,closed,2021-08-07T22:17:30Z,2021-08-09T13:12:59Z,data is not definedawait trainModel(model; inputs; labels);   //  gives an error saying Uncaught SyntaxError: await is only valid in async functions and the top level bodies of modules,"['Hello; the trainModel function returns a promise it can be resolved either by then statement:`trainModel(model; inputs; labels).then( console.log(""done"") )` .or by putting it inside an async function and await:```async function name(){ await trainModel(model; inputs; labels); }```====='; ""@bibs2091 thank you ; yes please wrap your code inside a '`async`' function. Thank you =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5445"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5445"">No</a>=====']",Reference Error,Crash,API Misuse,,,syntax modifier,syntax modifier,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/5411,SavedModel and converted tfjs model give different predictions,2,closed,2021-08-01T10:20:32Z,2021-08-12T17:42:22Z,"I've trained with [TensorFlow Model Maker][1] and exported it to the `SavedModel` format. Using the following script:```pythondef preprocess_image(image):    image = tf.cast(image; tf.float32)    image /= tf.constant(255; dtype=image.dtype)    image = tf.compat.v1.image.resize(image; [224; 224])    return imagedef predict_top_k(model; data; labels; k=1):    predicted_prob = model.predict(data)    topk_prob; topk_id = tf.math.top_k(predicted_prob; k=k)    topk_label = np.array(labels)[topk_id.numpy()]    label_prob = []    for label; prob in zip(topk_label; topk_prob.numpy()):        label_prob = list(zip(label; prob))    return label_probmodel = tf.keras.models.load_model(model_path)image = tf.keras.preprocessing.image.load_img(image_path)input_arr = tf.keras.preprocessing.image.img_to_array(image)preprocessed = preprocess_image(input_arr)predicted = predict_top_k(    model; np.expand_dims(preprocessed; axis=0); data.index_to_label; k=4)```I get the following predictions for a single file:```[('bk'; 0.95859015); ('l'; 0.017178109); ('p'; 0.014439273); ('bg'; 0.009792461)]```I've converted this model to the tfjs format using ` tensorflowjs_converter` and use it in typescript as follows:```typescriptconst getTopKClasses = async (probs: tf.Tensor; topK: number = Infinity): Promise<Prediction[]> => {  const predictions = tf.softmax(probs);  const values = await predictions.data();  return Array.from(values)    .map((value: number; i: number) => ({ label: CLASSES[i]; value }))    .sort((a: Prediction; b: Prediction) => b.value - a.value)    .slice(0; topK);};const predict = async (modelUrl: string; data: HTMLImageElement) => {  const model = await tf.loadGraphModel(modelUrl);  const input = tf.browser.fromPixels(data);  const resized = tf.image.resizeBilinear(input; [224; 224]);  const preprocessed = tf.div(resized.asType(""float32""); 255).reshape([1; ...resized.shape]);  const result = model.execute(preprocessed);  const predictions = getTopKClasses(result);  return predictions;}```Although the class with the highest probability is generally the same;  I get different predictions with this method. For example; these are the predictions for the same image file as above:```0: {label: ""bk""; value: 0.46371275186538696}1: {label: ""l""; value: 0.17934054136276245}2: {label: ""p""; value: 0.1789223849773407}3: {label: ""bg""; value: 0.1780242621898651}```What could cause this difference?  [1]: https://www.tensorflow.org/lite/guide/model_maker","[""`tf.math.top_k` and `tf.softmax` don't do the same thing - you should rewrite typescript code to find top results without applying `softmax` op.unrelated; why not `.expandDims(0)` instead of `.reshape([1; ...resized.shape]);`?=====""; ""Thanks; can't believe I missed that!=====""]",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change API,Replace API with another effective one,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5377,How to use execute instead of executeAsync for object detection model?,3,open,2021-07-25T12:49:09Z,2021-08-24T20:32:35Z,Good day!I have integrated Unity WebGL with TensorflowJS.When calling the JavaScript script from Unity; I need to return the search result for objects detection.The result must be obtained synchronously; since it is used in Unity for post-processing (realtime; in the same frame).How can I use execute instead of executeAsync for object detection (mobilenet v3)?Now; when I try to call executeAsync; I get the error:Uncaught (in promise) Error: This execution contains the node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Exit_2'; which has the dynamic op 'Exit'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3],"['Please provide a codepen example or git repo for us to reproduce the same ; meanwhile can you please check below related issue https://github.com/tensorflow/tfjs/issues/4579 ; thank you ====='; ""**rthadur**;You can see my sample code and model here:https://github.com/tensorflow/tfjs/issues/5259It work with version 3.3.0 when using the executeAsync function and doesn't work with execute function.=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",Data & Model Error,Crash,Improper Model Attribute,,,re-converter model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,C.2
https://github.com/tensorflow/tfjs/issues/5377,How to use execute instead of executeAsync for object detection model?,3,open,2021-07-25T12:49:09Z,2021-08-24T20:32:35Z,Good day!I have integrated Unity WebGL with TensorflowJS.When calling the JavaScript script from Unity; I need to return the search result for objects detection.The result must be obtained synchronously; since it is used in Unity for post-processing (realtime; in the same frame).How can I use execute instead of executeAsync for object detection (mobilenet v3)?Now; when I try to call executeAsync; I get the error:Uncaught (in promise) Error: This execution contains the node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Exit_2'; which has the dynamic op 'Exit'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3],"['Please provide a codepen example or git repo for us to reproduce the same ; meanwhile can you please check below related issue https://github.com/tensorflow/tfjs/issues/4579 ; thank you ====='; ""**rthadur**;You can see my sample code and model here:https://github.com/tensorflow/tfjs/issues/5259It work with version 3.3.0 when using the executeAsync function and doesn't work with execute function.=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",Data & Model Error,Crash,Improper Model Attribute,,,re-converter model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.3.1,C.2
https://github.com/tensorflow/tfjs/issues/5377,How to use execute instead of executeAsync for object detection model?,3,open,2021-07-25T12:49:09Z,2021-08-24T20:32:35Z,Good day!I have integrated Unity WebGL with TensorflowJS.When calling the JavaScript script from Unity; I need to return the search result for objects detection.The result must be obtained synchronously; since it is used in Unity for post-processing (realtime; in the same frame).How can I use execute instead of executeAsync for object detection (mobilenet v3)?Now; when I try to call executeAsync; I get the error:Uncaught (in promise) Error: This execution contains the node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Exit_2'; which has the dynamic op 'Exit'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3],"['Please provide a codepen example or git repo for us to reproduce the same ; meanwhile can you please check below related issue https://github.com/tensorflow/tfjs/issues/4579 ; thank you ====='; ""**rthadur**;You can see my sample code and model here:https://github.com/tensorflow/tfjs/issues/5259It work with version 3.3.0 when using the executeAsync function and doesn't work with execute function.=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",Regression,Poor Performance,Improper Model Attribute,,,re-converter model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,C.2
https://github.com/tensorflow/tfjs/issues/5377,How to use execute instead of executeAsync for object detection model?,3,open,2021-07-25T12:49:09Z,2021-08-24T20:32:35Z,Good day!I have integrated Unity WebGL with TensorflowJS.When calling the JavaScript script from Unity; I need to return the search result for objects detection.The result must be obtained synchronously; since it is used in Unity for post-processing (realtime; in the same frame).How can I use execute instead of executeAsync for object detection (mobilenet v3)?Now; when I try to call executeAsync; I get the error:Uncaught (in promise) Error: This execution contains the node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Exit_2'; which has the dynamic op 'Exit'. Please use model.executeAsync() instead. Alternatively; to avoid the dynamic ops; specify the inputs [Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3],"['Please provide a codepen example or git repo for us to reproduce the same ; meanwhile can you please check below related issue https://github.com/tensorflow/tfjs/issues/4579 ; thank you ====='; ""**rthadur**;You can see my sample code and model here:https://github.com/tensorflow/tfjs/issues/5259It work with version 3.3.0 when using the executeAsync function and doesn't work with execute function.=====""; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.=====']",Regression,Poor Performance,Improper Model Attribute,,,re-converter model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",B.3.1,C.2
https://github.com/tensorflow/tfjs/issues/5371,No backend found in registry error when trying to launch blazepose,5,open,2021-07-23T11:22:04Z,2021-10-20T16:37:40Z,framework,"[""@TheUltragon can you try without `import '@tensorflow/tfjs-backend-webgl';` ?=====""; 'Seems to have been the issue. Created a pull request on that repo (https://github.com/tensorflow/tfjs-models/pull/768) that fixes this; along with some other issues I faced getting this running. Will close this once that one is merged.====='; 'Thank you @TheUltragon for fixing this.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5371"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5371"">No</a>====='; 'Why was this issue closed? Was it resolved? I dont see any changes in those particular parts of the codebase reflecting that; nor do I see my pull request being accepted.=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,delete import,Fix import confusion in program,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] No backend found in registry""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5324,Model.executeAsync() adds another not-disposed Tensor.,4,closed,2021-07-13T19:12:28Z,2021-07-19T20:03:10Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 20.04- TensorFlow.js installed from (npm or script link):  script link- TensorFlow.js version (use command below):- Browser version: 3.7.0- Tensorflow.js Converter Version: 3.7.0**Describe the current behavior**After executing this code `prediction=await Model.executeAsync(); `  Expecting `prediction.dispose()` to dispose all children; but `executeAsync` model seems to produce at least a single Tensor which isn't disposed by the Model and can't be disposed manually.**Describe the expected behavior**no tensor can't be disposed **Standalone code to reproduce the issue**Before  executing `Model.executeAsync()`; the `tf.memory().numTensors` is 128;  after executing `predictions=Model.executeAsync()`; the `tf.memory().numTensors` is 130;  and then execute `prediction.dispose()`; the expected  `tf.memory().numTensors` is 128;  but it is 129.```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>layer_test</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""> </script></head><body><script>console.log('start'; tf.memory().numTensors); // start 0var m;(async()=>{m=await tf.loadGraphModel(""http://qw7y9u2r4.hb-bkt.clouddn.com/model.json"");console.log('+ model'; tf.memory().numTensors); // + model 128const prediction = await m.executeAsync(tf.tensor2d(new Array(300).fill(34); [1;300];'float32'));console.log('+ executeAsync'; tf.memory().numTensors); // + executeAsync 130prediction.dispose();console.log('+ predict'; tf.memory().numTensors); // + predict 129m.dispose(); console.log('- model'; tf.memory().numTensors); // - model 1})();</script></body></html>```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.","['@liliquan0118 Your model seems to be layers model not a graph model. The code you shared does not seem to work. Can you create a working example on codepen or other site? thanks====='; '> @liliquan0118 Your model seems to be layers model not a graph model. The code you shared does not seem to work. Can you create a working example on codepen or other site? thanksThank you for your reply; Sorry there is a problem with the model link in the code; I re-updated the model; now the code can work normally.  the new code is:```<!DOCTYPE html><html lang=""en""><head>    <meta charset=""UTF-8"">    <title>layer_test</title>    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""> </script></head><body><script>console.log(\'start\'; tf.memory().numTensors); // start 0var m;(async()=>{m=await tf.loadGraphModel(""http://qw7y9u2r4.hb-bkt.clouddn.com/model.json"");console.log(\'+ model\'; tf.memory().numTensors); // + model 128const prediction = await m.executeAsync(tf.tensor2d(new Array(300).fill(34); [1;300];\'float32\'));console.log(\'+ executeAsync\'; tf.memory().numTensors); // + executeAsync 130prediction.dispose();console.log(\'+ predict\'; tf.memory().numTensors); // + predict 129m.dispose();console.log(\'- model\'; tf.memory().numTensors); // - model 1})();</script></body></html>```Or; you can run the following bug-5324.html file directly.[bug-5324.zip](https://github.com/tensorflow/tfjs/files/6813778/bug-5324.zip)====='; ""@liliquan0118 in your code; you have created a new tensor as input; you need to manually dispose it as shown below:```const input = tf.tensor2d(new Array(300).fill(34); [1;300];'float32');const prediction = await m.executeAsync(input);...prediction.dispose();input.dispose();```=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5324"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5324"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3"",
    ""specific_type"": ""A.3.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/5319,Trying to use ML5.js with Posenet,2,closed,2021-07-12T21:36:27Z,2021-07-15T00:42:20Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04):MacOs Big Sur- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link):script link- TensorFlow.js version:- CUDA/cuDNN version:**Describe the problem**I am trying to use two different tfjs libraries ml5.js and posenet in a single project. They both work in seperate projects; but I get errors building when they are used together. To run https://unpkg.com/ml5@latest/dist/ml5.min.js I need https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5; but that version of tfjs is incompatible with https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection and https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core.**Provide the exact sequence of commands / steps that you executed before running into the problem**Right now I am using the following to load a ml model:https://unpkg.com/ml5@latest/dist/ml5.min.js with https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5and in a different project the following to do pose detectionhttps://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection and https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core with https://cdn.jsdelivr.net/npm/@tensorflow/tfjsEach project works separately; but not together.**Any other info / logs**Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks; please include the full traceback. Large logs and files should be attached.When I run the pose detection project with https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.5 I get the following error:<img width=""665"" alt=""Screen Shot 2021-07-12 at 2 32 12 PM"" src=""https://user-images.githubusercontent.com/20036774/125358652-06f25180-e31e-11eb-975b-d89acafa1f3b.png"">","[""This question is better suited for https://github.com/ml5js/ml5-library/issues ; we will not be able to support ML5.js. One thing to mention is you can't use tfjs and tfjs-core together because tfjs is a union package which includes tfjs-core.So please use only one package. =====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5319"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5319"">No</a>=====']",Reference Error,Crash,Import Error,,,use one package,Changing version,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Dependency Version Mismatch""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.6
https://github.com/tensorflow/tfjs/issues/5301,error on mobile devices,7,closed,2021-07-08T12:22:53Z,2021-07-16T07:01:07Z,**System information**- OS Platform and Distribution: Android 11- Mobile device : Samsung galaxy S10; iphone 7- TensorFlow.js installed from npm:- TensorFlow.js 3.7.0:- tfjs-tflite.js 0.0.1-alpha.4:- Browser version: Chrome 91**Describe the current behavior**- tflite_web_api_cc_simd.js:9 Uncaught (in promise) RuntimeError: abort(undefined). Build with -s ASSERTIONS=1 for more info.    at abort (tflite_web_api_cc_simd.js:9)    at _abort (tflite_web_api_cc_simd.js:9)    at tflite_web_api_cc_simd.wasm:0x188bf    at tflite_web_api_cc_simd.wasm:0x1806fa    at tflite_web_api_cc_simd.wasm:0x181025    at tflite_web_api_cc_simd.wasm:0x181123    at tflite_web_api_cc_simd.wasm:0x26afd3    at tflite_web_api_cc_simd.wasm:0x28262f    at tflite_web_api_cc_simd.wasm:0x28268b    at tflite_web_api_cc_simd.wasm:0x33555**Standalone code to reproduce the issue**import { loadTFLiteModel; TFLiteModel; setWasmPath; getWasmFeatures } from '@tensorflow/tfjs-tflite';setWasmPath('tflite-wasms/');,"[""Hi @clancyGruver;Thanks for the report! I tried the following code on my Android 11 emulator with Chrome 91 (SIMD enabled) and it seems to be working fine ([screenshot](https://drive.google.com/file/d/1HRrbWqP9Dx83ddEeCQ0XJa7aKrKi_Gmz/view?usp=sharing)). Maybe there was more code after `setWasmPath` you didn't post? (looks like the error occurred when you were trying to load a model?).```js// Import @tensorflow/tfjs-tflite.import {getWasmFeatures; loadTFLiteModel; setWasmPath} from '@tensorflow/tfjs-tflite';setWasmPath('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/');async function start() {  console.log('start');  const wasmFeatures = await getWasmFeatures();  console.log(      'simd :' + wasmFeatures.simd +      '; multi-threading: ' + wasmFeatures.multiThreading);  // Load model runner with the cartoonizer tflite model.  await loadTFLiteModel(      'https://tfhub.dev/sayakpaul/lite-model/cartoongan/fp16/1';  );  console.log('model loaded');}start();```=====""; ""Hello; thanks for your answer.An emulator isn't the best solution for testing phones; because you still use your processor instead of phone's one.Here is a console screenshot for a mobile phone. It's Galaxy S10.The same code works perfectly on a desktop.![image](https://user-images.githubusercontent.com/14860440/125576570-22ef4c0e-c0e2-4f46-b2f4-2861adc63935.png)=====""; 'Thank you! You are right about the emulator. I will try to find an Android device to test this (works fine on my iPhone). From the error message; it looks like something inside the TFLite runtime. I will ask the team about this and report back if I get anything. Thanks!====='; 'Thank! It will be great.====='; ""I was able to use the browserstack's Real Device Cloud to run the same code on Galaxy S10 (with Android 9 and Chrome 89); but I didn't see the error you saw. I also tried Galaxy S21 with Android 11 and Chrome 91 and it seems to be working fine as well. Is it possible that the model is not downloaded successfully from tfhub? (or are you using a different tflite model?) Thanks!![Screen Shot 2021-07-15 at 11 39 06 AM](https://user-images.githubusercontent.com/8752427/125840348-96d0603f-63e4-4459-9a96-c9fd889a8d28.png)![Screen Shot 2021-07-15 at 11 33 52 AM](https://user-images.githubusercontent.com/8752427/125840361-9304bb2c-830c-4016-bd74-b19b9d0e625e.png)=====""; ""Thanks a lot for your answer. It's an awesome explanation. You're right I have used my own model. On cartoonize model from tfhub all works fine. It looks like something is wrong with my model.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5301"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5301"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,API Misuse,,,change model,Changing model,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.3
https://github.com/tensorflow/tfjs/issues/5255,Unsigned APK Not Working As Expected,5,closed,2021-06-28T08:21:28Z,2021-07-12T09:32:26Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**Describe the problem**Before you say this is a react native issue; I want to say that I was always able to get apk files that works as expected until tensorflow.   Apk is not working as it should have been when i build it by commands: npx react-native startnpx react-native run-android**Provide the exact sequence of commands / steps that you executed before running into the problem**What I did: npx react-native bundle --platform android --dev false --entry-file index.js --bundle-output android/app/src/main/assets/index.android.bundle --assets-dest android/app/src/main/rescd android && ./gradlew assembleDebugand as expected I found the apk in this path mobile-app/android/app/build/outputs/apk/debug/app-debug.apkHowever; this .apk file is not working like it's supposed to like when I run the commands npx react-native startnpx react-native run-androidPackage.json: ```{  ""name"": ""Myapp"";  ""version"": ""0.0.1"";  ""private"": true;  ""scripts"": {    ""android"": ""react-native run-android"";    ""ios"": ""react-native run-ios"";    ""start"": ""react-native start"";    ""test"": ""jest"";    ""lint"": ""eslint .""  };  ""dependencies"": {    ""@react-native-community/async-storage"": ""^1.12.1"";    ""@react-native-community/masked-view"": ""^0.1.11"";    ""@react-native-picker/picker"": ""^1.16.1"";    ""@react-navigation/bottom-tabs"": ""^5.11.11"";    ""@react-navigation/native"": ""^5.9.4"";    ""@react-navigation/stack"": ""^5.14.5"";    ""@teachablemachine/image"": ""^0.8.4"";    ""@tensorflow-models/mobilenet"": ""^2.1.0"";    ""@tensorflow/tfjs"": ""^3.7.0"";    ""@tensorflow/tfjs-core"": ""^3.7.0"";    ""@tensorflow/tfjs-react-native"": ""^0.5.0"";    ""axios"": ""^0.21.1"";    ""expo-av"": ""^9.1.2"";    ""expo-camera"": ""^11.0.3"";    ""expo-gl"": ""^10.3.0"";    ""expo-gl-cpp"": ""^10.3.0"";    ""expo-image-manipulator"": ""^9.1.0"";    ""lottie-react-native"": ""^4.0.2"";    ""react"": ""17.0.1"";    ""react-native"": ""0.64.2"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-gesture-handler"": ""^1.10.3"";    ""react-native-picker-select"": ""^8.0.4"";    ""react-native-reanimated"": ""^2.2.0"";    ""react-native-responsive-screen"": ""^1.4.2"";    ""react-native-safe-area-context"": ""^3.2.0"";    ""react-native-screens"": ""^3.4.0"";    ""react-native-shapes"": ""^0.1.0"";    ""react-native-unimodules"": ""^0.13.3""  };  ""devDependencies"": {    ""@babel/core"": ""^7.12.9"";    ""@babel/runtime"": ""^7.12.5"";    ""@react-native-community/eslint-config"": ""^2.0.0"";    ""babel-jest"": ""^26.6.3"";    ""eslint"": ""7.14.0"";    ""jest"": ""^26.6.3"";    ""metro-react-native-babel-preset"": ""^0.64.0"";    ""react-test-renderer"": ""17.0.1""  };  ""jest"": {    ""preset"": ""react-native""  }}```","['looking at the package.json I see you have both tfjs and tfjs-core ; the later will take care of installing all the dependencies of tfjs ; also did you check the steps here https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native ?====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'I deleted ""@tensorflow/tfjs-core"": ""^3.7.0"";   apk works but on the camera page (tensorflow\'s cameraWithSensors) is not providing any camera preview is a black screen and not recording video.====='; 'My bad; After setting permissions for camera; unsigned apk has started to work as expected. ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5255"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5255"">No</a>=====']",Reference Error,Crash,Import Error,,,delete import,Fix import confusion in program,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Unsigned APK Not Working""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.6
https://github.com/tensorflow/tfjs/issues/5223,tfnode.node.loadSavedModel() would not load after switching between models,3,open,2021-06-16T03:53:09Z,2021-06-22T01:28:09Z,running node v15.7.0; tfjs-node v3.3.0 on Mac Big SurThis is happening when switching between models.tfnode.node.loadSavedModel(model1); models gets loaded and working as expectedswitch to load model2 tfnode.node.loadSavedModel(model2); model2 gets loaded and working as expectedswitch back to model1; tfnode.node.loadSavedModel(model1); this time it won't load model1. it continues to use model2.,['In order to expedite the trouble-shooting process; please provide a codepen or GitHub repo to reproduce the issue here. Thanks! ====='; '@rthadur https://github.com/playground/demo-model-service/tree/ui-docker/publicyou can copy/paste model from model-old to model-new; it will stop loading on the 3rd rotation.  Is there a way to force reload or clear cache?====='; '@rthadur are you able to reproduce the issue?====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,empty cache,empty cache,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1"",
    ""specific_type"": ""D.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/5179,Error: Cannot evaluate flag 'DEBUG': no evaluation function found.,9,closed,2021-06-05T07:31:21Z,2021-06-19T11:41:23Z,**System information**- OS Platform and Distribution: **Windows 10**- Mobile device: **Mi 8 Lite running on Android 10**- TensorFlow.js installed from (npm or script link): **npm**- TensorFlow.js version: **3.7.0**Every time I tried to run the apps and import @tensorflow/tfjs; I got an error message like this in Metro.![image](https://user-images.githubusercontent.com/19814559/120883919-3d93c880-c60a-11eb-9ee8-f7fa3b9873ad.png)These are the dependencies I used (this is literally a fresh new React Native project created by react-native cli.![image](https://user-images.githubusercontent.com/19814559/120883943-6fa52a80-c60a-11eb-9165-c25a3e9cbb33.png)I ran into this problem every time I tried to run the apps; I thought by creating a new project and install tfjs first without installing any other library/dependencies will solve this; but it still the same.Any solution?,"['I am getting this error too on `3.7.0`. <img width=""1048"" alt=""Screen Shot 2021-06-06 at 6 26 36 PM"" src=""https://user-images.githubusercontent.com/11457183/120934125-f7fffa00-c6f4-11eb-8b95-4cfed2765eb3.png""><img width=""423"" alt=""Screen Shot 2021-06-06 at 6 30 06 PM"" src=""https://user-images.githubusercontent.com/11457183/120934201-457c6700-c6f5-11eb-9476-041fc2088bc3.png"">- ====='; 'is this solved ? I have same issue====='; 'I just have solved this issue; before you import tensorflowjs on react native . you MUST follow this configuration first https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native====='; '> I just have solved this issue; before you import tensorflowjs on react native . you MUST follow this configuration first https://github.com/tensorflow/tfjs/tree/master/tfjs-react-nativeI can make sure that I have followed those steps; and what steps did you do or you missed before?====='; 'I think I missed on react-native-fs configuration; but I forgot precisely. For the ease; I suggest you try to create new project and then follow those step first before you code your program and import tensorflow.js. It really worked for me. By the way; I use React Native CLI====='; ""I've tried that; even before you tell me. I'm using React Native CLI too.May I see your metro.config.js?=====""; '> I\'ve tried that; even before you tell me. I\'m using React Native CLI too.> May I see your metro.config.js?This is my **metro.config.js**: const { getDefaultConfig } = require(\'metro-config\'); module.exports = (async () => {   const defaultConfig = await getDefaultConfig();   const { assetExts } = defaultConfig.resolver;   return {     resolver: {       // Add bin to assetExts       assetExts: [...assetExts; \'bin\'];     }   }; })();And just in case; if you want to see my **package.json** :{  ""name"": ""MyApp"";  ""version"": ""0.0.1"";  ""private"": true;  ""scripts"": {    ""android"": ""react-native run-android"";    ""ios"": ""react-native run-ios"";    ""start"": ""react-native start"";    ""test"": ""jest"";    ""lint"": ""eslint .""  };  ""dependencies"": {    ""@react-native-community/async-storage"": ""^1.12.1"";    ""@tensorflow/tfjs"": ""^3.7.0"";    ""@tensorflow/tfjs-react-native"": ""^0.5.0"";    ""expo-camera"": ""^11.0.3"";    ""expo-file-system"": ""^11.0.2"";    ""expo-gl"": ""^10.3.0"";    ""expo-gl-cpp"": ""^10.3.0"";    ""jpeg-js"": ""^0.4.3"";    ""react"": ""17.0.1"";    ""react-native"": ""0.64.2"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-image-picker"": ""^4.0.3"";    ""react-native-unimodules"": ""^0.13.3"";    ""whatwg-fetch"": ""^3.6.2""  };  ""devDependencies"": {    ""@babel/core"": ""^7.12.9"";    ""@babel/runtime"": ""^7.12.5"";    ""@react-native-community/eslint-config"": ""^2.0.0"";    ""babel-jest"": ""^26.6.3"";    ""eslint"": ""7.14.0"";    ""jest"": ""^26.6.3"";    ""metro-react-native-babel-preset"": ""^0.64.0"";    ""react-test-renderer"": ""17.0.1""  };  ""jest"": {    ""preset"": ""react-native""  }}====='; ""Okay so I found the problem.I tried to merge the metro.config.js from tfjs installation instruction with the existing one; so it become like this```const { getDefaultConfig } = require('metro-config');module.exports = (async () => {  const defaultConfig = await getDefaultConfig();  const { assetExts } = defaultConfig.resolver;  return {    transformer: {      getTransformOptions: async () => ({        transform: {          experimentalImportSupport: false;          inlineRequires: true;        };      });    };    resolver: {      // Add bin to assetExts      assetExts: [...assetExts; 'bin'];    };  };})();```And it gives me the errors aboveBut when I change it into```const { getDefaultConfig } = require('metro-config');module.exports = (async () => {  const defaultConfig = await getDefaultConfig();  const { assetExts } = defaultConfig.resolver;  return {    resolver: {      // Add bin to assetExts      assetExts: [...assetExts; 'bin'];    };  };})();```It worked; no error shown.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5179"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5179"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,build/install configuration,Modifying dependency configuration,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Metro Configuration Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5167,undefined is not an object (evaluating 'env().platform.fetch') for poseNet in React Native,6,closed,2021-06-03T19:02:26Z,2021-06-09T04:11:53Z,**System information**- OS : Windows 10- Platform : Android - TensorFlow.js installed from npm - TensorFlow.js version:3.6.0- @tensorflow-models/tfjs-react-native version:0.5.0- @tensorflow-models/posenet  version: 2.2.2 **Describe the problem**I always get this response when I tried to `load()` the posenet from  `@tensorflow-models/posenet`. I am using React Native CLI not Expo.I installed and configured the packages as per the document.Is TensorFlow supported for React Native CLI or just Expo projects?,"['Yes React Native CLI is supported https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native#step-1-create-your-react-native-app ; you need to install all the expo dependencies even though you are not using the expo. Please refer a related issue  here https://github.com/tensorflow/tfjs/issues/4475====='; 'Thanks for your response @rthadur ====='; 'Can we have the . tflite file for poseNet bundled with the app ? If yes how do I get it. I can only download the .tflite file for it.  And in the docs they specify some path for .json and .bin  How would I generate .json and .bin from .tflite for pose estimation?====='; 'Sorry;I did not quite get it ; are you trying to use tflite models in tfjs ? if yes please refer here https://github.com/tensorflow/tfjs/blob/master/tfjs-tflite/README.mdcc @jinjingforever ====='; 'Thanks again @rthadur ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5167"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5167"">No</a>=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,install dependency,Modifying dependency configuration,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5143,Unhandled Rejection (Error): No backend found in registry on Tensorflow.js using mobilenet model and KNN classifier,1,closed,2021-05-30T11:46:07Z,2021-05-30T17:41:18Z,After I import into my project ReactJS `import * as mobilenet from '@tensorflow-models/mobilenet';``import * as knnClassifier from '@tensorflow-models/knn-classifier';`And later I create model mobilenet and knn classifier `  const classifier = knnClassifier.create();``const mobilenetModule = await mobilenet.load();`then it errors as image under.Screenshot: ![image](https://user-images.githubusercontent.com/70431419/120102719-98df3a00-c176-11eb-96a3-689e8d1a95d6.png),['I have fixed the bug already. Only need to reinstall the package ` mobilenet ` with version 2.0.4 and package ` tfjs ` version 1.6.0 are handled issue. ====='],Initialization Faliure,Build & Initialization Failure,Import Error,,,change framework version,Changing version,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.5""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/5115,React Native : Huge Memory Leak when using face-landmark-detection in estimateFaces() function,3,closed,2021-05-24T21:11:05Z,2021-06-06T20:28:27Z,"<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): No- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Linux Ubuntu 18.04- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Samsung Tablet S6- TensorFlow.js installed from (npm or script link): yarn add @tensorflow/tfjs- TensorFlow.js version (use command below): 3.6.0- Browser version: -- Tensorflow.js Converter Version: -**Describe the current behavior**Huge Memory Leak happening when calling estimateFaces() inside TensorCamera Loop(**Memory usage reach 3.5 GB in less than 1 Minute !!!!**)**Describe the expected behavior**No Memory Leak **Standalone code to reproduce the issue**```""dependencies"": {    ""@react-native-community/async-storage"": ""^1.12.1"";    ""@tensorflow-models/face-landmarks-detection"": ""^0.0.3"";    ""@tensorflow/tfjs"": ""^3.6.0"";    ""@tensorflow/tfjs-react-native"": ""^0.5.0"";    ""expo"": ""~41.0.1"";    ""expo-camera"": ""~11.0.2"";    ""expo-gl"": ""~10.2.0"";    ""expo-gl-cpp"": ""~10.1.0"";    ""expo-splash-screen"": ""~0.10.2"";    ""expo-status-bar"": ""~1.0.4"";    ""expo-updates"": ""~0.5.4"";    ""react"": ""16.13.1"";    ""react-dom"": ""16.13.1"";    ""react-native"": ""~0.63.4"";    ""react-native-fs"": ""^2.18.0"";    ""react-native-gesture-handler"": ""~1.10.2"";    ""react-native-reanimated"": ""~2.1.0"";    ""react-native-screens"": ""~3.0.0"";    ""react-native-unimodules"": ""~0.13.3"";    ""react-native-web"": ""~0.13.12""  };``````import { StatusBar } from 'expo-status-bar';import React; {useState; useEffect} from 'react';import { StyleSheet; Text; View } from 'react-native';import {Camera} from 'expo-camera';import * as tf from '@tensorflow/tfjs';import {cameraWithTensors; bundleResourceIO} from '@tensorflow/tfjs-react-native';import * as FaceModel from '@tensorflow-models/face-landmarks-detection';const modelJson = require('./assets/model.json');const modelWeights = require('./assets/group1-shard1of1.bin');export default function App() {  const [tensorReady; setTensorReady] = useState(false);  const [modelReady; setModelReady] = useState(null);  const TensorCamera = cameraWithTensors(Camera);  useEffect(()=>{    (async () => {      await tf.ready();      await tf.loadGraphModel(bundleResourceIO(modelJson; modelWeights));      setTensorReady(true);      setModelReady(await FaceModel.load(FaceModel.SupportedPackages.mediapipeFacemesh; {maxFaces: 1; shouldLoadIrisModel: true}));      console.log(tf.getBackend());      console.log(""READY!!!"");    })();  }; [])  const handleImage = (images; updatePreview; gl) => {              const loop = async () => {                let nextImageTensor = images.next().value;                const result = await modelReady.estimateFaces({input: nextImageTensor});                console.log(result);                // if autorender is false you need the following two lines.                // updatePreview(); gl.endFrameEXP();                requestAnimationFrame(loop);              };              loop();            };  return (    <View style={styles.container}>      <Text>Open up App.js to start working on your app!</Text>      <StatusBar style=""auto"" />      {(modelReady &&           <TensorCamera            style={[styles.camera; {height: 200; width: 200}]}            type={Camera.Constants.Type.front}            onReady={handleImage}            cameraTextureHeight={1080}            cameraTextureWidth={1920}            resizeHeight={200}            resizeWidth={152}            resizeDepth={3}            autorender={true}          />      )}    </View>  );}const styles = StyleSheet.create({  container: {    flex: 1;    backgroundColor: '#fff';    alignItems: 'center';    justifyContent: 'center';  };});```**Other info / logs** TensorFlow was loaded properly (with 'rn-webgl' backend engine)The Model is also loaded properly (Attached)Setup steps are following: https://www.npmjs.com/package/@tensorflow/tfjs-react-native[facemesh_1_default_1.tar.gz](https://github.com/tensorflow/tfjs/files/6535008/facemesh_1_default_1.tar.gz)","['@OckyKristanto-TomTom Please dispose the image tensors from TensorCamera.Please take a look at the example [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsx#L101)====='; '@pyu10055 . Indeed that dispose line solves the Memory Leak. Thanks a lot for the swift reaction and helpSuggestion: maybe its worth it to add the dispose line on the documentation: https://js.tensorflow.org/api_react_native/0.5.0/#cameraWithTensors====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5115"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/5115"">No</a>=====']",Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/4976,knn classifier fails with npm,2,closed,2021-04-23T09:31:16Z,2021-05-19T23:38:15Z,"**System information**- OS : Linux n-Lenovo-ideapad-320-15ISK 5.4.0-70-generic #78-Ubuntu SMP Fri Mar 19 13:29:52 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux- TensorFlow.js installed from (npm or script link): `npm install '@tensorflow/tfjs'`- TensorFlow.js version:                 ""@tensorflow-models/coco-ssd"": ""^2.2.2"";        ""@tensorflow-models/mobilenet"": ""^2.1.0"";        ""@tensorflow/tfjs"": ""^3.4.0"";        ""@tensorflow/tfjs-node"": ""^3.4.0"";- CUDA/cuDNN version: NA**Describe the problem**npm installation fails **Exact sequence of steps  executed before running into the problem**Have coco-ssd and mobilenet already installed and working smoothly also want to add knn classifier `npm i @tensorflow-models/knn-classifier`**Any other info / logs** npm i @tensorflow-models/knn-classifiernpm ERR! code ERESOLVEnpm ERR! ERESOLVE unable to resolve dependency treenpm ERR! npm ERR! Found: @tensorflow/tfjs-core@3.4.0npm ERR! node_modules/@tensorflow/tfjs-corenpm ERR!   peer @tensorflow/tfjs-core@""^3.3.0"" from @tensorflow-models/coco-ssd@2.2.2npm ERR!   node_modules/@tensorflow-models/coco-ssdnpm ERR!     @tensorflow-models/coco-ssd@""^2.2.2"" from the root projectnpm ERR!   peer @tensorflow/tfjs-core@""^3.1.0"" from @tensorflow-models/mobilenet@2.1.0npm ERR!   node_modules/@tensorflow-models/mobilenetnpm ERR!     @tensorflow-models/mobilenet@""^2.1.0"" from the root projectnpm ERR!   6 more (@tensorflow/tfjs; @tensorflow/tfjs-backend-cpu; ...)npm ERR! npm ERR! Could not resolve dependency:npm ERR! @tensorflow-models/knn-classifier@""*"" from the root projectnpm ERR! npm ERR! Conflicting peer dependency: @tensorflow/tfjs-core@1.7.4npm ERR! node_modules/@tensorflow/tfjs-corenpm ERR!   peer @tensorflow/tfjs-core@""^1.2.1"" from @tensorflow-models/knn-classifier@1.2.2npm ERR!   node_modules/@tensorflow-models/knn-classifiernpm ERR!     @tensorflow-models/knn-classifier@""*"" from the root projectnpm ERR! npm ERR! Fix the upstream dependency conflict; or retrynpm ERR! this command with --force; or --legacy-peer-depsnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.npm ERR! npm ERR! See /home/n/.npm/eresolve-report.txt for a full report.npm ERR! A complete log of this run can be found in:","['I guess there are duplicate dependencies in the package.json ; which needs to be resolved ; please check. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tfjs) since it is not a bug or feature request. There is also a larger community that reads questions there.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4976"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4976"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Import Error,,,delete dependency,Modifying dependency configuration,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.2] npm Package Installation Failure"",
    ""specific_type"": ""[C.2.1] Dependency Resolution Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.2] Dependency Error""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/4964,Browser hangs during model loading,3,closed,2021-04-21T15:40:33Z,2021-04-21T17:21:28Z,While initalizing `new Holistic(...)` model; browser hangs for 10+ seconds with following output```holistic_solution_wasm_bin.js:9 I0421 21:07:14.783000       1 gl_context_webgl.cc:149] Successfully created a WebGL context with major version 3 and handle 3put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 I0421 21:07:14.786000       1 gl_context.cc:348] GL version: 3.0 (OpenGL ES 3.0 (WebGL 2.0 (OpenGL ES 3.0 Chromium)))put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 W0421 21:07:14.790000       1 gl_context.cc:802] Drishti OpenGL error checking is disabledput_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 ERROR: Following operations are not supported by GPU delegate:put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 DEQUANTIZE: put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 577 operations will run on the GPU; and the remaining 0 operations will run on the CPU.put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 ERROR: Following operations are not supported by GPU delegate:put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 DEQUANTIZE: put_char @ holistic_solution_wasm_bin.js:9holistic_solution_wasm_bin.js:9 164 operations will run on the GPU; and the remaining 0 operations will run on the CPU.```Is there any way to load the model without making the browser hang?Thanks;Rakesh,['In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks! ====='; 'Hi; I used js solution api fromhttps://google.github.io/mediapipe/solutions/holistic.html with zerochanges.On Wed 21 Apr; 2021; 21:54 Rajeshwar Reddy T; ***@***.***>wrote:> In order to expedite the trouble-shooting process; please provide a code> snippet to reproduce the issue reported here. Thanks!>> —> You are receiving this because you authored the thread.> Reply to this email directly; view it on GitHub> <https://github.com/tensorflow/tfjs/issues/4964#issuecomment-824192926>;> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AK4GSXOMMKANTP6JLGHGQA3TJ334XANCNFSM43KTU2IA>> .>====='; 'This issue better asked in this repo https://github.com/google/mediapipe/issues it is not related to tfjs directly.====='],Browser Hangs,Poor Performance,Improper Model Attribute,,,change API,Replace API with another effective one,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.2""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4""
  }
}
```",B.1.2,C.2
https://github.com/tensorflow/tfjs/issues/4944,TypeError: t is not a function,2,closed,2021-04-16T13:11:27Z,2021-04-21T05:16:24Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js):``` javascript  let img = new Image();  img.width = 260;  img.height = 260;  img.src = captureEl.current.toDataURL('image/jpg');  model.predict(img2x(img));  // where error came// img2xexport const img2x = (imgEl: HTMLImageElement): tf.Tensor => {  return tf.tidy(() => {    const input = tf.browser.fromPixels(imgEl)      .toFloat()      .sub(255/2)      .div(255/2)      .reshape([1; 260; 260; 3]); // the model's inputShape    return input;  })}```- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MACOS 11.2.3- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: no- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):  3.3.0- Browser version: Chrome  89.0.4389.128- Tensorflow.js Converter Version:  no**Describe the current behavior**![截屏2021-04-16 下午9 06 32](https://user-images.githubusercontent.com/33031512/115028663-b5415300-9ef7-11eb-8665-df1afc992fc1.png)**Describe the expected behavior**The model will predict the image and return an index of classes.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.,"[""Already resolve this problem. It's caused by two version of `tensorflow.js`'s import conflict.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4944"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4944"">No</a>=====']",Reference Error,Crash,Import Error,,,use one package,Changing version,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.2] Function Inaccessible""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.6] Import Error""
  }
}
```",A.1,A.6
https://github.com/tensorflow/tfjs/issues/4935,Unable to load TensorFlow Lite models from memory,2,closed,2021-04-15T06:17:53Z,2021-04-26T20:32:44Z,**System information**- `@tensorflow/tfjs-tflite ^0.0.1-alpha.1`**Describe the feature and the current behavior/state.**The `@tensorflow/tfjs-tflite` package currently only provides a `loadTFLiteModel()` function which accepts a URL to load a TensorFlow Lite model from. We already have the model in memory (as an `ArrayBuffer`) and were hoping to avoid the need for a round trip to the server.**Will this change the current api? How?**We noticed a `TFLiteWebModelRunner.CreateFromBufferAndOptions()` constructor and were hoping it could be exposed to the end user in some form. The overall signature may look something like this:```tsexport function loadTFLiteModelFromBuffer(model: ArrayBuffer; options?: TFLiteWebModelRunnerOptions): Promise<TFLiteModel>;```This would be a purely additive change and just re-exporting functionality that is already used inside `loadTFLiteModel()`.**Who will benefit with this feature?**Any user who wants to use `@tensorflow/tfjs-tflite` in their apps without needing to fetch the model from a remote location. This provides benefits for:- Privacy and Intellectual Property (keeping the model on the end user's device means it is less likely to be accessed without authorization)- Latency/Performance - no need to do a round trip to a server to download the model if you've already got a version locally- Convenience - you can use `@tensorflow/tfjs-tflite` without needing to provision; maintain; and pay for a server- Dependency injection/Purity - What if I want to retrieve the model via something other than HTTP?,['cc @pyu10055====='; 'Hi; Please try version `0.0.1-alpha.2` which supports passing arraybuffer to `loadTFLiteModel`:https://github.com/tensorflow/tfjs/blob/master/tfjs-tflite/src/tflite_model.ts#L279Thank you! (and sorry for the delay)====='],Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4898,loadLayersModel can't load model from a Server that assigns a Token or Key to the Files,1,closed,2021-04-02T11:07:00Z,2021-04-02T15:09:56Z,loadLayersModel function takes in a URL to model.json file. It then updates the URL to load bin files holding weights of the model by replacing model.json with the bin file name. It works fine until the files are saved on a server that assigns a token or key to the files URL; in my case; it was Firebase Storage.So the URL of my model.json file looks like this:https://firebasestorage.googleapis.com/v0/b/project-foo.com/o/model%2Fmodel.json?alt=media&token=***************Now; the loadLayersModel() is trying to load the group1-shard1of1.bin file using the URL:https://firebasestorage.googleapis.com/v0/b/project-foo.com/o/model%2Fgroup1-shard1of1.bin?alt=media&token=***************Do you see the problem? Token didn't get replaced with the token of bin file AKA the URL is incorrect.,"[""I found out we don't need TOKEN in the URL to the model.json; when I removed the TOKEN the error was still there because the URL misses folderName%2F before group-shard1of3.bin file(s). You can just edit that in model.json in the Paths. Just add %2F before the names of the bin files.Can't believe it took me so long to figure it out. All I needed was a little break from the bug. Well that's a lesson. Take Breaks guys!=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.3] Fetch Failure"",
    ""specific_type"": ""[A.3.1] URL Format Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.3,C.1
https://github.com/tensorflow/tfjs/issues/4847,Unknown op 'TensorListFromTensor'.,2,closed,2021-03-21T07:40:24Z,2021-03-22T21:50:17Z,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md); we only address code/doc bugs; performance issues; feature requests and build/installation issues on GitHub. tag:build_template</em>**System information**- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Codesandbox - Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device:- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version: 1.7.4- CUDA/cuDNN version:**Describe the problem**I found a problem that my models worked in Python; but I did not work in TFJS. My steps were below:Step 1) Create modelI created the object detection model with my custom dataset and `efficientdet-d0`. You can see more details from my Colab belew:https://colab.research.google.com/drive/1fjnVbDnHnGnILuvDTv7zAwUwIMntrC7p?usp=sharing Step 2) convert saved_model to web_model```bashtensorflowjs_converter \    --input_format=tf_saved_model \    --output_format=tfjs_graph_model \    --signature_name=serving_default \    --saved_model_tags=serve \    saved_model \    web_model```Step 3) import it on my Code sandbox but it didn't workhttps://codesandbox.io/s/object-detection-with-tensorflow-js-forked-6r45k?file=/src/index.js| Python | JS ||----|----||<img src=""https://user-images.githubusercontent.com/4177529/111897340-bdc27d00-8a62-11eb-97bb-d4992da89134.png""> | <img src=""https://user-images.githubusercontent.com/4177529/111897414-37f30180-8a63-11eb-875a-781058dfbd53.png""> |**Any other info / logs**When I replaced my model with Kangaroo-detector from [this blog post](https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html)...And it worked?! I have no idea what makes this kinda difference.<img width=""1243"" alt=""스크린샷 2021-03-21 오후 4 39 04"" src=""https://user-images.githubusercontent.com/4177529/111897590-1ba39480-8a64-11eb-8f51-97e19b6b9989.png"">","['I see you are using older version ; please use latest version(3.X) as this op was added here https://github.com/tensorflow/tfjs/pull/3432. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4847"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4847"">No</a>=====']",Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4846,[rn-tfjs] TensorCamera memory leak on iOS,1,open,2021-03-21T05:35:14Z,2021-07-08T23:12:38Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- I used the code from the camera demo to extract the pose : https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsx- - OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOS - Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iOS- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below): 2.7.0**Describe the current behavior**When I compile my app with expo; and display the performance monitor; I observe that every-time the app is fast refreshed; the RAM increases of 20MB.  So I start around 300 MB and after several fast refresh I reach +600 MBThe memory leak comes from the TensorCamera; since I don't observe any RAM increase when I comment it out. RAM increases steadily even when TensorCamera doesn't receive any prop.**Describe the expected behavior**No increase of the RAM between two fast refresh. **Standalone code to reproduce the issue**https://github.com/tensorflow/tfjs/blob/master/tfjs-react-native/integration_rn59/components/webcam/realtime_demo.tsxthen```expo start```and then fast refresh by saving several times the same file.,['@valentinchelle  I was just curious if you had faced any issue on running the realtime_demo.tsx file. I have some error executing it https://github.com/tensorflow/tfjs/issues/5304====='],Memory Leak,Poor Performance,Incorrect Code Logic,,,memory management,Add API usage for memory management,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.2"",
    ""specific_type"": ""B.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",B.2.1,A.4
https://github.com/tensorflow/tfjs/issues/4841,Cannot read property 'backend' of undefined when running the face-landmarks-detection in a comlink web worker,7,closed,2021-03-19T17:01:59Z,2021-04-04T20:46:28Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): no- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): Windows 10- TensorFlow.js installed from (npm or script link): 3.3.0- Browser version: Chrome 89.0.4389.90**Describe the current behavior**I ran into an issue building a snapchat filter clone app thing. I've setup an example project of the same issue I had. I'd like to run the model in a web worker. When I move my landmark face detection code to a comlink web worker; I get following issue:![image](https://user-images.githubusercontent.com/27679518/111815707-482ca480-88dc-11eb-9ded-3b53617a5659.png)**Describe the expected behavior**It should work as if it ran in the main thread.**Standalone code to reproduce the issue**codesandbox: https://codesandbox.io/s/github/driescroons/tensorflow-comlink-issue (gives an error on the loader; so I suggest you use the gh link)github link: https://github.com/driescroons/tensorflow-comlink-issueIn following diff you can see I just move the code from the already working app to the comlink webworker:https://github.com/driescroons/tensorflow-comlink-issue/commit/1c239d4a14406c825e90e17e4e3f39a2a3c316ecLet me know if you need anything else!,"['Thanks for the reproduction code ; I could not run the code as I was getting below error ;![image](https://user-images.githubusercontent.com/43972606/112201899-fd59a800-8bcd-11eb-99ad-b6bb3870c351.png)====='; 'as mentioned; you need to check the gh repo. The comlink-loader does not seem to work within csb.====='; 'any update on this? Let me know if you need any more examples / help.====='; 'Sorry Dries; I have been busy with other projects; but I will take a look at this issue some time this week. Thanks for providing the sample project.====='; 'I think the problem is the usage of [`Comlink.proxy(video)`](https://github.com/driescroons/tensorflow-comlink-issue/blob/main/src/Landmark.js#L34). It is not possible to pass the video stream like this from the main thread to webworker. The stream data needs to be serialized in some form which can then be passed to webworker. You can try drawing the stream to a canvas; get the ImageData; and pass the ImageData to webworker (face landmarks detection supports ImageData as input). Check out [this demo](https://codepen.io/oceangermanique/pen/LqaPgO) for some ideas (give it camera permission). Also check out [this post](https://stackoverflow.com/questions/12746053/pass-large-amounts-of-data-between-web-worker-and-main-thread/27242554#27242554) for some performance advice.(I tested using face landmark detection in a webworker with ImageData as input; and it seems to work fine)Hope it helps!====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4841"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4841"">No</a>====='; ""Thanks for the reply @jinjingforever. I've tried what you suggested and got it to work. Seems like rendering the video to an offscreen canvas and passing the imageData to my webworker is a bit slower than running everything in the main thread though. I should do some benchmarking.For anyone else; here are the changes I've made:https://github.com/driescroons/tensorflow-comlink-issue/compare/feat/imagedata =====""]",Reference Error,Crash,API Misuse,,,change data,change data,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4815,BodyPix - drawBokehEffect stretched background on Safari Browsers (MacOS - iOS),2,closed,2021-03-16T15:11:18Z,2021-03-16T17:49:39Z,reopening issue #4808,"['Thank you @guillaume250 ; original issue is reopened https://github.com/tensorflow/tfjs/issues/4808 ; will close this. Thank you ====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4815"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4815"">No</a>=====']",Incorrect Functionality,Incorrect Functionality,Unknown,,,,,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.2] Browser Incompatibility""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/4808,BodyPix - drawBokehEffect stretched background on Safari Browsers (MacOS - iOS),4,open,2021-03-12T03:48:44Z,2021-03-16T16:23:46Z,**System information**- Wrote custom code and tried stock examples.- macOS Big Sur 11.1- Safari iOS browser- TensorFlow.js installed from (npm and script link): 1.2 for script link & tfjs-core 3.1.0 npm version- Browser version: Safari Version 14.0.2 (16610.3.7.1.9)- Tensorflow.js Converter Version: 3.1.0**Describe the current behavior**On Safari browsers when blurring the background of video tracks coming from a `navigator.mediaDevices.getUserMedia` stream; the blurred background seems stretched. Assuming this is a Safari only issue; I think an aspect ratio fix on the [cpuBlur](https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/blur.ts) function should do it. 😉**Describe the expected behavior**drawBokehEffect expected behavior **Standalone code to reproduce the issue**Opening the codepen below in Safari should immediately reproduce the bug.https://codepen.io/guillaume250/pen/ZEBVPaG- Cheers,"[""Hi @guillaume250. Thanks for the codepen reproduction! `drawBokehEffect` [bases its output dimensions on the video element's `width` and `height` properties](https://github.com/tensorflow/tfjs-models/blob/master/body-pix/src/output_rendering_util.ts#L503-L504) (although perhaps it shouldn't). If you set the video element's width and height to match the camera's aspect ratio; the output should have the correct aspect ratio. See [my fork of your codepen](https://codepen.io/mattsoulanille/pen/eYBxvqM?editors=1111) as an example. Feel free to re-open if this doesn't solve the issue.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4808"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4808"">No</a>====='; 'Hi @mattsoulanille; thanks for the quick response; and the fork.I just opened your fork in a safari browser and still see the same issue.<img width=""913"" alt=""Screen Shot 2021-03-15 at 1 36 47 PM"" src=""https://user-images.githubusercontent.com/26730301/111196976-4a1cfd80-8594-11eb-9bab-1e707f1d37ad.png""> <img width=""912"" alt=""Screen Shot 2021-03-15 at 1 37 31 PM"" src=""https://user-images.githubusercontent.com/26730301/111197107-73d62480-8594-11eb-9774-2f8a433b56f7.png"">====='; ""I initially thought your report was about the incorrect aspect ratio of the output image; but I've been able to reproduce the issue you're seeing on Safari. I'll take a look. Thanks!=====""]",Incorrect Functionality,Incorrect Functionality,Unknown,,,,,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.4"",
    ""specific_type"": ""D.4.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/4800,After setting TensorflowJS model's backend as WASM is too slow,10,closed,2021-03-10T11:23:27Z,2021-04-12T04:52:49Z,It's an **LSTM model.** The model total size is **23.2 MB**(model.json; group1.bin; ...).### Before setting a backend as wasm; the prediction runtime is <1500ms; after setting wasm >5000ms.I use this set of code to change the backend as wasm.```setWasmPaths({	'tfjs-backend-wasm.wasm': 'assets/wasm/tfjs-backend-wasm.wasm';	'tfjs-backend-wasm-simd.wasm': 'assets/wasm/tfjs-backend-wasm-simd.wasm';	'tfjs-backend-wasm-threaded-simd.wasm': 'assets/wasm/tfjs-backend-wasm-threaded-simd.wasm'});setBackend('wasm').then(() => {...}```**Is it possible to fix this issue?**TensorflowJS - 3.2.0Angular - 11.2.3Brave - 1.21.73 Chromium - 89.0.4389.72OS - ubuntu 20.04.2,"['In order to expedite the trouble-shooting process; please provide a code snippet to reproduce the issue reported here. Thanks! ====='; '### I used this code snippet for wasm.```import { loadLayersModel; LayersModel; Tensor; squeeze; setBackend } from \'@tensorflow/tfjs\';import { setWasmPaths } from ""@tensorflow/tfjs-backend-wasm"";private model: LayersModel;private topValues: number = 5;private minPredictionValue = 0.7;constructor() {    setWasmPaths({        \'tfjs-backend-wasm.wasm\': \'assets/wasm/tfjs-backend-wasm.wasm\';        \'tfjs-backend-wasm-simd.wasm\': \'assets/wasm/tfjs-backend-wasm-simd.wasm\';        \'tfjs-backend-wasm-threaded-simd.wasm\': \'assets/wasm/tfjs-backend-wasm-threaded-simd.wasm\'    });        setBackend(\'wasm\').then(() => {        this.loadModel();    }}private loadModel(): void {    const modelURL = ""assets/model/model.json"";        loadLayersModel(modelURL).then(        model => {            this.model = model;        };        error => alert(error)    );}private predict(input: string): number[] {    const predictedValue = this.model.predict(input);    const predictedIndex = this.decodePrediction(predictedValue);    return predictedIndex;}private decodePrediction(predictedValue: Tensor): number[] {    const {values; indices} = squeeze(predictedValue; [0]).topk(this.topValues);        const valueArray = values.dataSync();    const indexArray = indices.dataSync();        let output: number[] = [];        for(let i = 0; i < this.topValues; i++) {            if(valueArray[i] > this.minPredictionValue) {            output[i] = indexArray[i];        } else {            break;        }    }        return output; }```### Before using wasm; I used this code snippet.```...constructor() {    this.loadModel();}...```====='; 'is it possible to share the model ? or similar model which we can use for reproduction ?====='; '[I modified the model from this](https://keras.io/examples/generative/lstm_character_level_text_generation/); other processes are the same as this. [JS model is here](https://drive.google.com/drive/folders/1kvcKukV6fEjgB-J_HsOCwnXI8N-YXHIW?usp=sharing)====='; 'Any updates? @rthadur====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; 'Any updates? @rthadur ====='; '@sivarajakani Quick question: is the default backend (before you set it) WebGL? If so; you are probably just seeing the difference in performance between the two backends on your machine. As an example: on my machine; WASM is often about 10x slower than WebGL for a MobileNetV2 prediction; but for someone who uses my project; WASM performs faster than WebGL. So it can vary a lot based on the setup.====='; ""**is the default backend (before you set it) WebGL?**I think CPU is the default backend in TensorFlowJS; but I'm not sure about that. So I tried CPU and WebGL. WebGL runtime same as the default runtime; and the CPU runtime is greater than the default runtime. From these results; I confirmed WebGL is the default backend in my project.Thank you; @wingman-jr-addon.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4800"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4800"">No</a>=====']",Slow Execution,Poor Performance,Misconfiguration,,,change backend,changing backend,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""B"",
    ""subcategory"": ""B.1"",
    ""specific_type"": ""B.1.1""
  },
  ""root_cause"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.1""
  }
}
```",B.1.1,B.1
https://github.com/tensorflow/tfjs/issues/4778,Error when load model using tf.loadLayersModel,10,open,2021-03-04T18:29:06Z,2021-03-26T16:51:33Z,"when run the code `(async function () {  console.log(""wating for model load!"");  const model = await tf.loadLayersModel(""simple/model.json"");  console.log(""model: ""; model);})();`**I got error**Uncaught (in promise) Error: Input 0 is incompatible with layer flatten_3: expected min_ndim=3; found ndim=2.    at new e (errors.ts:48)    at e.assertInputCompatibility (topology.ts:790)    at topology.ts:985    at rh (common.ts:43)    at e.apply (topology.ts:978)    at e.add (models.ts:500)    at e.fromConfig (models.ts:948)    at Rp (generic_utils.ts:277)    at cd (serialization.ts:31)    at models.ts:300I use script tag verson 1.0.0 OR 2.0.0 same error `<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js""></script>`How can fix it ??","['Please use latest version 3.2.0 and let us know ; if that does not help ; please share reproducible code in codepen or in a GitHub repo. Thank you====='; 'this is my code on github https://github.com/AmmarYasir29/tensorflowJSI try the last verson that you tell me ans the same error: > Uncaught (in promise) Error: Input 0 is incompatible with layer flatten_3: expected min_ndim=3; found ndim=2.>     at t.n.assertInputCompatibility (topology.js:790)====='; '@rthadur any help ??====='; 'i guess the model file is missing ; is it fine to share the model also ; thank you ====='; 'I convert model from .h5 format to .json to loadembedded it to web by`console.log(""wating form model load!"");``const model = await tf.loadLayersModel(""http://localhost:3000/fjs/model.json"");`But the error appear [IMG Error](https://ibb.co/QPP5GGz) and I cant fix that The mdoel in h5 format [the model h5](https://github.com/missdafer/the_Simpsons_classification_project/blob/acb254dd3fbaa4a4c24d7ad00bcf2d5080687530/ResNet50_model.h5)and the [json format ](https://drive.google.com/drive/folders/1RVCzN9Ycx5hE6JiYb0bZaLaq3aStsQ8V?usp=sharing)====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '@rthadur the error still appear and the model not load Is there solution ?  ====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '@lina128 can you help me ? ====='; ""if you are loading the model from local folder you need to use below code `model = await tf. loadLayersModel('model/model.json');`=====""]",Data & Model Error,Crash,API Misuse,,,change model path,Modifying model file path/extension,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.2"",
    ""specific_type"": ""A.2.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.2,A.3
https://github.com/tensorflow/tfjs/issues/4751,How to build tfjs locally ?,6,closed,2021-02-26T04:26:57Z,2021-03-11T16:43:17Z,**System information**- OS Platform and Distribution: Linux Ubuntu 18.04**Describe the problem**- I modified some files under directory of `tfjs` and `tfjs-backend-webgl` and I wanna build it. Is there a script to build  `tfjs`  ?- Do I have to add a new webpack and add some rules there? Thanks,"['not an official way; but my quick hack:```bashgit clone --depth 1 https://github.com/tensorflow/tfjscd tfjsfind . -maxdepth 1 -type d | while read d; do  pushd $PWD  cd $d  if [ -f package.json ]; then    yarn install --ignore-optional --ignore-engines  fi  popddonecd ./tfjsyarn run build-depsyarn run buildyarn run build-npmcd ../tfjs-nodeyarn run build-npmcd ../tfjs-node-gpu./prep-gpu.shyarn run build-npmcd ../tfjs-backend-webgpuyarn run build-npm```====='; '@vladmandic  Thanks for your reply. I followed above commands. Error happens when  `yarn  run build-deps` is executed. I think this is something related to node or tsc; but I\'m not quite sure. By the way; I am building tfjs-v2.8.4 tag version.--- error message: ```bash/home/xlpiao/tfjs/tfjs:$ yarn run build-depsyarn run v1.22.5$ yarn build-core && yarn build-layers && yarn build-converter && yarn build-data && yarn build-backend-cpu && yarn build-backend-webgl$ cd ../tfjs-core && yarn && yarn build[1/5] Validating package.json...[2/5] Resolving packages...success Already up-to-date.$ node ./scripts/enumerate-tests.js && tsc && yarn bundle../../node_modules/@types/eslint/index.d.ts:260:41 - error TS2694: Namespace \'""/home/xlpiao/tfjs/tfjs-core/node_modules/@types/estree/index""\' has no exported member \'ChainExpression\'.260         ChainExpression?: (node: ESTree.ChainExpression & NodeParentExtension) => void;                                            ~~~~~~~~~~~~~~~../../node_modules/@types/eslint/index.d.ts:283:42 - error TS2694: Namespace \'""/home/xlpiao/tfjs/tfjs-core/node_modules/@types/estree/index""\' has no exported member \'ImportExpression\'.283         ImportExpression?: (node: ESTree.ImportExpression & NodeParentExtension) => void;                                             ~~~~~~~~~~~~~~~~Found 2 errors.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.error Command failed with exit code 2.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.```====='; 'Hi Xianglan;I checked out tfjs-v2.8.4 but didn\'t see any issues when running ""yarn build-deps"" from tfjs/tjfs. It is a little bit strange to see that the error is in `tfjs-core/node_modules/@types/eslint`; because I don\'t actually see it on my machine. Here is everything under `tfjs-core/node_modules/@types`:```color-nameestreejasminelongminimatchnodenode-fetchoffscreencanvasresolveseedrandomwebgl-ext```Is it possible that this might be caused by some new code you wrote locally? Thanks! ====='; '@jinglescode Thanks for you reply. I put `tfjs` under some other project and that might be the reason for the building error. Since I moved tfjs out of that project directory; above commands works well. Issue resolved. ====='; 'Thank you for confirming ; closing this issue.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4751"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4751"">No</a>=====']",Build & Install Failure,Build & Initialization Failure,Misconfiguration,,,change framework position,,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.1] TF.js/JS Application Compile Failure"",
    ""specific_type"": ""[C.1.1] Build Script Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.1] Multi-environment Misconfiguration""
  }
}
```",C,B.1
https://github.com/tensorflow/tfjs/issues/4724,Built in image decoder for note js.,9,closed,2021-02-22T12:52:29Z,2021-04-27T22:36:34Z,**System information**- Operating system: macOS- TensorFlow.js version (you are using): 2.8.6- Are you willing to contribute it (Yes/No): No**Describe the feature and the current behaviour/state.**A built-in image decoder for node js. At the moment image needs to be decoded through node-canvas; which is extremely slow for real-time detection.```  if(data && data.body) {    const img = new Image()    img.src = data.body;    const canvas = createCanvas(320; 180);    const ctx = canvas.getContext('2d');    const input = tf.browser.fromPixels(canvas);    try {      predictions = await model.estimateSinglePose(input; {        flipHorizontal: false      });    } catch(e) {      console.log(e);    }  }```**Will this change the current api? How?**Yes; it'll add another method to decode the image.**Who will benefit with this feature?**Anybody who to use Posenet without sacrificing the client frame rate.**Any Other info.**,"['Decode image for Node is already implemented here https://github.com/tensorflow/tfjs/blob/master/tfjs-node/src/image.ts#L155 ; please verify====='; 'So I\'ve tried the following code;`CLIENT````    const imageData = context.getImageData(0; 0; 320; 180);    const buffer = imageData.data.buffer;    socket.emit(""signal""; buffer); //Pass it to the server through websocket````BACKEND````socket.on(""signal""; (data)=> {\u3000\u3000const buffer = new Uint8Array(data);\u3000\u3000const imgData = ts.node.decodeImage(buffer); //error throwns here.})```on the backend; I\'ve tried to decode the buffer; but this error was thrown out.```throw new Error(\'Expected image (BMP; JPEG; PNG; or GIF); but got unsupported \' +        ^Error: Expected image (BMP; JPEG; PNG; or GIF); but got unsupported image type    at getImageType (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-node/dist/image.js:351:15)    at Object.decodeImage (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-node/dist/image.js:196:21)    at Socket.socket.on (/Users/xxx/app/server/app.js:37:29)    at Socket.emit (events.js:182:13)    at Socket.emitUntyped (/Users/xxx/app/server/node_modules/socket.io/dist/typed-events.js:69:22)    at process.nextTick (/Users/xxx/app/server/node_modules/socket.io/dist/socket.js:428:39)    at process._tickCallback (internal/process/next_tick.js:61:11)```Is there something I\'ve missed?====='; ""you do not need to explicitly convert to Unit8Array ; here is the sample code from one of the [SO questions ](https://stackoverflow.com/questions/65429456/cant-open-jpg-or-png-after-saving-it-using-node-js-fs-and-http)`saveImageToDisk(imageurl;filepath)            const img_buffer = fs.readFileSync(filepath)            const img = tf.node.decodeImage(img_buffer)            coco.load().then(model => {                // detect objects in the image.                model.detect(img).then(predictions => {                    console.log('Predictions: '; predictions);                });              });`=====""; '@rthadur thanks; but I have a different use case. The reason I need the Unit8Array is that I need to pass the webcam image to the server.Please check my original post.====='; 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.====='; '@bebensiganteng when you use the getImageData() call; it returns the pixel values of the image; which means it is decoded; you can use it directly to create the tensors for model execution.====='; '@pyu10055 could you elaborate; please? And did you actually tried it? because it threw me this error.`CLIENT````const imageData = context.getImageData(0; 0; 320; 180);socket.emit(""signal""; imageData); //Pass it to the server through websocket````BACKEND````  socket.on(""signal""; async (data)=> {    if(model) {      try {        const pose = await model.estimateSinglePose(data; {          flipHorizontal: false        });        console.log(pose);      } catch(e) {        console.log(e);      }    }  })``````Error: Tensor must have a shape comprised of positive integers but got shape [;;3].   at assert (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:335:15)   at /Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:813:9   at Array.forEach (<anonymous>)   at assertNonNegativeIntegerDimensions (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:812:11)   at makeTensor (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:4081:9)   at tensor3d (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:7445:12)   at fromPixels_ (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:7588:12)   at Object.fromPixels__op [as fromPixels] (/Users/xxx/app/server/node_modules/@tensorflow/tfjs-core/dist/tf-core.node.js:3986:29)   at toInputTensor (/Users/xxx/app/server/node_modules/@tensorflow-models/posenet/dist/util.js:211:60)   at /Users/xxx/app/server/node_modules/@tensorflow-models/posenet/dist/util.js:247:27```====='; 'you need to create a tensor from the data; with something as following:height = 180; width=320; channels=4 ```    const temp = tf.tensor(new Uint8Array(data); [height; width; channels]);    const image = tf.slice(temp; [0; 0; 0]; [-1; -1; 3]);    temp.dispose();        const pose = await model.estimateSinglePose(image; {          flipHorizontal: false    });    console.log(pose);```====='; 'It worked! thank you! `CLIENT````          const imageData = refCam.current.ctx.getImageData(0; 0; 320; 180);          const buffer = imageData.data.buffer;          socket.emit(""signal""; buffer);````BACKEND````    let pose = []    if(model) {      const buffer = new Uint8Array(data);      const temp = tf.tensor(buffer; [HEIGHT; WIDTH; CHANNELS]);      const image = tf.slice(temp; [0; 0; 0]; [-1; -1; 3]);      temp.dispose();      pose = await model.estimateSinglePose(image; {        flipHorizontal: false      });    }    socket.emit(""messageAll""; pose);```@pyu10055 If you don\'t mind could explain what `tf.slice `means; please?=====']",Reference Error,Crash,API Misuse,,,change API,Replace API with another effective one,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Incorrect Input Shape""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.4] Incorrect Code Logic""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4596,How to read modelWeights.bin from file system,19,open,2021-01-25T05:29:16Z,2021-01-28T04:27:08Z,As it is raised before; I am not able to load model as below. It gives `TypeError: Network request failed `error.  `const model = await tf.loadLayersModel('file://path/to/my-model/model.json');`I need to read model.json and model_weights.bin manually and bundle them together. I am able to read model.json without any issues but I am having troubles to get right type for bin file. ```const rootPath = Platform.OS === 'ios' ? RNFS.MainBundlePath : RNFS.DocumentDirectoryPath;const modelWeights = await RNFS.readFile(`${rootPath}/model_weights.bin`; 'base64');```I am not sure how to convert modelWeights (string) into number to use `bundleResourceIO(modelJson: io.ModelJSON; modelWeightsId: number)`I saw suggestions to convert base64 string into Uint8Array but I couldn't find a way to convert it into 'number' type.```const buffer = tf.util.encodeString(modelWeights; 'base64').buffer;const arr = new Uint8Array(buffer);```Any suggestions how to bundle them together or any other ways loading model from native file system? PS: Statically bundle works without any issues. ```import modelJson from './assets/model.json';import modelWeights from './assets/model_weights.bin';const model = await tf.loadGraphModel(bundleResourceIO(modelJson; modelWeights));```**Standalone code to reproduce the issue**```import React from 'react';import {Button; View} from 'react-native';import * as tf from '@tensorflow/tfjs';import {bundleResourceIO} from '@tensorflow/tfjs-react-native';import modelJson from './assets/model/model.json';import modelGroup from './assets/model/group1-shard1of1.bin';import RNFS from 'react-native-fs';class Home extends React.Component {  triggerModel = async () => {    await tf.ready();    //Successful    const model = await tf.loadGraphModel(      bundleResourceIO(modelJson; modelGroup);    );    //model.json and group1-shard1of1.bin exists in file system    //returns model.json file successfully    const modelJson1 = await RNFS.readFile(      `${RNFS.DocumentDirectoryPath}/model/model.json`;    );    //Throws TypeError: Network request failed    const model1 = await tf.loadGraphModel(modelJson1; {      weightUrlConverter: async (weightFileName) => {        return `file://${RNFS.DocumentDirectoryPath}/model/group1-shard1of1.bin`;      };    });    //Throws TypeError: Network request failed    const model2 = await tf.loadGraphModel(      `${RNFS.DocumentDirectoryPath}/model/model.json`;    );  };  render() {    return (      <View>        <Button onPress={() => this.triggerModel()}>Trigger Model</Button>      </View>    );  }}export default Home;```,"['@kscgl Please update if you get any solution on this.I am working on something similar.====='; 'Also; I have a similar question. That is how to load a model with two different links to json and weights file. If anybody here knows how to do it; do let me know.Thanks.====='; 'There is an optional argument in the loadGraphModel called `weightUrlConverter` which can be used to modified the weights path. It worked for me. Hope this helps.====='; ""Thank you @nischal-sanil for suggestion. I tried as below but I got `TypeError: Network request failed````const modelLoad = await tf.loadGraphModel(modelJson; {            weightUrlConverter:              'file://' +              RNFS.DocumentDirectoryPath +              '/model/group1-shard1of1.bin';          });```=====""; ""@kscgl You should pass an async function to the `weightUrlConverter`. This function will receive a `weightFileName: string` as input and should return path to the weights. If you path is correct something like this should work.```const modelLoad = await tf.loadGraphModel(modelJson; {            weightUrlConverter: async function (weightFileName) => {              return 'file://' +              RNFS.DocumentDirectoryPath +              '/model/group1-shard1of1.bin';          }});```=====""; ""@nischal-sanil I am getting the same network request failed error. ```  const modelLoad = await tf.loadGraphModel(modelJson; {            weightUrlConverter: this.getWeightFile(              'model/group1-shard1of1.bin';            );          });  getWeightFile = async (weightFileName) => {    return `file://${RNFS.DocumentDirectoryPath}/${weightFileName}`;  };```=====""; 'Do not call the function. By calling it you are again passing a string as an input. Try something like this.```  const modelLoad = await tf.loadGraphModel(modelJson; {            weightUrlConverter : async (weightFileName) => {                  return `file://${RNFS.DocumentDirectoryPath}/${weightFileName}`;          }});```Tensorflowjs will call this function behind the scenes.====='; '@nischal-sanil I see. I tried that one but same error. ```const modelLoad = await tf.loadGraphModel(modelJson; {            weightUrlConverter: async (weightFileName) => {              return `file://${RNFS.DocumentDirectoryPath}/model/group1-shard1of1.bin`;            };          });```====='; 'hmmm; Not sure why this is the case then. If the path is right it should have worked. May be you can try another optional argument called `weightPathPrefix` instead of `weightUrlConverter`. Check out the [docs](https://js.tensorflow.org/api/latest/#loadGraphModel)====='; ""@nischal-sanil That won't work either because it always give `TypeError: Network request failed` wherever I provide a location from file system. =====""; ""Aren't you using a local server?=====""; '@nischal-sanil For what purpose? I provide the path of phone file system itself. ====='; '@kscgl can you please share minimal code for reproduction ? if you are loading local file system ; you should not see above error; are you trying to load using script tags ?====='; '@rthadur I updated the initial post to add standalone code. Please let me know if you need more details.  ====='; ""@HarshalRohit I decided to go with [asyncStorageIO handler](https://js.tensorflow.org/api_react_native/0.2.1/#asyncStorageIO) until I figure this out. ```...const modelName = 'customModelName';await model.save(asyncStorageIO(modelName));//Save modelName somewhere. I use local database...//Fetch your model name then call your modelconst modelName = fetchModelName();const model = await tf.loadGraphModel(asyncStorageIO(modelName));...```=====""; '@kscgl Thanks!!====='; 'I am getting **Error** `Row too big to fit into CursorWindow requiredPos=0; totalRows=1` on calling `tf.LoadLayersModel`.@kscgl How much is your model size (as returned by  `model.save(asyncStorageIO(modelName))` ? Mine is 6 MB.@jinjingforever I understand this is due sqlite limits; but is there any workaround for this?? ====='; ""@HarshalRohit Mine is around 1 MB. It's probably due to Android AsyncStorage limit which is 6 MB. You can look into how to increase AsyncStorage size and try if that works. =====""; ""@kscgl Increasing the AsyncStorage size doesn't work.I tried increasing the size and `model.save` works fine but`model.load` throws the error `Row too big to fit into CursorWindow requiredPos=0; totalRows=1`.I guess I will have to write custom IO handler.=====""]",Fetch Failure,Crash,Data/Model Inaccessibility,,,parameter modifier,Modify API Parameter usage,web application,Model Loading,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",A.3,C.1
https://github.com/tensorflow/tfjs/issues/4593,[tfjs-react-native] [iOS only] Initialization of backend rn-webgl failed [native code],5,closed,2021-01-24T17:29:12Z,2021-08-13T19:33:38Z,**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): iOS 14.3 Simulator / Device- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: iPhone 11```yarn list v1.22.10├─ @tensorflow/tfjs-automl@1.0.0├─ @tensorflow/tfjs-backend-cpu@2.7.0├─ @tensorflow/tfjs-backend-webgl@2.7.0├─ @tensorflow/tfjs-converter@2.7.0├─ @tensorflow/tfjs-core@2.7.0├─ @tensorflow/tfjs-data@2.7.0├─ @tensorflow/tfjs-layers@2.7.0├─ @tensorflow/tfjs-react-native@0.5.0└─ @tensorflow/tfjs@2.7.0```**Describe the current behavior**On launch; when I tf.ready(); I get the warning Initialization of backend rn-webgl failed**Describe the expected behavior**rn-webgl platform loads**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.```// index.jsimport '@tensorflow/tfjs-react-native'``````// App.tsximport * as tf from '@tensorflow/tfjs'...const [tfReady; setTfReady] = useState(false)useEffect(() => {  tf.ready()  // <-- this is when the warning happens    .then(() => setTfReady(true))}; [])if (!tfReady) {  return null}// render app```**Other info / logs** Include any logs or source code that would be helpful todiagnose the problem. If including tracebacks; please include the fulltraceback. Large logs and files should be attached.```yarn list v1.22.10├─ expo-asset@8.2.1├─ expo-camera@9.0.0├─ expo-constants@9.2.0├─ expo-file-system@9.2.0├─ expo-font@8.3.0├─ expo-gl-cpp@9.1.2├─ expo-gl@9.1.1├─ expo-image-loader@1.2.0├─ expo-linking@2.0.0│  └─ expo-constants@9.3.5├─ expo-location@9.0.1├─ expo-media-library@9.2.1├─ expo-network@2.4.0├─ expo-permissions@9.3.0├─ expo-splash-screen@0.8.1└─ expo-store-review@2.3.0```All of these dependencies are configured and working on their own. WebGL initializes and works properly on Android emulator and device. This is an issue I'm encountering only for iOS; both on simulator and device.,"[""Standing up a new project with `npx create-react-native-app` and doing the minimum dependency installations allowed me to get the rn-webgl backend loading. Closing the issue for now until I can figure out why it's not working in my app.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4593"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4593"">No</a>====='; 'Did you ever figure out what caused it? ====='; '@Cannonball2134 > Did you ever figure out what caused it?No idea; but creating a new RN project and copying over all my code fixed it====='; 'Hi @zholmes1; I face the same error like you did. From your case; I kinda found out a trick which can be a temporary solution for this problem.I manually install @tensorflow/tfjs-backend-wasm and run `pod install`. The problem fixed by itself.After a few successful build; the error show up again. I uninstall @tensorflow/tfjs-backend-wasm and run `pod install`. The problem fixed again. @Cannonball2134 hopefully this is still helpful for you=====']",Initialization Faliure,Build & Initialization Failure,Import Error,,,recreate project,recreate project,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure"",
    ""specific_type"": ""[C.3.1] Backend Initialization Error""
  },
  ""root_cause"": {
    ""primary_category"": ""[D] Execution Environment Error"",
    ""subcategory"": ""[D.1] Device Incompatibility""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/4497,No Backend registry,4,closed,2021-01-05T22:25:10Z,2021-01-07T16:05:59Z,it says no backend registry heres the code; the error log and i even put the package.json in as well<h3>What I was doing</h3>I was trying to use the qna model but i keep getting the same error message and i've got no clue what im doing wrong.<br /> I also am using the wikipedia article found here https://en.wikipedia.org/wiki/Google about google <br />and then in the second argument I have a question about the article.<br>there are only 2 inputs on it.<hr />![image](https://user-images.githubusercontent.com/74203498/103705893-b1994a00-4f60-11eb-9700-e9854fea7157.png)![image](https://user-images.githubusercontent.com/74203498/103706022-f1f8c800-4f60-11eb-9966-96c4c6f923cd.png),"['@SkellyM386 Please; fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please; share colab link or simple standalone code in editable format  with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!====='; '> @SkellyM386 > > Please; fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).> Please; share colab link or simple standalone code in editable format  with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!Im sorry i might be misunderstanding you but i think i included the package.json where you can see the dependencies that i used and then the whold code context in the first picture along with the console error.And then in the ask function the first argument is:Google LLC is an American multinational technology company that specializes in Internet-related services and products; which include online advertising technologies; a search engine; cloud computing; software; and hardware. It is considered one of the Big Five technology companies in the U.S. information technology industry; alongside Amazon; Facebook; Apple; and Microsoft.And then the second argument is ""what does google specialize in.""====='; ""If you are using qna model with tfjs 2.x you need to import a backend at the top of your program. In your case I would add the following to the top of the program `import '@tensorflow/tfjs-node';`=====""; ""> If you are using qna model with tfjs 2.x you need to import a backend at the top of your program. In your case I would add the following to the top of the program `import '@tensorflow/tfjs-node';`Thank you!=====""]",Initialization Faliure,Build & Initialization Failure,Import Error,,,add import,Fix import confusion in program,web application,Environment Integration,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[C] Build & Initialization Failure"",
    ""subcategory"": ""[C.3] Multi-backend Initialization Failure""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.3] API Misuse""
  }
}
```",C,A.6
https://github.com/tensorflow/tfjs/issues/4458,[Codelab]: Making Predictions from 2D Data,3,closed,2020-12-29T22:31:21Z,2021-01-08T21:38:13Z,"In the Extra Credit section... Given the info from the codelab I am not able to ""bend"" the test prediction graph as it is on the codelab's example picture. I have tried different combinations of adding more units and layers e.g.:```javascriptfunction createModel() {    // Create a sequential model    const model = tf.sequential();    // Add a single input layer    model.add(tf.layers.dense({ inputShape: [1]; units: 1000; useBias: true }));    model.add(tf.layers.dense({ units: 50; activation: 'sigmoid' }));    model.add(tf.layers.dense({ units: 50; activation: 'sigmoid' }));    model.add(tf.layers.dense({ units: 50; activation: 'sigmoid' }));    model.add(tf.layers.dense({ units: 50; activation: 'sigmoid' }));    // Add an output layer    model.add(tf.layers.dense({ units: 1; useBias: true }));    return model;}```but all I got was:![flat](https://user-images.githubusercontent.com/17248853/103318010-4e258e80-4a2d-11eb-8334-d15ca7d1c6a9.png)Could you please explain how to achieve this? AFAIK adding more layers should do the job but I obviously am missing sth...","[""Increase the number of epochs you are training for. For example set it to 150 epochs and see what happens. However you results look like a horizontal line so I don't know if there is another bug in your code. Here is a link to a version with a solution that produces a more curved result. https://glitch.com/edit/#!/tfjs-training-one?path=script.js%3A89%3A17=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4458"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4458"">No</a>====='; 'Thanks. Works now. The only difference was the number of epochs - it MUST be over 100.Fyi. your glitch example uses only 50 epochs and if you retry it couple of times; you may see a result similar to mine. This is from your glitch:<img width=""516"" alt=""Screenshot 2021-01-08 at 21 52 36"" src=""https://user-images.githubusercontent.com/17248853/104066611-f98acc00-5201-11eb-986c-fc2a60102f4c.png"">Cheers!=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,parameter modifier,Modify API Parameter usage,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4444,Managing channels in tf.node.encodeJpeg,2,closed,2020-12-24T06:09:38Z,2021-01-27T08:45:18Z,"I am using [`tf.node.encodeJpeg`](https://js.tensorflow.org/api_node/1.3.0/#node.encodeJpeg) to convert my model output tensor to an image; however it seems like `tf.node.encodeJpeg` is unable to properly define the color channels. Here is the example code I use and the difference in outputs:```js// make a predictionlet outputTensor = await model.predict({ 'input_1': input });const endTime = tf.util.now();console.log(endTime - startTime);//specifying the output nodeoutputTensor = outputTensor.add_171;//convert [1;512;512;3] to [512;512;3] to the save imageoutputTensor = outputTensor.squeeze();outputTensor = await tf.node.encodeJpeg(outputTensor);fs.writeFileSync(""ouptut.jpeg""; outputTensor);```Here is the image I receive compared to the output from Python execution and it seems like the color channels are messed up. Could you help me out with this and if I am missing something here?|Output|Python Execution||---|---||![Output](https://user-images.githubusercontent.com/39672672/103065045-9121d500-45db-11eb-880c-305ad78b6ac4.Jpeg)|![Python Execution](https://user-images.githubusercontent.com/39672672/103065300-49e81400-45dc-11eb-8262-a1cbc02c96bb.PNG)|",['Hey; @lina128 any update on this issue?====='; 'Thanks.I finally seem to have solved this after a pretty long debug session. As I understood while doing some of the pixel values were not in the expected range so I ended up writing an algorithm to transform these values (a collection of some basic tensor ops) followed by a filter to clip the values in expected intervals and that seems to work really well.====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data preprocess,Add data processing,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4433,div error when tfjs-backend-wasm=2.8.1,1,open,2020-12-21T08:34:55Z,2021-02-16T21:10:32Z,<em>Please make sure that this is a bug. As per our[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md);we only address code/doc bugs; performance issues; feature requests andbuild/installation issues on GitHub. tag:bug_template</em>**System information**- Have I written custom code : yes - OS Platform and Distribution : macOS 10.14.6- TensorFlow.js installed from : npm - TensorFlow.js version : tfjs-backend-wasm 2.8.1- Browser version: Chrome 87.0.4280.88- Tensorflow.js Converter Version:**Describe the current behavior**use div in wasm backend;it didn't work.**Describe the expected behavior**it will work**Standalone code to reproduce the issue**open `chrome://flags/#enable-webassembly-simd` in chrome;and enable `WebAssembly SIMD support.` and `WebAssembly threads support`  tfjs-backend-wasm 2.6.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.7.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.7.0 with `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.0 without `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.0 with `WebAssembly SIMD support.` and `WebAssembly threads support`  works tfjs-backend-wasm 2.8.1 without `WebAssembly SIMD support.` and `WebAssembly threads support`  work error tfjs-backend-wasm 2.8.1 with `WebAssembly SIMD support.` and `WebAssembly threads support`  work error```javascriptvar point1= tf.tensor2d([[-0.6070004; 0.881     ; -0.0700001];     [-0.5930004; 0.8440001 ; -0.069    ];     [-0.5609999; 0.8100002 ; -0.064    ]])var N=tf.tensor1d([0.015242; -0.054074; -0.3919158])point1.dot(N).abs().div(N.norm())```Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.**Other info / logs** Include any logs or source code that would be helpful to```logUncaught RuntimeError: abort(Assertion failed: Cannot call unknown function RealDiv; make sure it is exported). Build with -s ASSERTIONS=1 for more info.    at abort (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:13285)    at assert (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6125)    at getCFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6211)    at ccall (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:6765)    at eval (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/wasm-out/tfjs-backend-wasm-threaded-simd.js:9:7337)    at kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/dist/kernels/binary_kernel.js:52:34)    at Object.kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-backend-wasm/dist/kernels/binary_kernel.js:55:13)    at kernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:455:30)    at eval (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:519:27)    at Engine.scopedRun (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:348:25)    at Engine.runKernelFunc (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:517:14)    at Engine.runKernel (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/engine.js:406:21)    at div_ (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/ops/div.js:65:59)    at div__op (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/ops/operation.js:49:28)    at Tensor._tensor__WEBPACK_IMPORTED_MODULE_1__.Tensor.div (webpack-internal:///./node_modules/@tensorflow/tfjs-core/dist/public/chained_ops/div.js:24:64)    at eval (eval at ransacRodeSegment (webpack-internal:///./src/views/tool/editor/ai/pointcloud/rodeSegment.js); <anonymous>:1:23)```,"[""Could you please provide the version numbers of other tfjs packages you're using on this page (e.g. tfjs-core; tfjs)? I suspect this has to do with a version mismatch (earlier versions of tfjs-core do not export RealDiv).=====""]",Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Data Processing,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4406,Model is not predicting well after loading from localStorage,1,closed,2020-12-15T09:44:07Z,2020-12-18T14:28:20Z,"I have build model with the following structure in **Node JS** using **TFJS@1.7.0**``` let model = tf.sequential({      layers: [        tf.layers.dense({inputShape: [34]; units: 16; activation: 'relu'});        tf.layers.dense({units: 3; activation: 'softmax'});      ]    });    model.compile({      optimizer: tf.train.adam(0.02);      loss: 'categoricalCrossentropy';      metrics: ['accuracy']    });```Model is giving >90% accuracy. Model has been tested and it is predicting well according to it's accuracy.I saved model in localStorage with **model.save(""localstorage://model"")**.But when i load model with **model.loadLayersModel(""localstorage://model"")** in different page;The performance of model has got down to 50%; it is predicting like **non-trained models**.Any wrong in saving or loading model.",['Model saving and Loading only works in a single page. If moved from one page to another within the same domain; the performance of model decreases to 50% if load from localStorage.====='],Incorrect Functionality,Incorrect Functionality,Unknown,,,,,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""C"",
    ""subcategory"": ""C.2""
  }
}
```",D,E
https://github.com/tensorflow/tfjs/issues/4405,Uncaught TypeError: webglBackend.incRef is not a function for tfjs-modes/face-landmarks-detection,7,closed,2020-12-15T04:57:26Z,2020-12-15T18:42:28Z,"**System information**package.json dependencies:```  ""dependencies"": {    ""@tensorflow-models/face-landmarks-detection"": ""^0.0.2"";    ""@tensorflow/tfjs"": ""^2.8.0"";    ""@tensorflow/tfjs-backend-webgl"": ""2.4.0"";    ""@tensorflow/tfjs-converter"": ""2.4.0"";    ""@tensorflow/tfjs-core"": ""2.4.0"";    ""p5"": ""^1.1.9"";    ""stats.js"": ""^0.17.0""  };```**Describe the current behavior**When importing another .js file that imports '@tensorflow/tfjs' it throws the error:```Uncaught TypeError: webglBackend.incRef is not a function    at reshape (Reshape.ts:50)    at Object.oneHot [as kernelFunc] (OneHot.ts:36)    at kernelFunc (engine.ts:576)    at engine.ts:634    at Engine.scopedRun (engine.ts:439)    at Engine.runKernelFunc (engine.ts:631)    at Engine.runKernel (engine.ts:508)    at oneHot_ (one_hot.ts:58)    at Object.oneHot__op (operation.ts:51)    at Object.parcelRequire.wailaModel.js.@tensorflow/tfjs (wailaModel.js:14)```These warnings are also displayed before the error is thrown:![image](https://user-images.githubusercontent.com/32425272/102172729-58f5f600-3e67-11eb-8db7-138ccf03e13b.png)**Standalone code to reproduce the issue**Here are the imports to the modified demo file from tfjs-models/face-landmarks-detection/demo/index.js:```import {load; SupportedPackages} from '@tensorflow-models/face-landmarks-detection';import '@tensorflow/tfjs-backend-webgl'import * as tf from '@tensorflow/tfjs-core';import someObject from './someFileThatImportsTfjs'```And here are the imports in someFileThatImportsTfjs:`import * as tf from '@tensorflow/tfjs'`","['Similar issue has been reported here https://github.com/tensorflow/tfjs/issues/4309 ; will leave it to @annxingyuan for further comments; thank you ====='; ""@rthadur Thanks for the response. Just to give some more information:The demo worked out of the box for me; but the problem occurs when I add `import someObject from './someFileThatImportsTfjs'` in demo/index.js=====""; 'One potential issue is that you are importing the backends twice with two different versions.This package:```""@tensorflow/tfjs"": ""^2.8.0"";```Includes copies of these packages (but at version 2.8.0):```""@tensorflow/tfjs-backend-webgl"": ""2.4.0"";""@tensorflow/tfjs-converter"": ""2.4.0"";""@tensorflow/tfjs-core"": ""2.4.0"";```Since you are already using ""@tensorflow/tfjs"": ""^2.8.0""; you can remove the others from your package json and just `import @tensorflow/tfjs` in your project.====='; ""@tafsiri Okay I see. I removed those redundant packages and the previous error does not get thrown. However this gets thrown:```Uncaught (in promise) Error: method must be bilinear or nearest; but was undefinedassert                                                            \tutil_base.ts:108cropAndResize_                                                    \tcrop_and_resize.ts:85cropAndResize__op                                                 \toperation.ts:51y.startPoint                                                      \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tengine.ts:442scopedRun                                                         \tengine.ts:453tidy                                                              \tengine.ts:440tidy                                                              \tglobals.ts:192(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17(anonymous function)                                              \tface-la…-detection.esm.js:17s                                                                 \tface-la…-detection.esm.js:17Async call from Promise.thensetUp                                                             \tindex.js:61Async call from async functionsetUp                                                             \tindex.js:47main                                                              \tindex.js:76parcelRequire.index.js.@tensorflow-models/face-landmarks-detection\tindex.js:170newRequire                                                        \teye_tra…_main.e31bb0bc.js:47(anonymous function)                                              \teye_tra…_main.e31bb0bc.js:81(anonymous function)                                              \teye_tra…_main.e31bb0bc.js:120```My imports in demo/index.js are now:```import {load; SupportedPackages} from '@tensorflow-models/face-landmarks-detection'import * as tf from '@tensorflow/tfjs';import someObject from './someFileThatImportsTfjs'```I am trying to follow the stack trace; but it is a little difficult with face-landmarks-detection.esm.js. I'm not too familiar with Parcel; but it seems like I do not have sourcemapping set up properly?=====""; 'Hi @scottzockoll - this is indeed a separate issue; which we are working on a fix for: https://github.com/tensorflow/tfjs/pull/4407====='; '@annxingyuan Great! Thanks for the information. I will close this since it is a duplicate.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4405"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4405"">No</a>=====']",Reference Error,Crash,Import Error,,,use one package,Changing version,web application,Model Inference,"```json
{
  ""bug_symptom"": {
    ""primary_category"": ""[A] Crash"",
    ""subcategory"": ""[A.1] Reference Error"",
    ""specific_type"": ""[A.1.1] DL Operator Exception""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.5] Incompatibility between 3rd-party DL Library and TF.js""
  }
}
```",A.1,A.6
https://github.com/tensorflow/tfjs/issues/4339,Incorrect predictions for a known precise model (React Native),2,closed,2020-12-02T20:19:12Z,2020-12-02T22:20:12Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): React Native 0.63.3- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: Galaxy S20+- TensorFlow.js installed from (npm or script link): npm- TensorFlow.js version (use command below):- Tensorflow.js Converter Version: 2.7.0""@tensorflow/tfjs"": ""^2.7.0"";""@tensorflow/tfjs-react-native"": ""^0.5.0"";**Describe the current behavior**Graph model is predicting incorrectly; but consistently. If I take a picture of a black background; it outputs index 120 with ~.5 confidence. If I take a picture of anything not black; it outputs index 1034 with confidence of 1; and all other indexes are 0.**Describe the expected behavior**Correct predictions. I know this is possible because I uploaded images in the google cloud vision console and this same model was predicting accurately.**Standalone code to reproduce the issue**Provide a reproducible test case that is the bare minimum necessary to generatethe problem. If possible; please share a link to Colab/CodePen/any notebook.Load model from assets on launch. I did change my metro config to allow loading of .bin assets.```useEffect(() => {  tfReady()    .then(async () => {      return loadGraphModel(        bundleResourceIO(          require('../../../../assets/model.json');          require('../../../../assets/model_weights.bin')        )      )    })    .then(setModel)    .catch(e => console.error(e))}; [])```After base64 comes in from camera```const raw = Uint8Array.from(toByteArray(base64)) // from 'base64-js' npm packageconst imageTensor = decodeJpeg(raw)  .expandDims(0)  .resizeBilinear([224; 224]; false)const output = model.predict(imageTensor)// @ts-ignoreconst data: number[] = output.dataSync()console.log(data.length)````data.length` is the correct number of classes. But it always contains incorrect data as I stated in the section above.I know this model works because after I trained it on Google Cloud AutoML; I was able to upload images (that the model had never seen) of all different classes and it was predicting correctly with high confidence.","['Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4339"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4339"">No</a>====='; 'It must have been something to do with the way I was processing my image. I switched to using the `ImageClassificationModel` from @tensorflow/tfjs-automl and it started classifying the images properly.=====']",Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,change model,Changing model,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4335,[Question] Performance Gap in using SavedModels in TFJS,4,closed,2020-12-02T10:13:00Z,2020-12-04T02:25:49Z,To try out a basic version of a use-case I am working on. I use a Mobilenet based model with TFJS and observe a huge performance gap than the original SavedModel. I make use of [this model](https://tfhub.dev/google/aiy/vision/classifier/food_V1/1) to test out a basic version of the use-case.I converted the SavedModel to TFJS without any compression techniques and use the TFJS Converter to do so. I use these few lines of code to test out the model-```jsasync function run() {  const MODEL_URL = 'no-comp/model.json';  const model = await tf.loadGraphModel(MODEL_URL);  const img = document.getElementById('img');  let inp = tf.browser.fromPixels(img; 3)    .resizeNearestNeighbor([192; 192])    .expandDims()    .toFloat();  const result = model.predict(inp).data();  console.log(result)}```This; as expected; gives me an array of shape `[1;2024]` and I would ideally be picking up the label with the highest confidence but this gives me a lot of mispredictions whereas running the original `SavedModel` gives a lot better performance. This also seems odd to me since I have used TFJS multiple times and not faced these kinds of issues.,['@Rishit-dagli can you share what version of TFJS you are using and on what type of devices you are seeing this degrade of accuracy? Also; if you can share the models (both TF saved model and tfjs) with us to reproduce this problem would be great.====='; '@pyu10055 I am using the latest version of TFJS; as of now I tried running the model on multiple desktops and am facing the same problem with all the devices I run on. Here is the [SavedModel](https://storage.googleapis.com/rishit-dagli/tfjs%234335/saved-model/saved_model.pb) and the [TFJS model](https://storage.googleapis.com/rishit-dagli/tfjs%234335/tfjs/tfjs.zip) which I have added as a `zip`.====='; '@Rishit-dagli your model is quite straightforward; can you share your python code on how to execute the model?For mobilenet model; it is common that the input needs to be normalized; your js code is not doing that; can you verify that?====='; 'To try out a basic version of a use-case I am working on I make use of [this model](https://tfhub.dev/google/aiy/vision/classifier/food_V1/1) since this on TF Hub itself I make use of the embed on the model website to test the `SavedModel`.Thank you for pointing that out; I was using this as an example code so I used a readily available Mobilenet inference script with TFJS and simply changed the pixel values since the normalizing steps were built-in. Thank you for pointing out that I missed the normalization steps; a small mistake from my end. I use-```js.sub(offset).div(offset)```to normalize the input image and that solves it. ====='],Incorrect Functionality,Incorrect Functionality,Incorrect Code Logic,,,add data preprocess,Add data processing,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",D,A.4
https://github.com/tensorflow/tfjs/issues/4316,Unknown op TensorListReserve,2,closed,2020-11-28T02:56:07Z,2020-11-30T21:23:09Z,"Hello;I created a LSTM model; converted it using the CLI and serving the same using node http-server.Below is the code that attempts to run prediction from within html code and below error was shown on the console. Can you pls provide guidance?PS: Earlier; I tried model.predict and it recommended using executeAsync.<img width=""1074"" alt=""Screen Shot 2020-11-27 at 8 53 52 PM"" src=""https://user-images.githubusercontent.com/10361685/100492532-adbd1100-30f2-11eb-80c1-ab6f0f9fe670.png""><img width=""989"" alt=""Screen Shot 2020-11-27 at 8 49 40 PM"" src=""https://user-images.githubusercontent.com/10361685/100492535-b7467900-30f2-11eb-9503-03cc7ff3fbd7.png"">",['@sahas- Looks like you are using an old version of TFJS (tfjs@2.0.0); TensorListReserve op should have been supported; can you upgrade to the latest version of the TFJS (2.7.0)? thanks====='; 'It works with 2.7.0. Thanks.====='],Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.1] Inconsistency between Backends/Platforms/Devices"",
    ""specific_type"": ""[D.1.1] Unimplemented Operator""
  },
  ""root_cause"": {
    ""primary_category"": ""[A] Incorrect Programming"",
    ""subcategory"": ""[A.1] Unimplemented Operator""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4312,Use tfjs-react-native with posenet and bundleResourceIO,6,closed,2020-11-26T21:02:08Z,2021-12-11T17:02:21Z,I want to build an react native app with the use of posenet. A requirement is; that it works offline.so the examples work fine. when i do:```import * as posenet from '@tensorflow-models/posenet'this.model = await posenet.load();``` so what i did next is converting the model to one weigth file and load the model locally with:```const modelJson = require('./assets/model/original/model-stride32.json');const modelWeights = require('./assets/model/original/weigth.bin');    this.model = await tf.loadGraphModel(   bundleResourceIO(modelJson; modelWeights));```so what i want to do next is to run something like this:```const predictions = await this.model.estimateSinglePose(imageTensor)```but of course this doesn't work because the model doesn't know the function. i know i can instanciate the model by using something like this:```const checkpointLoader = new posenet.CheckpointLoader(checkpoint.url);const variables = await checkpointLoader.getAllVariables();const model =  new posenet.MobileNet(variables; checkpoint.architecture);```but the checkpoint loader needs an url. i want to use the local files.does anyone has a hint for me how to use posenet without rewriting the whole node-module?thx in advance,['Try using the [modelUrl param](https://github.com/tensorflow/tfjs-models/tree/c37b1193358f12cc6726262285ad3d6b0f1046a0/posenet#config-params-in-posenetload) of posenet but passing the IO handler rather than a string; it would look something like this```const net = await posenet.load({  modelUrl: bundleResourceIO(modelJson; modelWeights)});```====='; 'Hey @tafsiri;thx alot; it is working now. You saved my day and my project.====='; '@dabass51 could you tell me where you got the modelJson and modelWeights files? Thanks :)====='; '> @dabass51 could you tell me where you got the modelJson and modelWeights files? Thanks :)You can check on Tensorflow Hub; this [link](https://tfhub.dev/s?deployment-format=tfjs&q=posenet) has all the available models which you can download. ====='; '@dabass51 I want to achieve pose estimation in my React Native app can you provide me with link to some docs or instructions on how to add tfjs and poseet into a React Native app for android. I was able to set up tfjs by installing necessary dependencies inside my React Native project. Can you provide something to move forward? Thanks====='; 'Could anyone say me how can I use the model bodypix in react native? ====='],Reference Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""D"",
    ""subcategory"": ""D.2"",
    ""specific_type"": ""D.2.1""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4280,model.save(url) | ValueError: Cannot find any save handlers for URL,5,closed,2020-11-20T00:47:38Z,2021-09-07T19:58:49Z,"**System information**- OS Platform: Mac OSX 10.14.6- tfjs: 2.7- tfjs-node: 2.7**Standalone code to reproduce the issue**```const serverAddr = ""http://localhost:5001""model = await tf.loadLayersModel(serverAddr + '/get-model')model.compile({    optimizer: tf.train.adam();    loss: 'categoricalCrossentropy';    metrics: ['accuracy']  });model.train(...)await model.save(serverAddr + '/upload-model')    // ValueError occurs```A flask server is setup with endpoints /get-model and /upload-model. The /get-model endpoint provides an appropriate json; and so the model compile and trains properly. However; when trying to save the model to my Flask server; I receive the following error.`ValueError: Cannot find any save handlers for URL 'http://localhost:5001/upload-model' at new ValueError (/Users/colinpeppler/tfplayground/node_modules/@tensorflow/tfjs-layers/dist/tf-layers.node.js:186:28)`. Running the code on the browser works good when saving to the url. I also tried saving to the filesystem and that worked fine; but the HTTP request method doesn't work with node.","['cc @pyu10055 @tafsiri there is a similar issue for saving in file system that got resolved [here](https://github.com/tensorflow/tfjs/issues/723). seems to be bug in tfjs-layers ====='; 'A workaround in the meantime is to use https://js.tensorflow.org/api/latest/#io.http directly and pass that to tf.loadLayersModel.====='; ""I ended up just saving the model to the file directory and creating a request with multipart/form-data encoding. See [this](https://www.tensorflow.org/js/guide/save_load#https_request)for how tfjs constructs the request's body.=====""; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4280"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4280"">No</a>====='; 'This is a very common issues; most of the tfjs-examples models cannot e saved; ex jena-weather; boston-housing=====']",Reference Error,Crash,Incorrect Code Logic,,,change API,Replace API with another effective one,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.3""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.3""
  }
}
```",A.1,A.4
https://github.com/tensorflow/tfjs/issues/4276,.predict & .execute optional arguments type error,7,closed,2020-11-19T20:51:42Z,2020-11-25T15:55:17Z,"**System information**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): yes- OS Platform and Distribution (e.g.; Linux Ubuntu 16.04): MacOSX Big Sur- Mobile device (e.g. iPhone 8; Pixel 2; Samsung Galaxy) if the issue happens on mobile device: n/a- TensorFlow.js installed from (npm or script link): 2.7.0- TensorFlow.js version (use command below): 2.7.0- Browser version: n/a- Tensorflow.js Converter Version: n/a**Describe the current behavior**Using Typescript; `model.predict` & `model.execute` require 2 arguments; however in the docs these arguments are shown to be optional – https://js.tensorflow.org/api/latest/#tf.GraphModel.execute – This therefore throws an error. I'm hesitant to throw an empty object into `model.predict` because I can't see how the optional keys inside the object are set; and it's I can't find any documentation passing the default option for `model.execute` from my model so I can't bypass the typechecking.**Describe the expected behavior**For both `model.predict` & `model.execute` I should be able to pass one argument – the tensor inputs and it pass type check;.**Standalone code to reproduce the issue**https://codesandbox.io/s/admiring-river-u3jgf?file=/src/index.ts**Other info / logs** Include any logs or source code that would be helpful to<img width=""476"" alt=""image"" src=""https://user-images.githubusercontent.com/37798644/99721131-9fde0f00-2aa6-11eb-86a8-5d56d1d7c0db.png"">","[""I want to add; I'd do a PR but I want to be sure which is incorrect the type or the docs.=====""; '@joshuaellis looks like there is some inconsistency on the predict/execute method between InferenceModel interface and GraphModel.Can you check if you type the model as tf.GraphModel instead; will the type check pass?====='; ""I've changed it to `tf.GraphModel` but `model.execute` still fails with `tf.LayersModel`.in `training.d.ts` line 296 execute is written as –```jsexecute(inputs: Tensor | Tensor[] | NamedTensorMap; outputs: string | string[]): Tensor | Tensor[];```so outputs is still required.=====""; '@joshuaellis you can use predict() method with tf.layersModel; the output strings are not required there.====='; 'That would mean the docs are wrong then? as it says execute has an optional argument `outputs`====='; '@joshuaellis The doc for GraphModel and LayersModel are different; only execute on GraphModel says `outputs` is optional; LayersModel is consistent with InferenceModel interface; the `outputs` is required argument.====='; 'Are you satisfied with the resolution of your issue?<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4276"">Yes</a><a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/4276"">No</a>=====']",Reference Error,Crash,API Misuse,,,parameter modifier,Modify API Parameter usage,web application,Model Inference,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""[D] Incorrect Functionality"",
    ""subcategory"": ""[D.2] Poor Accuracy"",
    ""specific_type"": ""[D.2.1] Inconsistent Argument Requirements""
  },
  ""root_cause"": {
    ""primary_category"": ""[B] Configuration & Dependency Error"",
    ""subcategory"": ""[B.4] Confused Document""
  }
}
```",A.1,A.3
https://github.com/tensorflow/tfjs/issues/4269,Uncaught (in promise) TypeError: Cannot read property 'length' of null,1,closed,2020-11-19T04:01:14Z,2020-11-20T18:37:30Z,"<img width=""510"" alt=""Screen Shot 2020-11-19 at 11 54 16 AM"" src=""https://user-images.githubusercontent.com/11975415/99619229-f4a96780-2a5d-11eb-9e8a-43c19b6af25e.png"">I was following the [TensorFlow.js - Audio recognition using transfer learning](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#7) tutorial. When I called the train() function by pressing the 'train' button; an error came as above. Was it me or was there a mistake in the tutorial?  (I did follow the guide step by step...and test it with a localhost in the latest version of chrome)[code_from_tutorial.zip](https://github.com/tensorflow/tfjs/files/5564636/code_from_tutorial.zip)",['https://stackoverflow.com/questions/64905349/uncaught-in-promise-typeerror-cannot-read-property-length-of-null problem solved====='],Reference Error,Crash,API Misuse,,,change framework version,Changing version,web application,Model Training,"```
{
  ""bug_symptom"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.1"",
    ""specific_type"": ""A.1.4""
  },
  ""root_cause"": {
    ""primary_category"": ""A"",
    ""subcategory"": ""A.4""
  }
}
```",A.1,A.3
